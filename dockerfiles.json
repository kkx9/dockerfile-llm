{
    "What is the difference between the 'COPY' and 'ADD' commands in a Dockerfile?": "You should check the ADD and COPY documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that ADD can do more than COPY:\nADD allows <src> to be a URL\nReferring to comments below, the ADD documentation states that:\nIf is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.\nNote that the Best practices for writing Dockerfiles suggests using COPY where the magic of ADD is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy keep_this_archive_intact.tar.gz into your container, but instead, you spray the contents onto your filesystem.",
    "How do I pass environment variables to Docker containers?": "You can pass environment variables to your containers with the -e (alias --env) flag.\ndocker run -e xx=yy\nAn example from a startup script:\nsudo docker run -d -t -i -e REDIS_NAMESPACE='staging' \\ \n-e POSTGRES_ENV_POSTGRES_PASSWORD='foo' \\\n-e POSTGRES_ENV_POSTGRES_USER='bar' \\\n-e POSTGRES_ENV_DB_NAME='mysite_staging' \\\n-e POSTGRES_PORT_5432_TCP_ADDR='docker-db-1.hidden.us-east-1.rds.amazonaws.com' \\\n-e SITE_URL='staging.mysite.com' \\\n-p 80:80 \\\n--link redis:redis \\  \n--name container_name dockerhub_id/image_name\nOr, if you don't want to have the value on the command-line where it will be displayed by ps, etc., -e can pull in the value from the current environment if you just give it without the =:\nsudo PASSWORD='foo' docker run  [...] -e PASSWORD [...]\nIf you have many environment variables and especially if they're meant to be secret, you can use an env-file:\n$ docker run --env-file ./env.list ubuntu bash\nThe --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, mimicking the argument passed to --env. Comment lines need only be prefixed with #",
    "docker push error: denied: requested access to the resource is denied": "You may need to switch your docker repo to private before docker push.\nThanks to the answer provided by Dean Wu and this comment by ses, before pushing, remember to log out, then log in from the command line to your docker hub account\n# you may need log out first `docker logout` ref. https://stackoverflow.com/a/53835882/248616\ndocker login\nAccording to the docs:\nYou need to include the namespace for Docker Hub to associate it with your account.\nThe namespace is the same as your Docker Hub account name.\nYou need to rename the image to YOUR_DOCKERHUB_NAME/docker-whale.\nSo, this means you have to tag your image before pushing:\ndocker tag firstimage YOUR_DOCKERHUB_NAME/firstimage\nand then you should be able to push it.\ndocker push YOUR_DOCKERHUB_NAME/firstimage",
    "Difference between RUN and CMD in a Dockerfile": "RUN is an image build step, the state of the container after a RUN command will be committed to the container image. A Dockerfile can have many RUN steps that layer on top of one another to build the image.\nCMD is the command the container executes by default when you launch the built image. A Dockerfile will only use the final CMD defined. The CMD can be overridden when starting a container with docker run $image $other_command.\nENTRYPOINT is also closely related to CMD and can modify the way a CMD is interpreted when a container is started from an image.",
    "What's the difference between Docker Compose vs. Dockerfile": "Dockerfile\nA Dockerfile is a simple text file that contains the commands a user could call to assemble an image.\nExample, Dockerfile\nFROM ubuntu:latest\nMAINTAINER john doe \n\nRUN apt-get update\nRUN apt-get install -y python python-pip wget\nRUN pip install Flask\n\nADD hello.py /home/hello.py\n\nWORKDIR /home\nDocker Compose\nDocker Compose\nis a tool for defining and running multi-container Docker applications.\ndefine the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.\nget an app running in one command by just running docker-compose up\nExample, docker-compose.yml\nversion: \"3\"\nservices:\n  web:\n    build: .\n    ports:\n    - '5000:5000'\n    volumes:\n    - .:/code\n    - logvolume01:/var/log\n    links:\n    - redis\n  redis:\n    image: redis\n    volumes:\n      logvolume01: {}",
    "COPY with docker but with exclusion": "Create file .dockerignore in your docker build context directory (so in this case, most likely a directory that is a parent to node_modules) with one line in it:\n**/node_modules\nalthough you probably just want:\nnode_modules\nInfo about dockerignore: https://docs.docker.com/engine/reference/builder/#dockerignore-file",
    "How do I make a comment in a Dockerfile?": "You can use # at the beginning of a line to start a comment (whitespaces before # are allowed):\n# do some stuff\nRUN apt-get update \\\n    # install some packages\n    && apt-get install -y cron\n#'s in the middle of a string are passed to the command itself, e.g.:\nRUN echo 'we are running some # of cool things'",
    "How to set image name in Dockerfile?": "Workaround using docker-compose\nTagging of the image isn't supported inside the Dockerfile. This needs to be done in your build command. As a workaround, you can do the build with a docker-compose.yml that identifies the target image name and then run a docker-compose build. A sample docker-compose.yml would look like\nversion: '2'\n\nservices:\n  man:\n    build: .\n    image: dude/man:v2\nThat said, there's a push against doing the build with compose since that doesn't work with swarm mode deploys. So you're back to running the command as you've given in your question:\ndocker build -t dude/man:v2 .\nPersonally, I tend to build with a small shell script in my folder (build.sh) which passes any args and includes the name of the image there to save typing. And for production, the build is handled by a ci/cd server that has the image name inside the pipeline script.",
    "Dockerfile copy keep subdirectory structure": "Remove star from COPY, with this Dockerfile:\nFROM ubuntu\nCOPY files/ /files/\nRUN ls -la /files/*\nStructure is there:\n$ docker build .\nSending build context to Docker daemon 5.632 kB\nSending build context to Docker daemon \nStep 0 : FROM ubuntu\n ---> d0955f21bf24\nStep 1 : COPY files/ /files/\n ---> 5cc4ae8708a6\nRemoving intermediate container c6f7f7ec8ccf\nStep 2 : RUN ls -la /files/*\n ---> Running in 08ab9a1e042f\n/files/folder1:\ntotal 8\ndrwxr-xr-x 2 root root 4096 May 13 16:04 .\ndrwxr-xr-x 4 root root 4096 May 13 16:05 ..\n-rw-r--r-- 1 root root    0 May 13 16:04 file1\n-rw-r--r-- 1 root root    0 May 13 16:04 file2\n\n/files/folder2:\ntotal 8\ndrwxr-xr-x 2 root root 4096 May 13 16:04 .\ndrwxr-xr-x 4 root root 4096 May 13 16:05 ..\n-rw-r--r-- 1 root root    0 May 13 16:04 file1\n-rw-r--r-- 1 root root    0 May 13 16:04 file2\n ---> 03ff0a5d0e4b\nRemoving intermediate container 08ab9a1e042f\nSuccessfully built 03ff0a5d0e4b",
    "How to add users to Docker container?": "The trick is to use useradd instead of its interactive wrapper adduser. I usually create users with:\nRUN useradd -ms /bin/bash newuser\nwhich creates a home directory for the user and ensures that bash is the default shell.\nYou can then add:\nUSER newuser\nWORKDIR /home/newuser\nto your dockerfile. Every command afterwards as well as interactive sessions will be executed as user newuser:\ndocker run -t -i image\nnewuser@131b7ad86360:~$\nYou might have to give newuser the permissions to execute the programs you intend to run before invoking the user command.\nUsing non-privileged users inside containers is a good idea for security reasons. It also has a few drawbacks. Most importantly, people deriving images from your image will have to switch back to root before they can execute commands with superuser privileges.",
    "How to copy multiple files in one layer using a Dockerfile?": "COPY README.md package.json gulpfile.js __BUILD_NUMBER ./\nor\nCOPY [\"__BUILD_NUMBER\", \"README.md\", \"gulpfile\", \"another_file\", \"./\"]\nYou can also use wildcard characters in the sourcefile specification. See the docs for a little more detail.\nDirectories are special! If you write\nCOPY dir1 dir2 ./\nthat actually works like\nCOPY dir1/* dir2/* ./\nIf you want to copy multiple directories (not their contents) under a destination directory in a single command, you'll need to set up the build context so that your source directories are under a common parent and then COPY that parent.",
    "Docker: How to use bash with an Alpine based docker image?": "Alpine docker image doesn't have bash installed by default. You will need to add the following commands to get bash:\nRUN apk update && apk add bash\nIf you're using Alpine 3.3+ then you can just do:\nRUN apk add --no-cache bash\nTo keep the docker image size small. (Thanks to comment from @sprkysnrky)\nIf you just want to connect to the container and don't need bash, you can use:\ndocker run --rm -i -t alpine /bin/sh --login",
    "Difference between links and depends_on in docker_compose.yml": "The post needs an update after the links option is deprecated.\nBasically, links is no longer needed because its main purpose, making container reachable by another by adding environment variable, is included implicitly with network. When containers are placed in the same network, they are reachable by each other using their container name and other alias as host.\nFor docker run, --link is also deprecated and should be replaced by a custom network.\ndocker network create mynet\ndocker run -d --net mynet --name container1 my_image\ndocker run -it --net mynet --name container1 another_image\ndepends_on expresses start order (and implicitly image pulling order), which was a good side effect of links.",
    "Change directory command in Docker?": "To change into another directory use WORKDIR. All the RUN, CMD and ENTRYPOINT commands after WORKDIR will be executed from that directory.\nRUN git clone XYZ \nWORKDIR \"/XYZ\"\nRUN make",
    "Add a volume to Docker, but exclude a sub-folder": "Using docker-compose I'm able to use node_modules locally, but ignore it in the docker container using the following syntax in the docker-compose.yml\nvolumes:\n   - './angularApp:/opt/app'\n   - /opt/app/node_modules/\nSo everything in ./angularApp is mapped to /opt/app and then I create another mount volume /opt/app/node_modules/ which is now empty directory - even if in my local machine ./angularApp/node_modules is not empty.",
    "Dockerfile if else condition with external arguments": "It might not look that clean but you can have your Dockerfile (conditional) as follow:\nFROM centos:7\nARG arg\nRUN if [[ -z \"$arg\" ]] ; then echo Argument not provided ; else echo Argument is $arg ; fi\nand then build the image as:\ndocker build -t my_docker .  --build-arg arg=45\nor\ndocker build -t my_docker . ",
    "How to get an environment variable value into Dockerfile during \"docker build\"?": "You should use the ARG directive in your Dockerfile which is meant for this purpose.\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nSo your Dockerfile will have this line:\nARG request_domain\nor if you'd prefer a default value:\nARG request_domain=127.0.0.1\nNow you can reference this variable inside your Dockerfile:\nENV request_domain=$request_domain\nthen you will build your container like so:\n$ docker build --build-arg request_domain=mydomain Dockerfile\n\nNote 1: Your image will not build if you have referenced an ARG in your Dockerfile but excluded it in --build-arg.\nNote 2: If a user specifies a build argument that was not defined in the Dockerfile, the build outputs a warning:\n[Warning] One or more build-args [foo] were not consumed.",
    "An error, \"failed to solve with frontend dockerfile.v0\"": "I had experienced this issue after upgrading to the latest Docker Desktop version on Mac. Solved with the comment on this issue.\nSolution: Don't use buildkit and it works for me.\nexport DOCKER_BUILDKIT=0\nexport COMPOSE_DOCKER_CLI_BUILD=0",
    "Build and run Dockerfile with one command": "If you want to avoid tagging, docker build -q outputs nothing but the final image hash, which you can use as the argument to docker run:\ndocker run -it $(docker build -q .)\nAnd add --rm to docker run if you want the container removed automatically when it exits.\ndocker run --rm -it $(docker build -q .)",
    "Docker - unable to prepare context: unable to evaluate symlinks in Dockerfile path: GetFileAttributesEx": "While executing the following command,\ndocker build -t docker-whale .\ncheck that Dockerfile is present in your current working directory.",
    "How to define a variable in a Dockerfile?": "You can use ARG - see https://docs.docker.com/engine/reference/builder/#arg\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag. If a user specifies a build argument that was not defined in the Dockerfile, the build outputs an error.\nCan be useful with COPY during build time (e.g. copying tag specific content like specific folders) For example:\nARG MODEL_TO_COPY\nCOPY application ./application\nCOPY $MODEL_TO_COPY ./application/$MODEL_TO_COPY\nWhile building the container:\ndocker build --build-arg MODEL_TO_COPY=model_name -t <container>:<model_name specific tag> .",
    "What is the point of WORKDIR on Dockerfile?": "According to the documentation:\nThe WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.\nAlso, in the Docker best practices it recommends you to use it:\n... you should use WORKDIR instead of proliferating instructions like RUN cd \u2026 && do-something, which are hard to read, troubleshoot, and maintain.\nI would suggest to keep it.\nI think you can refactor your Dockerfile to something like:\nFROM node:latest\nWORKDIR /usr/src/app\nCOPY package.json .\nRUN npm install\nCOPY . ./\nEXPOSE 3000\nCMD [ \"npm\", \"start\" ] ",
    "Why doesn't Python app print anything when run in a detached docker container?": "Finally I found a solution to see Python output when running daemonized in Docker, thanks to @ahmetalpbalkan over at GitHub. Answering it here myself for further reference :\nUsing unbuffered output with\nCMD [\"python\",\"-u\",\"main.py\"]\ninstead of\nCMD [\"python\",\"main.py\"]\nsolves the problem; you can see the output now (both, stderr and stdout) via\ndocker logs myapp\nwhy -u ref\n- print is indeed buffered and docker logs will eventually give you that output, just after enough of it will have piled up\n- executing the same script with python -u gives instant output as said above\n- import logging + logging.warning(\"text\") gives the expected result even without -u\nwhat it means by python -u ref. > python --help | grep -- -u\n-u     : force the stdout and stderr streams to be unbuffered;",
    "Understanding \"VOLUME\" instruction in DockerFile": "In short: No, your VOLUME instruction is not correct.\nDockerfile's VOLUME specify one or more volumes given container-side paths. But it does not allow the image author to specify a host path. On the host-side, the volumes are created with a very long ID-like name inside the Docker root. On my machine this is /var/lib/docker/volumes.\nNote: Because the autogenerated name is extremely long and makes no sense from a human's perspective, these volumes are often referred to as \"unnamed\" or \"anonymous\".\nYour example that uses a '.' character will not even run on my machine, no matter if I make the dot the first or second argument. I get this error message:\ndocker: Error response from daemon: oci runtime error: container_linux.go:265: starting container process caused \"process_linux.go:368: container init caused \"open /dev/ptmx: no such file or directory\"\".\nI know that what has been said to this point is probably not very valuable to someone trying to understand VOLUME and -v and it certainly does not provide a solution for what you try to accomplish. So, hopefully, the following examples will shed some more light on these issues.\nMinitutorial: Specifying volumes\nGiven this Dockerfile:\nFROM openjdk:8u131-jdk-alpine\nVOLUME vol1 vol2\n(For the outcome of this minitutorial, it makes no difference if we specify vol1 vol2 or /vol1 /vol2 \u2014 this is because the default working directory within a Dockerfile is /)\nBuild it:\ndocker build -t my-openjdk\nRun:\ndocker run --rm -it my-openjdk\nInside the container, run ls in the command line and you'll notice two directories exist; /vol1 and /vol2.\nRunning the container also creates two directories, or \"volumes\", on the host-side.\nWhile having the container running, execute docker volume ls on the host machine and you'll see something like this (I have replaced the middle part of the name with three dots for brevity):\nDRIVER    VOLUME NAME\nlocal     c984...e4fc\nlocal     f670...49f0\nBack in the container, execute touch /vol1/weird-ass-file (creates a blank file at said location).\nThis file is now available on the host machine, in one of the unnamed volumes lol. It took me two tries because I first tried the first listed volume, but eventually I did find my file in the second listed volume, using this command on the host machine:\nsudo ls /var/lib/docker/volumes/f670...49f0/_data\nSimilarly, you can try to delete this file on the host and it will be deleted in the container as well.\nNote: The _data folder is also referred to as a \"mount point\".\nExit out from the container and list the volumes on the host. They are gone. We used the --rm flag when running the container and this option effectively wipes out not just the container on exit, but also the volumes.\nRun a new container, but specify a volume using -v:\ndocker run --rm -it -v /vol3 my-openjdk\nThis adds a third volume and the whole system ends up having three unnamed volumes. The command would have crashed had we specified only -v vol3. The argument must be an absolute path inside the container. On the host-side, the new third volume is anonymous and resides together with the other two volumes in /var/lib/docker/volumes/.\nIt was stated earlier that the Dockerfile can not map to a host path which sort of pose a problem for us when trying to bring files in from the host to the container during runtime. A different -v syntax solves this problem.\nImagine I have a subfolder in my project directory ./src that I wish to sync to /src inside the container. This command does the trick:\ndocker run -it -v $(pwd)/src:/src my-openjdk\nBoth sides of the : character expects an absolute path. Left side being an absolute path on the host machine, right side being an absolute path inside the container. pwd is a command that \"print current/working directory\". Putting the command in $() takes the command within parenthesis, runs it in a subshell and yields back the absolute path to our project directory.\nPutting it all together, assume we have ./src/Hello.java in our project folder on the host machine with the following contents:\npublic class Hello {\n    public static void main(String... ignored) {\n        System.out.println(\"Hello, World!\");\n    }\n}\nWe build this Dockerfile:\nFROM openjdk:8u131-jdk-alpine\nWORKDIR /src\nENTRYPOINT javac Hello.java && java Hello\nWe run this command:\ndocker run -v $(pwd)/src:/src my-openjdk\nThis prints \"Hello, World!\".\nThe best part is that we're completely free to modify the .java file with a new message for another output on a second run - without having to rebuild the image =)\nFinal remarks\nI am quite new to Docker, and the aforementioned \"tutorial\" reflects information I gathered from a 3-day command line hackathon. I am almost ashamed I haven't been able to provide links to clear English-like documentation backing up my statements, but I honestly think this is due to a lack of documentation and not personal effort. I do know the examples work as advertised using my current setup which is \"Windows 10 -> Vagrant 2.0.0 -> Docker 17.09.0-ce\".\nThe tutorial does not solve the problem \"how do we specify the container's path in the Dockerfile and let the run command only specify the host path\". There might be a way, I just haven't found it.\nFinally, I have a gut feeling that specifying VOLUME in the Dockerfile is not just uncommon, but it's probably a best practice to never use VOLUME. For two reasons. The first reason we have already identified: We can not specify the host path - which is a good thing because Dockerfiles should be very agnostic to the specifics of a host machine. But the second reason is people might forget to use the --rm option when running the container. One might remember to remove the container but forget to remove the volume. Plus, even with the best of human memory, it might be a daunting task to figure out which of all anonymous volumes are safe to remove.",
    "Integrating Python Poetry with Docker": "There are several things to keep in mind when using Poetry together with Docker.\nInstallation\nOfficial way to install Poetry is via:\ncurl -sSL https://install.python-poetry.org | python3 -\nThis way allows Poetry and its dependencies to be isolated from your dependencies.\nYou can also use pip install 'poetry==$POETRY_VERSION'. But, this will install Poetry and its dependencies into your main site-packages/. It might not be ideal.\nAlso, pin this version in your pyproject.toml as well:\n[build-system]\n# Should be the same as `$POETRY_VERSION`:\nrequires = [\"poetry-core>=1.6\"]\nbuild-backend = \"poetry.core.masonry.api\"\nIt will protect you from version mismatch between your local and Docker environments.\nCaching dependencies\nWe want to cache our requirements and only reinstall them when pyproject.toml or poetry.lock files change. Otherwise builds will be slow. To achieve working cache layer we should put:\nCOPY poetry.lock pyproject.toml /code/\nafter Poetry is installed, but before any other files are added.\nVirtualenv\nThe next thing to keep in mind is virtualenv creation. We do not need it in Docker. It is already isolated. So, we use POETRY_VIRTUALENVS_CREATE=false or poetry config virtualenvs.create false setting to turn it off.\nDevelopment vs. Production\nIf you use the same Dockerfile for both development and production as I do, you will need to install different sets of dependencies based on some environment variable:\npoetry install $(test \"$YOUR_ENV\" == production && echo \"--only=main\")\nThis way $YOUR_ENV will control which dependencies set will be installed: all (default) or production only with --only=main flag.\nYou may also want to add some more options for better experience:\n--no-interaction not to ask any interactive questions\n--no-ansi flag to make your output more log friendly\nResult\nYou will end up with something similar to:\nFROM python:3.11.5-slim-bookworm\n\nARG YOUR_ENV\n\nENV YOUR_ENV=${YOUR_ENV} \\\n  PYTHONFAULTHANDLER=1 \\\n  PYTHONUNBUFFERED=1 \\\n  PYTHONHASHSEED=random \\\n  PIP_NO_CACHE_DIR=off \\\n  PIP_DISABLE_PIP_VERSION_CHECK=on \\\n  PIP_DEFAULT_TIMEOUT=100 \\\n  # Poetry's configuration:\n  POETRY_NO_INTERACTION=1 \\\n  POETRY_VIRTUALENVS_CREATE=false \\\n  POETRY_CACHE_DIR='/var/cache/pypoetry' \\\n  POETRY_HOME='/usr/local'\n  POETRY_VERSION=1.7.1\n  # ^^^\n  # Make sure to update it!\n\n# System deps:\nRUN curl -sSL https://install.python-poetry.org | python3 -\n\n# Copy only requirements to cache them in docker layer\nWORKDIR /code\nCOPY poetry.lock pyproject.toml /code/\n\n# Project initialization:\nRUN poetry install $(test \"$YOUR_ENV\" == production && echo \"--only=main\") --no-interaction --no-ansi\n\n# Creating folders, and files for a project:\nCOPY . /code\nYou can find a fully working real-life example here.",
    "How to name Dockerfiles": "[Please read the full answer]Don't change the name of the dockerfile if you want to use the autobuilder at hub.docker.com. Don't use an extension for docker files, leave it null. File name should just be: (no extension at all)\nDockerfile\nHowever, now you can name dockerfiles like,\ntest1.Dockerfile\n$ docker build -f dockerfiles/test1.Dockerfile  -t test1_app .\nor\nDockerfile.test1\n$ docker build -f dockerfiles/Dockerfile.test1  -t test1_app .\nThis will also work.\nIf you handle multiple files that live in the same context, you could use STDIN:\ntest1.Dockerfile\n$ docker build -t test1_app - < test1.Dockerfile",
    "Why is Docker installed but not Docker Compose?": "You also need to install Docker Compose.\nSee the manual. Here are the commands you need to execute:\nsudo curl -L \"https://github.com/docker/compose/releases/download/v2.29.2/docker-compose-$(uname -s)-$(uname -m)\"  -o /usr/local/bin/docker-compose\nsudo mv /usr/local/bin/docker-compose /usr/bin/docker-compose\nsudo chmod +x /usr/bin/docker-compose      \nNote:\nMake sure that the link pointing to the GitHub release is not outdated!. Check out the latest releases on GitHub.",
    "Multiple RUN vs. single chained RUN in Dockerfile, which is better?": "When possible, I always merge together commands that create files with commands that delete those same files into a single RUN line. This is because each RUN line adds a layer to the image, the output is quite literally the filesystem changes that you could view with docker diff on the temporary container it creates. If you delete a file that was created in a different layer, all the union filesystem does is register the filesystem change in a new layer, the file still exists in the previous layer and is shipped over the networked and stored on disk. So if you download source code, extract it, compile it into a binary, and then delete the tgz and source files at the end, you really want this all done in a single layer to reduce image size.\nNext, I personally split up layers based on their potential for reuse in other images and expected caching usage. If I have 4 images, all with the same base image (e.g. debian), I may pull a collection of common utilities to most of those images into the first run command so the other images benefit from caching.\nOrder in the Dockerfile is important when looking at image cache reuse. I look at any components that will update very rarely, possibly only when the base image updates and put those high up in the Dockerfile. Towards the end of the Dockerfile, I include any commands that will run quick and may change frequently, e.g. adding a user with a host specific UID or creating folders and changing permissions. If the container includes interpreted code (e.g. JavaScript) that is being actively developed, that gets added as late as possible so that a rebuild only runs that single change.\nIn each of these groups of changes, I consolidate as best I can to minimize layers. So if there are 4 different source code folders, those get placed inside a single folder so it can be added with a single command. Any package installs from something like apt-get are merged into a single RUN when possible to minimize the amount of package manager overhead (updating and cleaning up).\nUpdate for multi-stage builds:\nI worry much less about reducing image size in the non-final stages of a multi-stage build. When these stages aren't tagged and shipped to other nodes, you can maximize the likelihood of a cache reuse by splitting each command to a separate RUN line.\nHowever, this isn't a perfect solution to squashing layers since all you copy between stages are the files, and not the rest of the image meta-data like environment variable settings, entrypoint, and command. And when you install packages in a linux distribution, the libraries and other dependencies may be scattered throughout the filesystem, making a copy of all the dependencies difficult.\nBecause of this, I use multi-stage builds as a replacement for building binaries on a CI/CD server, so that my CI/CD server only needs to have the tooling to run docker build, and not have a jdk, nodejs, go, and any other compile tools installed.",
    "What is the use of PYTHONUNBUFFERED in docker file?": "Setting PYTHONUNBUFFERED to a non-empty value different from 0 ensures that the python output i.e. the stdout and stderr streams are sent straight to terminal (e.g. your container log) without being first buffered and that you can see the output of your application (e.g. django logs) in real time.\nThis also ensures that no partial output is held in a buffer somewhere and never written in case the python application crashes.\nSince this has been mentioned in several comments and supplementary answers, note that PYTHONUNBUFFERED has absolutely no influence on the input (i.e. the stdin stream).\nIn other words, turning off buffering to stdout/stderr in a docker container is mainly a concern of getting as much information from your running application as fast as possible in the container log and not loosing anything in case of a crash.\nNote that turning buffering off can have an impact on performance depending on your hardware/environment. Meanwhile it should be minor in most situations (unless you have slow disks or are writing a tremendous amount of logs or had the bad idea to configure your docker daemon to write your logs on a slow network drive...). If this is a concern, buffering can be left on and you can flush the buffer directly from your application when needed. See link [4] below on this subject.\nReferences:\n[1] https://docs.python.org/3/using/cmdline.html#envvar-PYTHONUNBUFFERED\n[2] https://alphacoder.xyz/dockerizing-django/\n[3] https://towardsdatascience.com/how-to-contain-your-first-django-in-docker-and-access-it-from-aws-fdb0081bdf1d\n[4] https://github.com/aws/amazon-sagemaker-examples/issues/319",
    "How to pass arguments to a Dockerfile?": "As of Docker 1.9, You are looking for --build-arg and the ARG instruction.\nCheck out this document for reference. This will allow you to add ARG arg to the Dockerfile and then build with\ndocker build --build-arg arg=2.3 .",
    "How do I Docker COPY as non root?": "For versions v17.09.0-ce and newer\nUse the optional flag --chown=<user>:<group> with either the ADD or COPY commands.\nFor example\nCOPY --chown=<user>:<group> <hostPath> <containerPath>\nThe documentation for the --chown flag is now live on the main Dockerfile Reference page.\nIssue 34263 has been merged and is available in release v17.09.0-ce.\nFor versions older than v17.09.0-ce\nDocker doesn't support COPY as a user other than root. You need to chown / chmod the file after the COPY command.\nExample Dockerfile:\nfrom centos:6\nRUN groupadd -r myuser && adduser -r -g myuser myuser\nUSER myuser\n#Install code, configure application, etc...\nUSER root\nCOPY run-my-app.sh /usr/local/bin/run-my-app.sh\nRUN chown myuser:myuser /usr/local/bin/run-my-app.sh && \\\n    chmod 744 /usr/local/bin/run-my-app.sh\nUSER myuser\nENTRYPOINT [\"/usr/local/bin/run-my-app.sh\"]\nPrevious to v17.09.0-ce, the Dockerfile Reference for the COPY command said:\nAll new files and directories are created with a UID and GID of 0.\nHistory This feature has been tracked through multiple GitHub issues: 6119, 9943, 13600, 27303, 28499, Issue 30110.\nIssue 34263 is the issue that implemented the optional flag functionality and Issue 467 updated the documentation.",
    "Connect to mysql in a docker container from the host": "If your Docker MySQL host is running correctly you can connect to it from local machine, but you should specify host, port and protocol like this:\nmysql -h localhost -P 3306 --protocol=tcp -u root\nChange 3306 to port number you have forwarded from Docker container (in your case it will be 12345).\nBecause you are running MySQL inside Docker container, socket is not available and you need to connect through TCP. Setting \"--protocol\" in the mysql command will change that.",
    "standard_init_linux.go:190: exec user process caused \"no such file or directory\" - Docker": "Use notepad++, go to edit -> EOL conversion -> change from CRLF to LF.\nupdate: For VScode users: you can change CRLF to LF by clicking on CRLF present on lower right side in the status bar",
    "/bin/sh: apt-get: not found": "The image you're using is Alpine based, so you can't use apt-get because it's Ubuntu's package manager.\nTo fix this just use:\napk update and apk add",
    "Multiple FROMs - what it means": "As of May 2017, multiple FROMs can be used in a single Dockerfile.\nSee \"Builder pattern vs. Multi-stage builds in Docker\" (by Alex Ellis) and PR 31257 by T\u00f5nis Tiigi.\nThe general syntax involves adding FROM additional times within your Dockerfile - whichever is the last FROM statement is the final base image. To copy artifacts and outputs from intermediate images use COPY --from=<base_image_number>.\nFROM golang:1.7.3 as builder\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go    .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /go/src/github.com/alexellis/href-counter/app    .\nCMD [\"./app\"]  \nThe result would be two images, one for building, one with just the resulting app (much, much smaller)\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n\nmulti               latest              bcbbf69a9b59        6 minutes ago       10.3MB  \ngolang              1.7.3               ef15416724f6        4 months ago        672MB  \nwhat is a base image?\nA set of files, plus EXPOSE'd ports, ENTRYPOINT and CMD.\nYou can add files and build a new image based on that base image, with a new Dockerfile starting with a FROM directive: the image mentioned after FROM is \"the base image\" for your new image.\ndoes it mean that if I declare neo4j/neo4j in a FROM directive, that when my image is run the neo database will automatically run and be available within the container on port 7474?\nOnly if you don't overwrite CMD and ENTRYPOINT.\nBut the image in itself is enough: you would use a FROM neo4j/neo4j if you had to add files related to neo4j for your particular usage of neo4j.\n2018: With the introduction of the --target option in docker build, you gain even more control over multi-stage builds.\nThis feature enables you to select which FROM statement in your Dockerfile you wish to build, allowing for more modular and efficient Docker images. This is especially useful in scenarios where you might want to:\nBuild Only the Dependencies: Create an image that only contains the dependencies of your project. This can be useful for caching purposes or for environments where you only need to run tests or static analysis tools.\nSeparate Build and Runtime Environments: Compile or build your application in a full-featured build environment but create a smaller, more secure image for deployment that only includes the runtime environment and the compiled application.\nCreate Images for Different Environments: Have different stages for development, testing, and production environments, each tailored with the specific tools and configurations needed for those environments.\nExample Using --target\nGiven a Dockerfile with multiple stages named builder, tester, and deployer, you can build up to the tester stage using the --target option like so:\ndocker build --target tester -t myapp-test .\nThis command tells Docker to stop building after the tester stage has been completed, thus creating an image that includes everything from the base image up to the tester stage, but excluding anything from deployer stage and beyond.\nDockerfile Example with --target Usage\n# Builder stage\nFROM golang:1.7.3 as builder\nWORKDIR /go/src/github.com/example/project/\n# Assume app.go exists and has a function\nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\n# Tester stage\nFROM builder as tester\nCOPY . .\nRUN go test ./...\n\n# Deployer stage\nFROM alpine:latest as deployer\nCOPY --from=builder /go/src/github.com/example/project/app /app\nCMD [\"/app\"]\nUsing the --target option with this Dockerfile allows for flexibility in building images tailored for specific steps of the development lifecycle.\nAs illustrated in \"Building a multi-stage Dockerfile with --target flag builds all stages instead of just the specified one\", this works well with BuildKit, which is now (2023+) the default builder.\nFrom that page, you have Igor Kulebyakin's answer:\nIf one wants to make sure that the current target stage is force re-built even if it has already been cached without rebuilding the previous dependent stages, once can use the docker build --no-cache-filter flag.\nAn example, given you have a multi-stage Dockerfile with a 'test' stage, would be:\ndocker build --no-cache-filter test --target test --tag your-image-name:version .",
    "How can I use a variable inside a Dockerfile CMD?": "When you use an execution list, as in...\nCMD [\"django-admin\", \"startproject\", \"$PROJECTNAME\"]\n...then Docker will execute the given command directly, without involving a shell. Since there is no shell involved, that means:\nNo variable expansion\nNo wildcard expansion\nNo i/o redirection with >, <, |, etc\nNo multiple commands via command1; command2\nAnd so forth.\nIf you want your CMD to expand variables, you need to arrange for a shell. You can do that like this:\nCMD [\"sh\", \"-c\", \"django-admin startproject $PROJECTNAME\"]\nOr you can use a simple string instead of an execution list, which gets you a result largely identical to the previous example:\nCMD django-admin startproject $PROJECTNAME",
    "How do I set environment variables during the \"docker build\" process?": "ARG is for setting environment variables which are used during the docker build process - they are not present in the final image, which is why you don't see them when you use docker run.\nYou use ARG for settings that are only relevant when the image is being built, and aren't needed by containers which you run from the image. You can use ENV for environment variables to use during the build and in containers.\nWith this Dockerfile:\nFROM ubuntu\nARG BUILD_TIME=abc\nENV RUN_TIME=123\nRUN touch /env.txt\nRUN printenv > /env.txt\nYou can override the build arg as you have done with docker build -t temp --build-arg BUILD_TIME=def .. Then you get what you expect:\n> docker run temp cat /env.txt                                                                                         \nHOSTNAME=b18b9cafe0e0                                                                                                  \nRUN_TIME=123                                                                                                           \nHOME=/root                                                                                                             \nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin                                                      \nBUILD_TIME=def                                                                                                         \nPWD=/ ",
    "what is docker run -it flag?": "-it is short for --interactive + --tty. When you docker run with this command it takes you straight inside the container.\n-d is short for --detach, which means you just run the container and then detach from it. Essentially, you run container in the background.\nEdit: So if you run the Docker container with -itd, it runs both the -it options and detaches you from the container. As a result, your container will still be running in the background even without any default app to run.",
    "How do I use Docker environment variable in ENTRYPOINT array?": "You're using the exec form of ENTRYPOINT. Unlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, ENTRYPOINT [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: ENTRYPOINT [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.(from Dockerfile reference)\nIn your case, I would use shell form\nENTRYPOINT ./greeting --message \"Hello, $ADDRESSEE\\!\"",
    "Conditional COPY/ADD in Dockerfile?": "Here is a simple workaround:\nCOPY foo file-which-may-exist* /target\nMake sure foo exists, since COPY needs at least one valid source.\nIf file-which-may-exist is present, it will also be copied.\nNOTE: You should take care to ensure that your wildcard doesn't pick up other files which you don't intend to copy. To be more careful, you could use file-which-may-exist? instead (? matches just a single character).\nOr even better, use a character class like this to ensure that only one file can be matched:\nCOPY foo file-which-may-exis[t] /target",
    "apt-get install tzdata noninteractive": "This is the script I used\n(Updated Version with input from @elquimista from the comments)\n#!/bin/bash\n\nln -fs /usr/share/zoneinfo/America/New_York /etc/localtime\nDEBIAN_FRONTEND=noninteractive apt-get install -y tzdata\ndpkg-reconfigure --frontend noninteractive tzdata\nSeems to work fine.\nAs one liner:\nDEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends tzdata",
    "Alpine Dockerfile advantages of --no-cache vs. rm /var/cache/apk/*": "The --no-cache option allows to not cache the index locally, which is useful for keeping containers small.\nLiterally it equals apk update in the beginning and rm -rf /var/cache/apk/* in the end.\nSome example where we use --no-cache option:\n$ docker run -ti alpine:3.7\n/ # apk add nginx\nWARNING: Ignoring APKINDEX.70c88391.tar.gz: No such file or directory\nWARNING: Ignoring APKINDEX.5022a8a2.tar.gz: No such file or directory\nERROR: unsatisfiable constraints:\n  nginx (missing):\n    required by: world[nginx]\n/ # \n/ # apk add --no-cache nginx\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\n(1/2) Installing pcre (8.41-r1)\n(2/2) Installing nginx (1.12.2-r3)\nExecuting nginx-1.12.2-r3.pre-install\nExecuting busybox-1.27.2-r7.trigger\nOK: 6 MiB in 13 packages\n/ # \n/ # ls -la /var/cache/apk/\ntotal 8\ndrwxr-xr-x    2 root     root          4096 Jan  9 19:37 .\ndrwxr-xr-x    5 root     root          4096 Mar  5 20:29 ..\nAnother example where we don't use --no-cache option:\n$ docker run -ti alpine:3.7\n/ # apk add nginx\nWARNING: Ignoring APKINDEX.70c88391.tar.gz: No such file or directory\nWARNING: Ignoring APKINDEX.5022a8a2.tar.gz: No such file or directory\nERROR: unsatisfiable constraints:\n  nginx (missing):\n    required by: world[nginx]\n/ # \n/ # apk update\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\nv3.7.0-107-g15dd6b8ab3 [http://dl-cdn.alpinelinux.org/alpine/v3.7/main]\nv3.7.0-105-g4b8b158c40 [http://dl-cdn.alpinelinux.org/alpine/v3.7/community]\nOK: 9048 distinct packages available\n/ # \n/ # apk add nginx\n(1/2) Installing pcre (8.41-r1)\n(2/2) Installing nginx (1.12.2-r3)\nExecuting nginx-1.12.2-r3.pre-install\nExecuting busybox-1.27.2-r7.trigger\nOK: 6 MiB in 13 packages\n/ # \n/ # ls -la /var/cache/apk/\ntotal 1204\ndrwxr-xr-x    2 root     root          4096 Mar  5 20:31 .\ndrwxr-xr-x    6 root     root          4096 Mar  5 20:31 ..\n-rw-r--r--    1 root     root        451508 Mar  3 00:30 APKINDEX.5022a8a2.tar.gz\n-rw-r--r--    1 root     root        768680 Mar  5 09:39 APKINDEX.70c88391.tar.gz\n/ # \n/ # rm -vrf /var/cache/apk/*\nremoved '/var/cache/apk/APKINDEX.5022a8a2.tar.gz'\nremoved '/var/cache/apk/APKINDEX.70c88391.tar.gz'\nAs you can see both cases are valid. As for me, using --no-cache option is more elegant.",
    "What is .build-deps for apk add --virtual command?": "If you see the documentation\n-t, --virtual NAME    Instead of adding all the packages to 'world', create a new \n                      virtual package with the listed dependencies and add that \n                      to 'world'; the actions of the command are easily reverted \n                      by deleting the virtual package\nWhat that means is when you install packages, those packages are not added to global packages. And this change can be easily reverted. So if I need gcc to compile a program, but once the program is compiled I no more need gcc.\nI can install gcc, and other required packages in a virtual package and all of its dependencies and everything can be removed this virtual package name. Below is an example usage\nRUN apk add --virtual mypacks gcc vim \\\n && apk del mypacks\nThe next command will delete all 18 packages installed with the first command.\nIn docker these must be executed as a single RUN command (as shown above), otherwise it will not reduce the image size.",
    "ARG or ENV, which one to use in this case?": "From Dockerfile reference:\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nThe ENV instruction sets the environment variable <key> to the value <value>.\nThe environment variables set using ENV will persist when a container is run from the resulting image.\nSo if you need build-time customization, ARG is your best choice.\nIf you need run-time customization (to run the same image with different settings), ENV is well-suited.\nIf I want to add let's say 20 (a random number) of extensions or any other feature that can be enable|disable\nGiven the number of combinations involved, using ENV to set those features at runtime is best here.\nBut you can combine both by:\nbuilding an image with a specific ARG\nusing that ARG as an ENV\nThat is, with a Dockerfile including:\nARG var\nENV var=${var}\nYou can then either build an image with a specific var value at build-time (docker build --build-arg var=xxx), or run a container with a specific runtime value (docker run -e var=yyy)",
    "npm ERR! Tracker \"idealTree\" already exists while creating the Docker image for Node project": "This issue is happening due to changes in NodeJS starting with version 15. When no WORKDIR is specified, npm install is executed in the root directory of the container, which is resulting in this error. Executing the npm install in a project directory of the container specified by WORKDIR resolves the issue.\nUse the following Dockerfile:\n# Specify a base image\nFROM node:alpine\n\n#Install some dependencies\n\nWORKDIR /usr/app\nCOPY ./ /usr/app\nRUN npm install\n\n# Set up a default command\nCMD [ \"npm\",\"start\" ]",
    "Docker-compose check if mysql connection is ready": "version: \"2.1\"\nservices:\n    api:\n        build: .\n        container_name: api\n        ports:\n            - \"8080:8080\"\n        depends_on:\n            db:\n                condition: service_healthy\n    db:\n        container_name: db\n        image: mysql\n        ports:\n            - \"3306\"\n        environment:\n            MYSQL_ALLOW_EMPTY_PASSWORD: \"yes\"\n            MYSQL_USER: \"user\"\n            MYSQL_PASSWORD: \"password\"\n            MYSQL_DATABASE: \"database\"\n        healthcheck:\n            test: [\"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\"]\n            timeout: 20s\n            retries: 10\nThe api container will not start until the db container is healthy (basically until mysqladmin is up and accepting connections.)",
    "Dockerfile - set ENV to result of command": "As an addition to DarkSideF answer.\nYou should be aware that each line/command in Dockerfile is ran in another container.\nYou can do something like this:\nRUN export bleah=$(hostname -f);echo $bleah;\nThis is run in a single container.",
    "How to update /etc/hosts file in Docker image during \"docker build\"": "With a more recent version of docker, this could be done with docker-compose and its extra_hosts directive\nAdd hostname mappings.\nUse the same values as the docker run client --add-host parameter (which should already be available for docker 1.8).\nextra_hosts:\n  - \"somehost:162.242.195.82\"\n  - \"otherhost:50.31.209.229\"\nIn short: modify /etc/hosts of your container when running it, instead of when building it.\nWith Docker 17.x+, you have a docker build --add-host mentioned below, but, as commented in issue 34078 and in this answer:\nThe --add-host feature during build is designed to allow overriding a host during build, but not to persist that configuration in the image.\nThose links point to strategies for dealing with the problem at hand:\nRun an internal DNS; you can set the default DNS server to use in the daemon; that way every container started will automatically use the configured DNS by default\nUse docker compose and provide a docker-compose.yml to your developers.\nThe docker compose file allows you to specify all the options that should be used when starting a container, so developers could just docker compose up to start the container with all the options they need to set.\nThese solutions can take advantage of using of the docker-compose method that was suggested earlier in the answer, with its extra_hosts directive.",
    "Docker images - types. Slim vs slim-stretch vs stretch vs alpine": "Per docker library docs (quote and links below), here's a summary:\nopenjdk:<version>\nThe defacto image. Use it if unsure.\nopenjdk:<version>-buster, openjdk:<version>-stretch and openjdk:<version>-jessie\nbuster, jessie or stretch are the suite code names for releases of Debian and indicate which release the image is based on.\nopenjdk:<version>-alpine\nSimilarly, this image is based on the Alpine Linux, thus being a very small base image. It is recommended if you need an image size is as small as possible. The caveat is that it uses some unusual libs, but shouldn't be a problem for most software. In doubt, check the official docs below.\nopenjdk:<version> (from 12 onwards), openjdk:<version>-oracle and openjdk:<version>-oraclelinux7\nStarting with openjdk:12 the default image as well as the -oracle and -oraclelinux7 variants are based on the official Oracle Linux 7 image. The OpenJDK binaries in the default image as well as the -oracle and -oraclelinux7 variants are built by Oracle and are sourced from the OpenJDK community.\nopenjdk:<version>-slim\nThis image only contains the minimal packages needed to run Java (and is missing many of the UI-related Java libraries, for instance). Unless you are working in an environment where only the openjdk image will be deployed and you have space constraints, the default image is recommended over this one.\nopenjdk:<version>-windowsservercore\nThis image is based on Windows Server Core (microsoft/windowsservercore).\n\nFull docs (version shown below here, latest version here):\nImage Variants\nThe openjdk images come in many flavors, each designed for a specific use case.\nopenjdk:<version>\nThis is the defacto image. If you are unsure about what your needs are, you probably want to use this one. It is designed to be used both as a throw away container (mount your source code and start the container to start your app), as well as the base to build other images off of.\nSome of these tags may have names like jessie or stretch in them. These are the suite code names for releases of Debian and indicate which release the image is based on.\nopenjdk:<version>-alpine\nThis image is based on the popular Alpine Linux project, available in the alpine official image. Alpine Linux is much smaller than most distribution base images (~5MB), and thus leads to much slimmer images in general.\nThis variant is highly recommended when final image size being as small as possible is desired. The main caveat to note is that it does use musl libc instead of glibc and friends, so certain software might run into issues depending on the depth of their libc requirements. However, most software doesn't have an issue with this, so this variant is usually a very safe choice. See this Hacker News comment thread for more discussion of the issues that might arise and some pro/con comparisons of using Alpine-based images.\nTo minimize image size, it's uncommon for additional related tools (such as git or bash) to be included in Alpine-based images. Using this image as a base, add the things you need in your own Dockerfile (see the alpine image description for examples of how to install packages if you are unfamiliar).\nopenjdk:<version>-windowsservercore\nThis image is based on Windows Server Core (microsoft/windowsservercore). As such, it only works in places which that image does, such as Windows 10 Professional/Enterprise (Anniversary Edition) or Windows Server 2016.\nFor information about how to get Docker running on Windows, please see the relevant \"Quick Start\" guide provided by Microsoft:\nWindows Server Quick Start\nWindows 10 Quick Start\nopenjdk:<version>-slim\nThis image installs the -headless package of OpenJDK and so is missing many of the UI-related Java libraries and some common packages contained in the default tag. It only contains the minimal packages needed to run Java. Unless you are working in an environment where only the openjdk image will be deployed and you have space constraints, we highly recommend using the default image of this repository.",
    "Deploying a minimal flask app in docker - server connection issues": "The problem is you are only binding to the localhost interface, you should be binding to 0.0.0.0 if you want the container to be accessible from outside. If you change:\nif __name__ == '__main__':\n    app.run()\nto\nif __name__ == '__main__':\n    app.run(host='0.0.0.0')\nIt should work.\nNote that this will bind to all interfaces on the host, which may in some circumstances be a security risk - see https://stackoverflow.com/a/58138250/4332 for more information on binding to a specific interface.",
    "docker-compose, run a script after container has started?": "This is the way I use for calling a script after a container is started without overriding the entrypoint.\nIn my example, I used it for initializing the replicaset of my local MongoDB\nservices:\n  mongo:\n    image: mongo:4.2.8\n    hostname: mongo\n    container_name: mongodb\n    entrypoint: [\"/usr/bin/mongod\",\"--bind_ip_all\",\"--replSet\",\"rs0\"]\n    ports:\n      - 27017:27017\n  mongosetup:\n    image: mongo:4.2.8\n    depends_on:\n      - mongo\n    restart: \"no\"\n    entrypoint: [ \"bash\", \"-c\", \"sleep 10 && mongo --host mongo:27017 --eval 'rs.initiate()'\"]      \nIn the first part, I simply launch my service (mongo)\nThe second service use a \"bash\" entry point AND a restart: no <= important\nI also use a depends_on between service and setup service for manage the launch order.",
    "Run a script in Dockerfile": "RUN and ENTRYPOINT are two different ways to execute a script.\nRUN means it creates an intermediate container, runs the script and freeze the new state of that container in a new intermediate image. The script won't be run after that: your final image is supposed to reflect the result of that script.\nENTRYPOINT means your image (which has not executed the script yet) will create a container, and runs that script.\nIn both cases, the script needs to be added, and a RUN chmod +x /bootstrap.sh is a good idea.\nIt should also start with a shebang (like #!/bin/sh)\nConsidering your script (bootstrap.sh: a couple of git config --global commands), it would be best to RUN that script once in your Dockerfile, but making sure to use the right user (the global git config file is %HOME%/.gitconfig, which by default is the /root one)\nAdd to your Dockerfile:\nRUN /bootstrap.sh\nThen, when running a container, check the content of /root/.gitconfig to confirm the script was run.",
    "Dockerfile build - possible to ignore error?": "Sure. Docker is just responding to the error codes returned by the RUN shell scripts in the Dockerfile. If your Dockerfile has something like:\nRUN make\nYou could replace that with:\nRUN make; exit 0\nThis will always return a 0 (success) exit code. The disadvantage here is that your image will appear to build successfully even if there are actual errors in the build process.",
    "How to pass environment variable to docker-compose up": "You have two options (option 2. overrides 1.):\nCreate the .env file as already suggested in another answer.\nPrepend KEY=VALUE pair(s) to your docker-compose command, e.g:\nKB_DB_TAG_VERSION=kb-1.3.20-v1.0.0 docker-compose up\nExporting it earlier in a script should also work, e.g.:\nexport KB_DB_TAG_VERSION=kb-1.3.20-v1.0.0\ndocker-compose up\nKeep in mind that these options just pass an environment varible to the docker-compose.yml file, not to a container. For an environment variable to be actually passed to a container you always need something like this in your docker-compose.yml:\n  environment:\n    - KB_DB_TAG_VERSION=$KB_DB_TAG_VERSION",
    "Docker follow symlink outside context": "That is not possible and will not be implemented. Please have a look at the discussion on github issue #1676:\nWe do not allow this because it's not repeatable. A symlink on your machine is the not the same as my machine and the same Dockerfile would produce two different results. Also having symlinks to /etc/paasswd would cause issues because it would link the host files and not your local files.",
    "Failed to solve with frontend Dockerfile": "The name of Docker files doesn't have any extension. It's just Dockerfile with capital D and lowercase f.\nYou can also specify the Dockerfile name, such as docker build . -f Dockerfile.txt if you'd like to name it something else.",
    "What is the purpose of VOLUME in Dockerfile?": "A volume is a persistent data stored in /var/lib/docker/volumes/...\nYou can either declare it in a Dockerfile, which means each time a container is started from the image, the volume is created (empty), even if you don't have any -v option.\nYou can declare it on runtime docker run -v [host-dir:]container-dir.\nCombining the two (VOLUME + docker run -v) means that you can mount the content of a host folder into your volume persisted by the container in /var/lib/docker/volumes/...\ndocker volume create creates a volume without having to define a Dockerfile and build an image and run a container. It is used to quickly allow other containers to mount said volume.\nIf you had persisted some content in a volume, but since then, deleted the container (which by default does not delete its associated volume, unless you are using docker rm -v), you can re-attach said volume to a new container (declaring the same volume).\nSee \"Docker - How to access a volume not attached to a container?\".\nWith docker volume create, this is easy to reattach a named volume to a container.\ndocker volume create --name aname\ndocker run -v aname:/apath --name acontainer\n...\n# modify data in /apath\n...\ndocker rm acontainer\n\n# let's mount aname volume again\ndocker run -v aname:/apath --name acontainer\nls /apath\n# you find your data back!\nWhy volumes were introduced in the first place?\nDocker volumes were introduced primarily to solve the challenge of data persistence and data sharing in containerized environments.\nIn the world of Docker, containers are ephemeral and lightweight, meaning they can be created, started, stopped, and destroyed with ease, and they are designed to be stateless.\nHowever, applications often need to store data permanently, access configuration files, or share data between different containers or between containers and the host system. That is where Docker volumes come into play.\nVolumes provide a mechanism to persist data generated by and used by Docker containers.\nUnlike the container's writable layer, which is tightly coupled to the container's lifecycle and gets destroyed when the container is removed, volumes are managed by Docker and are designed to exist independently of containers.\nThat means data in volumes survives container restarts and can be securely shared among multiple containers. And volumes are platform-independent, which simplifies data migration and backup processes.\nSee \"Docker Engine / Storage / Manage data in Docker\"\nAdditionally, volumes address performance and security concerns. Since they are stored outside the container's filesystem, they offer improved I/O performance, especially important for database storage or heavy read/write operations. They also provide a safer way to handle sensitive data, as volumes can be more securely isolated from the core container filesystem.",
    "Share variable in multi-stage Dockerfile: ARG before FROM not substituted": "ARGs only last for the build phase of a single image. For the multistage, renew the ARG by simply stating:\nARG DARSHAN_VER\nafter your FROM instructions.\ncf. https://docs.docker.com/engine/reference/builder/#arg\nARG DARSHAN_VER=3.1.6\n\nFROM fedora:29 as build\nARG DARSHAN_VER\nRUN dnf install -y \\\n        gcc \\\n        make \\\n        bzip2 bzip2-devel zlib zlib-devel\nRUN curl -O \"ftp://ftp.mcs.anl.gov/pub/darshan/releases/darshan-${DARSHAN_VER}.tar.gz\" \\\n    && tar ...\n\n\nFROM fedora:29\nARG DARSHAN_VER\nCOPY --from=build \"/usr/local/darshan-${DARSHAN_VER}\" \"/usr/local/darshan-${DARSHAN_VER}\"\n...\nYou will notice how I declared the initial value at the top of the script, and pull it in on each image.",
    "How to write commands with multiple lines in Dockerfile while preserving the new lines?": "You can use what is called \"ANSI-C quoting\" with $'...'. It was originally a ksh93 feature but it is now available in bash, zsh, mksh, FreeBSD sh and in busybox's ash (but only when it is compiled with ENABLE_ASH_BASH_COMPAT).\nAs RUN uses /bin/sh as shell by default you are required to switch to something like bash first by using the SHELL instruction.\nStart your command with $', end it with ' and use \\n\\ for newlines, like this:\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN echo $'[repo] \\n\\\nname            = YUM Repository \\n\\\nbaseurl         = https://example.com/packages/ \\n\\\nenabled         = 1 \\n\\\ngpgcheck        = 0' > /etc/yum.repos.d/Repo.repoxyz",
    "Docker how to run pip requirements.txt only if there was a change?": "I'm assuming that at some point in your build process, you're copying your entire application into the Docker image with COPY or ADD:\nCOPY . /opt/app\nWORKDIR /opt/app\nRUN pip install -r requirements.txt\nThe problem is that you're invalidating the Docker build cache every time you're copying the entire application into the image. This will also invalidate the cache for all subsequent build steps.\nTo prevent this, I'd suggest copying only the requirements.txt file in a separate build step before adding the entire application into the image:\nCOPY requirements.txt /opt/app/requirements.txt\nWORKDIR /opt/app\nRUN pip install -r requirements.txt\nCOPY . /opt/app\n# continue as before...\nAs the requirements file itself probably changes only rarely, you'll be able to use the cached layers up until the point that you add your application code into the image.",
    "How to copy folders to docker image from Dockerfile?": "Use ADD (docs)\nThe ADD command can accept as a <src> parameter:\nA folder within the build folder (the same folder as your Dockerfile). You would then add a line in your Dockerfile like this:\nADD folder /path/inside/your/container\nor\nA single-file archive anywhere in your host filesystem. To create an archive use the command:\ntar -cvzf newArchive.tar.gz /path/to/your/folder\nYou would then add a line to your Dockerfile like this:\nADD /path/to/archive/newArchive.tar.gz  /path/inside/your/container\nNotes:\nADD will automatically extract your archive.\npresence/absence of trailing slashes is important, see the linked docs",
    "Is there a way to combine Docker images into 1 container?": "You can, with the multi-stage builds feature introduced in Docker Engine 17.05\nTake a look at this:\nFROM golang:1.7.3\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=0 /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  \nThen build the image normally:\ndocker build -t alexellis2/href-counter:latest\nFrom : https://docs.docker.com/develop/develop-images/multistage-build/\nThe end result is the same tiny production image as before, with a significant reduction in complexity. You don\u2019t need to create any intermediate images and you don\u2019t need to extract any artifacts to your local system at all.\nHow does it work? The second FROM instruction starts a new build stage with the alpine:latest image as its base. The COPY --from=0 line copies just the built artifact from the previous stage into this new stage. The Go SDK and any intermediate artifacts are left behind, and not saved in the final image.",
    "How to cache the RUN npm install instruction when docker build a Dockerfile": "Ok so I found this great article about efficiency when writing a docker file.\nThis is an example of a bad docker file adding the application code before running the RUN npm install instruction:\nFROM ubuntu\n\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\" > /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get -y install python-software-properties git build-essential\nRUN add-apt-repository -y ppa:chris-lea/node.js\nRUN apt-get update\nRUN apt-get -y install nodejs\n\nWORKDIR /opt/app\n\nCOPY . /opt/app\nRUN npm install\nEXPOSE 3001\n\nCMD [\"node\", \"server.js\"]\nBy dividing the copy of the application into 2 COPY instructions (one for the package.json file and the other for the rest of the files) and running the npm install instruction before adding the actual code, any code change wont trigger the RUN npm install instruction, only changes of the package.json will trigger it. Better practice docker file:\nFROM ubuntu\nMAINTAINER David Weinstein <david@bitjudo.com>\n\n# install our dependencies and nodejs\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\" > /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get -y install python-software-properties git build-essential\nRUN add-apt-repository -y ppa:chris-lea/node.js\nRUN apt-get update\nRUN apt-get -y install nodejs\n\n# use changes to package.json to force Docker not to use the cache\n# when we change our application's nodejs dependencies:\nCOPY package.json /tmp/package.json\nRUN cd /tmp && npm install\nRUN mkdir -p /opt/app && cp -a /tmp/node_modules /opt/app/\n\n# From here we load our application's code in, therefore the previous docker\n# \"layer\" thats been cached will be used if possible\nWORKDIR /opt/app\nCOPY . /opt/app\n\nEXPOSE 3000\n\nCMD [\"node\", \"server.js\"]\nThis is where the package.json file added, install its dependencies and copy them into the container WORKDIR, where the app lives:\nADD package.json /tmp/package.json\nRUN cd /tmp && npm install\nRUN mkdir -p /opt/app && cp -a /tmp/node_modules /opt/app/\nTo avoid the npm install phase on every docker build just copy those lines and change the ^/opt/app^ to the location your app lives inside the container.",
    "Docker build gives \"unable to prepare context: context must be a directory: /Users/tempUser/git/docker/Dockerfile\"": "You need to point to the directory instead. You must not specify the dockerfile.\ndocker build -t ubuntu-test:latest . does work.\ndocker build -t ubuntu-test:latest ./Dockerfile does not work.",
    "What is the difference between `docker-compose build` and `docker build`?": "docker-compose can be considered a wrapper around the docker CLI (in fact it is another implementation in python as said in the comments) in order to gain time and avoid 500 characters-long lines (and also start multiple containers at the same time). It uses a file called docker-compose.yml in order to retrieve parameters.\nYou can find the reference for the docker-compose file format here.\nSo basically docker-compose build will read your docker-compose.yml, look for all services containing the build: statement and run a docker build for each one.\nEach build can specify a Dockerfile, a context and args to pass to docker.\nTo conclude with an example docker-compose.yml file:\nversion: '3.2'\n\nservices:\n  database:\n    image: mariadb\n    restart: always\n    volumes:\n      - ./.data/sql:/var/lib/mysql\n\n  web:\n    build:\n      dockerfile: Dockerfile-alpine\n      context: ./web\n    ports:\n      - 8099:80\n    depends_on:\n      - database \nWhen calling docker-compose build, only the web target will need an image to be built. The docker build command would look like:\ndocker build -t web_myproject -f Dockerfile-alpine ./web",
    "npm WARN old lockfile The package-lock.json file was created with an old version of npm": "There are several ways to deal with this. (People really seem to like #4.)\nIgnore it. It's just a warning and does not affect the installation of modules.\nRun npm install --package-lock-only (with the newer version of npm) to regenerate a package-lock.json. Commit the updated version of package-lock.json to the repo/Docker image or whatever.\nDowngrade npm to an older version in production. Consider running npm version 6 as that is what ships with the current (as of this writing) Long Term Support (LTS) version of Node.js. In the case being asked about in this question, I imagine you can just leave out the RUN npm -g install npm@7.19.1 from the Dockerfile and instead use the version of npm that is installed with the Docker image (which in this case will almost certainly be npm@6 since that is what ships with Node.js 14.x).\nIf you already have a version of npm installed but want to run one command with an older version of npm but otherwise keep the newer version, you can use npx (which ships with npm) to do that. For example, npx npm@6 ci would run npm ci with npm version 6 even if you have version 7 installed.",
    "Difference between Docker ENTRYPOINT and Kubernetes container spec COMMAND?": "Kubernetes provides us with multiple options on how to use these commands:\nWhen you override the default Entrypoint and Cmd in Kubernetes .yaml file, these rules apply:\nIf you do not supply command or args for a Container, the defaults defined in the Docker image are used.\nIf you supply only args for a Container, the default Entrypoint defined in the Docker image is run with the args that you supplied.\nIf you supply a command for a Container, only the supplied command is used. The default EntryPoint and the default Cmd defined in the Docker image are ignored. Your command is run with the args supplied (or no args if none supplied).\nHere is an example:\nDockerfile:\nFROM alpine:latest\nCOPY \"executable_file\" /\nENTRYPOINT [ \"./executable_file\" ]\nKubernetes yaml file:\n spec:\n    containers:\n      - name: container_name\n        image: image_name\n        args: [\"arg1\", \"arg2\", \"arg3\"]\nhttps://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/",
    "How to view logs for a docker image?": "Had the same problem, I solved it using\ndocker build --no-cache --progress=plain -t my-image .",
    "Docker expose all ports or range of ports from 7000 to 8000": "Since Docker 1.5 you can now expose a range of ports to other linked containers using:\nThe Dockerfile EXPOSE command:\nEXPOSE 7000-8000\nor The Docker run command:\ndocker run --expose=7000-8000\nOr instead you can publish a range of ports to the host machine via Docker run command:\ndocker run -p 7000-8000:7000-8000",
    "E: Package 'mysql-client' has no installation candidate in php-fpm image build using docker compose": "If you still want to use the mysql client, it's called default-mysql-client now.",
    "Can we pass ENV variables through cmd line while building a docker image through dockerfile?": "Containers can be built using build arguments (in Docker 1.9+) which work like environment variables.\nHere is the method:\nFROM php:7.0-fpm\nARG APP_ENV=local\nENV APP_ENV=${APP_ENV}\nRUN cd /usr/local/etc/php && ln -sf php.ini-${APP_ENV} php.ini\nand then build a production container:\ndocker build --build-arg APP_ENV=prod .\nFor your particular problem:\nFROM debian\nENV http_proxy=${http_proxy}\nand then run:\ndocker build --build-arg http_proxy=10.11.24.31 .\nNote that if you build your containers with docker-compose, you can specify these build-args in the docker-compose.yml file, but not on the command-line. However, you can use variable substitution in the docker-compose.yml file, which uses environment variables.",
    "How can I set Bash aliases for docker containers in Dockerfile?": "Basically like you always do, by adding it to the user's .bashrc file:\nFROM foo\nRUN echo 'alias hi=\"echo hello\"' >> ~/.bashrc\nAs usual this will only work for interactive shells:\ndocker build -t test .\ndocker run -it --rm --entrypoint /bin/bash test hi\n/bin/bash: hi: No such file or directory\ndocker run -it --rm test bash\n$ hi\nhello\nFor non-interactive shells you should create a small script and put it in your path, i.e.:\nRUN echo -e '#!/bin/bash\\necho hello' > /usr/bin/hi && \\\n    chmod +x /usr/bin/hi\nIf your alias uses parameters (ie. hi Jim -> hello Jim), just add \"$@\":\nRUN echo -e '#!/bin/bash\\necho hello \"$@\"' > /usr/bin/hi && \\\n    chmod +x /usr/bin/hi",
    "ARG substitution in RUN command not working for Dockerfile": "Another thing to be careful about is that after every FROM statement, all the ARGs get collected and are no longer available. Be careful with multi-stage builds.\nYou can reuse ARG with omitted default value inside FROM to get through this problem:\nARG VERSION=latest\nFROM busybox:$VERSION\nARG VERSION\nRUN echo $VERSION > image_version\nExample taken from docs: https://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact",
    "Docker Copy and change owner": "A --chown flag has finally been added to COPY:\nCOPY --chown=patrick hostPath containerPath\nThis new syntax seems to work on Docker 17.09.\nSee the PR for more information.",
    "Docker - image operating system \"windows\" cannot be used on this platform": "Your Docker host is configured to run Linux containers inside of a VM. To run Windows containers, you need to right click on the Docker icon in the system tray, and select \"Switch to Windows containers\u2026\" in the Docker menu. This option is not available in \"Home\" versions of Windows. Documentation is available here.",
    "How can I use a local file on container?": "Yes, you can do this. What you are describing is a bind mount. See https://docs.docker.com/storage/bind-mounts/ for documentation on the subject.\nFor example, if I want to mount a folder from my home directory into /mnt/mydata in a container, I can do:\ndocker run -v /Users/andy/mydata:/mnt/mydata myimage\nNow, /mnt/mydata inside the container will have access to /Users/andy/mydata on my host.\nKeep in mind, if you are using Docker for Mac or Docker for Windows there are specific directories on the host that are allowed by default:\nIf you are using Docker Machine on Mac or Windows, your Docker Engine daemon has only limited access to your macOS or Windows filesystem. Docker Machine tries to auto-share your /Users (macOS) or C:\\Users (Windows) directory. So, you can mount files or directories on macOS using.\nUpdate July 2019:\nI've updated the documentation link and naming to be correct. These type of mounts are called \"bind mounts\". The snippet about Docker for Mac or Windows no longer appears in the documentation but it should still apply. I'm not sure why they removed it (my Docker for Mac still has an explicit list of allowed mounting paths on the host).",
    "How to remove entrypoint from parent Image in Dockerfile": "Per the discussion here, you should be able to reset the entrypoint with\nENTRYPOINT []",
    "Docker: What is the default WORKDIR in a Dockerfile?": "docker workdir\nsays it is /, so the root directory",
    "Install packages in Alpine docker": "The equivalent of apt or apt-get in Alpine is apk\nA typical Dockerfile will contain, for example:\nRUN apk add --no-cache wget\n--no-cache is the equivalent to: apk add wget && rm -rf /var/cache/apk/*\nor, before the --no-cache option was available:\nRUN apk update && apk add wget\nAlpine rm -rf /var/cache/apk/* has the Debian equivalent rm -rf /var/lib/apt/lists/*.\nSee the Alpine comparison with other distros for more details.",
    "Can a Dockerfile extend another one?": "Using multi-stage build is definitely one part of the answer here.\ndocker-compose v3.4 target being the second and last.\nHere is a example to have 2 containers (1 normal & 1 w/ xdebug installed) living together :\nDockerfile\nFROM php:7-fpm AS php_base \nENV DEBIAN_FRONTEND noninteractive\n\nRUN apt-get update && \\\n    apt-get install -y git libicu-dev libmagickwand-dev libmcrypt-dev libcurl3-dev jpegoptim\nRUN pecl install imagick && \\\n    docker-php-ext-enable imagick\n\nRUN docker-php-ext-install intl\nRUN docker-php-ext-install pdo_mysql\nRUN docker-php-ext-install opcache\nRUN docker-php-ext-install mcrypt\nRUN docker-php-ext-install curl\nRUN docker-php-ext-install zip\n\nFROM php_base AS php_test\n\nRUN pecl install xdebug \nRUN docker-php-ext-enable xdebug\ndocker-compose.yml\nversion: '3.4'\n\nservices:\n  php:\n    build:\n      context: ./\n      target: php_base\n\n  php_test:\n    build:\n      context: ./\n      target: php_test\n  \n# ...",
    "Single file volume mounted as directory in Docker": "Maybe that's clear in the answers above... but it took me some time to figure it out in my case.\nThe underlying reason causing the file being shared with -v to appear as a directory instead of a file is that Docker could not find the file on the host. So Docker creates a new directory in the container with the name being the name of the non existing file on the host as docker thinks that the user just want to share a volume/directory that will be created in the future.\nSo in the problem reported above, if you used a relative directory in the -v command and docker does not understand relative directories, that means that the file was not found on the host and so docker created a directory. And the answer above which suggests to use $(pwd) will be the correct solution when the problem is due to a relative directory.\nBut for those reading this page who are not using a relative directory and are having the same problem... then try to understand why the file is missing on the host.\nIt could just be a stupid typo...\nIt could be that you're running the \"docker run\" command from a client which spawns the docker container on a different host and the file being shared does not exist on that different host. The file being shared with -v must exist on the host where the docker agent will spawn the container... not necessarily on the client where the \"docker run -v ...\" command is executed (although they will be the same in many cases).\nThere are other possible explanations above for Mac and Windows... that could be it too.\nSo the file missing from the host is the problem... troubleshoot the problem in your setup... using $(pwd) could be the solution but not always.",
    "Conditional ENV in Dockerfile": "Yes, it is possible, but you need to use your build argument as flag. You can use parameter expansion feature of shell to check condition. Here is a proof-of-concept Docker file:\nFROM debian:stable\nARG BUILD_DEVELOPMENT\n# if --build-arg BUILD_DEVELOPMENT=1, set NODE_ENV to 'development' or set to null otherwise.\nENV NODE_ENV=${BUILD_DEVELOPMENT:+development}\n# if NODE_ENV is null, set it to 'production' (or leave as is otherwise).\nENV NODE_ENV=${NODE_ENV:-production}\nTesting build:\ndocker build --rm -t env_prod ./\n...\ndocker run -it env_prod bash\nroot@2a2c93f80ad3:/# echo $NODE_ENV \nproduction\nroot@2a2c93f80ad3:/# exit\ndocker build --rm -t env_dev --build-arg BUILD_DEVELOPMENT=1 ./\n...\ndocker run -it env_dev bash\nroot@2db6d7931f34:/# echo $NODE_ENV\ndevelopment",
    "Dockerfile: how to redirect the output of a RUN command to a variable?": "You cannot save a variable for later use in other Dockerfile commands (if that is your intention). This is because each RUN happens in a new shell.\nHowever, if you just want to capture the output of ls you should be able to do it in one RUN compound command. For example:\nRUN file=\"$(ls -1 /tmp/dir)\" && echo $file\nOr just using the subshell inline:\nRUN echo $(ls -1 /tmp/dir)\nIf you have an actual error or problem to solve I could expand on this instead of a hypothetical answer.\nA full example Dockerfile demonstrating this would be:\nFROM alpine:3.7\nRUN mkdir -p /tmp/dir && touch /tmp/dir/file1 /tmp//dir/file2\nRUN file=\"$(ls -1 /tmp/dir)\" && echo $file\nRUN echo $(ls -1 /tmp/dir)\nWhen building you should see steps 3 and 4 output the variable (which contains the list of file1 and file2 creating in step 2). The option for --progress plain forces the output to show the steps in later version of Docker:\n$ docker build --no-cache --progress plain -t test .\nSending build context to Docker daemon  2.048kB\nStep 1/4 : FROM alpine:3.7\n ---> 3fd9065eaf02\nStep 2/4 : RUN mkdir -p /tmp/dir && touch /tmp/dir/file1 /tmp//dir/file2\n ---> Running in abb2fe683e82\nRemoving intermediate container abb2fe683e82\n ---> 2f6dfca9385c\nStep 3/4 : RUN file=\"$(ls -1 /tmp/dir)\" && echo $file\n ---> Running in 060a285e3d8a\nfile1 file2\nRemoving intermediate container 060a285e3d8a\n ---> 2e4cc2873b8c\nStep 4/4 : RUN echo $(ls -1 /tmp/dir)\n ---> Running in 528fc5d6c721\nfile1 file2\nRemoving intermediate container 528fc5d6c721\n ---> 1be7c54e1f29\nSuccessfully built 1be7c54e1f29\nSuccessfully tagged test:latest",
    "Dockerfile: Setting multiple environment variables in single line": "There are two formats for specifying environments. If you need single variable then you below format\nENV X Y\nThis will assign X as Y\nENV X Y Z\nThis will assign X as Y Z\nIf you need to assign multiple environment variables then you use the other format\nENV X=Y Z=A\nThis will assign X as Y and Z as A. So your Dockerfile should be\nFROM alpine:3.6\nENV RUBY_MAJOR=2.4 \\\n    RUBY_VERSION=2.4.1 \\\n    RUBY_DOWNLOAD_SHA256=4fc8a9992de3e90191de369270ea4b6c1b171b7941743614cc50822ddc1fe654 \\\n    RUBYGEMS_VERSION=2.6.12 \\\n    BUNDLER_VERSION=1.15.3\n\nRUN env",
    "How to copy file from host to container using Dockerfile": "Use COPY command like this:\nCOPY foo.txt /data/foo.txt\n# where foo.txt is the relative path on host\n# and /data/foo.txt is the absolute path in the image\nread more details for COPY in the official documentation\nAn alternative would be to use ADD but this is not the best practise if you dont want to use some advanced features of ADD like decompression of tar.gz files.If you still want to use ADD command, do it like this:\nADD abc.txt /data/abc.txt\n# where abc.txt is the relative path on host\n# and /data/abc.txt is the absolute path in the image\nread more details for ADD in the official documentation",
    "Copy multiple directories with one command": "That's the documented behavior of the copy command:\nIf <src> is a directory, the entire contents of the directory are copied, including filesystem metadata.\nNote: The directory itself is not copied, just its contents.\nBest workaround I can suggest is to change your directory layout in your build folder, move the three folders under one parent folder and add the parent.",
    "\"The headers or library files could not be found for jpeg\" installing Pillow on Alpine Linux": "For debian\nsudo apt install libjpeg-dev zlib1g-dev\npip install Pillow",
    "Can't create a docker image for COPY failed: stat /var/lib/docker/tmp/docker-builder error": "You should put those files into the same directory with Dockerfile.",
    "Docker filling up storage on macOS": "WARNING:\nBy default, volumes are not removed to prevent important data from being deleted if there is currently no container using the volume. Use the --volumes flag when running the command to prune volumes as well:\nDocker now has a single command to do that:\ndocker system prune -a --volumes\nSee the Docker system prune docs",
    "How to pass arguments within docker-compose?": "This can now be done as of docker-compose v2+ as part of the build object;\ndocker-compose.yml\nversion: '2'\nservices:\n    my_image_name:\n        build:\n            context: . #current dir as build context\n            args:\n                var1: 1\n                var2: c\nSee the docker compose docs.\nIn the above example \"var1\" and \"var2\" will be sent to the build environment.\nNote: any env variables (specified by using the environment block) which have the same name as args variable(s) will override that variable.",
    "Activate python virtualenv in Dockerfile": "You don't need to use virtualenv inside a Docker Container.\nvirtualenv is used for dependency isolation. You want to prevent any dependencies or packages installed from leaking between applications. Docker achieves the same thing, it isolates your dependencies within your container and prevent leaks between containers and between applications.\nTherefore, there is no point in using virtualenv inside a Docker Container unless you are running multiple apps in the same container, if that's the case I'd say that you're doing something wrong and the solution would be to architect your app in a better way and split them up in multiple containers.\nEDIT 2022: Given this answer get a lot of views, I thought it might make sense to add that now 4 years later, I realized that there actually is valid usages of virtual environments in Docker images, especially when doing multi staged builds:\nFROM python:3.9-slim as compiler\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /app/\n\nRUN python -m venv /opt/venv\n# Enable venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY ./requirements.txt /app/requirements.txt\nRUN pip install -Ur requirements.txt\n\nFROM python:3.9-slim as runner\nWORKDIR /app/\nCOPY --from=compiler /opt/venv /opt/venv\n\n# Enable venv\nENV PATH=\"/opt/venv/bin:$PATH\"\nCOPY . /app/\nCMD [\"python\", \"app.py\", ]\nIn the Dockerfile example above, we are creating a virtualenv at /opt/venv and activating it using an ENV statement, we then install all dependencies into this /opt/venv and can simply copy this folder into our runner stage of our build. This can help with minimizing docker image size.",
    "Editing Files from dockerfile": "I would use the following approach in the Dockerfile\nRUN   echo \"Some line to add to a file\" >> /etc/sysctl.conf\nThat should do the trick. If you wish to replace some characters or similar you can work this out with sed by using e.g. the following:\nRUN   sed -i \"s|some-original-string|the-new-string |g\" /etc/sysctl.conf\nHowever, if your problem lies in simply getting the settings to \"bite\" this question might be of help.",
    "apt-get update' returned a non-zero code: 100": "Because you have an https sources. Install apt-transport-https before executing update.\nFROM ubuntu:14.04.4\nRUN apt-get update && apt-get install -y apt-transport-https\nRUN echo 'deb http://private-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.4.2.0 HDP main' >> /etc/apt/sources.list.d/HDP.list\nRUN echo 'deb http://private-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/ubuntu14 HDP-UTILS main'  >> /etc/apt/sources.list.d/HDP.list\nRUN echo 'deb [arch=amd64] https://apt-mo.trafficmanager.net/repos/azurecore/ trusty main' >> /etc/apt/sources.list.d/azure-public-trusty.list\n\n....\nRest of your Dockerfile.",
    "MYSQL_ROOT_PASSWORD is set but getting \"Access denied for user 'root'@'localhost' (using password: YES)\" in docker container": "The below description is specifically for MySQL but many other official db docker images (postgres, mongodb....) work a similar way. Hence the symptom (i.e. access denied with configured credentials) and workaround (i.e. delete the data volume to start initialization from scratch) are the same.\nTaking for granted you have shown your entire start log, it appears you started your mysql container against a pre-existing db_data volume already containing a mysql database filesystem.\nIn this case, absolutely nothing will be initialized on container start and environment variables are useless. Quoting the official image documentation in the \"Environment Variables\" section:\nDo note that none of the variables below will have any effect if you start the container with a data directory that already contains a database: any pre-existing database will always be left untouched on container startup.\nIf you want your instance to be initialized, you have to start from scratch. It is quite easy to do with docker compose when using a named volume like in your case. Warning: this will permanently delete the contents in your db_data volume, wiping out any previous database you had there. Create a backup first if you need to keep the contents.\ndocker-compose down -v\ndocker-compose up -d\nIf you ever convert to a bind mount, you will have to delete all it's content yourself (i.e. rm -rf /path/to/bind/mount/*)",
    "Why the \"none\" image appears in Docker and how can we avoid it": "Below are some parts from What are Docker <none>:<none> images?\nThe Good <none>:<none>\nThese are intermediate images and can be seen using docker images -a. They don't result into a disk space problem but it is definitely a screen \"real estate\" problem. Since all these <none>:<none> images can be quite confusing as what they signify.\nThe Bad <none>:<none>\nThese images are the dangling ones, which can cause disk space problems. These <none>:<none> images are being listed as part of docker images and need to be pruned.\n(a dangling file system layer in Docker is something that is unused and is not being referenced by any images. Hence we need a mechanism for Docker to clear these dangling images)\nSo,\nif your case has to do with dangling images, it's ok to remove them with:\n docker rmi $(docker images -f \"dangling=true\" -q)\nThere is also the option of docker image prune but the client and daemon API must both be at least v1.25 to use this command.\nif your case has to do with intermediate images, it's ok to keep them, other images are pointing references to them.\nRelated documentation:\ndocker rmi\ndocker image rm\ndocker image prune",
    "Docker COPY from ubuntu absolute path": "The absolute path of your resources refers to an absolute path within the build context, not an absolute path on the host. So all the resources must be copied into the directory where you run the docker build and then provide the path of those resources within your Dockerfiles before building the image. (This refers to the location where you run your Dockerfile)\nThere is a closed issue for this as well.",
    "Why won't my docker-entrypoint.sh execute?": "I was tearing my hair out with an issue very similar to this. In my case /bin/bash DID exist. But actually the problem was Windows line endings.\nIn my case the git repository had an entry point script with Unix line endings (\\n). But when the repository was checked out on a windows machine, git decided to try and be clever and replace the line endings in the files with windows line endings (\\r\\n).\nThis meant that the shebang didn't work because instead of looking for /bin/bash, it was looking for /bin/bash\\r.\nThe solution for me was to disable git's automatic conversion:\ngit config --global core.autocrlf input\nReset the repo using this (don't forget to save your changes):\ngit rm --cached -r .\ngit reset --hard\nAnd then rebuild.\nSome more helpful info here: How to change line-ending settings and here http://willi.am/blog/2016/08/11/docker-for-windows-dealing-with-windows-line-endings/\nFor repo owners and contributors\nIf you own a repo or contribute to it, set mandatory LF line endings for .sh files right in the repo by adding the .gitattributes file with the following line:\n*.sh text eol=lf",
    "How can I see Dockerfile for each docker image?": "As far as I know, no, you can't. Because a Dockerfile is used for building the image, it is not packed with the image itself. That means you should reverse engineer it. You can use docker inspect on an image or container, thus getting some insight and a feel of how it is configured. The layers an image are also visible, since you pull them when you pull a specific image, so that is also no secret.\nHowever, you can usually see the Dockerfile in the repository of the image itself on Dockerhub. I can't say most repositories have Dockerfiles attached, but the most of the repositories I seen do have it.\nDifferent repository maintainers may opt for different ways to document the Dockerfiles. You can see the Dockerfile tab on the repository page if automatic builds are set up. But when multiple parallel versions are available (like for Ubuntu), maintainers usually opt to put links the Dockerfiles for different versions in the description. If you take a look here: https://hub.docker.com/_/ubuntu/, under the \"Supported tags\" (again, for Ubuntu), you can see there are links to multiple Dockerfiles, for each respective Ubuntu version.",
    "Docker multiline CMD or ENTRYPOINT": "It was a typo in the dockerfile. I missed a space between ENTRYPOINT and [. Dockerfile supports multiline ENTRYPOINT and CMD by terminating the line with \\, same as RUN. So, in my case it can be\nENTRYPOINT [ \"/path/myprocess\", \\\n             \"arg1\",            \\\n             \"arg2\"             \\\n]",
    "How to configure different dockerfile for development and production": "As a best practice you should try to aim to use one Dockerfile to avoid unexpected errors between different environments. However, you may have a use case where you cannot do that.\nThe Dockerfile syntax is not rich enough to support such a scenario, however you can use shell scripts to achieve that.\nCreate a shell script, called install.sh that does something like:\nif [ ${ENV} = \"DEV\" ]; then \n    composer install\nelse\n    npm install\nfi\nIn your Dockerfile add this script and then execute it when building\n...\nCOPY install.sh install.sh\nRUN chmod u+x install.sh && ./install.sh\n...\nWhen building pass a build arg to specify the environment, example:\ndocker build --build-arg \"ENV=PROD\" ...",
    "docker run pass arguments to entrypoint": "Use ENTRYPOINT in its exec form\nENTRYPOINT [\"java\", \"-jar\", \"/dir/test-1.0.1.jar\"]\nthen when you run docker run -it testjava $value, $value will be \"appended\" after your entrypoint, just like java -jar /dir/test-1.0.1.jar $value",
    "Docker COPY files using glob pattern?": "There is a solution based on multistage-build feature:\nFROM node:12.18.2-alpine3.11\n\nWORKDIR /app\nCOPY [\"package.json\", \"yarn.lock\", \"./\"]\n# Step 2: Copy whole app\nCOPY packages packages\n\n# Step 3: Find and remove non-package.json files\nRUN find packages \\! -name \"package.json\" -mindepth 2 -maxdepth 2 -print | xargs rm -rf\n\n# Step 4: Define second build stage\nFROM node:12.18.2-alpine3.11\n\nWORKDIR /app\n# Step 5: Copy files from the first build stage.\nCOPY --from=0 /app .\n\nRUN yarn install --frozen-lockfile\n\nCOPY . .\n\n# To restore workspaces symlinks\nRUN yarn install --frozen-lockfile\n\nCMD yarn start\nOn Step 5 the layer cache will be reused even if any file in packages directory has changed.",
    "standard_init_linux.go:211: exec user process caused \"exec format error\"": "This can also happen when your host machine has a different architecture from your guest container image.\nE.g. running an arm container on a host with x86-64 architecture",
    "Docker command/option to display or list the build context": "Answers above are great, but there is a low-tech solution for most cases - ncdu. This utility will show pretty and interactive tree structure with sizes. It has an option that will take patterns from a file and exclude them from scan. So you can just do ncdu -X .dockerignore. You will get something like this:\nThis is pretty close to what you will get in your docker image. One caveat is thou if you add a dot directory (like .yarn) into an image, it will not show in ncdu output.",
    "How to pass ARG value to ENTRYPOINT?": "Like Blake Mitchell sais, you cannot use ARG in ENTRYPOINT. However you can use your ARG as a value for ENV, that way you can use it with ENTRYPOINT:\nDockerfile\nARG my_arg\nENV my_env_var=$my_arg\n\nENTRYPOINT echo $my_env_var\nand run:\ndocker build --build-arg \"my_arg=foo\" ...",
    "What is the difference between Docker Service and Docker Container?": "In short: Docker service is used mostly when you configured the master node with Docker swarm so that docker containers will run in a distributed environment and it can be easily managed.\nDocker run: The docker run command first creates a writeable container layer over the specified image, and then starts it using the specified command.\nThat is, docker run is equivalent to the API /containers/create then /containers/(id)/start\nsource: https://docs.docker.com/engine/reference/commandline/run/#parent-command\nDocker service: Docker service will be the image for a microservice within the context of some larger application. Examples of services might include an HTTP server, a database, or any other type of executable program that you wish to run in a distributed environment.\nWhen you create a service, you specify which container image to use and which commands to execute inside running containers. You also define options for the service including:\nthe port where the swarm will make the service available outside the swarm\nan overlay network for the service to connect to other services in the swarm\nCPU and memory limits and reservations\na rolling update policy\nthe number of replicas of the image to run in the swarm\nsource: https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#services-tasks-and-containers",
    "Docker using gosu vs USER": "Dockerfiles are for creating images. I see gosu as more useful as part of a container initialization when you can no longer change users between run commands in your Dockerfile.\nAfter the image is created, something like gosu allows you to drop root permissions at the end of your entrypoint inside of a container. You may initially need root access to do some initialization steps (fixing uid's, host mounted volume permissions, etc). Then once initialized, you run the final service without root privileges and as pid 1 to handle signals cleanly.\nEdit: Here's a simple example of using gosu in an image for docker and jenkins: https://github.com/bmitch3020/jenkins-docker\nThe entrypoint.sh looks up the gid of the /var/lib/docker.sock file and updates the gid of the docker user inside the container to match. This allows the image to be ported to other docker hosts where the gid on the host may differ. Changing the group requires root access inside the container. Had I used USER jenkins in the dockerfile, I would be stuck with the gid of the docker group as defined in the image which wouldn't work if it doesn't match that of the docker host it's running on. But root access can be dropped when running the app which is where gosu comes in.\nAt the end of the script, the exec call prevents the shell from forking gosu, and instead it replaces pid 1 with that process. Gosu in turn does the same, switching the uid and then exec'ing the jenkins process so that it takes over as pid 1. This allows signals to be handled correctly which would otherwise be ignored by a shell as pid 1.",
    "What does minikube docker-env mean?": "The command minikube docker-env returns a set of Bash environment variable exports to configure your local environment to re-use the Docker daemon inside the Minikube instance.\nPassing this output through eval causes bash to evaluate these exports and put them into effect.\nYou can review the specific commands which will be executed in your shell by omitting the evaluation step and running minikube docker-env directly. However, this will not perform the configuration \u2013 the output needs to be evaluated for that.\nThis is a workflow optimization intended to improve your experience with building and running Docker images which you can run inside the minikube environment. It is not mandatory that you re-use minikube's Docker daemon to use minikube effectively, but doing so will significantly improve the speed of your code-build-test cycle.\nIn a normal workflow, you would have a separate Docker registry on your host machine to that in minikube, which necessitates the following process to build and run a Docker image inside minikube:\nBuild the Docker image on the host machine.\nRe-tag the built image in your local machine's image registry with a remote registry or that of the minikube instance.\nPush the image to the remote registry or minikube.\n(If using a remote registry) Configure minikube with the appropriate permissions to pull images from the registry.\nSet up your deployment in minikube to use the image.\nBy re-using the Docker registry inside Minikube, this becomes:\nBuild the Docker image using Minikube's Docker instance. This pushes the image to Minikube's Docker registry.\nSet up your deployment in minikube to use the image.\nMore details of the purpose can be found in the minikube docs.",
    "Multiple commands on docker ENTRYPOINT": "In case you want to run many commands at entrypoint, the best idea is to create a bash file. For example commands.sh like this\n#!/bin/bash\nmkdir /root/.ssh\necho \"Something\"\ncd tmp\nls\n...\nAnd then, in your DockerFile, set entrypoint to commands.sh file (that execute and run all your commands inside)\nCOPY commands.sh /scripts/commands.sh\nRUN [\"chmod\", \"+x\", \"/scripts/commands.sh\"]\nENTRYPOINT [\"/scripts/commands.sh\"]\nAfter that, each time you start your container, commands.sh will be execute and run all commands that you need. You can take a look here https://github.com/dangminhtruong/drone-chatwork",
    "entrypoint file not found": "I had this problem with Docker for Windows and the solution was changing the entrypoint script file from CRLF -> LF.",
    "Use docker run command to pass arguments to CMD in Dockerfile": "Make sure your Dockerfile declares an environment variable with ENV:\nENV environment default_env_value\nENV cluster default_cluster_value\nThe ENV <key> <value> form can be replaced inline.\nThen you can pass an environment variable with docker run. Note that each variable requires a specific -e flag to run.\ndocker run -p 9000:9000 -e environment=dev -e cluster=0 -d me/app\nOr you can set them through your compose file:\nnode:\n  environment:\n    - environment=dev\n    - cluster=0\nYour Dockerfile CMD can use that environment variable, but, as mentioned in issue 5509, you need to do so in a sh -c form:\nCMD [\"sh\", \"-c\", \"node server.js ${cluster} ${environment}\"]\nThe explanation is that the shell is responsible for expanding environment variables, not Docker. When you use the JSON syntax, you're explicitly requesting that your command bypass the shell and be executed directly.\nSame idea with Builder RUN (applies to CMD as well):\nUnlike the shell form, the exec form does not invoke a command shell.\nThis means that normal shell processing does not happen.\nFor example, RUN [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ].\nWhen using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nAs noted by Pwnstar in the comments:\nWhen you use a shell command, this process will not be started with id=1.",
    "How does the new Docker --squash work": "If I add a secret file in my first layer, then use the secret file in my second layer, and the finally remove my secret file in the third layer, and then build with the --squash flag.\nWill there be any way now to get the secret file?\nAnswer: Your image won't have the secret file.\nHow --squash works:\nOnce the build is complete, Docker creates a new image loading the diffs from each layer into a single new layer and references all the parent's layers.\nIn other words: when squashing, Docker will take all the filesystem layers produced by a build and collapse them into a single new layer.\nThis can simplify the process of creating minimal container images, but may result in slightly higher overhead when images are moved around (because squashed layers can no longer be shared between images). Docker still caches individual layers to make subsequent builds fast.\nPlease note this feature squashes all the newly built layers into a single layer, it is not squashing to scratch.\nSide notes:\nDocker 1.13 also has support for compressing the build context that is sent from CLI to daemon using the --compress flag. This will speed up builds done on remote daemons by reducing the amount of data sent.\nPlease note as of Docker 1.13 this feature is experimental.\nUpdate 2024: Squash has been moved to buildkit and later on deprecated from buildkit\nWARNING: experimental flag squash is removed with BuildKit. You should squash inside build using a multi-stage Dockerfile for efficiency.\nAs the warning suggests you need to use multi-stage builds instead of squashing layers.\nExample:\n# syntax=docker/dockerfile:1\nFROM golang:1.21\nWORKDIR /src\nCOPY <<EOF ./main.go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n  fmt.Println(\"hello, world\")\n}\nEOF\nRUN go build -o /bin/hello ./main.go\n\nFROM scratch\nCOPY --from=0 /bin/hello /bin/hello\nCMD [\"/bin/hello\"]",
    "Wordpress Docker won't increase upload limit": "it worked for me as follows: i created uploads.ini alongside of docker-compose.yml with following lines. this is exactly how it stated in fist post.\nfile_uploads = On\nmemory_limit = 500M\nupload_max_filesize = 500M\npost_max_size = 500M\nmax_execution_time = 600\nafter this i added\nvolumes: \n   - ./uploads.ini:/usr/local/etc/php/conf.d/uploads.ini\nto my .yml file as it states in first post.\nafter this i had to delete the container/images (basically start from scratch):\ndocker stop [image name]\ndocker rm [image name]\ndocker image rm [image name]\nsome places i end up using ID insted of image name. name or ID basically you have to stop, remove container and image. bottom line is start from scratch with additional line in your .yml file as describe in first post. remember, you'll lose all your wp work. now run\ndocker-compose up -d --build\nupload limit should be increased now. i was able to upload my new bigger theme after this change. no more upload file size error. only question is if you need to increase this upload size limit in the middle of your work then how would you do this?...",
    "rebuild docker image from specific step": "You can rebuild the entire thing without using the cache by doing a\ndocker build --no-cache -t user/image-name\nTo force a rerun starting at a specific line, you can pass an arg that is otherwise unused. Docker passes ARG values as environment variables to your RUN command, so changing an ARG is a change to the command which breaks the cache. It's not even necessary to define it yourself on the RUN line.\nFROM ubuntu:14.04\nMAINTAINER Samuel Alexander <samuel@alexander.com>\n\nRUN apt-get -y install software-properties-common\nRUN apt-get -y update\n\n# Install Java.\nRUN echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections\nRUN add-apt-repository -y ppa:webupd8team/java\nRUN apt-get -y update\nRUN apt-get install -y oracle-java8-installer\nRUN rm -rf /var/lib/apt/lists/*\nRUN rm -rf /var/cache/oracle-jdk8-installer\n\n# Define working directory.\nWORKDIR /work\n\n# Define commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\n\n# JAVA PATH\nENV PATH /usr/lib/jvm/java-8-oracle/bin:$PATH\n\n# Install maven\nRUN apt-get -y update\nRUN apt-get -y install maven\n\n# Install Open SSH and git\nRUN apt-get -y install openssh-server\nRUN apt-get -y install git\n\n# clone Spark\nRUN git clone https://github.com/apache/spark.git\nWORKDIR /work/spark\nRUN mvn -DskipTests clean package\n\n# clone and build zeppelin fork, changing INCUBATOR_VER will break the cache here\nARG INCUBATOR_VER=unknown\nRUN git clone https://github.com/apache/incubator-zeppelin.git\nWORKDIR /work/incubator-zeppelin\nRUN mvn clean package -Pspark-1.6 -Phadoop-2.6 -DskipTests\n\n# Install Supervisord\nRUN apt-get -y install supervisor\nRUN mkdir -p var/log/supervisor\n\n# Configure Supervisord\nCOPY conf/supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\n# bash\nRUN sed -i s#/home/git:/bin/false#/home/git:/bin/bash# /etc/passwd\n\nEXPOSE 8080 8082\nCMD [\"/usr/bin/supervisord\"]\nAnd then just run it with a unique arg:\ndocker build --build-arg INCUBATOR_VER=20160613.2 -t user/image-name .\nTo change the argument with every build, you can pass a timestamp as the arg:\ndocker build --build-arg INCUBATOR_VER=$(date +%Y%m%d-%H%M%S) -t user/image-name .\nor:\ndocker build --build-arg INCUBATOR_VER=$(date +%s) -t user/image-name .\nAs an aside, I'd recommend the following changes to keep your layers smaller, the more you can merge the cleanup and delete steps on a single RUN command after the download and install, the smaller your final image will be. Otherwise your layers will include all the intermediate steps between the download and cleanup:\nFROM ubuntu:14.04\nMAINTAINER Samuel Alexander <samuel@alexander.com>\n\nRUN DEBIAN_FRONTEND=noninteractive \\\n    apt-get -y install software-properties-common && \\\n    apt-get -y update\n\n# Install Java.\nRUN echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections && \\\n    add-apt-repository -y ppa:webupd8team/java && \\\n    apt-get -y update && \\\n    DEBIAN_FRONTEND=noninteractive \\\n    apt-get install -y oracle-java8-installer && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    rm -rf /var/cache/oracle-jdk8-installer && \\\n\n# Define working directory.\nWORKDIR /work\n\n# Define commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\n\n# JAVA PATH\nENV PATH /usr/lib/jvm/java-8-oracle/bin:$PATH\n\n# Install maven\nRUN apt-get -y update && \\\n    DEBIAN_FRONTEND=noninteractive \\\n    apt-get -y install \n      maven \\\n      openssh-server \\\n      git \\\n      supervisor && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# clone Spark\nRUN git clone https://github.com/apache/spark.git\nWORKDIR /work/spark\nRUN mvn -DskipTests clean package\n\n# clone and build zeppelin fork\nARG INCUBATOR_VER=unknown\nRUN git clone https://github.com/apache/incubator-zeppelin.git\nWORKDIR /work/incubator-zeppelin\nRUN mvn clean package -Pspark-1.6 -Phadoop-2.6 -DskipTests\n\n# Configure Supervisord\nRUN mkdir -p var/log/supervisor\nCOPY conf/supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\n# bash\nRUN sed -i s#/home/git:/bin/false#/home/git:/bin/bash# /etc/passwd\n\nEXPOSE 8080 8082\nCMD [\"/usr/bin/supervisord\"]",
    "Execute command on host during docker build": "(Just a suggestion)\nWe usually have the following structure for building our docker images:\nmy-image/\n\u251c\u2500\u2500 assets\n\u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2514\u2500\u2500 install.sh\n\u251c\u2500\u2500 build.sh\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 VERSION\nbuild.sh: This is where you should invoke script_that_creates_magic_file.sh. Other common tasks involve downloading required files or temporarily copying ssh keys from the host. Finally, this script will call docker build .\nDockerfile: As usual, but depending on the number of commands we need to run we might have an install.sh\ninstall.sh: This is copied and run inside the container, installs packages, removes unnecessary files, etc. Without being 100% sure - I think such an approach reduces the number of layers avoiding multiple commands in a single RUN\nentrypoint.sh: Container's entrypoint. Allows us to perform tasks when the container starts (like parse environment variables) and print debugging info\nI find the above structure convenient and self-documented since everyone in the team can build any image (no special instructions/steps). The README is there to explain what the image is doing... but I won't lie to you... it is usually empty... (or has an h1 for the gitlab to display) :)",
    "Docker compose with name other than dockerfile": "In your docker-compose, under the service:\nservices:\n  serviceA:\n    build: \n      context: <folder of your project>\n      dockerfile: <path and name to your Dockerfile>",
    "How to mount local volumes in docker machine": "Docker-machine automounts the users directory... But sometimes that just isn't enough.\nI don't know about docker 1.6, but in 1.8 you CAN add an additional mount to docker-machine\nAdd Virtual Machine Mount Point (part 1)\nCLI: (Only works when machine is stopped)\nVBoxManage sharedfolder add <machine name/id> --name <mount_name> --hostpath <host_dir> --automount\nSo an example in windows would be\n/c/Program\\ Files/Oracle/VirtualBox/VBoxManage.exe sharedfolder add default --name e --hostpath 'e:\\' --automount\nGUI: (does NOT require the machine be stopped)\nStart \"Oracle VM VirtualBox Manager\"\nRight-Click <machine name> (default)\nSettings...\nShared Folders\nThe Folder+ Icon on the Right (Add Share)\nFolder Path: <host dir> (e:)\nFolder Name: <mount name> (e)\nCheck on \"Auto-mount\" and \"Make Permanent\" (Read only if you want...) (The auto-mount is sort of pointless currently...)\nMounting in boot2docker (part 2)\nManually mount in boot2docker:\nThere are various ways to log in, use \"Show\" in \"Oracle VM VirtualBox Manager\", or ssh/putty into docker by IP address docker-machine ip default, etc...\nsudo mkdir -p <local_dir>\nsudo mount -t vboxsf -o defaults,uid=`id -u docker`,gid=`id -g docker` <mount_name> <local_dir>\nBut this is only good until you restart the machine, and then the mount is lost...\nAdding an automount to boot2docker:\nWhile logged into the machine\nEdit/create (as root) /mnt/sda1/var/lib/boot2docker/bootlocal.sh, sda1 may be different for you...\nAdd\nmkdir -p <local_dir>\nmount -t vboxsf -o defaults,uid=`id -u docker`,gid=`id -g docker` <mount_name> <local_dir>\nWith these changes, you should have a new mount point. This is one of the few files I could find that is called on boot and is persistent. Until there is a better solution, this should work.\nOld method: Less recommended, but left as an alternative\nEdit (as root) /mnt/sda1/var/lib/boot2docker/profile, sda1 may be different for you...\nAdd\nadd_mount() {\n  if ! grep -q \"try_mount_share $1 $2\" /etc/rc.d/automount-shares ; then\n    echo \"try_mount_share $1 $2\" >> /etc/rc.d/automount-shares\n  fi\n}\n\nadd_mount <local dir> <mount name>\nAs a last resort, you can take the slightly more tedious alternative, and you can just modify the boot image.\ngit -c core.autocrlf=false clone https://github.com/boot2docker/boot2docker.git\ncd boot2docker\ngit -c core.autocrlf=false checkout v1.8.1 #or your appropriate version\nEdit rootfs/etc/rc.d/automount-shares\nAdd try_mount_share <local_dir> <mount_name> line right before fi at the end. For example\ntry_mount_share /e e\nJust be sure not to set the to anything the os needs, like /bin, etc...\ndocker build -t boot2docker . #This will take about an hour the first time :(\ndocker run --rm boot2docker > boot2docker.iso\nBackup the old boot2docker.iso and copy your new one in its place, in ~/.docker/machine/machines/\nThis does work, it's just long and complicated\ndocker version 1.8.1, docker-machine version 0.4.0",
    "Docker create network should ignore existing network": "Building on @AndyTriggs' answer, a neat (and correct) solution would be:\ndocker network inspect my_local_network >/dev/null 2>&1 || \\\n    docker network create --driver bridge my_local_network",
    "Can we mount sub-directories of a named volume in docker?": "2023: As noted by Michael Bolli in the comments, that feature is now a work-in-progress:\nPR 45687: \"volumes: Implement subpath mount\"\nMake it possible to mount subdirectory of a named volume.\nQ1 2024: this is merged! Commit 31ccdbb.\nPossibly for Moby 26.0.\n[Documentation: docker/docs PR 20577 with example:\nMount a volume subdirectory\nWhen you mount a volume to a container, you can specify a subdirectory of the volume to use, with the volume-subpath parameter for the --mount flag. The subdirectory that you specify must exist in the volume before you attempt to mount it into a container; if it doesn't exist, the mount fails.\nSpecifying volume-subpath is useful if you only want to share a specific portion of a volume with a container. Say for example that you have multiple containers running and you want to store logs from each container in a shared volume. You can create a subdirectory for each container in the shared volume, and mount the subdirectory to the container.\nThe following example creates a logs volume and initiates the subdirectories app1 and app2 in the volume. It then starts two containers and mounts one of the subdirectories of the logs volume to each container. This example assumes that the processes in the containers write their logs to /var/log/app1 and /var/log/app2.\n$ docker volume create logs\n$ docker run --rm \\\n  --mount src=logs,dst=/logs \\\n  alpine mkdir -p /logs/app1 /logs/app2\n$ docker run -d \\\n  --name=app1 \\\n  --mount src=logs,dst=/var/log/app1/,volume-subpath=app1 \\\n  app1:latest\n$ docker run -d \\\n  --name=app2 \\\n  --mount src=logs,dst=/var/log/app2,volume-subpath=app2 \\\n  app2:latest\nWith this setup, the containers write their logs to separate subdirectories of the logs volume. The containers can't access the other container's logs.\nMarch 2024, as mentioned in issue 32582:\nCLI already supports it (starting from v26.0.0-rc1): docker/cli PR #4331\n(and you can already install it from the test channel)\nCompose support is still WIP (it needs to move to v26 first).\nMarch 2024: Pawe\u0142 Gronowski confirms in the same issue:\nmoby v26.0.0 is released, so you can already try out this feature.\n2016: No because compose/config/config.py#load(config_details) check if datavolume/sql_data matches a named volume (in compose/config/validation.py#match_named_volumes()).\ndatavolume would, datavolume/sql_data would not.\nAs memetech points out in the comments, the is an issue tracking this since April 2017:\nmoby/moby issue 32582: \"[feature] Allow mounting sub-directories of named volumes\".\nIn that issue, Joohansson adds (see comment)\nIn the meantime, I use this workaround to mount the whole volume on a separate path and then symlink it to the sub path.\n# In the Dockerfile:\nRUN mkdir -p /data/subdir\nRUN ln -s /data/subdir /var/www/subdir\nThen mount the volume as normal.\nThe /subdir must exist in the volume.\ndocker run -d -v myvol:/data mycontainer\nNow anything read or written by the webserver will be stored in the volume subdir and can't access the other data.",
    "\"docker build\" requires exactly 1 argument(s)": "Parameter -f changes the name of the Dockerfile (when it's different than regular Dockerfile). It is not for passing the full path to docker build. The path goes as the first argument.\nSyntax is:\ndocker build [PARAMS] PATH\nSo in your case, this should work:\ndocker build -f MyDockerfile -t proj:myapp /full/path/to/\nor in case you are in the project directory, you just need to use a dot:\ndocker build -f MyDockerfile -t proj:myapp .",
    "Docker build pull access denied, repository does not exist or may require": "You have stages called base, build, and debug. Then in the final stage you have:\nCOPY --from=publish/app /app .\nWhen docker can't find the stage with that name, publish/app, it tries to find that image, which doesn't exist. I'm guessing you want to copy from the build stage, e.g.\nCOPY --from=build /app .",
    "Is it possible to show the `WORKDIR` when building a docker image?": "There's no builtin way for Docker to print the WORKDIR during a build. You can inspect the final workdir for an image/layer via the .Config.WorkingDir property in the inspect output:\ndocker image inspect -f '{{.Config.WorkingDir}}' {image-name}\nIt's possible to view a Linux containers build steps workdir by printing the shells default working directory:\nRUN pwd\nor the shell often stores the working directory in the PWD environment variable\nRUN echo \"$PWD\"\nIf the RUN step has run before and is cached, add the --no-cache flag. If you are using newer versions of docker with BuildKit, stdout from the build will need to be enabled with --progress=plain\ndocker build --no-cache --progress=plain . ",
    "How to give folder permissions inside a docker container Folder": "I guess you are switching to user \"admin\" which doesn't have the ownership to change permissions on /app directory. Change the ownership using \"root\" user. Below Dockerfile worked for me -\nFROM python:2.7\nRUN pip install Flask==0.11.1 \nRUN useradd -ms /bin/bash admin\nCOPY app /app\nWORKDIR /app\nRUN chown -R admin:admin /app\nRUN chmod 755 /app\nUSER admin\nCMD [\"python\", \"app.py\"] \nPS - Try to get rid of \"777\" permission. I momentarily tried to do it in above Dockerfile.",
    "/bin/sh: 1: apk: not found while creating docker image": "As larsks mentions, apk is for Alpine distributions and you selected FROM ubuntu:trusty which is Debian based with the apt-get command. Change your FROM line to FROM alpine:3.4 to switch to the Alpine based image with apk support.",
    "How do I run a Bash script in an Alpine Docker container?": "Alpine comes with ash as the default shell instead of bash.\nSo you can\nHave a shebang defining /bin/bash as the first line of your sayhello.sh, so your file sayhello.sh will begin with bin/sh\n#!/bin/sh\nInstall Bash in your Alpine image, as you seem to expect Bash is present, with such a line in your Dockerfile:\nRUN apk add --no-cache --upgrade bash",
    "How to add user with dockerfile?": "Use useradd instead of its interactive adduser to add user.\nRUN useradd -ms /bin/bash  vault\nBelow command will not create user .\nUSER vault\nWORKDIR /usr/local/bin/vault\nit will use vault user\nplease Refer Dockerfile User Documentation\nThe USER instruction sets the user name or UID to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile.\nNOTE : Ensures that bash is the default shell.\nIf default shell is /bin/sh you can do like:\nRUN ln -sf /bin/bash /bin/sh\nRUN useradd -ms /bin/bash  vault",
    "How to ADD all files/directories except a hidden directory like .git in Dockerfile": "You may exclude unwanted files with the help of the .dockerignore file",
    "How to prevent Dockerfile caching git clone": "Another workaround:\nIf you use GitHub (or gitlab or bitbucket too most likely) you can ADD the GitHub API's representation of your repo to a dummy location.\nADD https://api.github.com/repos/$USER/$REPO/git/refs/heads/$BRANCH version.json\nRUN git clone -b $BRANCH https://github.com/$USER/$REPO.git $GIT_HOME/\nThe API call will return different results when the head changes, invalidating the docker cache.\nIf you're dealing with private repos you can use github's x-oauth-basic authentication scheme with a personal access token like so:\nADD https://$ACCESS_TOKEN:x-oauth-basic@api.github.com/repos/$USER/$REPO/git/refs/heads/$BRANCH version.json\n(thx @captnolimar for a suggested edit to clarify authentication)",
    "Docker-Compose file has yaml.scanner.ScannerError": "Ok, I wasted around 3 hours to debug a similar issue.\nIf you guys ever get the below error\nERROR: yaml.scanner.ScannerError: mapping values are not allowed here\nin \".\\docker-compose.yml\", line 2, column 9\nIt's because a space is needed between\nversion:'3' <-- this is wrong\nversion: '3' <-- this is correct.\nAlso, if you are using eclipse, do yourself a favor and install the YEdit YAML editor plugin",
    "alpine package py-pip missing": "Do update first:\napk add --update py-pip\nOr:\napk update\napk add py-pip",
    "Error response from daemon: Dockerfile parse error Unknown flag: mount": "tl;dr\nDockerfile\n# syntax=docker/dockerfile:experimental\nFROM continuumio/miniconda3\n\nRUN --mount=type=ssh pip install git+ssh://git@github.com/myrepo/myproject.git@develop\nRUN conda install numpy\n...\nNote: the comment on the first line is required voodoo\nThen build your docker image with:\nDOCKER_BUILDKIT=1 docker build --ssh default -t my_image .\nWith this, you will be able to use the --mount option for the RUN directive in your Dockerfile.\nLong answer\nAs found in the documentation here, ssh forwarding when building docker image is enabled only when using the BuildKit backend:\nExternal implementation features\nThis feature is only available when using the BuildKit backend.\nDocker build supports experimental features like cache mounts, build secrets and ssh forwarding that are enabled by using an external implementation of the builder with a syntax directive. To learn about these features, refer to the documentation in BuildKit repository.\nFor this you need Docker 18.09 (or later) and you also need to run the docker build command with the DOCKER_BUILDKIT=1 environment variable and start your Docker file with the following magic comment : # syntax=docker/dockerfile:experimental.\nAlso you can edit /etc/docker/daemon.json and add :\n{\n    \"experimental\" : false,\n    \"debug\" : true,\n    \"features\": {\n        \"buildkit\" : true\n    }\n}",
    "Operation of the mkdir command with dockerfile": "The reason is that you are mounting a volume from your host to /var/www/html. Step by step:\nRUN mkdir -p /var/www/html/foo creates the foo directory inside the filesystem of your container.\ndocker-compose.yml ./code:/var/www/html \"hides\" the content of /var/www/html in the container filesystem behind the contents of ./code on the host filesystem.\nSo actually, when you exec into your container you see the contents of the ./code directory on the host when you look at /var/www/html.\nFix: Either you remove the volume from your docker-compose.yml or you create the foo-directory on the host before starting the container.\nAdditional Remark: In your Dockerfile you declare a volume as VOLUME ./code:/var/www/html. This does not work and you should probably remove it. In a Dockerfile you cannot specify a path on your host.\nQuoting from docker:\nThe host directory is declared at container run-time: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability. since a given host directory can\u2019t be guaranteed to be available on all hosts. For this reason, you can\u2019t mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.",
    "Should I minimize the number of docker layers?": "I work in a small team with a private Docker registry. We won't ever meet the 42 layers restriction and care mostly about performance and development speed.\nIf so, should I minimize the number of docker layers?\nIn your case, no.\nWhat needs to be minimized is the build time, which means:\nmaking sure the most general steps, and the longest are first, that will then cached, allowing you to fiddle with the last lines of your Dockerfile (the most specific commands) while having a quick rebuild time.\nmaking sure the longest RUN command come first and in their own layer (again to be cached), instead of being chained with other RUN commands: if one of those fail, the long command will have to be re-executed. If that long command is isolated in its own (Dockerfile line)/layer, it will be cached.\nThat being said, the documentation you mention comes from docker/docker.github.io, precisely PR 4992 and PR 4854, after a docker build LABEL section.\nSo this section comes after a similar remark about LABEL, and just emphasize the commands creating layers.\nAgain, in your case, that would not be important.",
    "Difference between 'image' and 'build' within docker compose": "image means docker compose will run a container based on that image\nbuild means docker compose will first build an image based on the Dockerfile found in the path associated with build (and then run a container based on that image).\nPR 2458 was eventually merged to allow both (and use image as the image name when building, if it exists).\ntherobyouknow mentions in the comments:\ndockerfile: as a sub-statement beneath build: can be used to specify the filename/path of the Dockerfile.\nversion: '3'\nservices:\n  webapp:\n    build:\n      context: ./dir\n      dockerfile: Dockerfile-alternate\n      args:\n        buildno: 1",
    "How are Packer and Docker different? Which one should I prefer when provisioning images?": "Docker is a system for building, distributing and running OCI images as containers. Containers can be run on Linux and Windows.\nPacker is an automated build system to manage the creation of images for containers and virtual machines. It outputs an image that you can then take and run on the platform you require.\nFor v1.8 this includes - Alicloud ECS, Amazon EC2, Azure, CloudStack, DigitalOcean, Docker, Google Cloud, Hetzner, Hyper-V, Libvirt, LXC, LXD, 1&1, OpenStack, Oracle OCI, Parallels, ProfitBricks, Proxmox, QEMU, Scaleway, Triton, Vagrant, VirtualBox, VMware, Vultr\nDocker's Dockerfile\nDocker uses a Dockerfile to manage builds which has a specific set of instructions and rules about how you build a container.\nImages are built in layers. Each FROM RUN ADD COPY commands modify the layers included in an OCI image. These layers can be cached which helps speed up builds. Each layer can also be addressed individually which helps with disk usage and download usage when multiple images share layers.\nDockerfiles have a bit of a learning curve, It's best to look at some of the official Docker images for practices to follow.\nPacker's Docker builder\nPacker does not require a Dockerfile to build a container image. The docker plugin has a HCL or JSON config file which start the image build from a specified base image (like FROM).\nPacker then allows you to run standard system config tools called \"Provisioners\" on top of that image. Tools like Ansible, Chef, Salt, shell scripts etc. This image will then be exported as a single layer, so you lose the layer caching/addressing benefits compared to a Dockerfile build.\nPacker allows some modifications to the build container environment, like running as --privileged or mounting a volume at build time, that Docker builds will not allow.\nTimes you might want to use Packer are if you want to build images for multiple platforms and use the same setup. It also makes it easy to use existing build scripts if there is a provisioner for it.",
    "Does putting ARG at top of Dockerfile prevent layer re-use?": "In general, it is better to place ARG just before it is used rather than at the top of the file.\nTo be more precise, not all lines are cache invalidated after an ARG declaration. Only those that use ARG values and RUNs. The docker documentation provides details:\nImpact on build caching\nARG variables are not persisted into the built image as ENV variables are. However, ARG variables do impact the build cache in similar ways. If a Dockerfile defines an ARG variable whose value is different from a previous build, then a \u201ccache miss\u201d occurs upon its first usage, not its definition. In particular, all RUN instructions following an ARG instruction use the ARG variable implicitly (as an environment variable), thus can cause a cache miss. All predefined ARG variables are exempt from caching unless there is a matching ARG statement in the Dockerfile.\nhttps://docs.docker.com/engine/reference/builder/#impact-on-build-caching\nYou'll have to move your ARGs under the RUNs that would not need the argument in order to keep layer cache optimized.\nFor more info:\nhttps://github.com/moby/moby/issues/18017\nhttps://github.com/moby/moby/pull/18161\nRUN explanation here: https://github.com/moby/moby/pull/21885",
    "Docker image error: \"/bin/sh: 1: [python,: not found\"": "Use \" instead of ' in CMD. (Documentation)",
    "Maven docker cache dependencies": "You should also consider using mvn dependency:resolve or mvn dependency:go-offline accordingly as other comments & answers suggest.\nUsually, there's no change in pom.xml file but just some other source code changes when you're attempting to start docker image build. In such circumstance you can do this:\nFROM maven:3-jdk-8\n\nENV HOME=/home/usr/app\n\nRUN mkdir -p $HOME\n\nWORKDIR $HOME\n\n# 1. add pom.xml only here\n\nADD pom.xml $HOME\n\n# 2. start downloading dependencies\n\nRUN [\"/usr/local/bin/mvn-entrypoint.sh\", \"mvn\", \"verify\", \"clean\", \"--fail-never\"]\n\n# 3. add all source code and start compiling\n\nADD . $HOME\n\nRUN [\"mvn\", \"package\"]\n\nEXPOSE 8005\n\nCMD [\"java\", \"-jar\", \"./target/dist.jar\"]\nSo the key is:\nadd pom.xml file.\nthen mvn verify --fail-never it, it will download maven dependencies.\nadd all your source file then, and start your compilation(mvn package).\nWhen there are changes in your pom.xml file or you are running this script for the first time, docker will do 1 -> 2 -> 3. When there are no changes in pom.xml file, docker will skip step 1\u30012 and do 3 directly.\nThis simple trick can be used in many other package management circumstances(gradle, yarn, npm, pip).",
    "Hidden file .env not copied using Docker COPY": "If you have .dockerignore file then it might be you added to ignore hidden files like .git, .vagrant etc.\nIf .dockerfile ignoring hidden files then either you can enable to not ignore or change file name.\nFor more info about .dockerignore file",
    "Using docker-compose to set containers timezones": "This is simple solution:\nenvironment:\n  - TZ=America/Denver",
    "How to run kubectl commands inside a container?": "I would use kubernetes api, you just need to install curl, instead of kubectl and the rest is restful.\ncurl http://localhost:8080/api/v1/namespaces/default/pods\nIm running above command on one of my apiservers. Change the localhost to apiserver ip address/dns name.\nDepending on your configuration you may need to use ssl or provide client certificate.\nIn order to find api endpoints, you can use --v=8 with kubectl.\nexample:\nkubectl get pods --v=8\nResources:\nKubernetes API documentation\nUpdate for RBAC:\nI assume you already configured rbac, created a service account for your pod and run using it. This service account should have list permissions on pods in required namespace. In order to do that, you need to create a role and role binding for that service account.\nEvery container in a cluster is populated with a token that can be used for authenticating to the API server. To verify, Inside the container run:\ncat /var/run/secrets/kubernetes.io/serviceaccount/token\nTo make request to apiserver, inside the container run:\ncurl -ik \\\n     -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n     https://kubernetes.default.svc.cluster.local/api/v1/namespaces/default/pods",
    "Docker - What is proper way to rebuild and push updated image to docker cloud?": "I found the problem, thanks to @lorenzvth7!\nI've had two images with same tag (which i was pushing to cloud).\nSolution is:\nInspect your images and find two or more with the same tag:\ndocker images \nDelete them:\ndocker rmi --force 'image id'\nThats it! Follow steps from my question above.",
    "Dockerfile CMD instruction will exit the container just after running it": "A docker container will run as long as the CMD from your Dockerfile takes.\nIn your case your CMD consists of a shell script containing a single echo. So the container will exit after completing the echo.\nYou can override CMD, for example:\nsudo docker run -it --entrypoint=/bin/bash <imagename>\nThis will start an interactive shell in your container instead of executing your CMD. Your container will exit as soon as you exit that shell.\nIf you want your container to remain active, you have to ensure that your CMD keeps running. For instance, by adding the line while true; do sleep 1; done to your shell.sh file, your container will print your hello message and then do nothing any more until you stop it (using docker stop in another terminal).\nYou can open a shell in the running container using docker exec -it <containername> bash. If you then execute command ps ax, it will show you that your shell.sh is still running inside the container.",
    "Reuse inherited image's CMD or ENTRYPOINT": "As mentioned in the comments, there's no built-in solution to this. From the Dockerfile, you can't see the value of the current CMD or ENTRYPOINT. Having a run-parts solution is nice if you control the upstream base image and include this code there, allowing downstream components to make their changes. But docker there's one inherent issue that will cause problems with this, containers should only run a single command that needs to run in the foreground. So if the upstream image kicks off, it would stay running without giving your later steps a chance to run, so you're left with complexities to determine the order to run commands to ensure that a single command does eventually run without exiting.\nMy personal preference is a much simpler and hardcoded option, to add my own command or entrypoint, and make the last step of my command to exec the upstream command. You will still need to manually identify the script name to call from the upstream Dockerfile. But now in your start.sh, you would have:\n#!/bin/sh\n\n# run various pieces of initialization code here\n# ...\n\n# kick off the upstream command:\nexec /upstream-entrypoint.sh \"$@\"\nBy using an exec call, you transfer pid 1 to the upstream entrypoint so that signals get handled correctly. And the trailing \"$@\" passes through any command line arguments. You can use set to adjust the value of $@ if there are some args you want to process and extract in your own start.sh script.",
    "Docker container doesn't expose ports when --net=host is mentioned in the docker run command": "I was confused by this answer. Apparently my docker image should be reachable on port 8080. But it wasn't. Then I read\nhttps://docs.docker.com/network/host/\nTo quote\nThe host networking driver only works on Linux hosts, and is not supported on Docker for Mac, Docker for Windows, or Docker EE for Windows Server.\nThat's rather annoying as I'm on a Mac. The docker command should report an error rather than let me think it was meant to work.\nDiscussion on why it does not report an error\nhttps://github.com/docker/for-mac/issues/2716\nNot sure I'm convinced.\nUpdated 2024: As per comments and other answers there have been changes in this area. See Docker container doesn't expose ports when --net=host is mentioned in the docker run command",
    "Externalising Spring Boot properties when deploying to Docker": "DOCKER IMAGE CONFIGURATION\nIf you look to the way Spring recommends to launch a Spring Boot powered docker container, that's what you find:\nFROM openjdk:8-jdk-alpine\nVOLUME /tmp\nARG JAR_FILE\nCOPY ${JAR_FILE} app.jar\nENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/app.jar\"]\nThat means your image extends openjdk and your container has its own environment. If you're doing like that, it would be enough to declare what you want to override as environment properties and Spring Boot will fetch them, since environment variables take precedence over the yml files.\nEnvironment variables can be passed in your docker command too, to launch the container with your desired configuration. If you want to set some limit for the JVM memory, see the link below.\nDOCKER COMPOSE SAMPLE\nHere you have an example of how I launch a simple app environment with docker compose. As you see, I declare the spring.datasource.url property here as an environment variable, so it overrides whatever you've got in your application.yml file.\nversion: '2'\nservices:\n    myapp:\n        image: mycompany/myapp:1.0.0\n        container_name: myapp\n        depends_on:\n        - mysql\n        environment:\n            - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/myapp?useUnicode=true&characterEncoding=utf8&useSSL=false\n        ports:\n            - 8080:8080\n\n    mysql:\n        image: mysql:5.7.19\n        container_name: mysql\n        volumes:\n            - /home/docker/volumes/myapp/mysql/:/var/lib/mysql/\n        environment:\n            - MYSQL_USER=root\n            - MYSQL_ALLOW_EMPTY_PASSWORD=yes\n            - MYSQL_DATABASE=myapp\n        command: mysqld --lower_case_table_names=1 --skip-ssl --character_set_server=utf8\nSee also:\nHow do I pass environment variables to Docker containers?\nLimit JVM memory consumption in a Docker container",
    "Error in docker: network \"path\" declared as external, but could not be found": "I solved the issue, finally. The issue came from the fact that I had in docker-compose.yml remaxmdcrm_remaxmd-network declared as external. The external network was not created during installation, thus I needed to create a bridging network. I ran the command docker network create \"name_of_network\"\nFor further details, here is the full documentation this",
    "Dockerfile: $HOME is not working with ADD/COPY instructions": "Here's your problem:\nWhen you use the USER directive, it affects the userid used to start new commands inside the container. So, for example, if you do this:\nFROM ubuntu:utopic\nRUN useradd -m aptly\nUSER aptly\nRUN echo $HOME\nYou get this:\nStep 4 : RUN echo $HOME\n ---> Running in a5111bedf057\n/home/aptly\nBecause the RUN commands starts a new shell inside a container, which is modified by the preceding USER directive.\nWhen you use the COPY directive, you are not starting a process inside the container, and Docker has no way of knowing what (if any) environment variables would be exposed by a shell.\nYour best bet is to either set ENV HOME /home/aptly in your Dockerfile, which will work, or stage your files into a temporary location and then:\nRUN cp /skeleton/myfile $HOME/myfile\nAlso, remember that when you COPY files in they will be owned by root; you will need to explicitly chown them to the appropriate user.",
    "Docker CMD exec-form for multiple command execution": "The short answer is, you cannot chain together commands in the exec form.\n&& is a function of the shell, which is used to chain commands together. In fact, when you use this syntax in a Dockerfile, you are actually leveraging the shell functionality.\nIf you want to have multiple commands with the exec form, then you have do use the exec form to invoke the shell as follows...\nCMD [\"sh\",\"-c\",\"mkdir -p ~/my/new/directory/ && cd ~/my/new/directory && touch new.file\"]",
    "Installing Java in Docker image": "I was able to install OpenJDK 8 via the steps below (taken from here). My Dockerfile inherits from phusion/baseimage-docker, which is based on Ubuntu 16.04 LTS.\n# Install OpenJDK-8\nRUN apt-get update && \\\n    apt-get install -y openjdk-8-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean;\n    \n# Fix certificate issues\nRUN apt-get update && \\\n    apt-get install ca-certificates-java && \\\n    apt-get clean && \\\n    update-ca-certificates -f;\n\n# Setup JAVA_HOME -- useful for docker commandline\nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\nRUN export JAVA_HOME\nTo install OpenJDK 7 instead, you may need to prepend\nadd-apt-repository ppa:openjdk-r/ppa\nsuch that the first step becomes\n# Install OpenJDK-7\nRUN add-apt-repository ppa:openjdk-r/ppa && \\\n    apt-get update && \\\n    apt-get install -y openjdk-7-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean;",
    "Differences Between Dockerfile Instructions in Shell and Exec Form": "There are two differences between the shell form and the exec form. According to the documentation, the exec form is the preferred form. These are the two differences:\nThe exec form is parsed as a JSON array, which means that you must use double-quotes (\u201c) around words not single-quotes (\u2018).\nUnlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, CMD [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ \"sh\", \"-c\", \"echo $HOME\" ]. When using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nSome additional subtleties here are:\nThe exec form makes it possible to avoid shell string munging, and to RUN commands using a base image that does not contain the specified shell executable.\nIn the shell form you can use a \\ (backslash) to continue a single RUN instruction onto the next line.\nThere is also a third form for CMD:\nCMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\nAdditionally, the exec form is required for CMD if you are using it as parameters/arguments to ENTRYPOINT that are intended to be overwritten.",
    "Dockerfile replicate the host user UID and GID to the image": "You can pass it as a build arg. Your Dockerfile can be static:\nFROM ubuntu:xenial-20170214\nARG UNAME=testuser\nARG UID=1000\nARG GID=1000\nRUN groupadd -g $GID -o $UNAME\nRUN useradd -m -u $UID -g $GID -o -s /bin/bash $UNAME\nUSER $UNAME\nCMD /bin/bash\nThen you'd pass the options on your build command:\ndocker build --build-arg UID=$(id -u) --build-arg GID=$(id -g) \\\n  -f bb.dockerfile -t testimg .\nNote that I've solved similar problems to this a different way, by running an entrypoint as root that looks a file/directory permissions of the host volume mount, and adjust the uid/gid of the users inside the container to match the volume uid/gid. After making that change, it drops access from the root user to the modified uid/gid user and runs the original command/entrypoint. The result is the image can be run unchanged on any developer machine. An example of this can be found in my jenkins-docker repo:\nhttps://github.com/sudo-bmitch/jenkins-docker",
    "Is it redundant in a Dockfile to run USER root since you're already root?": "If an image was generated from a source that changed root to a user, you may not have access to all resources inside it. However, if you load the image:\nFROM xxxxxx/xxxxxx:latest\nUSER root\nThat will give you root access to the images resources. I just used that after being refused access to change /etc/apt/sources.list in an existing image that was not mine. It worked fine and let me change the sources.list",
    "Docker compose how to mount path from one to another container?": "What you want to do is use a volume, and then mount that volume into whatever containers you want it to appear in.\nCompletely within Docker\nYou can do this completely inside of Docker.\nHere is an example (stripped-down - your real file would have much more than this in it, of course).\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - asset-volume:/var/lib/assets\n  asset:\n    volumes:\n      - asset-volume:/var/lib/assets\n\nvolumes:\n  asset-volume:\nAt the bottom is a single volume defined, named \"asset-volume\".\nThen in each of your services, you tell Docker to mount that volume at a certain path. I show example paths inside the container, just adjust these to be whatever path you wish them to be in the container.\nThe volume is an independent entity not owned by any particular container. It is just mounted into each of them, and is shared. If one container modifies the contents, then they all see the changes.\nNote that if you prefer only one can make changes, you can always mount the volume as read-only in some services, by adding :ro to the end of the volume string.\nservices:\n  servicename:\n    volumes:\n      - asset-volume:/var/lib/assets:ro\nUsing a host directory\nAlternately you can use a directory on the host and mount that into the containers. This has the advantage of you being able to work directly on the files using your tools outside of Docker (such as your GUI text editor and other tools).\nIt's the same, except you don't define a volume in Docker, instead mounting the external directory.\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - ./assets:/var/lib/assets\n  asset:\n    volumes:\n      - ./assets:/var/lib/assets\nIn this example, the local directory \"assets\" is mounted into both containers using the relative path ./assets.\nUsing both depending on environment\nYou can also set it up for a different dev and production environment. Put everything in docker-compose.yml except the volume mounts. Then make two more files.\ndocker-compose.dev.yml\ndocker-compose.prod.yml\nIn these files put only the minimum config to define the volume mount. We'll mix this with the docker-compose.yml to get a final config.\nThen use this. It will use the config from docker-compose.yml, and use anything in the second file as an override or supplemental config.\ndocker-compose -f docker-compose.yml \\\n    -f docker-compose.dev.yml \\\n    up -d\nAnd for production, just use the prod file instead of the dev file.\nThe idea here is to keep most of the config in docker-compose.yml, and only the minimum set of differences in the alternative files.\nExample:\ndocker-compose.prod.yml\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - asset-volume:/var/lib/assets\ndocker-compose.dev.yml\nversion: '3'\nservices:\n  nginx:\n    volumes:\n      - ./assets:/var/lib/assets",
    "Docker COPY not updating files when rebuilding container": "This is because of cache.\nRun,\ndocker-compose build --no-cache\nThis will rebuild images without using any cache.\nAnd then,\ndocker-compose -f docker-compose-staging.yml up -d",
    "Install python package in docker file": "Recommended base image\nAs suggested in my comment, you could write a Dockerfile that looks like:\nFROM python:3\n\nRUN pip install --no-cache-dir --upgrade pip && \\\n    pip install --no-cache-dir nibabel pydicom matplotlib pillow med2image\n    # Note: we had to merge the two \"pip install\" package lists here, otherwise\n    # the last \"pip install\" command in the OP may break dependency resolution\u2026\n\nCMD [\"cat\", \"/etc/os-release\"]\nAnd the command example above could confirm at runtime (docker build --pull -t test . && docker run --rm -it test) that this image is based on the GNU/Linux distribution \"Debian stable\".\nGeneric Dockerfile template\nFinally to give a comprehensive answer, note that a good practice regarding Python dependencies consists in specifying them in a declarative way in a dedicated text file (in alphabetical order, to ease review and update) so that for your example, you may want to write the following file:\nrequirements.txt\nmatplotlib\nmed2image\nnibabel\npillow\npydicom\nand use the following generic Dockerfile\nFROM python:3\n\nWORKDIR /usr/src/app\n\nCOPY requirements.txt ./\n\nRUN pip install --no-cache-dir --upgrade pip \\\n  && pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"./your-daemon-or-script.py\"]\nTo be more precise, this is the approach suggested in the documentation of the Docker official image python, \u00a7. How to use this image",
    "Docker-compose: deploying service in multiple hosts": "We can do this with docker compose v3 now.\nhttps://docs.docker.com/engine/swarm/#feature-highlights https://docs.docker.com/compose/compose-file/\nYou have to initialize the swarm cluster using command\n$ docker swarm init\nYou can add more nodes as worker or manager -\nhttps://docs.docker.com/engine/swarm/join-nodes/\nOnce you have your both nodes added to the cluster, pass your compose v3 i.e deployment file to create a stack. Compose file should just contain predefined images, you can't give a Dockerfile for deployment in Swarm mode.\n$ docker stack deploy -c dev-compose-deploy.yml --with-registry-auth PL\nView your stack services status -\n$ docker stack services PL\nTry to use Labels & Placement constraints to put services on different nodes.\nExample \"dev-compose-deploy.yml\" file for your reference -\nversion: \"3\"\n\nservices:\n\n  nginx:\n    image: nexus.example.com/pl/nginx-dev:latest\n    extra_hosts:\n      - \"dev-pldocker-01:10.2.0.42\u201d\n      - \"int-pldocker-01:10.2.100.62\u201d\n      - \"prd-plwebassets-01:10.2.0.62\u201d\n    ports:\n      - \"80:8003\"\n      - \"443:443\"\n    volumes:\n      - logs:/app/out/\n    networks:\n      - pl\n    deploy:\n      replicas: 3\n      labels:\n        feature.description: \u201cFrontend\u201d\n      update_config:\n        parallelism: 1\n        delay: 10s\n      restart_policy:\n        condition: any\n      placement:\n        constraints: [node.role == worker]\n    command: \"/usr/sbin/nginx\"\n\n  viz:\n    image: dockersamples/visualizer\n    ports:\n      - \"8085:8080\"\n    networks:\n      - pl\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    deploy:\n      replicas: 1\n      labels:\n        feature.description: \"Visualizer\"\n      restart_policy:\n        condition: any\n      placement:\n        constraints: [node.role == manager]\n\nnetworks:\npl:\n\nvolumes:\nlogs:",
    "How to copy files from dockerfile to host?": "This is now possible since Docker 19.03.0 in July 2019 introduced \"custom build outputs\". See the official docs about custom build outputs.\nTo enable custom build outputs from the build image into the host during the build process, you need to activate the BuildKit which is a newer recommended back-compatible way for the engine to do the build phase. See the official docs for enabling BuildKit.\nThis can be done in 2 ways:\nSet the environment variable DOCKER_BUILDKIT=1, or\nSet it in the docker engine by default by adding \"features\": { \"buildkit\": true } to the root of the config json.\nFrom the official docs about custom build outputs:\ncustom exporters allow you to export the build artifacts as files on the local filesystem instead of a Docker image, which can be useful for generating local binaries, code generation etc.\n...\nThe local exporter writes the resulting build files to a directory on the client side. The tar exporter is similar but writes the files as a single tarball (.tar).\nIf no type is specified, the value defaults to the output directory of the local exporter.\n...\nThe --output option exports all files from the target stage. A common pattern for exporting only specific files is to do multi-stage builds and to copy the desired files to a new scratch stage with COPY --from.\ne.g. an example Dockerfile\nFROM alpine:latest AS stage1\nWORKDIR /app\nRUN echo \"hello world\" > output.txt\n\nFROM scratch AS export-stage\nCOPY --from=stage1 /app/output.txt .\nRunning\nDOCKER_BUILDKIT=1 docker build --file Dockerfile --output out .\nThe tail of the output is:\n => [export-stage 1/1] COPY --from=stage1 /app/output.txt .\n0.0s\n => exporting to client\n0.1s\n => => copying files 45B\n0.1s\nThis produces a local file out/output.txt that was created by the RUN command.\n$ cat out/output.txt\nhello world\nAll files are output from the target stage\nThe --output option will export all files from the target stage. So using a non-scratch stage with COPY --from will cause extraneous files to be copied to the output. The recommendation is to use a scratch stage with COPY --from.\nWindows is not supported for now:\nhttps://docs.docker.com/build/buildkit/",
    "How to show the full (not truncated) \"CREATED BY\" commands in \"docker history\" output?": "Use the docker history --no-trunc option to show the full command.",
    "Is Docker Compose suitable for production?": "Really you need to define \"production\" in your case.\nCompose simply starts and stops multiple containers with a single command. It doesn't add anything to the mix you couldn't do with regular docker commands.\nIf \"production\" is a single docker host, with all instances and relationships defined, then compose can do that.\nBut if instead you want multiple hosts and dynamic scaling across the cluster then you are really looking at swarm or another option.",
    "Docker parallel operations limit": "The options are set in the configuration file (Linux-based OS it is located in the path: /etc/docker/daemon.json and C:\\ProgramData\\docker\\config\\daemon.json on Windows)\nOpen /etc/docker/daemon.json (If doesn't exist, create it)\nAdd the values(for push/pulls) and set parallel operations limit\n{\n    \"max-concurrent-uploads\": 1,\n    \"max-concurrent-downloads\": 1\n}\nRestart daemon: sudo service docker restart",
    "How to push Docker containers managed by Docker-compose to Heroku?": "Just an update on this question since it seems to be getting a lot of traction lately.\nThere is now an officially supported \"Heroku.yml\" solution offered by Heroku. You can now write a .yml file (with a format similar to docker-compose) and Heroku will work out your images. Just follow the link above for details.\nHappy Heroku-ing.",
    "Run jar file in docker image": "There is a difference between images and containers.\nImages will be built ONCE\nYou can start containers from Images\nIn your case:\nChange your image:\nFROM anapsix/alpine-java\nMAINTAINER myNAME \nCOPY testprj-1.0-SNAPSHOT.jar /home/testprj-1.0-SNAPSHOT.jar\nCMD [\"java\",\"-jar\",\"/home/testprj-1.0-SNAPSHOT.jar\"]\nBuild your image:\ndocker build -t imageName .\nNow invoke your program inside a container:\ndocker run --name myProgram imageName\nNow restart your program by restarting the container:\ndocker restart myProgram\nYour program changed? Rebuild the image!:\ndocker rmi imageName\ndocker build -t imageName .",
    "What is --from used in copy command in dockerfile": "This is a multi-stage build. This is used to keep the running docker container small while still be able to build/compile things needing a lot of dependencies.\nFor example a go application could be built by using:\nFROM golang:1.7.3 AS builder\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  \nSo in the first part we need a complete go environment to compile our software. Notice the name for the first part and the alias builder\nFROM golang:1.7.3 AS builder\nIn the second part beginning from the second FROM we only need the compiled app and no other go dependencies anymore. So we can change the base image to using a much smaller alpine Linux. But the compiled files are still in our builder image and not part of the image we want to start. So we need to copy files from the builder image via\nCOPY --from=builder\nYou can have as many stages as you want. The last one is the one defining the image which will be the template for the docker container.\nYou can read more about it in the official documentation: https://docs.docker.com/develop/develop-images/multistage-build/",
    "Docker RabbitMQ persistency": "TL;DR\nDidn't do too much digging on this, but it appears that the simplest way to do this is to change the hostname as Pedro mentions above.\n\nMORE INFO:\nUsing RABBITMQ_NODENAME\nIf you want to edit the RABBITMQ_NODENAME variable via Docker, it looks like you need to add a hostname as well since the Docker hostnames are generated as random hashes.\nIf you change the RABBITMQ_NODENAME var to something static like my-rabbit, RabbitMQ will throw something like an \"nxdomain not found\" error because it's looking for something like\nmy-rabbit@<docker_hostname_hash>. If you know the Docker hostname and can automate pulling it into your RABBITMQ_NODENAME value like so, my-rabbit@<docker_hostname_hash> I believe it would work.\nUPDATE\nI previously said,\nIf you know the Docker hostname and can automate pulling it into your RABBITMQ_NODENAME value like so, my-rabbit@<docker_hostname_hash> I believe it would work.\nThis would not work as described precisely because the default docker host name is randomly generated at launch, if it is not assigned explicitly. The hurdle would actually be to make sure you use the EXACT SAME <docker_hostname_hash> as your originating run so that the data directory gets picked up correctly. This would be a pain to implement dynamically/robustly. It would be easiest to use an explicit hostname as described below.\nThe alternative would be to set the hostname to a value you choose -- say, app-messaging -- AND ALSO set the RABBITMQ_NODENAME var to something like rabbit@app-messaging. This way you are controlling the full node name that will be used in the data directory.\nUsing Hostname\n(Recommended)\nThat said, unless you have a reason NOT to change the hostname, changing the hostname alone is the simplest way to ensure that your data will be mounted to and from the same point every time.\nI'm using the following Docker Compose file to successfully persist my setup between launches.\nversion: '3'\nservices:\n  rabbitmq:\n    hostname: 'mabbit'\n    image: \"${ARTIFACTORY}/rabbitmq:3-management\"\n    ports:\n      - \"15672:15672\"\n      - \"5672:5672\"\n    volumes:\n      - \"./data:/var/lib/rabbitmq/mnesia/\"\n    networks:\n      - rabbitmq\n\nnetworks:\n  rabbitmq:\n    driver: bridge\nThis creates a data directory next to my compose file and persists the RabbitMQ setup like so:\n./data/\n  rabbit@mabbit/\n  rabbit@mabbit-plugins-expand/\n  rabbit@mabbit.pid\n  rabbit@mabbit-feature_flags",
    "Cache Rust dependencies with Docker build": "Seems like you are not alone in your endeavor to cache rust dependencies via the docker build process. Here is a great article that helps you along the way.\nThe gist of it is you need a dummy.rs and your Cargo.toml first, then build it to cache the dependencies and then copy your application source later in order to not invalidate the cache with every build.\nDockerfile\nFROM rust\nWORKDIR /var/www/app\nCOPY dummy.rs .\nCOPY Cargo.toml .\nRUN sed -i 's#src/main.rs#dummy.rs#' Cargo.toml\nRUN cargo build --release\nRUN sed -i 's#dummy.rs#src/main.rs#' Cargo.toml\nCOPY . .\nRUN cargo build --release\nCMD [\"target/release/app\"]\nCMD application name \"app\" is based on what you have specified in your Cargo.toml for your binary.\ndummy.rs\nfn main() {}\nCargo.toml\n[package]\nname = \"app\"\nversion = \"0.1.0\"\nauthors = [\"...\"]\n[[bin]]\nname = \"app\"\npath = \"src/main.rs\"\n\n[dependencies]\nactix-web = \"1.0.0\"\nsrc/main.rs\nextern crate actix_web;\n\nuse actix_web::{web, App, HttpServer, Responder};\n\nfn index() -> impl Responder {\n    \"Hello world\"\n}\n\nfn main() -> std::io::Result<()> {\n    HttpServer::new(|| App::new().service(web::resource(\"/\").to(index)))\n        .bind(\"0.0.0.0:8080\")?\n        .run()\n}",
    "How do I reduce a python (docker) image size using a multi-stage build?": "ok so my solution is using wheel, it lets us compile on first image, create wheel files for all dependencies and install them in the second image, without installing the compilers\nFROM python:2.7-alpine as base\n\nRUN mkdir /svc\nCOPY . /svc\nWORKDIR /svc\n\nRUN apk add --update \\\n    postgresql-dev \\\n    gcc \\\n    musl-dev \\\n    linux-headers\n\nRUN pip install wheel && pip wheel . --wheel-dir=/svc/wheels\n\nFROM python:2.7-alpine\n\nCOPY --from=base /svc /svc\n\nWORKDIR /svc\n\nRUN pip install --no-index --find-links=/svc/wheels -r requirements.txt\nYou can see my answer regarding this in the following blog post\nhttps://www.blogfoobar.com/post/2018/02/10/python-and-docker-multistage-build",
    "How to install Go in alpine linux": "I just copied it over using multi stage builds, seems to be ok so far\nFROM XXX\n \nCOPY --from=golang:1.13-alpine /usr/local/go/ /usr/local/go/\n \nENV PATH=\"/usr/local/go/bin:${PATH}\"",
    "What is \"/app\" working directory for a Dockerfile?": "There are two important directories when building a docker image:\nthe build context directory.\nthe WORKDIR directory.\nBuild context directory\nIt's the directory on the host machine where docker will get the files to build the image. It is passed to the docker build command as the last argument. (Instead of a PATH on the host machine it can be a URL). Simple example:\ndocker build -t myimage .\nHere the current dir (.) is the build context dir. In this case, docker build will use Dockerfile located in that dir. All files from that dir will be visible to docker build.\nThe build context dir is not necessarily where the Dockerfile is located. Dockerfile location defaults to current dir and is otherwise indicated by the -f otpion. Example:\ndocker build -t myimage -f ./rest-adapter/docker/Dockerfile ./rest-adapter\nHere build context dir is ./rest-adapter, a subdirectory of where you call docker build; the Dokerfile location is indicated by -f.\nWORKDIR\nIt's a directory inside your container image that can be set with the WORKDIR instruction in the Dockerfile. It is optional (default is /, but the parent image might have set it), but setting it is considered a good practice. Subsequent instructions in the Dockerfile, such as RUN, CMD and ENTRYPOINT will operate in this dir. As for COPY and ADD, they use both...\nCOPY and ADD use both dirs\nThese two commands have <src> and <dest>.\n<src> is relative to the build context directory.\n<dest> is relative to the WORKDIR directory.\nFor example, if your Dockerfile contains...\nWORKDIR /myapp\nCOPY . .\nADD data/mydata.csv /usr/share/mydata.csv\nthen...\nthe COPY command will copy the contents of your build context directory to the /myapp dir inside your docker image.\nthe ADD command will copy file data/mydata.csv under your build context dir to the /usr/share dir at the docker image filesystem root level (<dest> in this case starts with /, so it's an absolute path).",
    "How to install docker in docker container?": "I had a similar problem trying to install Docker inside a Bamboo Server image. To solve this:\nfirst remove the line: RUN docker run hello-world from your Dockerfile\nThe simplest way is to just expose the Docker socket, by bind-mounting it with the -v flag or mounting a volume using Docker Compose:\ndocker run -v /var/run/docker.sock:/var/run/docker.sock ...",
    "Build postgres docker container with initial schema": "According to the usage guide for the official PostreSQL Docker image, all you need is:\nDockerfile\nFROM postgres\nENV POSTGRES_DB my_database\nCOPY psql_dump.sql /docker-entrypoint-initdb.d/\nThe POSTGRES_DB environment variable will instruct the container to create a my_database schema on first run.\nAnd any .sql file found in the /docker-entrypoint-initdb.d/ of the container will be executed.\nIf you want to execute .sh scripts, you can also provide them in the /docker-entrypoint-initdb.d/ directory.",
    "How to unset \"ENV\" in dockerfile?": "It depends on what effect you are trying to achieve.\nNote that, as a matter of pragmatics (i.e. how developers actually speak), \"unsetting a variable\" can mean two things: removing it from the environment, or setting the variable to an empty value. Technically, these are two different operations. In practice though I have not run into a case where the software I'm trying to control differentiates between the variable being absent from the environment, and the variable being present in the environment but set to an empty value. I generally can use either method to get the same result.\nIf you don't care whether the variable is in the layers produced by Docker, but leaving it with a non-empty value causes problems in later build steps.\nFor this case, you can use ENV VAR_NAME= at the point in your Dockerfile from which you want to unset the variable. Syntactic note: Docker allows two syntaxes for ENV: this ENV VAR=1 is the same as ENV VAR 1. You can separate the variable name from the value with a space or an equal sign. When you want to \"unset\" a variable by setting it to an empty value you must use the equal sign syntax or you get an error at build time.\nSo for instance, you could do this:\nENV NOT_SENSITIVE some_value\nRUN something\n\nENV NOT_SENSITIVE=\nRUN something_else\nWhen something runs, NOT_SENSITIVE is set to some_value. When something_else runs, NOT_SENSITIVE is set to the empty string.\nIt is important to note that doing unset NOT_SENSITIVE as a shell command will not affect anything else than what executes in this shell. Here's an example:\nENV NOT_SENSITIVE some_value\nRUN unset NOT_SENSITIVE && printenv NOT_SENSITIVE || echo \"does not exist\"\n\nRUN printenv NOT_SENSITIVE\nThe first RUN will print does not exist because NOT_SENSITIVE is unset when printenv executes and because it is unset printenv returns a non-zero exit code which causes the echo to execute. The second RUN is not affected by the unset in the first RUN. It will print some_value to the screen.\nBut what if I need to remove the variable from the environment, not just set it to an empty value?\nIn this case using ENV VAR_NAME= won't work. I don't know of any way to tell Docker \"from this point on, you must remove this variable from the environment, not just set it to an empty value\".\nIf you still want to use ENV to set your variable, then you'll have to start each RUN in which you want the variable to be unset with unset VAR_NAME, which will unset it for that specific RUN only.\nIf you want to prevent the variable from being present in the layers produced by Docker.\nSuppose that variable contains a secret and the layer could fall into the hands of people who should not have the secret. In this case you CANNOT use ENV to set the variable. A variable set with ENV is baked into the layers to which it applies and cannot be removed from those layers. In particular, (assuming the variable is named SENSITIVE) running\nRUN unset SENSITIVE\ndoes not do anything to remove it from the layer. The unset command above only removes SENSITIVE from the shell process that RUN starts. It affects only that shell. It won't affect shells spawned by CMD, ENTRYPOINT, or any command provided through running docker run at the command line.\nIn order to prevent the layers from containing the secret, I would use docker build --secret= and RUN --mount=type=secret.... For instance, assuming that I've stored my secret in a file named sensitive, I could have a RUN like this:\nRUN --mount=type=secret,id=sensitive,target=/root/sensitive \\\n export SENSITIVE=$(cat /root/sensitive) \\\n && [[... do stuff that requires SENSITIVE ]] \\\nNote that the command given to RUN does not need to end with unset SENSITIVE. Due to the way processes and their environments are managed, setting SENSITIVE in the shell spawned by RUN does not have any effect beyond what that shell itself spawns. Environment changes in this shell won't affect future shells nor will it affect what Docker bakes into the layers it creates.\nThen the build can be run with:\n$ DOCKER_BUILDKIT=1 docker build --secret id=secret,src=path/to/sensitive [...]\nThe environment for the docker build command needs DOCKER_BUILDKIT=1 to use BuildKit because this method of passing secrets is only available if Docker uses BuildKit to build the images.",
    "Rails server is still running in a new opened docker container": "You are using an onbuild image, so your working direcotry is mounted in the container image. This is very good for developing, since your app is updated in realtime when you edit your code, and your host system gets updated for example when you run a migration.\nThis also means that your host system tmp directory will be written with the pid file every time a server is running and will remain there if the server is not shut down correctly.\nJust run this command from your host system:\nsudo rm tmp/pids/server.pid \nThis can be a real pain when you are for example using foreman under docker-compose, since just pressing ctrl+c will not remove the pid file.",
    "Multiline comments in Dockerfiles": "As of today, no.\nAccording to Dockerfile reference documentation:\nDocker treats lines that begin with # as a comment, unless the line is a valid parser directive. A # marker anywhere else in a line is treated as an argument.:\nThere is no further details on how to comment lines.\nAs said by some comments already, most IDE will allow you to perform multiline comments easily (such as CTRL + / on IntelliJ)",
    "Dockerfile strategies for Git": "From Ryan Baumann's blog post \u201cGit strategies for Docker\u201d\nThere are different strategies for getting your Git source code into a Docker build. Many of these have different ways of interacting with Docker\u2019s caching mechanisms, and may be more or less appropriately suited to your project and how you intend to use Docker.\nRUN git clone\nIf you\u2019re like me, this is the approach that first springs to mind when you see the commands available to you in a Dockerfile. The trouble with this is that it can interact in several unintuitive ways with Docker\u2019s build caching mechanisms. For example, if you make an update to your git repository, and then re-run the docker build which has a RUN git clone command, you may or may not get the new commit(s) depending on if the preceding Dockerfile commands have invalidated the cache.\nOne way to get around this is to use docker build --no-cache, but then if there are any time-intensive commands preceding the clone they\u2019ll have to run again too.\nAnother issue is that you (or someone you\u2019ve distributed your Dockerfile to) may unexpectedly come back to a broken build later on when the upstream git repository updates.\nA two-birds-one-stone approach to this while still using RUN git clone is to put it on one line1 with a specific revision checkout, e.g.:\nRUN git clone https://github.com/example/example.git && cd example && git checkout 0123abcdef\nThen updating the revision to check out in the Dockerfile will invalidate the cache at that line and cause the clone/checkout to run.\nOne possible drawback to this approach in general is that you have to have git installed in your container.\nRUN curl or ADD a tag/commit tarball URL\nThis avoids having to have git installed in your container environment, and can benefit from being explicit about when the cache will break (i.e. if the tag/revision is part of the URL, that URL change will bust the cache). Note that if you use the Dockerfile ADD command to copy from a remote URL, the file will be downloaded every time you run the build, and the HTTP Last-Modified header will also be used to invalidate the cache.\nYou can see this approach used in the golang Dockerfile.\nGit submodules inside Dockerfile repository\nIf you keep your Dockerfile and Docker build in a separate repository from your source code, or your Docker build requires multiple source repositories, using git submodules (or git subtrees) in this repository may be a valid way to get your source repos into your build context. This avoids some concerns with Docker caching and upstream updating, as you lock the upstream revision in your submodule/subtree specification. Updating them will break your Docker cache as it changes the build context.\nNote that this only gets the files into your Docker build context, you still need to use ADD commands in your Dockerfile to copy those paths to where you expect them in the container.\nYou can see this approach used in the here\nDockerfile inside git repository\nHere, you just have your Dockerfile in the same git repository alongside the code you want to build/test/deploy, so it automatically gets sent as part of the build context, so you can e.g. ADD . /project to copy the context into the container. The advantage to this is that you can test changes without having to potentially commit/push them to get them into a test docker build; the disadvantage is that every time you modify any files in your working directory it will invalidate the cache at the ADD command. Sending the build context for a large source/data directory can also be time-consuming. So if you use this approach, you may also want to make judicious use of the .dockerignore file, including doing things like ignoring everything in your .gitignore and possibly the .git directory itself.\nVolume mapping\nIf you\u2019re using Docker to set up a dev/test environment that you want to share among a wide variety of source repos on your host machine, mounting a host directory as a data volume may be a viable strategy. This gives you the ability to specify which directories you want to include at docker run-time, and avoids concerns about docker build caching, but none of this will be shared among other users of your Dockerfile or container image.",
    "How to start apache2 automatically in a ubuntu docker container?": "The issue is here: CMD service apache2 start When you execute this command process apache2 will be detached from the shell. But Docker works only while main process is alive.\nThe solution is to run Apache in the foreground. Dockerfile must look like this: (only last line changed).\nFROM ubuntu\n\n# File Author / Maintainer\nMAINTAINER rmuktader\n\n# Update the repository sources list\nRUN apt-get update\n\n# Install and run apache\nRUN apt-get install -y apache2 && apt-get clean\n\n#ENTRYPOINT [\"/usr/sbin/apache2\", \"-k\", \"start\"]\n\n\n#ENV APACHE_RUN_USER www-data\n#ENV APACHE_RUN_GROUP www-data\n#ENV APACHE_LOG_DIR /var/log/apache2\n\nEXPOSE 80\nCMD apachectl -D FOREGROUND",
    "docker-compose change name of main container": "When refering to your main container, you are probably refering to the project name, which you could usually set via the -p flag. (See other answers)\nFor docker-compose, you can set the top level variable name to your desired project name.\ndocker-compose.yml file:\nversion: \"3.9\"\nname: my-project-name\nservices:\n  myService:\n    ...\nIf you are using Docker Desktop, make sure Use Docker Compose V2 is enabled there.",
    "Docker-compose volume mount before run": "Erik Dannenberg's is correct, the volume layering means that what I was trying to do makes no sense. (There is another really good explaination on the Docker website if you want to read more). If I want to have Docker do the npm install then I could do it like this:\nFROM node\n\nADD . /usr/src/app\nWORKDIR /usr/src/app\n\nRUN npm install --global gulp-cli \\\n && npm install\n\nCMD [\"gulp\", \"watch\"]\nHowever, this isn't appropriate as a solution for my situation. The goal is to use NPM to install project dependencies, then run gulp to build my project. This means I need read and write access to the project folder and it needs to persist after the container is gone.\nI need to do two things after the volume is mounted, so I came up with the following solution...\ndocker/gulp/Dockerfile:\nFROM node\n\nRUN npm install --global gulp-cli\n\nADD start-gulp.sh .\n\nCMD ./start-gulp.sh\ndocker/gulp/start-gulp.sh:\n#!/usr/bin/env bash\n\nuntil cd /usr/src/app && npm install\ndo\n    echo \"Retrying npm install\"\ndone\ngulp watch\ndocker-compose.yml:\nversion: '2'\n\nservices:\n  build_tools:\n    build: docker/gulp\n    volumes_from:\n      - build_data:rw\n\n  build_data:\n    image: debian:jessie\n    volumes:\n      - .:/usr/src/app\nSo now the container starts a bash script that will continuously loop until it can get into the directory and run npm install. This is still quite brittle, but it works. :)",
    "Benefits of repeated apt cache cleans": "The main reason people do this is to minimise the amount of data stored in that particular docker layer. When pulling a docker image, you have to pull the entire content of the layer.\nFor example, imagine the following two layers in the image:\nRUN apt-get update\nRUN rm -rf /var/lib/apt/lists/*\nThe first RUN command results in a layer containing the lists, which will ALWAYS be pulled by anyone using your image, even though the next command removes those files (so they're not accessible). Ultimately those extra files are just a waste of space and time.\nOn the other hand,\nRUN apt-get update && rm -rf /var/lib/apt/lists/*\nDoing it within a single layer, those lists are deleted before the layer is finished, so they are never pushed or pulled as part of the image.\nSo, why have multiple layers which use apt-get install? This is likely so that people can make better use of layers in other images, as Docker will share layers between images if they're identical in order to save space on the server and speed up builds and pulls.",
    "How to enable/disable buildkit in docker?": "You must adjust the Docker Engine's daemon settings, stored in the daemon.json, and restart the engine. As @Zeitounator suggests, you should be able to temporarily disable the buildkit with DOCKER_BUILDKIT=0 docker build .. Docker CLI will parse that environment variable and should honor it as that checking is done here in the docker/cli source code.\nTo adjust the Docker daemon's buildkit settings, you can follow the instructions below.\nFrom these docs. Partially on the command line, you can do that this way in Powershell:\nOpen the file, on the command line the easiest way to do this is:\nnotepad \"$env:USERPROFILE\\.docker\\daemon.json\"\nChange the value of \"buildkit\" to false so it looks like this:\n{\n  \"registry-mirrors\": [],\n  \"insecure-registries\": [],\n  \"debug\": true,\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": false\n  }\n}\nRestart the Docker service:\nRestart-Service *docker*\nAlternatively, on Docker Desktop for Windows app:\nOpen the Dashboard > Settings:\nSelect Docker Engine and edit the json \"features\" field to read false if it's not already:",
    "How to build a docker container for a Java application": "The docker registry hub has a Maven image that can be used to create java containers.\nUsing this approach the build machine does not need to have either Java or Maven pre-installed, Docker controls the entire build process.\nExample\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 pom.xml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main\n    \u2502   \u251c\u2500\u2500 java\n    \u2502   \u2502   \u2514\u2500\u2500 org\n    \u2502   \u2502       \u2514\u2500\u2500 demo\n    \u2502   \u2502           \u2514\u2500\u2500 App.java\n    \u2502   \u2514\u2500\u2500 resources\n    \u2502       \u2514\u2500\u2500 log4j.properties\n    \u2514\u2500\u2500 test\n        \u2514\u2500\u2500 java\n            \u2514\u2500\u2500 org\n                \u2514\u2500\u2500 demo\n                    \u2514\u2500\u2500 AppTest.java\nImage is built as follows:\ndocker build -t my-maven .\nAnd run as follows:\n$ docker run -it --rm my-maven\n0    [main] INFO  org.demo.App  - hello world\nDockerfile\nFROM maven:3.3-jdk-8-onbuild\nCMD [\"java\",\"-jar\",\"/usr/src/app/target/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\"]\nUpdate\nIf you wanted to optimize your image to exclude the source you could create a Dockerfile that only includes the built jar:\nFROM java:8\nADD target/demo-1.0-SNAPSHOT-jar-with-dependencies.jar /opt/demo/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\nCMD [\"java\",\"-jar\",\"/opt/demo/demo-1.0-SNAPSHOT-jar-with-dependencies.jar\"]\nAnd build the image in two steps:\ndocker run -it --rm -w /opt/maven \\\n   -v $PWD:/opt/maven \\\n   -v $HOME/.m2:/root/.m2 \\\n   maven:3.3-jdk-8 \\\n   mvn clean install\n\ndocker build -t my-app .\n__\nUpdate (2017-07-27)\nDocker now has a multi-stage build capability. This enables Docker to build an image containing the build tools but only the runtime dependencies.\nThe following example demonstrates this concept, note how the jar is copied from target directory of the first build phase\nFROM maven:3.3-jdk-8-onbuild \n\nFROM java:8\nCOPY --from=0 /usr/src/app/target/demo-1.0-SNAPSHOT.jar /opt/demo.jar\nCMD [\"java\",\"-jar\",\"/opt/demo.jar\"]",
    "`docker pull` returns `denied: access forbidden` from private gitlab registry": "If this is an authenticated registry, then you need to run docker login <registryurl> on the machine where you are building this.\nThis only needs to be done once per host. The command then caches the auth in a file\n$ cat ~/.docker/config.json\n{\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"......=\"\n        }\n    }\n}",
    "How to install packages with miniconda in Dockerfile?": "This will work using ARG and ENV:\nFROM ubuntu:18.04\n\nENV PATH=\"/root/miniconda3/bin:${PATH}\"\nARG PATH=\"/root/miniconda3/bin:${PATH}\"\n\n# Install wget to fetch Miniconda\nRUN apt-get update && \\\n    apt-get install -y wget && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Install Miniconda on x86 or ARM platforms\nRUN arch=$(uname -m) && \\\n    if [ \"$arch\" = \"x86_64\" ]; then \\\n    MINICONDA_URL=\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\"; \\\n    elif [ \"$arch\" = \"aarch64\" ]; then \\\n    MINICONDA_URL=\"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh\"; \\\n    else \\\n    echo \"Unsupported architecture: $arch\"; \\\n    exit 1; \\\n    fi && \\\n    wget $MINICONDA_URL -O miniconda.sh && \\\n    mkdir -p /root/.conda && \\\n    bash miniconda.sh -b -p /root/miniconda3 && \\\n    rm -f miniconda.sh\n\nRUN conda --version",
    "Docker build: failed to fetch oauth token for openjdk?": "It looks like you have BuildKit enabled in your docker configuration. BuildKit can cause these type of problems. Please try it again with BuildKit disabled.\nIn Linux, using environment variables:\nexport DOCKER_BUILDKIT=0\nexport COMPOSE_DOCKER_CLI_BUILD=0\nIn Windows and macOS, start the Docker Desktop application, go to Settings, select Docker Engine and look for the existing entry:\n\"buildkit\": true\nChange this entry to disable buildkit:\n\"buildkit\": false\nThen click on Apply & Restart and try it again.",
    "Error: \"error creating aufs mount to\" when building dockerfile": "I had some unresolved errors after removing /var/lib/docker/aufs, which a couple extra steps cleared up.\nTo add to @benwalther answer, since I lack the reputation to comment:\n# Cleaning up through docker avoids these errors\n#   ERROR: Service 'master' failed to build:\n#     open /var/lib/docker/aufs/layers/<container_id>: no such file or directory\n#   ERROR: Service 'master' failed to build: failed to register layer:\n#     open /var/lib/docker/aufs/layers/<container_id>: no such file or directory\ndocker rm -f $(docker ps -a -q)\ndocker rmi -f $(docker images -a -q)\n\n# As per @BenWalther's answer above\nsudo service docker stop\nsudo rm -rf /var/lib/docker/aufs\n\n# Removing the linkgraph.db fixed this error:\n#   Conflict. The name \"/jenkins_data_1\" is already in use by container <container_id>.\n#   You have to remove (or rename) that container to be able to reuse that name.\nsudo rm -f /var/lib/docker/linkgraph.db\n\n\nsudo service docker start",
    "/bin/sh: 1: sudo: not found when running dockerfile": "by default docker container runs as root user\nremove the sudo from Dockerfile and run again.",
    "Syntax highlighting for Dockerfile in Sublime Text?": "Of course you can, by installing this package from Package Control:\nDockerfile Syntax Highlighting, https://packagecontrol.io/packages/Dockerfile%20Syntax%20Highlighting",
    "Can I run an intermediate layer of a Docker image?": "You can run an intermediate image of a Docker layer, which is probably what you want.\nDuring your build you might need to inspect an image at certain point (step) in the build process e.g. in your Docker build output you'll see:\nStep 17/30 : RUN rm -rf /some/directory\n---> Running in 5ab963f2b48d\nWhere 5ab963f2b48d is an image ID, and when you list your images, you'd see that ID in the list e.g.\n$ docker image ls --all\nREPOSITORY    TAG      IMAGE ID       CREATED        SIZE\n<none>        <none>   5ab963f2b48d   7 minutes ago  1.18GB\nTo run that image (with a terminal) for example, you can simply:\ndocker run -i -t 5ab963f2b48d /bin/bash\nAlso see: Run a Docker Image as a Container",
    "Dockerfile and docker-compose not updating with new instructions": "It seems that when using the docker-compose command it saves an intermediate container that it doesnt show you and constantly reruns that never updating it correctly. Sadly the documentation regarding something like this is poor. The way to fix this is to build it first with no cache and then up it like so\ndocker-compose build --no-cache\ndocker-compose up -d",
    "Cannot \"pip install cryptography\" in Docker Alpine Linux 3.3 with OpenSSL 1.0.2g and Python 2.7": "For those who are still experiencing problems installing cryptography==2.1.4 in Alpine 3.7 like this:\nwriting manifest file 'src/cryptography.egg-info/SOURCES.txt'\nrunning build_ext\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_padding.c'\ncreating build/temp.linux-x86_64-2.7\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_constant_time.c'\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_openssl.c'\nbuilding '_openssl' extension\ncreating build/temp.linux-x86_64-2.7/build\ncreating build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7\ngcc -fno-strict-aliasing -Os -fomit-frame-pointer -g -DNDEBUG -Os -fomit-frame-pointer -g -DTHREAD_STACK_SIZE=0x100000 -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_openssl.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_openssl.o -Wconversion -Wno-error=sign-conversion\nbuild/temp.linux-x86_64-2.7/_openssl.c:493:30: fatal error: openssl/opensslv.h: No such file or directory\n #include <openssl/opensslv.h>\n                              ^\ncompilation terminated.\nerror: command 'gcc' failed with exit status 1\nSolution\nInstall these dependencies in the Alpine container:\n$ apk add --no-cache libressl-dev musl-dev libffi-dev\nTo install these dependencies using a Dockerfile:\nRUN apk add --no-cache \\\n        libressl-dev \\\n        musl-dev \\\n        libffi-dev && \\\n    pip install --no-cache-dir cryptography==2.1.4 && \\\n    apk del \\\n        libressl-dev \\\n        musl-dev \\\n        libffi-dev\nReference\nInstallation instructions for cryptography on Alpine can be found here:\nhttps://cryptography.io/en/latest/installation/#building-cryptography-on-linux\nA version from the time of writing is available on github\nHere is the relevant portion:\nBuilding cryptography on Linux\n[skipping over the part for non-Alpine Linux] \u2026\n$ pip install cryptography\nIf you are on Alpine or just want to compile it yourself then cryptography requires a compiler, headers for Python (if you're not using pypy), and headers for the OpenSSL and libffi libraries available on your system.\nAlpine\nReplace python3-dev with python-dev if you're using Python 2.\n$ sudo apk add gcc musl-dev python3-dev libffi-dev openssl-dev\nIf you get an error with openssl-dev you may have to use libressl-dev.",
    "How can I install lxml in docker": "I added RUN apk add --update --no-cache g++ gcc libxslt-dev before RUN pip install -r requirements.txt and it worked.",
    "Multiple images, one Dockerfile": "You can use a docker-compose file using the target option:\nversion: '3.4'\nservices:\n  img1:\n    build:\n      context: .\n      target: img1\n  img2:\n    build:\n      context: .\n      target: img2\nusing your Dockerfile with the following content:\nFROM alpine as img1\nCOPY file1.txt .\n\nFROM alpine as img2\nCOPY file2.txt .",
    "SIGTERM not received by java process using 'docker stop' and the official java image": "Assuming you launch a Java service by defining the following in your Dockerfile:\nCMD java -jar ...\nWhen you now enter the container and list the processes e.g. by docker exec -it <containerName> ps AHf (I did not try that with the java but with the ubuntu image) you see that your Java process is not the root process (not the process with PID 1) but a child process of a /bin/sh process:\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 18:27 ?        00:00:00 /bin/sh -c java -jar ...\nroot         8     1  0 18:27 ?        00:00:00   java -jar ...\nSo basically you have a Linux shell that is the main process with PID 1 which has a child process (Java) with PID 8.\nTo get signal handling working properly you should avoid those shell parent process. That can be done by using the builtin shell command exec. That will make the child process taking over the parent process. So at the end the former parent process does not exist any more. And the child process becomes the process with the PID 1. Try the following in your Dockerfile:\nCMD exec java -jar ...\nThe process listing then should show something like:\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 18:30 ?        00:00:00 java -jar ...\nNow you only have that one process with PID 1. Generally a good practice is to have docker containers only contain one process - the one with PID 1 (or if you really need more processes then you should use e.g. supervisord as PID 1 which itself takes care of signal handling for its child processes).\nWith that setup the SIGTERM will be treated directly by the Java process. There is no shell process any more in between which could break signal handling.\nEDIT:\nThe same exec effect could be achieved by using a different CMD syntax that does it implicitly (thanks to Andy for his comment):\nCMD [\"java\", \"-jar\", \"...\"]",
    "Docker: Using COPY when the Dockerfile is located in a subdirectory": "All you need to do here is add context: . and dockerfile in your build section inside your docker-compose.yml file so that your service understands the complete directory structure.\n# docker-compose.yml\nversion: \"3\"\nservices:\n  webserver:\n    build:\n      context: .\n      dockerfile: ./dockerfiles/webserver/Dockerfile\n    image: webserver:php-apache",
    "How can I start spring boot application in docker with profile?": "We have 3 ways:\n1. Passing Spring Profile in a Dockerfile\nFROM openjdk:8-jre-alpine\n...\nENTRYPOINT [\"java\", \"-Djava.security.egd=file:/dev/./urandom\",\"-Dspring.profiles.active=test\",\"-jar\",\"app.jar\"]\n2. Passing Spring Profile in Docker run\ndocker run -d -p 8080:8080 -e \"SPRING_PROFILES_ACTIVE=test\" --name my-app:latest\n3. Passing Spring Profile in DockerCompose\nversion: \"3.5\"\nservices:\n  my-app:\n     image: my-app:latest\n     ports:\n       - \"8080:8080\" \n     environment:\n       - \"SPRING_PROFILES_ACTIVE=test\"",
    "Error in anyjson setup command: use_2to3 is invalid": "Downgrading setuptools worked for me\npip install \"setuptools<58.0.0\"\nAnd then\npip install django-celery",
    "How to specify working directory for ENTRYPOINT in Dockerfile": "WORKDIR /App is a command you can use in your dockerfile to change the working directory.",
    "Why is ARG in a DOCKERFILE not recommended for passing secrets?": "Update August 2018:\nYou now have docker build --secret id=mysecret,src=/secret/file.\nSee \"safe way to use build-time argument in Docker\".\nUpdate January 2017:\nDocker (swarm) 1.13 has docker secret.\nHowever, as commented by Steve Hoffman (bacoboy):\n[...]The secret command only helps swarm users is not a more general solution (like they did with attaching persistent volumes).\nHow you manage your secrets (what they are and who has access to them) is very system dependent and depends on which bits of paid and/or OSS you cobble together to make your \"platform\".\nWith Docker the company moving into providing a platform, I'm not surprised that their first implementation is swarm based just as Hashicorp is integrating Vault into Atlas -- it makes sense.\nReally how the secrets are passed falls outside the space of docker run.\nAWS does this kind of thing with roles and policies to grant/deny permissions plus an SDK.\nChef does it using encrypted databags and crypto \"bootstrapping\" to auth.\nK8S has their own version of what just got released in 1.13.\nI'm sure mesos will add a similar implementation in time.\nThese implementations seem to fall into 2 camps.\npass the secret via volume mount that the \"platform\" provides or (chef/docker secret/k8s\npass credentials to talk to an external service to get things at boot (iam/credstash/etc)\nOriginal answer: Nov. 2015\nThis was introduced in commit 54240f8 (docker 1.9, Nov 2015), from PR 15182,\nThe build environment is prepended to the intermediate continer's command string for aiding cache lookups.\nIt also helps with build traceability. But this also makes the feature less secure from point of view of passing build time secrets.\nissue 13490 reiterates:\nBuild-time environment variables: The build-time environment variables were not designed to handle secrets. By lack of other options, people are planning to use them for this. To prevent giving the impression that they are suitable for secrets, it's been decided to deliberately not encrypt those variables in the process.\nAs mentioned in 9176 comments:\nenv variables are the wrong way to pass secrets around. We shouldn't be trying to reinvent the wheel and provide a crippled security distribution mechanism right out of the box.\nWhen you store your secret keys in the environment, you are prone to accidentally expose them -- exactly what we want to avoid:\nGiven that the environment is implicitly available to the process, it's incredibly hard, if not impossible, to track access and how the contents get exposed\nIt is incredibly common having applications grabbing the whole environment and print it out, since it can be useful for debugging, or even send it as part of an error report. So many secrets get leaked to PagerDuty that they have a well-greased internal process to scrub them from their infrastructure.\nEnvironment variables are passed down to child processes, which allows unintended access and breaks the principle of least privilege. Imagine that as part of your application you call to third-party tool to perform some action, all of a sudden that third-party tool has access to your environment, and god knows what it will do with it.\nIt is very common for applications that crash to store the environment variables in log-files for later debugging. This means secrets in plain-text on disk.\nPutting secrets in env variables quickly turns into tribal knowledge. New engineers don't know they are there, and are not aware they should be careful when handling environment variables (filtering them to sub-processes, etc).\nOverall, secrets in env variables break the principle of least surprise, are a bad practice and will lead to the eventual leak of secrets.",
    "Dockerfile FROM --platform option": "update your docker file, you are missing =\nARG arch\nFROM --platform=linux/${arch} bounz/hgbe.base",
    "Difference between VOLUME declaration in Dockerfile and -v as docker run parameter": "The -v parameter and VOLUME keyword are almost the same. You can use -v to have the same behavior as VOLUME.\ndocker run -v /data\nSame as\nVOLUME /data\nBut also -v have more uses, one of them is where map to the volume:\ndocker run -v data:/data # Named volumes\ndocker run -v /var/data:/data # Host mounted volumes, this is what you refer to -v use, but as you can see there are more uses,\nSo the question is: what is the use of VOLUME in a Dockerfile?\nThe container filesystem is made of layers so writing there, is slower and limited (because the fixed number of layers) than the plain filesystem.\nYou declare VOLUME in your Dockerfile to denote where your container will write application data. For example a database container, its data will go in a volume regardless what you put in your docker run.\nIf you create a docker container for JBoss and you want to use fast filesystem access with libaio yo need to declare the data directory as a VOLUME or JBoss will crash on startup.\nIn summary VOLUME declares a volume regardless what you do in docker run. In fact in docker run you cannot undo a VOLUME declaration made in Dockerfile.\nRegards",
    "Docker image layer: What does `ADD file:<some_hash> in /` mean?": "That Docker Hub history view doesn't show the actual Dockerfile; instead, it shows content essentially extracted from the docker history of the image. That doesn't preserve the specific details you're looking for: it doesn't remember the names of base images, or the build-context file names of things that get ADDed or COPYed in.\nChasing through GitHub and Docker Hub links, the golang:*-buster Dockerfile is built FROM buildpack-deps:...-scm; buildpack-deps:buster-scm is FROM buildpack-deps:buster-curl; that is FROM debian:buster; and that has a very simple Dockerfile (quoted here in its entirety):\nFROM scratch\nADD rootfs.tar.xz /\nCMD [\"bash\"]\nFROM scratch starts from a completely totally empty image; that is the base of the Docker image tree (and what tells docker history and similar tools to stop). The ADD line unpacks a tar file of a Debian system image.\nIf you look at docker history or the Docker Hub history view you cite, you should be able to see these same steps happening. The ADD file:4b0... in / corresponds to the ADD rootfs.tar.gz /, and the second line is the CMD [\"bash\"]. It is not split up by Dockerfile or image, and the original filenames from ADD aren't saved. (You couldn't reproduce the image anyways without the contents of the rootfs.tar.gz, so it's merely slightly helpful to know its filename but not essential.)\nThe ADD file:hash in /path syntax is not standard Dockerfile syntax (the word in in particular is not part of it). I'm not sure there's a reliable way to translate from the host file or URL to the hash, but building the image and looking at its docker history would tell you (assuming you've got a perfect match for the file metadata). There's no way to get back to the original filename or syntax, and definitely no way to get back to the file contents.",
    "Container command '/start.sh' not found or does not exist, entrypoint to container is shell script": "On windows, while building the docker image, i also used to get the same error after building the image, that shell script is missing.. the path and the shebang was also correct.\nLater on, i read some where that it may be due to the encoding issue. I just opened the file in sublime editor and then..VIEW->Line Endings-> UNIX and just save the file, and rebuilded the image. Everything worked fine.\nI was getting this error, when i was building a image from windows.\nOther Option:\nSometime, we forgot to manually change the line format. So,what we can do is add this Run statement before the EntryPoint in dockerfile. It will encode the file in LF format.\n RUN sed -i 's/\\r$//' $app/filename.sh  && \\  \n        chmod +x $app/filename.sh\n\nENTRYPOINT $app/filename.sh",
    "Redis Docker connection refused": "You need to provide more information about your environment (OS, Docker installation, etc), but basically, if you start your Redis container like this:\ndocker run --name=redis-devel --publish=6379:6379 --hostname=redis --restart=on-failure --detach redis:latest\nIt should expose the port no matter what. The only reason you might not be able to connect to it, is if you've messed up your bridge interface, if you're on Linux, or you're using a docker machine with its own network interface and IP address and you're not connecting to that IP address. If you're using Docker for Mac, then that only supports routing to the localhost address, since bridging on Mac hosts doesn't work yet.\nAnyway, on MacOS with Docker for Mac (not the old Docker Toolbox), the following should be enough to get your started:\n\u279c  ~ docker run --name=redis-devel --publish=6379:6379 --hostname=redis --restart=on-failure --detach redis:latest\n6bfc6250cc505f82b56a405c44791f193ec5b53469f1625b289ef8a5d7d3b61e\n\u279c  ~ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES\n6bfc6250cc50        redis:latest        \"docker-entrypoint.s\u2026\"   10 minutes ago      Up 10 minutes       0.0.0.0:6379->6379/tcp   redis-devel\n\u279c  ~ redis-cli ping\nPONG\n\u279c  ~ ",
    "How to fill user input for interactive command for \"RUN\" command?": "Did you try to disable it?\nFROM ubuntu:19.04\n\nENV DEBIAN_FRONTEND noninteractive\n\nRUN apt update && apt install -y tcl",
    "What is the purpose of the Docker build context?": "TL;DR: \"because the client and daemon may not even run on the same machine\"\nThe docker command is the docker client of the dockerd that is the service that can run directly in your PC (linux) or under a Linux VM under OSX or Windows.\nQ: What is the purpose of the Docker build context?\nFrom here:\nWould probably be good to also mention that this has to happen this way because the client and daemon may not even run on the same machine, so without this \"context\" the daemon machine wouldn't have any other way to get files for ADD or otherwise\nQ: If the build process compresses the current directory contents and sends it to the daemon, where does it go?\nThe docker daemon receives the compressed directory and process it on the fly; does not matter where it is stored in that moment.\nQ: Why doesn't it make that content available for use in the image?\nThink this: How can docker know where do you want to put each file/directory in the target image? With COPY/ADD directives you can control where put each one. The case that you've mentioned is only a trivial example where you have a single directory and a single target.",
    "What does working_dir tag mean in a docker-compose yml file": "working_dir sets the working directory of the container that is created. It is the same as the --workdir flag to docker run.",
    "Load Postgres dump after docker-compose up": "Reading https://hub.docker.com/_/postgres/, the section 'Extend this image' explains that any .sql in /docker-entrypoint-initdb.d will be executed after build.\nI just needed to change my Dockerfile.db to:\nFROM postgres\n\nADD ./devops/db/dummy_dump.sql /docker-entrypoint-initdb.d\nAnd it works!",
    "How to build Docker Images with Dockerfile behind HTTP_PROXY by Jenkins?": "Note: Docker 1.9 might help solve this:\n\"Issue 14634\": Builder - Build-time argument passing (e.g., HTTP_PROXY)\n\"PR 15182\": Support for passing build-time variables in build context\nUsage (proposed):\ndocker build --build-arg http_proxy=http://my.proxy.url  --build-arg foo=bar <<MARK\nFROM busybox\nRUN <command that need http_proxy>\nARG --description=\"foo's description\" foo\nUSER $foo\nMARK",
    "How to run my python script on docker?": "Going by question title, and if one doesn't want to create docker image but just want to run a script using standard python docker images, it can run using below command\ndocker run -it --rm --name my-running-script -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp python:3.7-alpine python script_to_run.py",
    "Dockerfile define multiple ARG arguments in a single line": "YES, After release 1.2.0\nPreviously it was not possible and it threw the following error\nARG requires exactly one argument definition\nBut after release 1.2.0 it is now possible to define ARG like following\nARG CDN_ENDPOINT \\\nAWS_S3_BUCKET",
    "What is the most light weight base image I can use to build a Dockerfile?": "This really depends on your requirements:\nFROM scratch: if you are able to statically compile your application and don't need any other binaries (libraries, shells, or any other command period), then you can use the completely empty \"scratch\". You'll see this used as the starting point for the other base images, and it's also found in a lot of pre-compiled Go commands.\nDistroless: these images are built for a handful of use cases, and ship without a package manager or even shell (excluding their developer images). If you fit in their specific use case, these can be very small, but like with scratch images, difficult to debug.\nBusybox: I consider this less of a base image and more of a convenient utility container. You get a lot of common commands in a very small size. Busybox is a single binary with various commands linked to it, and that binary implements each of the commands depending on the CLI. What you don't get is the general package manager to easily install other components.\nAlpine: This is a minimal distribution, based on busybox, but with the apk package manager. The small size comes at a cost, things like glibc are not included, preferring the musl libc implementation instead. You will find that many of the official images are based on Alpine, so inside of the container ecosystem, this is a very popular option.\nDebian, Ubuntu, and CentOS: These are less of the lightweight base images. But what they lose with size they gain with a large collection of packages you can pull from and lots of people that are testing, fixing bugs, and contributing to things upstream. They also come with a collection of libraries that some applications may expect to be preinstalled.\nWhile that last option is a bit larger, keep in mind that base images should only be pushed over the wire and stored on disk once. After that, unless you change them, any images built on top of them only need to send the manifest that references layers in that base image and the docker engine will see that it already has those layers downloaded. And with the union fs, those layers never need to be copied even if you run 100 containers all pointing back to that image, they each use the same read-only layer on disk for all the image layers and write their changes to the their container specific RW layer.\nIf you find yourself installing a set of common tools on many of your images, the better option is to build your own base image, extending an upstream base image with your common tools. That way those tools only get packaged into a layer once and reused by multiple images.",
    "SSH agent forwarding during docker build": "For Docker 18.09 and newer\nYou can use new features of Docker to forward your existing SSH agent connection or a key to the builder. This enables for example to clone your private repositories during build.\nSteps:\nFirst set environment variable to use new BuildKit\nexport DOCKER_BUILDKIT=1\nThen create Dockerfile with new (experimental) syntax:\n# syntax=docker/dockerfile:experimental\n\nFROM alpine\n\n# install ssh client and git\nRUN apk add --no-cache openssh-client git\n\n# download public key for github.com\nRUN mkdir -p -m 0600 ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts\n\n# clone our private repository\nRUN --mount=type=ssh git clone git@github.com:myorg/myproject.git myproject\nAnd build image with\ndocker build --ssh default .\nRead more about it here: https://medium.com/@tonistiigi/build-secrets-and-ssh-forwarding-in-docker-18-09-ae8161d066",
    "How do I declare multiple maintainers in my Dockerfile?": "You can only specify one MAINTAINER instruction in a Dockerfile.\nFurthermore, MAINTAINER will be deprecated in the upcoming 1.13.0 release, see deprecations and this pull request.\nThe recommended solution is to use LABEL instead, e.g.\nLABEL authors=\"first author,second author\"\nLabels have a key=value syntax. This means you cannot assign the same label more than once and you cannot assign multiple values to a given label. But you can combine multiple values into one with a syntax of your choice as illustrated in the example..",
    "where should I put docker-compose.yml": "I'm asking myself the same question: Where to put my docker-compose.yml file?\nI've decided to do it the following way:\nOne Repo for the docker-compose.yml along with deployment scripts (Jenkins pipeline in my case).\nOne Repo per micro service along with a Dockerfile and its build logic (built image is pushed to private docker registry).\nWhy?\nThe docker-compose.yml describes a system (the composition of micro services) and how it is deployed.\nA micro services should be independent from the system infrastructure.\nThere could be more than one system (different composition of the micro services) with a second docker-compose.yml (and Repo).\nHowever, of course there are reasonable exceptions. Assume you're deploying a customized Database along with a management tool (e.g. customized mariadb and adminer), then most things may live in one repository.",
    "installing `lightdm` in Dockerfile raises interactive keyboard layout menu": "This should work:\nDEBIAN_FRONTEND=noninteractive apt-get install lightdm -y\nI had the same problem installing CUDA and this fixed it.",
    "Docker and .bash_history": "It is the example from the documentation about volume: Mount a host file as a data volume:\ndocker run --rm -it -v ~/.bash_history:/root/.bash_history ubuntu /bin/bash\nThis will drop you into a bash shell in a new container, you will have your bash history from the host and when you exit the container, the host will have the history of the commands typed while in the container.",
    "\"Empty continuation lines will become errors\"\u2026 how should I comment my Dockerfile now?": "On top of what others have said above (the error might be related to comments inside continuation blocks and/or windows cr/lf characters = use dos2unix), this message can also show up when your last command ends with a backslash \\ character. For example, if you have this:\nRUN apt-get update \\\n    && apt-get upgrade \\\n    && apt-get -y install build-essential curl gnupg libfontconfig ca-certificates bzip2 \\\n    && curl -sL https://deb.nodesource.com/setup_16.x  | bash - \\\n    && apt-get -y install nodejs \\\n    && apt-get clean \\\n    && rm -rf /tmp/* /var/lib/apt/lists/* \\\nNotice the last \\ at the end. This will get you the same error:\ndocker [WARNING]: Empty continuation line found in:\nSo, just remove that last \\ and you're all set.",
    "Command line arguments to Docker CMD": "Use ENTRYPOINT for stuff like this. Any CMD parameters are appended to the given ENTRYPOINT.\nSo, if you update the Dockerfile to:\nFROM ubuntu:15.04\nENTRYPOINT [\"/bin/bash\", \"-c\", \"cat\"]\nThings should work as you wish.\nAlso, as you don't need the $1, you should be able to change it to:\nFROM ubuntu:15.04\nENTRYPOINT [\"/bin/cat\"]\nI haven't tested any of this, so let me know if it doesn't work.",
    "How to tag an image in a Dockerfile? [duplicate]": "Unfortunately it is not possible. You can use build.sh script, which contains like this:\n#!/usr/bin/env bash\nif [ $# -eq 0 ]\n  then\n    tag='latest'\n  else\n    tag=$1\nfi\n\ndocker build -t project:$tag .\nRun ./build.sh for creating image project:latest or run ./build.sh your_tag to specify image tag.",
    "How to measure Docker build steps duration?": "BuildKit, which was experimental in 18.06 and generally available in 18.09, has this functionality built in. To configure the dockerd daemon with experimental mode, you can setup the daemon.json:\n$ cat /etc/docker/daemon.json\n{\n  \"experimental\": true\n}\nThen you can enable BuildKit from the client side with an environment variable:\n$ export DOCKER_BUILDKIT=1\n$ docker build -t java-test:latest .\n[+] Building 421.6s (13/13) FINISHED\n => local://context (.dockerignore)                                                                           1.6s\n => => transferring context: 56B                                                                              0.3s\n => local://dockerfile (Dockerfile)                                                                           2.0s\n => => transferring dockerfile: 895B                                                                          0.4s\n => CACHED docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55bac26b655d2cfb751f8b9412d6f  0.1s\n => docker-image://docker.io/library/openjdk:8-jdk-alpine                                                     1.0s\n => => resolve docker.io/library/openjdk:8-jdk-alpine                                                         0.0s\n => local://context                                                                                           1.7s\n => => transferring context: 6.20kB                                                                           0.4s\n => docker-image://docker.io/library/openjdk:8-jre-alpine                                                     1.3s\n => => resolve docker.io/library/openjdk:8-jre-alpine                                                         0.0s\n => /bin/sh -c apk add --no-cache maven                                                                      61.0s\n => copy /src-0/pom.xml java/pom.xml                                                                          1.3s\n => /bin/sh -c mvn dependency:go-offline                                                                    339.4s\n => copy /src-0 java                                                                                          0.9s\n => /bin/sh -c mvn package -Dmaven.test.skip=true                                                            10.2s\n => copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar                                                  0.8s\n => exporting to image                                                                                        1.2s\n => => exporting layers                                                                                       1.0s\n => => writing image sha256:d57028743ca10bb4d0527a294d5c83dd941aeb1033d4fe08949a135677846179                  0.1s\n => => naming to docker.io/library/java-test:latest                                                           0.1s\nThere's also an option to disable the tty console output which generates output more suitable for scripting with each section having a start, stop, and duration:\n$ docker build -t java-test:latest --progress plain .                                                                                                                         \n\n#1 local://dockerfile (Dockerfile)                                                      \n#1       digest: sha256:da721b637ea85add6e26070a48520675cefc2bed947c626f392be9890236d11b\n#1         name: \"local://dockerfile (Dockerfile)\"      \n#1      started: 2018-09-05 19:30:53.899809093 +0000 UTC\n#1    completed: 2018-09-05 19:30:53.899903348 +0000 UTC\n#1     duration: 94.255\u00b5s\n#1      started: 2018-09-05 19:30:53.900069076 +0000 UTC\n#1 transferring dockerfile: 38B done\n#2 ...              \n\n#2 local://context (.dockerignore)  \n#2       digest: sha256:cbf55954659905f4d7bd2fc3e5e52d566055eecd94fd7503565315022d834c21\n#2         name: \"local://context (.dockerignore)\"       \n#2      started: 2018-09-05 19:30:53.899624016 +0000 UTC\n#2    completed: 2018-09-05 19:30:53.899695455 +0000 UTC\n#2     duration: 71.439\u00b5s\n#2      started: 2018-09-05 19:30:53.899839335 +0000 UTC\n#2    completed: 2018-09-05 19:30:54.359527504 +0000 UTC\n#2     duration: 459.688169ms                                                            \n#2 transferring context: 34B done                                \n\n\n#1 local://dockerfile (Dockerfile)\n#1    completed: 2018-09-05 19:30:54.592304408 +0000 UTC\n#1     duration: 692.235332ms\n\n\n#3 docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55ba...           \n#3       digest: sha256:39386c91e9f27ee70b2eefdee12fc8a029bf5edac621b91eb5f3e6001d41dd4f\n#3         name: \"docker-image://docker.io/tonistiigi/copy:v0.1.3@sha256:e57a3b4d6240f55bac26b655d2cfb751f8b9412d6f7bb1f787e946391fb4b21b\"\n#3      started: 2018-09-05 19:30:54.731749377 +0000 UTC \n#3    completed: 2018-09-05 19:30:54.732013326 +0000 UTC\n#3     duration: 263.949\u00b5s\n\n\n#5 docker-image://docker.io/library/openjdk:8-jdk-alpine\n#5       digest: sha256:d680c6a82813d080081fbc3c024d21ddfa7ff995981cc7b4bfafe55edf80a319\n#5         name: \"docker-image://docker.io/library/openjdk:8-jdk-alpine\"\n#5      started: 2018-09-05 19:30:54.731483638 +0000 UTC\n#5    completed: 2018-09-05 19:30:54.732480345 +0000 UTC\n#5     duration: 996.707\u00b5s\n\n\n#4 docker-image://docker.io/library/openjdk:8-jre-alpine\n#4       digest: sha256:9ed31df4e6731a1718ea93bfa77354ad1ea2d1625c1cb16e2087d16d0b84bd00\n#4         name: \"docker-image://docker.io/library/openjdk:8-jre-alpine\"                \n#4      started: 2018-09-05 19:30:54.73176516 +0000 UTC\n#4    completed: 2018-09-05 19:30:54.732603067 +0000 UTC\n#4     duration: 837.907\u00b5s                              \n\n\n#7 local://context\n#7       digest: sha256:efe765161a29e2bf7a41439cd2e6656fcf6fa6bc97da825ac9b5a0d8adecf1ac\n#7         name: \"local://context\"\n#7      started: 2018-09-05 19:30:54.73178732 +0000 UTC\n#7    completed: 2018-09-05 19:30:54.731880943 +0000 UTC\n#7     duration: 93.623\u00b5s\n#7      started: 2018-09-05 19:30:54.792740019 +0000 UTC\n#7 transferring context: 473B done\n#7    completed: 2018-09-05 19:30:55.059008345 +0000 UTC\n#7     duration: 266.268326ms\n\n\n#9 /bin/sh -c mvn dependency:go-offline\n#9       digest: sha256:2197672cd7a44d93e0dba40aa00d7ef41f8680226d91f469d1c925646bdc8d6d\n#9         name: \"/bin/sh -c mvn dependency:go-offline\"\n#9      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#9    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#9     duration: 0s\n#9       cached: true\n\n\n#10 copy /src-0 java\n#10       digest: sha256:36cf252c34be098731bd8c5fb3f273f9c1437a5f74a65a3555d71150c2092fa7\n#10         name: \"copy /src-0 java\"\n#10      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#10    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#10     duration: 0s\n#10       cached: true\n\n#11 /bin/sh -c mvn package -Dmaven.test.skip=true\n#11       digest: sha256:390464b1fdc7a4c833b3476033d95b7714e22bcbfd018469e97b04781cb41532\n#11         name: \"/bin/sh -c mvn package -Dmaven.test.skip=true\"\n#11      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#11    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#11     duration: 0s\n#11       cached: true\n\n\n#12 copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar\n#12       digest: sha256:a7d60191a720f80de72a77ebe0d4bd1b0fd55d44e623661e80916b7fd1952076\n#12         name: \"copy /src-0/gs-spring-boot-docker-0.1.0.jar java/app.jar\"\n#12      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#12    completed: 2018-09-05 19:30:55.203555216 +0000 UTC\n#12     duration: 106.069\u00b5s\n#12       cached: true\n\n\n#6 /bin/sh -c apk add --no-cache maven\n#6       digest: sha256:db505db5e418f195c7bad3a710ad40bec3d91d47ff11a6f464b3ae37af744e7d\n#6         name: \"/bin/sh -c apk add --no-cache maven\"\n#6      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#6    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#6     duration: 0s\n#6       cached: true\n\n\n#8 copy /src-0/pom.xml java/pom.xml\n#8       digest: sha256:f032d4ff111c6ab0efef1a4e37d2467fffe43f48a529b8d56291ec81f96296ab\n#8         name: \"copy /src-0/pom.xml java/pom.xml\"\n#8      started: 2018-09-05 19:30:55.203449147 +0000 UTC\n#8    completed: 2018-09-05 19:30:55.203449147 +0000 UTC\n#8     duration: 0s\n#8       cached: true\n\n\n#13 exporting to image\n#13       digest: sha256:d536dc2895c30fbde898bb4635581350a87c21f3695913ba21850a73d31422d9\n#13         name: \"exporting to image\"\n#13      started: 2018-09-05 19:30:55.203674127 +0000 UTC\n#13 exporting layers done\n#13 writing image sha256:d57028743ca10bb4d0527a294d5c83dd941aeb1033d4fe08949a135677846179 0.1s done\n#13 naming to docker.io/library/java-test:latest\n#13    completed: 2018-09-05 19:30:55.341300051 +0000 UTC\n#13     duration: 137.625924ms\n#13 naming to docker.io/library/java-test:latest 0.0s done",
    "Connect docker python to SQL server with pyodbc": "Need to Run:\nsudo apt-get install gcc\nneed to add a odbcinst.ini file containing:\n[FreeTDS]Description=FreeTDS Driver Driver=/usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so Setup=/usr/lib/x86_64-linux-gnu/odbc/libtdsS.so\nneed to add folowing to docker file\nADD odbcinst.ini /etc/odbcinst.ini\nRUN apt-get update\nRUN apt-get install -y tdsodbc unixodbc-dev\nRUN apt install unixodbc-bin -y\nRUN apt-get clean -y\nneed to change connection in .py to\nconnection = pyodbc.connect('Driver={FreeTDS};'\n                            'Server=xxxxx;'\n                            'Database=DCMM;'\n                            'UID=xxxxx;'\n                            'PWD=xxxxx')\nNow the container compiles, and gets data from SQL server",
    "connecting to local mongodb from docker container": "On Docker for Mac, you can use host.docker.internal if your mongo is running on your localhost. You could have your code read in an env variable for the mongo host and set it in the Dockerfile like so:\nENV MONGO_HOST \"host.docker.internal\"\nSee here for more details on https://docs.docker.com/docker-for-mac/networking/#use-cases-and-workarounds",
    "Can I mount same volume to multiple docker containers": "Yes you can add same location as a volume to many docker containers.\nAdditionally you can use --volumes-from to mount your log directory in one container not actually running any application and then use the volumes from this container in your other containers without having to repeat the paths everywhere.\nWorth a read Docker volumes",
    "Building common dependencies with docker-compose": "It's possible. There's a kind of workaround. You're close, but you were missing explicit image tags (so you had little ability on child images to declare which image you inherited from).\nversion: '3.2'\nservices:\n  base:\n    image: mybaseimage\n    build: ./path-to-base-dockerfile\n  child1:\n    build: ./path-to-child1-dockerfile\n    depends_on:\n      - base\n  child2:\n    build: ./path-to-child2-dockerfile\n    depends_on:\n      - base\nLet's say you have no images built. You run docker-compose up. The following things will happen:\ndocker-compose sees that child1 and child2 services depend on base. So it will deploy base first.\ndocker-compose sees that you have not yet tagged any image as mybaseimage. It knows how to build mybaseimage (you gave it a build path), so it will build it now, and tag it as mybaseimage.\ndocker-compose deploys the base service.\nideally you should design base so that it quits immediately, or has no entrypoint. since we don't actually want it to run this service.\ndocker-compose considers deploying child1 and child2\ndocker-compose sees that you have not yet tagged any image as child1. It knows how to build child1 (you gave it a build path), so it will build it now, and tag it as child1.\ndocker-compose deploys the child1 service\nsame sequence of steps for child2\nThe next docker-compose up will be simpler (we have tagged images available, so we skip all build steps).\nIf you already have tagged images, and want to rebuild: use docker-compose build to tell it to build all images (yes, base and children will both be rebuilt).",
    "How to use Docker's COPY/ADD instructions to copy a single file to an image": "As stated in the Dockerfile documentation:\nIf <src> is any other kind of file [besides a local tar archive], it is copied individually along with its metadata. In this case, if <dest> ends with a trailing slash /, it will be considered a directory and the contents of <src> will be written at <dest>/base(<src>).\nIf <dest> does not end with a trailing slash, it will be considered a regular file and the contents of <src> will be written at <dest>.\nThus, you have to write COPY test.txt /usr/src/app/ with a trailing /.",
    "Commands to execute background process in Docker CMD": "Besides the comments on your question that already pointed out a few things about Docker best practices you could anyway start a background process from within your start.sh script and keep that start.sh script itself in foreground using the nohup command and the ampersand (&). I did not try it with mongod but something like the following in your start.sh script could work:\n#!/bin/sh\n...\nnohup sh -c mongod --dbpath /test &\n...",
    "How to Specify $docker build --network=\"host\" mode in docker-compose at the time of build": "@dkanejs is right, and here is how you use it (the version number is important):\nversion: '3.4'\nservices:\n  my_image:\n    build:\n      context: .\n      network: host",
    ".dockerignore mentioned files are not ignored": "The .dockerignore rules follow the filepath/#Match.\nTry (for testing) Gemfile.lock instead of /Gemfile.lock.\nAnd check that the eol (end of line) characters are unix-style, not Windows style in your .dockerignore file.\nApparently, (docker 1.10, March 2016) using rule starting with / like /xxx ( or /.*) is not well supported.",
    "How to set timezone inside alpine base docker image?": "You need to install the tzdata package and then set the enviroment variable TZ to a timezone. (List with all the timezones)\nFROM alpine:latest\nRUN apk add --no-cache tzdata\nENV TZ=Europe/Copenhagen\nOutput\n$ docker run --rm alpine date\nTue Aug 31 09:52:08 UTC 2021\n\n$ docker run --rm myimage date\nTue Aug 31 11:52:13 CEST 2021",
    "Failed to create endpoint on network nat: hnsCall failed in Win32: The process cannot access the file": "Not sure how wise this is, but I checked the port wasn't in use with another app and still got the error.\nThis has fixed the issue a couple of times for me. In an Administrative PowerShell console, run the following:\nStop-Service docker\nStop-service hns\nStart-service hns\nStart-Service docker\ndocker network prune\nPartially sourced from this post.",
    "Docker: failed to export image: failed to create image: failed to get layer": "This problem occurs with a specific sequence of COPY commands in a multistage build.\nMore precisely, the bug triggers when there is a COPY instruction producing null effect (for example if the content copied is already present in the destination, with 0 diff), followed immediately by another COPY instruction.\nA workaround could be to add RUN true between COPY statements:\nCOPY ./lib/ /usr/src/app/BOOT-INF/lib/\nRUN true\nCOPY ./lib/entities-1.0-SNAPSHOT.jar /usr/src/app/BOOT-INF/lib/entities-1.0-SNAPSHOT.jar\nRUN true\nCOPY ./app/ /usr/src/app/\nAnother way that seems to work is to launch the build using BUILDKIT, like that:\nDOCKER_BUILDKIT=1 docker build --tag app:test .\nSee: https://github.com/moby/moby/issues/37965",
    "npm ERR! code ENOTEMPTY while npm install": "I think the following command might be more appropriate:\nrm -r node_modules\nThis will remove the node_modules folder in your repository. The command npm install should work now.\nIf you are using Webpack, you can also remove the dist folder using rm -r dist and re-build your repository.",
    "dockerfile: how use CMD or ENTRYPOINT from base image": "If you left it blank in your new Dockerfile, it will inherit the one from the base image.\nFor example:\nbase\nFROM ubuntu\nCMD [\"echo\", \"AAA\"]\nlayer1\nFROM base\nIf you build above images and run layer1 you will get the following:\n$ sudo docker run -it layer1\nAAA",
    "Speed up NPM install in Docker container": "This method works like magic:\nhttps://blog.playmoweb.com/speed-up-your-builds-with-docker-cache-bfed14c051bf\nDocker has a special way of caching things for you, and apparently it's best to use the inborn caching ability.\nCannot say I completely understand how it works, but it does work.\nIf you follow this pattern, it will work for you:\nFROM mhart/alpine-node:5.6.0\nWORKDIR /src\n\n# Expose the port 3000\nEXPOSE 3000\n\n# Set the default command to run when a container starts\nCMD [\"node\", \"server.js\"]\n\n# Install app dependencies\nCOPY package.json /src\nRUN npm install\n\n# Copy your code in the docker image\nCOPY . /src",
    "strconv.Atoi: parsing \"\": invalid syntax": "I also faced the same issue. There was no error except for:\nstrconv.Atoi: parsing \"\": invalid syntax\nScenario:\na go project\ndockerfile\ndocker-compose.yaml\nWindows 10 with docker-desktop\nThe following command will stop and delete all the docker containers that are not defined in docker compose file. There might be many useful containers running in your system that were not created by the docker-compose file or were created by someone else. Use this command with caution!\nSolution:\ndocker-compose down --remove-orphans\nExplaination: For some reason, the docker-compose create some unknown containers along with service. This command remove those unknown problematic containers.",
    "How to avoid question during the Docker build?": "I had the same issue in Dockerfile, then I used ARG DEBIAN_FRONTEND=noninteractive after base image and it works for me:\nExample Dockerfile:\nFROM ubuntu\nARG DEBIAN_FRONTEND=noninteractive",
    "How can I prevent a Dockerfile instruction from being cached?": "A build-time argument can be specified to forcibly break the cache from that step onwards. For example, in your Dockerfile, put\nARG CACHE_DATE=not_a_date\nand then give this argument a fresh value on every new build. The best, of course, is the timestamp.\ndocker build --build-arg CACHE_DATE=$(date +%Y-%m-%d:%H:%M:%S) ...\nMake sure the value is a string without any spaces, otherwise docker client will falsely take it as multiple arguments.\nSee a detailed discussion on Issue 22832.",
    "Multiple images inside one container": "Keep images light. Run one service per container. Use the official images on docker hub for mongodb, nodejs, rabbitmq, nginx etc. Extend them if needed. If you want to run everything in a fat container you might as well just use a VM.\nYou can of course do crazy stuff in a dev setup, but why spend time setting up something that has zero value in a production environment? What if you need to scale up one of the services? How do set memory and cpu constraints on each service? .. and the list goes on.\nDon't make monolithic containers.\nA good start is to use docker-compose to configure a set of services that can talk to each other. You can make a prod and dev version of your docker-compose.yml file.\nGetting into the right frame of mind\nIn a perfect world you would run your containers in clustered environment in production to be able to scale your system and have concurrency, but that might be overkill depending on what you are running. It's at least good to have this in the back of your head because it can help you to make the right decisions.\nSome points to think about if you want to be a purist :\nHow do you have persistent volume storage across multiple hosts?\nReverse proxy / load balancer should probably be the entry point into the system that talks to the containers using the internal network.\nIs my service even able run in a clustered environment (multiple instances of the container)\nYou can of course do dirty things in dev such as mapping in host volumes for persistent storage (and many people who use docker standalone in prod do that as well).\nIdeally we should separate docker in dev and docker i prod. Docker is a fantastic tool during development as you can have redis, memcached, postgres, mongodb, rabbitmq, node or whatnot up and running in minutes sharing that compose setup with the rest of the team. Docker in prod can be a completely different beast.\nI would also like to add that I'm generally against the fanaticism that \"everything should be running in docker\" in prod. Run services in docker when it makes sense. It's also not uncommon for larger companies to make their own base images. This can be a lot of work and will require maintenance to keep up with security fixes etc. It's not necessarily the first thing you jump on when starting with docker.",
    "Dockerfile: COPY / ADD with space character in path (Windows)": "Maybe you can use ARG to help you, like this:\nDockerfile:\nFROM jfloff/alpine-python:2.7\nARG src=\"Folder 1/File.txt\"\nARG target=\"Dir 1/\"\nCOPY ${src} ${target}\nBTW, a / has to be add at the end of Dir 1 if you treat really want to treat it as a folder.\nAnd, JSON format is also ok, just you miss ,, it should be:\nFROM jfloff/alpine-python:2.7\nCOPY [\"Folder 1/File.txt\", \"Dir 1/\"]\nUpdate for your comments:\nIn official guide, it said:\nWhen copying files or directories that contain special characters (such as [ and ]), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern.\nSo, for your case, it should be:\nFROM jfloff/alpine-python:2.7\nARG src=\"[[]Folder 1]/__SLIM_TEMPLATE.mm\"\nARG target=\"[Folder 1]/\"\nCOPY ${src} ${target}\nOr:\nFROM jfloff/alpine-python:2.7\nCOPY [\"[[]Folder 1]/__SLIM_TEMPLATE.mm\", \"[Folder 1]/\"]",
    "How are intermediate containers formed?": "Yes, Docker images are layered. When you build a new image, Docker does this for each instruction (RUN, COPY etc.) in your Dockerfile:\ncreate a temporary container from the previous image layer (or the base FROM image for the first command;\nrun the Dockerfile instruction in the temporary \"intermediate\" container;\nsave the temporary container as a new image layer.\nThe final image layer is tagged with whatever you name the image - this will be clear if you run docker history raghavendar/hands-on:2.0, you'll see each layer and an abbreviation of the instruction that created it.\nYour specific queries:\n1) 532 is a temporary container created from image ID b17, which is your FROM image, ubuntu:14.04.\n2) ea6 is the image layer created as the output of the instruction, i.e. from saving intermediate container 532.\n3) yes. Docker calls this the Union File System and it's the main reason why images are so efficient.",
    "Docker: Mount directory from one container to another": "Rene's answer works, but you could share data without using the host's directory (container1 ==> container2):\ndocker run -v /data/myfolder --name container1 image-name-1\ndocker run --volumes-from container1 image-name-2",
    "How to test the container or image after docker build?": "You have to give a command your container will have to process.\nExample : sh\nyou could try :\ndocker run -ti yourimage sh\n(-ti is used to keep a terminal open)\nIf you want to launch a daemon (like a server), you will have to enter something like :\ndocker run -d yourimage daemontolaunch\nUse docker help run for more options.\nYou also can set a default behaviour with CMD instruction in your Dockerfile so you won't have to give this command to your container each time you want to run it.\nEDIT - about container removing :\nContainers and images are different. A container is an instance of an image. You can run several containers from the same image.\nThe container automatically stops when the process it runs terminates. But the container isn't deleted (just stopped, so you can restart it). But if you want to remove it (removing a container doesn't remove the image) you have two ways to do :\nautomatically removing it at the end of the process by adding --rm option to docker run.\nManually removing it by using the docker rm command and giving it the container ID or its name (a container has to be stopped before being removed, use docker stop for this).\nA usefull command :\nUse docker ps to list containers. -q to display only the container IDs, -a to display even stopped containers.\nMore here.\nEDIT 2:\nThis could also help you to discover docker if you didn't try it.",
    "FIle could not be opened in append mode: failed to open stream: Permission denied laradock": "This worked for me:\nchown -R www-data:www-data \"project foldername\"",
    "How to access GIT repo with my private key from Dockerfile": "The error message Host key verification failed. is not complaining about your private key, but rather the host key for github.com. You can do this to add the github hostkey:\nssh-keyscan -t rsa github.com > ~/.ssh/known_hosts\nPerhaps you have your reasons, but in general cloning the git repo in to the image is not the preferred way to run your code in a container. Instead, put a Dockerfile at the root of your repo, and within the Dockerfile use the ADD command to include your source code in the container.\nAs you have it written now, your private key is part of the Docker image. Anyone you share the image with will also have your private key.",
    "How to know if a docker container is running in privileged mode": "From the docker host\nUse the docker inspect command:\ndocker inspect --format='{{.HostConfig.Privileged}}' <container id>\nAnd within a bash script you could have a test:\nif [[ $(docker inspect --format='{{.HostConfig.Privileged}}' <container id>) == \"false\" ]]; then\n    echo not privileged\nelse\n    echo privileged\nfi\nFrom inside the container itself\nYou have to try to run a command that requires the --privileged flag and see if it fails\nFor instance ip link add dummy0 type dummy is a command which requires the --privileged flag to be successful:\n$ docker run --rm -it ubuntu ip link add dummy0 type dummy\nRTNETLINK answers: Operation not permitted\nwhile\n$ docker run --rm -it --privileged ubuntu ip link add dummy0 type dummy\nruns fine.\nIn a bash script you could do something similar to this:\nip link add dummy0 type dummy >/dev/null\nif [[ $? -eq 0 ]]; then\n    PRIVILEGED=true\n    # clean the dummy0 link\n    ip link delete dummy0 >/dev/null\nelse\n    PRIVILEGED=false\nfi",
    "Pull docker images from a private repository during docker build?": "I was facing the same issue in 2019. I solved this using arguments (ARG).\nhttps://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact\nArguments allow you to set optional parameters (with defaults) that can be used in your FROM line.\nDockerfile-project-dev\nARG REPO_LOCATION=privaterepo.company.net/\nARG BASE_VERSION=latest\nFROM ${REPO_LOCATION}project/base:${BASE_VERSION}\n...\nFor my use-case I normally want to pull from the private repo, but if I'm working on the Dockerfiles I may want to be able to build from an image on my own machine, without having to modify the FROM line in my Dockerfile. To tell Docker to search my local machine for the image at build time I would do this:\ndocker build -t project/dev:latest -f ./Dockerfile-project-dev --build-arg REPO_LOCATION='' .",
    "How to install git on a docker ubuntu image?": "This Dockerfile works for me\nFROM ubuntu:18.04\nRUN apt update\nRUN apt install -y git\nInside the container\n$ which git\n/usr/bin/git",
    "Can dockerfile be put in .dockerignore?": "Yes, you can; you can even throw the .dockerignore itself in there!\nYou're likely doing something else incorrect - possibly in the wrong directory?\nDirectory listing:\n\u279c  ls -la\ntotal 16\ndrwxr-xr-x  4 tj    wheel  128 Nov 30 13:42 .\ndrwxrwxrwt  7 root  wheel  224 Nov 30 13:42 ..\n-rw-r--r--  1 tj    wheel   26 Nov 30 13:41 .dockerignore\n-rw-r--r--  1 tj    wheel   28 Nov 30 13:42 Dockerfile\nContent of files:\n\u279c  cat .dockerignore\n.dockerignore\nDockerfile\n\n\u279c  test_docker_ignore cat Dockerfile\nFROM ubuntu:16.04\nADD . .\nBuild it once; specifying --no-cache to be verbose:\n\u279c  docker build -t test --no-cache .\nSending build context to Docker daemon  3.072kB\nStep 1/2 : FROM ubuntu:16.04\n ---> 20c44cd7596f\nStep 2/2 : ADD . .\n ---> 4d8ded297954\nSuccessfully built 4d8ded297954\nSuccessfully tagged test:latest\nAdd something to the Dockerfile and rebuild: The build will use the cache as it ignores the changes made to the Dockerfile\n\u279c  echo \"# A Test Comment\" >> Dockerfile\n\u279c  docker build -t test .\nSending build context to Docker daemon  3.072kB\nStep 1/2 : FROM ubuntu:16.04\n ---> 20c44cd7596f\nStep 2/2 : ADD . .\n ---> Using cache\n ---> 4d8ded297954\nSuccessfully built 4d8ded297954\nSuccessfully tagged test:latest",
    "The command '/bin/sh -c returned a non-zero code: 127": "Solution to the image with error is to add before the wget CMD\nRUN yum -y install wget\nIf you write it like this, it is the same result, just different execution:\n RUN wget http://www.us.apache.org/dist/tomcat/tomcat-6/v6.0.44/bin/apache-tomcat-6.0.44.tar.gz\nDon't use the quotes and comma in RUN command.",
    "HEALTHCHECK: Dockerfile vs docker-compose.yml": "Adding health check to the Dockerfile, will make the health-check part of the image, so that anyone pulling the image from the registry will get the health check by default.\nCompose files are usually less shared than the actual docker images they run. The dockercompose health-check allows adding/overrriding healthchecks for images for someone who is not creating the image but rather is pulling it from a remote registry. It is more suitable in situations where the pulled image doesn't have a health-check by default.\nIn your case, since you are creating the image, adding the health-check to the dockerfile makes more sense.",
    "How to declare a Named Volume in a Dockerfile?": "It's not possible. I think the docs are worded maybe misleadingly.\n\u201cThe specified name\u201d refers to the path / directory name at which the volume will be created.",
    "Pass args to the Dockerfile from docker-compose": "the key word ARG has a different scope before and after the FROM instruction\nTry using ARG twice in your Dockerfile, and/or you can try the ENV variables\nARG LANDING_PAGE_DOMAIN\nFROM nginx:alpine\n\nCOPY nginx.conf /etc/nginx/nginx.conf\nCOPY production/* /etc/nginx/conf.d/\n\nARG LANDING_PAGE_DOMAIN\nENV LANDING_PAGE_DOMAIN=${LANDING_PAGE_DOMAIN}\n\nRUN sed -i s/{LANDING_PAGE_DOMAIN}/${LANDING_PAGE_DOMAIN}/g /etc/nginx/conf.d/landing.conf\n\nEXPOSE 80 443",
    "Dockerfile COPY instruction failing?": "At the time I originally wrote this, Docker didn\u2019t expand ~ or $HOME. Now it does some expansions inside the build context, but even so they are probably not what you want\u2014they aren\u2019t your home directory outside the context. You need to reference the file explicitly, or package it relative to the Dockerfile itself.",
    "What's the difference between pm2 and pm2-runtime?": "The main difference between pm2 and pm2-runtime is\npm2-runtime designed for Docker container which keeps an application in the foreground which keep the container running,\npm2 is designed for normal usage where you send or run the application in the background.\nIn simple words, the life of the container is the life of CMD or entrypoint.\nFor example\nDockerfile\nFROM node:alpine\nRUN npm install pm2 -g\nCOPY . /app\nWORKDIR /app\nCMD [ \"pm2\", \"start\",\"/app/server.js\"]\nIn this case, the container will die as soon as it run the process.\nTo deal with this, you have pm2-runtime\nFROM node:alpine\nRUN npm install pm2 -g\nCOPY . /app\nWORKDIR /app\nENV NODE_ENV=development\nCMD [ \"pm2-runtime\", \"start\",\"/app/bin/www\"]\nAs the container keeps running and it allocates tty session.\nFrom the documentation\nThe goal of pm2-runtime is to wrap your applications into a proper Node.js production environment. It solves major issues when running Node.js applications inside a container like:\nSecond Process Fallback for High Application Reliability\nProcess Flow Control\nAutomatic Application Monitoring to keep it always sane and high performing\nAutomatic Source Map Discovery and Resolving Support\nFurther than that, using PM2 as a layer between the container and the application brings PM2 powerful features like application declaration file, customizable log system and other great features to manage your Node.js application in production environment.\ndocker-pm2-nodejs",
    "Docker-compose --force-recreate specific service": "If your service depends on (or links to) other services, you can try:\ndocker-compose up --force-recreate --no-deps service-name\nThis will only recreate the specified service, linked or depended services will keep untouched.",
    "Starting container process caused \"exec: \\\"/bin/sh\\\": stat /bin/sh: no such file or directory\": unknown": "There are two things happening here.\nA Dockerfile that starts FROM scratch starts from a base image that has absolutely nothing at all in it. It is totally empty. There is not a set of base tools or libraries or anything else, beyond a couple of device files Docker pushes in for you.\nThe ENTRYPOINT echo ... command gets rewritten by Docker into ENTRYPOINT [\"/bin/sh\", \"-c\", \"echo ...\"], and causes the CMD to be totally ignored. Unless overridden with docker run --entrypoint, this becomes the main process the container runs.\nSince it is a FROM scratch image and contains absolutely nothing at all, it doesn't contain a shell, hence the \"/bin/sh: no such file or directory\" error.",
    "does docker always need an operating system as base image": "The containers on a host share the (host's) kernel but each container must provide (the subset of) the OS that it needs.\nIn Windows, there's a 1:1 mapping of kernel:OS but, with Linux, the kernel is bundled into various OSs: Debian, Ubuntu, Alpine, SuSE, CoreOS etc.\nThe FROM statement often references an operating system but it need not and it is often not necessary (nor a good idea) to bundle an operating system in a container. The container should only include what it needs.\nThe NGINX image uses Debian (Dockerfile).\nIn some cases, the container process has no dependencies beyond the kernel. In these cases, a special FROM: scratch may be used that adds nothing else. It's an empty image (link).",
    "Use sudo inside Dockerfile (Alpine)": "The su-exec can be used in alpine. Do add it the package, if not already available, add the following to your Dockerfile\nRUN apk add --no-cache su-exec\nInside your scripts you'd run inside docker you can use the following to become another user:\nexec su-exec <my-user> <my command>\nAlternatively, you could add the more familiair sudo package while building your docker-file Add the following to your Dockerfile that's FROM alpine\nRUN set -ex && apk --no-cache add sudo\nAfter that you can use sudo\nsudo -u <my-user> <my command>",
    "Copy current directory in to docker image": "Just add / at the end of src in ADD statement.\nADD ./* $HOME/src/",
    "How to create a Mongo Docker Image with default collections and data?": "The problem was that information could not be saved on /db/data, so I've created a solution creating my own data directory.\n# Parent Dockerfile https://github.com/docker-library/mongo/blob/982328582c74dd2f0a9c8c77b84006f291f974c3/3.0/Dockerfile\nFROM mongo:latest\n\n# Modify child mongo to use /data/db2 as dbpath (because /data/db wont persist the build)\nRUN mkdir -p /data/db2 \\\n    && echo \"dbpath = /data/db2\" > /etc/mongodb.conf \\\n    && chown -R mongodb:mongodb /data/db2\n\nCOPY . /data/db2\n\nRUN mongod --fork --logpath /var/log/mongodb.log --dbpath /data/db2 --smallfiles \\\n    && CREATE_FILES=/data/db2/scripts/*-create.js \\\n    && for f in $CREATE_FILES; do mongo 127.0.0.1:27017 $f; done \\\n    && INSERT_FILES=/data/db2/scripts/*-insert.js \\\n    && for f in $INSERT_FILES; do mongo 127.0.0.1:27017 $f; done \\\n    && mongod --dbpath /data/db2 --shutdown \\\n    && chown -R mongodb /data/db2\n\n# Make the new dir a VOLUME to persists it \nVOLUME /data/db2\n\nCMD [\"mongod\", \"--config\", \"/etc/mongodb.conf\", \"--smallfiles\"]\nThanks to @yosifkit from the docker-library/mongo Github project for pointing that the volume would store the data in the resulting image. I missed that on the documentation.",
    "Mount a volume in docker-compose. How is it done?": "Your config is probably not working because your version of docker-compose does not execute shell expansions while creating your container. That means that docker compose is trying to find a literal path $PWD/logstash instead of expanding $PWD to your present directory. Later versions of docker compose do allow for environment variable expansion.\nDocker-compose does allow relative paths though, through the use of ./, which references the folder the compose file is in, not necessarily your pwd, so you just need to change your compose file to be:\nvolumes:\n    - ./logstash:/config_dir",
    "Where does the output for \"docker -build\" go?": "Use:\ndocker images\nto see it\nExample:\nREPOSITORY     TAG                 IMAGE ID            CREATED             SIZE\nsample         latest              c660b762fcd1        5 days ago          1.46GB",
    "How to use multiple base images to build a docker image": "The latest version of docker has the concept of multi stage builds. Refer: (https://docs.docker.com/engine/userguide/eng-image/multistage-build/)\nWith multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image.",
    "Docker Compose: How to specify path to docker file while building as if in different directory?": "This can be done like this, add build in docker-compose.yml and run the build from ABOVE directory:\nversion: '3.8'\n\nservices:\n  your_service:\n    build: # \"context\" and \"dockerfile\" fields have to be under \"build\"\n      context: .\n      dockerfile: <Below_Directory>/Dockerfile",
    "copy a file with different name in destination directory using COPY in Dockerfile": "You can simply do it like COPY test.txt /dest/test_renamed.txt without a trailing /.\nIf you put a trailing / like COPY test.txt /dest/test_2/, it will copy the test.txt file (with the same name, of course) into the directory at /dest/test_2/.",
    "Copying files with execute permissions in Docker Image": "The default file permission is whatever the file permission is in your build context from where you copy the file. If you control the source, then it's best to fix the permissions there to avoid a copy-on-write operation. Otherwise, if you cannot guarantee the system building the image will have the execute bit set on the files, a chmod after the copy operation will fix the permission. E.g.\nCOPY entrypoint.sh .\nRUN chmod +x entrypoint.sh\nA better option with newer versions of docker (and which didn't exist when this answer was first posted) is to use the --chmod flag (the permissions must be specified in octal at last check):\nCOPY --chmod=0755 entrypoint.sh .\nYou do not need to know who will run the container. The user inside the container is typically configured by the image creator (using USER) and doesn't depend on the user running the container from the docker host. When the user runs the container, they send a request to the docker API which does not track the calling user id.\nThe only time I've seen the host user matter is if you have a host volume and want to avoid permission issues. If that's your scenario, I often start the entrypoint as root, run a script called fix-perms to align the container uid with the host volume uid, and then run gosu to switch from root back to the container user.",
    "Using Vault with docker-compose file": "This is my current docker-compose config for using Vault in dev, but I use dedicated servers (not Docker) in production.\n# docker_compose.yml\nversion: '2'\nservices:\n    myvault:\n        image: vault\n        container_name: myvault\n        ports:\n          - \"127.0.0.1:8200:8200\"\n        volumes:\n          - ./file:/vault/file:rw\n          - ./config:/vault/config:rw\n        cap_add:\n          - IPC_LOCK\n        entrypoint: vault server -config=/vault/config/vault.json\nThe volume mounts ensure the vault config is saved if you have to rebuild the container.\nTo use the 'file' backend, to make this setup portable for Docker/Git, you will also need to create a directory called config and put this file into it, named vault.json:\n# config/vault.json\n{\n  \"backend\": {\"file\": {\"path\": \"/vault/file\"}},\n  \"listener\": {\"tcp\": {\"address\": \"0.0.0.0:8200\", \"tls_disable\": 1}},\n  \"default_lease_ttl\": \"168h\",\n  \"max_lease_ttl\": \"0h\"\n}\nNotes:\nAlthough the ROOT_TOKEN is static in this configuration (will not change between container builds), any generated VAULT_TOKEN issued for an app_role will be invalidated every time the vault has to be unsealed.\nI have found the Vault sometimes becomes sealed when the container is restarted.",
    "Docker build taking too long when installing grpcio via pip": "I had the same issue and it was solved by upgrading pip:\n$ pip3 install --upgrade pip\nHere's a word from one of the maintainers of grpc project:\npip grpcio install is (still) very slow #22815",
    "Docker multiple environments": "You could take some clues from \"Using Compose in production\"\nYou\u2019ll almost certainly want to make changes to your app configuration that are more appropriate to a live environment. These changes may include:\nRemoving any volume bindings for application code, so that code stays inside the container and can\u2019t be changed from outside\nBinding to different ports on the host\nSetting environment variables differently (e.g., to decrease the verbosity of logging, or to enable email sending)\nSpecifying a restart policy (e.g., restart: always) to avoid downtime\nAdding extra services (e.g., a log aggregator)\nThe advice is then not quite similar to the example you mention:\nFor this reason, you\u2019ll probably want to define an additional Compose file, say production.yml, which specifies production-appropriate configuration. This configuration file only needs to include the changes you\u2019d like to make from the original Compose file.\ndocker-compose -f docker-compose.yml -f production.yml up -d\nThis overriding mechanism is better than trying to mix dev and prod logic in one compose file, with environment variable to try and select one.\nNote: If you name your second dockerfile docker-compose.override.yml, a simple docker-compose up would read the overrides automatically.\nBut in your case, a name based on the environment is clearer.",
    "unable to add certificates to alpine linux container": "I think below worked for me (I was adding a root certificate on blackfire/blackfire image which extends from alpine):\nRUN apk update && apk add ca-certificates && rm -rf /var/cache/apk/* \\\n  mkdir /usr/local/share/ca-certificates/extra\nCOPY .docker/other/cert_Intertrials-CA.crt /usr/local/share/ca-certificates/extra\nRUN update-ca-certificates\nI then logged into that VM and see it has added it to the merged cert file, /etc/ssl/certs/ca-certificates.crt (I believe i heard it takes each cert file from inside /usr/local/share/ca-certificates and merges into the /etc/ssl/certs/ca-certificates.crt file).\nNow you will get that 'does not contain exactly one certificate or CRL: skipping' error probably, but i heard that is fine.\nhttps://github.com/gliderlabs/docker-alpine/issues/30 mentions: \"that this is just a warning and shouldn't affect anything.\"\nhttps://github.com/gliderlabs/docker-alpine/issues/52 mentions: \"The WARNING: ca-certificates.crt does not contain exactly one certificate or CRL: skipping is just what it says it is, a warning. It is saying that ca-certificates.crt doesn't contain only one certificate (because it is the concatenation of all the certificates), therefore it is skipped and not included in ca-certificates.crt (since it cannot include itself).\"\n\"The warning shown is normal.\"",
    "Docker-compose.yml file that builds a base image, then children based on it?": "Doing a bit more research based on @amiasato 's anser, it looks as if there is a replicated key, which you can set to 0 like so:\nversion: \"3\"\nservices:\n  base-image:\n    build:\n      context: .\n      dockerfile: Dockerfile-base\n    deploy:\n      mode: replicated\n      replicas: 0\nSee https://docs.docker.com/compose/compose-file/compose-file-v3/#replicas",
    "Mysql docker container keeps restarting": "Simply remove the MYSQL_USER and it will work fine because the root user gets created automatically.\nPS. This seems to be a problem with a newer docker version because this used to work before and not throw an error.",
    "Deprecation warning when installing nodejs on docker container using nodesource install script": "The notice from the script is\n  This script, located at https://deb.nodesource.com/setup_X, used to\n  install Node.js is deprecated now and will eventually be made inactive.\n\n  Please visit the NodeSource distributions Github and follow the\n  instructions to migrate your repo.\n  https://github.com/nodesource/distributions \n\n  The NodeSource Node.js Linux distributions GitHub repository contains\n  information about which versions of Node.js and which Linux distributions\n  are supported and how to install it.\n  https://github.com/nodesource/distributions\nThe instructions on github amount to a Dockerfile RUN\nFROM docker.io/debian:12-slim\nRUN set -uex; \\\n    apt-get update; \\\n    apt-get install -y ca-certificates curl gnupg; \\\n    mkdir -p /etc/apt/keyrings; \\\n    curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key \\\n     | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg; \\\n    NODE_MAJOR=18; \\\n    echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" \\\n     > /etc/apt/sources.list.d/nodesource.list; \\\n    apt-get -qy update; \\\n    apt-get -qy install nodejs;\nThe docker.io/node:18 image maintained by the Node.js project is Debian based if you want to save some time.\nFROM docker.io/node:18-bookworm-slim",
    "docker container exits immediately even with Console.ReadLine() in a .NET Core console application": "If you switch your app to target .NET Core 2.0, you can use the Microsoft.Extensions.Hosting package to host a .NET Core console application by using the HostBuilder API to start/stop your application. Its ConsoleLifetime class would process the general application start/stop method.\nIn order to run your app, you should implement your own IHostedService interface or inherit from the BackgroundService class, then add it to host context within ConfigureServices.\nnamespace Microsoft.Extensions.Hosting\n{\n    //\n    // Summary:\n    //     Defines methods for objects that are managed by the host.\n    public interface IHostedService\n    {\n        // Summary:\n        // Triggered when the application host is ready to start the service.\n        Task StartAsync(CancellationToken cancellationToken);\n\n        // Summary:\n        // Triggered when the application host is performing a graceful shutdown.\n        Task StopAsync(CancellationToken cancellationToken);\n    }\n}\nHere's a sample hosted service:\npublic class TimedHostedService : IHostedService, IDisposable\n{\n    private readonly ILogger _logger;\n    private Timer _timer;\n\n    public TimedHostedService(ILogger<TimedHostedService> logger)\n    {\n        _logger = logger;\n    }\n\n    public Task StartAsync(CancellationToken cancellationToken)\n    {\n        _logger.LogInformation(\"Timed Background Service is starting.\");\n\n        _timer = new Timer(DoWork, null, TimeSpan.Zero, \n            TimeSpan.FromSeconds(5));\n\n        return Task.CompletedTask;\n    }\n\n    private void DoWork(object state)\n    {\n        _logger.LogInformation(\"Timed Background Service is working.\");\n    }\n\n    public Task StopAsync(CancellationToken cancellationToken)\n    {\n        _logger.LogInformation(\"Timed Background Service is stopping.\");\n\n        _timer?.Change(Timeout.Infinite, 0);\n\n        return Task.CompletedTask;\n    }\n\n    public void Dispose()\n    {\n        _timer?.Dispose();\n    }\n}\nThen creating the HostBuilder and adding the service and other components (logging, configuration).\npublic class Program\n{\n    public static async Task Main(string[] args)\n    {\n        var hostBuilder = new HostBuilder()\n             // Add configuration, logging, ...\n            .ConfigureServices((hostContext, services) =>\n            {\n                // Add your services with depedency injection.\n            });\n\n        await hostBuilder.RunConsoleAsync();\n    }\n}",
    "Running a Docker file stored locally": "The process to run Dockerfile is:\ndocker build . -t [tag] -f /path/to/Dockerfile\nAnd then:\ndocker run -d tag",
    "Interactive command in Dockerfile": "You can also do it in several steps, begin with a Dockerfile with instructions until before the interactive part. Then\ndocker build -t image1 .\nNow just\ndocker run -it --name image2 image1 /bin/bash\nyou have a shell inside, you can do your interactive commands, then do something like\ndocker commit image2 myuser/myimage:2.1\nThe doc for docker commit\nhttps://docs.docker.com/engine/reference/commandline/commit/\nyou may need to specify a new CMD or ENTRYPOINT, as stated in the doc\nCommit a container with new CMD and EXPOSE instructions\nFor example some docker images using wine do it in several steps, install wine, then launch and configure the software launched in wine, then docker commit",
    "How to make Network_Mode : \"host\" work in docker-compose.yml file": "network_mode: host is used for sharing the same networking space with the Host. For example you can want to access an application that is running on your Linux PC from the container. If you want to link services together, you can use links, or depends_on, and if the services are on different hosts just create an overlay network",
    "Create a docker image/container from EC2 AMI": "Here is how I did it.\nOn source AMI locate root volume snapshot id in the description\n/dev/sda1=snap-eb79b0b1:15:true:gp2\nLaunch instance with public Ubuntu 14.04 AMI\nCreate volume from snapshot snap-eb79b0b1 (in the same region that the instance runs).\nAttach volume to the instance as /dev/sdf\nmount volume to /mnt\nmount /dev/xvdf /mnt\n(or)\nmount /dev/xvdf1 /mnt\ninstall docker\nhttps://docs.docker.com/engine/installation/ubuntulinux/\nimport docker image from mounted root volume\ntar -c -C /mnt/ . | docker import - appcimage-master-1454216413\nrun\ndocker run -t -i 6d6614111fcb03d5ca79541b8a23955202dfda74995d968b5ffb5d45c7e68da9 /bin/bash",
    "Docker buildkit cache location/size and ID": "Yes, it is somewhat vague in docker 20.10.5. Could use a pull request or two to update documentation.\nThe docker driver cache uses the same storage driver as used for image layers. Metadata is stored in databases at /var/lib/docker/buildkit. When docker uses overlay2 storage driver, the layer is in /var/lib/docker/overlay2/<ID>/diff/. For <ID>, see below. /var/lib/docker can vary depending on data-root in your dockerd configuration. Builders using docker-container or kubernetes driver keeps the data on a volume.\ndocker buildx [--builder name] du --verbose lists build cache. You can also inspect the docker driver caches from docker system df -v --format '{{ .BuildCache | json }}'. The cache type exec.cachemount is the RUN --mount type=cache. You can find the layer using the ID, which is not the same as used in --mount id. The mount type is implemented by buildkit, so the docker run --mount does not recognize it. To get rid of it either docker buildx prune or docker build --no-cache.\nThe cache key is the value from id=. id defaults to value of target. You need to specify id when you need different cache at the same target.\nYes. They are the same cache regardless of the target, Dockerfile or platform. Different builders have their own caches.",
    "How to run wp cli in docker-compose.yml": "Well there are a couple of problems. The first one is that those two containers (wordpress and wordpress-cli) don't share a volume. So while wordpress has a wordpress installation ready, the wordpress-cli doesn't.\nSo you can add volumes to both containers, and then wordpress-cli will find the wordpress installation.\nThen there's a second problem: the wordpress:latest and wordpress:cli images both run with the user www-data, but the problem is that the individual www-data users have different user-id's:\n$ docker run --rm wordpress:latest grep www-data /etc/passwd \nwww-data:x:33:33:www-data:/var/www:/usr/sbin/nologin\n$ docker run --rm wordpress:cli grep www-data /etc/passwd   \nwww-data:x:82:82:Linux User,,,:/home/www-data:/bin/false\nIt seems they aren't exactly compatible here. So if you use a shared volume you have to make sure they both use the same user-id. I solved this by having the wordpress:cli run with the user xfs which also has the user id 33.\nThe last problem is that your containers have dependencies on each other. Wordpress needs a running MySQL instance and the wordpress-cli needs also the MySQL and the Wordpress to be ready. To make sure MySQL is ready for the wordpress cli installation you either use something like \"wait-for-it\" or in a simple case you can just wait a couple of seconds and then try it.\nI have tested all those changes and came up with the following docker-compose.yml. I have annotated all the changes I've made with \"vstm\":\nversion: \"3.3\"\nservices:\n  db:\n    image: mysql:5.7\n    volumes:\n      - db_data:/var/lib/mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: somewordpress\n      MYSQL_DATABASE: wordpress\n      MYSQL_USER: wordpress\n      MYSQL_PASSWORD: wordpress\n\n  wordpress:\n    depends_on:\n      - db\n    image: wordpress:latest\n    ports:\n      - 8000:80\n    restart: always\n    environment:\n      WORDPRESS_DB_HOST: db:3306\n      WORDPRESS_DB_NAME: wordpress\n      WORDPRESS_DB_USER: wordpress\n      WORDPRESS_DB_PASSWORD: wordpress\n      WORDPRESS_TABLE_PREFIX: \"wp_\"\n      WORDPRESS_DEBUG: 1\n    # vstm: add shared volume\n    volumes:\n      - wp_data:/var/www/html\n\n  wordpress-cli:\n    depends_on:\n      - db\n      - wordpress\n    image: wordpress:cli\n    # vstm: This is required to run wordpress-cli with the same\n    # user-id as wordpress. This way there are no permission problems\n    # when running the cli\n    user: '33'\n    # vstm: The sleep 10 is required so that the command is run after\n    # mysql is initialized. Depending on your machine this might take\n    # longer or it can go faster.\n    command: >\n      /bin/sh -c '\n      sleep 10;\n      wp core install --path=\"/var/www/html\" --url=\"http://localhost:8000\" --title=\"Local Wordpress By Docker\" --admin_user=admin --admin_password=secret --admin_email=foo@bar.com\n      '\n    # vstm: add shared volume\n    volumes:\n      - wp_data:/var/www/html\n    # WP CLI needs the environment variables used for the Wordpress image\n    environment:\n      WORDPRESS_DB_HOST: db:3306\n      WORDPRESS_DB_NAME: wordpress\n      WORDPRESS_DB_USER: wordpress\n      WORDPRESS_DB_PASSWORD: wordpress\n\nvolumes:\n  db_data:\n  # vstm: add shared volume\n  wp_data:\nIt uses a docker-volume but you can also map it to a filesystem. Depends on how you plan to use your docker-compose.",
    "pg_restore in postgres docker container": "Here is a way to restore from a file located on the host machine:\ndocker exec -i container_name pg_restore -U postgres_user -v -d database_name < /dir_backup_outside_container/file_name.tar",
    "Building docker images from a source directory using different dockerfiles": "Since Docker 1.5, you can use the -f argument to select the Dockerfile to use e.g:\ndocker build -t doronaviugy/myproject -f dockerfiles/first.docker .\nIf you use stdin to build your image ( the - < first.docker syntax), you won't have a build context so you will be unable to use COPY or ADD instructions that refer to local files.\nIf you have to use an older version of Docker, you'll need to use some scripting to copy your specific Dockerfiles to Dockerfile at the root of the build context before calling docker build.",
    "Docker EXPOSE using run-time environment variables": "A little late but you could also use build args and change your code to:\nFROM python:3.6.5-stretch\n\n[ ... ]\n\nARG MY_SERVICE_PORT=8080\nARG MY_SERVICE_PORT_RPC=50051\n# 8080 and 50051 would be the defaults\n\n[ ... ]\n# Still functions like environment variables :)\nEXPOSE ${MY_SERVICE_PORT}\nEXPOSE ${MY_SERVICE_PORT_RPC}\nThen you can build with docker build --build-arg MY_SERVICE_PORT=80 -t image_tag before you run. This way you could have your containerized application and your container running with the same ports without getting too complex.",
    "How can I keep a docker debian container open?": "You need to explicitly run bash:\ndocker run -it debian /bin/bash\nThe -i means \"run interactively\", and -t means \"allocate a pseudo-tty\".\nA good place to read a bit more is the section Running an interactive shell in the Quickstart documentation.",
    "Docker + mssql-server-linux: How to launch .sql file during build (from Dockerfile)": "I ended up using a slightly modified version of VDR's solution which waits for the sqlservr to start by checking the logs instead of sleeping 10 seconds:\nRUN ( /opt/mssql/bin/sqlservr --accept-eula & ) | grep -q \"Service Broker manager has started\" \\\n    && /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P 'P@ssw0rd' -i /opt/mssql-scripts/000_create_db.sql \\\n    && pkill sqlservr ",
    "How to access local file when building from Dockerfile?": "Why don't you COPY the file inside the container before executing RUN?\nFROM centos:6\nCOPY hello.rpm /tmp/hello.rpm\nRUN rpm -ivh /tmp/hello.rpm\nThis assumes that hello.rpm is next to your Dockerfile when you build it.",
    "Docker Compose + Rails: best practice to migrate?": "From https://docs.docker.com/engine/reference/builder/#cmd:\nIf you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD. See ENTRYPOINT\nhttps://docs.docker.com/engine/reference/builder/#entrypoint\ntl;dr\nYou could define an entrypoint under app and define a bash file there:\napp:\n  entrypoint: [bin/entry]\n  ..\nbin/entry file example:\n#!/bin/bash\nset -e\n\nrake db:create\nrake db:migrate\n\nexec \"$@\"",
    "How to pass environment variables to a frontend web application?": "The way that I resolved this is as follows:\n1.Set the value in the enviroment.prod.ts with a unique and identificable String:\nexport const environment = {\n  production: true,\n  REST_API_URL: 'REST_API_URL_REPLACE',\n};\n2.Create a entryPoint.sh, this entryPoint will be executed every time that you done a docker run of the container.\n#!/bin/bash\nset -xe\n: \"${REST_API_URL_REPLACE?Need an api url}\"\n\nsed -i \"s/REST_API_URL_REPLACE/$REST_API_URL_REPLACE/g\" /usr/share/nginx/html/main*bundle.js\n\nexec \"$@\"\nAs you can see, this entrypoint get the 'REST_API_URL_REPLACE' argument and replace it (in this case) in the main*bundle.js file for the value of the var.\n3.Add the entrypoint.sh in the dockerfile before the CMD (it need execution permissions):\nFROM node:alpine as builder\nCOPY package.json ./\nRUN npm i && mkdir /app && cp -R ./node_modules ./app\nWORKDIR /app\nCOPY . .\nRUN $(npm bin)/ng build --prod\n\nFROM nginx:alpine\nCOPY nginx/default.conf /etc/nginx/conf.d/\nRUN rm -rf /usr/share/nginx/html/*\nCOPY --from=builder /app/dist /usr/share/nginx/html\n\n# Copy the EntryPoint\nCOPY ./entryPoint.sh /\nRUN chmod +x entryPoint.sh\n\nENTRYPOINT [\"/entryPoint.sh\"]\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n4.Lauch the image with the env or use docker-compose (the slash must be escaped):\ndocker run -e REST_API_URL_REPLACE='http:\\/\\/backend:8080\\/api'-p 80:80 image:tag\nProbably exists a better solution that not need to use a regular expresion in the minified file, but this works fine.",
    "How to source a script with environment variables in a docker build process?": "Each Dockerfile RUN step runs a new container and a new shell. If you try to set an environment variable in one shell, it will not be visible later on. For example, you might experiment with this Dockerfile:\nFROM busybox\nENV FOO=foo1\nRUN export FOO=foo2\nRUN export BAR=bar\nCMD echo FOO is $FOO, BAR is $BAR\n# Prints \"FOO is foo1, BAR is \"\nThere are three good solutions to this. In order from easiest/best to hardest/most complex:\nAvoid needing the environment variables at all. Install software into \u201csystem\u201d locations like /usr; it will be isolated inside the Docker image anyways. (Don\u2019t use an additional isolation tool like Python virtual environments, or a version manager like nvm or rvm; just install the specific thing you need.)\nUse ENV. This will work:\nFROM busybox\nENV FOO=foo2\nENV BAR=bar\nCMD echo FOO is $FOO, BAR is $BAR\n# Prints \"FOO is foo2, BAR is bar\"\nUse an entrypoint script. This typically looks like:\n#!/bin/sh\n# Read in the file of environment settings\n. /opt/wherever/env\n# Then run the CMD\nexec \"$@\"\nCOPY this script into your Dockerfile. Make it be the ENTRYPOINT; make the CMD be the thing you\u2019re actually running.\nFROM busybox\nWORKDIR /app\nCOPY entrypoint.sh .\nCOPY more_stuff .\nENTRYPOINT [\"/app/entrypoint.sh\"]\nCMD [\"/app/more_stuff/my_app\"]\nIf you care about such things, environment variables you set via this approach won\u2019t be visible in docker inspect or a docker exec debug shell; but if you docker run -it ... sh they will be visible. This is a useful and important enough pattern that I almost always use CMD in my Dockerfiles unless I\u2019m specifically trying to do first-time setup like this.",
    "How do I copy variables between stages of multi stage Docker build?": "You got 3 options: The \"ARG\" solution, the \"base\"-stage solution, and \"file\" solution.\n\"ARG\" solution\nARG version_default=v1\n\nFROM alpine:latest as base1\nARG version_default\nENV version=$version_default\nRUN echo ${version}\nRUN echo ${version_default}\n\nFROM alpine:latest as base2\nARG version_default\nRUN echo ${version_default}\n\"base\"-stage solution\nanother way is to use base container for multiple stages:\nFROM alpine:latest as base\nARG version_default\nENV version=$version_default\n\nFROM base\nRUN echo ${version}\n\nFROM base\nRUN echo ${version}\nYou can find more details here: https://github.com/moby/moby/issues/37345\n\"file\" solution\nSave the hash into a file in the first stage, copy it in the second stage, and then read it and use it there.",
    "Unable to load shared library \"libgdiplus\" - Docker [ .NET application with Aspose API]": "You're installing libgdiplus in your build container image, but not in your final container image. You need to make sure libgdiplus is installed in your final container image.\nYou can consider amending your Dockerfile like this:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.0 AS base\nRUN apt-get update && apt-get install -y libgdiplus\n\nWORKDIR /app\nFROM mcr.microsoft.com/dotnet/core/sdk:3.0 AS build\n[...]",
    "When using COPY with more than one source file, the destination must be a directory and end with a /": "In my case adding the (forward) slash at the end (following docker error message) was enough, like this:\nCOPY package*.json . # (fails!)\nCOPY package*.json ./ # (works:)",
    "How to add docker label after image is made": "It's indeed not possible to add a label to an existing image. Strictly speaking, adding a label would change the images checksum, thus its id, thus it's no longer the same image.\nBut you can build an image based on your existing image with a label added, and then tag this image with the name of the previously existing image. Technically it adds a layer on top of your existing image and thus just \"overrides\" previous labels.\nIt's also possible to do this with a single command. Given you want to add a label to the image \"debian:latest\", you build FROM that image and tag the new image at the same time.\necho \"FROM debian:latest\" | docker build --label my_new_label=\"arbitrary value\" -t \"debian:latest\" -\nProof that \"adding\" the label worked:\n$ docker inspect -f \"{{json .Config.Labels }}\" debian:latest\n{\"my_new_label\":\"arbitrary value\"}",
    "What happens when a volume links an existing populated host and container dir": "When you run a container and mount a volume from the host, all you see in the container is what is on the host - the volume mount points at the host directory, so if there was anything in the directory in the image it gets bypassed.\nWith an image from this Dockerfile:\nFROM ubuntu\nWORKDIR /vol\nRUN touch /vol/from-container\nVOLUME /vol\nWhen you run it without a host mount, the image contents get copied into the volume:\n> docker run vol-test ls /vol\nfrom-container \nBut mount the volume from the host and you only see the host's content:\n> ls $(pwd)/host\nfrom-host\n> docker run -v $(pwd)/host:/vol vol-test ls /vol\nfrom-host\nAnd no, you don't need the VOLUME instruction. The behaviour is the same without it.",
    "Is there a way to lint the Dockerfile?": "Try:\nEither the Haskell Dockerfile Linter (\"hadolint\"), also available online. hadolint parses the Dockerfile into an AST and performs checking and validation based on best practice Docker images rules. It also uses Shellcheck to lint the Bash code on RUN commands.\nOr dockerlinter (node.js-based).\nI've performed a simple test against of a simple Docker file with RUN, ADD, ENV and CMD. dockerlinter was smart about grouping the same violation of rules together but it was not able to inspect as thorough as hadolinter possibly due to the lack of Shellcheck to statically analyze the Bash code.\nAlthough dockerlinter falls short in the scope it can lint, it does seem to be much easier to install. npm install -g dockerlinter will do, while compiling hadolinter requires a Haskell compiler and build environment that takes forever to compile.\n$ hadolint ./api/Dockerfile\nL9 SC2046 Quote this to prevent word splitting.\nL11 SC2046 Quote this to prevent word splitting.\nL8 DL3020 Use COPY instead of ADD for files and folders\nL10 DL3020 Use COPY instead of ADD for files and folders\nL13 DL3020 Use COPY instead of ADD for files and folders\nL18 DL3020 Use COPY instead of ADD for files and folders\nL21 DL3020 Use COPY instead of ADD for files and folders\nL6 DL3008 Pin versions in apt get install. Instead of `apt-get install <package>` use `apt-get install <package>=<version>`\nL6 DL3009 Delete the apt-get lists after installing something\nL6 DL3015 Avoid additional packages by specifying `--no-install-recommends`\n\n$ dockerlint ./api/Dockerfile\nWARN:  ADD instruction used instead of COPY on line 8, 10, 13, 18, 21\nERROR: ./api/Dockerfile failed.\nUpdate in 2018. Since hadolint has the official Docker repository now, you can get the executable quickly:\nid=$(docker create hadolint/hadolint:latest)\ndocker cp \"$id\":/bin/hadolint .\ndocker rm \"$id\"\nor you can use this command\ndocker container run --rm -i hadolint/hadolint hadolint - < Dockerfile\nThis is a statically compiled executable (according to ldd hadolint), so it should run regardless of installed libraries. A reference on how the executable is built: https://github.com/hadolint/hadolint/blob/master/docker/Dockerfile.",
    "Logrotate - nginx logs not rotating inside docker container": "As stated on the edit on my question the problem was that CMD from nginx:1.11 was only starting the nginx process. A work around is to place the following command on my Dockerfile\nCMD service cron start && nginx -g 'daemon off;'\nThis will start nginx as nginx:1.11 starts it and well as start the cron service.\nThe Dockerfile would look something like:\nFROM nginx:1.11\n\n# Remove sym links from nginx image\nRUN rm /var/log/nginx/access.log\nRUN rm /var/log/nginx/error.log\n\n# Install logrotate\nRUN apt-get update && apt-get -y install logrotate\n\n# Copy MyApp nginx config\nCOPY config/nginx.conf /etc/nginx/nginx.conf\n\n#Copy logrotate nginx configuration\nCOPY config/logrotate.d/nginx /etc/logrotate.d/\n\n# Start nginx and cron as a service\nCMD service cron start && nginx -g 'daemon off;'",
    "Dockerfile ARG substitution in a string in RUN command": "Docker doesn't expand ARG values in the RUN command. Instead, it injects the ARG as an environment variable. The shell itself expands the variable, and all of the Linux shells I've used behave differently based on the type of quote.\nThe single quotes direct the shell not to expand anything, and you only need to escape the single quotes and escape characters. While the double quotes include variable expansion along with many other escape characters. See the man page on your shell for more details.\nSo the solution as you've already found is:\nRUN echo \"Hello $w\"",
    "Docker: Set container name inside Dockerfile": "The Dockerfile is for creating images not containers.\nYou can now give names to your containers using the new --name flag for docker run.\nIf --name is not provided Docker will automatically generate an alphanumeric string for the container name.",
    "You installed esbuild on another platform than the one you're currently using": "You've copied node_modules from your local environment to the container. Locally you have packages for the darwin-arm64 arch, but inside the container, it is a Linux system that requires packages for linux-arm64.\nTo avoid such errors you should not copy node_modules to the container.\nAll you need is to add node_modules to .dockerignore file",
    "Docker Compose + Spring Boot + Postgres connection": "Each container has its own network interface with its own localhost. So change how Java points to Postgres:\nspring.datasource.url=jdbc:postgresql://localhost:5432/sample\nTo:\nspring.datasource.url=jdbc:postgresql://db:5432/sample\ndb will resolve to the proper Postgres IP.\nBonus. With docker-compose you don't need to build your image by hand. So change:\nweb:\n  image: myuser/manager:latest\nTo:\nweb:\n  build: .",
    "$(uname -a) returning the same in docker host or any docker container": "Docker uses the host operating system kernel, there is no custom or additional kernel inside the container. All containers running on the machine are sharing this \"host\" kernel.\nSee for more information this question on SuperUser.",
    "How to handle specific hostname like -h option in Dockerfile": "This isn't generally possible in a Dockerfile.\nDepending on the software, you might be able to do some kind of work-around. For example, you could try something like\nRUN echo $(grep $(hostname) /etc/hosts | cut -f1) my.host.name >> /etc/hosts && install-software\nBy setting the hostname within the same RUN command as you install the software, it'll happen inside the same layer of the container. Docker will later overwrite the hostname and you'll have to set it anew when running, but your software might be OK with that.\nIf you have to do a lot of this, you might try Packer for building containers. It can build Docker containers, but doesn't use multiple layers. This makes it slower to rebuild, faster to download the built images, and makes it more convenient to do multiple operations on an image before freezing it into a container.",
    "Dockerfile - How to pass an answer to a prompt post apt-get install?": "This answer has an explanation for the difference between \"assume yes\" and a non-interactive mode.\nI also found an example of a Dockerfile that installs jackd2 here, and it's setting DEBIAN_FRONTEND to 'noninteractive' before installing jackd2.",
    "what does VOLUME command do in Dockerfile? [duplicate]": "The VOLUME command will specify a mount point in the container. This mount point will be mapped to a location on the host that is either specified when the container is created or (when not specified) chosen automatically from a directory created in /var/lib/docker/volumes.\nIf the directory chosen as the mount point contains any files then these files will be copied into this volume. The advantage over mkdir is that it will persist the files to a location on the host machine after the container is terminated.\nIt appears some people have questioned why you would use the VOLUME command since it creates an anonymous volume. Anonymous volumes don't have much use any more and are basically an artifact of the early days of Docker before volumes could be named. You would normally specify the volume name when creating the container:\ndocker container run -v my-volume:/data image_tag\nIn this example, /data is the mount point in the container and my-volume is the volume on the local host. If my-volume does not exist when this command is run then it is created on the local host.",
    "How to output a multiline string in Dockerfile with a single command": "There is another question similar to this with a solution: How to write commands with multiple lines in Dockerfile while preserving the new lines?\nThe answer to this question is more particular in how to use multiline strings in bash rather than how to use Docker.\nFollowing this solution you may accomplish what you want to do as shown below:\nRUN echo $' \\n\\\n*****first row ***** \\n\\\n*****second row ***** \\n\\\n*****third row ***** ' >> /home/myfile\nMore info about this leading dollar sign here: How does the leading dollar sign affect single quotes in Bash?\nNote that this syntax relies on the run command using /bin/bash, not /bin/sh.",
    "What does Docker STOPSIGNAL do?": "SIGTERM is the default signal sent to containers to stop them: https://docs.docker.com/engine/reference/commandline/stop/\nSTOPSIGNAL does allow you to override the default signal sent to the container. Leaving it out of the Dockerfile causes no harm - it will remain the default of SIGTERM.\nThis being said, it is unclear why the author has explicitly defined the STOPSIGNAL as SIGTERM.\nLooking at this commit, we can see that the STOPSIGNAL used to be set to SIGQUIT.\nMy guess is that they left it in explicitly for documentation's sake after making the change.\nDiscussion of the change here: https://github.com/nginxinc/docker-nginx/issues/167",
    "determine OS distribution of a docker image": "The Filesystem Hierarchy Standard has a standard definition for /etc/os-release, which should be available on most distributions:\nThe /etc/os-release and /usr/lib/os-release files contain operating system identification data.\nThe basic file format of os-release is a newline-separated list of environment-like shell-compatible variable assignments. It is possible to source the configuration from shell scripts.\nThis means you can just source /etc/os-release and use $NAME or $ID to identify the distribution. As an example, on Fedora it looks like this:\n% source /etc/os-release\n% echo $NAME\nFedora\n% echo $ID\nfedora\nOn Debian:\n% source /etc/os-release\n% echo $NAME\nDebian GNU/Linux\n% echo $ID\ndebian",
    "How can I run script automatically after Docker container startup": "The image itself has an entrypoint ENTRYPOINT [\"/run/entrypoint.sh\"] specified in the Dockerfile. You can replace it by your own script. So for example create a new script, mount it and first call /run/entrypoint.sh and then wait for start of elasticsearch before running your init_sg.sh.",
    "How to override the CMD command in the docker run line": "The right way to do it is deleting cmd [\"...\"]\n docker run --name test test/test-backend /srv/www/bin/gunicorn.sh",
    "Is there a more elegant way to copy specific files using Docker COPY to the working directory?": "2021: with BuildKit, see \".NET package restore in Docker cached separately from build\" from Palec.\n2018: Considering that wildcard are not well-supported by COPY (moby issue 15858), you can:\neither experiment with adding .dockerignore files in the folder you don't want to copy (while excluding folders you do want): it is cumbersome\nor, as shown here, make a tar of all the folders you want\nHere is an example, to be adapted in your case:\nfind .. -name '*.csproj' -o -name 'Finomial.InternalServicesCore.sln' -o -name 'nuget.config' \\\n  | sort | tar cf dotnet-restore.tar -T - 2> /dev/null\nWith a Dockerfile including:\nADD docker/dotnet-restore.tar ./\nThe idea is: the archive gets automatically expanded with ADD.\nThe OP sturmstrike mentions in the comments \"Optimising ASP.NET Core apps in Docker - avoiding manually copying csproj files (Part 2)\" from Andrew Lock \"Sock\"\nThe alternative solution actually uses the wildcard technique I previously dismissed, but with some assumptions about your project structure, a two-stage approach, and a bit of clever bash-work to work around the wildcard limitations.\nWe take the flat list of csproj files, and move them back to their correct location, nested inside sub-folders of src.\n# Copy the main source project files\nCOPY src/*/*.csproj ./  \nRUN for file in $(ls *.csproj); do mkdir -p src/${file%.*}/ && mv $file src/${file%.*}/; done\nL01nl suggests in the comments an alternative approach that doesn't require compression: \"Optimising ASP.NET Core apps in Docker - avoiding manually copying csproj files\", from Andrew Lock \"Sock\".\nFROM microsoft/aspnetcore-build:2.0.6-2.1.101 AS builder\nWORKDIR /sln\n\nCOPY ./*.sln ./NuGet.config  ./\n\n# Copy the main source project files\nCOPY src/*/*.csproj ./\nRUN for file in $(ls *.csproj); do mkdir -p src/${file%.*}/ && mv $file src/${file%.*}/; done\n\n# Copy the test project files\nCOPY test/*/*.csproj ./\nRUN for file in $(ls *.csproj); do mkdir -p test/${file%.*}/ && mv $file test/${file%.*}/; done\n\nRUN dotnet restore\n\n# Remainder of build process\nThis solution is much cleaner than my previous tar-based effort, as it doesn't require any external scripting, just standard docker COPY and RUN commands.\nIt gets around the wildcard issue by copying across csproj files in the src directory first, moving them to their correct location, and then copying across the test project files.",
    "Docker: Error response from daemon: OCI runtime create failed: container_linux.go:296:": "Docker is telling you that the command hit an error. It is trying to run the node image with the command -w. Since -w is not a command, it throws this error.\nThis is because you have written node in a place you probably didn't mean to.\nYour command is being interpreted like this:\ndocker run -p [port_info] -v [volume_info] node [command]\nYou can rewrite your command like so and it should work fine:\ndocker run -p 8085:3000 -v /home/joel/workspace/plural_docker_webdev:/var/www -w \"/var/www\" node npm start",
    "Correct way to deploy WAR files in docker image": "You should actually ALWAYS deploy the exploded .war.\nThere are two elements of speed to think about here:\nHow fast is it to be able to push up your image to a container repository?\nand\nHow quickly can a new instance of my container start serving requests? (important in an elastic-scaling environment)\nThe answer to both is the same: You are better off exploding the .war file when creating your container and NOT copying the .war file to it.\nThis has the following two very positive effects:\nIt makes the differences between container versions much smaller, and so your upload time is less.\nIt means that, when dynamically scaling to meet application demand, your new container instances don't have to unzip your .war file before they can start responding to requests.\nFor those of us burdened by slow-upload connections, it's also a great idea to use a CI server or even a cloud-hosted VM to build and push your docker images to dockerhub or another container registry. That way you can take advantage of gigabit-scale upload speeds.",
    "appSettings.json for .NET Core app in Docker?": "Try replacing this line:\nENV ASPNET_ENV Development\nWith this:\nENV ASPNETCORE_ENVIRONMENT Development\nYour original Environment Variable name was used in older .NET Core, but has been changed. It can be a pain finding tutorials, etc. for .NET Core because of all of the changes that have happened since it first started!\nDon't get me started on project.json files!\nMore info:\nhttps://learn.microsoft.com/en-us/aspnet/core/fundamentals/configuration https://learn.microsoft.com/en-us/aspnet/core/fundamentals/environments",
    "Failed to install gcc on Python-3.7-alpine docker container": "For me installation of these packages within docker container helped:\nRUN apk update && apk add python3-dev \\\n                        gcc \\\n                        libc-dev",
    "module 'numpy' has no attribute 'object' [closed]": "Since version 1.24 of numpy, np.object is deprecated, and needs to be replaced with object (cf. numpy release notes).\nYou either need to update this in your code, or another package you're using needs to be updated (not possible to answer without more information).\nOne (dirty) workaround for now would be to fix your numpy version to the last version still supporting np.object with pip install numpy==1.23.4",
    "Multiple Docker build args from docker-compose .env file": "Facepalm \ud83e\udd26\u200d\u2640\ufe0f - This is working perfectly. I was putting the - DB_PWD=$DB_PWD argument under the wrong service in my docker-compose.yaml file. I will leave this here as a reference on how to use the .env file with docker build arguments -- and as a reminder to my self that I'm an idiot. I'm embarrassed --100 SOF reputation",
    "How to pass environment variables from docker-compose into the NodeJS project?": "Use process.env in node.js code, like this\nprocess.env.BACKEND_SERVER\nMention your variable in docker-compose file.\nversion: \"3\"\nservices:\n  client-side-app:\n    image: my-client-side-docker-image\n    environment:\n      - BACKEND_SERVER=\"here we need to enter backend server\"\n    ports:\n      - \"8080:8080\"\n  server-side-app:\n    image: my-server-side-docker-image\n    ports:\n      - \"3000:3000\"",
    "Dockerfile \"RUN chmod\" not taking effect": "You should set the owner directly when you copy the files:\nFROM joomla:3.9-php7.2-apache\n\nRUN apt-get update \\\n&& apt-get install -y apt-utils vim curl\n\nCOPY --chown=www-data:www-data ./joomla_html /var/www/html\n\nRUN chmod -R 765 /var/www/html/\n\nCOPY ./docker/php.ini /usr/local/etc/php/conf.d/php-extras.ini\n\nEXPOSE 80",
    "Purpose of FROM command - Docker file": "Containers don't run a full OS, they share the kernel of the host OS (typically, the Linux kernel). That's the \"Host Operating System\" box in your right image.\nThey do provide what's called \"user space isolation\" though - roughly speaking, this means that every container manages its own copy of the part of the OS which runs in user mode- typically, that's a Linux distribution such as Ubuntu. In your right image, that would be contained in the \"Bins/Libs\" box.\nYou can leave out the FROM line in your Dockerfile, or use FROM scratch, to create a blank base image, then add all the user mode pieces on top of a blank kernel yourself.",
    "Dockerfile, how install snap, snapd: unrecognized service": "first of all, you don't want to install the \"snap\" package, as it is not related to \"snapd\". Secondly, myself stumbled across this issue of installing snapd within a docker container: TLDR; Running snapd that way is currently not supported.\nBut that question has been asked already at the snapcraft forums. One of snapd's dependencies is systemd and the snapd-service isn't properly initialized without a reboot or relogin. That is the required procedure according to the documentation across all distributions, but obviously isn't an option within docker.\nAt least this open question replicates your question most: unable-to-install-snapcraft-snap-in-docker-image-ubuntu-19-10\nAnd Evan at the snapcraft forum here posted an approach, that I couldn't get to work either.\nThe only approach that might work is similar to running docker inside of docker, i.e.:\ninstall snapd on the docker host\nmount the snapd-socket at runtime into the container that has snapd installed.\nBut same warnings/side-effects apply as they do to running docker-in-docker.",
    "Docker COPY with folder wildcards": "If you use the dotnet command to manage your solution you can use this piece of code:\nCopy the solution and all project files to the WORKDIR\nList projects in the solution with dotnet sln list\nIterate the list of projects and move the respective *proj files into newly created directories.\nCOPY *.sln ./\nCOPY */*/*.*proj ./\nRUN dotnet sln list | \\\n      tail -n +3 | \\\n      xargs -I {} sh -c \\\n        'target=\"{}\"; dir=\"${target%/*}\"; file=\"${target##*/}\"; mkdir -p -- \"$dir\"; mv -- \"$file\" \"$target\"'",
    "launch a CAT command unix into Dockerfile": "Based on this comment to an issue posted on Github, this works:\nRUN echo 'All of your\\n\\\nmultiline that you ever wanted\\n\\\ninto a dockerfile\\n'\\\n>> /etc/example.conf",
    "Create Docker container with both Java and Node.js": "The best way for you is to take java (which is officially deprecated and it suggests you use openjdk image) and install node in it.\nSo, start with\nFROM openjdk:latest\nThis will use the latest openjdk image, which is 8u151 at this time. Then install node and other dependencies you might need:\nRUN apt-get install -y curl \\\n  && curl -sL https://deb.nodesource.com/setup_9.x | bash - \\\n  && apt-get install -y nodejs \\\n  && curl -L https://www.npmjs.com/install.sh | sh\nYou might want to install things like grunt afterwards, so this might come in handy as well.\nRUN npm install -g grunt grunt-cli\nIn total you will get the following Dockerfile:\nFROM openjdk:latest\n\nRUN apt-get install -y curl \\\n  && curl -sL https://deb.nodesource.com/setup_9.x | bash - \\\n  && apt-get install -y nodejs \\\n  && curl -L https://www.npmjs.com/install.sh | sh \\\nRUN npm install -g grunt grunt-cli\nYou may clone the Dockerfile from my gitlab repo here",
    "Docker error with read-only file system unknown": "If I remember correct after clearing space and Restarting Docker from scratch worked for me.",
    "Error running docker container: starting container process caused \"exec: \\\"python\\\": executable file not found in $PATH\": unknown": "There is no /usr/bin/python in a docker image built by the code above. But there is /usr/bin/python3. So you could either use python3 directly as your ENTRYPOINT or create a symlink.",
    "What is the location of redis.conf in official docker image?": "The default image from redis does not have a redis.conf.\nHere is the link for the image on dockerhub. https://hub.docker.com/_/redis/\nYou will have to copy it to image or have it mapped on the host using a volume mapping.",
    "Docker - Execute command after mounting a volume": "I generally agree with Chris's answer for local development. I am going to offer something that combines with a recent Docker feature that may set a path for doing both local development and eventual production deployment with the same image.\nLet's first start with the image that we can build in a manner that can be used for either local development or deployment somewhere that contains the code and dependencies. In the latest Docker version (17.05) there is a new multi-stage build feature that we can take advantage of. In this case we can first install all your Composer dependencies to a folder in the build context and then later copy them to the final image without needing to add Composer to the final image. This might look like:\nFROM composer as composer\nCOPY . /app\nRUN composer install --ignore-platform-reqs --no-scripts\n\nFROM php:fpm\nWORKDIR /var/www/root/\nRUN apt-get update && apt-get install -y \\\n        libfreetype6-dev \\\n        libjpeg62-turbo-dev \\\n        libmcrypt-dev \\\n        libpng12-dev \\\n        zip \\\n        unzip \\\n    && docker-php-ext-install -j$(nproc) iconv mcrypt \\\n    && docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/ \\\n    && docker-php-ext-install -j$(nproc) gd \\\n    && docker-php-ext-install mysqli \\\n    && docker-php-ext-enable opcache\nCOPY . /var/www/root\nCOPY --from=composer /app/vendor /var/www/root/vendor\nThis removes all of Composer from the application image itself and instead uses the first stage to install the dependencies in another context and copy them over to the final image.\nNow, during development you have some options. Based on your docker-compose.yml command it sounds like you are mounting the application into the container as .:/var/www/root. You could add a composer service to your docker-compose.yml similar to my example at https://gist.github.com/andyshinn/e2c428f2cd234b718239. Here, you just do docker-compose run --rm composer install when you need to update dependencies locally (this keeps the dependencies build inside the container which could matter for native compiled extensions, especially if you are deploying as containers and developing on Windows or Mac).\nThe other option is to just do something similar to what Chris has already suggested, and use the official Composer image to update and manage dependencies when needed. I've done something like this locally before where I had private dependencies on GitHub which required SSH authentication:\ndocker run --rm --interactive --tty \\\n            --volume $PWD:/app:rw,cached \\\n            --volume $SSH_AUTH_SOCK:/ssh-auth.sock \\\n            --env SSH_AUTH_SOCK=/ssh-auth.sock \\\n            --volume $COMPOSER_HOME:/composer \\\n            composer:1.4 install --ignore-platform-reqs --no-scripts\nTo recap, the reasoning for this method of building the image and installing Composer dependencies using an external container / service:\nPlatform specific dependencies will be built correctly for the container (Linux architecture vs Windows or Mac).\nNo Composer or PHP is required on your local computer (it is all contained inside Docker and Docker Compose).\nThe initial image you built is runnable and deployable without needing to mount code into it. In development, you are just overriding the /var/www/root folder with a local volume.",
    "How to bring credentials into a Docker container during build": "A few new Docker features make this more elegant and secure than it was in the past. The new multi-phase builds let us implement the builder pattern with one Dockerfile. This method puts our credentials into a temporary \"builder\" container, and then that container builds a fresh container that doesn't hold any secrets.\nYou have choices for how you get your credentials into your builder container. For example:\nUse an environment variable: ENV creds=user:pass and curl https://$creds@host.com\nUse a build-arg to pass credentials\nCopy an ssh key into the container: COPY key /root/.ssh/id_rsa\nUse your operating system's own secure credentials using Credential Helpers\nMulti-phase Dockerfile with multiple FROMs:\n## Builder\nFROM alpine:latest as builder\n#\n# -- insert credentials here using a method above --\n#\nRUN apk add --no-cache git\nRUN git clone https://github.com/some/website.git /html\n\n## Webserver without credentials\nFROM nginx:stable\nCOPY --from=builder /html /usr/share/nginx/html",
    "Docker multistage: how to COPY built files between stages?": "Well, apparently, I was mislead by the COPY step used in the first stage in the doc example. In my case, this is actually useless, and I can just COPY --from=haskell in my second stage, without any COPY in the first stage.\nThe following Dockerfile builds without issues:\nFROM haskell:8.6.5 as haskell\nRUN git clone https://gitlab+deploy-token-75:sakyTxfe-PxPHDwqsoGm@gitlab.pasteur.fr/bli/bioinfo_utils.git\nWORKDIR bioinfo_utils/remove-duplicates-from-sorted-fastq/Haskell\nRUN stack --resolver ghc-8.6.5 build && \\\n    stack --resolver ghc-8.6.5 install --local-bin-path .\n\nFROM python:3.7-buster\nRUN python3.7 -m pip install snakemake\nRUN mkdir -p /opt/bin\nCOPY --from=haskell /bioinfo_utils/remove-duplicates-from-sorted-fastq/Haskell/remove-duplicates-from-sorted-fastq /opt/bin/remove-duplicates-from-sorted-fastq\nCMD [\"/bin/bash\"]",
    "How to download and unzip in Dockerfile": "Best to use a multistage docker build. You will need the latest version of docker and buildkit enabled. Then do something along these lines\n# syntax=docker/dockerfile:1\nFROM alpine:latest AS unzipper\nRUN apk add unzip wget curl\nRUN mkdir /opt/ ; \\\n  curl <some-url> | tar xvzf - -C /opt\n\nFROM wordpress:fpm\nCOPY  --from=unzipper /opt/ /var/www/html/wp-content/themes/wordpress/    \nEven better is if there is a Docker image built already with the stuff in you want you just need the 'copy --from' line and give it the image name.\nFinally dont worry about any mess in the 1st stage as its discarded when the build completes, so the fact its alpine, and not using no-cache is irrelevant, and none of the installed packages end up in the final image",
    "How Docker calculates the hash of each layer? Is it deterministic?": "Thanks @thaJeztah. Answer is in https://gist.github.com/aaronlehmann/b42a2eaf633fc949f93b#id-definitions-and-calculations\nlayer.DiffID: ID for an individual layer\nCalculation: DiffID = SHA256hex(uncompressed layer tar data)\nlayer.ChainID: ID for a layer and its parents. This ID uniquely identifies a filesystem composed of a set of layers.\nCalculation:\nFor bottom layer: ChainID(layer0) = DiffID(layer0)\nFor other layers: ChainID(layerN) = SHA256hex(ChainID(layerN-1) + \" \" + DiffID(layerN))\nimage.ID: ID for an image. Since the image configuration references the layers the image uses, this ID incorporates the filesystem data and the rest of the image configuration.\nCalculation: SHA256hex(imageConfigJSON)",
    "Docker add warfile to official Tomcat image": "Reading from the documentation of the repo you would do something like that\nFROM tomcat\nMAINTAINER xyz\n\nADD your.war /usr/local/tomcat/webapps/\n\nCMD [\"catalina.sh\", \"run\"]\nThen build your image with docker build -t yourName <path-to-dockerfile>\nAnd run it with:\ndocker run --rm -it -p 8080:8080 yourName\n--rm removes the container as soon as you stop it\n-p forwards the port to your host (or if you use boot2docker to this IP)\n-it allows interactive mode, so you see if something get's deployed",
    "How to dynamically change the docker's base image": "From Docker version 17.05, you can do something like this:\nARG MYAPP_IMAGE=myorg/myapp:latest\nFROM $MYAPP_IMAGE\nYou can provide MYAPP_IMAGE as a command line paramether:\ndocker build -t container_tag --build-arg MYAPP_IMAGE=localimage:latest .\nMore info here: https://www.jeffgeerling.com/blog/2017/use-arg-dockerfile-dynamic-image-specification",
    "Docker dotnet watch run error: Unable to bind to https://localhost:5000 on the IPv6 loopback interface": "Just ran into this problem myself. I don't think dotnet watch run plays nicely with localhost type urls. Try setting your hosting url to https://0.0.0.0:5000 in your container.\nIn the dockerfile with:\nENTRYPOINT [ \"dotnet\", \"watch\", \"run\", \"--no-restore\", \"--urls\", \"https://0.0.0.0:5000\"]\nOr in launchSettings.json like:\n{\n  \"profiles\": {\n    \"[Put your project name here]\": {\n      \"commandName\": \"Project\",\n      \"launchBrowser\": true,\n      \"environmentVariables\": {\n        \"ASPNETCORE_ENVIRONMENT\": \"Development\",\n        \"DOTNET_USE_POLLING_FILE_WATCHER\": \"true\"\n      },\n      \"applicationUrl\": \"https://0.0.0.0:5000/\"\n    }\n  }\n}\nNow to get it to automatically reload from within the container you have to use the polling file watcher. That's what the second environment variable is for. (This is pretty common, you've got to do this with webpack, angular, etc).\nIn your case, you need to change the esportsapp.volume to a directory on your host:\nvolumes:\n  - ./:/app\nThat will map the /app volume in your container to the docker-compose directory. The problem you're facing is that the app is built in a volume on your project's default docker-compose network, so when you change a file in the source directory, it's not actually changing in that volume. With this fix, however, you'll run into the problem of the dotnet restore and dotnet watch inside the container changing your host's files. There's a fix for all of that, if you're interested...\nMy Usual .Net Core App Docker setup\nTo debug, run: docker-compose -f run.yml up --build\nTo build a release: docker-compose -f build.yml up --build\nProject structure\n/                                               # source control root\n/build.yml                                      # docker-compose file for building a release\n/run.yml                                        # docker-compose file for running locally & debugging\n/project                                        # an application\n/project/build.Dockerfile                       # the docker container that will build \"project\" for release\n/project/run.Dockerfile                         # the docker container that will build and run \"project\" locally for debugging\n/project/.dockerignore                          # speeds up container builds by excluding large directories like \"packages\" or \"node_modules\"\n/project/src                                    # where I hide my source codez\n/project/src/Project.sln\n/project/src/Project/Project.csproj\n/project/src/Project/Directory.Build.props      # keeps a docker mapped volume from overwriting .dlls on your host\n/project/src/Project.Data/Project.Data.csproj   # typical .Net project structure\n/web-api                                        # another application...\nDirectory.Build.props (put this in the same folder as your .csproj, keeps your dotnet watch run command from messing with the source directory on your host)\n<Project>\n\n  <PropertyGroup>\n    <DefaultItemExcludes>$(DefaultItemExcludes);$(MSBuildProjectDirectory)/obj/**/*</DefaultItemExcludes>\n    <DefaultItemExcludes>$(DefaultItemExcludes);$(MSBuildProjectDirectory)/bin/**/*</DefaultItemExcludes>\n  </PropertyGroup>\n\n  <PropertyGroup Condition=\"'$(DOTNET_RUNNING_IN_CONTAINER)' == 'true'\">\n    <BaseIntermediateOutputPath>$(MSBuildProjectDirectory)/obj/container/</BaseIntermediateOutputPath>\n    <BaseOutputPath>$(MSBuildProjectDirectory)/bin/container/</BaseOutputPath>\n  </PropertyGroup>\n\n  <PropertyGroup Condition=\"'$(DOTNET_RUNNING_IN_CONTAINER)' != 'true'\">\n    <BaseIntermediateOutputPath>$(MSBuildProjectDirectory)/obj/local/</BaseIntermediateOutputPath>\n    <BaseOutputPath>$(MSBuildProjectDirectory)/bin/local/</BaseOutputPath>\n  </PropertyGroup>\n\n</Project>\nrun.yml (docker-compose.yml for debugging)\nversion: \"3.5\"\nservices:\n  project:\n    build:\n      context: ./project\n      dockerfile: run.Dockerfile\n    ports:\n      - 5000:80\n    volumes:\n      - ./project/src/Project:/app\nrun.Dockerfile (the Dockerfile for debugging)\nFROM microsoft/dotnet:2.1-sdk\n\n# install the .net core debugger\nRUN apt-get update\nRUN apt-get -y --no-install-recommends install unzip\nRUN apt-get -y --no-install-recommends install procps\nRUN rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l /vsdbg\n\nVOLUME /app\nWORKDIR /app\n\nCMD dotnet watch run --urls http://0.0.0.0:80\nbuild.yml (the docker-compose.yml for building release versions)\nversion: \"3.5\"\nservices:\n  project:\n    build:\n      context: ./project\n      dockerfile: build.Dockerfile\n    volumes:\n      - ./project:/app\nbuild.Dockerfile (the Dockerfile for building release versions)\nFROM microsoft/dotnet:2.1-sdk\n\nVOLUME /app\n\n# restore as a separate layer to speed up builds\nWORKDIR /src\nCOPY src/Project/Project.csproj .\nRUN dotnet restore\n\nCOPY src/Project/ .\nCMD dotnet publish -c Release -o /app/out/",
    "Connecting two docker containers [duplicate]": "Using --link was the only way of connecting containers before the advent of docker networks. These provide a \"cleaner\" solution to the problem of inter-container communication and at the same time solves 2 of the major limits of links:\nrestart of linked container breaks the link\nlinks are not supported between containers running on different hosts\nUsing docker network you would use the --net option to start the containers on the specified network:\ndocker network create example\ndocker run -d --net example --name container1 <image>\ndocker run -d --net example --name container2 <image>\nAt this point the 2 container are mutually reachable via the address <container-name>.example: that is container1.example and container2.example.",
    "Docker File: Chmod on Entrypoint Script": "Docker will copy files into the container with the permissions of their source. If you strip the Linux executable bits somewhere in the chain of pushing to your code repo, or on your build host, then you'll need to add those execute permissions back. I've seen this issue most often reported by Windows users, who are downloading code to a filesystem that doesn't support the Linux permission bits. Hopefully we'll get a COPY --chmod solution soon that will eliminate the need for an extra layer.",
    "Should Dockerfile execute \"npm install\" and \"npm run build\" or should it only copy those files over?": "Building inside a container guarantees a predictable and reproducible build artifact. Running npm install on macOS and Linux can produce different node_modules, for example node-gyp.\nPeople often build node_modules with multi-stage build (if the actual container you're trying to build is not a Node.js application). That is to say, your actual nginx application per se does not depend on Node.js, but the node_modules directory and its containing files instead. So we generate node_modules in a node container, and copy it to the new container (nginx).\nThus, everyone building with the multi-stage Dockerfile will produce exact same container. If you copy your local node_modules into a container during build, other coworkers will not be able to predict the content of node_modules.",
    "Vite: Could not resolve entry module (index.html)": "Vite uses an html page as an entry point by default. So you either need to create one or if you don't have an html page, you can use it in \"library mode\".\nhttps://vitejs.dev/guide/build.html#library-mode\nFrom the docs:\n// vite.config.js\nconst path = require('path')\nconst { defineConfig } = require('vite')\n\nmodule.exports = defineConfig({\n  build: {\n    lib: {\n      entry: path.resolve(__dirname, 'lib/main.js'),\n      name: 'MyLib',\n      fileName: (format) => `my-lib.${format}.js`\n    },\n    rollupOptions: {\n      // make sure to externalize deps that shouldn't be bundled\n      // into your library\n      external: ['vue'],\n      output: {\n        // Provide global variables to use in the UMD build\n        // for externalized deps\n        globals: {\n          vue: 'Vue'\n        }\n      }\n    }\n  }\n})",
    "Docker swarm: 'build' configuration in docker compose file ignored during stack deployment": "Short answer is, you can not use the build command with docker stack deploy.\nFrom the docs:\nNote: The docker stack command build option is ignored when deploying a stack in swarm mode with a (version 3) Compose file. The docker stack command accepts only pre-built images.\nAn alternative is to build the docker image before deploying the your swarm cluster.\nUse the docker build command to create the docker image; Push the created image to a (public or private) docker registry; and reference it in your docker compose file.",
    "Using ARG and ENV in Dockerfile": "In a Dockerfile, each FROM line starts a new image, and generally resets the build environment. If your image needs to specify ARGs, they need to come after the FROM line; if it's a multi-stage build, they need to be repeated in each image as required. ARG before the first FROM are only useful to allow variables in the FROM line and to supply defaults, but can't be used otherwise.\nThis is further discussed under Understand how ARG and FROM interact in the Dockerfile documentation.\nFROM centos:7\n\n# _After_ the FROM line\nARG my_arg\nARG other_arg=other_default\n...",
    "Install package in running docker container": "You can use docker commit:\nStart your container sudo docker run IMAGE_NAME\nAccess your container using bash: sudo docker exec -it CONTAINER_ID bash\nInstall whatever you need inside the container\nExit container's bash\nCommit your changes: sudo docker commit CONTAINER_ID NEW_IMAGE_NAME\nIf you run now docker images, you will see NEW_IMAGE_NAME listed under your local images.\nNext time, when starting the docker container, use the new docker image you just created:\nsudo docker run **NEW_IMAGE_NAME** - this one will include your additional installations.\nAnswer based on the following tutorial: How to commit changes to docker image",
    "How to add private nuget source in dotnet dockerfile?": "Answer for year 2023\nWithout security\nThis code for WebApplication3 works just fine. We use BaGet NuGet server to have a proxy between Nuget.org and our build servers for faster loads of common packages we use.\n#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\nWORKDIR /src\nCOPY [\"WebApplication3/WebApplication3.csproj\", \"WebApplication3/\"]\n\n# !!!IMPORTANT PART HERE !!!\n\n# Add your NuGet server here\nRUN dotnet nuget add source https://nuget.yourdomain.com/v3/index.json\n# For our purposes, to hide nuget.org behind a NuGet proxy we disable its source, you can skip that\nRUN dotnet nuget disable source \"nuget.org\"\n\n# Just to see if two lines above work\nRUN dotnet nuget list source\n\nRUN dotnet restore \"WebApplication3/WebApplication3.csproj\"\n\nCOPY . .\nWORKDIR \"/src/WebApplication3\"\nRUN dotnet build \"WebApplication3.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"WebApplication3.csproj\" -c Release -o /app/publish /p:UseAppHost=false\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"WebApplication3.dll\"]\nWith basic authentication\n#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\n\n# !!!IMPORTANT PART HERE !!!\n\nARG NUGET_USERNAME\nARG NUGET_PASSWORD\n\nENV NUGET_USERNAME=${NUGET_USERNAME}\nENV NUGET_PASSWORD=${NUGET_PASSWORD}\n\n# Adds this source with basic authentication, other authentication types exist but I'm not sure if they are applicable here in Linux based container\nRUN dotnet nuget add source https://nuget.yourdomain.com/v3/index.json --name=\"Your source name\" --username ${NUGET_USERNAME} --valid-authentication-types basic --store-password-in-clear-text --password ${NUGET_PASSWORD}\n\n\nWORKDIR /src\nCOPY [\"WebApplication3/WebApplication3.csproj\", \"WebApplication3/\"]\n\nRUN dotnet nuget disable source \"nuget.org\"\n\n# Just to see if two lines above work\nRUN dotnet nuget list source\n\nRUN dotnet restore \"WebApplication3/WebApplication3.csproj\"\n\nCOPY . .\nWORKDIR \"/src/WebApplication3\"\nRUN dotnet build \"WebApplication3.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"WebApplication3.csproj\" -c Release -o /app/publish /p:UseAppHost=false\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"WebApplication3.dll\"]",
    "Why does chown increase size of docker image?": "Every step in a Dockerfile generates a new intermediate image, or \"layer\", consisting of anything that changed on the filesystem from the previous layer. A docker image consists of a collection of layers that are applied one on top of another to create the final filesystem.\nIf you have:\nRUN adduser example -D -h /example -s /bin/sh\nThen you are probably changing nothing other than a few files in /etc (/etc/passwd, /etc/group, and their shadow equivalents).\nIf you have:\nRUN adduser example -D -h /example -s /bin/sh && \\\n    chown -R example.example /lib\nThen the list of things that have changed includes, recursively, everything in /lib, which is potentially larger. In fact, in my alpine:edge container, it looks like the contents of /lib is 3.4MB:\n/ # du -sh /lib\n3.4M    /lib\nWhich exactly accounts for the change in image size in your example.\nUPDATE\nUsing your actual Dockerfile, with the npm install ... line commented out, I don't see any difference in the final image size whether or not the adduser and chown commands are run. Given:\nRUN echo \"http://nl.alpinelinux.org/alpine/edge/main\" > /etc/apk/repositories && \\\n    echo \"http://nl.alpinelinux.org/alpine/edge/testing\" >> /etc/apk/repositories && \\\n    apk add -U wget iojs && \\\n    apk upgrade && \\\n    wget -q --no-check-certificate https://ghost.org/zip/ghost-0.6.0.zip -O /tmp/ghost.zip && \\\n    unzip -q /tmp/ghost.zip -d /ghost && \\\n    cd /ghost && \\\n#    npm install --production && \\\n    sed 's/127.0.0.1/0.0.0.0/' /ghost/config.example.js > /ghost/config.js && \\\n    sed -i 's/\"iojs\": \"~1.2.0\"/\"iojs\": \"~1.6.4\"/' package.json && \\\n#   adduser ghost -D -h /ghost -s /bin/sh && \\\n#   chown -R ghost.ghost * && \\\n    npm cache clean && \\\n    rm -rf /var/cache/apk/* /tmp/*\nI get:\n$ docker build -t sotest .\n[...]\nSuccessfully built 058d9f41988a\n$ docker inspect -f '{{.VirtualSize}}' 058d9f41988a\n31783340\nWhereas given:\nRUN echo \"http://nl.alpinelinux.org/alpine/edge/main\" > /etc/apk/repositories && \\\n    echo \"http://nl.alpinelinux.org/alpine/edge/testing\" >> /etc/apk/repositories && \\\n    apk add -U wget iojs && \\\n    apk upgrade && \\\n    wget -q --no-check-certificate https://ghost.org/zip/ghost-0.6.0.zip -O /tmp/ghost.zip && \\\n    unzip -q /tmp/ghost.zip -d /ghost && \\\n    cd /ghost && \\\n#    npm install --production && \\\n    sed 's/127.0.0.1/0.0.0.0/' /ghost/config.example.js > /ghost/config.js && \\\n    sed -i 's/\"iojs\": \"~1.2.0\"/\"iojs\": \"~1.6.4\"/' package.json && \\\n    adduser ghost -D -h /ghost -s /bin/sh && \\\n    chown -R ghost.ghost * && \\\n    npm cache clean && \\\n    rm -rf /var/cache/apk/* /tmp/*\nI get:\n$ docker build -t sotest .\n[...]\nSuccessfully built 696b481c5790\n$ docker inspect -f '{{.VirtualSize}}' 696b481c5790\n31789262\nThat is, the two images are approximately the same size (the difference is around 5 Kb).\nI would of course expect the resulting image to be larger if the npm install command could run successfully (because that would install additional files).",
    "Install libssl-dev for Docker": "Some packages are built against libressl in Alpine 3.6. Try replacing line 6 in your Dockerfile with the following\nRUN apk add libressl-dev",
    "Docker CentOS image does not auto start httpd": "You need to run apache (httpd) directly - you should not use init.d script.\nTwo options:\nyou have to run apache in foreground: /usr/sbin/apache2 -DFOREGROUND ... (or /usr/sbin/httpd in CentOS)\nyou have to start all services (including apache configured as auto-run) by executing /sbin/init as entrypoint.",
    "How to build Docker with env file": "If you are using docker-compose (which now comes bundled with Docker), .env is the default filename for the file that contains variables that are made available to the parser for the docker-compose.yml file ONLY, and not to the build process or container environment variables.\nUsing the --env-file command line argument or setting the env_file variable in your docker-compose.yml file will identify the file from which to load variables into the container environment (and not the build process). You can run set from a shell in your running container to view the environment variables that are loaded.\nIt is a common antipattern to set env_file=.env in docker-compose.yml which is a very confusing thing to do as it makes the .env file provide both variables to the docker-compose.yml parser AND the running container environment - it is almost certainly the reason that you are reading this.\nIf you want variables in your .env to be available to the build process, i.e. you want to use them in your Dockerfile, then you will need to explicitly set them in docker-compose.yml AND you will need to load them at the top of your Dockerfile. This example shows the syntax to use in .env, docker-compose.yml, and Dockerfile:\n# This excerpt is from .env\n# ...\n# The quotes here are important if you have any special \n# characters and want them to work on mac, pc and linux:\nARG_1=\"value of first argument\"\nARG_2=\"value of second argument\"\nARG_3=\"value of third argument\"\n# ...\n# This excerpt is from docker-compose.yml\n# ...\n# env_file (.env.staging) is loaded in the container at runtime:\n    env_file:\n      - .env.staging\n# ...\n    build:\n      context: ./\n      dockerfile: docker/app/Dockerfile\n# The following will let you use ${ARG_x} during your build in docker/app/Dockerfile\n      args:\n        ARG_1: ${ARG_1}\n        ARG_2: ${ARG_2}\n        ARG_3: ${ARG_3}\n# ...\n# This excerpt is from /docker/app/Dockerfile\nFROM somerepo/someimage:ver as something\n# Import variables from docker-compose.yml:\nARG ARG_1=$ARG_1\nARG ARG_2=$ARG_2\nARG ARG_3=$ARG_3\n# ...\n# Now you can use ${ARG_1}, ${ARG_2} etc to reference values from your .env file\n# ...",
    "Use buildx build linux/arm64 in docker-compose file": "You can use buildx in docker-compose by setting ENV variable COMPOSE_DOCKER_CLI_BUILD=1, also if buildx is not set as default, you should add DOCKER_BUILDKIT=1:\nCOMPOSE_DOCKER_CLI_BUILD=1 DOCKER_BUILDKIT=1 docker-compose build",
    "localhost not working docker windows 10": "Most likely a different application already runs at port 80. You'll have to forward your web site to a different port, eg:\ndocker run -d -p 5000:80 --name myapp myasp\nAnd point your browser to http://localhost:5000.\nWhen you start a container you specify which inner ports will be exposed as ports on the host through the -p option. -p 80:80 exposes the inner port 80 used by web sites to the host's port 80.\nDocker won't complain though if another application already listens at port 80, like IIS, another web application or any tool with a web interface that runs on 80 by default.\nThe solution is to:\nMake sure nothing else runs on port 80 or\nForward to a different port.\nForwarding to a different port is a lot easier.\nTo ensure that you can connect to a port, use the telnet command, eg :\ntelnet localhost 5000\nIf you get a blank window immediatelly, it means a server is up and running on this port. If you get a message and timeout after a while, it means nobody is running. You anc use this both to check for free ports and ensure you can connect to your container web app.\nPS I run into this just a week ago, as I was trying to set up a SQL Server container for tests. I run 1 default and 2 named instances already, and docker didn't complain at all when I tried to create the container. Took me a while to realize what was wrong.",
    "Dockerfile COPY from a Windows file system to a docker container": "First, change your Dockerfile to:\nFROM php:7.1-apache\nLABEL maintainer=\"rburton@agsource.com\"\nCOPY MyAgsourceAPI /var/www\nThen, to go your code directory: cd Users/rburton/code.\nWithin that directory, run: docker build -t <image_name> .",
    "Docker daemon memory leak due to logs from long running process": "There is still at least one outstanding issue relating to memory leaks with logs: https://github.com/docker/docker/issues/9139",
    "Install fonts in Linux container for ASP.NET Core": "Got it. Revise the start of your Dockerfile as follows:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\n\n#Add these two lines\nRUN sed -i'.bak' 's/$/ contrib/' /etc/apt/sources.list\nRUN apt-get update; apt-get install -y ttf-mscorefonts-installer fontconfig\n\nWORKDIR /app\nEXPOSE 80\n[...]\nThe first line updates the default /etc/apt/sources.list file in the Linux OS to include the 'contrib' archive area (which is where ttf-mscorefonts-installer lives). That ensures apt-get can find it and install it as normal in the second line (along with fontconfig, which you'll also need.)\nFor the record, this page suggested using the \"fonts-liberation\" package instead of ttf-mscorefonts-installer, which you can also get working with two different lines at the start of the Dockerfile as follows:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\n\n#Add these two lines for fonts-liberation instead\nRUN apt-get update; apt-get install -y fontconfig fonts-liberation\nRUN fc-cache -f -v\n\nWORKDIR /app\nEXPOSE 80\n\n[...]",
    "Iterate in RUN command in Dockerfile": "It looks like you're using backticks. What's in backticks gets executed and the text in the backticks gets replaced by what's returned by the results.\nTry using single quotes or double quotes instead of backticks.\nTry getting rid of the backticks like so:\nRUN for i in x y z; do echo \"$i\"; done",
    "ssh key generation using dockerfile": "The problem is that ssh-keygen is not available in your container yet. This can be easily solved, for example by installing the openssl-client package on a ubuntu base image.\nThe following Dockerfile does precisely that and places a key in the container's root folder\nFROM ubuntu:latest\n\nRUN apt-get -y install openssh-client\nRUN ssh-keygen -q -t rsa -N '' -f /id_rsa\nBUT READ THIS: My strong advice is not to place keys, certificates whatsoever into the container's file system at all! This might lead to strong security risks, as essentially anyone who obtains the container image can authenticate himself at services the key is valid for; it forces you to handle container images with the same care you would treat cryptographic keys and certificates!\nHence, it is advisable to keep the keys outside of the container. This can be easily achieved by using Docker VOLUMES; and you'd simply mount a volume holding keys/containers into the Docker container when launching it.\nCREATING KEYS OUTSIDE THE CONTAINER The following Dockerfile does instead create the key once the container is started, and it may be used to create the key outside the container's file system\nFROM ubuntu:latest\nRUN apt-get -y install openssh-client \nCMD ssh-keygen -q -t rsa -N '' -f /keys/id_rsa\nFirst, build the container with the following command:\ndocker build -t keygen-container .\nStarting the container using\ndocker run -v /tmp/:/keys keygen-container\nwill create a key on the host in /tmp.",
    "How to check whether python package is installed or not in Docker?": "I figured out.\ndocker exec <container ID> pip list",
    "Create table in PostgreSQL docker image": "In the docker-entrypoint.sh script of the official docker image of postgres is written:\npsql+=( --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" )\n\n        echo\n        for f in /docker-entrypoint-initdb.d/*; do\n            case \"$f\" in\n                *.sh)     echo \"$0: running $f\"; . \"$f\" ;;\n                *.sql)    echo \"$0: running $f\"; \"${psql[@]}\" < \"$f\"; echo ;;\n                *.sql.gz) echo \"$0: running $f\"; gunzip -c \"$f\" | \"${psql[@]}\"; echo ;;\n                *)        echo \"$0: ignoring $f\" ;;\n            esac\n            echo\ndone\nSo every .sql file you want to execute inside your docker image can just be placed inside that folder. So my dockerfile looks like\nFROM postgres:9.3\nENV POSTGRES_USER docker\nENV POSTGRES_PASSWORD docker\nENV POSTGRES_DB docker\nADD CreateDB.sql /docker-entrypoint-initdb.d/\nAnd the content of my CreateDB.sql:\nCREATE TABLE web_origins (\n    client_id character varying(36) NOT NULL,\n    value character varying(255)\n);\nSo I just start my container with:\ndocker run -d my-postgres\nTo check:\ndocker exec -it 6250aee43d12 bash\nroot@6250aee43d12:/# psql -h localhost -p 5432 -U docker -d docker\npsql (9.3.13)\nType \"help\" for help.\n\ndocker=# \\c\nYou are now connected to database \"docker\" as user \"docker\".\ndocker=# \\dt\n           List of relations\n Schema |    Name     | Type  | Owner\n--------+-------------+-------+--------\n public | web_origins | table | docker\n(1 row)\nYou can find the details for mysql here in this blog.",
    "Bash brace expansion not working on Dockerfile RUN command": "You're not using brace expansion, because you're not using Bash. If you look at the documentation for the RUN command:\nRUN (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows)\nAnd also:\nNote: To use a different shell, other than \u2018/bin/sh\u2019, use the exec form passing in the desired shell. For example, RUN [\"/bin/bash\", \"-c\", \"echo hello\"]\nSo, just change the command to use the exec form and explicitly use a Bash shell:\nRUN [ \"/bin/bash\", \"-c\", \"mkdir -p /opt/seagull/{diameter-env,h248-env,http-env,msrp-env,octcap-env,radius-env,sip-env,synchro-env,xcap-env}/logs\" ]",
    "TSC not found in Docker build": "It's probably a NODE_ENV environment variable problem.\nENV NODE_ENV=production\nIf you set this way, the dependencies in devDependencies will not be installed.",
    "How to get proper docker-compose Multiline environment variables formatting?": "In your first example the last element of the first sequence of the document is a plain scalar (i.e. not having single or double quotes) that extends over multiple lines. In a plain scalar newlines are replaced by spaces (and empty lines replaced by a newline).\nSo if you want newlines within that element you should use (only showing relevant part):\n  - WORDPRESS_DB_PASSWORD=xxxxxxxxxxxxxxx\n  - WORDPRESS_DEBUG=1\n  - WORDPRESS_CONFIG_EXTRA=\n\n      define( 'WP_REDIS_CLIENT', 'predis' );\n\n      define( 'WP_REDIS_SCHEME', 'tcp' );\n\n      define( 'WP_REDIS_HOST', 'redis' );\n\n      define( 'WP_REDIS_PORT', '6379' );\n\n      define( 'WP_REDIS_PASSWORD', 'xxxxxxxxxxxxxxx' );\n\n      define( 'WP_REDIS_DATABASE', '0' );\n\n      define( 'WP_REDIS_MAXTTL', '21600' );\n\n      define( 'WP_CACHE_KEY_SALT', 'xx_ ');\n\n      define( 'WP_REDIS_SELECTIVE_FLUSH', 'xx_ ');\n\n      define( 'WP_AUTO_UPDATE_CORE', false );\nvolumes:\n  - ./wordpress:/var/www/html\nor:\n  - WORDPRESS_DB_PASSWORD=xxxxxxxxxxxxxxx\n  - WORDPRESS_DEBUG=1\n  - |\n    WORDPRESS_CONFIG_EXTRA=\n    define( 'WP_REDIS_CLIENT', 'predis' );\n    define( 'WP_REDIS_SCHEME', 'tcp' );\n    define( 'WP_REDIS_HOST', 'redis' );\n    define( 'WP_REDIS_PORT', '6379' );\n    define( 'WP_REDIS_PASSWORD', 'xxxxxxxxxxxxxxx' );\n    define( 'WP_REDIS_DATABASE', '0' );\n    define( 'WP_REDIS_MAXTTL', '21600' );\n    define( 'WP_CACHE_KEY_SALT', 'xx_ ');\n    define( 'WP_REDIS_SELECTIVE_FLUSH', 'xx_ ');\n    define( 'WP_AUTO_UPDATE_CORE', false );\nvolumes:\n  - ./wordpress:/var/www/html\nUsing |- instead of | excludes the final newline from that element. What you tried ( WORDPRESS_CONFIG_EXTRA: | ) is something completely different, as you split the single scalar element into a mapping with a single key-value pair.\nAlthough the above load as string values with embedded newlines, it can still happen that the processing done by docker-compose, in particular passing things to a shell, can change the newlines into spaces.\nI have also used programs where, if you might have to escape the newline for the \"folllowing\" processing by ending each line with a backslash (\\)",
    "How to COPY/ADD file into current WORKDIR in Dockerfile": "It turns out to be very simple. I just need to use dot to copy to current workdir.\nCOPY local.conf .\nStill cannot figure out if this has some gotchas. But it just work as intended.",
    "How to fix: The feature watch recursively is unavailable on the current platform, which is being used to run Node.js": "Node v14 introduced a breaking change to the fs.watch() API, specifically that the recursive option (which has never been supported on Linux) now raises the ERR_FEATURE_UNAVAILABLE_ON_PLATFORM error if used on Linux.\nA bug report and fix have been submitted to filewatcher: https://github.com/fgnass/filewatcher/pull/6\nUntil that fix is merged and a new version released, you'll need to stick to NodeJS < v14, or override the filewatcher package installed locally to include that patch.",
    "Activate and switch Anaconda environment in Dockerfile during build": "You've got way too many RUN commands in your Dockerfile. It's not just that each RUN creates a new layer in the image. It's also that each RUN command starts a new shell, and conda activate applies only to the current shell.\nYou should combine logical groups of actions into a single RUN command. Use && to combine commands, and \\ to break lines for readability:\nRUN conda activate <myenv> \\\n && conda install <whatever> \\\n && ...\nKeep in mind: at the end of that RUN command, the shell will be gone. So if you want to do something else to that conda environment afterwards, you've got to run conda activate again, or else use -n <myenv> to put something into an environment without activating it first.\nWhen you start a container from the image, you will also have to call conda activate inside the container.",
    "How to mount a directory in a Docker container to the host?": "First, a little information about Docker volumes. Volume mounts occur only at container creation time. That means you cannot change volume mounts after you've started the container. Also, volume mounts are one-way only: From the host to the container, and not vice-versa. When you specify a host directory mounted as a volume in your container (for example something like: docker run -d --name=\"foo\" -v \"/path/on/host:/path/on/container\" ubuntu), it is a \"regular ole\" linux mount --bind, which means that the host directory will temporarily \"override\" the container directory. Nothing is actually deleted or overwritten on the destination directory, but because of the nature of containers, that effectively means it will be overridden for the lifetime of the container.\nSo, you're left with two options (maybe three). You could mount a host directory into your container and then copy those files in your startup script (or if you bring cron into your container, you could use a cron to periodically copy those files to that host directory volume mount).\nYou could also use docker cp to move files from your container to your host. Now that is kinda hacky and definitely not something you should use in your infrastructure automation. But it does work very well for that exact purpose. One-off or debugging is a great situation for that.\nYou could also possibly set up a network transfer, but that's pretty involved for what you're doing. However, if you want to do this regularly for your log files (or whatever), you could look into using something like rsyslog to move those files off your container.",
    "Docker: How to update your container when your code changes": "Even though there are multiple good answers to this question, I think they missed the point, as the OP is asking about the local dev environment. The command I usually use in this situation is:\ndocker-compose up -d --build\nIf there aren't any errors in Dockerfile, it should rebuild all the images before bringing up the stack. It could be used in a shell script if needed.\n#!/bin/bash\n\nsudo docker-compose up -d --build\nIf you need to tear down the whole stack, you can have another script:\n#!/bin/bash\n\nsudo docker-compose down -v\nThe -v flag removes all the volumes so you can have a fresh start.\nNOTE: In some cases, sudo might not be needed to run the command.",
    "How to use wait-for-it in docker-compose file?": "Generally you wouldn't put it in your docker-compose.yml file at all. The script needs to be built into the image, and its standard startup sequence needs to know to run it.\nThere's a fairly common pattern of using an entrypoint script to do some initial setup, and then use exec \"$@\" to run the container's command as the main process. This lets you, for example, use the wait-for-it.sh script to wait for the backend to be up, then run whatever the main command happens to be. For example, a docker-entrypoint.sh script might look like:\n#!/bin/sh\n\n# Abort on any error (including if wait-for-it fails).\nset -e\n\n# Wait for the backend to be up, if we know where it is.\nif [ -n \"$CUSTOMERS_HOST\" ]; then\n  /usr/src/app/wait-for-it.sh \"$CUSTOMERS_HOST:${CUSTOMERS_PORT:-6000}\"\nfi\n\n# Run the main container command.\nexec \"$@\"\nYou need to make sure this script is executable, and make it the image's ENTRYPOINT, but otherwise you can leave the Dockerfile pretty much unchanged.\nFROM node:12.14.0\nWORKDIR /usr/src/app\nCOPY package*.json ./\n# Do this _before_ copying the entire application in\n# to avoid repeating it on rebuild\nRUN npm install\n# Includes wait-for-it.sh and docker-entrypoint.sh\nCOPY . ./\nRUN chmod +x ./wait-for-it.sh ./docker-entrypoint.sh\nEXPOSE 4555\n# Add this line\n# It _must_ use the JSON-array syntax\nENTRYPOINT [\"./docker-entrypoint.sh\"]\nCMD [\"node\", \"main.js\"]\nIn your docker-compose.yml you need to add the configuration to say where the backend is, but you do not need to override command:.\nmain:\n  depends_on: \n    - customers\n  build: './main'\n  ports:\n    - \"4555:4555\"\n  environment:\n    - CUSTOMERS_HOST=customers",
    "Installing specific version of NodeJS and NPM on Alpine docker image": "Use official versions\nI don't know why you are using NVM.\nAccording to your words. It doesn't matter how but you have to install a specific version of node on a specific version of alpine!\nJust change NODE_VERSION and ALPINE_VERSION to what you need.\nHere's my way using an alpine image not a node one:\nARG NODE_VERSION=18.16.0\nARG ALPINE_VERSION=3.17.2\n\nFROM node:${NODE_VERSION}-alpine AS node\n\nFROM alpine:${ALPINE_VERSION}\n\nCOPY --from=node /usr/lib /usr/lib\nCOPY --from=node /usr/local/lib /usr/local/lib\nCOPY --from=node /usr/local/include /usr/local/include\nCOPY --from=node /usr/local/bin /usr/local/bin\n\nRUN node -v\n\nRUN npm install -g yarn --force\n\nRUN yarn -v\n\nCMD [\"node\", \"path/to/your/script.js\"]\nI removed your unnecessary package installations but you can add them if you need them!\ne.g.: RUN apk add bash git helm openssh yq github-cli\ne.g.: RUN rc-update add docker boot\nExplanation\nAdd NODE_VERSION as ARG so you can use it in everywhere!\nCopy binary files from official build of node:<version>-alpine to your version of alpine!\nSimply install yarn using --force flag to avoid errors and DONE!!!!\nCMD path/to/your/script.js is enough to run your long running JS. Don't use additional tools!\nCompile the official versions\nYou can use the code provided by node it self from here. Just change the versions in the first two lines!\nFROM alpine:3.17.2\n\nENV NODE_VERSION 18.16.0\n\nRUN addgroup -g 1000 node \\\n    && adduser -u 1000 -G node -s /bin/sh -D node \\\n    && apk add --no-cache \\\n        libstdc++ \\\n    && apk add --no-cache --virtual .build-deps \\\n        curl \\\n    && ARCH= && alpineArch=\"$(apk --print-arch)\" \\\n      && case \"${alpineArch##*-}\" in \\\n        x86_64) \\\n          ARCH='x64' \\\n          CHECKSUM=\"f3ad9443e8d9d53bfc00ec875181e9dc2ccf86205a50fce119e0610cdba8ccf1\" \\\n          ;; \\\n        *) ;; \\\n      esac \\\n  && if [ -n \"${CHECKSUM}\" ]; then \\\n    set -eu; \\\n    curl -fsSLO --compressed \"https://unofficial-builds.nodejs.org/download/release/v$NODE_VERSION/node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\"; \\\n    echo \"$CHECKSUM  node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" | sha256sum -c - \\\n      && tar -xJf \"node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" -C /usr/local --strip-components=1 --no-same-owner \\\n      && ln -s /usr/local/bin/node /usr/local/bin/nodejs; \\\n  else \\\n    echo \"Building from source\" \\\n    # backup build\n    && apk add --no-cache --virtual .build-deps-full \\\n        binutils-gold \\\n        g++ \\\n        gcc \\\n        gnupg \\\n        libgcc \\\n        linux-headers \\\n        make \\\n        python3 \\\n    # use pre-existing gpg directory, see https://github.com/nodejs/docker-node/pull/1895#issuecomment-1550389150\n    && export GNUPGHOME=\"$(mktemp -d)\" \\\n    # gpg keys listed at https://github.com/nodejs/node#release-keys\n    && for key in \\\n      4ED778F539E3634C779C87C6D7062848A1AB005C \\\n      141F07595B7B3FFE74309A937405533BE57C7D57 \\\n      74F12602B6F1C4E913FAA37AD3A89613643B6201 \\\n      DD792F5973C6DE52C432CBDAC77ABFA00DDBF2B7 \\\n      61FC681DFB92A079F1685E77973F295594EC4689 \\\n      8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600 \\\n      C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8 \\\n      890C08DB8579162FEE0DF9DB8BEAB4DFCF555EF4 \\\n      C82FA3AE1CBEDC6BE46B9360C43CEC45C17AB93C \\\n      108F52B48DB57BB0CC439B2997B01419BD92F80A \\\n    ; do \\\n      gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \"$key\" || \\\n      gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \"$key\" ; \\\n    done \\\n    && curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION.tar.xz\" \\\n    && curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" \\\n    && gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\\n    && gpgconf --kill all \\\n    && rm -rf \"$GNUPGHOME\" \\\n    && grep \" node-v$NODE_VERSION.tar.xz\\$\" SHASUMS256.txt | sha256sum -c - \\\n    && tar -xf \"node-v$NODE_VERSION.tar.xz\" \\\n    && cd \"node-v$NODE_VERSION\" \\\n    && ./configure \\\n    && make -j$(getconf _NPROCESSORS_ONLN) V= \\\n    && make install \\\n    && apk del .build-deps-full \\\n    && cd .. \\\n    && rm -Rf \"node-v$NODE_VERSION\" \\\n    && rm \"node-v$NODE_VERSION.tar.xz\" SHASUMS256.txt.asc SHASUMS256.txt; \\\n  fi \\\n  && rm -f \"node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" \\\n  && apk del .build-deps \\\n  # Run some smoke tests\n  && node --version \\\n  && npm --version\n\nENV YARN_VERSION 1.22.19\n\nRUN apk add --no-cache --virtual .build-deps-yarn curl gnupg tar \\\n  # use pre-existing gpg directory, see https://github.com/nodejs/docker-node/pull/1895#issuecomment-1550389150\n  && export GNUPGHOME=\"$(mktemp -d)\" \\\n  && for key in \\\n    6A010C5166006599AA17F08146C2130DFD2497F5 \\\n  ; do \\\n    gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \"$key\" || \\\n    gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \"$key\" ; \\\n  done \\\n  && curl -fsSLO --compressed \"https://yarnpkg.com/downloads/$YARN_VERSION/yarn-v$YARN_VERSION.tar.gz\" \\\n  && curl -fsSLO --compressed \"https://yarnpkg.com/downloads/$YARN_VERSION/yarn-v$YARN_VERSION.tar.gz.asc\" \\\n  && gpg --batch --verify yarn-v$YARN_VERSION.tar.gz.asc yarn-v$YARN_VERSION.tar.gz \\\n  && gpgconf --kill all \\\n  && rm -rf \"$GNUPGHOME\" \\\n  && mkdir -p /opt \\\n  && tar -xzf yarn-v$YARN_VERSION.tar.gz -C /opt/ \\\n  && ln -s /opt/yarn-v$YARN_VERSION/bin/yarn /usr/local/bin/yarn \\\n  && ln -s /opt/yarn-v$YARN_VERSION/bin/yarnpkg /usr/local/bin/yarnpkg \\\n  && rm yarn-v$YARN_VERSION.tar.gz.asc yarn-v$YARN_VERSION.tar.gz \\\n  && apk del .build-deps-yarn \\\n  # smoke test\n  && yarn --version\n\nCOPY docker-entrypoint.sh /usr/local/bin/\nENTRYPOINT [\"docker-entrypoint.sh\"]\n\nCMD [ \"node\" ]",
    "Install ODBC driver in Alpine Linux Docker Container": "I was facing the same issue. I solved this issue by adding RUN apk update before RUN apk add commands.(I was using python:3.6-alpine)\nDockerfile\nFROM python:3.6-alpine\nRUN apk update\nRUN apk add gcc libc-dev g++ libffi-dev libxml2 unixodbc-dev mariadb-dev postgresql-dev",
    "Docker .Net 6 error Program does not contain a static 'Main' method suitable for an entry point": "The underlying issue is that it's not finding files where it's been instructed to. See the bit in your Dockerfile where it says\nCOPY . . ?\nMove the\nWORKDIR \"/src/xxxxxx\"\nline from below it to above it.\nThis will copy the restored packages from the preceding line to correct directory before attempting the\nRUN dotnet build \"xxxxx.csproj\" -c Release -o /app/build\nand that should now work",
    "docker python custom module not found": "You set PYTHONPATH to /test_project/utils. When trying resolve the module utils, it is looking for one of:\nfile /test_project/utils/utils.py\ndirectory /test_project/utils/utils/ that contains __init__.py.\nIt looks like you have this?\nutils/math.py\nutils/logger.py\nI wonder if what you really mean to do is\n# different path...\nENV PYTHONPATH /test_project\n\nfrom utils import math\nfrom utils import logger",
    "How to share & WebRTC stream from /dev/videoX device from a Chromium on host and Chromium in a docker container": "If you just want separate Chrome sessions you can simply start it with:\nchromium-browser --user-data-dir=/tmp/chrome1 \nand another instance with\nchromium-browser --user-data-dir=/tmp/chrome2\nDocker is just a way to document and repeat setting up of a very specific environment. The additional layer of security that it adds in minimal, especially with the extra permissions you're passing on and especially in comparison to Chrome's really well tested security.\nIf you need a bit more isolation, you can create separate users:\n # run these lines individualy, just press Enter for everything, don't set passwords for them, they won't be able to log in. \n sudo adduser chrome1\n sudo adduser chrome2 \n\n # if you want to give each access to only one of the cams you can try this\n sudo chown chrome1:chrome1 /dev/video0 \n sudo chown chrome2:chrome2 /dev/video1\n # keeping in mind unplugging and replugging the camera might reset the permissions unless you update the relevant /etc files\n\n # to allow anyone to use your X\n xhost +\n\n # run the two separate browsers\n sudo su - chrome1 -c chromium-browser & \n sudo su - chrome2 -c chromium-browser &",
    "Docker run script in host on docker-compose up": "I just wish to know the best practices and examples of how to run a script on HOST from INSIDE a CONTAINER, so that the deploy can be as easy for the installing operator to just run docker-compose up\nIt seems that there is no best practice that can be applied to your case. A workaround proposed here: How to run shell script on host from docker container? is to use a client/server trick.\nThe host should run a small server (choose a port and specify a request type that you should be waiting for)\nThe container, after it starts, should send this request to that server\nThe host should then run the script / trigger the changes you want\nThis is something that might have serious security issues, so use at your own risk.",
    "Mongorestore in a Dockerfile": "With help from this answer, Marc Young's answer, and the Dockerfile reference I was able to get this working.\nDockerfile\nFROM mongo\n\nCOPY dump /home/dump\nCOPY mongo.sh /home/mongo.sh\nRUN chmod 777 /home/mongo.sh\n\nCMD /home/mongo.sh\nmongo.sh\n#!/bin/bash\n\n# Initialize a mongo data folder and logfile\nmkdir -p /data/db\ntouch /var/log/mongodb.log\nchmod 777 /var/log/mongodb.log\n\n# Start mongodb with logging\n# --logpath    Without this mongod will output all log information to the standard output.\n# --logappend  Ensure mongod appends new entries to the end of the logfile. We create it first so that the below tail always finds something\n/entrypoint.sh mongod --logpath /var/log/mongodb.log --logappend &\n\n# Wait until mongo logs that it's ready (or timeout after 60s)\nCOUNTER=0\ngrep -q 'waiting for connections on port' /var/log/mongodb.log\nwhile [[ $? -ne 0 && $COUNTER -lt 60 ]] ; do\n    sleep 2\n    let COUNTER+=2\n    echo \"Waiting for mongo to initialize... ($COUNTER seconds so far)\"\n    grep -q 'waiting for connections on port' /var/log/mongodb.log\ndone\n\n# Restore from dump\nmongorestore --drop /home/dump\n\n# Keep container running\ntail -f /dev/null",
    "What is a Dockerfile.dev and how is it different from Dockerfile": "It is a common practice to have seperate Dockerfiles for deployments and development systems.\nYou can define a non default dockerfile while building:\ndocker build -f Dockerfile.dev -t devimage .\nOne image could use a compiled version of the source, and a other image could mount the /src folder into the system for live updates.",
    "docker push fails : manifest invalid": "Ran into the same problem. The issue was that repo does not let you over-write images tags. I changed the tag to a new major version.",
    "Huge files in Docker containers": "Is there a better way of referencing such files?\nIf you already have some way to distribute the data I would use a \"bind mount\" to attach a volume to the containers.\ndocker run -v /path/to/data/on/host:/path/to/data/in/container <image> ...\nThat way you can change the image and you won't have to re-download the large data set each time.\nIf you wanted to use the registry to distribute the large data set, but want to manage changes to the data set separately, you could use a data volume container with a Dockerfile like this:\nFROM tianon/true\nCOPY dataset /dataset\nVOLUME /dataset\nFrom your application container you can attach that volume using:\ndocker run -d --name dataset <data volume image name>\ndocker run --volumes-from dataset <image> ...\nEither way, I think https://docs.docker.com/engine/tutorials/dockervolumes/ are what you want.",
    "How to silent install Postgresql in Ubuntu via. Dockerfile?": "add this to your Dockerfile\nARG DEBIAN_FRONTEND=noninteractive\nbefore installing postgresql\nand I think you may want to use apt-get instead of apt to avoid this warning:\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.",
    "Create files / folders on docker-compose build or docker-compose up": "The image does contain those files\nThe Dockerfile contains instructions on how to build an image. The image you built from that Dockerfile does contain index.html and images/.\nBut, you over-rode them in the container\nAt runtime, you created a container from the image you built. In that container, you mounted the external directory ./docroot as /var/www/html.\nA mount will hide whatever was at that path before, so this mount will hide the prior contents of /var/www/html, replacing them with whatever is in ./docroot.\nPutting stuff in your mount\nIn the comments you asked\nis there a possibility then to first mount and then create files or something? Or is that impossible?\nThe way you have done things, you mounted over your original files, so they are no longer accessible once the container is created.\nThere are a couple of ways you can handle this.\nChange their path in the image\nIf you put these files in a different path in your image, then they will not be overwritten by the mount.\nWORKDIR /var/www/alternate-html\n\nRUN touch index.html \\\n    && mkdir images\n\nWORKDIR /var/www/html\nNow, at runtime you will still have this mount at /var/www/html, which will contain the contents from the external directory. Which may or may not be an empty directory. You can tell the container on startup to run a script and copy things there, if that's what you want.\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod 0755 /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\n(This is assuming you do not have a defined entrypoint - if you do, you'll maybe just need to adjust your existing script instead.)\nentrypoint.sh:\n#!/bin/sh\n\ncp -r /var/www/alternate-html/* /var/www/html\nexec \"$@\"\nThis will run the cp command, and then hand control over to whatever the CMD for this image is.\nHandling it externally\nYou also have the option of simply pre-populating the files you want into ./docroot externally. Then they will just be there when the container starts and adds the directory mount.",
    "Docker container not updating on code change": "When you make a change, you need to run docker-compose up --build. That will rebuild your image and restart containers as needed.\nDocker has no facility to detect code changes, and it is not intended as a live-reloading environment. Volumes are not intended to hold code, and there are a couple of problems people run into attempting it (Docker file sync can be slow or inconsistent; putting a node_modules tree into an anonymous volume actively ignores changes to package.json; it ports especially badly to clustered environments like Kubernetes). You can use a host Node pointed at your Docker MongoDB for day-to-day development, and still use this Docker-based setup for deployment.",
    "Docker build command with --tag unable to tag images": "Okay! I found out the reason for issue.\nDOCKER BUILD PROCESS\nWhen we build a docker image, while creating an image, several other intermediate images are generated in the process. We never see them in docker images because with the generation of next intermediate image the earlier image is removed. And in the end we have only one which is the final image.\nThe tag we provide using -t or --tag is for the final build, and obviously no intermediate container is tagged with the same.\nISSUE EXPLANATION\nWhen we try to build a docker image with Dockerfile sometimes the process is not successfully completed with a similar message like Successfully built image with IMAGEID\nSo it is so obvious that the build which has failed will not be listed in docker images\nNow, the image with tag <none> is some other image (intermediate). This creates a confusion that the image exists but without a tag, but the image is actually not what the final build should be, hence not tagged.",
    "How to install Python on nodejs Docker image": "In fact, this is not a docker question, just a debian question. You need always do apt-get update before install package. So, for you scenario, it should be:\nRUN apt-get update || : && apt-get install python -y\nAs per your comments:\nW: Failed to fetch http://deb.debian.org/debian/dists/jessie-updates/InRelease Unable to find expected entry 'main/binary-amd64/Packages' in Release file (Wrong sources.list entry or malformed file) E: Some index files failed to download. They have been ignored, or old ones used instead. The command '/bin/sh -c apt-get update && apt-get install python -y' returned a non-zero code: 100\nSo, you can add || : after apt-get to ignore the error, as at that time python meta data already finished downloading with other previous url hit, so you can bypass the error.\nUpdate:\nA whole workable solution in case you need to compare:\na.py:\nprint(\"success\")\nindex.js:\nconst spawn = require(\"child_process\").spawn;\nconsole.log('PATH:::::');\n\nconsole.log(process.env.PATH);\nconst pythonProcess = spawn('python', ['/app/a.py']);\npythonProcess.stdout.on('data', (data) => {\n    console.log('DATA::::');\n    console.log(data.toString());\n});\n\npythonProcess.stderr.on('data', (data) => {\n    console.log(\"wow\");\n    console.log(data.toString());\n});\nDockerfile:\nFROM node:9-slim\n\nRUN apt-get update || : && apt-get install python -y\n\nWORKDIR /app\nCOPY . /app\nCMD [\"node\", \"index.js\"]\nTry command:\norange@orange:~/gg$ docker build -t abc:1 .\nSending build context to Docker daemon  4.096kB\n...\nSuccessfully built 756b13952760\nSuccessfully tagged abc:1\n\norange@orange:~/gg$ docker run abc:1\nPATH:::::\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nDATA::::\nsuccess",
    "Extract unit test results from multi-stage Docker build (.NET Core 2.0)": "Thanks for your question - I needed to solve the same thing.\nI added a separate container stage based on results of the build. The tests and its output are all handled in there so they never reach the final container. So build-env is used to build and then an intermediate test container is based on that build-env image and final is based on runtime container with results of build-env copied in.\n# ---- Test ----\n# run tests and capture results for later use. This use the results of the build stage\nFROM build AS test\n#Use label so we can later obtain this container from the multi-stage build\nLABEL test=true\nWORKDIR /\n#Store test results in a file that we will later extract \nRUN dotnet test --results-directory ../../TestResults/ --logger \"trx;LogFileName=test_results.xml\" \"./src/ProjectNameTests/ProjectNameTests.csproj\"\nI added a shell script as a next step that then tags the image as project-test.\n#!bin/bash\nid=`docker images --filter \"label=test=true\"  -q`\ndocker tag $id projectname-test:latest\nAfter that, I basically do what you do which is use docker cp and get the file out. The difference is my test results were never in the final image so I don't touch the final image.\nOverall I think the correct way to handle tests is probably create a test image (based on the build image) and run it with a mounted volume for test results and have it run the unit tests when that container starts. Having a proper image/container would also allow you to run integration tests etc. This article is older but details similar https://blogs.infosupport.com/build-deploy-test-aspnetcore-docker-linux-tfs2015/",
    "How does docker image size impact runtime characteristics?": "The size of the image it's only size of the directories. So it will never take more CPU or RAM (if you did not delete something, that will be loaded in RAM at startup from 'A') And a few words about pulling process: the image will be pulled from registry only first time and will be cached locally. Once you pull base image ('A') only differences will be pulled for image 'B'",
    "Docker - ERROR: failed to register layer: symlink": "To solve this issue, you just Stop and Start docker service from terminal.\n# service docker stop\n# service docker start",
    "Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? inside a Dockerfile": "Try this cmd:\nsudo service docker restart",
    "Running a Docker container that accept traffic from the host": "It is saying port 80 is busy ... run this to see who is using port 80\nsudo netstat -tlnp | grep 80 # sudo apt-get install net-tools # to install netstat\n\ntcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1380/nginx -g daemo\ntcp6       0      0 :::80                   :::*                    LISTEN      1380/nginx -g daemo\nscroll to far right to see offending PID of process holding port 80 ... its PID 1380 so lets do a process list to see that pid\nps -eaf | grep 1380\n\nroot      1380     1  0 11:33 ?        00:00:00 nginx: master process /usr/sbin/nginx -g daemon on; master_process on;\nso teardown that offending process to free up the port 80\nsudo kill 1380  # if you know the pid ( 1380 for example )\n__ or __\nsudo fuser -k 80/tcp #  just kill whatever pid is using port 80 tcp\nIf after doing above its still saying busy then probably the process which you killed got auto relaunched in which case you need to kill off its watcher however you can walk up the process tree from netstat output to identify this parent process and kill that too\nHere is how to identify the parent pid of a given process pid\nps -eafww\n\neve         2720    2718  0 07:56 ?        00:00:00 /usr/share/skypeforlinux/skypeforlinux --type=zygote\nin above pid is 2720 and its parent will be the next column to right pid 2718 ... there are commands to show a process tree to visualize these relationships\nps -x --forest  \nor\npstree  -p\nwith sample output of\nsystemd(1)\u2500\u252c\u2500ModemManager(887)\u2500\u252c\u2500{ModemManager}(902)\n           \u2502                   \u2514\u2500{ModemManager}(906)\n           \u251c\u2500NetworkManager(790)\u2500\u252c\u2500{NetworkManager}(872)\n           \u2502                     \u2514\u2500{NetworkManager}(877)\n           \u251c\u2500accounts-daemon(781)\u2500\u252c\u2500{accounts-daemon}(792)\n           \u2502                      \u2514\u2500{accounts-daemon}(878)\n           \u251c\u2500acpid(782)\n           \u251c\u2500avahi-daemon(785)\u2500\u2500\u2500avahi-daemon(841)\n           \u251c\u2500colord(1471)\u2500\u252c\u2500{colord}(1472)\n           \u2502              \u2514\u2500{colord}(1475)\n           \u251c\u2500containerd(891)\u2500\u252c\u2500containerd-shim(1836)\u2500\u252c\u2500registry(1867)\u2500\u252c\u2500{registry}(1968)\n           \u2502                 \u2502                       \u2502                \u251c\u2500{registry}(1969)\n           \u2502                 \u2502                       \u2502                \u251c\u2500{registry}(1970)",
    "OCI runtime exec failed: exec failed: container_linux.go:344: starting container process": "Before reading this answer just let you know, it's my 2nd day of learning docker, It may not be the perfect help for you.\nThis error may also occur when the ping package is not installed in the container, I resolved the problem as follow, bash into the container like this\ndocker container exec -it my_nginx /bin/bash\nthen install ping package\napt-get update\napt-get install inetutils-ping\nThis solved my problem.",
    "Add shell or bash to a docker image (Distroless based on Debian GNU/Linux)": "You can do it by copying the statically compiled shell from official busybox image in a multi-stage build in your Dockerfile. Or just COPY --from it.\nThe static shell doesn't have too many dependencies, so it will work for a range of different base images. It may not work for some advanced cases, but otherwise it gets the job done.\nThe statically compiled shell is tagged with uclibc. Depending on your base image you may have success with other flavours of busybox as well.\nExample:\nFROM busybox:1.35.0-uclibc as busybox\n\nFROM gcr.io/distroless/base-debian11\n\n# Now copy the static shell into base image.\nCOPY --from=busybox /bin/sh /bin/sh\n\n# You may also copy all necessary executables into distroless image.\nCOPY --from=busybox /bin/mkdir /bin/mkdir\nCOPY --from=busybox /bin/cat /bin/cat\n\nENTRYPOINT [\"/bin/sh\", \"/entrypoint.sh\"]\nThe single-line COPY --from directly from image would also work:\nFROM gcr.io/distroless/base-debian11\n\nCOPY --from=busybox:1.35.0-uclibc /bin/sh /bin/sh\n\nENTRYPOINT [\"/bin/sh\", \"/entrypoint.sh\"]",
    "generate a self signed certificate in docker": "What is wrong with simple RUN command? It works for me and the self-signed certificate is created successfully.\nFROM debian:wheezy\n\nRUN apt-get update && \\\n    apt-get install -y openssl && \\\n    openssl genrsa -des3 -passout pass:x -out server.pass.key 2048 && \\\n    openssl rsa -passin pass:x -in server.pass.key -out server.key && \\\n    rm server.pass.key && \\\n    openssl req -new -key server.key -out server.csr \\\n        -subj \"/C=UK/ST=Warwickshire/L=Leamington/O=OrgName/OU=IT Department/CN=example.com\" && \\\n    openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\nOnce in Dockerfile, the certificate is created only once during the image build; then you have the certificate available in the image.\nIf you need a new self-signed certificate each time a container starts, it's possible with the use of an external shell script. Like so:\n#!/bin/bash\n\nopenssl genrsa -des3 -passout pass:x -out server.pass.key 2048\nopenssl rsa -passin pass:x -in server.pass.key -out server.key\nrm server.pass.key\nopenssl req -new -key server.key -out server.csr \\\n    -subj \"/C=UK/ST=Warwickshire/L=Leamington/O=OrgName/OU=IT Department/CN=example.com\"\nopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\nAnd then put that shell script into your Dockerfile and set up the default execution:\nFROM debian:wheezy\n\nRUN apt-get update && \\\n    apt-get install -y openssl\n\nCOPY generate-certificate.sh /tmp/generate-certificate.sh\n\nCMD [ \"/tmp/generate-certificate.sh\" ]\nIn this case each time you start a container with docker run ...., a new unique certificate is generated.",
    "Using SSH agent with Docker Compose and Dockerfile": "They have added the ssh flag as option to the build key in compose: https://github.com/compose-spec/compose-spec/pull/234\nservices:\n  sample:\n    build:\n      context: .\n      ssh:\n        - default",
    "docker RUN append to /etc/hosts in Dockerfile not working [duplicate]": "Docker will generate /etc/hosts dynamically every time you create a new container. So that it can link others. You can use --add-host option:\ndocker run --add-host www.domain.com:8.8.8.8 ubuntu ping www.domain.com",
    "Dockerfile: understanding VOLUME instruction": "A container's volumes are just directories on the host regardless of what method they are created by. If you don't specify a directory on the host, Docker will create a new directory for the volume, normally under /var/lib/docker/vfs.\nHowever the volume was created, it's easy to find where it is on the host by using the docker inspect command e.g:\n$ ID=$(docker run -d -v /data debian echo \"Data container\")\n$ docker inspect -f {{.Mounts}} $ID\n[{0d7adb21591798357ac1e140735150192903daf3de775105c18149552a26f951 /var/lib/docker/volumes/0d7adb21591798357ac1e140735150192903daf3de775105c18149552a26f951/_data /data local  true }]\n \nWe can see that Docker has created a directory for the volume at /var/lib/docker/volumes/0d7adb21591798357ac1e140735150192903daf3de775105c18149552a26f951/_data.\nYou are free to modify/add/delete files in this directory from the host, but note that you may need to use sudo for permissions.\nDocker will only delete volume directories in two circumstances:\nIf the --rm option is given to docker run, any volumes will be deleted when the container exits\nIf a container is deleted with docker rm -v CONTAINER, any volumes will be removed.\nIn both cases, volumes will only be deleted if no other containers refer to them. Volumes mapped to specific host directories (the -v HOST_DIR:CON_DIR syntax) are never deleted by Docker. However, if you remove the container for a volume, the naming scheme means you will have a hard time figuring out which directory contains the volume.\nSo, specific questions:\nYes and yes, with above caveats.\nEach Docker managed volume gets a new directory on the host\nThe VOLUME instruction is identical to -v without specifying the host dir. When the host dir is specified, Docker does not create any directories for the volume, will not copy in files from the image and will never delete the volume (docker rm -v CONTAINER will not delete volumes mapped to user-specified host directories).\nMore information here:\nhttps://blog.container-solutions.com/understanding-volumes-docker",
    "How to cat a file inside a docker image?": "This seems to work dependably for me as it resolves the entrypoint conflict and assures output to stdout. It also kills the container immediately after gathering the data to keep things clean, almost as good as not running it at all. I hope it'll help others.\ndocker run -it --rm -a stdout --entrypoint cat <image> <filename>\nAlso easy to alias if you do this a lot. Add the first line to your ~/.bash_aliases or ~/.bashrc.\n$ alias dcat='docker run -it --rm -a stdout --entrypoint cat'\n$ dcat <image> <filename>",
    "Docker, how to run .sql file in an image?": "You can load the sql file during the build phase of the image. To do this you create a Dockerfile for the db service that will look something like this:\nFROM mysql:5.6\nCOPY setup.sh /mysql/setup.sh\nCOPY setup.sql /mysql/setup.sql\nRUN /mysql/setup.sh\nwhere setup.sh looks something like this:\n#!/bin/bash\nset -e\nservice mysql start\nmysql < /mysql/setup.sql\nservice mysql stop\nAnd in your docker-compose.yml you'd change image to build: ./db or the path where you put your files.\nNow this works if you have all your sql in a raw .sql file, but this wont be the case if you're using rails or a similar framework where the sql is actually stored in code. This leaves you with two options.\nInstead of using FROM mysql:5.6 you can use FROM your_app_image_that_has_the_code_in_it and apt-get install mysql .... This leaves you with a larger image that contains both mysql and your app, allowing you to run the ruby commands above. You'd replace the mysql < /mysql/setup/sql with the rails-app bundle exec rake db:create lines. You'd also have to provide an app config that hits a database on localhost:3306 instead of db:3306\nMy preferred option is to create a script which exports the sql into a .sql file, which you can then use to build your database container. This is a bit more work, but is a lot nicer. It means that instead of running rails-app bundle exec rake db:create you'd just run the script to load a db.\nSuch a script would look something like this:\n#!/bin/bash\nset -e\ndocker-compose build rails-app\ndocker run -d --name mysql_empty mysql:5.6\ndocker run --link mysql_empty:db -v $PWD:/output project_rails-app export.sh\nwhere export.sh looks something like this:\n#!/bin/bash\nset -e\nRAILS_ENV=development\nrails-app bundle exec rake db:create\nmysqldump > /output/setup.sql\nYou could also replace the docker run script with a second compose file if you wanted to.",
    "What is the practical purpose of VOLUME in Dockerfile?": "Instructions like VOLUME and EXPOSE are a bit anachronistic. Named volumes as we know them today were introduced in Docker 1.9, almost three years ago.\nBefore Docker 1.9, running a container whose image had one or more VOLUME instructions (or using the --volume option) was the only way to create volumes for data sharing or persistence. In fact, it used to be a best practice to create data-only containers whose sole purpose was to hold one or more volumes, and then share those volumes with your application containers using the --volumes-from option. Here's some articles that describe this outdated pattern.\nDocker Data Containers\nWhy Docker Data Containers (Volumes!) are Good\nAlso, check out moby/moby#17798 (Data-only containers obsolete with docker 1.9.0?) where the change from data-only containers to named volumes was discussed.\nToday, I consider the VOLUME instruction as an advanced tool that should only be used for specialized cases, and after careful thought. For example, the official postgres image declares a VOLUME at /var/lib/postgresql/data. This can improve the performance of postgres containers out of the box by keeping the database data out of the layered filesystem. Docker doesn't have to search through all the layers of the container image for file requests at /var/lib/postgresql/data.\nHowever, the VOLUME instruction does come at a cost.\nUsers might not be aware of the unnamed volumes being created, and continuing to take up storage space on their Docker host after containers are removed.\nThere is no way to remove a volume declared in a Dockerfile. Downstream images cannot add data to paths where volumes exist.\nThe latter issue results in problems like these.\nHow to \u201cundeclare\u201d volumes in docker image?\nGitLab on Docker: how to persist user data between deployments?\nFor the GitLab question, someone wants to extend the GitLab image with pre-configured data for testing purposes, but it's impossible to commit that data in a downstream image because of the VOLUME at /var/opt/gitlab in the parent image.\ntl;dr: VOLUME was designed for a world before Docker 1.9. Best to just leave it out.",
    "How to pass parameters to a .NET core project with dockerfile": "You can do this with a combination of ENTRYPOINT to set the command, and CMD to set default options.\nExample, for an ASP.NET Core app:\nENTRYPOINT [\"dotnet\", \"app.dll\"]\nCMD [\"argument\"]\nIf you run the container with no command, it will execute this command when the container starts:\ndotnet app.dll argument\nAnd the args array will have one entry, \"argument\". But you can pass a command o docker run to override the CMD definition:\ndocker run app arg1 arg2",
    "How to create postgres database and run migration when docker-compose up": "entrypoint.sh (in here I get createdb: command not found)\nRunning createdb in the nodejs container will not work because it is postgres specific command and it's not installed by default in the nodejs image.\nIf you specify POSTGRES_DB: pg_development env var on postgres container, the database will be created automatically when the container starts. So no need to run createdb anyway in entrypoint.sh that is mounted in the nodejs container.\nIn order to make sequelize db:migrate work you need to:\nadd sequelize-cli to dependencies in package.json\nrun npm install so it gets installed\nrun npx sequelize db:migrate\nHere is a proposal:\n# docker-compose.yml\n\nversion: '3'\nservices:\n  db:\n    image: \"postgres:11.2\"\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ./pgData:/var/lib/postgresql/data\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD:\n      POSTGRES_DB: pg_development\n\n  app:\n    working_dir: /restify-pg\n    entrypoint: [\"/bin/bash\", \"./entrypoint.sh\"]\n    image: node:10.12.0\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - .:/restify-pg\n    environment:\n      DB_HOST: db\n# package.json\n\n{\n  ...\n  \"dependencies\": {\n    ...\n    \"pg\": \"^7.9.0\",\n    \"pg-hstore\": \"^2.3.2\",\n    \"sequelize\": \"^5.2.9\",\n    \"sequelize-cli\": \"^5.4.0\"\n  }\n}\n# entrypoint.sh\n\nnpm install\nnpx sequelize db:migrate\nnpm run dev",
    "How to integrate 'npm install' into ASP.NET CORE 2.1 Docker build": "Found the solution:\nFROM microsoft/dotnet:2.1-aspnetcore-runtime AS base\nWORKDIR /app\nEXPOSE 80\n\nFROM microsoft/dotnet:2.1-sdk AS build\nWORKDIR /src\nCOPY --from=frontend . .\nCOPY [\"myProject.WebUi/myProject.WebUi.csproj\", \"myProject.WebUi/\"]\nCOPY [\"myProject.SearchIndex/myProject.SearchIndex.csproj\", \"myProject.SearchIndex/\"]\nCOPY [\"myProject.SearchIndex.Common/myProject.SearchIndex.Common.csproj\", \"myProject.SearchIndex.Common/\"]\n\nRUN dotnet restore \"myProject.WebUi/myProject.WebUi.csproj\"\nCOPY . .\nWORKDIR \"/src/myProject.WebUi\"\nRUN apt-get update -yq && apt-get upgrade -yq && apt-get install -yq curl git nano\nRUN curl -sL https://deb.nodesource.com/setup_8.x | bash - && apt-get install -yq nodejs build-essential\nRUN npm install -g npm\nRUN npm install\nRUN dotnet build \"myProject.WebUi.csproj\" -c Release -o /app\n\nFROM build AS publish\nRUN dotnet publish \"myProject.WebUi.csproj\" -c Release -o /app\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app .\nENTRYPOINT [\"dotnet\", \"myProject.WebUi.dll\"]",
    "Can you make any sense of Dockers error-messages?": "You need to run docker build -f [docker_file_name] . (don't miss the dot at the end).\nIf the name of your file is Dockerfile then you don't need the -f and the filename.",
    "Run `docker-php-ext-install` FROM container other than php": "New solution\nYou need to create new Dockerfile for specific service, in this case php:\nphp/Dockerfile\nFROM php:7.1.1-fpm\nRUN apt -yqq update\nRUN apt -yqq install libxml2-dev\nRUN docker-php-ext-install pdo_mysql\nRUN docker-php-ext-install xml\nAnd then link to it in your docker-compose.yml file, just like this:\nservices:\n  // other services\n  php:\n    build: ./php\n    ports:\n      - \"9000:9000\"\n    volumes:\n      - .:/dogopic\n    links:\n      - mariadb\nPlease look at build parameter - it points to directory in which is that new Dockerfile located.\nOld solution\nI walked around the problem. I've figured out that I can still run this docker-php-ext-install script using following command:\ndocker-compose exec <your-php-container> docker-php-ext-install pdo pdo_mysql mbstring\nAnd because of the convenience I've created this simple Batch file to simplify composing containers just to one command: ./docker.bat\n@ECHO OFF\n\ndocker-compose build\ndocker-compose exec php docker-php-ext-install pdo pdo_mysql mbstring\ndocker-compose up",
    "FROM...AS in Dockerfile not working as I expect": "The FROM...AS is for multi-stage builds:\nWith multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image. To show how this works, let\u2019s adapt the Dockerfile from the previous section to use multi-stage builds.\nYour dockerfile just has one stage, meanless to use it, a valid use case is next:\nFROM golang:1.7.3 AS builder\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go    .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=builder /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  \nHere, the built out binary in first stage(builder) could be copied to the second stage which with a new base(FROM alpine:latest). The benefit is: it can reduce the golang tool chain setup in second stage, just use the binary from the first stage.\nUPDATE 20221012 to fit new comments:\nLooks official guide did not afford a sample app.go, next is a sample:\npackage main\nfunc main() {\n}",
    "--mount=type=cache in buildkit": "It's best to think of --mount=type=cache as being like a named volume in docker, managed by BuildKit, and potentially deleted if the BuildKit cache gets full or a prune is requested. The next time you run a build, that same named volume may be available, significiantly reducing the build time spent downloading dependencies. While very useful, that doesn't appear to be what you're looking for here. To use a cache like this, you'd need to include the go-offline as an earlier step in the Dockerfile:\n#syntax=docker/dockerfile:experimental\nFROM maven:3.6.1-jdk-11 AS build\nWORKDIR /\n# copy just the pom.xml for cache efficiency\nCOPY ./pom.xml /\n# go-offline using the pom.xml\nRUN --mount=type=cache,target=/root/.m2 mvn dependency:go-offline\n# now copy the rest of the code and run an offline build\nCOPY . /\nRUN --mount=type=cache,target=/root/.m2 mvn -o install \n\nFROM scratch\nCOPY --from=build /admin/admin-rest/target/admin-rest.war /webapps/ROOT.war\nTo mount a directory into the container from the host, what you appear to be looking for is a bind mount. And with BuildKit's experimental settings, that is available, but only to the build context, not to any arbitrary directory on the build host. For that, you can place your .m2 directory in the build context directory and then use the following line in your Dockerfile:\nRUN --mount=type=bind,source=./.m2,target=/root/.m2,rw mvn -o install\nNote if any of the dependencies change, then Maven may try to connect over the network again.",
    "How to update source code without rebuilding image each time?": "Quickly answer\nIs there a way to avoid rebuilding my Docker image each time I make a change in my source code ?\nIf your app needs a build step, you cannot skip it.\nIn your case, you can install the requirements before the python app, so on each source code modification, you just need to run your python app, not the entire stack: postgress, proxy, etc\nDocker purpose\nThe main docker goal or feature is to enable developers to package applications into containers which are easy to deploy anywhere, simplifying your infrastructure.\nSo, in this sense, docker is not strictly for the developer stage. In the developer stage, the programmer should use an specialized IDE (eclipse, intellij, visual studio, etc) to create and update the source code. Also some languages like java, c# and frameworks like react/ angular needs a build stage.\nThese IDEs has features like hot reload (automatic application updates when source code change), variables & methods auto-completion, etc. These features achieve to reduce the developer time.\nDocker for source code changes by developer\nIs not the main goal but if you don't have an specialized ide or you are in a very limited developer workspace(no admin permission, network restrictions, windows, ports, etc ), docker can rescue you\nIf you are a java developer (for instance), you need to install java on your machine and some IDE like eclipse, configure the maven, etc etc. With docker, you could create an image with all the required techs and the establish a kind of connection between your source code and the docker container. This connection in docker is called Volumes\ndocker run --name my_job -p 9000:8080 \\\n-v /my/python/microservice:/src \\\npython-workspace-all-in-one\nIn the previous example, you could code directly on /my/python/microservice and you only need to enter into my_job and run python /src/main.py. It will work without python or any requirement on your host machine. All will be in python-workspace-all-in-one\nIn case of technologies that need a build process: java & c#, there is a time penalty because, the developer should perform a build on any source code change. This is not required with the usage of specialized ide as I explained.\nI case of technologies who not require build process like: php, just the libraries/dependencies installation, docker will work almost the same as the specialized IDE.\nDocker for local development with hot-reload\nIn your case, your app is based on python. Python don't require a build process. Just the libraries installation, so if you want to develop with python using docker instead the classic way: install python, execute python app.py, etc you should follow these steps:\nDon't copy your source code to the container\nJust pass the requirements.txt to the container\nExecute the pip install inside of container\nRun you app inside of container\nCreate a docker volume : your source code -> internal folder on container\nHere an example of some python framework with hot-reload:\nFROM python:3\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\nCOPY requirements.txt /usr/src/app\nRUN pip install -r requirements.txt\nCMD [ \"mkdocs\", \"serve\",  \"--dev-addr=0.0.0.0:8000\" ]\nand how build as dev version:\ndocker build -t myapp-dev .\nand how run it with volumes to sync your developer changes with the container:\ndocker run --name myapp-dev -it --rm -p 8000:8000 -v $(pwd):/usr/src/app mydocs-dev\nAs a summary, this would be the flow to run your apps with docker in a developer stage:\nstart the requirements before the app (database, apis, etc)\ncreate an special Dockerfile for development stage\nbuild the docker image for development purposes\nrun the app syncing the source code with container (-v)\ndeveloper modify the source code\nif you can use some kind of hot-reload library on python\nthe app is ready to be opened from a browser\nDocker for local development without hot-reload\nIf you cannot use a hot-reload library, you will need to build and run whenever you want to test your source code modifications. In this case, you should copy the source code to the container instead the synchronization with volumes as the previous approach:\nFROM python:3\nRUN mkdir -p /usr/src/app\nCOPY . /usr/src/app\nWORKDIR /usr/src/app\nRUN pip install -r requirements.txt\nRUN mkdocs build\nWORKDIR /usr/src/app/site\nCMD [\"python\", \"-m\", \"http.server\", \"8000\" ]\nSteps should be:\nstart the requirements before the app (database, apis, etc)\ncreate an special Dockerfile for development stage\ndeveloper modify the source code\nbuild\ndocker build -t myapp-dev.\nrun\ndocker run --name myapp-dev -it --rm -p 8000:8000 mydocs-dev",
    "\"key cannot contain a space\" error while running docker compose": "Ok this is resolved finally! After beating my head around, I was able to finally resolve this issue by doing the following things:\nUnchecked the option to use \"Docker Compose v2\" from my docker desktop settings. Here is the setting in Docker Desktop\nClosed the docker desktop app and restarted it.\nPlease try these steps in case you face the issue. Thanks!",
    "Docker Compose file is invalid, additional properties not allowed": "You are missing a services keyword, your correct .yml is:\nversion: '2'\nservices:\n  config-server:\n    image: ccc/config-server\n    restart: always\n  registration-server:\n    image: ccc/registration-server\n    restart: always\n    ports:\n      - 1111:1111",
    "Sharing volume between Docker containers": "You may find a lot of pointers mentioning data-only containers and --volumes-from. However, since docker 1.9, volumes have become first class citizens, they can have names, and have more flexibility:\nIt's now easy to achieve the behavior you want, here's an example :\nCreate a named data volume with name service-data:\ndocker volume create --name service-data\nYou can then create a container that mounts it in your /public folder by using the -v flag:\ndocker run -t -i -v service-data:/public debian:jessie /bin/bash\nFor testing purpose we create a small text file in our mapped folder:\ncd public\necho 'hello' > 'hello.txt'\nYou may then attach your named volume to a second container, but this time under the data folder:\ndocker run -t -i -v service-data:/data debian:jessie /bin/bash\nls /data       #-->shows \"hello.txt\"\nJust remember, if both containers are using different images, be careful with ownership and permissions!",
    "docker toolbox mount file on windows": "Try to run it with additional / for volume like:\ndocker run -d --name simple2 -v /c/Users/src://usr/share/nginx/html -p 8082:80 ng1\nOr even for host OS, as\ndocker run -d --name simple2 -v //c/Users/src://usr/share/nginx/html -p 8082:80 ng1\nDue to this issue:\nThis is something that the MSYS environment does to map POSIX paths to Windows paths before passing them to executables.",
    "Is it possible to get the architecture of the docker engine in a Dockerfile?": "If you build using BuildKit, there are some predefined ARGs you can use:\nTARGETPLATFORM - platform of the build result. Eg linux/amd64, linux/arm/v7, windows/amd64.\nTARGETOS - OS component of TARGETPLATFORM\nTARGETARCH - architecture component of TARGETPLATFORM\nTARGETVARIANT - variant component of TARGETPLATFORM\nBUILDPLATFORM - platform of the node performing the build.\nBUILDOS - OS component of BUILDPLATFORM\nBUILDARCH - OS component of BUILDPLATFORM\nBUILDVARIANT - OS component of BUILDPLATFORM\nThese are documented in \"Automatic platform ARGs in the global scope\" in the builder documentation.\nTo use BuildKit, you can enable it within your shell with a variable:\nexport DOCKER_BUILDKIT=1\nAnd then build using docker build (support for BuildKit using docker-compose is being worked on, likely in the next release).",
    "Should I copy `package-lock.json` to the container image in Dockerfile?": "You should absolutely copy the package-lock.json file in. It has a slightly different role from the package.json file: package.json can declare \"I'm pretty sure my application works with version 17 of the react package\", where package-lock.json says \"I have built and tested with exactly version 17.0.1 of that package\".\nOnce you have both files, there is a separate npm ci command that's optimized for this case.\nCOPY package.json package-lock.json .\n# Run `npm ci` _before_ copying the application in\nRUN NODE_ENV=production npm ci\n# If any file in `dist` changes, this will stop Docker layer caching\nCOPY ./dist ./dist",
    "How to do a health check of a Spring Boot application running in a Docker Container?": "If you want to use the spring boot actuator/health as a docker healthcheck, you have to add it like this on your docker-compose file:\n    healthcheck:\n      test: \"curl --fail --silent localhost:8081/actuator/health | grep UP || exit 1\"\n      interval: 20s\n      timeout: 5s\n      retries: 5\n      start_period: 40s\nEdit: here the port is the management.server.port. If you don't have specified it, it should be the server.port value (8080 by default)",
    "Why does Docker build take long time in \"Sending context to daemon\" step?": "I solved it, silly mistake.\nSeems like Docker build tars up the current working directory (i.e. the folder containing the dockerfile). And it then uploads it to the Docker Daemon for the build steps. I had accidently put a big test data file (2.9 GB) in the working directory. And that was getting included in the build context. After removing it things are back to normal.",
    "Is CMD or ENTRYPOINT necessary to mention in Dockerfile?": "Specifying a command at the end of the docker run command line supplies (or overrides) CMD; similarly, the docker run --entrypoint option supplies (or overrides) ENTRYPOINT. In your example you gave a command /bin/sh so there's something for the container to do; if you leave it off, you'll get an error.\nAs a matter of style your Dockerfiles should almost always declare a CMD, unless you're extending a base image that's already running the application automatically (nginx, tomcat). That will let you docker run the image and launch the application embedded in it without having to remember a more specific command-line invocation.",
    "$PWD is not set in ENV instruction in a Dockerfile": "PWD is an special variable that is set inside a shell. When docker RUN something it does that with this form sh -c 'something', passing the pre-defined environment variables from ENV instructions, where PWD is not in that list (see it with docker inspect <image-id>).\nENV instructions does not launch a shell. Simply add or update the current list of env vars in the image metadata.\nI would write your Dockerfile as this:\nFROM ubuntu:16.04\nENV APP_PATH=/some/path\nWORKDIR $APP_PATH\nCOPY . .\nENV PYTHONUSERBASE=$APP_PATH/pyenv PATH=$APP_PATH/pyenv/bin:$PATH\nRUN echo \"PWD is: $PWD\"\nRUN echo \"PYENV is: $PYTHONUSERBASE\"\nFurther info in docs:\nThe WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.",
    "How to pass local machine's SSH key to docker container?": "This works for me :\nUsing this workaround : https://stackoverflow.com/a/47544999/3957754 to pass files as build args\nDockerfile\nARG SSH_KEY\nENV SSH_KEY=$SSH_KEY\n\n# Make ssh dir\nRUN mkdir /root/.ssh/\n \n# Create id_rsa from string arg, and set permissions\n\nRUN echo \"$SSH_KEY\" > /root/.ssh/id_rsa\nRUN chmod 600 /root/.ssh/id_rsa\n \n# Create known_hosts\nRUN touch /root/.ssh/known_hosts\n\n# Add git providers to known_hosts\nRUN ssh-keyscan bitbucket.org >> /root/.ssh/known_hosts\nRUN ssh-keyscan github.com >> /root/.ssh/known_hosts\nRUN ssh-keyscan gitlab.com >> /root/.ssh/known_hosts\nBuild\ndocker build -t some-app --build-arg SSH_KEY=\"$(cat ~/file/outside/build/context/id_rsa)\" .\nWith this, you can perform git clone git@github.com... (gitlab, or bitbucket) at build stage or at run stage using ENTRYPOINT [\"docker-entrypoint.sh\"].\nThis could works if you need to pass any file as parameter to your container\nSecurity\nAs commenters said, to pass a file to a container at build time is not safe. The workaround and best practice is : clone the project in the c.i (jenkins, bamboo, circleci, etc) and the perform the docker build .... Clone the project inside of docker is usually just for old required libraries, not for the main source code.",
    "How upgrade a docker image without creating new image?": "Those <none>:<none> are actually the old version of your application, that has lost its named pointer, since you moved the nick_app:latest to your new build.\nWhen building your image, I don't think that you can tell it to destroy the old image, it will simply create a new one. But there is a command that helps you to list or remove those dangling images :\ndocker images --filter dangling=true #lists all images that are dangling and has no pointer to it\ndocker rmi `docker images --filter dangling=true -q` #Removes all those images.",
    "How do I attach to intermediate docker container when building with buildkit": "I think it is not possible at the moment see buildkit/issue#1472.\nBut BuildKit still caches all layers so you could use a work around.\nInspecting the image before the failing RUN command, comment out the failing and all subsequent RUN commands. Rerun docker build and then do docker run to inspect the image.\nInspecting the image after the failing RUN command, add || true at the end of your RUN command to force the command to succeed. Rerun docker build and then do docker run to inspect the image.",
    "How to access the metadata of a docker container from a script running inside the container?": "To get the labels (and anything from the remote API), you could pass the socket into the container and use curl >= 7.40 (it's the minimum version that supports --unix-socket flag) from within the container to access the remote API via the socket:\nDockerfile:\nFROM ubuntu:16.04 \nRUN apt-get update \\\n    && apt-get install curl -y\nLABEL abc = abc_value1\nBuild and run\ndocker build -t image1 .\ndocker run -v /var/run/docker.sock:/var/run/docker.sock -it image1 /bin/bash\nFrom inside the container\ncurl --unix-socket /var/run/docker.sock http:/containers/$(hostname)/json\nFrom here you'll have a huge chunk of JSON (similar to docker inspect). You can then use a CLI tool like jq to pluck out the labels.\nSee more information on docker's website: https://docs.docker.com/engine/reference/api/docker_remote_api/#/docker-remote-api\nAll that said-- this isn't very secure, and environment variables are probably a better bet.",
    "Could not auto-determine entry point from rollupOptions": "You are probably using the command yarn dev run instead of yarn run dev to run the dev server",
    "Why is docker build taking so long to run?": "I have no idea why the file size was 213.8. The only directory that is large is node_modules and that contains .dockerignore so it shouldn't be touching that directory.\nThat's not how .dockerignore works. The .dockerignore file should be in the same directory as your Dockerfile and lists patterns to ignore. Create a file in backend called .dockerignore which simply contains the line node_modules.\nSee here for more information: https://docs.docker.com/engine/reference/builder/#dockerignore-file",
    "Docker-compose command doesn't override Dockerfile CMD": "Overriding takes place at runtime, which is when you create a container based on an image and you then start it. That last step you see is during the building phase of the actual image which (correctly) follows the instructions of your Dockerfile.\nSo, if you continue with docker-compose up, the container that will be created and started, will have the \"overridden\" configuration provided in the docker-compose.yaml file.\nFrom Overriding Dockerfile image defaults\nOverriding Dockerfile image defaults\nWhen a developer builds an image from a Dockerfile or when she commits it, the developer can set a number of default parameters that take effect when the image starts up as a container. Four of the Dockerfile commands cannot be overridden at runtime: FROM, MAINTAINER, RUN, and ADD. Everything else has a corresponding override in docker run.",
    "How do you dockerize a WebSocket Server?": "When you specify a hostname or IP address to listen on (in this case localhost which resolves to 127.0.0.1), then your server will only listen on that IP address.\nListening on localhost isn't a problem when you are outside of a Docker container. If your server only listens on 127.0.0.1:8000, then your client can easily connect to it since the connection is also made from 127.0.0.1.\nWhen you run your server inside a Docker container, it'll only listen on 127.0.0.1:8000 as before. The 127.0.0.1 is a local loopback address and it not accessible outside the container.\nWhen you fire up the docker container with -p 8000:8000, it'll forward traffic heading to 127.0.0.1:8000 to the container's IP address, which in my case is 172.17.0.2.\nThe container gets an IP addresses within the docker0 network interface (which you can see with the ip addr ls command)\nSo, when your traffic gets forwarded to the container on 172.17.0.2:8000, there's nothing listening there and the connection attempt fails.\nThe fix:\nThe problem is with the listen address:\nserver := http.Server{Addr: \"localhost:8000\"}\nTo fix your problem, change it to\nserver := http.Server{Addr: \":8000\"}\nThat'll make your server listen on all it container's IP addresses.\nAdditional info:\nWhen you expose ports in a Docker container, Docker will create iptables rules to do the actual forwarding. See this. You can view these rules with:\niptables -n -L \niptables -t nat -n -L",
    "How to pass Java options/variables to Springboot app in docker run command": "Solution 1\nYou can override any property from your configuration by passing it to docker container using -e option. As explained in Externalized configuration the environment variable name should be uppercased and splitted using underscore. So for example to pass spring.profiles.active property you could use SPRING_PROFILES_ACTIVE environment variable during container run :\ndocker run -p 8000:80 -e SPRING_PROFILES_ACTIVE=dockerdev demo-app\nAnd this variable should be picked automatically by Spring from environment.\nSolution 2\nChange Dockerfile to :\nFROM openjdk:8-jdk-alpine\nARG JAR_FILE=target/demo-app-1.0-SNAPSHOT.jar\n\n# environment variable with default value\nENV SPRING_PROFILE=dev\n\nCOPY ${JAR_FILE} /opt/lib/demo-app.jar\n\nEXPOSE 80\n\n#run with environment variable\nENTRYPOINT java -Dspring.profiles.active=$SPRING_PROFILE -jar /opt/lib/demo-app.jar\nand then run the container passing the environment variable :\ndocker run -p 8000:80 --rm -e SPRING_PROFILE=dockerdev demo-app",
    "Docker: $'\\r': command not found on Windows": "I was passing for the same problem and I found this error is caused for caracters that only the windows use. To solution this error, write the file shellscript using nano or other linux file editors.\nIf you use windows 10, you can start a sub system linux to write the shellscript.\nHope this helps",
    "What is the difference between node images for Docker and when to use which?": "All of the standard Docker Hub images have a corresponding Docker Hub page; for Node it's https://hub.docker.com/_/node. There's a section entitled \"Image Variants\" on that page that lists out the major options.\nDebian/Ubuntu/Alpine. Most Docker images are built on top of one of these base Linux distributions. Debian and Ubuntu are very similar and in fact are closely related; if you see something labeled \"bullseye\", \"buster\", or \"stretch\", these are the names of specific Debian major releases. Alpine has a reputation for being very small, but with that tininess comes some potential low-level C library issues. I'd recommend the Debian-based option unless you really need the tens of megabytes of space savings.\n\"Slim\" images. The node image has a \"slim\" variant. The default image mentions that it's built on top of a buildpack-deps image, but that base image is not small (~300 MB). That image includes a full C toolchain and many development headers. Some npm install commands could need this, but you don't want it in your final image. Prefer the \"slim\" option if it's a choice.\nVersion tags. The top of the Docker Hub page has a bewildering list of tags. If you look at these, each line has several names; as of this writing, node:16, node:16.14, node:16.14.0, node:lts, node:16-buster, and several other things are all actually the same image. One thing to note is that only \"current\" versions get any sort of updates at all; if Node 16.14.0 is the current version then no node:16.13 package will ever be rebuilt. I'd suggest picking a specific major version and using a \"long-term support\" version if it's an option, but not specifying a minor or patch version.\nCombining all of those together, my default would be something like\nFROM node:16-slim # Debian-based",
    "How can I upgrade pip inside a venv inside a Dockerfile?": "The single easiest answer to this is to just not bother with a virtual environment in a Docker image. A virtual environment gives you an isolated filesystem space with a private set of Python packages that don't conflict with the system install, but so does a Docker image. You can just use the system pip in a Docker image and it will be fine.\nFROM python:3.7\nRUN pip install --upgrade pip\nWORKDIR /usr/src/app\nCOPY . .\nRUN pip install .\nCMD [\"myscript\"]\nIf you really want a virtual environment, you either need to specifically run the wrapper scripts from the virtual environment's path\nRUN python -m venv venv\nRUN venv/bin/pip install --upgrade pip\nor run the virtual environment \"activate\" script on every RUN command; the environment variables it sets won't carry over from one step to another. (Each RUN command in effect does its own docker run; docker commit sequence under the hood and will launch a new shell in a new container; the Dockerfile reference describes this a little bit.)\nRUN python -m venv venv\nRUN . venv/bin/activate \\\n && pip install --upgrade pip\nCOPY . .\nRUN . venv/bin/activate \\\n && pip install .\nCMD [\"venv/bin/myscript\"]\nTrying to activate the virtual environment in its own RUN instruction does nothing beyond generate a no-op layer.\n# This step does nothing\nRUN . venv/bin/activate\n# And therefore this upgrades the system pip\nRUN pip install --upgrade pip",
    "How to use pipes(ioredirection) in Dockerfile RUN?": "You can try a sh -c command\nRUN sh -c 'git archive master | tar -x -C /path'\nIf not, you can include that command in a script, COPY the script and RUN it.",
    "Docker file FROM node:12.2.0-alpine": "Alpine is the base image which is based on Alpine Linux, a very compact Linux distribution. So, node:12.2.0-alpine is a Alpine Linux image with node 12.2.0 installed.\nFor the latest Alpine based image you can simply do node:alpine. If you want latest but not specifically Alpine you can do node:latest, that image will be based on stretch which is a Debian distribution.\nYou can find a full list of all supported tags here: https://hub.docker.com/_/node/",
    "How to limit memory usage in docker-compose?": "deploy key only works in swarm mode and with docker-compose file version 3 and above.\nIn your case, use docker-compose file version 2 and define resource limits:\nversion: \"2.2\"\n\nservices:\n  app:\n    image: foo\n    cpus: \"0.5\"\n    mem_limit: 23m\nSee official docs here",
    "Can Docker COPY commands be chained": "Since COPY commands cannot be chained, it's typically best to structure your context (directories you are copying from) in a way that is friendly to copy into the image.\nSo instead of:\nCOPY ./folder1A/* /home/user/folder1B/ && \\\n     ./folder2A/* /home/user/folder2B/ && \\\n     ./folder3A/* /home/user/folder3B/ && \\\n     ./folder4A/* /home/user/folder4B/ && \\\nPlace those folders into a common directory and run:\nCOPY user/ /home/user/\nIf you are copying files, you can copy multiple into a single target:\nCOPY file1.zip file2.txt file3.cfg /target/app/\nIf you try to do this with a directory, you'll find that docker flattens it by one level, hence the suggestion to reorganize your directories into a common parent folder.",
    "Error: \"user\" directive makes sense only if the master process runs with super-user privileges": "NGINX has now an official unprivileged Docker image, with more fine-grained changes (below are only \"notable\" ones, there is more of them):\nremoving user directive in /etc/nginx/nginx.conf\nmoving PID from /var/run/nginx.pid to /tmp/nginx.pid\nchanging *_temp_path variables to /tmp/*\nchanging the listening port to a non-root one (80->8080).\nTo see all these changes, please check out the source at nginxinc/docker-nginx-unprivileged or simply pull one of the resulting unprivileged Docker images from the Docker Hub (nginxinc/nginx-unprivileged), and I strongly recommend the one based on Alpine rather than on Debian to avoid frequent vulnerabilities:\ndocker pull nginxinc/nginx-unprivileged:stable-alpine",
    "Xvfb & Docker - cannot open display": "I solved this by writing a startup script which will:\nstart xvfb\nstart firefox\nExecuting the script via CMD allows the proper sequence of commands to run on container startup.\nDockerfile\n...\nENV DISPLAY :99\n\nADD run.sh /run.sh\nRUN chmod a+x /run.sh\n\nCMD /run.sh\nrun.sh\nXvfb :99 -screen 0 640x480x8 -nolisten tcp &\nfirefox",
    "Is it necessary to RUN apk update && apk upgrade in a docker build stage?": "First of all: the command you propose is not efficient because the --no-cache option affects the 2nd apk only.\nDemonstration\n/ # ls -la /var/cache/apk/\ntotal 0\ndrwxr-xr-x    2 root     root             6 Mar 29 14:27 .\ndrwxr-xr-x    4 root     root            29 Mar 29 14:27 ..\n\n/ # apk update && apk upgrade --no-cache\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.16/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.16/community/x86_64/APKINDEX.tar.gz\nv3.16.5-131-g8a958b888f7 [https://dl-cdn.alpinelinux.org/alpine/v3.16/main]\nv3.16.5-127-g643d8ee0752 [https://dl-cdn.alpinelinux.org/alpine/v3.16/community]\nOK: 17042 distinct packages available\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.16/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.16/community/x86_64/APKINDEX.tar.gz\n(1/5) Upgrading musl (1.2.3-r2 -> 1.2.3-r3)\n(2/5) Upgrading ca-certificates-bundle (20220614-r0 -> 20230506-r0)\n(3/5) Upgrading libcrypto1.1 (1.1.1t-r2 -> 1.1.1u-r1)\n(4/5) Upgrading libssl1.1 (1.1.1t-r2 -> 1.1.1u-r1)\n(5/5) Upgrading musl-utils (1.2.3-r2 -> 1.2.3-r3)\nExecuting busybox-1.35.0-r17.trigger\nOK: 6 MiB in 14 packages\n\n/ # ls -la /var/cache/apk/\ntotal 2416\ndrwxr-xr-x    1 root     root            70 Jun  9 12:51 .\ndrwxr-xr-x    1 root     root            29 Mar 29 14:27 ..\n-rw-r--r--    1 root     root        657130 Jun  9 12:51 APKINDEX.77a9a2bb.tar.gz\n-rw-r--r--    1 root     root       1810672 Jun  9 12:51 APKINDEX.af244049.tar.gz\nAs you can see, the apk cache is not empty after the execution of your command.\nThe correct command is\nRUN apk upgrade --no-cache\nbecause --no-cache\nallows users to install packages with an index that is updated and used on-the-fly and not cached locally\nas explained at https://github.com/gliderlabs/docker-alpine/blob/master/docs/usage.md\nNow, to answer your question, upgrading the image used for the build is not particularly useful. Upgrading the deployment base image makes much more sense, however I upgrade it only if trivy says that it has some CVE and Docker Hub hasn't a newer version of that base image. In this case I create an upgraded base image and reuse it for all my apps (you could also use it for build stages).\nFor example, at today's date, trivy says that the latest builds of Alpine Linux 3.16, 3.17 and 3.18 have some CVE that has been already fixed but the fixes are not part of an official Alpine release yet. So I have just created my own Alpine with this Dockerfile:\nFROM alpine:3.16 as alpine-upgraded\n\nRUN apk upgrade --no-cache\n\n# Main image\nFROM scratch\n\nCOPY --from=alpine-upgraded / /\nCMD [\"/bin/sh\"]\nTrivy says it is vulnerability-free and docker images says that the new image has the same size of the original (not upgraded) one: 5.54 MB.\nSee also the description of this Docker image.",
    "Pass host environment variables to dockerfile": "I was experiencing the same issue. My solution was to provide the variable inside of a docker-compose.yml because yml supports the use of environment variables.\nIn my opinion this is the most efficient way for me because I didn't like typing it over and over again in the command line using something like docker run -e myuser=$USER . . .\nDeclaring ENV myuser=$USER will NOT work, in the container, $myuser will be set to null.\nSo your docker-compose.yml could look something like this:\nversion: '3'\nservices:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n       - \"myuser=${USER}\"\nand can be run with the short command docker-compose up\nTo check that the variable has been applied, run docker exec -it container-name printenv to list all variables in the container.",
    "How to Dockerfile FROM another Dockerfile?": "If you want to avoid playing around with unnecessary intermediate tags and you use Docker 20.10+, you can also do this:\n# syntax = edrevo/dockerfile-plus\n\nINCLUDE+ Dockerfile.common\n\n# The rest of Dockerfile.foo would go after the INCLUDE+ instruction\n\nRUN echo \"Hello World\"\nThe INCLUDE+ instruction gets imported by the first line in the Dockerfile. You can read more about the dockerfile-plus at https://github.com/edrevo/dockerfile-plus",
    "What does the -j option do for docker-php-ext-install?": "It is the number of Jobs for the make calls contained inside the script docker-php-ext-install (line 53 stores the option in the variable $j and 105-106 call make -j$j).\nThe command nproc is giving directly to the script the number of physical thread available for your system. For example, on my system it will be reduced to:\nmake -j$(nproc) -> make -j8\nthus it runs make with 8 parallel recipes.\nFrom make manual:\n-j [jobs], --jobs[=jobs]: Specifies the number of jobs (commands) to run simultaneously. If there is more than one -j option, the last one is effective. If the -j option is given without an argument, make will not limit the number of jobs that can run simultaneously.\nwith more information in the GNU make documentation about parallel jobs:\nGNU make knows how to execute several recipes at once. Normally, make will execute only one recipe at a time, waiting for it to finish before executing the next. However, the -j or --jobs option tells make to execute many recipes simultaneously. [...] On MS-DOS, the -j option has no effect, since that system doesn\u2019t support multi-processing.\nIf the -j option is followed by an integer, this is the number of recipes to execute at once; this is called the number of job slots. If there is nothing looking like an integer after the -j option, there is no limit on the number of job slots. The default number of job slots is one, which means serial execution (one thing at a time).\nIdeally, if that number is equal to the number of the physical threads available (roughly the number of processors, or as in this case the number returned by nproc), you should get the fastest compilation possible.\nnproc is a linux command - see man nproc too. It means \"number of processing units available\" - your CPU Cores. You can imagine it as \"Number of processors\" [source: @tron5 comment]\nYou must consider the memory available though. For example, if you allocate 8 slots with only 1GB of RAM and the compilation of 3 simultaneous jobs fill the RAM, then when the fourth will start it will exit with an error due to insufficient memory, arresting the whole compilation process.",
    "Run a script when docker is stopped": "docker stop sends a SIGTERM signal to the main process running inside the Docker container (the entry script). So you need a way to catch the signal and then trigger the exit script.\nSee This link for explanation on signal trapping and an example (near the end of the page)",
    "Mount docker host volume but overwrite with container's contents": "The docker 1.10+ way of sharing files would be through a volume, as in docker volume create.\nThat means that you can use a data volume directly (you don't need a container dedicated to a data volume).\nThat way, you can share and mount that volume in a container which will then keep its content in said volume.\nThat is more in line with how a container is working: isolating memory, cpu and filesystem from the host: that is why you cannot \"mount a volume and have the container's files take precedence over the host file\": that would break that container isolation and expose to the host its content.",
    "Is there a way to suppress \"update-alternatives: warning: skip creation\" warnings when building a Docker image?": "If you just want to save disk space and/or network bandwidth, try this:\necho 'path-exclude /usr/share/doc/*' >/etc/dpkg/dpkg.cfg.d/docker-minimal && \\ \necho 'path-exclude /usr/share/man/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/groff/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/info/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/lintian/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/linda/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-exclude /usr/share/locale/*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal && \\\necho 'path-include /usr/share/locale/en*' >>/etc/dpkg/dpkg.cfg.d/docker-minimal\nAs long as you'll try to install some APT packages, no man pages would be saved to disk. The warning will still be there, but the man pages not.\nThe problem here is that some packages calls the update-alternatives binary from within the postinst section of the .deb packages. See https://manpages.debian.org/buster/dpkg/update-alternatives.1.en.html. Don't know if you can also ask DPKG to not execute the postinst section at all, but it does not sounds good! You may also write a small wrapper to update-alternatives in order to suppress the warnings, but this sound silly.",
    "docker swarm - how to balance already running containers in a swarm cluster?": "Swarm currently (18.03) does not move or replace containers when new nodes are started, if services are in the default \"replicated mode\". This is by design. If I were to add a new node, I don't necessarily want a bunch of other containers stopped, and new ones created on my new node. Swarm only stops containers to \"move\" replicas when it has to (in replicated mode).\ndocker service update --force <servicename> will rebalance a service across all nodes that match its requirements and constraints.\nFurther advice: Like other container orchestrators, you need to give capacity on your nodes in order to handle the workloads of any service replicas that move during outages. You're spare capacity should match the level of redundancy you plan to support. If you want to handle capacity for 2 nodes failing at once, for instance, you'd need a minimum percentage of resources on all nodes for those workloads to shift to other nodes.",
    "How to Serve HTML Files from a Nginx Server Using Docker": "The default directory that static assets are served out of is /usr/share/nginx/html, not /var/www in the Official NGINX Docker image.\nWith that said, you're also mapping your entire root directory and not the /public/ folder where your folder contents live - unless of course you're running this from that directory on a pre-build image.\nYou'll probably want something like:\n\u279c  docker run -p 80:80 -v $(pwd):/usr/share/nginx/html nginx",
    "failed to solve with frontend dockerfile.v0: failed to read dockerfile?": "I encountered a different issue, so sharing as an FYI. On Windows, I created the docker file as DockerFile instead of Dockerfile. The capital F messed things up.",
    "where does the \"Unable to find fallback package folder\" nuget error come from, when building project in a dockerfile?": "Had the same issue. After some Googling I found this nice issue: https://github.com/dotnet/dotnet-docker/issues/2050\nTo recap the answer there: if you have already built the project outside of Docker, then you will have output folders which will get copied into the Docker build environment causing this problem.\nThe solution is then to add a .dockerignore file which prevents this from happening. Something small like this should do the trick:\n# directories\n**/bin/\n**/obj/\n**/out/\n\n# files\nDockerfile*\n**/*.md",
    "Docker compose Invalid volume destination path: '.' mount path must be absolute": "This is just not allowed in the Compose file, since you do not have a template engine there.\nYou will not need to define\nvolumes: \n        - /opt/h2-data\nSince that will be done automatically (anonymous volume). If you want to have a named volume use\nvolumes: \n        - myname:/opt/h2-data\nor a host mount\nvolumes: \n        - /path/on/the/host:/opt/h2-data\nSo ${DATA_DIR} is not expanded in the volumes ( from the ENV ) in a compose file. There are dialects like rancher-compose providing this, but in general that is not possible\nUPDATED: Updated my answer since I somehow mixed the Dockerfile/docker-compose.yml file. It makes sense in the Dockerfile, since it is just used as a variable. Thank you for hinting me on that @Bmitch (once again)",
    "Dockerfile vs docker-compose.yml": "Dockerfile:\nis a recipe for a Docker Image\nonly supports portable options (others options have to be specified at container run time)\ndocker-compose.yaml:\nis a recipe for a group of running services\nsupports overriding portable options that were defined in the Dockerfile\nsupports non-portable options\nsupports creating and configuring networks and volumes\ncan also configure the build of an Image by using build:\nIt is common to use both together.\nA Dockerfile is almost always used to create a custom image. Some images might be used to run services (long running processes), but some images might be used to run short-lived interactive processes (like running unit tests).\ndocker-compose.yaml is useful when you want to run one or more services.",
    "Consume secret inside dockerfile": "The secrets should be used during run time and provided by execution environment.\nAlso everything that is executing during a container build is written down as layers and available later to anyone who is able to get access to an image. That's why it's hard to consume secrets during the build in a secure way.\nIn order to address this, Docker recently introduced a special option --secret. To make it work, you will need the following:\nSet environment variable DOCKER_BUILDKIT=1\nUse the --secret argument to docker build command\nDOCKER_BUILDKIT=1 docker build --secret id=mysecret,src=mysecret.txt\nAdd a syntax comment to the very top of your Docker file\n# syntax = docker/dockerfile:1.0-experimental\nUse the --mount argument to mount the secret for every RUN directive that needs it\nRUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret\nPlease note that this needs Docker version 18.09 or later.",
    "Docker File - Skipping Project. Because it was not found": "Based on your input I propose below folder structure and Dockerfile.\n[Solution] 'BuySellApi' (3 Projects)\n  |\n  +-- Dockerfile\n  | \n  +-- [BuySellApi]\n  |    |\n  |    +--- BuySellApi.csproj\n  |\n  +-- [BuySellApi.Core]\n  |    |\n  |    +--- BuySellApi.Core.csproj\n  |\n  +-- [BuySellApi.Data]\n       |\n       +--- BuySellApi.Data.csproj\nDockerfile\nFROM microsoft/dotnet:2.2-aspnetcore-runtime AS base\nWORKDIR /app\nEXPOSE 5000\nENV ASPNETCORE_URLS=http://+:5000\n    \nFROM microsoft/dotnet:2.2-sdk AS build\nWORKDIR /src\nCOPY . .\nRUN dotnet restore \". BuySellApi/BuySellApi.csproj\"\nWORKDIR \"/src/BuySellApi\"\nRUN dotnet build \"BuySellApi.csproj\" -c Release -o /app\n    \nFROM build AS publish\nWORKDIR \"/src/BuySellApi\"\nRUN dotnet publish \"BuySellApi.csproj\" -c Release -o /app\n    \nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app .\nENTRYPOINT [\"dotnet\", \"BuySellApi.dll\", \"--server.urls\", \"http://0.0.0.0:5000\"]",
    "How can I pass 'yes' response when npm installing on Dockerfile": "I used the following to install Angular without usage statistics sharing.\nRUN echo n | npm install -g --silent @angular/cli\nI think echo y should work for you",
    "What is causing \"Could not find data collector 'XPlat Code Coverage'\" error in Docker image?": "I was getting the same error and the fix for me was to add the nuget reference below.\n<PackageReference Include=\"coverlet.collector\" Version=\"1.0.1\" />\nI was trying to get the code coverage working in my azure devops pipeline and this did the trick for me.\nI was following below tutorial, with the default template api (WeatehrForecast example)\nhttps://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/dotnet-core?view=azure-devops",
    "Dockerizing React in production mode: FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory": "The NODE_OPTIONS solution did not work for me, I am using Node v14 and React Scripts v4.\nThis solution from GitHub finally helped - https://github.com/wojtekmaj/react-pdf/issues/496#issuecomment-566200248\nGENERATE_SOURCEMAP=false\nAdded to my Dockerfile right before the build command.",
    "How to make lightweight docker image for python app with pipenv": "The problem comes when you need things like ciso8601, or some libraries, requiring build process. Build tools are not \"incorporated\" into the both slim and alpine variants, for low-size footprint.\nSo to install deps, you'll have to:\nInstall build tools\nDeploy dependencies from Pipfile.lock system-wide\nUninstall build tools and clean caches\nAnd do that 3 actions inside a single RUN layer, like following:\nFROM python:3.7-slim\n\nWORKDIR /app\n\n# both files are explicitly required!\nCOPY Pipfile Pipfile.lock ./\n\nRUN pip install pipenv && \\\n  apt-get update && \\\n  apt-get install -y --no-install-recommends gcc python3-dev libssl-dev && \\\n  pipenv install --deploy --system && \\\n  apt-get remove -y gcc python3-dev libssl-dev && \\\n  apt-get autoremove -y && \\\n  pip uninstall pipenv -y\n\nCOPY app ./\n\nCMD [\"python\", \"app.py\"]\nManipulating build system would cost you around 300MiB and some extra time\nUninstalling pipenv would save you another 20MiB (which is 10% of resulting size).\nSeparating RUN commands would not delete data from layers, and would result in ~500MiB image. That's docker specifics.\nSo that would result in perfectly working ~200MiB sized image, which is\n5 times less than original python:3.7, (that is >1.0GiB)\nHas no alpine incompabilities (these are typically tied to glibc replacement)\nAt the time, we're fine with slim (debian buster) build variants, preferring slim over alpine (for most compatibility). If you're really up to further size optimization, I'd recommend you to take a look at some excellent builds of these guys:\nAlpine Python\n12.7MiB MariaDB",
    "Docker ENTRYPOINT with ENV variable and optional arguments": "It appears it isn't possible to create an ENTRYPOINT that directly supports both variable expansion and additional command line arguments. While the shell form of ENTRYPOINT will expand ENV variables at run time, it does not accept additional (appended) arguments from the docker run command. While the exec form of ENTRYPOINT does support additional command line arguments, it does not create a shell environment by default so ENV variables are not expanded.\nTo get around this, bash can be called explicitly in the exec form to execute a script that then expands ENV variables and passes command line args to the python process. Here is an example Dockerfile that does this:\nFROM ubuntu:16.04\nARG MODULE_NAME=foo\nENV MODULE_NAME=${MODULE_NAME}\n\nRUN apt-get update -y && apt-get install -y python3.5\n\n# Create the module to be run\nRUN echo \"import sys; print('Args are', sys.argv)\" > /foo.py\n\n# Create a script to pass command line args to python\nRUN echo \"/usr/bin/python3.5 -m $MODULE_NAME \\$@\" > /run_module.sh\n\nENTRYPOINT [\"/bin/bash\", \"/run_module.sh\"]\nOutput from the docker image:\n$ docker run my-image\nArgs are ['/foo.py']\n\n$ docker run my-image a b c\nArgs are ['/foo.py', 'a', 'b', 'c']\nNote that variable expansion occurs during the RUN commands (since they are using shell form) so the contents of run_script.py in the image are:\n/usr/bin/python3.5 -m foo $@\nIf the final RUN command is replaced with this:\n    RUN echo \"/usr/bin/python3.5 -m \\$MODULE_NAME \\$@\" > /run_module.sh\nthen the run_script.sh would contain\n/usr/bin/python3.5 -m $MODULE_NAME $@\nBut output from the running container would be the same since variable expansion will occur at run time. A potential benefit of the second version is that one could override the module to be run at run time without replacing the ENTRYPOINT.",
    "Docker non-root User Best Practices for Python Images?": "In general, the easiest safe approach is to do everything in your Dockerfile as the root user until the very end, at which point you can declare an alternate USER that gets used when you run the container.\nFROM ???\n# Debian adduser(8); this does not have a specific known uid\nRUN adduser --system --no-create-home nonroot\n\n# ... do the various install and setup steps as root ...\n\n# Specify metadata for when you run the container\nUSER nonroot\nEXPOSE 12345\nCMD [\"my_application\"]\nFor your more specific questions:\nIs installing packages with apt-get as root ok?\nIt's required; apt-get won't run as non-root. If you have a base image that switches to a non-root user you need to switch back with USER root before you can run apt-get commands.\nBest location to install these packages?\nThe normal system location. If you're using apt-get to install things, it will put them in /usr and that's fine; pip install will want to install things into the system Python site-packages directory; and so on. If you're installing things by hand, /usr/local is a good place for them, particularly since /usr/local/bin is usually in $PATH. The \"user home directory\" isn't a well-defined concept in Docker and I wouldn't try to use it.\nWhen installing python packages with pip as root, I get the following warning...\nYou can in fact ignore it, with the justification you state. There are two common paths to using pip in Docker: the one you show where you pip install things directly into the \"normal\" Python, and a second path using a multi-stage build to create a fully-populated virtual environment that can then be COPYed into a runtime image without build tools. In both cases you'll still probably want to be root.\nAnything else I am missing or should be aware of?\nIn your Dockerfile:\n## get UID/GID of host user for remapping to access bindmounts on host\nARG UID\nARG GID\nThis is not a best practice, since it means you'll have to rebuild the image whenever someone with a different host uid wants to use it. Create the non-root user with an arbitrary uid, independent from any specific host user.\nRUN usermod -aG sudo flaskuser\nIf your \"non-root\" user has unrestricted sudo access, they are effectively root. sudo has some significant issues in Docker and is never necessary, since every path to run a command also has a way to specify the user to run it as.\nRUN chown flaskuser:users /tmp/requirements.txt\nYour code and other source files should have the default root:root ownership. By default they will be world-readable but not writeable, and that's fine. You want to prevent your application from overwriting its own source code, intentionally or otherwise.\nRUN chmod -R  777 /usr/local/lib/python3.11/site-packages/*\nchmod 0777 is never a best practice. It gives a place for unprivileged code to write out their malware payloads and execute them. For a typical Docker setup you don't need chmod at all.\nThe bind mounted workspace is only for development, for a production image I would copy the necessary files/artifacts into the image/container.\nIf you use a bind mount to overwrite all of the application code with content from the host, then you're not actually running the code from the image, and some or all of the Dockerfile's work will just be lost. This means that, when you go to production without the bind mount, you're running an untested setup.\nSince your development environment will almost always be different from your production environment in some way, I'd recommend using a non-Docker Python virtual environment for day-to-day development, have good (pytest) unit tests that can run outside the container, and do integration testing on the built container before deploying.\nPermission issues can also come up if your application is trying to write out files to a host directory. The best approach here is to restructure your application to avoid it, storing the data somewhere else, like a relational database. In this answer I discuss permission setup for a bind-mounted data directory, though that sounds a little different from what you're asking about here.",
    "Docker container with entrypoint variable expansion and CMD parameters": "With /bin/sh -c \"script\" syntax, anything after the -c argument becomes an argument to your script. You can reach them with $0 and $@ as part of your /bin/sh script:\nENTRYPOINT [\"/bin/sh\", \"-c\", \"exec /usr/bin/mycmd --token=$MY_TOKEN $0 $@\"]\nCMD [\"pull\", \"stuff\"]\nNote that you could also change your entrypoint to be a shell script added to your image that runs exec /usr/bin/mycmd --token=$MY_TOKEN \"$@\" and execute that shell script with docker's exec syntax:\nENTRYPOINT [\"/entrypoint.sh\"]",
    "How to use the path with the space in docker file": "Use the JSON form, you have to use double backslashes inside the braces\nFROM microsoft/windowsservercore \nCOPY [\"C:\\\\docker\\\\prerequisites\\\\MicrosoftSDKs\", \"C:\\\\Program Files (x86)\\\\MicrosoftSDKs\"]\nYou can also use slash:\nCOPY [\"C:/Program Files/nodejs\", \"/windows/system32\"]",
    "Add a new entrypoint to a docker image": "I finally ended up calling the original entrypoint bash script in my new entrypoint bash script, before doing other extra configuration steps.",
    "Dockerfile ADD failed : No Source files were specified": "It generally is recommended to use COPY before ADD, because it serves a lesser purpose and is somewhat more lightweight.\nTo copy your whole directory into the image, just add the following line after editing:\n COPY . /path/to/dir/in/image\nSome helpful links to start writing dockerfiles:\nReference\nBest Practices\nPostgresql example",
    "entrypoint: \"entrypoint.sh\" - docker compose": "entrypoint: \"entrypoint.sh\" overrides ENTRYPOINT [\"test.sh\"] from Dockerfile.\nFrom the docs:\nSetting entrypoint both overrides any default entrypoint set on the service\u2019s image with the ENTRYPOINT Dockerfile instruction, and clears out any default command on the image - meaning that if there\u2019s a CMD instruction in the Dockerfile, it is ignored.\nENTRYPOINT [\"test.sh\"] is set in Dockerfile describing docker image\nentrypoint: \"entrypoint.sh\" is set in docker-compose file which describes multicontainer environment while referencing the Dockerfile.\ndocker-compose build builder will build image and set entrypoint to ENTRYPOINT [\"test.sh\"] set in Dockerfile.\ndocker-compose up builder will start container with entrypoint entrypoint.sh pip wheel --no-index '-f /build' . set in docker-compose file",
    "How to run an electron app on docker": "I will try to help you here in this answer - too long for comment.\nI tried your Docker file on my Win10 and with the same problems. But I figured it out by adding required packages and successfully created docker image. Here is Dockerfile\nFROM node:slim\n\nCOPY . /usr/scr/app\n\n#RUN rm bdstart.sh\nRUN apt-get update\n\n# I think you need to install following \nRUN apt-get -y install libgtkextra-dev libgconf2-dev libnss3 libasound2 libxtst-dev libxss1\nRUN npm install --save-dev electron\n\nRUN npm install\n\nCMD [\"/usr/scr/app/start.sh\"]\nand here is your start.sh\n#!/bin/sh\n./node_modules/.bin/electron ./src\nActually I don't have access to your files and so on, but with this DockerFile was able to create docker image without problems. I also went inside docker container and check whether is possible to run electron - worked.\nIf you want to go into container, you just need to build docker image. I have done it by (simplest way) following command (open console where Dockerfile is located and run):\ndocker build -t test-image .\nAfter Successfully build of image you can run container. If any problems I recommend you to run container with bash entrypoint and debug what fails - bash will open in the same console where you type following script)\ndocker run -it test-image bash",
    "Docker - cannot copy to non-directory: /var/lib/docker/overlay2/xw77p2bxfkhhnwqs5umpl7cbi/merged/app/.git": "Just verify the .git is actually a file or folder. Or check for any name conflict between a folder and file.\nSeems like you are trying to copy a folder to a file(non-directory).\nI have faced a similar issue error: failed to solve: cannot replace to directory /var/lib/docker/overlay2/*/*/folder_a with file. Turns out i have a binary file with a name 'folder_a'.\nDeleting the file which matches the folder_name solved the issue for me.",
    "Docker Multi-stage build - Copy failing": "When you build the application it's building in another cointainer/layer. You i'll need to build the application before and copy the build folder to /usr/src/app.\nSo, this is fine:\nFROM node:9.6.1 as builder\n\nWORKDIR /usr/src/app\n\nENV PATH /usr/src/app/node_modules/.bin:$PATH\n\nCOPY package.json .\nCOPY public public\nCOPY src src\n\nRUN npm install --silent\nRUN npm run build\n\nCOPY build .\n\nRUN rm -rf src\nRUN rm -rf build\n\nFROM nginx:1.13.9-alpine\n\nCOPY --from=builder /usr/src/app /usr/share/nginx/html\n\nEXPOSE 80\nI'm removing the src and build folders since that's not necessary and can expose an critical part of your application.\nHence, no doubt about the security of dockerizing an application.",
    "Fixing World-writable MySql error in Docker": "I just encountered this issue and the fix for me is just to set my.cnf file to read-only in Windows.",
    "Import Data on MongoDB using Docker-Compose": "I ended up removing the Dockerfile, adding the commands in a bash script, then calling the script from the docker-compose file. Used a script rather than one command in the docker-compose file because I'm importing several files thus several commands that are not shown in my example. I needed to use mongo:3.2.6 to make this work. There may be other versions but this one works for sure.\ndocker-compose.yml\nversion: '3'\nservices:\n  mongodb:\n    image: mongo:3.2.6\n    ports:\n      - 27017:27017\n\n  mongo_seed:\n    image: mongo:3.2.6\n    links:\n      - mongodb\n    volumes:\n      - ./mongo-seed:/mongo-seed\n    command:\n      /mongo-seed/import.sh\n/mongo-seed/import.sh\n#! /bin/bash\n\nmongoimport --host mongodb --db test --collection census --type json --file /mongo-seed/census.json --jsonArray",
    "Docker \"Failed to solve: Canceled: context canceled\" when loading build context": "Altough downgrading docker worked, the actual problem was that I didn't exclude node_modules in Dockerignore. I had been running the containers fine for quite a long time.\nAfter purging all containers and images and adding the node_modules/ line to my .dockerignore it fixed it.\nI'm guessing the error had to do with the amount of files inside certain directories.",
    "How to dockerize Jupyter lab": "When you start jupyter lab you should define --ip parameter. For example, --ip=0.0.0.0.\nAfter this you will have another error:\n[C 08:14:56.973 LabApp] Running as root is not recommended. Use --allow-root to bypass.\nSo, if you want to proceed you need to add --allow-root as well.\nThe final Dockerfile is:\nFROM python:3.6\n\nWORKDIR /jup\n\nRUN pip install jupyter -U && pip install jupyterlab\n\nEXPOSE 8888\n\nENTRYPOINT [\"jupyter\", \"lab\",\"--ip=0.0.0.0\",\"--allow-root\"]",
    "rpc error: code = Unknown desc = failed to build LLB": "After a whole day of struggle, I fixed this by giving Docker more RAM (from 2Gb to 6Gb) and CPU (from 2 to 3)...",
    "Build docker in ASP.NET Core: \"no such file or directory\" error": "It's happening because you didn't published your solution. The error message is self-explanatory:\nno such file or directory\nBy default, when you add Docker support to you ASP.NET Core Visual Studio 2017 project, it creates a bunch of docker-compose.yml files, one of them is docker-compose.ci.build.yml which handles the build process. Then, when you build the project through Visual Studio, full docker-compose pipeline is executed.\nThe contents of docker-compose.ci.build.yml, are similiar to this (it depends on custom config and project names obviously):\nversion: '2'\n\nservices:\n  ci-build:\n    image: microsoft/aspnetcore-build:1.0-1.1\n    volumes:\n      - .:/src\n    working_dir: /src\n    command: /bin/bash -c \"dotnet restore ./SolutionName.sln && dotnet publish ./SolutionName.sln -c Release -o ./obj/Docker/publish\"\nAs you can see in the last line, there is a dotnet publish command invoked, which actually builds & publishes your project.\nSo the solution for your issue, will be just building the project before calling docker:\ndotnet publish ./SolutionName.sln -c Release -o ./obj/Docker/publish\ndocker build -t my-docker-image-test .",
    "Request against localhost relative url \"Cannot assign requested address\"": "I am using the same ASP.NET Core docker version and it seems work if you do this:\nreplace http://localhost:<port>\nwith\nhttp://host.docker.internal:<port>\nTry again and check if it works for you!",
    "Docker compose global level logging": "You could also configure the Docker default for this, all your container will have the configuration (that you can override per container).\nHere an example of solution with YAML anchor:\nversion: \"2\"\n\nservices:\n\n  proxy:\n    build: proxy\n    image: kinoulink/proxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    container_name: ktv_manager_proxy\n    environment:\n        - HTTP_AUTH_PASSWORD=$KTV_MANAGER_PASSWORD\n    logging: &logging\n      driver: \"awslogs\"\n      options:\n      awslogs-region: eu-west-1\n      awslogs-group: docker\n\n  rancher:\n    image: rancher/server:v1.1.3\n    volumes:\n      - rancher_mysql:/var/lib/mysql\n      - rancher_cattle:/var/lib/cattle\n    labels:\n      ktv.infra.proxy.domain: 'rancher'\n      ktv.infra.proxy.port: '8080'\n    logging:\n      <<: *logging\nFrom v3.4 (as @tekHedd said), you can use \"extension field\" syntax:\nversion: \"3.4\"\n\nx-logging: \n      &default-logging\n      driver: \"awslogs\"\n      options:\n      awslogs-region: eu-west-1\n      awslogs-group: docker\n\nservices:\nproxy:\n    build: proxy\n    image: kinoulink/proxy\n    ports:\n      - 80:80\n      - 443:443\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    container_name: ktv_manager_proxy\n    environment:\n        - HTTP_AUTH_PASSWORD=$KTV_MANAGER_PASSWORD\n    logging: *default-logging\n\n  rancher:\n    image: rancher/server:v1.1.3\n    volumes:\n      - rancher_mysql:/var/lib/mysql\n      - rancher_cattle:/var/lib/cattle\n    labels:\n      ktv.infra.proxy.domain: 'rancher'\n      ktv.infra.proxy.port: '8080'\n    logging: *default-logging",
    "Docker add files to VOLUME": "Whatever you put in your Dockerfile is just evaluated at build time (and not when you are creating a new container).\nIf you want to make file from the host available in your container use a data volume:\ndocker run -v /host_dir:/container_dir ...\nIn case you just want to copy files from the host to a container as a one-off operation you can use:\ndocker cp /host_dir mycontainer:/container_dir",
    "Dockerfile COPY and RUN in one layer": "take a look to multi-stage:\nUse multi-stage builds\nWith multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don\u2019t want in the final image. To show how this works, let\u2019s adapt the Dockerfile from the previous section to use multi-stage builds.\nDockerfile:\nFROM golang:1.7.3\nWORKDIR /go/src/github.com/alexellis/href-counter/\nRUN go get -d -v golang.org/x/net/html  \nCOPY app.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .\n\nFROM alpine:latest  \nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\nCOPY --from=0 /go/src/github.com/alexellis/href-counter/app .\nCMD [\"./app\"]  ",
    "Passing env variables to DOCKER Spring Boot": "The easiest (and probably the best way) to do it via environment variable in a docker container:\nSPRING_PROFILES_ACTIVE=dev,swagger\nUPDATE:\nIn order to set environment variables to docker, you do not need to modify Dockerfile. Just build your docker image and then run it with the env variables set:\ndocker run your-docker-container -e SPRING_PROFILES_ACTIVE='dev,swagger' -p 8080:8080",
    "run sed in dockerfile to replace text with build arg value": "Replace the single quotes ' around s/localhost/${DOCKER_HOST}/g with double quotes \". Variables will not be interpolated within single quotes.",
    "Running an executable in a dockerfile": "Bear in mind that name.exe have to be in same directory as your dockerfile. From the documentation:\nThe <src> path must be inside the context of the build; you cannot COPY ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\nYour dockerfile could look like this then:\nFROM ubuntu\nMAINTAINER me@gmail.com\nCOPY name.exe /bin/\nCMD [\"/bin/name.exe\", \"input1\", \"output\"]\nYou can build it then like this:\ndocker build --tag=me/my-image .\nAnd when you run it (docker run me/my-image), it will run /bin/name.exe input1 output.",
    "Docker alpine + oracle java: cannot find java": "You cannot achieve what you want\nAlpine Linux uses MUSL as a Standard C library.\nOracle's Java for linux depends on GNU Standard C library (gclib).\nHere is a bit more detailed info and official stance from Oracle on the topic\nthe JDK source code has not yet been ported to Alpine Linux, or more specifically, the musl C library. That is, it turns out that the thing about Alpine Linux that sticks out/is different from a JDK source code perspective is the C library.\nThe solution\nIf you looking for small Java Docker images, use OpenJDK ones.\nopenjdk:11-jre-slim image is only 77MB.\nIf you insist, on your head be it...\nThere is theoretical way, but it is not as trivial as you think.\nYou can find many examples of Alpine images running with OracleJDK like here or see expert's answer to this question as well. They add the missing Standard GNU C library.\nBe warned however...\nAll of these solutions could be in breach of Oracle's license agreement stating that the license is non-transferable, and the distributable is non-modifiable. In the Dockerfiles you will find however:\nCookie: oraclelicense=accept-securebackup-cookie\"\nand many entries similar to\nrm -rf ${JAVA_HOME}/*src.zip\nFor further details about legality of prepackaged Oracle's JRE or JDK Docker images see this article.",
    "Run SQL script after start of SQL Server on docker": "RUN gets used to build the layers in an image. CMD is the command that is run when you launch an instance (a \"container\") of the built image.\nAlso, if your script depends on those environment variables, if it's an older version of Docker, it might fail because those variables are not defined the way you want them defined!\nIn older versions of docker the Dockerfile ENV command uses spaces instead of \"=\"\nYour Dockerfile should probably be:\nFROM microsoft/mssql-server-windows-express\nCOPY ./create-db.sql .\nENV ACCEPT_EULA Y\nENV SA_PASSWORD ##$wo0RD!\nRUN sqlcmd -i create-db.sql \nThis will create an image containing the database with your password inside it.\n(If the SQL file somehow uses the environment variables, this wouldn't make sense as you might as well update the SQL file before you copy it over.) If you want to be able to override the password between the docker build and docker run steps, by using docker run --env sa_password=##$wo0RD! ..., you will need to change the last line to:\nCMD sqlcmd -i create-db.sql && .\\start -sa_password $env:SA_PASSWORD \\\n-ACCEPT_EULA $env:ACCEPT_EULA -attach_dbs \\\"$env:attach_dbs\\\" -Verbose\nWhich is a modified version of the CMD line that is inherited from the upstream image.",
    "Setting up docker nodejs application with local npm dependencies": "Yes, it's possible but a little bit ugly. The problem for you is that Docker is very restrictive when it comes to its build context. I'm not sure how familiar you are already with that concept, so here is the introduction from the documentation:\nThe docker build command builds an image from a Dockerfile and a context.\nFor example, docker build . uses . as its build context, and since it's not specified otherwise, ./Dockerfile as the Dockerfile. Files or paths outside the build context cannot be referenced in the Dockerfile (so no COPY ..).\nThe issue for you is that during a Docker build, the build context cannot be left. If you have multiple applications that you want to build, you would normally add a Dockerfile for each app.\nsrc/\n\u251c\u2500\u2500 apps   \n\u2502   \u251c\u2500\u2500 my_app\n\u2502   \u2502   \u2514\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 my_other_app\n\u2502       \u2514\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 shared\n    \u2514\u2500\u2500 shared_module\nNaturally, you would cd into my_app and use docker build . to build the application's Docker image. The issue with this is that you can't access ../../shared from the build, since it's outside of the context.\nSo you need to make sure both apps and shared is in the build context. One way would be to place all Dockerfile in src like so:\nsrc/\n\u251c\u2500\u2500 Dockerfile.my_app\n\u251c\u2500\u2500 Dockerfile.my_other\n\u251c\u2500\u2500 apps\n\u2502   \u251c\u2500\u2500 my_app\n\u2502   \u2514\u2500\u2500 my_other_app\n\u2514\u2500\u2500 shared\n    \u2514\u2500\u2500 shared_module\nYou can then build the applications by explicitly specifying the context and the Dockerfile:\nsrc$ docker build -f Dockerfile.my_app .\nAlternatively, you can keep the Dockerfiles inside my_app and my_other_app, and point to them:\nsrc$ docker build -f apps/my_app/Dockerfile .\nThat should also work. In both cases, the build is executed from within src, which means you need to pay a little attention to the paths in the Dockerfile. The working directory is still src:\nCOPY ./apps/my_app /src/apps/my_app\nBy mirroring the folder structure you have locally, you should be able to make your dependencies work without any changes:\nRUN mkdir -p /src\nCOPY ./shared /src/shared\nCOPY ./apps/my_app /src/apps/my_app\nRUN cd /src/apps/my_app && npm install\nHope that helps you get started.",
    "Tilde Expansion Doesn't Work in Docker COPY Command": "Tilde expansion for COPY is not supported.\nFrom The COPY docs:\nThe dest is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container.\nExample:\nCOPY test relativeDir/   # adds \"test\" to `WORKDIR`/relativeDir/\nCOPY test /absoluteDir/  # adds \"test\" to /absoluteDir/",
    "How to build docker with non-root user privileges to setup python application with pipenv?": "There's nothing wrong with installing software \"globally\" in a Docker image (which will generally only do one thing), and to committing to some implementation details like container-internal usernames and paths. It's totally fine to install software as root and switch to a non-root user to actually run the image.\nI might write this Dockerfile like:\nFROM python:3.6\n\n# Globally install pipenv\nRUN pip3 install pipenv\n\n# Set up the app directory (Docker will create it for us)\nWORKDIR /myapp\nCOPY . ./\nRUN pipenv install --system --deploy --ignore-pipfile\n\n# Establish the runtime user (with no password and no sudo)\nRUN useradd -m myapp\nUSER myapp\n\n# Normal image metadata\nEXPOSE 8002\nCMD gunicorn -k tornado server:app -b 0.0.0.0:8002 -w 4 -p server.pid",
    "How can I run ENTRYPOINT as root user?": "Delete the USER jenkins line in your Dockefile.\nChange the user at the end of your entrypoint script (/root/startup.sh).\nby adding: su - jenkins man su\nExample:\nDockerfile\nFROM debian:8\n\nRUN useradd -ms /bin/bash exemple\n\nCOPY entrypoint.sh /root/entrypoint.sh\n\nENTRYPOINT \"/root/entrypoint.sh\"\nentrypoint.sh\n#!/bin/bash\n\necho \"I am root\" && id\n\nsu - exemple\n\n# needed to run parameters CMD\n$@\nNow you can run\n$ docker build -t so-test .\n$ docker run --rm -it so-test bash\nI am root\nuid=0(root) gid=0(root) groups=0(root)\nexemple@37b01e316a95:~$ id\nuid=1000(exemple) gid=1000(exemple) groups=1000(exemple)\nIt's just a simple example, you can also use the su -c option to run command with changing user.",
    "Pass arguments to parent Dockerfile": "Build arguments are not persisted in images, so they will not be available in builds FROM a parent image.\nUnlike an ARG instruction, ENV values are always persisted in the built image.\nARG variables are not persisted into the built image as ENV variables are.\nThe arguments can be persisted by storing them somewhere, the easiest place is in an environment variable.\nARG IMAGE_USER=jenkins\nENV IMAGE_USER=$IMAGE_USER\nAll RUN steps in the child image will then have access to IMAGE_USER in their environment.",
    "Reloading code in a dockerized node.js app with docker-compose": "I have seen the phrase \"live reload\" used to apply to two different types of reloading:\nkilling & restarting the application when the code changes, and\nautomatically reloading HTML & assets on a client browser when the code changes.\nBased on your question, I think you're referring to the first type, so the answer that follows addresses that.\nThe problem here is one of context.\nRemember that the docker container is isolated from your host - specifically, the processes running in the container are distinct from (and generally cannot interact with) processes running on the host. In your case, you have chosen to mount a host directory in the container, but that's just the filesystem, not the processes.\nThink through what your Docker image does when you instantiate a new container: it runs node index.js in the WORKDIR. Where is the code to stop it and restart it when the code changes? Presumably it's running in a process on the host. This means that it cannot touch the node process running in the container (because it's isolated).\nNow, you haven't mentioned what method you're using to handle the live reloading, but that shouldn't make too much of a difference. They all basically work the same way: on a change to the application code, kill the existing process and start a new one.\nTo solve this, you have two options:\nrun the \"live reloading\" code inside the container, or\nrun your development code outside the container\nFor the first, you could follow @MarkS's suggestion and use nodemon. This should be as simple as replacing\nCMD [\"node\", \"index.js\"]\nin your Dockerfile with\nCMD [\"nodemon\", \"index.js\"]\nprovided, of course, that you have nodemon properly installed in the image.\nThe alternative, and this is what I do, is to run the code on the host outside the Docker environment during development, and then package it up in an image at deployment. This solves two problems:\nthe problem you're running into with isolated node processes, and\npermission problems.\nRemember that apps running in Docker are run as root. This means that if your app creates files, they're going to be owned by root. I tried developing in a Docker environment, but got frustrated by problems where, for example, I wanted to delete a file created by the app and had to sudo (sign in as root) just to clean up stuff.",
    "Run Grunt / Gulp inside Docker container or outside?": "I'd like to suggest a third approach that I have done for a static generated site, the separate build image.\nIn this approach, your main Dockerfile (the one in project root) becomes a build and development image, basically doing everything in the second approach. However, you override the CMD at run time, which is to tar up the built dist folder into a dist.tar or similar.\nThen, you have another folder (something like image) that has a Dockerfile. The role of this image is only to serve up the dist.tar contents. So we do a docker cp <container_id_from_tar_run> /dist. Then the Dockerfile just installs our web server and has a ADD dist.tar /var/www.\nThe abstract is something like:\nBuild the builder Docker image (which gets you a working environment without webserver). At thist point, the application is built. We could run the container in development with grunt serve or whatever the command is to start our built in development server.\nInstead of running the server, we override the default command to tar up our dist folder. Something like tar -cf /dist.tar /myapp/dist.\nWe now have a temporary container with a /dist.tar artifact. Copy it to your actual deployment Docker folder we called image using docker cp <container_id_from_tar_run> /dist.tar ./image/.\nNow, we can build the small Docker image without all our development dependencies with docker build ./image.\nI like this approach because it is still all Docker. All the commands in this approach are Docker commands and you can really slim down the actual image you end up deploying.\nIf you want to check out an image with this approach in action, check out https://github.com/gliderlabs/docker-alpine which uses a builder image (in the builder folder) to build tar.gz files that then get copied to their respective Dockerfile folder.",
    "How do I change the save location of docker containers logs?": "Straight- forward answer - NO, you can't\nWhy?\nThe file written by the json-file logging driver are not intended for consumption by external software, and should be regarded the \"internal storage mechanism\" for the JSON logging driver. For that reason, the location is not configurable.\nIf you want to have the logs written to a different location, consider using (e.g.) the syslog driver, journald, or one of the drivers that allow sending the logs to a central log aggregator\nSource: https://github.com/moby/moby/issues/29680",
    "How to install kubectl in kubernetes container through docker image": "put this in your Dockerfile\nRUN curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\nRUN chmod +x ./kubectl\nRUN mv ./kubectl /usr/local/bin",
    "can't create virtualenv on Ubuntu 18.04 with Python 3.8": "As workaround\nsudo ln -s   /usr/lib/python3.8/_sysconfigdata__linux_x86_64-linux-gnu.py  /usr/lib/python3.8/_sysconfigdata__x86_64-linux-gnu.py\nBut i hope it will be fixed soon in Ubuntu.",
    "Install Oracle Instant client into Docker container for Python cx_Oracle": "After many hours trying it, I finally solved it with this Dockerfile\nNote I am using python 3.7, Django 3.0, Oracle Database 12c and Pipenv for package management\nFROM python:3.7.5-slim-buster\n\n# Installing Oracle instant client\nWORKDIR    /opt/oracle\nRUN        apt-get update && apt-get install -y libaio1 wget unzip \\\n            && wget https://download.oracle.com/otn_software/linux/instantclient/instantclient-basiclite-linuxx64.zip \\\n            && unzip instantclient-basiclite-linuxx64.zip \\\n            && rm -f instantclient-basiclite-linuxx64.zip \\\n            && cd /opt/oracle/instantclient* \\\n            && rm -f *jdbc* *occi* *mysql* *README *jar uidrvci genezi adrci \\\n            && echo /opt/oracle/instantclient* > /etc/ld.so.conf.d/oracle-instantclient.conf \\\n            && ldconfig\n\nWORKDIR    /app\nCOPY       . .  # Copy my project folder content into /app container directory\nRUN        pip3 install pipenv\nRUN        pipenv install\nEXPOSE     8000\n# For this statement to work you need to add the next two lines into Pipfilefile\n# [scripts]\n# server = \"python manage.py runserver 0.0.0.0:8000\"\nENTRYPOINT [\"pipenv\", \"run\", \"server\"]",
    "How to install a local rpm file when building docker instance?": "Put this line before your rpm -i command:\nADD /host/abs/path/to/chrpath-0.13-14.el7.x86_64.rpm /chrpath-0.13-14.el7.x86_64.rpm\nThen you'll be able to do\nRUN rpm -i chrpath-0.13-14.el7.x86_64.rpm",
    "How to retrieve file from docker container?": "There are multiple ways to do this.\nUsing docker cp:\ndocker cp <container_hash>:/path/to/zip/file.zip /path/on/host/new_name.zip\nUsing docker volumes:\nAs you were leading to in your question, you can also mount a path from the container to your host. You can either do this by specifying where on the host you want the mount point to be or don't specify where the mount point is and let docker choose. Both these paths require different approaches.\nLet docker choose host mount location\ndocker volume create random_volume_name\ndocker run -d --name ubuntu-libs -v random_volume_name:<path/to/mount/in/container> ubuntu-libs\nThe content will be located on your host, here:\nls -l /var/lib/docker/volumes/random_volume_name/_data/ \nLet me choose host mount location\ndocker run -d --name ubuntu-libs -v <existing/mount/point/on/host>:<path/to/mount/in/container> ubuntu-libs\nThis creates a clean/empty location that is shared as per the locations defined in the command. Now you need to modify your Dockerfile to copy the artifacts to this path, something like:\nFROM ubuntu\n\nRUN apt-get update && apt-get install -y build-essentials gcc\n\nENTRYPOINT [\"zip\",\"-r\",\"-9\"]\nCMD [\"sh\", \"-c\", \"/lib64.zip\", \"/lib64\", \"cp\", \"path/to/zip/file.zip\", \"<path/to/mount/in/container>\"]\nThe content will now be located on your host, here:\nls -l <existing/mount/point/on/host>\nI got to give a shout out to @joaofnfernandes from here, who does a great job explaining.",
    "How to setup Node environment variable in Dockerfile for running node.js application?": "There a two ways, while building the image or when running the container.\nFor builds:\nAdd to your Dockerfile\nENV NODE_ENV=whatEver\nOr use build arguments if you don't want them to stick during runtime Docker build --build-args NODE_ENV whatEver\nWhen running:\nRun your container with \"-e\"\ndocker run -e NODE_ENV=whatever mycontainer",
    "How to check for unuse images for your docker containers?": "Above answers help us find and remove the dangling images,, but not unused.\nSo to fetch all the unused docker images on the machine\nFetch all the images belonging to the running containers(which are not stopped or exited)\nFetch all the images on the machine\nThen filter the images in step 1 from step 2\nBelow is the basic shell script which could help do that\nrunningImages=$(docker ps --format {{.Image}})\ndocker images --format {{.Repository}}:{{.Tag}} | grep -v \"$runningImages\"\nJust be sure before removing unused images(not the dangling ones) just list them and then decide which one to delete manually.",
    "How to install and run wkhtmltopdf Docker image": "Perhaps this solution will help. Wkhtmltopdf will be install to /usr/bin/wkhtmltopdf\nRUN apt-get update \\\n    && apt-get install -y \\\n    ...\n    wkhtmltopdf \\\n    ...",
    "Docker: Error response from daemon: rpc error: code = 2 desc = \"oci runtime error: exec format error\"": "Did you post your complete entrypoint.sh? The kernel tries to recognize the file type by looking at the first bytes of the executable. For scripts you need to add a so-called shebang line. You might need to add a shebang line at the very top of your entrypoint.sh, e.g.:\n#!/bin/sh\n/usr/bin/docker-quickstart\nservice hadoop-hdfs-namenode restart\nhdfs dfs -mkdir -p input\nhdfs dfs -put /twitter.avro /input/twitter.avro\nspark-submit --class com.abhi.HelloWorld --master local[1] SparkIntegrationTestsAssembly.jar /input/twitter.avro /output",
    "Docker Django 404 for web static files, but fine for admin static files": "This was issue with the STATICFILES_DIRS configuration in the settings.py file.\nThis setting defines the additional locations the staticfiles app will traverse if the FileSystemFinder finder is enabled, e.g. if you use the collectstatic or findstatic management command or use the static file serving view.\nFollowing was the configuration in my settings.py:\nSTATIC_URL = '/static/'\nSTATIC_ROOT      =  os.path.join(BASE_DIR, \"static\") \nNow I updated this code to:\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, \"static\"),\n]\nAnd every files is loading fine.\nReference Link",
    "How to answer install question in dockerfile?": "I tested your Dockerfile and didn't have to answer any question, but if you want to auto answer when you run a command, you can use yes (for \"y\" and \"n\" responses) or echo.\nEx:\nyes | <YOUR COMMAND>\nyes n | <YOUR COMMAND> \"n\" response\necho | <YOUR COMMAND> echo generate a new line (enter)\necho \"some response\" | <YOUR COMMAND> response with \"some response\"",
    "What Docker image size is considered 'too large'?": "In my opinion, ideal size is only ideal for your exact case. For me and my current company, we have no image bigger than 1GB.\nIf you use an image of 10GB size and have no problems (is it even possible?!), then it is ok for your case.\nAs example of a problem case, you could consider a question such as: \"Is it ok that I am waiting 1-2 hours while my image is deploying over internet to the remote server/dev machine?\" In all likelihood, this is not ok. On the another hand, if you are not facing such a problem, then you have no problems at all.\nAnother problem is while small images start up for a couple of seconds, the huge one starts up for minutes. It also can break a \"hot deploy\" scheme if you use it.\nIt also could be appropriate to check why your image is so big. You can read how layers work.\nConsider the following two Dockerfiles:\nFirst:\nRUN download something huge that weighs 5GB\nRUN remove that something huge from above\nSecond:\nRUN download something huge that weighs 5GB &&\\\n    remove that something huge from above\nThe image built from the second Dockerfile weighs 5GB less than that from the first, while they are the same inside.\nAnother trick is to use a small, basic image from the beginning. Just compare these differences:\nIMAGE NAME     SIZE\nbusybox        1 MB\nalpine         3 MB\ndebian         125 MB\nubuntu         188 MB \nWhile debian and ubuntu are almost the same inside, debian will save you 50MB from the start, and will need fewer dependencies in future.",
    "Output of `tail -f` at the end of a docker CMD is not showing": "The docker filesystem uses copy-on-write with it's layered union fs. So when you write to a file that's part of the image, it will first make a copy of that file to the container filesystem which is a layer above all the image layers.\nWhat that means is when you append a line to the /var/log/cron.log, it will get a new inode in the filesystem and the file that the tail command is following at is no longer the one you see when you docker exec into the container. You can solve that with a minor change to append \"nothing\" to the file which also modifies the last update timestamp which forces a copy-on-write:\nCMD echo \"starting\" && echo \"continuing\" && (cron) \\\n && echo \"tailing...\" && : >> /var/log/cron.log && tail -f /var/log/cron.log\nI put together a gist that goes through this issue with a lot more detail over here: https://gist.github.com/sudo-bmitch/f91a943174d6aff5a57904485670a9eb",
    "Cannot assign requested address (localhost:xxxx) - Docker + Linux Containers": "Using host.docker.internal instead of localhost worked for me.\nSource: https://hamy.xyz/labs/docker-dotnet-core-cannot-assign-requested-address",
    "Building Docker image as non root user": "In order to use Docker, you don't need to be a root user, you just need to be inside of the docker user group.\nOn Linux:\nIf there is not already a docker group, you can create one using the command sudo groupadd docker.\nAdd yourself and any other users you would like to be able to access docker to this group using the command sudo usermod -aG docker [username of user].\nRelog, so that Linux can re-evaluate user groups.\nIf you are not trying to run the command as root, but rather want to run the container as non-root, you can use the following DOCKERFILE contents (insert after FROM but before anything else.)\n# Add a new user \"john\" with user id 8877\nRUN useradd -u 8877 john\n# Change to non-root privilege\nUSER john",
    "Which Dockerfile instructions are inherited in deriving images?": "These instruction are inherited from the base image along with system files.\nEXPOSE\nIf the base image mentioned these EXPOSE 8080 9090 ports in Dockerfile, then the extend Dockerfile do not to need to expose these port. But there is a difference between exposing and publish.\nENV\nIf the base image has some ENV like test-a=abc then extended image will have these ENVs.\nWorkingDir\nIf the base image have set \"WorkingDir\": \"/root\", then extended iamge will have working direcotry /root\nMAINTAINER\nMAINTAINER adiii extended image will have the same author if not overide.\nLabels\nThe extended image will have the same label as the base image\nonbuild\nDesigned to run by extended image.\nENTRYPOINT\nSame entrypoint as in the base image, unless you overwrite it.\nCMD\nThe extended image has the same CMD as the base image, as long as you do not overwrite the entrypoint instruction, see below.\nYou can try it.\nDockerfile A\nFROM node:8.16\nMAINTAINER adiii\nLABEL key=test\nEXPOSE 8080 9090\nENV test-a=abc\nWORKDIR /root\nENTRYPOINT /root\nCMD [\"npm\", \"run\", \"start\"]\nNow build docker image B\nDockerfile B\nFROM a\ndocker build -t b . Inspect the image b docker inspect b:latest you will see the above instruction iherited from the base image, because Dockerfile B did not overwrite the entrypoint instruction.\nIf the extended image overwrites the entrypoint, the documentation says CMD will reset to an empty value and must be redefined if wanted.",
    "adding startup script to dockerfile": "You can use the entrypoint to run the startup script. In the entrypoint you can specify your custom script, and then run catlina.sh.\nExample:\nENTRYPOINT \"bin/startup.sh && catalina.sh run\"\nThis will run your startup script and then start your tomcat server, and it won't exit the container.",
    "git commit hash in dockerfile as label": "I found it atlast,\nuse docker build --build-arg vcs-ref=$(git rev-parse --short HEAD)\nwhile building.\nBut have to initialize the variable in vcs-ref in Dockerfile\nARG vcs-ref=0\nENV vcs-ref=$vcs-ref",
    "How to push docker compose to docker hub": "You should add image names to your services, including your docker hub id, e.g.:\nservices:\n  web:\n    build: ./\n    image: docker-hub-id/web:latest\n    ...\nNow, you can just call docker-compose push.\nSee docker-compose push",
    "Unable to run a Docker image with a Rust executable": "As @Oleg Sklyar pointed out, the problem is that the Rust binary is dynamically-linked.\nThis may be a bit confusing because many people who have heard of Rust have also heard that Rust binaries are statically-linked, but this refers to the Rust code in crates: crates are linked statically because they are all known at the moment of compilation. This does not refer to existing C dynamic libraries that the program may link to, such as libc and other must-have libraries. Often times, these libraries can also be built as statically-linked artifacts (see the end of this post). To check whether your program or library is dynamically-linked, you can use ldd utility:\n$ ldd target/release/t\n    linux-vdso.so.1 (0x00007ffe43797000)\n    libdl.so.2 => /usr/lib/libdl.so.2 (0x00007fa78482d000)\n    librt.so.1 => /usr/lib/librt.so.1 (0x00007fa784625000)\n    libpthread.so.0 => /usr/lib/libpthread.so.0 (0x00007fa784407000)\n    libgcc_s.so.1 => /usr/lib/libgcc_s.so.1 (0x00007fa7841f0000)\n    libc.so.6 => /usr/lib/libc.so.6 (0x00007fa783e39000)\n    /lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x00007fa784ca2000)\nYou'll need these libraries in your Docker image. You will also need the interpreter; to get its path you can use objdump utility:\n$ LANG=en objdump -s -j .interp target/release/t\n\ntarget/release/t:     file format elf64-x86-64\n\nContents of section .interp:\n 0270 2f6c6962 36342f6c 642d6c69 6e75782d  /lib64/ld-linux-\n 0280 7838362d 36342e73 6f2e3200           x86-64.so.2.  \nCopy the files into the expected directories and everything works okay.\nThere is also a second option which is to use the rust-musl-builder docker image. There are some problems with postgresql and diesel but for most of projects it would be good. It works by producing a statically-linked executable which you may just copy and use. This option is much more preferred than using an interpreter and dynamic libraries if you want to provide a docker image with less size and without having all that useless extra data such as interpreter, unused libraries and so on.",
    "unable to edit /etc/resolv.conf in docker container": "This is by design. /etc/resolv.conf is used by docker engine to handle service discovery. Documentation states the following:\nHow can Docker supply each container with a hostname and DNS configuration, without having to build a custom image with the hostname written inside? Its trick is to overlay three crucial /etc files inside the container with virtual files where it can write fresh information \u2026 This arrangement allows Docker to do clever things like keep resolv.conf up to date across all containers when the host machine receives new configuration over DHCP later. The exact details of how Docker maintains these files inside the container can change from one Docker version to the next, so you should leave the files themselves alone and use the following Docker options instead.\nIf you want to override/reconfigure some dns settings, use --dns parameters during container starting. See more details:\nConfigure DNS in Docker",
    "OCI runtime create failed: container_linux.go:296 - no such file or directory": "OCI runtime create failed: container_linux.go:296\nIn my experience this is an error with the docker daemon itself, not the container you are trying to run. Try deleting all containers, restarting the daemon. I think we also had to clean up the docker networks.",
    "How to install VS Code extensions in a Dockerfile?": "If your goal is not to repeat the installation of the VS code extensions, my suggestion is to mount $HOME/.vscode-server/.\nFor example, in a docker-compose.yml\nservices:\n    your_container:\n        ...\n        volumes:\n            - ./volume/vscode-server:$HOME/.vscode-server\nOr in docker run\ndocker run -it -v ./volume/vscode-server:$HOME/.vscode-server your_image bash\nThen, install the required extensions inside the container. The next time you set up the container, there will be no need to reinstall extensions.",
    "How to execute the Entrypoint of a Docker images at each \"exec\" command?": "if your goal is to run the docker exec with a specific user inside of the container, you can use the --user option.\ndocker exec --user myuser container-name [... your command here]\nIf you want to run gosu every time, you can specify that as the command with docker exec\ndocke exec container-name gosu 1000:1000 [your actual command here]\nin my experience, the best way to encapsulate this into something easily re-usable is with a .sh script (or .cmd file in windows).\ndrop this into a file in your local folder... maybe gs for example.\n#! /bin/sh\ndocker exec container-name gosu 1000:1000 \"$@\"\ngive it execute permissions with chmod +x gs and then run it with ./gs from the local folder",
    "How do you install something that needs restart in a Dockerfile?": "This entirely depends on why they require a reboot. For Linux, rebooting a machine would typically indicate a kernel modification, though it's possible it's for something more simple like a change in user permissions (which would be handled by logging out and back in again). If the install is trying to make an OS level change to the kernel, it should fail if done inside of a container. By default, containers isolate and restrict what the application can do to the running host OS which would impact the host or other running containers.\nIf, the reboot is to force the application service to restart, you should realize that this design doesn't map well to a container since each RUN command runs just that command in an isolated environment. And by running only that command, this also indicates that any OS services that would normally be started on OS bootup (cron, sendmail, or your application) will not be started in the container. Therefore, you'll need to find a way to run the installation command in addition to restarting any dependent services.\nThe last scenario I can think of they want different user permissions to take effect to the logged in user. In that case, the next RUN command will run the requested command with any changed access from prior RUN commands. So there's no need to take any specific action of your own to do a reboot, simply perform the install steps as if there's a complete restart between each step.",
    "Couldn't connect to Docker daemon at http+docker://localhost with docker-compose": "Check your privileges, Following command solved my problem :\nsudo chown $USER /var/run/docker.sock\nIt happens when you try to start docker as non super user and it couldn't get access to it own sockets.",
    "Docker run --mount make all files available in a different folder during RUN": "I think you have misunderstood what the RUN --mount=type=bind... syntax is for. From the documentation:\nThis mount type allows binding directories (read-only) in the context or in an image to the build container.\nIn other words, this does not permit you to access arbitrary host directories in the build stage. It is not an analog to the -v command line option on docker run. It only permits you to:\nMount directories from your build context, or\nMount directories from another stage in a multi-stage build\nSo for example I can do this do mount a directory from one build stage into a subsequent build stage:\n# syntax=docker/dockerfile:experimental\n\nFROM centos AS centos\n\nFROM alpine\nRUN --mount=type=bind,from=centos,source=/,target=/centos ls /centos > /root/centos.txt\nOr if I have a directory named example in my build context, I can do this to mount it during the build process:\n# syntax=docker/dockerfile:experimental\n\nFROM centos AS centos\n\nFROM alpine\nRUN --mount=type=bind,source=example,target=/data cp /data/* /root/\nThe syntax you're using (with no from specified)...\nRUN --mount=type=bind,target=/path/on/host\n...simply mounts the root of your build context on /path/on/host inside the container. Remember that target specifies the mountpoint inside the container. E.g., if my build context looks like this:\n.\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 example\n    \u2514\u2500\u2500 README.md\nAnd example/README.md contains:\nThis is a test.\nAnd the Dockerfile contains a RUN option similar to what you're using:\n# syntax=docker/dockerfile:experimental\n\nFROM centos AS centos\n\nFROM alpine\nRUN --mount=type=bind,target=/data cat /data/example/README.md > /root/README.md\nThen when the image is built, /root/README.md has the contents of example/README.md.",
    "Running apt-get update on docker ubuntu image on Mac causes \"File has unexpected size\"": "I had the same issue and the reasons turned out to be that I had content restriction set on my iPhone and these were automatically synced to my Mac.\nAfter switching the restrictions off everything worked as it should.\nGo to System Preferences > Screen Time > Turn Content & Privacy Restrictions off",
    "How to pnpm and Next.js in multi-stage docker file?": "Another solution is installing pnpm using npm. When you install nodejs it comes with npm as the default package manager. So you can install pnpm using npm using the following command npm install -g pnpm\nIn the docker file it will be written as;\nRUN npm install -g pnpm",
    "CMD in dockerfile vs command in docker-compose.yml": "In the common case, you should have a Dockerfile CMD and not a Compose command:.\ncommand: in the Compose file overrides CMD in the Dockerfile. There are some minor syntactic differences (notably, Compose will never automatically insert a sh -c shell wrapper for you) but they control the same thing in the container metadata.\nHowever, remember that there are other ways to run a container besides Compose. docker run won't read your docker-compose.yml file and so won't see that command: line; it's also not read in tools like Kubernetes. If you build the CMD into the image, it will be honored in all of these places.\nThe place where you do need a command: override is if you need to launch a non-default main process for a container.\nImagine you're building a Python application. You might have a main Django application and a Celery worker, but these have basically the same source code. So for this setup you might make the image's CMD launch the Django server, and override command: to run a Celery worker off the same image.\n# Dockerfile\n# ENTRYPOINT is not required\nCMD [\"./manage.py\", \"runserver\", \"0.0.0.0:8080\"]\n# docker-compose.yml\nversion: '3.8'\nservices:\n  web:\n    build: .\n    ports: ['8080:8080']\n    # no command:\n  worker:\n    build: .\n    command: celery worker",
    "How to set breakpoint in Dockerfile itself?": "You can't set a breakpoint per se, but you can get an interactive shell at an arbitrary point in your build sequence (between steps).\nLet's build your image:\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM ubuntu:20.04\n ---> 1e4467b07108\nStep 2/3 : RUN echo \"hello\"\n ---> Running in 917b34190e35\nhello\nRemoving intermediate container 917b34190e35\n ---> 12ebbdc1e72d\nStep 3/3 : RUN echo \"bye\"\n ---> Running in c2a4a71ae444\nbye\nRemoving intermediate container c2a4a71ae444\n ---> 3c52993b0185\nSuccessfully built 3c52993b0185\nEach of the lines that says ---> 0123456789ab with a hex ID has a valid image ID. So from here you can\ndocker run --rm -it 12ebbdc1e72d sh\nwhich will give you an interactive shell on the partial image resulting from the first RUN command.\nThere's no requirement that the build as a whole succeed. If a RUN step fails, you can use this technique to get an interactive shell on the image immediately before that step and re-run the command by hand. If you have a very long RUN command, you may need to break it into two to be able to get a debugging shell at a specific point within the command sequence.",
    "How to make docker-compose pull new images?": "I think you're looking for docker-compose pull:\n$ docker-compose help pull\nPulls images for services defined in a Compose file, but does not start the containers.\nSo docker-compose pull && docker-compose up should do what you want, without needing to constantly wipe your cache or hard-code container names outside of your compose file",
    "Dockerfile CMD not running at container start": "This:\ndocker run -d -p expoPort:contPort -t -i -v /$MOUNTED_VOLUME_DIR/$PROJECT:/$MOUNTED_VOLUME_DIR $CONTAINER_ID /bin/bash\nSays 'run /bin/bash' after instantiating the container. E.g. skip CMD.\nTry this:\ndocker run -d -p expoPort:contPort -t -i -v /$MOUNTED_VOLUME_DIR/$PROJECT:/$MOUNTED_VOLUME_DIR $CONTAINER_ID ",
    "docker copy file from one container to another?": "In recent versions of docker, named volumes replace data containers as the easy way to share data between containers.\ndocker volume create --name myshare\ndocker run -v myshare:/shared task1\ndocker run -v myshare:/shared -p 8080:8080 task2\n...\nThose commands will set up one local volume, and the -v myshare:/shared argument will make that share available as the folder /shared inside each of each container.\nTo express that in a compose file:\nversion: '2'\nservices:\n  task1:\n    build: ./task1\n  volumes:\n    - 'myshare:/shared'\n\n  task2:\n    build: ./task2\n  ports:\n    - '8080:8080'\n  volumes:\n    - 'myshare:/shared'\n\nvolumes:\n  myshare:\n    driver: local \nTo test this out, I made a small project:\n- docker-compose.yml (above)\n- task1/Dockerfile\n- task1/app.py\n- task2/Dockerfile\nI used node's http-server as task2/Dockerfile:\nFROM node\nRUN npm install -g http-server\nWORKDIR /shared\nCMD http-server\nand task1/Dockerfile used python:alpine, to show two different stacks writing and reading.\nFROM python:alpine\nWORKDIR /app\nCOPY . .\nCMD python app.py\nhere's task1/app.py\nimport time\n\ncount = 0\nwhile True:\n  fname = '/shared/{}.txt'.format(count)\n  with open(fname, 'w') as f:\n    f.write('content {}'.format(count))\n    count = count + 1\n  time.sleep(10)\nTake those four files, and run them via docker compose up in the directory with docker-compose.yml - then visit $DOCKER_HOST:8080 to see a steadily updated list of files.\nAlso, I'm using docker version 1.12.0 and compose version 1.8.0 but this should work for a few versions back.\nAnd be sure to check out the docker docs for details I've probably missed here:\nhttps://docs.docker.com/engine/tutorials/dockervolumes/",
    "Docker ENTRYPOINT to run after Volume Mount": "I'm not sure if this is the solution you want but I've been using this run command which uses cat command to supply my script.sh to the container:\ndocker run -it --name=some_name --rm \\\n  -v \"host/path:/path/inside/container\" \\\n  image_name \\\n  /bin/bash  -c \"$(cat ./script.sh)\"\nIn this case the script runs after the mount is complete. I am sure of this as I've used the files from the mounted volumes in the script.",
    "COPY failed: stat /var/lib/docker/tmp/docker-xxx : no such file or directory": "When you run\ndocker build . --file backend/Dockerfile ...\nThe path argument . becomes the context directory. (Docker actually sends itself a copy of this directory tree, which is where the /var/lib/docker/tmp/... path comes from.) The source arguments of COPY and ADD instructions are relative to the context directory, not relative to the Dockerfile.\nIf your source tree looks like\n.\n+-- backend\n| \\-- Dockerfile\n\\-- target\n  \\-- demo-0.0.1-SNAPSHOT.jar\nthat matches the Dockerfile you show. But if instead you have\n.\n+-- backend\n  +-- Dockerfile\n  \\-- target\n    \\-- demo-0.0.1-SNAPSHOT.jar\nyou'll get the error you see.\nIf you don't need to refer to anything outside of the context directory, you can just change what directory you're passing to docker build\nCOPY target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar\ndocker build backend ...\nOr, if you do have other content you need to copy in, you need to change the COPY paths to be relative to the topmost directory.\nCOPY backend/target/demo-0.0.1-SNAPSHOT.jar /opt/demo-0.0.1/lib/demo-0.0.1-SNAPSHOT.jar\nCOPY common/config/demo.yml /opt/demo-0.0.1/etc/demo.yml\ndocker build . -f backend/Dockerfile ...",
    "How do I get python2.7 and 3.7 both installed in an alpine docker image": "Use something like:\nRUN apk add --no-cache python2\nThis will install the latest version of Python 2 as python2 or python2.7. Python 3.7.3 will still be available using python3, or simply python.",
    "Npm install is failing with docker buildx linux/arm64": "A bit late, but for all others that have the same error. Check if you have installed multi platform support for buildx:\ndocker run --privileged --rm tonistiigi/binfmt --install all\nThe full documentation is on the Docker page here",
    "Heroku (Docker) PORT environment variable in nginx": "I got it working for my app by following this example :\nStep 1: listen to $PORT in default.conf.template\nserver {\n  listen $PORT default_server;\n\n  location / {\n    root   /usr/share/nginx/html;\n    index  index.html;\n  }\n}\nStep 2: add this directive to your Dockerfile\nCOPY default.conf.template /etc/nginx/conf.d/default.conf.template\nStep 3: add this at the end of your Dockerfile\nCMD /bin/bash -c \"envsubst '\\$PORT' < /etc/nginx/conf.d/default.conf.template > /etc/nginx/conf.d/default.conf\" && nginx -g 'daemon off;'",
    "Visual Studio 2017 Docker - change the target for multi stage builds": "To use build target configuration from docker file, you need to add a docker-compose.vs.debug.yml file. In that file, just specify build target for the service and that's it.\nExample:\nversion: '3.4'\n\nservices:\n  my.api:\n    build:\n      target: build",
    "Linking django and mysql containers using docker-compose": "In Django settings.py file make sure you have something like:\nDATABASES = {\n'default': {\n    'ENGINE': 'django.db.backends.mysql',\n    'NAME': 'django1',\n    'USER': 'django',\n    'PASSWORD': 'password', \n    'HOST': 'db',\n    'PORT': 3306,\n    }\n}\nthen in your docker-compose.yml file make sure you have something along the lines of:\ndb:\n  image: mysql\n  environment:\n    MYSQL_ROOT_PASSWORD: docker\n    MYSQL_DATABASE: docker\n    MYSQL_USER: docker\n    MYSQL_PASSWORD: docker\nthen as per the docker/django tutorial you are following run the following again to rebuild everything and things should start working\ndocker-compose run web django-admin.py startproject composeexample .\nIn response to a further question, the mysql root password variable is required by docker when creating new databases.\nEDIT: added run to docker-compose above; see edit comment",
    "Docker ENV in CMD": "To use environment variables, you need to use shell.\nhttps://docs.docker.com/engine/reference/builder/#cmd\nNote: Unlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, CMD [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ \"sh\", \"-c\", \"echo $HOME\" ]. When using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nBased on this, I think you can work fine by the following Dockerfile.\nFROM ubuntu:xenial\n\nARG EXECUTABLE\n\nENV EXECUTABLE ${EXECUTABLE}\n\nCMD [ \"sh\", \"-c\", \"/opt/foo/bin/${EXECUTABLE}\", \"-bar\"]",
    "Microsoft Compiler in Docker": "I assume you can already run Windows containers, ex. docker run -it microsoft/windowsservercore\nHere is my Dockerfile to install Visual C++ Build Tools 2015 in Docker container using Chocolatey:\n# escape=`\n\nFROM microsoft/windowsservercore\n\n# Install chocolatey\nRUN @powershell -NoProfile -ExecutionPolicy unrestricted -Command \"(iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))) >$null 2>&1\"\n\n# Install Visual C++ Build Tools, as per: https://chocolatey.org/packages/visualcpp-build-tools\nRUN choco install visualcpp-build-tools -version 14.0.25420.1 -y\n\n# Add msbuild to PATH\nRUN setx /M PATH \"%PATH%;C:\\Program Files (x86)\\MSBuild\\14.0\\bin\"\n\n# Test msbuild can be accessed without path\nRUN msbuild -version\n\nCMD [ \"cmd.exe\" ]\nUsing Choco is simpler, but you can get the same result with downloading native web installer and running it in quiet mode, for example:\nvisualcppbuildtools_full.exe /Q /L <LogFile> /Full",
    "How to evaluate a dynamic variable in a docker-compose.yml file?": "What you can do is to generate .env just before calling docker-compose:\n#!/usr/bin/env bash\n\ncat << EOF > .env\nNOW=$(date +%s)\nEOF\n\ndocker-compose up",
    "Setting docker env var from build secret": "To set a variable from a secret, you can use the $(cat /filename) syntax in shell. This affects the shell within that single step, so all of your uses of that variable need to be within the same step. You cannot extract a variable from a RUN step into an ENV step. If you need it to persist to other RUN steps, you would need to write the variable to the filesystem and have in included in the image, which is undesirable (instead just mount the secret a second time in the later RUN step).\nHere's a working example, you could also export that secret with export secret_var:\n$ cat df.secret\nFROM busybox\nRUN --mount=type=secret,id=secret \\\n    secret_var=\"$(cat /run/secrets/secret)\" \\\n && echo ${secret_var}\n\n$ cat secret.txt\nmy_secret\n\n$ docker build --progress=plain --secret id=secret,src=$(pwd)/secret.txt -f df.secret .\n#1 [internal] load build definition from df.secret\n#1 sha256:85a18e77d3e60159b744d6ee3d96908a6fed0bd4f6a46d038e2aa0201a1028de\n#1 DONE 0.0s\n\n#1 [internal] load build definition from df.secret\n#1 sha256:85a18e77d3e60159b744d6ee3d96908a6fed0bd4f6a46d038e2aa0201a1028de\n#1 transferring dockerfile: 152B done\n#1 DONE 0.0s\n\n#2 [internal] load .dockerignore\n#2 sha256:a5a676bca3eaa2c757a3ae40d8d5d5e91b980822056c5b3b6c5b3169fc65f0f1\n#2 transferring context: 49B done\n#2 DONE 0.0s\n\n#3 [internal] load metadata for docker.io/library/busybox:latest\n#3 sha256:da853382a7535e068feae4d80bdd0ad2567df3d5cd484fd68f919294d091b053\n#3 DONE 0.0s\n\n#5 [1/2] FROM docker.io/library/busybox\n#5 sha256:08a03f3ffe5fba421a6403c31e153425ced631d108868f30e04985f99d69326e\n#5 DONE 0.0s\n\n#4 [2/2] RUN --mount=type=secret,id=secret     secret=$(cat /run/secrets/secret)  && echo ${secret}\n#4 sha256:6ef91a8a7daf012253f58dba292a0bd86af1d1a33a90838b6a99aba5abd4cfaf\n#4 0.587 my_secret\n#4 DONE 0.7s\n\n#6 exporting to image\n#6 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n#6 exporting layers 0.0s done\n#6 writing image sha256:a52db3458ad88481406cd60627e2ed6f55b6720c1614f65fa8f453247a9aa4de done\n#6 DONE 0.0s\nNote the line #4 0.587 my_secret showing the secret was output.",
    "Read JSON-file into environment variable with Docker Compose": "An easy way to do this is to load the JSON file in to a local env var, then use that in your yaml file.\nIn docker-compose.yml\nenvironment:\n  METEOR_SETTINGS: ${METEOR_SETTINGS}\nLoad the settings file before invoking docker-compose:\n\u276f METEOR_SETTINGS=$(cat settings.json) docker-compose up",
    "dotnet restore fails from Docker container": "The actual error seems to be:\nUnable to load the service index for source https://api.nuget.org/v3/index.json\nWhich means that nuget is failing to access the endpoint when it is trying to download the dependencies.\nThere are a number of solutions to this listed here\nhttps://github.com/NuGet/Home/issues/2880\nand also\nNuget connection attempt failed \"Unable to load the service index for source\"",
    "Difference between using \"expose\" in dockerfile and docker-compose file?": "EXPOSE in Dockerfile is a just a metadata information. Which tells docker when someone uses docker run -P which ports need to be Exposed.\nUsing them in compose or docker run is a dynamic way of specifying these ports. So an image like nginx or apache which is always supposed to run on port 80 inside the container will use EXPOSE in Dockerfile itself.\nWhile an image which has dynamic port which may be controlled using an environment variable will then use expose in docker run or compose file\ndocker run -e UI_PORT=5556 --expose 5556 -P ....",
    "Writing data to file in Dockerfile": "When you RUN chmod 755 script.sh && ./script.sh it actually execute this script inside the docker container (ie: in the docker layer).\nWhen you ADD file.txt . you are trying to add a file from your local filesystem inside the docker container (ie: in a new docker layer).\nYou can't do that because the file.txt doesn't exist on your computer.\nIn fact, you already have this file inside docker, try docker run --rm -ti mydockerimage cat file.txt and you should see it's content displayed",
    "Dockerfile: Is there any way to read variables from .env file": "UPDATED\nAfter discussion in a chat was realised that there's no problem with the nodejs app container, and the issue comes from a wrongly configured nginx proxy.\nProof for the working nodejs app is the next docker-compose file.\nversion: \"3\"\nservices:\n    api:\n        build: .\n    curl:\n        image: curlimages/curl:7.70.0\n        depends_on:\n          - api\n        entrypoint: \"\"\n        command: curl -si --retry 5 --retry-delay 5 --retry-max-time 40 http://api:6000\n        restart: on-failure\nORIGINAL\nIf you want to change the port while a build process (it will be static later when you run a container) then use build-args\ndocker build --build-arg APP_PORT=3000\nFROM node:11-alpine\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\nARG APP_PORT=80\nEXPOSE ${APP_PORT}\n\nCOPY . .\nRUN APP_PORT=${APP_PORT} npm install\n\nCMD APP_PORT=${APP_PORT} npm run start\nif you want to be able to change the port when you're starting a container - then build-args don't fit and you need to stay with env variables. Notice that after build EXPOSE can't be changed.\nAnyway if you have different ports in EXPOSE and your app listens to - it doesn't break anything, the app's port will be available on the port you want, despite it wasn't specified in EXPOSE.\nYou can even skip EXPOSE in your file, because it's rather more a metadata information of your image than an instruction for a system to open the port: https://docs.docker.com/engine/reference/builder/#expose\nRegardless of the EXPOSE settings, you can override them at runtime by using the -p flag.\nif your image is static after the build (you don't plan to change .env) you can do next, then npm install and npm run start has the same env. And you're still allowed to change port later, but it won't affect npm install.\nFROM node:11-alpine\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\nCOPY . .\nRUN export $(cat .env) && npm install \n\nCMD export $(cat .env) && npm run start\nif you have to keep CMD as an array - then we need to create a bootstrap script\nFROM node:11-alpine\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\nCOPY . .\nRUN export $(cat .env) && npm install\n\nRUN echo '#!/usr/bin/env sh' > start.sh && echo 'export $(cat .env) && npm run start ${@}' >> start.sh\nCMD [\"sh\", \"./start.sh\"]",
    "error: command 'gcc' failed with exit status 1 when installing pip packages on alpine docker image": "Missing the header file Python.h , this file is provide by python2-dev ( -dev mean package for doing development with ) .\nWith this https://pkgs.alpinelinux.org/contents you can search all packages that have Python.h\nI was able to run pip install pygpgme by adding these 3 packages :\npython2-dev\ngpgme-dev\nlibc-dev\nAnd the Dockerfile will be :\nFROM alpine:latest\n\nRUN apk update && apk upgrade\nRUN apk add --no-cache bash\\\n                       python \\\n                       pkgconfig \\\n                       git \\\n                       gcc \\\n                       openldap \\\n                       libcurl \\\n                       python2-dev \\\n                       gpgme-dev \\\n                       libc-dev \\\n    && rm -rf /var/cache/apk/*\nRUN wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py\nRUN pip install setuptools==30.1.0",
    "What are the appropriate names for the parts of a docker image's name?": "According to the reference for docker tag:\nAn image name is made up of slash-separated name components, optionally prefixed by a registry hostname.\nThe tag is generally regarded as the part after the :. As such, though ignoring the case of more than 2 slash-separated components:\nmy-registry is the registry\nmy-registry/my-image is the (image) name\n0.1.0 is the tag (name)\nThere don't seem to be names for the units my-registry/my-image:0.1.0, my-image and my-image:0.1.0 (other than my-image being called a \"name component\"). You could conceivably call my-registry/my-image:0.1.0 the tagged name, and if you took the approach of calling my-image the project (which is my personal approach) then you could call my-image:0.1.0 the tagged project.\nNote that, despite the above, docker image refers to the my-registry/my-name unit as a \"repository\". I personally prefer to use the \"name\" terminology for this unit.",
    "How do I get memory usage of processes running in a Docker container?": "If each docker has mounted /proc/ as usual (see proc(5)...) you could use it (e.g. running pmap(1), etc...)",
    "Can I set docker container labels based on output of previous run command during build?": "Seems like it is just not possible. You need to use build arguments before launching the build.",
    "Docker build fails in Travis CI - \"Error checking context: 'syntax error in pattern'\"": "For those curious, this turned out to be the system's way of telling me that it was erroring on trying to do regex matching on the items in my .dockerignore file (i.e. that file had syntax errors in it -- in this case, I had backslashes instead of forward slashes on my file paths). Nice and cryptic; I had to dig through the Docker source code to figure out what was happening.\nHopefully this helps someone else encountering the same issue! :)",
    "GPG invalid signature error while running apt update inside arm32v7/ubuntu:20.04 docker": "I had this issue on Docker Desktop for Mac recently, running apt-get update in an Ubuntu 20.04 x86_64 container. It turned out the VM hosting the Docker images on macOS had run out of disk space. That somehow manifested itself as apt reporting invalid signatures on package index files. Pruning unused images to free space solved the issue for me:\ndocker image prune -a\nAs mentioned in comment by sema, if the issue is caused by insufficient disk space, another workaround is to increase the size of the virtual disk used by the virtual machine that is running docker. In Docker Desktop for Mac this can be done via Preferences > Resources > Disk image size.",
    "How to run Powershell script in DockerFile?": "To run a PS1 script file, you can do something like this:\nSHELL [\"cmd\", \"/S\", \"/C\"]    \nRUN powershell -noexit \"& \"\"C:\\Chocolatey\\lib\\chocolatey.0.10.8\\tools\\chocolateyInstall.ps1\"\"\"\nYou can also do:\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\nWORKDIR C:\\\nRUN .\\install_pfx.ps1",
    "I keep getting the \"docker build\" requires exactly 1 argument(s) error": "I was facing the same issue. I missed the little period ( . ) at the end of the command. I copy pasted the command from official docker docs. The period ( . ) at the end of the command tells Docker to use the shell\u2019s current working directory as the build context. Be sure include the period (.) at the end of the command, and be sure to run the command from the current working directory that contains the Dockerfile and application code.",
    "GraphQL ERESOLVE unable to resolve dependency tree when building my docker container": "The problem here is certainly with NPM and the packages you are trying to install rather than anything to do with Docker.\nUnfortunately, I am not able to reproduce the exact error that you are facing. That could be because:\nsomething changed in the time between now and whenever this problem occurred;\nthere are some essential details that you are not showing us.\nEither way, there's a general way in which such issues are solved, which should help. But first an explanation.\nDependencies, peer dependencies and conflicts\nNPM's package (dependency) management mechanism allows packages (dependencies) to have:\n(direct) dependencies - installed automatically with the package;\npeer dependencies - have to be manually installed by the consumer of the package.\nHowever, NPM does not allow multiple versions of the same package to coexist.\nAlso, as you may know, packages use standard semantic versioning, which means that a major version change indicates a breaking change.\nDue to these two reasons, clashes occur if one package requires dependency A to be v1, while another wants the same dependency A to be v2.\nNPM v7\nNPM v7 was recently released and this is the version that current (as of November 2020) node:current images use.\nProbably the biggest changes brought about by NPM7 relate to peer dependencies - NPM should now be able to install them automatically, if possible. Read more here.\nAs described in the document, in cases where it's not possible to solve the conflicts, NPM should now throw errors rather than warnings, which is what you are seeing.\nI, on the other hand, only managed to get warnings and no errors using your setup and NPM v7.0.8, and I don't know why. The problems reported were essentially the same, however, so the resolution ought to be very similar.\nHow to solve conflicts\nThe only solution that I'm aware of is manual conflict resolution - the developer needs to adjust their dependencies to play along.\nIn your specific case the problem seems to be with the graphql package. The latest graphql package is v15, which is also a peer dependency of the latest type-graphql package (v1).\nHowever, apollo-server-express has a few dependencies, which apparently only support graphql up to and including v14.\nWhile you wait for apollo-server-express to fully support v15, you may opt for graphql v14 altogether by downgrading the only package that requires v15. So if you change your npm install to this:\nnpm install --save cors apollo-server-express express graphql@14 reflect-metadata type-graphql@0 apollo-datasource-rest soap jsonwebtoken\nit ought to work... Notice that we are explicitly installing graphql@14 and type-graphql@0 (yes, version zero).\nAlternative solution\nGoing to give you some bad advice too. In some cases a missing peer dependency may not be a problem, particularly if you never use the related functionality. In your case, it may be even less of a problem because you do have the dependency, just not the required version. It's entirely possible that a wrong version would do just fine. If you feel lucky (or if you're sure of you're doing) and you really wish to proceed with graphql v15, you could either:\nsuppress any NPM output to silence the errors;\ndowngrade to NPM v6, which works quite differently (although it will still warn you of peer dependency problems).\nProceed with caution!",
    "Running a bash script before startup in an NGINX docker container": "NGINX 1.19 has a folder /docker-entrypoint.d on the root where place startup scripts executed by thedocker-entrypoint.sh script. You can also read the execution on the log.\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching\n[..........]\n/docker-entrypoint.sh: Configuration complete; ready for start up",
    "qemu: uncaught target signal 11 (Segmentation fault) - core dumped in docker containers": "I had the same issue using M1 chip with MacOS Monterey 12.5.\nAfter upgrading to MacOS Ventura 13.3 and selecting\nUse Rosetta for x86/amd64 emulation on Apple Silicon\nin Docker Desktop -> Settings -> Features in development, the error disappeared and everything worked fine.\nNote that this option is not available on MacOS versions lower than Ventura 13, so upgrade is required. Source: https://github.com/docker/for-mac/issues/6788\nUpdate December 2023\nYou can find the option in the General tab, as it is no longer in development for the latest versions of Docker Desktop.",
    "`docker build` show output from `RUN` [duplicate]": "Is that what are you looking for?\n$ docker build --progress=plain .\nSending build context to Docker daemon  4.096kB\nStep 1/3 : FROM alpine:3.14\n3.14: Pulling from library/alpine\n5843afab3874: Pull complete \nDigest: sha256:234cb88d3020898631af0ccbbcca9a66ae7306ecd30c9720690858c1b007d2a0\nStatus: Downloaded newer image for alpine:3.14\n ---> d4ff818577bc\nStep 2/3 : COPY . .\n ---> 106aa79185ae\nStep 3/3 : RUN echo \"here are some numbers: $(seq 10)\"\n ---> Running in 30a81b6d5035\nhere are some numbers: 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nRemoving intermediate container 30a81b6d5035\n ---> 3c059c9b6150\nSuccessfully built 3c059c9b6150\n$ docker --version\nDocker version 19.03.8, build afacb8b",
    "docker run [9] System error: exec format error": "Had the same issue, fixed it by adding #!/bin/sh at the top of the file instead of having other comments.",
    "How to use docker container as apache server?": "You will have to use port forwarding to be able to access your docker container from the outside world.\nFrom the Docker docs:\nBy default Docker containers can make connections to the outside world, but the outside world cannot connect to containers.\n\nBut if you want containers to accept incoming connections, you will need to provide special options when invoking docker run.\nSo, what does this mean? You will have to specify a port on your host machine (typically port 80) and forward all connections on that port to the docker container. Since you are running Apache in your docker container you probably want to forward the connection to port 80 on the docker container as well.\nThis is best done via the -p option for the docker run command.\nsudo docker run -p 80:80 -t -i <yourname>/supervisord\nThe part of the command that says -p 80:80 means that you forward port 80 from the host to port 80 on the container.\nWhen this is set up correctly you can use a browser to surf onto http://88.x.x.x and the connection will be forwarded to the container as intended.\nThe Docker docs describes the -p option thoroughly. There are a few ways of specifying the flag:\n# Maps the provided host_port to the container_port but only \n# binds to the specific external interface\n-p IP:host_port:container_port\n\n# Maps the provided host_port to the container_port for all \n# external interfaces (all IP:s)\n-p host_port:container_port\nEdit: When this question was originally posted there was no official docker container for the Apache web server. Now, an existing version exists.\nThe simplest way to get Apache up and running is to use the official Docker container. You can start it by using the following command:\n$ docker run -p 80:80 -dit --name my-app -v \"$PWD\":/usr/local/apache2/htdocs/ httpd:2.4\nThis way you simply mount a folder on your file system so that it is available in the docker container and your host port is forwarded to the container port as described above.",
    "Docker: RUN touch doesn't create file": "You are doing this during your build:\nRUN touch /var/log/node.log && /\n    node --help 2>&1 > /var/log/node.log\nThe file /var/log/node.log is created and fixed immutably into the resulting image.\nThen you run the container with this volume mount:\nvolumes:\n  - ./mongo/log/:/var/log/\nWhatever is in ./mongo/log/ is mounted as /var/log in the container, which hides whatever was there before (from the image). This is the thing that's making it look like your touch didn't work (even though it probably worked fine).\nYou're thinking about this backward - your volume mount doesn't expose the container's version of /var/log externally - it replaces whatever was there.\nNothing you do in Dockerfile (build) will ever show up in an external mount.",
    "docker deploy won't publish port in swarm": "As far as I understood for the moment you just can publish ports updating the service later the creation, like this:\ndocker service update my-service --publish-add 80:80",
    "Docker container save logs on the host directory": "All you need is a docker volume in order to persist the log files. So in the same directory as your docker-compose.yml create a logs directory, then define a volume mount. When defining a mount remember the syntax is <host_machine_directy>:<container_directory>.\nGive the following volume a try and let me know what you get back.\nversion: '3'\nservices:\n  myapp:\n    build: .\n    image: myapp\n    ports:\n      - \"9001:9001\"\n    volumes:\n      - ./logs:/home/logs\nAlso worth noting that persistence goes both ways with this approach. Any changes made to the files from within the container are reflected back onto the host. Any changes from the host, are also reflected inside the container.",
    "What is a clean way to add a user in Docker with sudo priviledges?": "Generally you should think of a Docker container as a wrapper around a single process. If you ask this question about other processes, it doesn't really make sense. (How do I add a user to my PostgreSQL server with sudo privileges? How do I add a user to my Web browser?)\nIn Docker you almost never need sudo, for three reasons: it's trivial to switch users in most contexts; you don't typically get interactive shells in containers (how do I get a directory listing from inside the cron daemon?); and if you can run any docker command at all you can very easily root the whole host. sudo is also hard to script, and it's very hard to usefully maintain a user password in Docker (writing a root-equivalent password in a plain-text file that can be easily retrieved isn't a security best practice).\nIn the context of your question, if you've already switched to some non-root user, and you need to run some administrative command, use USER to switch back to root.\nUSER janedoe\n...\nUSER root\nRUN apt-get update && apt-get install -y some-package\nUSER janedoe\nSince your containers have some isolation from the host system, you don't generally need containers to have the same user names or user IDs as the host system. The exception is when sharing files with the host using bind mounts, but there it's better to specify this detail when you start the container.\nThe typical practice I'm used to works like this:\nIn your Dockerfile, create some non-root user. It can have any name. It does not need a password, login shell, home directory, or any other details. Treating it as a \"system\" user is fine.\n FROM ubuntu:18.04\n RUN adduser --system --group --no-create-home appuser\nStill in your Dockerfile, do almost everything as root. This includes installing your application.\n RUN apt-get update && apt-get install ...\n WORKDIR /app\n COPY requirements.txt .\n RUN pip install -r requirements.txt\n COPY . .\nWhen you describe the default way to run the container, only then switch to the non-root user.\n EXPOSE 8000\n USER appuser\n CMD [\"./main.py\"]\nIdeally that's the end of the story: your code is built into your image and it stores all of its data somewhere external like a database, so it doesn't care about the host user space at all (there by default shouldn't be docker run -v or Docker Compose volumes: options).\nIf file permissions really matter, you can specify the numeric host user ID to use when you launch the container. The user doesn't specifically need to exist in the container's /etc/passwd file.\n docker run \\\n   --name myapp \\\n   -d \\\n   -p 8000:8000 \\\n   -v $PWD:/data \\\n   -u $(id -u) \\\n   myimage",
    "Docker for MAC | Cannot run program \"docker-credential-desktop\"": "If you are trying to use with Spring Boot - fabric8 plugin and on mac machine do the following :\nIn ~/.docker/config.json change credsStore to credStore",
    "Can docker compose build image from different Dockerfiles at the same folder": "The following worked for me\nabc:\n  build:\n    context: .\n    dockerfile: Dockerfile.one\ndef:\n  build:\n    context: .\n    dockerfile: Dockerfile.two\nOf course you can tweak the context as needed if you have different contexts. I also use this to have separate Dockerfile.production versions to set things up differently for production versions of the app.",
    "installing ssh in the docker containers": "Well, as part of the image file you'll simply have to install openssh-server:\nsudo apt-get install openssh-server\nThe problem then is that traditionally, a running docker container will only run a single command. You can get around this problem by using something like supervisord. There's an example in the docker docs: https://docs.docker.com/engine/admin/using_supervisord/\nYour dockerfile might look like this:\nFROM ubuntu:16.04\nMAINTAINER examples@docker.com\n\nRUN apt-get update && apt-get install -y openssh-server apache2 supervisor\nRUN mkdir -p /var/lock/apache2 /var/run/apache2 /var/run/sshd /var/log/supervisor\n\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\nEXPOSE 22 80\nCMD [\"/usr/bin/supervisord\"]\nYour supervisord.conf might look something like this:\n[supervisord]\nnodaemon=true\n\n[program:sshd]\ncommand=/usr/sbin/sshd -D\n\n[program:apache2]\ncommand=/bin/bash -c \"source /etc/apache2/envvars && exec /usr/sbin/apache2 -DFOREGROUND\"",
    "Update a docker image in registry": "Short: Upgrade to the latest docker version (preferred) or use the -t tag option.\nYour commands are the same as if you would issue the following two commands:\ndocker tag proj1:latest localhost.com:5000/proj/proj1:latest \ndocker push localhost.com:5000/proj/proj1:latest\nOlder versions of Docker are complaining, since you try to overwrite an existing image with existing tag :latest.\nThe quick&dirty solution is to try with\ndocker tag -f proj1 localhost.com:5000/proj/proj1 \ndocker push -f localhost.com:5000/proj/proj1\nThis will allow to overwrite the existing image localhost.com:5000/proj/proj1:latest on older versions of Docker.\nHowever, I recommend to upgrade docker to version >=1.12.0. There, the -t option is not available and not necessary anymore, since the image will always be replaced. This is the reason, why -f option is not described on the official documentation, but it is mentioned on the Docker Deprecated Engine Features page instead.",
    "Docker Alpine: unable to select packages: python (no such package) while building image for ARM": "RUN apk add --no-cache --virtual .gyp python3 make g++",
    "How can I fix the Error of Docker-compose up exited with code 1": "docker may fail due to many things in the build process. To find the solution here is my advice\nType docker ps -la (to list all containers that exited with error code or failed to start\nIn the result, you should look out for the id of the container\nThen check the logs using: docker logs <container_id>",
    "Docker build ARG always empty string": "This is obviously not the problem in your example, but I got this error when declaring an ARG before the FROM. Moving the ARG I needed below FROM resolved the problem.",
    "Define environment variable in Dockerfile or docker-compose?": "See this:\nYou can set environment variables in a service\u2019s containers with the 'environment' key, just like with docker run -e VARIABLE=VALUE ...\nAlso, you can use ENV in dockerfile to define a environment variable.\nThe difference is:\nEnvironment variable define in Dockerfile will not only used in docker build, it will also persist into container. This means if you did not set -e when docker run, it will still have environment variable same as defined in Dockerfile.\nWhile environment variable define in docker-compose.yaml just used for docker run.\nMaybe next example could make you understand more clear:\nDockerfile:\nFROM alpine\nENV http_proxy http://123\ndocker-compose.yaml:\napp:\n  environment:\n    - http_proxy=http://123\nIf you define environment variable in Dockerfile, all containers used this image will also has the http_proxy as http://123. But the real situation maybe when you build the image, you need this proxy. But, the container maybe run by other people maybe not need this proxy or just have another http_proxy, so they had to remove the http_proxy in entrypoint or just change to another value in docker-compose.yaml.\nIf you define environment variable in docker-compose.yaml, then user could just choose his own http_proxy when do docker-compose up, http_proxy will not be set if user did not configure it docker-compose.yaml.",
    "How do I override the entrypoint and pass Bash commands as a string?": "When Docker launches a container, it combines the \"entrypoint\" and \"command\" parts together into a single command. The docker run --entrypoint option only takes a single \"word\" for the entrypoint command.\nSo, say you need to run some command --with an-arg. You need to\nBreak this into words\nPass the first word as docker run --entrypoint, before the image name\nPass the remaining words as the command part, after the image name.\n# some command --with an-arg\ndocker run \\\n  --entrypoint some\n  image-name \\\n  command --with an-arg\n\n# ls -al /\ndocker run --rm \\\n  --entrypoint /bin/ls\n  image-name \\\n  -al /\n\n# bash -c \"echo aaaa\"\ndocker run --rm \\\n  --entrypoint /bin/bash \\\n  image-name \\\n  -c 'echo aaaa'\nThis construct is kind of awkward. If you control the image, I tend to recommend making the image's CMD be a complete command, and either omitting ENTRYPOINT or making it be a wrapper that takes a complete command as arguments (for example, a shell script that ends in exec \"$@\").\n# Hard to replace with an alternate command:\n# ENTRYPOINT python3 ./manage.py runserver\n\n# Better:\nCMD python3 ./manage.py runserver\n# Takes a complete command as arguments\n# (MUST use JSON-array form)\nENTRYPOINT [\"bundle\", \"exec\"]\n\n# Can be overridden at runtime\n# (But whatever you supply will run in a Ruby Bundler context)\nCMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]",
    "Does Docker EXPOSE make a new layer?": "Yes, every instruction in a Dockerfile generates a new layer for the resulting image.\nHowever, layers created via EXPOSE are empty layers. That is, their size is 0 bytes.\nWhile they don't impact you storage-wise, they do count for leveraging layer cache while building or pulling/pushing images from a registry.\nA good way to understand an image's layers is to use the docker history command. For instance, given the following Dockerfile:\nFROM scratch\n\nEXPOSE 4000\nEXPOSE 3000\ndo\ndocker build -t test/image .\nIf you then docker history test/image you'll see:\nIMAGE               CREATED             CREATED BY                           SIZE                COMMENT\nab9f435de7bc        4 seconds ago       /bin/sh -c #(nop)  EXPOSE 4000/tcp   0 B                 \n15e09691c313        5 seconds ago       /bin/sh -c #(nop)  EXPOSE 3000/tcp   0 B     \nIf you switch the order of the EXPOSE statements and build again, you'll see the layer cache being ignored.",
    "ldconfig seems no functional under alpine 3.3": "Alpine's version of ldconfig requires you to specify the target folder or library as an argument. Note that alpine has no /etc/ld.so.conf file, nor does it recognize one if you create it.\nExample with no target path:\n$ docker run -ti alpine sh -c \"ldconfig; echo \\$?\"\n1\nExample with target path:\n$ docker run -ti alpine sh -c \"ldconfig /; echo \\$?\"\n0\nHowever, even with that there are frequently linking errors. Others suggest:\nManual symbolic links\nInstalling glibc into your container.",
    "Pass ARG to ENTRYPOINT": "You could combine ARG and ENV in your Dockerfile, as I mention in \"ARG or ENV, which one to use in this case?\"\nARG FOO\nENV FOO=${FOO}\nThat way, you docker.r2g can access the ${FOO} environment variable.\nI guess the argument could also be passed with docker run instead of during the docker build phase?\nThat is also possible, if it makes more sense to give FOO a value at runtime:\ndocker run -e FOO=$(...) ...",
    "Is it possible to set a MAC address for `docker build`?": "Let's consider the below Dockerfile\nFROM alpine\nRUN ifconfig | grep -i hwaddr\nIf you build it using\ndocker build .\nYou get\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM alpine\n ---> 7328f6f8b418\nStep 2/2 : RUN ifconfig | grep -i hwaddr\n ---> Running in c092838dbe31\neth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02\nRemoving intermediate container c092838dbe31\n ---> 7038787f51b8\nNow we can't control Mac address of docker build, but we can control the network of build and we can control mac address of a container. So let us launch a container with our mac address\n$ docker run --name mac1234deb06b61 --mac-address=\"12:34:de:b0:6b:61\" -d alpine tail -f /dev/null\nc3579e4685933b757f51c5f9e36d620dbe3a62abd0e0d6a421b5f1c04045061c\n\n$ docker build --network container:mac1234deb06b61 --no-cache .\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM alpine\n ---> 7328f6f8b418\nStep 2/2 : RUN ifconfig | grep -i hwaddr\n ---> Running in 4390f13cbe8f\neth0      Link encap:Ethernet  HWaddr 12:34:DE:B0:6B:61\nRemoving intermediate container 4390f13cbe8f\n ---> b0b5f7321921\nSuccessfully built b0b5f7321921\nAs you can see, now the docker build takes a updated mac address",
    "Docker: How can I have sqlite db changes persist to the db file?": "You are not mounting volumes in a Dockerfile. VOLUME tells docker that content on those directories can be mounted via docker run --volumes-from\nYou're right. Docker doesn't allow relative paths on volumes on command line.\nRun your docker using absolute path:\ndocker run -v /host/db/local-db:/go/src/beginnerapp/local-db\nYour db will be persisted in the host file /host/db/local-db\nIf you want to use relative paths, you can make it work with docker-compose with \"volumes\" tag:\nvolumes:\n  - ./local-db:/go/src/beginnerapp/local-db\nYou can try this configuration:\nPut the Dockerfile in a directory, (e.g. /opt/docker/myproject)\ncreate a docker-compose.yml file in the same path like this:\nversion: \"2.0\"\nservices:\n  myproject:\n    build: .\n    volumes:\n      - \"./local-db:/go/src/beginnerapp/local-db\"\nExecute docker-compose up -d myproject in the same path.\nYour db should be stored in /opt/docker/myproject/local-db\nJust a comment. The content of local-db (if any) will be replaced by the content of ./local-db path (empty). If the container have any information (initialized database) will be a good idea to copy it with docker cp or include any init logic on an entrypoint or command shell script.",
    "Docker compose .env file array variable": "As Confidence mentioned above, write a comma separated string:\nTAGS=12345,67890\nThen in your application (Python for instance):\nos.getenv('TAGS').split(',')",
    "How to translate docker-compose.yml to Dockerfile": "TL;DR\nYou can pass some informations to your Dockefile (the command to run) but that wouldn't be equivalent and you can't do that with all the docker-compose.yml file content.\nYou can replace your docker-compose.yml file with commands lines though (as docker-compose is precisely to replace it).\nIn your case you can add the command to run to your Dockerfile as a default command (which isn't roughly the same as passing it to containers you start at runtime) :\nCMD [\"python\", \"jk/manage.py\", \"runserver\", \"0.0.0.0:8081\"]\nor pass this command directly in command line like the volume and port which should give something like :\ndocker run -d -v .:/code -p 8081:8080 yourimage python jk/manage.py runserver 0.0.0.0:8081\nBUT\nKeep in mind that Dockerfiles and docker-compose serve two whole different purposes.\nDockerfile are meant for image building, to define the steps to build your images.\ndocker-compose is a tool to start and orchestrate containers to build your applications (you can add some informations like the build context path or the name for the images you'd need, but not the Dockerfile content itself).\nSo asking to \"convert a docker-compose.yml file into a Dockerfile\" isn't really relevant.\nThat's more about converting a docker-compose.yml file into one (or several) command line(s) to start containers by hand.\nThe purpose of docker-compose is precisely to get rid of these command lines to make things simpler (it automates it).\nalso :\nFrom the manage.py documentation:\nDO NOT USE THIS SERVER IN A PRODUCTION SETTING. It has not gone through security audits or performance tests. (And that\u2019s how it\u2019s gonna stay.\nDjango's runserver included in the manage.py tool isn't meant for production.\nYou might want to consider using a WSGI server behind a proxy.",
    "Bind container port to host inside Dockerfile": "In dockerfile you can only use expose. The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. EXPOSE does not make the ports of the container accessible to the host.\nTo allocate Hostport to container you need to do publish (-p). Or the -P flag to publish all of the exposed ports.\nTo automate the process, You can use docker-compose. In docker compose file you can orchestrate multiple docker run commands with different arguments.",
    "docker COPY with file globbing": "For any non-standard build operation, I prefer wrapping the docker build command in a script (named 'build').\nHere I would\ncreate a subfolder tmp (just beside the Dockerfile, in order to keep it in the docker build context)\nmake the shell cp with globing: cp ./src/**/project.json tmp\ncall docker build, with a Dockerfile including COPY tmp/ /app/\ndeleting tmp.\nThat way, I pre-configure what I need from host, before building the image from the host context.",
    "Custom Docker image doesn't inherit CMD": "This behavior is trying to be more intuitive, but I agree that it's a bit confusing. You see the original issue here. That problem was that most people defining the ENTRYPOINT in a child image no longer wanted the CMD from the parent image.\nWith the current behavior, if you define an ENTRYPOINT in a child image, the CMD from the parent image will be null'd out, so you'll need to redefine it if you need to have it set.",
    "An assembly specified in the application dependencies manifest was not found:": "You need to specify -r linux-x64 parameter in dotnet publish command like that:\ndotnet publish -o obj/Docker/publish -c Release -r linux-x64\nThis will make a standalone deployment.",
    "How to: Docker reuse layers with different base images": "Problem with --cache-from:\nThe suggestion to use --cache-from will not work:\n$ cat df.cache-from\nFROM busybox\nARG UNIQUE_ARG=world\nRUN echo Hello ${UNIQUE_ARG}\nCOPY . /files\n\n$ docker build -t test-from-cache:1 -f df.cache-from --build-arg UNIQUE_ARG=docker .\nSending build context to Docker daemon   26.1MB\nStep 1/4 : FROM busybox\n ---> 54511612f1c4\nStep 2/4 : ARG UNIQUE_ARG=world\n ---> Running in f38f6e76bbca\nRemoving intermediate container f38f6e76bbca\n ---> fada1443b67b\nStep 3/4 : RUN echo Hello ${UNIQUE_ARG}\n ---> Running in ee960473d88c\nHello docker\nRemoving intermediate container ee960473d88c\n ---> c29d98e09dd8\nStep 4/4 : COPY . /files\n ---> edfa35e97e86\nSuccessfully built edfa35e97e86\nSuccessfully tagged test-from-cache:1\n\n$ docker build -t test-from-cache:2 -f df.cache-from --build-arg UNIQUE_ARG=world --cache-from test-from-cache:1 .                                                                                \nSending build context to Docker daemon   26.1MB\nStep 1/4 : FROM busybox\n ---> 54511612f1c4\nStep 2/4 : ARG UNIQUE_ARG=world\n ---> Using cache\n ---> fada1443b67b\nStep 3/4 : RUN echo Hello ${UNIQUE_ARG}\n ---> Running in 22698cd872d3\nHello world\nRemoving intermediate container 22698cd872d3\n ---> dc5f801fc272\nStep 4/4 : COPY . /files\n ---> addabd73e43e\nSuccessfully built addabd73e43e\nSuccessfully tagged test-from-cache:2\n\n$ docker inspect test-from-cache:1 -f '{{json .RootFS.Layers}}' | jq .\n[\n  \"sha256:6a749002dd6a65988a6696ca4d0c4cbe87145df74e3bf6feae4025ab28f420f2\",\n  \"sha256:01bf0fcfc3f73c8a3cfbe9b7efd6c2bf8c6d21b6115d4a71344fa497c3808978\"\n]\n\n$ docker inspect test-from-cache:2 -f '{\n{json .RootFS.Layers}}' | jq .                                                                                         \n[\n  \"sha256:6a749002dd6a65988a6696ca4d0c4cbe87145df74e3bf6feae4025ab28f420f2\",\n  \"sha256:c70c7fd4529ed9ee1b4a691897c2a2ae34b192963072d3f403ba632c33cba702\"\n]\nThe build shows exactly where it stops using the cache, when the command changes. And the inspect shows the change of the second layer id even though the same COPY command was run in each. And anytime the preceding layer differs, the cache cannot be used from the other image build.\nThe --cache-from option is there to allow you to trust the build steps from an image pulled from a registry. By default, docker only trusts layers that were locally built. But the same rules apply even when you provide this option.\nOption 1:\nIf you want to reuse the build cache, you must have the preceding layers identical in both images. You could try using a multi-stage build if the base image for each is small enough. However, doing this would lose all of the settings outside of the filesystem (environment variables, entrypoint specification, etc), so you'd need to recreate that as well:\nARG base_image\nFROM ${base_image} as base\n# the above from line makes the base image available for later copying\nFROM scratch\nCOPY large-content /content\nCOPY --from=base / /\n# recreate any environment variables, labels, entrypoint, cmd, or other settings here\nAnd then build that with:\ndocker build --build-arg base_image=base1 -t image1 .\ndocker build --build-arg base_image=base2 -t image2 .\ndocker build --build-arg base_image=base3 -t image3 .\nThis could also be multiple Dockerfiles if you need to change other settings. This will result in the entire contents of each base image being copied, so make sure your base image is significantly smaller to make this worth the effort.\nOption 2:\nReorder your build to keep common components at the top. I understand this won't work for you, but it may help others coming across this question later. It's the preferred and simplest solution that most people use.\nOption 3:\nRemove the large content from your image and add it to your containers externally as a volume. You lose the immutability + copy-on-write features of layers of the docker filesystem. And you'll manually need to ship the volume content to each of your docker hosts (or use a network shared filesystem). I've seen solutions where a \"sync container\" is run on each of the docker hosts which performs a git pull or rsync or any other equivalent command to keep the volume updated. If you can, consider mounting the volume with :ro at the end to make it read only inside the container where you use it to give you immutability.",
    "What is #syntax=docker/dockerfile:experimental?": "It's a way to enable new syntax in Dockerfiles when building with BuildKit. It's mentioned in the documentation:\nOverriding default frontends\nThe new syntax features in Dockerfile are available if you override the default frontend. To override the default frontend, set the first line of the Dockerfile as a comment with a specific frontend image:\n# syntax=<frontend image>, e.g. # syntax=docker/dockerfile:1.2\nThe examples on this page use features that are available in docker/dockerfile version 1.2.0 and up. We recommend using docker/dockerfile:1, which always points to the latest release of the version 1 syntax. BuildKit automatically checks for updates of the syntax before building, making sure you are using the most current version. Learn more about the syntax directive in the Dockerfile reference.\nI have used it to enable SSH Auth Sock forwarding.",
    "Read txt file from resources folder on maven Quarkus project From Docker Container": "You need to make sure that the resource is included in the native image (it isn't by default).\nAdd a src/main/resources/resources-config.json that includes something like:\n{\n  \"resources\": [\n    {\n      \"pattern\": \"151279\\\\.txt$\"\n    }\n  ]\n}\nYou will also need to set the following property:\nquarkus.native.additional-build-args =-H:ResourceConfigurationFiles=resources-config.json\nSee this for more details.",
    "convert Dockerfile to Bash script": "In short - no.\nBy parsing the Dockerfile with a tool such as dockerfile-parse you could run the individual RUN commands, but this would not replicate the Dockerfile's output.\nYou would have to be running the same version of the same OS.\nThe ADD and COPY commands affect the filesystem, which is in its own namespace. Running these outside of the container could potentially break your host system. Your host will also have files in places that the container image would not.\nVOLUME mounts will also affect the filesytem.\nThe FROM image (which may in turn be descended from other images) may have other applications installed.\nWriting Dockerfiles can be a slow process if there is a large installation or download step. To mitigate that, try adding new packages as a new RUN command (to take advantage of the cache) and add features incrementally, only optimising/compressing the layers when the functionality is complete.\nYou may also want to use something like ServerSpec to get a TDD approach to your container images and prevent regressions during development.\nBest practice docs here, gotchas and the original article.",
    "How can I run Selenium tests in a docker container with a visible browser?": "Please consider using Zalenium (https://opensource.zalando.com/zalenium/). The headline of Zalenium is - A flexible and scalable container based Selenium Grid with video recording, live preview, basic auth & dashboard.\nAs mentioned above, you can check the live preview of your test cases running on the browser.\nP.S.:- Zalenium is a wrapper built on top of Selenium Grid",
    "`--chown` option of COPY and ADD doesn't allow variables. There exists a workaround?": "You can create a user before running the --chown;\nmkdir -p test && cd test\nmkdir -p path/to/host/dir/\ntouch path/to/host/dir/myfile\nCreate your Dockerfile:\nFROM busybox\n\nARG USER_ID=1000\nARG GROUP_ID=1000\n\nRUN addgroup -g ${GROUP_ID} mygroup \\\n && adduser -D myuser -u ${USER_ID} -g myuser -G mygroup -s /bin/sh -h /\n\nCOPY --chown=myuser:mygroup /path/to/host/dir/ /path/to/container/dir\nBuild the image\ndocker build -t example .\nOr build it with a custom UID/GID:\ndocker build -t example --build-arg USER_ID=1234 --build-arg GROUP_ID=2345 .\nAnd verify that the file was chown'ed\ndocker run --rm example ls -la /path/to/container/dir\n\ntotal 8\ndrwxr-xr-x    2 myuser   mygroup       4096 Dec 22 16:08 .\ndrwxr-xr-x    3 root     root          4096 Dec 22 16:08 ..\n-rw-r--r--    1 myuser   mygroup          0 Dec 22 15:51 myfile\nVerify that it has the correct uid/gid:\ndocker run --rm example ls -lan /path/to/container/dir\n\ntotal 8\ndrwxr-xr-x    2 1234     2345          4096 Dec 22 16:08 .\ndrwxr-xr-x    3 0        0             4096 Dec 22 16:08 ..\n-rw-r--r--    1 1234     2345             0 Dec 22 15:51 myfile\nNote: there is an open feature-request for adding this functionality: issue #35018 \"Allow COPY command's --chown to be dynamically populated via ENV or ARG\"",
    "Restart terminal & run command using Dockerfile": "Every new RUN command creates a new layer in docker image, so you can treat it as a new terminal spawns for each \"RUN\".\nWhat seems to be the problem: when running a command in this way: piping it through bash curl https://raw.githubusercontent.com/creationix/nvm/v0.30.2/install.sh | bash it won't throw any errors if something fails.\nHad a similar issue a few weeks ago. Does your base image have all the dependencies? Depending of the base image, before running the script from github add a RUN task which will download curl wget ca-certificates. You can lookup how other people install nvm on their images: https://github.com/mikeyfarrow/docker-nvm/blob/master/Dockerfile",
    "Docker Load key \"/root/.ssh/id_rsa\": invalid format": "Another possible gotcha is if you're using a Makefile to run the docker build command. In that case the command in the Makefile would look something like:\ndocker-build:\n    docker build --build-arg SSH_PRIVATE_KEY=\"$(shell cat ~/.ssh/id_rsa)\"\nMake unfortunately replaces newlines with spaces (make shell)\nThis means that the ssh key which is written into the container has a different format, yielding the error above.\nI was unable to find a way to retain the newlines in the Makefile command, so I resorted to a workaround of copying the .ssh directory into the docker build context, copying the files through the Dockerfile, then removing them afterwards.",
    "You must use Bundler 2 or greater with this lockfile. When running docker-compose up locally": "Adding this line before RUN bundle install to the Dockerfile did the trick for me.\nRUN gem install bundler -v 2.0.1\nLeaving this here for future reference!",
    "Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'": "I suspect that you haven't copied over your requirements.txt file to your Docker image.\nTypically you add the following lines to your Dockerfile to copy your requirements.txt file and install it using pip:\nCOPY requirements.txt /tmp/requirements.txt\nRUN python3 -m pip install -r /tmp/requirements.txt\nIf you don't explicitly copy over anything to your Docker image your image has no data save for what is on the base image.",
    "docker-compose up error, Invalid address": "I just ran into this issue and my issue seems to be issues with how docker caches networks and containers. I had to docker network rm the network that was previously created. I also had to docker ps -a and docker rm the previously created containers as they're cached to use the network that you'll be removing. After I removed all those leftover artifacts, it started up correctly.",
    "Docker: How to add backports to sources.list via Dockerfile?": "You can do it by adding below\nRUN printf \"deb http://httpredir.debian.org/debian jessie-backports main non-free\\ndeb-src http://httpredir.debian.org/debian jessie-backports main non-free\" > /etc/apt/sources.list.d/backports.list",
    "What is tianon/true used for in Dockerfile?": "Now I understand that every container needs an image.\nIn this case appdata is a container that only pointing some directories and It will be used in another docker container.\nappdata:\n  image: tianon/true # Here is the image, if we remove it, it won't work.\n  volumes:\n    - /var/www/html\n    - ~/.composer:/var/www/.composer\n    - ./html/app/code:/home/gujarat/php/html/app/code\n    - ./html/app/design:/home/gujarat/php/html/app/design\n    - ./html/app/etc:/var/www/html/app/etc\n    - ./html/downloads:/var/www/html/downloads\nSo in my docker-compose.yml above it needs a docker image which is really small.And that is tianon/true. It would be waste of resource if we choose another large docker image.\nAnd I found in the short description in this link :\n125 bytes total - nothing but \"true\" (perfect for volume-only containers) Yes, those are \"regular bytes\" - static assembly for the win.\nso that's the tianon/true is used for. :D",
    "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)": "$ sudo apt install locales\n$ sudo locale-gen en_US.UTF-8\n$ sudo dpkg-reconfigure locales\nIn the last step you, would see a text based UI, select en_US.UTF-8 by moving using up and down arrow and selecting via spacebar or typing its id, which is 159.",
    "Assigning additional capabilities using a Docker file": "You can do that with docker-compose.\nThis works for version 2 and 3. For example:\nversion: '2'\nservices:\n  myapp:\n    cap_add:\n    - SYS_ADMIN\n    - DAC_READ_SEARCH",
    "How to use the mounted ssh in docker for subsequent commands in Dockerfile": "Just for posterity, there are 3 prerequisites of this working, so make sure that build is using buildx, inside the Dockerfile you use the RUN command with --mount=type=ssh and you are passing --ssh default parameter to the build command:\nexport DOCKER_BUILDKIT=1\nFROM ...\nRUN --mount=type=ssh composer install --no-dev --no-interaction\ndocker build --ssh default .",
    "Create a dockerfile that runs a python http server to display an html file": "Also, how would I build and run this file once it is done?\nYou were close. Several pointers:\nIf you use python3 then you have to either use http.server or install SimpleHTTPServer separately\nIf you use python 2.7 then you can't use 'latest' tag in manner you are using it\nContainer port and your desired target local port are not the same\nHere are Dockerfile variations for python 3:\nFROM python:latest\nCOPY index.html /\nEXPOSE 7000\nCMD python -m http.server 7000\nand python 2.7:\nFROM python:2.7\nCOPY index.html /\nEXPOSE 7000\nCMD python -m SimpleHTTPServer 7000\nalongside with build\ndocker build -t my-docker-image .\nand run commnand:\ndocker run --rm -it --name my-docker-instance -p 80:7000 my-docker-image\nAfter run you can go to http://localhost to get container's port 7000 there, providing your host doen't run something on port 80 (remap if so).\nNotes:\nUsing latest image is ok for development, but problematic in production\nwork dir is set to root, maybe you would like to position files appropriately\nrunning code off simple server is ok for defvelopment\nEdit: I see that b0gusb beat me to it :)",
    "docker-compose volume is empty even from initialize": "The problem is that you're expecting files from the Container to be mounted on your host.\nThis is not the way it works: it's the other way around:\nDocker mounts your host folder in the container folder you specify. If you go inside the container, you will see that where there were supposed to be the init files, there will be nothing (or whatever was in your host folder(s)), and you can write a file in the folder and it will show up on your host.\nYour best bet to get the init files and modify them for your container is to:\nCreate a container without mounting the folders (original container data will be there)\nRun the container (the container will have the files in the right place from the installation of nginx etc...) docker run <image>\nCopy the files out of the container with docker cp <container>:<container_folder>/* <host_folder>\nNow you have the 'original' files from the container init on your host.\nModify the files as needed for the container.\nRun the container mounting your host folders with the new files in them.\nEDIT: another way to export those files is to mount a folder somewhere else in the container (like /tmp), get into the container (docker exec), and move / copy the files from the container to that mounted folder. The files will appear on the host. Then quit the container, edit the files and re-mount the folder at the right mounting point.\nNotes: You might want to go inside the container with shell (docker run -it <image> /bin/sh) and zip up all the folders to make sure you got everything if there are nested folders, then docker cp ... the zip file\nAlso, be careful about filesystem case sensitivity: on linux files are case sensitive. On Mac OS X, they're not. So if you have Init.conf and init.conf in the same folder, they will collide when you copy them to a Mac OS X host.",
    "How to view Docker image layers on Docker Hub?": "Docker Hub is quite limited at the moment and does not offer the feature you asked for.\nWhen an image is configured to build from source at Docker Hub (an Automated Build) you can see what went into it, but when it is uploaded pre-built you have no information.",
    "Install pandas in a Dockerfile": "I realize this question has been answered, but I have recently had a similar issue with numpy and pandas dependancies with a dockerized project. That being said, I hope that this will be of benefit to someone in the future.\nMy solution:\nAs pointed out by Aviv Sela, Alpine does not contain build tools by default and will need to be added though the Dockerfile. Thus see below my Dockerfile with the build packages required for numpy and pandas for be successfully installed on Alpine for the container.\nFROM python:3.6-alpine3.7\n\nRUN apk add --no-cache --update \\\n    python3 python3-dev gcc \\\n    gfortran musl-dev g++ \\\n    libffi-dev openssl-dev \\\n    libxml2 libxml2-dev \\\n    libxslt libxslt-dev \\\n    libjpeg-turbo-dev zlib-dev\n\nRUN pip install --upgrade pip\n\nADD requirements.txt .\nRUN pip install -r requirements.txt\nThe requirements.txt\nnumpy==1.17.1\npandas==0.25.1\nEDIT:\nAdd the following (code snippet below) to the Dockerfile, before the upgrade pip RUN command. It is critical to the successful installation of pandas as pointed out by Bishwas Mishra in a comment.\nRUN pip install --upgrade cython",
    "How to modify a Docker image?": "As an existing docker image cannot be changed, what I did was that I created a dockerfile for a new Docker image based on my original Docker image for its contents, and modified it to include a test folder from local into the new image.\nThis link was helpful:\nBuild your own image - Docker Documentation\nFROM abc/def:latest\nThe above line in the Docker file tells Docker which image your image is based on. So, the contents from parent image are copied to new image.\nFinally, for including the test folder from local drive, I added the following command in my Docker file\nCOPY test /home/humpty-dumpty/test\n...and the test folder was added into that new image.\nHere is the dockerfile used to create the new image from the existing one.\nFROM abc/def:latest\n\n# Extras\nRUN sudo apt-get install -y vim\n\n# copies local folder into the image \nCOPY test /home/humpty-dumpty/test\nUpdate: For editing a file in the running docker image, we can open that file using vim editor installed through the docker file shown above:\nvim <filename>\nNow, the vim commands can be used to edit and save the file.",
    "Can a Helm Install create a container from a dockerfile?": "No. A helm chart is a templated set of kubernetes manifests. There will usually by a manifest for a Pod, Deployment, or Daemonset. Any of those will have a reference to a docker image (either hard coded or a parameter). That image will usually be in a container registry like dockerhub. You'll need to build your image using the docker file, push it to a registry, reference this image in a helm chart, then install or update helm.",
    "Setting context in docker-compose file for a parent folder": "You've set:\n dockerfile: .\nJust try to use a relative path to you Dockerfile from the set context:\ncontext: ../../\ndockerfile: ./folder1/folder2/Dockerfile",
    "Docker How to run /usr/sbin/init and then other scripts on start up [closed]": "Declaring\nENTRYPOINT [\"/usr/sbin/init\"]\nCMD [\"systemctl\"]\nWill result in:\n/usr/sbin/init systemctl\nIn other words, the ENTRYPOINT directive sets the executable which is used to execute the command given from the COMMAND directive.\nThe default ENTRYPOINT is /bin/sh -c so /bin/sh -c /data/docker-entrypoint.sh should work, if /data/docker-entrypoint.sh contains:\n/usr/sbin/init\nsystemctl restart autofs\npython /data/myapp.py\nThat means: You don't have to change the ENTRYPOINT\nIf you change the the ENTRYPOINT to /data/docker-entrypoint.sh than it should contain something like:\n/usr/sbin/init\nsystemctl restart autofs\npython /data/myapp.py\n# run the command given as arguments from CMD\nexec \"$@\"\nreference",
    "how to rsync from a host computer to docker container using docker cp": "The way to use rsync to copy files into a Docker container\nMake sure your Docker container has rsync installed, and define this alias:\nalias drsync=\"rsync -e 'docker exec -i'\"\nNow, you can use rsync with containers as if they are remote machines:\ndrsync -av /source/ container:/destination/",
    "Can't find module error when building docker for NodeJS app": "The problem was that our front-end developer considered that node imports are case insensitive and he was using windows. I tried to run Dockerfile on mac and that's why it couldn't find the modules. Module name was resetPass!",
    "Running a background process in container during one step in docker build": "As halfer states in his comment, this is not good practice.\nHowever for completeness I want to share a solution to the original question nevertheless:\nRUN nohup bash -c \"redis-server &\" && sleep 4 && /opt/gradle/gradle-4.6/bin/gradle build --info\nThis runs redis-server only for this single layer. The sleep 4 is just there to give redis enough time start up.\nSo the Dockerfile then looks as follows:\nFROM ubuntu:16.04\n\n# apt-get install stuff\n# ...\n# install gradle\n# build and install redis\n\nWORKDIR /app\nADD . /app\n\n# run unit tests / integration tests of app\nRUN nohup bash -c \"redis-server &\" && sleep 4 && /opt/gradle/gradle-4.6/bin/gradle build --info\n\n# TODO: uninstall redis\n\n# build app\nRUN ./gradlew assemble\n\n# start app with\n# docker run\nCMD [\"java\", \"-jar\", \"my_app.jar\"]",
    "If I run `docker-compose up` do I need a Dockerfile in the directory as well as a docker-compose.yml file?": "docker-compose lets you choose between the 2 options\nbuild the container from a specified image:\nservices:\n  example:\n    image: your/imagename\nbuild the container out of a Dockerfile:\nservices:\n  example:\n    build: \n      context: path/to/Dockerfile/dir\n      dockerfile: Dockerfile #here you specify the name of your Dockerfile file",
    "List all files in Build Context and/or in WORKDIR when building container image": "RUN. You can run any command you want in a container.\nRUN ls will run ls and print the output of the command.",
    "Create a Linux-based Docker file for .NET Framework project": "Finally, after a week of trying, I was able to get an answer.\nWe have to base the image on Nginx and install the mono on it.\nCreate a folder that contains the following:\nPublish your asp project in the dist folder.\nIn the Nginx folder create a folder with the sites-available name.\nIn the sites-available folder create a file with the default name and the following codes:\n    server {\n             listen   80;\n             access_log   /var/log/nginx/mono-fastcgi.log;\n             root /var/www/;\n             server_tokens off;\n             more_clear_headers Server X-AspNet-Version;\n    \n             location / {\n                     index index.html index.htm default.aspx Default.aspx;\n                     fastcgi_index /;\n                     fastcgi_pass unix:/var/run/mono-fastcgi.sock;\n                     include /etc/nginx/fastcgi_params;\n             }\n     }\nIn the Nginx folder create a file with the fastcgi_params name and the following codes:\nfastcgi_param  QUERY_STRING       $query_string;\nfastcgi_param  REQUEST_METHOD     $request_method;\nfastcgi_param  CONTENT_TYPE       $content_type;\nfastcgi_param  CONTENT_LENGTH     $content_length;\n\nfastcgi_param  SCRIPT_NAME        $fastcgi_script_name;\nfastcgi_param  REQUEST_URI        $request_uri;\nfastcgi_param  DOCUMENT_URI       $document_uri;\nfastcgi_param  DOCUMENT_ROOT      $document_root;\nfastcgi_param  SERVER_PROTOCOL    $server_protocol;\nfastcgi_param  REQUEST_SCHEME     $scheme;\nfastcgi_param  HTTPS              $https if_not_empty;\n\nfastcgi_param  GATEWAY_INTERFACE  CGI/1.1;\nfastcgi_param  SERVER_SOFTWARE    nginx/$nginx_version;\n\nfastcgi_param  REMOTE_ADDR        $remote_addr;\nfastcgi_param  REMOTE_PORT        $remote_port;\nfastcgi_param  SERVER_ADDR        $server_addr;\nfastcgi_param  SERVER_PORT        $server_port;\nfastcgi_param  SERVER_NAME        $server_name;\n\nfastcgi_param  PATH_INFO          \"\";\nfastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;\nIn the pools folder create a file with the sample.webapp name and the following codes:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<apps>\n    <web-application>\n        <name>root</name>\n        <vhost>*</vhost>\n        <vport>-1</vport>\n        <vpath>/</vpath>\n        <path>/var/www/sample-app/</path>\n    </web-application>\n</apps>\nsupervisord.conf file:\n[supervisord]\nlogfile=/var/log/supervisor/supervisord.log\nlogfile_maxbytes = 50MB\nnodaemon=true\nuser=root\n\n[program:mono]\ncommand=fastcgi-mono-server4 --appconfigdir=/etc/mono/pools --socket=unix --filename=/var/run/mono-fastcgi.sock --printlog --name=mono\nuser=root\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\n\n[program:nginx]\ncommand=nginx\nuser=root\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\nFinally Dockerfile content is as follows:\nFROM mono:latest\n\nRUN apt-get update \\\n  && apt-get install -y \\\n  iproute2 supervisor ca-certificates-mono fsharp mono-vbnc nuget \\\n  referenceassemblies-pcl mono-fastcgi-server4 nginx nginx-extras \\\n  && rm -rf /var/lib/apt/lists/* /tmp/* \\\n  && echo \"daemon off;\" | cat - /etc/nginx/nginx.conf > temp && mv temp /etc/nginx/nginx.conf \\\n  && sed -i -e 's/www-data/root/g' /etc/nginx/nginx.conf\n\nCOPY nginx/ /etc/nginx/\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\nCOPY pools /etc/mono/pools\nCOPY dist /var/www/sample-app\n\n\nEXPOSE 80\n\nENTRYPOINT [ \"/usr/bin/supervisord\", \"-c\", \"/etc/supervisor/conf.d/supervisord.conf\" ]",
    "How to use dotnet restore properly in Dockerfile": "When Docker builds an image, it maintains a build cache:\nWhen building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. As each instruction is examined, Docker looks for an existing image in its cache that it can reuse, rather than creating a new (duplicate) image.\nImportantly, the ADD and COPY instructions get special treatment:\nFor the ADD and COPY instructions, the contents of the file(s) in the image are examined and a checksum is calculated for each file. The last-modified and last-accessed times of the file(s) are not considered in these checksums. During the cache lookup, the checksum is compared against the checksum in the existing images. If anything has changed in the file(s), such as the contents and metadata, then the cache is invalidated.\nWhen building a .NET Core solution, we can be sure that after running dotnet restore, the result of running dotnet restore again will only change if the .csproj file has changed (e.g. a new package is added or a version is changed).\nBy copying the .csproj files into the image separately, we can take advantage of Docker's build cache, which means that so long as the .csproj file hasn't changed, the dotnet restore step will not be re-executed unnecessarily each and every time the image gets rebuilt.",
    "ERRO[0001] error waiting for container: context canceled": "alpine does not include bash by default. If you want to include bash, you should add RUN apk add --no-cache bash in your Dockerfile.",
    "docker not exposing the port with network host": "--net=host option\nThis option bind the virtual NIC of the container to the host physical NIC (by giving full access to local system services such as D-bus).\nWhen this option is used every program that request a network socket will be granted one by the host from the physical NIC. Your service will then be using the 5555 port as expected.\n-p 5555:5555 option\nThis option bind (through iptable-like mechanism) the network socket containter-ip:5555 to the network socket host-ip:5555.\nIn other words\nIt seems, IMHO, a bit illogical to use them both. If the needs is to publish the containerized service to the socket host-ip:5555 then the cleanest way is to only use the -p 5555:5555 option.",
    "Installed gems not found by bundler when BUNDLE_PATH changed with Docker": "I think that there is a lack of GEM_HOME/GEM_PATH in your code.\nGEM_HOME/GEM_PATH will be used by gem install xxx to install gems in a specific folder. BUNDLE_PATH will be used by bundle install to install gems in a specific folder but not by gem install xx\nTo have a working system you should do :\nFROM ruby:1.9.3\n\nRUN apt-get update -qq && apt-get install -y build-essential libpq-dev vim\nENV APP_HOME /next-reg\nRUN mkdir $APP_HOME\nWORKDIR $APP_HOME\n\nENV BUNDLE_PATH /box\nENV GEM_PATH /box\nENV GEM_HOME /box\n\nADD . $APP_HOME\n\nRUN gem install bundler\nRUN gem install tzinfo -v 1.2.2\n\nCOPY Gemfile Gemfile\n\nRUN  bundle install\nWith this Gemfile :\nsource 'https://rubygems.org'\n\ngem 'tzinfo', '1.2.2'\nWich will produce :\nStep 11/13 : RUN gem install tzinfo -v 1.2.2\n ---> Running in 8a87fa54fa19\nSuccessfully installed thread_safe-0.3.6\nSuccessfully installed tzinfo-1.2.2\n2 gems installed\n ---> 3c91d59bde8a\nRemoving intermediate container 8a87fa54fa19\n\nStep 13/13 : RUN bundle install\n ---> Running in 20f1e4ec93b1\nDon't run Bundler as root. Bundler can ask for sudo if it is needed, and\ninstalling your bundle as root will break this application for all non-root\nusers on this machine.\nFetching gem metadata from https://rubygems.org/...\nFetching version metadata from https://rubygems.org/.\nResolving dependencies...\nRubygems 1.8.23.2 is not threadsafe, so your gems will be installed one at a time. Upgrade to Rubygems 2.1.0 or higher to enable parallel gem installation.\nInstalling rake 12.0.0\nUsing thread_safe 0.3.6\nUsing bundler 1.14.6\nUsing tzinfo 1.2.2\nBundle complete! 2 Gemfile dependencies, 4 gems now installed.\nBundled gems are installed into /box.\nAs you can see in the result output, the bundle install re-use the preloaded gems from gem install",
    "Docker, docker-compose, restart: unless-stopped loose logs in console": "Easy Answer, it's neither a docker bug nor a bug on your end =)\nDocker logs are attached to containers respectively, so when you're trying to see the logs (after your new app container has been created) you'll notice that it's empty and there's no history to scroll up for. While in fact the logs you're looking for were attached to your old container that has been removed and replaced by the new app container.\nas a work around, just always mount volume the log files from within your rails app to outside docker, that way you wont lose any data.\nthe logs/data will be persistent even if you stop or remove the container.",
    "Docker run gives \"CreateProcess: failure in a Windows system call: The system cannot find the file specified. (0x2)\"": "In the runner config.toml, my runner was configured to use the shell pwsh.\nI replaced it with powershell and it worked.\nThe error comes from the fact that, for whatever reason, pwsh is not available.",
    "Dockerfile ENV variable substitution into another ENV variable": "Option 1: at container start\nYou can use a wrapper script to create your environment variables with the inheritance that you wish. Here is a simple wrapper script\nwrapper.sh\n#!/bin/bash\n\n# prep your environement variables\nexport command=\"echo Hello $var_env\"\n\n# run your actual command\necho \"Hello $command\"\nYour dockerfile needs to be adapted to use it\nFROM ubuntu\nCOPY ./wrapper.sh .\nENV var_env=Tarun\nENV command=\"echo Hello $var_env\"\nCMD [\"sh\",\"-c\",\"./wrapper.sh\"]\nOption 2: during build\nYou can archive this by rebuilding your image with different build args. Lets keep your dockerfile almost the same:\nFROM ubuntu\nARG var_env=Tarun\nENV command=\"echo Hello $var_env\"\nCMD [\"sh\",\"-c\",\"echo Hello $var_env\"]\nand run\ndocker build -t test .\nthis gives you your default image as defined in your dockerfile, but your var_env is no longer an environment variable.\nnext we run\ndocker build -t test --build-arg var_env=\"New Env Value\" .\nthis will invalidate the docker cache only from the line in which you have defined your build arg. So keep your definition of your ARG close to where it is used in order to maximize the caching functionality of docker build.\nYou can find more about build args here: https://docs.docker.com/engine/reference/commandline/build/",
    "How do I reuse the cache from a `RUN --mount=type=cache` docker build?": "There doesn't seem to be any way to extract this specific cache from the general docker working files.\nHowever, you can of course backup the whole of /var/lib/docker. This doesn't work for CircleCI's remote docker engine, because you don't have sudo access, but does work for GitHub Actions where you do.\nSee here for an example: https://github.com/Mahoney-playground/docker-cache-action",
    "Using docker --squash in docker-compose when building images": "Instead of using --squash, you can use Docker multi-stage builds.\nHere is a simple example for a Python app that uses the Django web framework. We want to separate out the testing dependencies into a different image, so that we do not deploy the testing dependencies to production. Additionally, we want to separate our automated documentation utilities from our test utilities.\nHere is the Dockerfile:\n# the AS keyword lets us name the image\nFROM python:3.6.7 AS base\nWORKDIR /app\nRUN pip install django\n\n# base is the image we have defined above\nFROM base AS testing\nRUN pip install pytest\n\n# same base as above\nFROM base AS documentation\nRUN pip install sphinx\nIn order to use this file to build different images, we need the --target flag for docker build. The argument of --target should name the name of the image after the AS keyword in the Dockerfile.\nBuild the base image:\ndocker build --target base --tag base .\nBuild the testing image:\ndocker build --target testing --tag testing .\nBuild the documentation image:\ndocker build --target documentation --tag documentation .\nThis lets you build images that branch from the same base image, which can significantly reduce build-time for larger images.\nYou can also use multi-stage builds in Docker Compose. As of version 3.4 of docker-compose.yml, you can use the target keyword in your YAML.\nHere is a docker-compose.yml file that references the Dockerfile above:\nversion: '3.4'\n\nservices:\n    testing:\n        build:\n            context: .\n            target: testing\n    documentation:\n        build:\n            context: .\n            target: documentation\nIf you run docker-compose build using this docker-compose.yml, it will build the testing and documentation images in the Dockerfile. As with any other docker-compose.yml, you can also add ports, environment variables, runtime commands, and so on.",
    "How can I see which file(s) caused a Dockerfile `COPY` statement to invalidate the cache?": "I don't think there is a way to see which file invalidated the cache with the current Docker image design.\nLayers and images since v1.10 are 'content addressable'. Their ID's are based on a SHA256 checksum which reflects their content.\nThe caching code just looks up the ID of the image/layer which will only exist in Docker Engine if the contents of the entire layer match (or possibly a collision).\nSo when you run docker build, a new build context is created for each command in the Dockerfile. A checksum is calculated for the entire layer that command would produce. Then docker checks to see if an existing layer is available with that checksum and run config.\nThe only way I can see to get individual file detail back would be to recompute the destination file checksums, which would probably negate most of the caching speed up. If you did want to do this anyway, the other problem is deciding which layer to check that against. You would have to lookup a previous image build tree (maybe by tag?) to find what the contents of the previous comparable layer were.",
    "Bad file descriptor ERROR during apk update in Docker container... Why?": "--no-cache option allows to not cache the index locally. That is helpful in keeping the container small.\nAlso, it is equivalent to apk update at the top and rm -rf /var/cache/apk/ in the end.\nSo you can try to use it this way:\nRUN apk add --update --no-cache bash \\\n    git \\\n    make \\\n    clang \\\n    g++ \\\n    go && \\\n    mkdir -p $REPO && \\\n    mkdir -p $GODIR/src && \\\n    rm -rf /usr/share/man && \\\n    apk del git clang",
    "Docker Ignore is not woking well with docker compose and my file structure": "At build time, the directive COPY . . (inside the Dockerfile) correctly copies all files not listed in .dockerignore in $RAILS_ROOT (inside the image). No problem here (check that by running docker run --rm custom-web:1.0 ls -al).\nBut here you run docker-compose to start the container, and you have defined a volume :\nvolumes:\n    - .:/var/www/${APP_NAME}_web\nThat means that files from the same directory as docker-compose.yml are shared between the host and the container. That's why you find all files (even those listed in .dockerignore) in $RAILS_ROOT (workdir of custom-web:1.0 image) after starting the container via docker-compose.\nIf you really need to share files between your host and the container (via a volume), I'll suggest you to mount the current directory in another location than the one specified in your Dockerfile, like :\nvolumes:\n    - .:/${APP_NAME}_web\nOtherwise, using COPY . . and a volume is redundant here.",
    "Docker build error OCI runtime create failed \"exec: \\\"/bin/bash\\\": stat /bin/bash": "Your container doesn't have bash installed but probably it has sh so run the container with (replace /bin/bash with /bin/sh):\ndocker exec -it username/imagename /bin/sh",
    "OSX Docker Build: How can I see the full build output? (--progress=plain not the solution!)": "Just set export DOCKER_BUILDKIT=0 in your shell....",
    "ERROR with \"Failed to set locale, defaulting to C\" on Centos at the docker environment (yum install)": "This is a good method to handle this issue, please follow the code to install the package \"glibc-langpack-en\" in your environment or put the command line in your dockerfile.\nDockerfile content\nFROM centos\nRUN yum install -y glibc-langpack-en\nCentos shell script\nsudo yum install -y glibc-langpack-en",
    "Install .NET Framework 3.5 on Windows Server Core Docker": "I took the following steps to resolve this issue:\nGot hold of the Windows Server 2016 Core ISO file. Mounted the file on local computer.\nExtracted the {mount}:/sources/sxs folder into a zip file (sxs.zip). Ensure that the .NET Framework 3.5 cab file (microsoft-windows-netfx3-ondemand-package.cab) is present in the sxs folder. In my case, this was the only file present in the sxs folder.\nCopy the sxs.zip file to my container. I copied it using the dockerfile of the image.\nUnzip the file to C:\\sources\\sxs folder in the container.\nUsed the Install-WindowsFeature powershell command to install the feature.\nInstall-WindowsFeature -Name NET-Framework-Features -Source C:\\sources\\sxs -Verbose\nHope this helps. I also found the following blog useful in understanding the on-demand features. https://blogs.technet.microsoft.com/askcore/2012/05/14/windows-8-and-net-framework-3-5/",
    "How to solve \"Can't separate key from value\" in docker": "Check for the version number of docker and if its 3.4+ then the docker compose v2 is enabled by default. To disable it, go to > docker desktop > preferences > experimental features > un-check \"use Docker Compose V2\" option. This is a move by docker hub to incorporate docker-compose as docker compose and may cause problems to your usual workflow. Enjoy :)",
    "Docker - Failed to ping backend": "I am also having a similar issue, and wanted to force quit the app, so I tried cmd + options + esc, but couldn't find the app in the options.\nI finally solved the issue by killing the docker process using Activity Monitor.\nSteps:\nSearch for activity monitor by pressing (cmd + space), start typing activity monitor, then hit enter\nGo to search bar and type docker, you will see the docker process in red with (not responding)\nDouble click on the docker process\nClick Quit and select Force quit.",
    "WORKDIR $HOME in Dockerfile does not seem to work": "Seems like the best way would be to explicitly set your own default value so you can be sure it's consistent, like:\nENV HOME /root\n\nWORKDIR $HOME\n.. do something in /root ..\n\nWORKDIR /tmp\n.. do something else in /tmp ..\n\nWORKDIR $HOME\n.. continue back in /root ..\nNote:\nThe WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile.\nhttps://docs.docker.com/engine/reference/builder/#/workdir",
    "Dockerfile running from Mysql cannot access apt-get": "Apparently since Oracle bought MySQL in 2010, they have been converting everything over to their proprietary OS. In the last few months, they switched the default mysql package to Oracle OS from Debian.\nSee the packages here: https://hub.docker.com/_/mysql\nYou now need to specify the debian package like:\nFROM mysql:5.7-debian\n\nRUN apt-get -y update && apt-get upgrade -y",
    "Why docker-compose down deletes my volume? how to avoid this action done by 'down'. (Postgresql)": "According to the documentation, docker compose down will not delete any volume unless the -v option is used.",
    "Dockerfile and dpkg command": "Not the most elegant but:\n# continue executing even if command fails\nRUN dpkg -i vufind_3.1.1.deb || true",
    "Dynamically set JAVA_HOME of docker container": "Set JAVA_HOME in docker container\nDefault Docker file of the official image is Dockerfile\nIf you still want your own image with Java home set. Add this lines to your Dockerfile\nRUN apt-get update && \\\n    apt-get install -y openjdk-8-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/ && \\\n    rm -rf /var/cache/oracle-jdk8-installer;\n    \nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\nRUN export JAVA_HOME\nCreate the docker-compose.yml, replace the dynamic path with container updating the environment variable in the environment:",
    "Installing and using Gradle in a docker image/container": "I solved the problem using the ENV docker instructions (link to the documentation).\nENV GRADLE_HOME=/app/gradle-2.4\nENV PATH=$PATH:$GRADLE_HOME/bin",
    "Does WORKDIR create a directory?": "The Dockerfile WORKDIR directive\n... sets the working directory.... If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.\nI occasionally see SO questions that RUN mkdir a directory before switching WORKDIR to it. Since WORKDIR will create the directory, this isn't necessary.\nAll paths in a Dockerfile are always inside the image, except for the source paths for COPY and ADD instructions, which are inside the build context directory on the host. Absolute paths like /code will be directly inside the root directory in the image, following normal Unix conventions.\nYou can run temporary containers off of your image to examine this, even if the Dockerfile isn't complete yet.\nhost$ docker build -t my-image .\nhost$ docker run --rm my-image ls -l /\nhost$ docker run --rm -it my-image /bin/sh\n0123456789ab# ls -l /\n0123456789ab# exit\n(This will always work, assuming the image includes core tools like sh and ls. docker exec requires the container to be running first; while you're refining the Dockerfile this may not be possible yet.)",
    "adding .net core to docker container with Jenkins": "As of this response you can use the following Dockerfile to get .NetCore 2 installed into the Jenkins container. You can obviously take this further and install the needed plugins and additional software as needed. I hope this helps you out!\nFROM jenkins/jenkins:lts\n # Switch to root to install .NET Core SDK\nUSER root\n\n# Just for my sanity... Show me this distro information!\nRUN uname -a && cat /etc/*release\n\n# Based on instructiions at https://learn.microsoft.com/en-us/dotnet/core/linux-prerequisites?tabs=netcore2x\n# Install depency for dotnet core 2.\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n    curl libunwind8 gettext apt-transport-https && \\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg && \\\n    mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg && \\\n    sh -c 'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-debian-stretch-prod stretch main\" > /etc/apt/sources.list.d/dotnetdev.list' && \\\n    apt-get update\n\n# Install the .Net Core framework, set the path, and show the version of core installed.\nRUN apt-get install -y dotnet-sdk-2.0.0 && \\\n    export PATH=$PATH:$HOME/dotnet && \\\n    dotnet --version\n\n# Good idea to switch back to the jenkins user.\nUSER jenkins",
    "What exactly is the frontend and backend of docker buildkit?": "TLDR; The frontend and backend concept was born with Buildkit and didn't exist in docker before. Frontend is like a compiler that converts a user's file (eg: Dockerfile) to LLB. Backend executes LLB in the most efficent way to build a docker image.\nHistory\nWithout Buildkit, a docker image is built directly using instructions in a Dockerfile. No intermeditate representation of these instructions is created. The instructions are passed to the docker engine (also called the Moby Engine or classic builder) which then builds the image.\nThen it was realised that to improve and optimise the build process further most of the fundamentals of the build operation would have to be redefined. Hence a proposal was made to create a new engine, and Buildkit was born along with frontend and backend separation of the build process.\nOne of the main design goals of buildkit is to separate frontend and backend concerns during a build process. A frontend is something designed for the users to describe their build definition. Backend solves the problem of finding a most efficient way to solve a common low-level description of the build operations, that has been prepared for them by the frontends.\nLLB\nThe separation of frontend and backend is acheived by LLB(low-leve builder).\nEverything about execution and caching of your builds is defined only in LLB.\nFrontend\nFrontends are components that run inside BuildKit and convert any build definition(file written by the user) to LLB. BuildKit supports loading frontends dynamically from container images by specifying: #syntax=.... A famous frontend is the dockerfile frontend because it is used with the docker engine. You can specify this container image with: #syntax=docker/dockerfile:latest.\nThere are plenty of other frontends that can be used, for example the mockerfile frontend with: #syntax=r2d4/mocker. This then allows you to use a slightly different syntax compared to the usual Dockerfile syntax.\nBackend\nThe Buildkit backend solves the LLB generated from any of a variety of frontends. Since the LLB is a dependency graph, it can be processed to: detect and skip executing unused build stages, parallelize building independent build stages etc. This is why Buildkit is able to improve performance, storage management etc. over the older build process. Also, the caching model has been entirely rewritten.\nThe core part of the builder(Buildkit) is a solver that takes a DAG of low-level build(LLB) instructions from the frontend and finds a way to execute them in a most efficient manner while keeping the cache for the next invocations.\nTo use the Buildkit backend specifyDOCKER_BUILDKIT=1.\nStarting with version 18.09, Docker supports a new backend for executing your builds that is provided by the moby/buildkit project.\nThe Moby Engine(the classic builder) can be called the original backend but remember it doesn't use LLB, therefore its build process doesn't have a frontend and backend as such.\nReferences and resouces:\nThe original proposal to create Buildkit by Tonis Tiigi\nTonis's introductory blog about Buildkit with an intro to LLB\nAn article on how docker build works internally without buildkit\nA Docker blog with content on Buildkit, frontends, and LLB\nMockerfile website, which includes code on how to generate LLB\nDockerfile reference",
    "DL4006 warning: Set the SHELL option -o pipefail before RUN with a pipe in it": "Oh, just found the solution in the wiki page at https://github.com/hadolint/hadolint/wiki/DL4006\nHere is my fixed version:\nFROM strimzi/kafka:0.20.1-kafka-2.6.0\n\nUSER root:root\nRUN mkdir -p /opt/kafka/plugins/debezium\n# Download, unpack, and place the debezium-connector-postgres folder into the /opt/kafka/plugins/debezium directory\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\nRUN curl -s https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.7.0.Final/debezium-connector-postgres-1.7.0.Final-plugin.tar.gz | tar xvz --transform 's/debezium-connector-postgres/debezium/' --directory /opt/kafka/plugins/\nUSER 1001\nThe reason adding SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] is at https://github.com/docker/docker.github.io/blob/master/develop/develop-images/dockerfile_best-practices.md#using-pipes\nBelow is a copy:\nSome RUN commands depend on the ability to pipe the output of one command into another, using the pipe character (|), as in the following example:\nRUN wget -O - https://some.site | wc -l > /number\nDocker executes these commands using the /bin/sh -c interpreter, which only evaluates the exit code of the last operation in the pipe to determine success. In the example above this build step succeeds and produces a new image so long as the wc -l command succeeds, even if the wget command fails.\nIf you want the command to fail due to an error at any stage in the pipe, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:\nRUN set -o pipefail && wget -O - https://some.site | wc -l > /number\nNot all shells support the -o pipefail option.\nIn cases such as the dash shell on Debian-based images, consider using the exec form of RUN to explicitly choose a shell that does support the pipefail option. For example:\nRUN [\"/bin/bash\", \"-c\", \"set -o pipefail && wget -O - https://some.site | wc -l > /number\"]",
    "Install oracle client in docker container": "I have figure out some different way to install Oracle instant client in ubuntu Docker, it might help others\nFollow these simple steps:\nDownload oracle instant client (.rpm file) from oracle official download center\nConvert into .deb (you can use apt-get install alien ) and move somewhere in your working directory.\nNow Update your Dockerfile and make build\nRUN apt-get update\nWORKDIR /opt\nADD ./ORACLE-INSTANT-CLIENT.deb  /opt\n#if libaio also required\nRUN apt-get install libaio1 \nRUN dpkg -i oracle-instantclient.deb",
    "Install build-essential in Docker image without having to do `apt-get update`?": "Create a base image which contains:\nFROM python:3.7-slim\n\nRUN apt-get update && apt-get install build-essential -y\nBuild it:\ndocker build -t mybase .\nThen use it for new images:\nFROM mybase",
    "apt-get install in Ubuntu 16.04 docker image: '/etc/resolv.conf': Device or resource busy": "As mentioned in https://github.com/moby/moby/issues/1297 you can add the following line to your Dockerfile:\nRUN echo \"resolvconf resolvconf/linkify-resolvconf boolean false\" | debconf-set-selections\nThis way it is possible to install resolvconf inside a container.",
    "How to append multi-lines to file in a dockerfile? [duplicate]": "That should do the trick:\nRUN echo $'first line \\n\\\nsecond line \\n\\\nthird line' > /etc/nginx/nginx.conf\nBasically it's wrapped in a $'' and uses \\n\\ for new lines.",
    "Nodemon inside docker container": "In you Dockerfile, you are running npm install after copying your package*json files. A node_modules directory gets correctly created in /usr/src/app and you're good to go.\nWhen you mount your local directory on /usr/src/app, though, the contents of that directory inside your container are overriden with your local version of the node project, which apparently is lacking the node_modules directory, causing the error you are experiencing.\nYou need to run npm install on the running container after you mounted your directory. For example you could run something like:\ndocker exec -ti <containername> npm install\nPlease note that you'll have to temporarily change your CMD instruction to something like:\nCMD [\"sleep\", \"3600\"]\nIn order to be able to enter the container.\nThis will cause a node_modules directory to be created in your local directory and your container should run nodemon correctly (after switching back to your current CMD).",
    "Docker Tomcat users configuration not working": "First you need to expose your application in the container, so you can connect to it from dockerhost/network.\ndocker run -d -p 8000:8080 tomcat:8.5.11-jre8\nYou need to change 2 files in order to access the mangaer app from remote host. (Browser on Docker host is considered remote, only packets received on containers loopback are considered local for tomcat)\n/usr/local/tomcat/webapps/manager/META-INF/context.xml Note the commented section.\n<Context antiResourceLocking=\"false\" privileged=\"true\" >\n<!--\n     <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n         allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" />\n-->\nPlease note the commented section.\n/usr/local/tomcat/conf/tomcat-users.xml as you stated in the question.\n<tomcat-users xmlns=\"http://tomcat.apache.org/xml\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          xsi:schemaLocation=\"http://tomcat.apache.org/xml tomcat-users.xsd\"\n          version=\"1.0\">\n<role rolename=\"manager-gui\"/>\n<role rolename=\"manager-script\"/>\n<user username=\"admin\" password=\"password\" roles=\"manager-gui,manager-script\" />\nIn order to make changes to files in the container, You can try building your own image, but I suggest using docker volumes or bind mounts.\nAlso make sure you restart the container so the changes take effect.",
    "What is the difference between `AS base` and `AS build` in the Dockerfile?": "There is no functional difference. it's a name for the build stage.\nA stage is a part of the dockerfile that starts at the FROM keyword and ends before the next FROM keyword.\nImage built in a stage of the dockerfile will be later accessible using that stage's name.\nFor example\nFROM image AS foo\n...\n...\n\nFROM foo AS bar\nRUN touch /example\n...\n...\n\nFROM foo\nCOPY --from=bar /example /var/example\nOptionally a name can be given to a new build stage by adding AS name to the FROM instruction. The name can be used in subsequent FROM and COPY --from=<name|index> instructions to refer to the image built in this stage.\nhttps://docs.docker.com/engine/reference/builder/#from\nIt was added in 17.05 for multistage builds, more on that: https://docs.docker.com/develop/develop-images/multistage-build/",
    "Docker build image complain about: fatal: not a git repository (or any of the parent directories): .git": "change the \"RUN cd /cloned-qt/\" command to \"WORKDIR cloned-qt\" and it will work as expected",
    "Docker error: standard_init_linux.go:228: exec user process caused: exec format error": "A \"multiarch\" Python interpreter built on MacOS is intended to target MacOS-on-Intel and MacOS-on-Apple's-arm64.\nThere is absolutely no binary compatibility with Linux-on-Apple's-arm64, or with Linux-on-aarch64. You can't run MacOS executables on Linux, no matter if the architecture matches or not.",
    "Cannot reach docker container - port not bound": "As David Maze pointed out, order does matter for the docker cmd command.\nThe -dp option needs to come before the image name.\nSo using\ndocker container run -dp 5000:5000 -t test_tag \nworks like a charm.",
    "How do I unset a Docker image label?": "Great question. I did some research and, as far as I know, it's not possible with the current Docker/Moby implementation. It's also a problem for other properties as well, as you can see here (the issue is from 2014!):\nhttps://github.com/moby/moby/issues/3465\nI know it's really annoying, but, if you really want to remove that you can try following this:\nhttps://github.com/moby/moby/issues/3465#issuecomment-383416201\nThe person automatized this process with a Python script that seems to let you do what you want:\nhttps://github.com/gdraheim/docker-copyedit\nIt appears to have the Remove Label operation (https://github.com/gdraheim/docker-copyedit/blob/92091ed4d7a91fda2de39eb3ded8dd280fe61a35/docker-copyedit.py#L304), that is what you want.\nI don't know if it works (I haven't had time to test that), but I think it's worth trying.",
    "docker-compose args from shell": "You can use the same syntax with docker-compose build:\ndocker-compose build --build-arg RSA=\"$(cat ~/.ssh/id_rsa)\"\nUnfortunately, you can't use the build-args option with compose up or start... So you will need to build and then run using the --no-build option",
    "Error [ERR_PACKAGE_PATH_NOT_EXPORTED]: Package subpath './public/extractFiles' is not defined by \"exports\" in": "same problem here.\nIn my case this was raised due to old npm package dependencies.\nextract-files Version 7 uses a deprecated node function:\n(node:2520) [DEP0148] DeprecationWarning: Use of deprecated folder mapping \"./public/\" in the \"exports\" field module resolution of the package at ...\\node_modules\\extract-files\\package.json.\nNode 17 raises this error you posted.\nPossible Solutions:\nUpdate extract-files package to latest\nUse Node 16.x\nBonus tip:\nsince you\u00b4re copying package-lock.json you can run\nnpm ci\ninstead of npm install. Its faster and made for CI/CD pipelines",
    "GeoDjango can't find gdal on docker python alpine based image": "I also struggled with this one for a while, the final solution proved quite simple (im using MySql so less dependencies):\nInstall the dependencies normally in the Dockerfile, e.g:\nRUN apk add --no-cache geos gdal \nAnd then setup their respective variables in the Django settings using glob, e.g:\nfrom glob import glob\n\nGDAL_LIBRARY_PATH=glob('/usr/lib/libgdal.so.*')[0]\nGEOS_LIBRARY_PATH=glob('/usr/lib/libgeos_c.so.*')[0]",
    "How to dockerize an ASP.NET Core 2.0 application?": "In case of related projects you have to run dotnet restore and dotnet publish against your solution file instead and to put your docker file at the solution level so you can access all projects from it.\nBasically the only change you need in the docker file it is:\nCOPY *.sln ./\nCOPY ./your_proj1_folder/*.csproj ./your_proj1_folder/\nCOPY ./your_proj2_folder/*.csproj ./your_proj2_folder/\nRUN dotnet restore",
    "How to use docker-compose yml file for production?": "Suggest you to use Multiple Compose files:\nUsing multiple Compose files enables you to customize a Compose application for different environments or different workflows.\nNext is an example:\n(NOTE: next omit some elements of compose file)\ndocker-compose.yml:\nweb:\n  image: example/my_web_app:latest\ndocker-compose.dev.yml:\nweb:\n  ports:\n    - 80:80\nExecute docker-compose -f docker-compose.yml -d will have no ports map.\nExecute docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d will make docker-compose.dev.yml to override some value of docker-compose.yml which make your aims.\nFor detail, refers to docker doc, it is the official suggestion to handle your scenario, FYI.\nUPDATED:\nYou use build: context: ./mariadb, so compose can always find Dockerfile in the folder mariadb to build, no matter in local dev server or prod server.\nJust above will have image build both on dev & prod server, this is one option for you to follow.\nAnother option as you said in comments:\nBut on prod server, I can only pull and run image, and the image would have to be built with the prod yml file beforehand\nSo you may not want to build image again on prod server?\nThen, next is a updated solution, just an example:\ndocker-compose.yml:\ndb:\n  image: your_maridb_image_name:your_maridb_image_version\n  networks:\n    - default\ndocker-compose.dev.yml:\ndb:\n  build:\n    context: ./mariadb\n  ports:\n    - \"xxx:xxx\"\ndocker-compose.prod.yml:\ndb:\n  otheroptions_special_for_prod_just_a_example: xxx\n1) docker-compose -f docker-compose.yml -f docker-compose.dev.yml -d\nThis will combine as next:\ndb:\n  image: your_maridb_image_name:your_maridb_image_version\n  networks:\n    - default\n  build:\n    context: ./mariadb\n  ports:\n    - \"xxx:xxx\"\nPer docker-compose syntax, if build:context was afford, compose will not docker pull image from docker registry, just find the Dockerfile in context, and finally build a image with the name your specified in image, here it's your_maridb_image_name:your_maridb_image_version.\nThen, you need to push it dockerhub, but you do need to stop your local container.\n2) docker-compose -f docker-compose.yml -f docker-compose.prod.yml -d\nThis will combine as next:\ndb:\n  image: your_maridb_image_name:your_maridb_image_version\n  networks:\n    - default\n  otheroptions_special_for_prod_just_a_example: xxx\nPer docker-compose syntax, no build:context was afford, so compose will directly docker pull image from remote registry(docker hub), remember you have pushed the image to dockerhub after you finished the development on local dev server? So no need to build image again.",
    "'Unable to Find User ContainerUser' when building Windows Dockerfile via Actions Runner": "I had the same issue until I removed the nanoserver part of the FROM statement\nTry:\nFROM mcr.microsoft.com/dotnet/sdk:7.0 AS build\nRather than:\nFROM mcr.microsoft.com/dotnet/sdk:7.0-nanoserver-1809 AS build",
    "How can I pass array into a dockerfile and loop through it?": "You can pass a space-separated string to builds then convert string to an array or just loop over the string.\nDockerfile\nFROM alpine\nARG items\nRUN for item in $items; do \\\n    echo \"$item\"; \\\n    done;\npass value during build time\ndocker build --build-arg items=\"item1 item2 item3 item4\" -t my_image .\noutput\nStep 3/3 : RUN for item in $items; do     echo \"$item\";     done;\n ---> Running in bee1fd1dd3c6\nitem1\nitem2\nitem3",
    "Possible to use pushd/popd in Dockerfile?": "It can be done but your image must have bash and all commands must be in the same RUN directive:\nDockerfile\nFROM debian\nRUN mkdir -p /test/dir1/dir2\nRUN bash -xc \"\\\npushd /test/dir1; \\\npwd; \\\npushd dir2; \\\npwd; \\\npopd; \\\npwd; \\\n\"\nRUN pwd\nSending build context to Docker daemon  77.25MB\nStep 1/4 : FROM debian\n ---> 2d337f242f07\nStep 2/4 : RUN mkdir -p /test/dir1/dir2\n ---> Using cache\n ---> d609d5e33b08\nStep 3/4 : RUN bash -xc \"pushd /test/dir1; pwd; pushd dir2; pwd; popd; pwd; \"\n ---> Running in 79aa21ebdd15\n+ pushd /test/dir1\n+ pwd\n+ pushd dir2\n+ pwd\n+ popd\n+ pwd\n/test/dir1 /\n/test/dir1\n/test/dir1/dir2 /test/dir1 /\n/test/dir1/dir2\n/test/dir1 /\n/test/dir1\nRemoving intermediate container 79aa21ebdd15\n ---> fb1a07d6e342\nStep 4/4 : RUN pwd\n ---> Running in 9dcb064b36bb\n/\nRemoving intermediate container 9dcb064b36bb\n ---> eb43f6ed241a\nSuccessfully built eb43f6ed241a\nSuccessfully tagged test:latest",
    "Multiple WordPress sites with one shared DB using Docker": "Yes, you can install multiple WordPress instances into one database. You just need to change the database prefix for each install when installing. Just check your wp-config and change prefix and DBs credentials.\n// ** MySQL settings - You can get this info from your web host ** //\n/** The name of the database for WordPress */\ndefine('DB_NAME', 'database_name_here');\n\n/** MySQL database username */\ndefine('DB_USER', 'username_here');\n\n/** MySQL database password */\ndefine('DB_PASSWORD', 'password_here');\n\n/** MySQL hostname */\ndefine('DB_HOST', 'localhost');\n\n/** Database Charset to use in creating database tables. */\ndefine('DB_CHARSET', 'utf8');\n\n/** The Database Collate type. Don't change this if in doubt. */\ndefine('DB_COLLATE', '');",
    "Error 'import path does not begin with hostname' when building docker with local package": "The application is built inside the docker container and you need to have your dependencies available when building.\ngolang:onbuild gives compact Dockerfiles for simple cases but it will not fetch your dependencies.\nYou can write your own Dockerfile with the steps needed to build your application. Depending on how your project looks you could use something like this:\nFROM golang:1.6\nADD . /go/src/yourapplication\nRUN go get github.com/jadekler/git-go-websiteskeleton\nRUN go install yourapplication\nENTRYPOINT /go/bin/yourapplication\nEXPOSE 8080\nThis adds your source and your dependency into the container, builds your application, starts it, and exposes it under port 8080.",
    "How to copy/add files in user's home directory in host to container's home directory?": "It has now been two years sice the question has been ask, but I want to cite to official documentation here which states the same as what @Sung-Jin Park already found out.\nADD obeys the following rules:\nThe path must be inside the context of the build; you cannot ADD ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\nDockerfile reference ADD",
    "Permission denied to Docker daemon socket at unix:///var/run/docker.sock": "A quick way to avoid that. Add your user to the group.\nsudo gpasswd -a $USER docker\nThen set the proper permissions.\nsudo setfacl -m \"user:$USER:rw\" /var/run/docker.sock\nShould be good from there.",
    "Build Docker Image From Go Code": "The following works for me;\npackage main\n\nimport (\n    \"archive/tar\"\n    \"bytes\"\n    \"context\"\n    \"io\"\n    \"io/ioutil\"\n    \"log\"\n    \"os\"\n\n    \"github.com/docker/docker/api/types\"\n    \"github.com/docker/docker/client\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    cli, err := client.NewEnvClient()\n    if err != nil {\n        log.Fatal(err, \" :unable to init client\")\n    }\n\n    buf := new(bytes.Buffer)\n    tw := tar.NewWriter(buf)\n    defer tw.Close()\n\n    dockerFile := \"myDockerfile\"\n    dockerFileReader, err := os.Open(\"/path/to/dockerfile\")\n    if err != nil {\n        log.Fatal(err, \" :unable to open Dockerfile\")\n    }\n    readDockerFile, err := ioutil.ReadAll(dockerFileReader)\n    if err != nil {\n        log.Fatal(err, \" :unable to read dockerfile\")\n    }\n\n    tarHeader := &tar.Header{\n        Name: dockerFile,\n        Size: int64(len(readDockerFile)),\n    }\n    err = tw.WriteHeader(tarHeader)\n    if err != nil {\n        log.Fatal(err, \" :unable to write tar header\")\n    }\n    _, err = tw.Write(readDockerFile)\n    if err != nil {\n        log.Fatal(err, \" :unable to write tar body\")\n    }\n    dockerFileTarReader := bytes.NewReader(buf.Bytes())\n\n    imageBuildResponse, err := cli.ImageBuild(\n        ctx,\n        dockerFileTarReader,\n        types.ImageBuildOptions{\n            Context:    dockerFileTarReader,\n            Dockerfile: dockerFile,\n            Remove:     true})\n    if err != nil {\n        log.Fatal(err, \" :unable to build docker image\")\n    }\n    defer imageBuildResponse.Body.Close()\n    _, err = io.Copy(os.Stdout, imageBuildResponse.Body)\n    if err != nil {\n        log.Fatal(err, \" :unable to read image build response\")\n    }\n}",
    "Building Docker images on Windows: Entrypoint script \"no such file or directory\"": "So, even though logs say \"no such file or directory,\" the actual problem (at least in my case) was due to the difference in end-of-line (EOL) characters on Windows and Linux. Windows uses CRLF represent the end of a line and Unix/Linux uses LF.\nI hadn't consider this as a potential problem since the files were freshly cloned from Github and were originally created on Linux. What I didn't know is that on Windows Git is set up to automatically convert EOL characters to CRLF.\nMaking Git retain original EOL characters (disabling autocrlf).\nThere are a few ways to go about doing this. autocrlf is the name of the attribute that decides whether git converts line endings. You'd only need to do one of the following options depending on what you need.\nDisable autocrlf for one command\nYou can clone the files with the following to disable autocrlf as just a one time thing.\ngit clone https://github.com/someuser/somerepo --config core.autocrlf=false\nSpecify EOL type in .gitattributes\nIf you have a single repo that you know you want to always have autocrlf disabled, you can specify it in that repo's .gitattributes file. Just add the following line to your .gitattributes file.\n* text eol=lf\nDisable autocrlf in Git's config file\nNavigate to the folder where Git is installed on your machine. For me it was installed at C:\\ProgramData\\Git. Open config in a text editor. Change autocrlf=true to autocrlf=false.\nChanging EOL characters on existing files.\nIf you've got existing entrypoint scripts that you need to convert, or if you're writing your entrypoint scripts in Windows in the first place, you can easily set the EOL type with most popular text editors. I'll outline how to do it in Vim, Notepad++, and Sublime, but it should be easy enough to figure out by searching \"change EOL\" and the name of your text editor of choice.\nUsing Vim\nTo change the line endings to be compatible with Linux, do :set ff=unix. To change them so that they are compatible with Windows, do :set ff=dos.\nUsing Notepad++\nOn the menu bar click on Edit and then go to EOL Conversion and select the desired conversion. You'll want to select Unix (LF) to make it compatible with Linux.\nUsing Sublime\nOn the menu bar click 'View' and go to 'Line Endings' and from there select the desired conversion. You'll want to select Unix to make it compatible with Linux.\nConverting EOL characters from your Dockerfile.\nAlternatively, there's a useful tool called dos2unix that you can install in your image and use to convert your entrypoint script. Assuming an Ubuntu or Debian based image which uses apt-get , you can use it in the following way.\nFROM php:7.2-fpm\n\nRUN apt-get update && \\\n    apt-get install -y dos2unix\n\nCOPY custom-docker-php-entrypoint /usr/local/bin/\n\nRUN dos2unix /usr/local/bin/custom-docker-php-entrypoint\n\nENTRYPOINT [\"custom-docker-php-entrypoint\"]\nIf your Docker image is based on Alpine linux using apk for a package manager, you'll want to do something like this,\nFROM alpine:latest\n\nRUN apk --update add bash && \\\n    apk add dos2unix\n\nCOPY entrypoint.sh /\n\nRUN dos2unix /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\nIf your Docker image is based on Centos using yum as a package manager, you'll want to do something like this,\nFROM centos:latest\n\nRUN yum update -y && \\\n    yum install dos2unix -y\n\nCOPY entrypoint.sh /\n\nRUN dos2unix /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]",
    "Docker: mounting volume and run node apps": "This is not the right way to use the instruction VOLUME in dockerfile. As documentation says \u201cThe VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers\u201d, and I don't think is what you want to do.\nYou don\u2019t need to specify the VOLUME instruction and you don\u2019t need to create the src directoy. Modify the Dockerfile as below:\nFROM node:6.0.0-slim\nMAINTAINER pyprism\n\n# Install dependencies\nWORKDIR /src/\n\nRUN npm install\n\n# Expose the app port\nEXPOSE 8000\n\n# Start the app\nCMD npm start\nNow you can navigate to the Dockerfile directory and build the image:\n$docker build -t node .\nThen run the container using the \u2013v option to mount your local directory, as below:\n $docker run -p 8000:8000 -v path/to/code/dir:/src/ <image-name>\nthis will mount your code directory in /src.\nif you want to use docker compose simply specify the volumes in the docker-compose.yml file:\n web:\n  build: .\n  volumes:\n    - path/to/code/dir:/src/ \n  ports:\n   - '8000:8000'",
    "Dockerfile Build Error: The system cannot find the path specified": "To build a docker image:\ncd /path/where/docker_file/lives\ndocker build .\nAbove is same as:\ndocker build -f Dockerfile .\nYou need to specify Dockerfile name only if it is not default:\ncd /path/where/docker_file/lives\ndocker build -f Dockerfile.modified .",
    "run django in docker container": "can't open file './manage.py runserver 0.0.0.0:8000 --settings=mysite.settings.prod'\nThis is telling you that it is treating that entire string as a single filename.\nI assume something like this works:\nCMD [ \"python\", \"./manage.py\", \"runserver\", \"0.0.0.0:8000\", \"--settings=mysite.settings.prod\" ]",
    "Installing GMP extention on PHP 7.4 FPM Aplpine (Docker)": "Like the error says: configure: error: GNU MP Library version 4.2 or greater required.\nYou can install GNU MP (GMP for short) on Alpine Linux by including the following in your Dockerfile:\nRUN apk add gmp-dev\n(or RUN apt-get install gmp-dev for other debian distros)",
    "COPY . . command in Dockerfile for ASP.NET": "The COPY . . copies the entire project, recursively into the container for the build.\nThe reason for the separation of the first 2 COPY commands with dotnet restore and then the complete COPY . . with dotnet build is a Docker caching trick to speed up container image builds. It is done this way so the project dependencies don't need to be reinstalled every time a code change is made.\nDocker images are built in layers. Docker compares the contents and instructions that would make up the each new layer to previous builds. If they match the SHA256 checksum for the existing layer, the build step for that layer can be skipped.\nCode changes a lot more than dependencies, and dependencies are usually fetched from a slow(ish) network now. If you copy the code after the dependency installs are completed then you don't bust the cached dependency layer for every other change.\nThis is a common theme across many languages with a dependency manager. Go, Python, Node.js etc. The Node.js equivalent does the package.json and package-lock.json before the rest of the application contents:\nWORKDIR /app\nCOPY package.json package-lock.json /app/\nRUN npm install\nCOPY . /app/\nCMD [\"node\", \"app/index.js\"]",
    "Dockerfile: how to Download a file using curl and copy into the container": "Not related but an easier way to handle downloads during build time is to use Docker's ADD directive without curl or wget.\nADD https://raw.githubusercontent.com/vishnubob/wait-for-it/master/wait-for-it.sh /tmp\nCOPY /tmp/wait-for-it.sh /app/wait-for-it.sh\nRight now, we recommend using Docker's ADD directive instead of running wget or curl in a RUN directive - Docker is able to handle the https URL when you use ADD, whereas your base image might not be able to use https, or might not even have wget or curl installed at all.\nhttps://github.com/just-containers/s6-overlay#usage",
    "How to create a Docker container of an AngularJS app?": "First of all, follow this best practice guide to build your angular app structure. The index.html should be placed in the root folder. I am not sure if the following steps will work, if it's not there.\nTo use a nginx, you can follow this small tutorial: Dockerized Angular app with nginx\n1.Create a Dockerfile in the root folder of your app (next to your index.html)\nFROM nginx\nCOPY ./ /usr/share/nginx/html\nEXPOSE 80\n2.Run docker build -t my-angular-app . in the folder of your Dockerfile.\n3.docker run -p 80:80 -d my-angular-app and then you can access your app http://localhost",
    "Customize ONBUILD environment in a dockerfile": "I was having the same problem and I managed to fix it adding this to the Dockerfile:\nCOPY pip.conf pip.conf\nENV PIP_CONFIG_FILE pip.conf\nRUN pip install <my_package_name>\nThe pip.conf file has the next structure:\n[global]\ntimeout = 60\nindex-url = https://pypi.org/simple\ntrusted-host = pypi.org\n               <my_server_page>\nextra-index-url = https://xxxx:yyyy@<my_server_page>:<package_location>\nThis is the only way I found for Docker to find the package from the pypi server. I hope this solution is general and helps other people having this problem.",
    "How to run .NET Core 2 application in Docker on Linux as non-root": "In linux, binding to a port less than 1024 requires the user to be superuser. You can just use the default port 5000 and then publish to port 80 on your host (if you don't have any reverse proxy).",
    "How can I prevent Docker from removing intermediate containers when executing RUN command?": "docker build --rm=false\nRemove intermediate containers after a successful build (default true)",
    "Why do I get \"unzip: short read\" when I try to build an image from Dockerfile?": "Somehow, curl on alpine linux distro can't set cookie headers correctly while downloading jce zip file. It seems it downloads a zip file but in fact it is an html error page. If you view the file you can see that it is an html file. I've used wget instead of curl and it successfully downloaded file. Then unzip operation worked as expected.\nFROM openjdk:8-jdk-alpine\nRUN  apk update && apk upgrade && apk add netcat-openbsd\nRUN mkdir -p /usr/local/configserver\nRUN cd /tmp/ && \\\n    wget 'http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip' --header \"Cookie: oraclelicense=accept-securebackup-cookie\" && \\\n    unzip jce_policy-8.zip && \\\n    rm jce_policy-8.zip && \\\n    yes |cp -v /tmp/UnlimitedJCEPolicyJDK8/*.jar /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/\nADD @project.build.finalName@.jar /usr/local/configserver/\nADD run.sh run.sh\nRUN chmod +x run.sh\nCMD ./run.sh",
    "How can I reduce the size of Docker images": "Using the official node alpine image as a base image, as most here suggested, is a simple solution to reduce the overall size of the image, because even the base alpine image is a lot smaller compared to the base ubuntu image.\nA Dockerfile could look like this:\nFROM node:alpine\n\nARG ENVIRONMENT\nARG PORT\n\nRUN mkdir /consumer_portal \\\n    && npm install -g express path\n\nCOPY . /consumer_portal\nWORKDIR /consumer_portal\n\nRUN npm cache clean \\\n    && npm install\n\nEXPOSE $PORT\n\nCMD [ \"node\",  \"server.js\" ]\nIt's nearly the same and should work as expected. Most of the commands from your ubuntu image can be applied the same way in the alpine image.\nWhen I add mock-data to be create a similar project as you might have, results in an ubuntu image with a size of 491 MB and the alpine version is only 62.5 MB big:\nREPOSITORY   TAG       IMAGE ID        CREATED          SIZE\nalpinefoo    latest    8ca6f338475e    5 minutes ago    62.5MB\nubuntufoo    latest    38620a1bd5a6    6 minutes ago    491MB",
    "Docker-Compose: How to depends_on a container on another network? I am getting an error saying container 'undefined' even though networks are linked": "Depends_on only works on services within the same compose file, so to do what you want, you would need to use something like wait-for-it.sh. Take a look here for more information: https://docs.docker.com/compose/startup-order/\nSomething like this may work for you or you can create a custom wait-for-it script as well:\nservices:\n  frontend:\n    container_name: frontend\n    restart: unless-stopped\n    stdin_open: true\n    build:\n      context: ../realm-frontend\n    volumes:\n      - static:/realm-frontend/build\n    command: [\"./wait-for-it.sh\", \"wordpress:80\", \"--\", \"yourfrontendcmd\"]\n    networks:\n      - cms_wpsite",
    "what does VOLUME inside Dockerfile do": "Docker Volumes:\nVolumes decouple the life of the data being stored in them from the life of the container that created them. This makes it so you can docker rm my_container and your data will not be removed.\nA volume can be created in two ways:\nSpecifying VOLUME /some/dir in a Dockerfile\nSpecying it as part of your run command as docker run -v /some/dir\nEither way, these two things do exactly the same thing. It tells Docker to create a directory on the host, within the docker root path (by default /var/lib/docker), and mount it to the path you've specified (/some/dir above). When you remove the container using this volume, the volume itself continues to live on.\nIf the path specified does not exist within the container, a directory will be automatically created.\nYou can tell docker to remove a volume along with the container:\ndocker rm -v my_container\nSometimes you've already got a directory on your host that you want to use in the container, so the CLI has an extra option for specifying this:\ndocker run -v /host/path:/some/path ...\nThis tells docker to use the specified host path specifically, instead of creating one itself within the docker root, and mount that to the specified path within the container (/some/path above).\nNote, that this can also be a file instead of a directory. This is commonly referred to as a bind-mount within docker terminology (though technically speaking, all volumes are bind-mounts in the sense of what is actually happening). If the path on the host does not exist, a directory will be automatically be created at the given path.\nFrom the docker documentation:\nVOLUME [\"/data\"]\nThe VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. The value can be a JSON array, VOLUME [\"/var/log/\"], or a plain string with multiple arguments, such as VOLUME /var/log or VOLUME /var/log /var/db. For more information/examples and mounting instructions via the Docker client, refer to Share Directories via Volumes documentation.\nThe docker run command initializes the newly created volume with any data that exists at the specified location within the base image. For example, consider the following Dockerfile snippet:\nFROM ubuntu\nRUN mkdir /myvol\nRUN echo \"hello world\" > /myvol/greeting\nVOLUME /myvol\nThis Dockerfile results in an image that causes docker run to create a new mount point at /myvol and copy the greeting file into the newly created volume.\nAnswer:\nSo in the above case , the VOLUME [\"/var/lib/bootstrap\"] instruction is persisting the data by creating a volume in /var/lib/docker on the host and mount it on /var/lib/bootstrap in the container.\nNotes about specifying volumes\nKeep the following things in mind about volumes in the Dockerfile.\nVolumes on Windows-based containers: When using Windows-based containers, the destination of a volume inside the container must be one of:\na non-existing or empty directory\na drive other than C:\nChanging the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\nJSON formatting: The list is parsed as a JSON array. You must enclose words with double quotes (\")rather than single quotes (').\nThe host directory is declared at container run-time: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can\u2019t be guaranteed to be available on all hosts. For this reason, you can\u2019t mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.",
    "Can't load prometheus.yml config file with docker (prom/prometheus)": "By \u201cthe file already exists\u201d, do you mean that the file is on your host at /prometheus-data/prometheus.yml? If so, then you need to bind mount it into your container for it to be accessible to Prometheus.\nsudo docker run -p 9090:9090 -v /prometheus-data/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus\nIt's covered under Volumes & bind-mount in the documentation.",
    "Remove sensitive information from environment variables in postgres docker container": "use args without values to build the image in your Dockerfile:\nARG PASSWORD \nand build it using\nexport PASSWORD=\"MYPASS\" && docker build ...\nin this way the ARG is not there when running the container\nhere is a complete example:\ndockerfile:\nFROM postgres:10.0-alpine\n\nARG my_user\nARG my_pass\nCompose:\nversion: \"3\"\nservices:\n       db:\n         build:\n           context: .\n           args:\n            - my_user\n            - my_pass       \n         environment:\n           - POSTGRES_USER=${my_user}\n           - POSTGRES_PASSWORD=${my_pass}\n           - POSTGRES_DB=db\nrun it:\nexport my_user=test && export my_pass=test1cd && docker-compose up -d --build\nnow if you login to the container and try echo $my_pass you get an empty string\nresult :\ndocker exec -ti 3b631d907153 bash\n\nbash-4.3# psql -U test db\npsql (10.0)\nType \"help\" for help.\n\ndb=#",
    "Adding large files to docker during build": "These files might change once in a while, and I don't mind to rebuild my container and re-deploy it when it happens.\nThen a source control is not the best fit for such artifact.\nA binary artifact storage service, like Nexus or Artifactory (which both have free editions, and have their own docker image if you need one) is more suited to this task.\nFrom there, your Dockerfile can fetch from Nexus/Artifactory your large file(s).\nSee here for proper caching and cache invalidation.",
    "How do I check if Oracle is up in Docker?": "Using docker-compose.yml and Official Oracle docker images you can use checkDBStatus.sh script as a healthcheck. The script returns non-0 while db is in ORA-01033 state. Below is an example. Notice the combination of db's service healthcheck and tomcat's depends_on with service_healthy condition:\n  tomcat:\n    image: \"tomcat:9.0\"\n    depends_on:\n      oracle-db:\n        condition: service_healthy\n    links:\n      - oracle-db\nservices:\n  oracle-db:\n    build:\n      context: src/main/docker/oracle_db\n      dockerfile: Dockerfile.xe\n    mem_reservation: 2g\n    environment:\n      - ORACLE_PWD=oracle\n    volumes:\n      - oracle-data:/opt/oracle/oradata\n    healthcheck:\n      test: [ \"CMD\", \"/opt/oracle/checkDBStatus.sh\"]\n      interval: 2s\n\nvolumes:\n  oracle-data:",
    "Docker, Copying image, error - ERROR: failed to solve: failed to compute cache key: failed to calculate checksum": "Double check your .dockerignore file, if it exists make sure that the file mentioned in the error message is not present in this file.",
    "How to get back to shell in nodejs:latest docker image?": "In order to overwrite the entry point of the docker image you're using, you will need to use the --entrypoint flag in the run command.\ndocker run -it --entrypoint bash node:latest\nFor better understanding on how to work with already running docker container you can refer to the following question",
    "Docker exec quoting variables": "Ok, I found a way to do it, all you need to do is evaluate command with bash\ndocker exec -it <container id> bash -c 'echo something-${CLI}'\nreturns something-/usr/local/bin/myprogram\nIf the CLI environment variable is not already set in the container, you can also pass it in such as:\ndocker exec -it -e CLI=/usr/local/bin/myprogram <container id> bash -c 'echo something-${CLI}'\nSee the help file:\n docker exec --help\n\n Usage: docker exec [OPTIONS] CONTAINER COMMAND [ARG...]\n\n Run a command in a running container\n\nOptions:\n-d, --detach               Detached mode: run command in the background\n-e, --env list             Set environment variables\n....",
    "Docker build ADD vs RUN curl": "ADD is executed in docker host.\nThe ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the image at the path <dest>.\nRUN is executed inside your container.\nThe RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.\nSpecifically the command RUN curl -o file.txt http://X.X.X.X/path/to/file/file.txt executes curl that must have already been installed in the image we are using. If the curl command has not been installed (and is not present in the base image) the entire RUN command fails. Instead the command ADD url can be performed even without having installed curl (or analogues) inside the container just because it is executed by the host (it uses the Go libraries with which it is written docker) during the creation of our image.\nIs http://X.X.X.X/path/to/file/file.txt accessible outside of your docker container?\nEdit: as confirmed by the author of the question:\nMy docker host lives behind a firewall that has a proxy set in the /etc/default/docker file. So while I wanted to grab a file internal to the network I'm on, the proxy caused it to look outside the network.",
    "Docker build error, archive/tar: sockets not supported": "It looks like your Dockerfile is probably in /home/shim/?\nWhen you do docker build ., docker will tar up the contents of the current directory and send it to the docker daemon. It looks like some of the files in /home/shim/.ServiceHub are actually sockets, so this operation fails.\nBest practice is to have the Dockerfile in its own, isolated, directory to avoid stuff like this.\nAlso, I suggest having a read through dockerfile_best-practices, in particular the bit about RUN & apt-get",
    "apt-key command works on shell but fails on Dockerfile": "Solved by adding --no-tty on the apt-key adv command.\nAny idea however why this was happening?",
    "How can I cache Maven dependencies and plugins in a Docker Multi Stage Build Layer?": "I came across the same question. I found out it's due to differences between Maven targets (e.g. dependency:resolve vs dependency:resolve-plugin). Basically, dependency:resolve is for application libraries, dependency:resolve-plugin is for plugin libraries. Hence, libraries are downloaded in both RUN steps.\ndependency:resolve tells Maven to resolve all dependencies and displays the version. JAVA 9 NOTE: will display the module name when running with Java 9.\ndependency:resolve-plugins tells Maven to resolve plugins and their dependencies.\nhttps://maven.apache.org/plugins/maven-dependency-plugin/index.html\nEven with dependency:resolve-plugins, Maven will not download all required libraries as package is a built-in target and requires additional libraries which dependency:resolve-plugin won't know to resolve in the first RUN. I also tried dependency:go-offline without success.\nOne solution is to run your build targets before and after adding your code to the build image. This will pull all the plugin dependencies into the lower layer allowing them to be re-used.\nApplying this solution to your example above is as follows:\nFROM maven:3-jdk-8 as mvnbuild\nRUN mkdir -p /opt/workspace\nWORKDIR /opt/workspace\nCOPY pom.xml .\nRUN mvn -B -s /usr/share/maven/ref/settings-docker.xml dependency:resolve-plugins dependency:resolve clean package\nCOPY . .\nRUN mvn -B -s /usr/share/maven/ref/settings-docker.xml clean package\n\nFROM openjdk:8-jre-alpine",
    "error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory": "Using openssl as a dependency like\nopenssl = { version = \"0.10.59\", features = [\"vendored\"] }\ndid fix it for me",
    "Replacing character in dockerfile variable - error: missing ':' in substitution": "The ${parameter/pattern/string} syntax is actually a Bash feature (cf. shell parameter expansion), not a POSIX feature.\nAccording to the official documentation, the Dockerfile directives only supports:\n$var\n${var}\n${var:-word} \u2192 if var is not set then word is the result;\n${var:+word} \u2192 if var is set then word is the result, otherwise the empty string\nWorkaround 1\nSo the problem does not have a \"direct\" solution, but if the variable you would like to substitute will be used, in the end, in some shell command (in a RUN, ENTRYPOINT or CMD directive), you could just as well keep the initial value as is (with no substitution), then substitute it later on?\nI mean for example, the following Dockerfile:\nFROM debian\n\nARG ABC_VERSION=1.2.3\nENV SOME_OTHER_VARIABLE=/app/${ABC_VERSION}\n\nWORKDIR /app\n\nRUN /bin/bash -c 'touch \"${SOME_OTHER_VARIABLE//./_}\"'\n\n# RUN touch \"${SOME_OTHER_VARIABLE//./_}\"\n# would raise /bin/sh: 1: Bad substitution\n\nCMD [\"/bin/bash\", \"-c\", \"ls -hal \\\"${SOME_OTHER_VARIABLE//./_}\\\"\"]\nAs an aside:\nI replaced ARG SOME_OTHER_VARIABLE with ENV SOME_OTHER_VARIABLE just to be able to use it from CMD.\nIt can be recalled that ENTRYPOINT and CMD directives should rather be written in exec form \u2212 CMD [\"\u2026\", \"\u2026\"] \u2212 rather in shell form (see e.g. that question: CMD doesn't run after ENTRYPOINT in Dockerfile).\nWorkaround 2\nOr as an alternative workaround, you may want to split your version number in major, minor, patch, to write something like this?\nARG MAJOR=1\nARG MINOR=2\nARG PATCH=3\nARG ABC_VERSION=$MAJOR.$MINOR.$PATCH\nARG SOME_OTHER_VARIABLE=/dir_name/abc_${MAJOR}_${MINOR}_${PATCH}\n\u2026\nA more concise syntax for workaround 1\nFollowing the OP's edit, I guess one concern is the relative verbosity of this line that I mentioned in the \"workaround 1\":\n\u2026\nRUN /bin/bash -c 'touch \"${SOME_OTHER_VARIABLE//./_}\"'\nTo alleviate this, Docker allows one to replace the implied shell (by default sh) with Bash, which does support the shell parameter expansion you are interested in. The key point is the following directive that has to be written before the RUN command (and which was precisely part of the Dockerfile the OP mentioned):\nSHELL [\"/bin/bash\", \"-c\"]\nThus, the Dockerfile becomes:\n\u2026    \nARG ABC_VERSION=1.2.3\n\nSHELL [\"/bin/bash\", \"-c\"]\nRUN touch \"/dir_name/abc_${ABC_VERSION//./_}\" \\\n  && ls -hal \"/dir_name/abc_${ABC_VERSION//./_}\"\nor taking advantage of some temporary environment variable:\n\u2026    \nARG ABC_VERSION=1.2.3\n\nSHELL [\"/bin/bash\", \"-c\"]\nRUN export SOME_OTHER_VARIABLE=\"/dir_name/abc_${ABC_VERSION//./_}\" \\\n  && touch \"$SOME_OTHER_VARIABLE\" \\\n  && ls -hal \"$SOME_OTHER_VARIABLE\"",
    "Specify dockerignore from command line": "2018: No, docker build does not offer an alternative to the .dockerignore file.\nThat is why I usually keep a symbolic link .dockerignore pointing to the actual .dockerignore_official, except for certain case, where I switch the symlink to .dockerignore_module.\nThis is a workaround, but that allows me to version the different version of dockerignore I might need, and choose between the two.\nUpdate April 2019: as mentioned by Alba Mendez in the comments, PR 901 should help:\ndockerfile: add dockerignore override support\nFrontend will first check for <path/to/Dockerfile>.dockerignore and, if it is found, it will be used instead of the root .dockerignore.\nSee moby/buildkit commit b9db1d2.\nIt is in Docker v19.03.0 beta1, and Alba has posted an example here:\nYou need to enable Buildkit mode to use:\n$ export DOCKER_BUILDKIT=1\n\n$ echo \"FROM ubuntu \\n COPY . tmp/\" > Dockerfile\n$ cp Dockerfile Dockerfile2\n$ touch foo bar\n$ echo \"foo\" > .dockerignore\n$ echo \"bar\" > Dockerfile2.dockerignore\n\n$ docker build -t container1 -f Dockerfile .\n$ docker build -t container2 -f Dockerfile2 .\n$ docker run container1 ls tmp\nDockerfile\nDockerfile2\nDockerfile2.dockerignore\nbar\n\n$ docker run container2 ls tmp\nDockerfile\nDockerfile2\nDockerfile2.dockerignore\nfoo\nUpdate August 2019: this is now in Docker 19.03, with the following comment from T\u00f5nis Tiigi:\n#12886 (comment) allows setting a dockerignore file per Dockerfile if the repository contains many. (Note that this was the exact description for the initial issue)\nBuildKit automatically ignores files that are not used, automatically removing the problem where different sets of files needed to ignore each other.\nThe cases where same Dockerfile uses different sets of files for different \"modes\" of build (eg. dev vs prod) can be achieved with multi-stage builds and defining the mode changes with build arguments.\nNote: issue 37129 \"add support for multiple (named) build-contexts\" reports in May 2023 it is now supported with:\ndocker build (23+)/docker buildx build\nCompose\ndocker buildx bake\nSee also \"Build docker image using different directory contexts\"",
    "How to install nvm in a Dockerfile?": "I made the following changes to your Dockerfile to make it work:\nFirst, replace...\nRUN sh /root/.nvm/install.sh;\n...with:\nRUN bash /root/.nvm/install.sh;\nWhy? On Redhat-based systems, /bin/sh is a symlink to /bin/bash. But on Ubuntu, /bin/sh is a symlink to /bin/dash. And this is what happens with dash:\nroot@52d54205a137:/# bash -c '[ 1 == 1 ] && echo yes!'\nyes!\nroot@52d54205a137:/# dash -c '[ 1 == 1 ] && echo yes!'\ndash: 1: [: 1: unexpected operator\nSecond, replace...\nRUN nvm ls-remote;\n...with:\nRUN bash -i -c 'nvm ls-remote';\nWhy? Because, the default .bashrc for a user in Ubuntu (almost at the top) contains:\n# If not running interactively, don't do anything\n[ -z \"$PS1\" ] && return\nAnd the source-ing of nvm's scripts takes place at the bottom. So we need to make sure that bash is invoked interactively by passing the argument -i.\nThird, you could skip the following lines in your Dockerfile:\nRUN export NVM_DIR=\"$HOME/.nvm\";\nRUN echo \"[[ -s $HOME/.nvm/nvm.sh ]] && . $HOME/.nvm/nvm.sh\" >> $HOME/.bashrc;\nWhy? Because bash /root/.nvm/install.sh; will automatically do it for you:\n[fedora@myhost ~]$ sudo docker run --rm -it 2a283d6e2173 tail -2 /root/.bashrc\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"  # This loads nvm",
    "Docker ADD doesn't extract file": "This is documented:\nIf is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory.\nNotice the emphasis on \"local\".\nThis was actually changed and reverted at some point, so it's unlikely this will ever change again.",
    "Managing Dockerfile dynamically for different tenants in CI/CD pipeline implementation": "Quoting from 12 Factor - Config\nAn app\u2019s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). This includes:\nResource handles to the database, Memcached, and other backing services\nCredentials to external services such as Amazon S3 or Twitter\nPer-deploy values such as the canonical hostname for the deploy\nYou should not build separate docker images for each tenant as the binary should be the same and any runtime configurations should be injected through the environment.\nThere are different options to inject runtime configuration\nEnvironment variables\nInstead of hardcoding the profile in the entrypoint add a environment variable\nENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=$TENANT_PROFILE\" , \"TestProject.war\"] \nThen inject the environment variable from the kubernetes deployment configuration Refer https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/\nMount the profile configuration as a config and refer it\nYour entrypoint will look like\nENTRYPOINT [\"java\", \"-jar\", --spring.config.location=\"file:/path/to/tenantconfig.yaml\" , \"TestProject.war\"]  Then mount the required config file as a kubernetes config.\nEither way externalize the runtime configuration from the docker image and inject it through the deployment configuration as a environment variable or a config.",
    "How to install ODBC Driver 17 in a docker image based on windows server core?": "So I ran the MSI manually in the container with logging enabled. It turns out that the failure was occurring due to missing VC++ redistributable.\nSo, I updated the Dockerfile by adding a line to copy and install vc_redist.x64.exe which fixed the issue for me.\nSnippet from the Dockerfile that solved the problem for me.\nFROM mcr.microsoft.com/dotnet/framework/aspnet:4.7.2\nCOPY vc_redist.x64.exe c:/ \\\n     msodbcsql_17.3.1.1_x64.msi c:/\nRUN c:\\\\vc_redist.x64.exe /install /passive /norestart \nRUN msiexec.exe /i C:\\\\msodbcsql_17.3.1.1_x64.msi /norestart /qn /quiet /passive IACCEPTMSODBCSQLLICENSETERMS=YES \n\n...\nJust posting this answer here in case someone else stumbles upon the same issue.",
    "docker-compose rebuild after each Gemfile update?": "First, about the workaround with docker exec. It's not a good approach to modify container state. What if you need to run one more instance of app container? There will be no changes made by exec. You'll have to install gems there again, or rebuild image. It's not a rare case when you need to run multiple containers. For example, you use docker-compose up to run dev environment, and docker-compose run --rm web bash in the near terminal to run shell in the second app container and use it to run tests, migrations, generators or use rails console without stopping containers launched by docker-compose up.\nNow about the solution. When you run docker-compose run --rm app bundle install, you create the new container, install new gems into it (this operation updates Gemfile.lock, and you see this changes, because your project dir is mounted to container), and exit. Container gets removed because of --rm flag. Changes made in container don't affect image.\nTo avoid image rebuilding on each gem install, you can add a service to store gems. Here is modified docker-compose.yml, based on the one from docs.\nversion: '3'\nservices:\n  db:\n    image: postgres\n  web:\n    build: .\n    command: bash -c \"bundle install && bundle exec rails s -p 3000 -b 0.0.0.0\"\n    volumes:\n      - .:/myapp\n      - bundle_cache:/bundle_cache\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - db\n    environment:\n      - BUNDLE_PATH=/bundle_cache\n  bundle_cache:\n    image: busybox\n    volumes:\n      - bundle_cache:/bundle_cache\n\nvolumes:\n  bundle_cache:\nWhen you use container that stores gems for all your app containers, you don't need to rebuild image because of adding new gems at all until you run docker-compose down that deletes all your containers (it's really rarely needed) or until you delete bundle_cache container yourself. And of course you don't need to use bundle exec for each container where you want to install new gems. So it's much easier and time-saving.\nThis, however, requires additional bundle install after initial docker-compose build, because on the creation and first mounting /bundle_cache to the container with application it will be empty. But after that your gems will be stored in the separate container, and this storage will be available for each started application container.",
    "Why does Dockerfile RUN print echo options?": "Try running:\nRUN bash -c 'echo -e ...'\nSource",
    "What's the reason for the 42 layer limit in Docker? [closed]": "I'm starting to suspect that there isn't any such a hard limit.\nCreate the following python script, and name it \"makeDockerfile.py\"\nwith open(\"Dockerfile\", \"w\") as file:\n    file.write(\"from alpine:3.8\\n\")\n    for i in range(0, 201):\n        file.write(\"run echo {i}\\n\".format(i=i))\nthen run python makeDockerfile.py && docker build --tag manylayer . && docker run -it manylayer /bin/sh You'll see that you are running a working container with > 200 layers.\n(note, this was tested with linux containers on linux)\nNote that this doesn't mean that this many layers are necessarily SUPPORTED, just that they are possible in some cases.\nIn fact, I've seen containers fail with far fewer than 42 layers, and removing any arbitrary layer seems to fix it. (see https://github.com/docker/for-win/issues/676#issuecomment-462991673 )\nEDIT:\nthaJeztah, maintainer of Docker, has this to say about it:\nThe \"42 layer limit\" on aufs was on older versions of aufs, but should no longer be the case.\nHowever, the 127 layer limit is still there. This is due to a restriction of Linux not accepting more than X arguments to a syscall that's used.\nAlthough this limit can be raised in modern kernels, it's not the default, so if we'd go beyond that maximum, an Image built on one machine may not be usable on another machine.\n( see https://github.com/docker/docker.github.io/issues/8230 )",
    "Many <none> images created after build a docker image": "Docker image is composed of layers:\ndocker history aario/centos\nEach row you see using command above is a separate layer. Also, with time, a number of \"orphaned\" layers will fill up your disk space. You can safely remove them with:\ndocker image prune",
    "Add private key to ssh-agent in docker file": "I spent several days going through the same issue. ssh-keygen -p ensured the passphrase was empty, but I needed to ssh-agent and ssh-add in my Dockerfile to be able to pull from a private repo. Several of my peers told me they were able to make it work; I would copy what they had and still be asked for a passphrase. Finally I came across this issue. After manually inputting in the rsa key line by line and seeing it succeed, I realized it was because I was building the image and passing in the key via a make target, and the Makefile was processing the newlines as whitespaces. Ultimately it was just a matter of updating how the key was being cat as an argument so that it ran as bash instead to preserve the newlines.\nHere was the build command inside my Makefile:\nmake container:    \n    docker build --rm \\\n    --build-arg ssh_prv_key=\"$$(cat ~/.ssh/id_rsa)\" \\\n    --squash -f Dockerfile -t $(DOCKER_IMAGE) .\nI will also note that I needed to include\necho \"StrictHostKeyChecking no\" >> /etc/ssh/ssh_config\nto one of my Dockerfile RUN commands as well",
    "cannot remove 'folder': Device or resource busy": "Another pretty much simple answer is following:\n1. Close all your terminal windows (bash, shell, etc...)\n2. Start a new terminal\n3. Execute your command again e.g.:\nrm -f -r ./folder\n4. done\nHopefully it helps others!",
    "How to name docker-compose files": "According to the -f documentation:\nIf you don\u2019t provide this flag on the command line, Compose [...] looks for a docker-compose.yml and a docker-compose.override.yml file.\nIf you don't want to use the -f flag you can use the default name docker-compose.yml and override it with docker-compose.override.yml.\nHowever, if you use -f, since you'll provide the filename, you can use whatever you want.\nI think a good way to name them depending on the environment could be docker-compose.{env}.yml and place them at the top level directory:\ndocker-compose.prod.yml\ndocker-compose.dev.yml\ndocker-compose.test.yml\ndocker-compose.staging.yml\nAnd you can use the default docker-compose.yml to define the base configuration that is common to all environments.",
    "Dockerfile ADD tar.gz does not extract on ubuntu VM with Docker": "This is a known issue with 17.06 and patched in 17.06.1. The documented behavior is to download the tgz but not unpack it when pulling from a remote URL. Automatically unpacking the tgz was an unexpected change in behavior in 17.06 that they reverted back to only downloading the tgz in 17.06.1.\nRelease notes for 17.06 (see the note at the top): https://github.com/docker/docker-ce/releases/tag/v17.06.0-ce\nRelease notes for 17.06.01: https://github.com/docker/docker-ce/releases/tag/v17.06.1-ce\nIssue: https://github.com/moby/moby/issues/33849\nPR of Fix: https://github.com/docker/docker-ce/pull/89\nEdit, the minimize the number of layers in your image, I'd recommend doing the download, unpack, and cleanup as a single RUN command in your Dockerfile. E.g. here are two different Dockerfiles:\n$ cat df.tgz-add\nFROM busybox:latest\nENV GO_VERSION 1.8\nWORKDIR /tmp\n\nADD https://storage.googleapis.com/golang/go$GO_VERSION.linux-amd64.tar.gz ./\nRUN tar -xzf go$GO_VERSION.linux-amd64.tar.gz \\\n && rm go$GO_VERSION.linux-amd64.tar.gz\n\nCMD ls -l .\n\n$ cat df.tgz-curl\nFROM busybox:latest\nENV GO_VERSION 1.8\nWORKDIR /tmp\n\nRUN wget -O go$GO_VERSION.linux-amd64.tar.gz https://storage.googleapis.com/golang/go$GO_VERSION.linux-amd64.tar.gz \\\n && tar -xzf go$GO_VERSION.linux-amd64.tar.gz \\\n && rm go$GO_VERSION.linux-amd64.tar.gz\n\nCMD ls -l .\nThe build output is truncated here...\n$ docker build -t test-tgz-add -f df.tgz-add .\n...\n\n$ docker build -t test-tgz-curl -f df.tgz-curl .\n...\nThey run identically:\n$ docker run -it --rm test-tgz-add\ntotal 4\ndrwxr-xr-x   11 root     root          4096 Aug 31 20:27 go\n\n$ docker run -it --rm test-tgz-curl\ntotal 4\ndrwxr-xr-x   11 root     root          4096 Aug 31 20:29 go\nHowever, doing a single RUN to download, build, and cleanup saves you the 80MB of download from your layer history:\n$ docker images | grep test-tgz\ntest-tgz-curl               latest                                     2776133659af        30 seconds ago      269MB\ntest-tgz-add                latest                                     d625455998ff        2 minutes ago       359MB",
    "Docker how to start container with defined nameservers in /etc/resolv.conf": "If you use docker-compose you can simple add your dns-server in docker-compose.yml\n  my-app:\n     build: my-app\n     dns:\n       - 10.20.20.1  # dns server 1\n       - 10.21.21.2  # dns server 2\n     dns_search: ibm-edv.ibmnet.int\nsee https://bitbucket.org/snippets/mountdiablo/9yKxG/docker-compose-reference-yaml-file-with",
    "How to determine the base image of a Docker image?": "For pulled images, I don't think there is a way to find the base image without seeing the actual dockerfile because when you pull an image, image manifest is downloaded only for the leaf layer. So the image id of the non-leaf layers in marked as <missing> in docker history and you wouldn't know the repo tags of those layers.\nIf the image is built on your machine but if you don't have the dockerfile, you can find the base image as follows:\ndocker history prints the image ids of the layers. Then you can get the repo tags using docker inspect. The base image used will usually be the last layer in the output of docker history.\neg:\n$ docker history t:1\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n10b4cce00fb8        3 days ago          /bin/sh -c #(nop)  CMD [\"flask\" \"run\"]          0B\n824987ef6cab        3 days ago          /bin/sh -c #(nop) COPY dir:1973b65388e92428e\u2026   406B\nd4b6f433a5df        3 days ago          /bin/sh -c pip install -r requirements.txt      4.98MB\n8827b3f01d00        3 days ago          /bin/sh -c #(nop) COPY file:98271fcaff00c6ef\u2026   0B\n65b8c98138e6        2 weeks ago         /bin/sh -c apk add --no-cache gcc musl-dev l\u2026   113MB\n01589531f46d        2 weeks ago         /bin/sh -c #(nop)  ENV FLASK_RUN_HOST=0.0.0.0   0B\n6c4640b8027a        2 weeks ago         /bin/sh -c #(nop)  ENV FLASK_APP=app.py         0B\nb4c8fc7f03d6        2 weeks ago         /bin/sh -c #(nop) WORKDIR /code                 0B\n16a54299f91e        2 weeks ago         /bin/sh -c #(nop)  CMD [\"python3\"]              0B\n$ docker inspect 16a54299f91e\n[\n    {\n        \"Id\": \"sha256:16a54299f91ef62cf19d7329645365fff3b7a3bff4dfcd8d62f46d0c9845b9c6\",\n        \"RepoTags\": [\n            \"python:3.7-alpine\"   ---> Base image used in FROM instruction. \nFollowing the output of docker history in reverse order, you can approximately recreate the Dockerfile.\nYou can also use the chenzj/dfimage image which runs a script which does the same to reconstruct the dockerfile.\nalias dfimage=\"docker run -v /var/run/docker.sock:/var/run/docker.sock --rm chenzj/dfimage\"\n\n$ dfimage 10b4cce00fb8\nFROM python:3.7-alpine\nWORKDIR /code\nENV FLASK_APP=app.py\nENV FLASK_RUN_HOST=0.0.0.0\nRUN /bin/sh -c apk add --no-cache gcc musl-dev linux-headers\nCOPY file:98271fcaff00c6efb6e58bd09ca726c29947e0cfe7031a8d98878cc01561fbbf in requirements.txt\nRUN /bin/sh -c pip install -r requirements.txt\nCOPY dir:1973b65388e92428e30f835a67ebc8c7b00ec648fbea0717af6d501af162186b in .\nCMD [\"flask\" \"run\"]\nbash-3.2$",
    "Copy files from Container to host using Dockerfile": "It is probably a bad idea to copy files from the container to the host during build. You should seriously consider your use case.\nHowever, it can be done and I will share with you a procedure because it is an opportunity for me to show off my Docker knowledge - not because I think you should do this. There are other ways to do this. My way is not better or worse - they are all kludges.\nModify your dockerd configuration as explained in https://success.docker.com/article/how-do-i-enable-the-remote-api-for-dockerd. Basically add -H tcp://0.0.0.0:2376. This is a very risky procedure b/c it opens you open to be rooted by anyone on your network. There are ways to mitigate this risk with authentication, but really the best way is to JUST DON'T DO IT.\nModify your Dockerfile:\nAdd a ARG DOCKER_HOST before the RUN blocks.\nIn the run blocks:\nInstall docker.\nAdd `export DOCKER_HOST=${DOCKER_HOST}.\nAdd docker container run --mount type=bind,source=/,destination=/srv/host alpine:3.4 ...\nDetermine the IP address of your host computer. Let us assume it is 10.10.20.100.\nModify your build command by adding --build-arg DOCKER_HOST=10.10.20.100.\nIn step 2.2.3 you have rooted the host computer and you can do whatever you want - including writing to any file.\nThis is a dumb idea, but it shows that since you can run docker from within a build, there really is not anything you can not do from inside a build. If you want to run a gui app from inside a build you can do it.",
    "Passing docker runtime environment variables in docker image": "I want to override the default environment variables being set below from whatever is passed in the docker run command mentioned in the end\nThat means overriding an image file (/usr/local/tomcat/conf/test.properties) when running the image as a container (docker run), not building the image (docker build and its --build-args option and its ARG Dockerfile entry).\nThat means you create locally a script file which:\nmodifies /usr/local/tomcat/conf/test.properties\ncalls catalina.sh run $@ (see also \"Store Bash script arguments $@ in a variable\" from \"Accessing bash command line args $@ vs $*\")\nThat is:\nmyscript.sh\n\n#!/bin/sh\necho dummy_url=$dummy_url >> /usr/local/tomcat/conf/test.properties\necho database=$database >> /usr/local/tomcat/conf/test.properties\nargs=(\"$@\")\ncatalina.sh run \"${args[@]}\"\nYou would modify your Dockerfile to COPY that script and call it:\nCOPY myscript.sh /usr/local/\n...\nENTRYPOINT [\"/usr/local/myscript.sh\"]\nThen, and only then, the -e options of docker run would work.",
    "Development dependencies in Dockerfile or separate Dockerfiles for production and testing": "No, you don't need to have different Dockerfiles and in fact you should avoid that.\nThe goal of docker is to ship your app in an immutable, well tested artifact (docker images) which is identical for production and test and even dev.\nWhy? Because if you build different artifacts for test and production how can you guarantee what you have already tested is working in production too? you can't because they are two different things.\nGiven all that, if by test you mean unit tests, then you can mount your source code inside docker container and run tests without building any docker images. And that's fine. Remember you can build image for tests but that terribly slow and makes development quiet difficult and slow which is not good at all. Then if your test passed you can build you app container safely.\nBut if you mean acceptance test that actually needs to run against your running application then you should create one image for your app (only one) and run tests in another container (mount test source code for example) and run tests against that container. This obviously means what your build for your app is different for npm installs for your tests.\nI hope this gives you some over view.",
    "docker run script which exports env variables": "There's no way to export a variable from a script to a child image. As a general rule, environment variables travel down, never up to a parent.\nENV will persist in the build environment and to child images and containers.\nDockerfile\nFROM busybox\nENV PLATFORM_HOME test\nRUN echo $PLATFORM_HOME\nDockerfile.child\nFROM me/platform\nRUN echo $PLATFORM_HOME\nCMD [\"sh\", \"-c\", \"echo $PLATFORM_HOME\"]\nBuild the parent\ndocker build -t me/platform .\nThen build the child:\n\u2192 docker build -f Dockerfile.child -t me/platform-test  .\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM me/platform\n ---> 539b52190af4\nStep 2/3 : RUN echo $PLATFORM_HOME\n ---> Using cache\n ---> 40e0bfa872ed\nStep 3/3 : CMD sh -c echo $PLATFORM_HOME\n ---> Using cache\n ---> 0c0e842f99fd\nSuccessfully built 0c0e842f99fd\nSuccessfully tagged me/platform-test:latest\nThen run\n\u2192 docker run --rm me/platform-test\ntest",
    "getting error /bin/sh: 1: source: not found [duplicate]": "From Docker docs:\nThe default shell for the shell form can be changed using the SHELL command.\nIn the shell form you can use a \\ (backslash) to continue a single RUN instruction onto the next line. For example, consider these two lines: RUN /bin/bash -c 'source $HOME/.bashrc ;\\ echo $HOME' Together they are equivalent to this single line: RUN /bin/bash -c 'source $HOME/.bashrc ; echo $HOME'\nNote: To use a different shell, other than \u2018/bin/sh\u2019, use the exec form passing in the desired shell. For example, RUN [\"/bin/bash\", \"-c\", \"echo hello\"]\nYou could try:\nRUN curl   https://raw.githubusercontent.com/creationix/nvm/v0.25.0/install.sh | bash\n# RUN source ~/.profile\nRUN [\"/bin/bash\", \"-c\", \"source ~/.profile\"]",
    "Why dockerize a service or application when you could install it? [closed]": "PROS:\nQuick local environment set up for your team - if you have all your services containerized. It will be a quick environment set up for your development team.\nHelps Avoid the \"It works on mine, but doesn't work on yours problem\" - a lot of our development issue usually stems from development environment setup. If you have your services containerized, a big chunk of this gets offloaded somewhere else.\nEasier deployments - while we all have different processes for deploying code, it goes to tell that having them containerized makes thing a hell lot easier.\nBetter Version Control - as you already know, can be tagged, which helps in VERSION CONTROL.\nEasier Rollbacks - since you have things version controlled, it goes to say that it is easier to rollback your code. Sometimes, by just simply pointing to your previously working version.\nEasy Multi-environment Setup - as most development teams do, we set up a local, integration, staging and production environment. This is done easier when services are containerized, and, most of the times, with just a switch of ENVIRONMENT VARIABLES.\nCommunity Support - we have a strong community of software engineers who continuously contribute great images that can be reused for developing great software. You can leverage that support. Why re-invent the wheel, right?\nMany more.. but there's a lot of great blogs out there you can read that from. =)\nCONS: I don't really see much cons with it but here's one I can think of.\nLearning Curve - yes, it does have some learning curve. But from what I have seen from my junior engineers, it doesn't take too much time to learn how to set it up. It usually takes you longer when you are figuring out how to containerized.\nSOME CONCERNS:\nData Persistence - some engineers are having concerns with data persistence. You can simply fix this by mounting a volume to your container. If you want to use your own database installation, you can simply switch your HOST, DB_NAME, USERNAME and PASSWORD with the one you have in your localhost:5432 and all should be fine.\nI hope this helps!",
    "Is it possible to to run a target build stage in docker without running all the previous build stages": "Set DOCKER_BUILDKIT=1 environment variable to use buildkit like this:\nDOCKER_BUILDKIT=1 docker build -t build-stage-tag --target build -<<EOF\nFROM alpine as base\nRUN echo \"running BASE commands\"\n\nFROM base AS test\nRUN echo \"running TEST commands\"\n\nFROM base AS build\nRUN echo \"running BUILD commands\"\nEOF\noutput:\n[+] Building 4.4s (7/7) FINISHED\n => [internal] load .dockerignore                                                                                                                               0.5s\n => => transferring context: 2B                                                                                                                                 0.0s\n => [internal] load build definition from Dockerfile                                                                                                            0.3s\n => => transferring dockerfile: 204B                                                                                                                            0.0s\n => [internal] load metadata for docker.io/library/alpine:latest                                                                                                0.0s\n => [base 1/2] FROM docker.io/library/alpine                                                                                                                    0.1s\n => => resolve docker.io/library/alpine:latest                                                                                                                  0.0s\n => [base 2/2] RUN echo \"running BASE commands\"                                                                                                                 1.4s\n => [build 1/1] RUN echo \"running BUILD commands\"                                                                                                               1.5s\n => exporting to image                                                                                                                                          0.7s\n => => exporting layers                                                                                                                                         0.6s\n => => writing image sha256:c6958c8bb64b1c6d5a975d8fa4b68c713ee5b374ba9a9fa00f8a0b9b5b314d5e                                                                    0.0s\n => => naming to docker.io/library/build-stage-tag                                                                                                              0.0s",
    "What does the DOCKER_TLS_VERIFY and DOCKER_CERT_PATH variable do?": "As mentioned in the README:\nBy default, boot2docker runs docker with TLS enabled. It auto-generates certificates and stores them in /home/docker/.docker inside the VM.\nThe boot2docker up command will copy them to ~/.boot2docker/certs on the host machine once the VM has started, and output the correct values for the DOCKER_CERT_PATH and DOCKER_TLS_VERIFY environment variables.\neval \"$(boot2docker shellinit)\" will also set them correctly.\nWe strongly recommend against running Boot2Docker with an unencrypted Docker socket for security reasons, but if you have tools that cannot be easily switched, you can disable it by adding DOCKER_TLS=no to your /var/lib/boot2docker/profile file.\nIn a more dynamic environment, where the boot2docker ip can change, see issue 944.",
    "Docker Container Listening on http://[::]:80": "As they've mentioned it seems your container is running on port 80. So for whatever reason that's the port being exposed. Maybe the EXPOSE $PORT is not returning 8081 as you expect?\nWhen you run the container, unless you specify where to map it, it will only be available at the container's IP at the exposed port (80 in your case). Find out this container Ip easily by running docker inspect <container_id>\nTest your image by doing something like docker run -p 8080:80 yourimage. You'll see that in addition to the port 80 that the image exposes, it is being mapped to your local port 8080 so that http://localhost:8080 should be reachable.\nSee this in case it helps you",
    "Docker build sometimes fail on \"file not found or excluded by .dockerignore\" for a nested ignored file": "Clean your system.\ndocker system prune -a\nNote this will remove also all images not used in a container.",
    "Sending build context to Docker daemon (it doesn't stop)": "mv your Dockerfile to an empty folder, and build it. When build a docker image, docker will \"use\" all files in current folder as its \"context\". You can also create a .dockerignore file to exclude files or directories like .gitignore.",
    "How to use the official docker elasticsearch container?": "I recommend using docker-compose (which makes lot of things much easier) with following configuration.\nConfiguration (for development)\nConfiguration starts 3 services: elastic itself and extra utilities for development like kibana and head plugin (these could be omitted, if you don't need them).\nIn the same directory you will need three files:\ndocker-compose.yml\nelasticsearch.yml\nkibana.yml\nWith following contents:\ndocker-compose.yml\nversion: '2'\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:5.4.0\n    container_name: elasticsearch_540\n    environment:\n      - http.host=0.0.0.0\n      - transport.host=0.0.0.0\n      - \"ES_JAVA_OPTS=-Xms1g -Xmx1g\"\n    volumes:\n      - esdata:/usr/share/elasticsearch/data\n      - ./elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n    ports:\n      - 9200:9200\n      - 9300:9300\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n      nofile:\n        soft: 65536\n        hard: 65536\n    mem_limit: 2g\n    cap_add:\n      - IPC_LOCK\n  kibana:\n    image: docker.elastic.co/kibana/kibana:5.4.0\n    container_name: kibana_540\n    environment:\n      - SERVER_HOST=0.0.0.0\n    volumes:\n      - ./kibana.yml:/usr/share/kibana/config/kibana.yml\n    ports:\n      - 5601:5601\n  headPlugin:\n    image: mobz/elasticsearch-head:5\n    container_name: head_540\n    ports:\n      - 9100:9100\n\nvolumes:\n  esdata:\n    driver: local\nelasticsearch.yml\ncluster.name: \"chimeo-docker-cluster\"\nnode.name: \"chimeo-docker-single-node\"\nnetwork.host: 0.0.0.0\n\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nhttp.cors.allow-headers: \"Authorization\"\nkibana.yml\nserver.name: kibana\nserver.host: \"0\"\nelasticsearch.url: http://elasticsearch:9200\nelasticsearch.username: elastic\nelasticsearch.password: changeme\nxpack.monitoring.ui.container.elasticsearch.enabled: true\nRunning\nWith above three files in the same directory and that directory set as current working directory you do (could require sudo, depends how you have your docker-compose set up):\ndocker-compose up\nIt will start up and you will see logs from three different services: elasticsearch_540, kibana_540 and head_540.\nAfter initial start up you will have your elastic cluster available for http under 9200 and for tcp under 9300. Validate with following curl if the cluster started up:\ncurl -u elastic:changeme http://localhost:9200/_cat/health \nThen you can view and play with your cluster using either kibana (with credentials elastic / changeme):\nhttp://localhost:5601/\nor head plugin:\nhttp://localhost:9100/?base_uri=http://localhost:9200&auth_user=elastic&auth_password=changeme",
    "docker build error Gem::Ext::BuildError: ERROR: Failed to build gem native extension for mimemagic-0.3.9": "Modify the Dockerfile to install the shared-mime-info package. E.g. on Debian-based images:\nRUN apt-get update && apt-get install -y shared-mime-info\nIf it still won't work, then you may need to update the mimemagic gem. On your host, update mimemagic in the Rails app's Gemfile/Gemfile.lock. You may need to install shared-mime-info first: If the host is macOS, you may need to run brew install shared-mime-info; if the host is Ubuntu, you may need to run apt-get install shared-mime-info. Then run\nbundle update mimemagic\nIf your Dockerfile downloads the Rails app from a repo, push your changes to that repo first. Or, for testing, modify the Dockerfile to copy in the Rails app from the host instead.",
    "Using Docker for windows to volume-mount a windows drive into a Linux container": "If you're just trying to mount a windows path to a Linux based container, here's an example using the basic docker run command, and a Docker Compose example as well:\ndocker run -d --name qbittorrent -v '/mnt/f/Fetched Media/Unsorted:/downloads' -v '/mnt/f/Fetched Media/Blackhole:/blackhole' linuxserver/qbittorrent\nThis example shares the f:\\Fetched Media\\Unsorted and f:\\Fetched Media\\Blackhole folders on the Windows host to the container; and within the Linux container you'd see the files from those Windows folders in their respective Linux paths shown to the right of the colon(s).\ni.e. the f:\\Fetched Media\\Unsorted folder will be in the /downloads folder in the Linux container.\n*First though, make sure you've shared those Windows folders within the Docker Desktop settings area in the GUI.\nUpdate for WSL(2):\nYou don't need to specifically share the Windows folder paths; that's only needed when not using WSL.\nUpdate:\nThis seems to be a popular answer, so I thought I'd also include a Docker Compose version of the above example, for the sake of thoroughness (includes how to set a path as read-write (rw), or read-only (ro)):\nqbittorrent:\n  image: 'linuxserver/qbittorrent:latest'\n  volumes:\n    - '/mnt/f/Fetched Media/Unsorted:/downloads:rw'\n    - '/mnt/f/Fetched Media/Blackhole:/blackhole:rw'\n    - '/mnt/e/Logs/qbittorrent:/config/logs:rw'\n    - '/opt/some-local-folder/you-want/read-only:/some-folder-inside-container:ro'",
    "Cannot connect to MongoDB via node.js in Docker": "Try:\nmongodb.MongoClient.connect('mongodb://mongo:27017', ... );\nChange your docker-compose.yml:\nversion: \"2\"\n\nservices:\n\n  web:\n    build: .\n    volumes:\n      - ./:/app\n    ports:\n      - \"3000:3000\"\n    links:\n      - mongo\n\n  mongo:\n    image: mongo\n    ports:\n      - \"27017:27017\"\nAnd use some docker compose commands:\ndocker-compose down\ndocker-compose build\ndocker-compose up -d mongo\ndocker-compose up web",
    "Docker Conditional build image": "Just to put this in right context, it is now (since May 2017) possible to achieve this with pure docker since 17.05 (https://github.com/moby/moby/pull/31352)\nDockerfile should look like (yes, commands in this order):\nARG APP_VERSION\nARG GIT_VERSION\nFROM app:$APP_VERSION-$GIT_VERSION\nThen build is invoked with\ndocker build --build-arg APP_VERSION=1 --build-arg GIT_VERSION=c351dae2 .\nDocker will try to base the build on image app:1-c351dae2\nHelped me immensely to reduce logic around building images.",
    "Access env file variables in docker-compose file": "To get this right it is important to correctly understand the differences between the environment and env_file properties and the .env file:\nenvironment and env_file let you specify environment variables to be set in the container:\nwith environment you can specify the variables explicitly in the docker-compose.yml\nwith env_file you implicitly import all variables from that file\nmixing both on the same service is bad practice since it will easily lead to unexpected behavior\nthe .env file is loaded by docker-compose into the environment of docker-compose itself where it can be used\nto alter the behavior of docker-compose with respective CLI environment variables\nfor variable substitution in the docker-compose.yml\nSo your current configuration does not do what you probably think it does:\nenv_file: project/myproject/.env\nWill load that .env file into the environment of the container and not of docker-compose. Therefor Database_User won't be set for variable substitution:\nenvironment:\n  - POSTGRES_USER=${Database_User}\nThe solution here: remove env_file from your docker-compose.yml and place .env in the same directory you are starting docker-compose from. Alternatively you can specify an .env file by path with the --env-file flag:\ndocker-compose --env-file project/myproject/.env up",
    "Docker [for mac] file system became read-only which breaks almost all features of docker": "Go to your docker for mac Icon in the top right, click on it and then click Restart. After that Docker works as expected.\nThis seems to be an temporary issue since I cannot reproduce it after restarting docker. My guess is that I had an network communication breakdown while docker tried to download and install the packages in the Dockerfile.",
    "Multiple Dockerfiles failure at building images in docker-compose": "TypeError: You must specify a directory to build in path\nWhen you add build: ./Dockerfile-headless-chrome to your docker-compose.yml, you are setting the context folder, and not the Dockerfile file. The attribute you want is build -> dockerfile:\n  services:\n    headless-chrome:\n      build:\n        context: .\n        dockerfile: Dockerfile-alternate\n    dev-server:\n      build: .\nFor dev-server I am not setting anything because it will use the default (context: . and dockerfile: Dockerfile). Actually, you also don't need the context: . at the headless-chrome, because it is the default.\nDocker Compose reference - build\nEdited - Part 2\nThere is also a problem at the command. For it to be automated, you have to change your /bin/bash.\nCreate a file for all your tests (entrypoint.sh):\nnpm run selenium-docker\nnpm run uat\nAdd this file to your image (ADD) and run it on start the container (CMD).\n# Dockerfile\n\n... your commands\n\nADD entrypoint.sh\nCMD bash entrypoint.sh\nAnd it is ready. Build again docker-compose build (because you edited the Dockerfile) and run docker-compose up",
    "Build docker image fail : Exiting on user command": "Simply add -y to yum.\nExample:\nRUN yum -y install git ...",
    "Docker how to make python 3.8 as default": "Replacing the system python in this way is usually not a good idea (as it can break operating-system-level programs which depend on those executables) -- I go over that a little bit in this video I made \"why not global pip / virtualenv?\"\nA better way is to create a prefix and put that on the PATH earlier (this allows system executables to continue to work, but bare python / python3 / etc. will use your other executable)\nin the case of deadsnakes which it seems like you're using, something like this should work:\nFROM ubuntu:bionic\n\nRUN : \\\n    && apt-get update \\\n    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n        software-properties-common \\\n    && add-apt-repository -y ppa:deadsnakes \\\n    && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \\\n        python3.8-venv \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && :\n\nRUN python3.8 -m venv /venv\nENV PATH=/venv/bin:$PATH\nthe ENV line is the key here, that puts the virtualenv on the beginning of the path\n$ docker build -t test . \n...\n$ docker run --rm -ti test bash -c 'which python && python --version && which pip && pip --version'\n/venv/bin/python\nPython 3.8.5\n/venv/bin/pip\npip 20.1.1 from /venv/lib/python3.8/site-packages/pip (python 3.8)\ndisclaimer: I'm the maintainer of deadsnakes",
    "How to list all directories and files inside docker container?": "Simply use the exec command.\nNow because you run a windowsservercore based image use powershell command (and not /bin/bash which you can see on many examples for linux based images and which is not installed by default on a windowsservercore based image) so just do:\ndocker exec -it <container_id> powershell\nNow you should get an iteractive terminal and you can list your files with simply doing ls or dir\nBy the way, i found this question :\nExploring Docker container's file system\nIt contains a tons of answers and suggestions, maybe you could find other good solutions there (there is for example a very friendly CLI tool to exploring containers : https://github.com/wagoodman/dive)",
    "docker repository name component must match": "So this regular expression: [a-z0-9]+(?:[._-][a-z0-9]+)* doesn't include any upper case letters. So you should change your image name to devopsclient",
    "How to reduce my java/gradle docker image size?": "I am really confused about your image size. I have typical Spring Boot applications offering a REST service including an embedded servlet container in less than 200MB! It looks like your project dependencies can and should be optimised.\nDocker Image\nThe openjdk:8 (243MB compressed) can be replaced by one with a reduced Alpine unix image like openjdk:8-jdk-alpine (52MB) as a base image but if you don't need compiler capabilities (e.g. don't use JSPs) you may also go for openjdk:8-jre-alpine (42MB) which includes the runtime only, have a look into Docker Hub. I use that for Spring Boot based REST services working great.\nJava Dependencies\nThe Java dependencies needed for compile and runtime have to be included but you may have unused dependencies included:\ncheck your dependencies, are the current compile/runtime dependencies really used or maybe can be removed or moved to test, see Gradle Java Plugin\nsome dependencies have a lot of transitive dependencies (display using gradle dependencies), check out for unnecessary ones and exclude them if unused, see Gradle Dependency Management. Be sure to do integration tests before applying finally, some transitive dependencies are not well documented but may be essential!",
    "Docker how to ADD a file without committing it to an image?": "According to the documentation, if you pass an archive file from the local filesystem (not a URL) to ADD in the Dockerfile (with a destination path, not a path + filename), it will uncompress the file into the directory given.\nIf <src> is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed. When a directory is copied or unpacked, it has the same behavior as tar -x: the result is the union of:\n1) Whatever existed at the destination path and 2) The contents of the source tree, with conflicts resolved in favor of \"2.\" on a file-by-file basis.\ntry:\nADD /files/apache-stratos.zip /opt/\nand see if the files are there, without further decompression.",
    "How to show a dockerfile of image docker": "The raw output, as described in \"How to generate a Dockerfile from an image?\", would be:\ndocker history --no-trunc <IMAGE_ID>\nBut a more complete output would be from CenturyLinkLabs/dockerfile-from-image\ndocker run -v /var/run/docker.sock:/var/run/docker.sock \\\n centurylink/dockerfile-from-image <IMAGE_TAG_OR_ID>\nNote there were limitations.\nIn 2020, as illustrated here:\ndocker run -v /var/run/docker.sock:/var/run/docker.sock --rm alpine/dfimage \\\n  -sV=1.36 <IMAGE_TAG_OR_ID>",
    "How to make Postgres (Docker) accessible for any IP remotely?": "You have to create postgresql.conf whith parameters, and set listen_addresses = '*'\nattach when starting your container.\ndocker run -p 5432:5432 -e POSTGRES_PASSWORD=123456789 \\\n -d postgres:9.3.6 \\\n -c config_file=/path/to/postgresql.conf\nnext solution. Create Dockerfile and add follows:\nFROM ubuntu\n\n# Add the PostgreSQL PGP key to verify their Debian packages.\n# It should be the same key as https://www.postgresql.org/media/keys/ACCC4CF8.asc\nRUN apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys B97B0AFCAA1A47F044F244A07FCC7D46ACCC4CF8\n\n# Add PostgreSQL's repository. It contains the most recent stable release\n#     of PostgreSQL, ``9.3``.\nRUN echo \"deb http://apt.postgresql.org/pub/repos/apt/ precise-pgdg main\" > /etc/apt/sources.list.d/pgdg.list\n\n# Install ``python-software-properties``, ``software-properties-common`` and PostgreSQL 9.3\n#  There are some warnings (in red) that show up during the build. You can hide\n#  them by prefixing each apt-get statement with DEBIAN_FRONTEND=noninteractive\nRUN apt-get update && apt-get install -y python-software-properties software-properties-common postgresql-9.3 postgresql-client-9.3 postgresql-contrib-9.3\n\n# Note: The official Debian and Ubuntu images automatically ``apt-get clean``\n# after each ``apt-get``\n\n# Run the rest of the commands as the ``postgres`` user created by the ``postgres-9.3`` package when it was ``apt-get installed``\nUSER postgres\n\n# Create a PostgreSQL role named ``docker`` with ``docker`` as the password and\n# then create a database `docker` owned by the ``docker`` role.\n# Note: here we use ``&&\\`` to run commands one after the other - the ``\\``\n#       allows the RUN command to span multiple lines.\nRUN    /etc/init.d/postgresql start &&\\\n    psql --command \"CREATE USER docker WITH SUPERUSER PASSWORD 'docker';\" &&\\\n    createdb -O docker docker\n\n# Adjust PostgreSQL configuration so that remote connections to the\n# database are possible.\nRUN echo \"host all  all    0.0.0.0/0  md5\" >> /etc/postgresql/9.3/main/pg_hba.conf\n\n# And add ``listen_addresses`` to ``/etc/postgresql/9.3/main/postgresql.conf``\nRUN echo \"listen_addresses='*'\" >> /etc/postgresql/9.3/main/postgresql.conf\n\n# Expose the PostgreSQL port\nEXPOSE 5432\n\n# Add VOLUMEs to allow backup of config, logs and databases\nVOLUME  [\"/etc/postgresql\", \"/var/log/postgresql\", \"/var/lib/postgresql\"]\n\n# Set the default command to run when starting the container\nCMD [\"/usr/lib/postgresql/9.3/bin/postgres\", \"-D\", \"/var/lib/postgresql/9.3/main\", \"-c\", \"config_file=/etc/postgresql/9.3/main/postgresql.conf\"]\nBuild an image from the Dockerfile assign it a name.\n$ docker build -t my_postgresql .\nRun the PostgreSQL server container (in the foreground):\n$ docker run --rm -P --name pg_test my_postgresql\nConnecting from your host system $ docker ps\nCONTAINER ID        IMAGE                  COMMAND                CREATED             STATUS              PORTS                                      NAMES\n5e24362f27f6        my_postgresql:latest   /usr/lib/postgresql/   About an hour ago   Up About an hour    0.0.0.0:49153->5432/tcp                    pg_test\n\n$ psql -h localhost -p 49153 -d docker -U docker --password",
    "Difference between pushing a docker image and installing helm image": "Amount to effort\nTo deploy a service on Kubernetes using docker image you need to manually create various configuration files like deployment.yaml. Such files keep on increasing as you have more and more services added to your environment.\nIn the Helm chart, we can provide a list of all services that we wish to deploy in requirements.yaml file and Helm will ensure that all those services get deployed to the target environment using deployment.yaml, service.yaml & values.yaml files.\nConfigurations to maintain\nAlso adding configuration like routing, config maps, secrets, etc becomes manually and requires configuration over-&-above your service deployment.\nFor example, if you want to add an Nginx proxy to your environment, you need to separately deploy it using the Nginx image and all the proxy configurations for your functional services.\nBut with Helm charts, this can be achieved by configuring just one file within your Helm chart: ingress.yaml\nFlexibility\nUsing docker images, we need to provide configurations for each environment where we want to deploy our services.\nBut using the Helm chart, we can just override the properties of the existing helm chart using the environment-specific values.yaml file. This becomes even easier using tools like ArgoCD.\nCode-Snippet:\nBelow is one example of deployment.yaml file that we need to create if we want to deploy one service using docker-image.\nInline, I have also described how you could alternatively populate a generic deployment.yaml template in Helm repository using different files like requirements.yaml and Values.yaml\ndeployment.yaml for one service\ncrazy-project/charts/accounts/templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: accounts\nspec:\n  replicas: 1\n  selector:\n     matchLabels:\n       app.kubernetes.io/name: accounts\n       app.kubernetes.io/instance: crazy-project\n  template:\n     metadata:\n       labels:\n         app.kubernetes.io/name: accounts\n         app.kubernetes.io/instance: crazy-project\n     spec:\n       serviceAccountName: default\n       automountServiceAccountToken: true\n       imagePullSecrets:\n         - name: regcred\n       containers:\n       - image: \"image.registry.host/.../accounts:1.2144.0\"   <-- This version can be fetched from 'requirements.yaml'\n         name: accounts\n         env:      <-- All the environment variables can be fetched from 'Values.yaml'\n         - name: CLUSTERNAME\n           value: \"com.company.cloud\"\n         - name: DB_URI\n           value: \"mongodb://connection-string&replicaSet=rs1\"\n         imagePullPolicy: IfNotPresent\n         volumeMounts:\n         - name: secretfiles\n           mountPath: \"/etc/secretFromfiles\"\n           readOnly: true\n         - name: secret-files\n           mountPath: \"/etc/secretFromfiles\"\n           readOnly: true\n         ports:\n         - name: HTTP\n           containerPort: 9586\n           protocol: TCP\n         resources:\n           requests:\n             memory: 450Mi\n             cpu: 250m\n           limits:\n             memory: 800Mi\n             cpu: 1\n       volumes:\n       - name: secretFromfiles\n         secret:\n           secretName: secret-from-files\n       - name: secretFromValue\n         secret:\n           secretName: secret-data-vault\n           optional: true\n           items:...\nYour deployment.yaml in Helm chart could be a generic template(code-snippet below) where the details are populated using values.yaml file.\nenv:\n{{- range $key, $value := .Values.global.envVariable.common }}\n    - name: {{ $key }}\n      value: {{ $value  | quote }}\n    {{- end }}\nYour Values.yaml would look like this:\naccounts:\n  imagePullSecrets:\n    - name: regcred\n  envVariable:\n    service:\n      vars:\n        spring_data_mongodb_database: accounts_db\n        spring_product_name: crazy-project\n        ...\nYour requirements.yaml would be like below. 'dependencies' are the services that you wish to deploy.\ndependencies:\n  - name: accounts\n    repository: \"<your repo>\"\n    version: \"= 1.2144.0\"\n  - name: rollover\n    repository: \"<your repo>\"\n    version: \"= 1.2140.0\"\nThe following diagram will help you visualize what I have mentioned above:",
    "How to find out which Linux is installed inside docker image?": "A docker image doesn't need an OS. There's a possibility of extending the scratch image which is purposely empty and the container may only contain one binary or some volume.\nHaving an entire OS is possible but also misleading: The host shares its kernel with the container. (This is not a virtual machine.)\nThat means that no matter what \"OS\" you are running, the same kernel in the container is found:\nBoth:\ndocker run --rm -it python:3.6 uname -a\ndocker run --rm -it python:3.6-alpine uname -a\nwill report the same kernel of your host machine.\nSo you have to look into different ways:\ndocker run --rm -it python:3.6 cat /etc/os-release\nor\nlsb_release -sirc\nor for Cent OS:\ncat /etc/issue\nInstead of scratch, a lot of images are also alpine-based, to avoid the size overhead. An Ubuntu base image can easily have 500MB fingerprint whereas alpine uses around 5MB; so I'd rather check for that as well.\nAlso avoid the trap of manually installing everything onto one Ubuntu image inside one big Dockerfile. Docker works best if each service is its own container that you link together. (For that, check out docker-compose.)\nIn the end, you, as a user, shouldn't care about the OS of an image, but rather its size. Only as a developer of the Dockerfile is it relevant to know the OS and that you'll find out either by looking into the Dockerfile the image was built (if it's on docker hub you can read it there).\nYou basically have to look what was used to create your image and use the appropriate tools for the job. (Debian-based images use apt-get, alpine uses apk, and Fedora uses yum.)",
    "Docker Container with Apache Spark in standalone cluster mode": "UPDATED ANSWER (for spark 2.4.0):\nTo start spark master on foreground, just set the ENV variable SPARK_NO_DAEMONIZE=true on your environment before running ./start-master.sh\nand you are good to go.\nfor more info, check $SPARK_HOME/sbin/spark-daemon.sh\n# Runs a Spark command as a daemon.\n#\n# Environment Variables\n#\n#   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.\n#   SPARK_LOG_DIR   Where log files are stored. ${SPARK_HOME}/logs by default.\n#   SPARK_MASTER    host:path where spark code should be rsync'd from\n#   SPARK_PID_DIR   The pid files are stored. /tmp by default.\n#   SPARK_IDENT_STRING   A string representing this instance of spark. $USER by default\n#   SPARK_NICENESS The scheduling priority for daemons. Defaults to 0.\n#   SPARK_NO_DAEMONIZE   If set, will run the proposed command in the foreground. It will not output a PID file.\n##",
    "Docker Ubuntu 18.04 unable to install msodbcsql17 SQL Server ODBC Driver 17": "I could get it working. Below is the updated Docker file snippet\nFROM ubuntu:18.04\n\nRUN apt update -y  &&  apt upgrade -y && apt-get update \nRUN apt install -y curl python3.7 git python3-pip openjdk-8-jdk unixodbc-dev\n\n# Add SQL Server ODBC Driver 17 for Ubuntu 18.04\nRUN curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\nRUN curl https://packages.microsoft.com/config/ubuntu/18.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\nRUN apt-get update\nRUN ACCEPT_EULA=Y apt-get install -y --allow-unauthenticated msodbcsql17\nRUN ACCEPT_EULA=Y apt-get install -y --allow-unauthenticated mssql-tools\nRUN echo 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' >> ~/.bash_profile\nRUN echo 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' >> ~/.bashrc\n\nCOPY startup.sh /\nRUN chmod +x /startup.sh\nENTRYPOINT [\"sh\",\"/startup.sh\"]",
    "How to Edit Docker Image?": "There are 4 Steps\nStart the image you want to edit: docker run IMAGE\nModify the running image by shelling into it with docker exec -it <container-id> (you can get the container id with docker ps)\nMake any modifications (install new things, make a directory or file)\nIn a new terminal tab/window run docker commit <container-id> my-new-image (substituting in the container id of the container you want to save)\nAn example\n# Run an existing image\ndocker run -dt existing_image \n\n# See that it's running\ndocker ps\n# CONTAINER ID   IMAGE            COMMAND   CREATED              STATUS              \n# c7e6409a22bf   existing-image   \"R\"       6 minutes ago        Up 6 minutes\n\n# Shell into it\ndocker exec -it c7e6409a22bf bash\n\n# Make a new directory for demonstration purposes\n# (note that this is inside the existing image)\nmkdir NEWDIRECTORY\n\n# Open another terminal tab/window, and save the running container you modified\ndocker commit c7e6409a22bf my-new-image\n\n# Inspect to ensure it saved correctly\ndocker image ls\n# REPOSITORY           TAG       IMAGE ID       CREATED         SIZE\n# existing-image       latest    a7dde5d84fe5   7 minutes ago   888MB\n# my-new-image         latest    d57fd15d5a95   2 minutes ago   888MB",
    "Default value with shell expressions in Dockerfile ARG and ENV": "From the documentation:\nThe ${variable_name} syntax also supports a few of the standard bash modifiers as specified below:\n${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result.\n${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string.\nENV is special docker build command and doesn't support this. What you are looking for is to run Shell commands in ENV. So this won't work.\nPossible solution is to use a bash script\ncuda_version.sh\n#!/bin/bash\nCUDA_FULL=\"${CUDA_VERSION:-8.0.61_375.26}\"\nCUDA_MAJOR=\"$(echo ${CUDA_VERSION:-8.0.61_375.26} | cut -d. -f1)\"\nCUDA_MINOR=\"$(echo ${CUDA_VERSION:-8.0.61_375.26} | cut -d. -f2)\"\nCUDA_MAJMIN=\"$CUDA_MAJOR.$CUDA_MINOR\" \nCUDNN_FULL=\"${CUDNN_VERSION:-7.0.1}\"\nCUDNN_MAJOR=\"$(echo ${CUDNN_VERSION:-7.0.1} | cut -d. -f1)\"\nCUDNN_MINOR=\"$(echo ${CUDNN_VERSION:-7.0.1} | cut -d. -f2)\"\nCUDNN_MAJMIN=\"$CUDNN_MAJOR.$CUDNN_MINOR\"\nAnd change your dockerfile to\nARG CUDA_VERSION=8.0.61_375.26\nARG CUDNN_VERSION=7.0.1\n\nENV CUDA_VERSION=${CUDA_VERSION} CUDNN_VERSION=${CUDNN_VERSION}\nCOPY cuda_version.sh /cuda_version.sh\nRUN bash -c \"source /cuda_version.sh && curl -LO https://.../${CUDNN_FULL}/.../...${CUDA_MAJMIN}...\"\nYou can remove the default values from your shell file as they will always be there from the Dockerfile arguments/environment",
    "/var/run/docker.sock: permission denied while running docker within Python CGI script": "Permission denied on a default install indicates you are trying to access the socket from a user other than root or that is not in the docker group. You should be able to run:\nsudo usermod -a -G docker $username\non your desired $username to add them to the group. You'll need to logout and back in for this to take effect (use newgrp docker in an existing shell, or restart the daemon if this is an external service accessing docker like your cgi scripts).\nNote that doing this effectively gives that user full root access on your host, so do this with care.",
    "Build started failing using Python:3.8 Docker image on apt-get update and install with GPG error: bookworm InRelease is not signed": "Why this happened?\nThe Python docker images have been updated recently to use Debian 12 bookworm version which was released on 10 June 2023 instead of Debian 10 buster.\nSources:\nGitHub > docker-library/python > Commit > add bookworm, remove buster\nWikipedia > Debian version history > Release table\nWhat is the root cause?\nIt is Docker with libseccomp so a newer syscall used in Debian Bookworm packages/libs is being blocked. libseccomp lets you configure allowed syscalls for a process. Docker sets a default seccomp profile for all containers such that only certain syscalls are allowed and everything else is blocked (so, newer syscalls that are not yet known to libseccomp or docker are blocked).\nSource: python:3.9 - Failed run apt update from the last version of the image #837\nPossible Solutions:\nEither\nAdd the following in the Dockerfile:\nRUN mv -i /etc/apt/trusted.gpg.d/debian-archive-*.asc  /root/\nRUN ln -s /usr/share/keyrings/debian-archive-* /etc/apt/trusted.gpg.d/\nOr\nUse any of the bullseye image (e.g., python:3.8-slim-bullseye).\nOr\nUpdate libseccomp and docker on the host running the containers.",
    "how to correctly use system user in docker container": "This sort of error will happen when the uid/gid does not exist in the /etc/passwd or /etc/group file inside the container. There are various ways to work around that. One is to directly map these files from your host into the container with something like:\n$ docker run -it --rm --user=999:998 \\\n  -v /etc/passwd:/etc/passwd:ro -v /etc/group:/etc/group:ro \\\n  my-image:latest bash\nI'm not a fan of that solution since files inside the container filesystem may now have the wrong ownership, leading to potential security holes and errors.\nTypically, the reason people want to change the uid/gid inside the container is because they are mounting files from the host into the container as a host volume and want permissions to be seamless across the two. In that case, my solution is to start the container as root and use an entrypoint that calls a script like:\nif [ -n \"$opt_u\" ]; then\n  OLD_UID=$(getent passwd \"${opt_u}\" | cut -f3 -d:)\n  NEW_UID=$(stat -c \"%u\" \"$1\")\n  if [ \"$OLD_UID\" != \"$NEW_UID\" ]; then\n    echo \"Changing UID of $opt_u from $OLD_UID to $NEW_UID\"\n    usermod -u \"$NEW_UID\" -o \"$opt_u\"\n    if [ -n \"$opt_r\" ]; then\n      find / -xdev -user \"$OLD_UID\" -exec chown -h \"$opt_u\" {} \\;\n    fi\n  fi\nfi\nThe above is from a fix-perms script that I include in my base image. What's happening there is the uid of the user inside the container is compared to the uid of the file or directory that is mounted into the container (as a volume). When those id's do not match, the user inside the container is modified to have the same uid as the volume, and any files inside the container with the old uid are updated. The last step of my entrypoint is to call something like:\nexec gosu app_user \"$@\"\nWhich is a bit like an su command to run the \"CMD\" value as the app_user, but with some exec logic that replaces pid 1 with the \"CMD\" process to better handle signals. I then run it with a command like:\n$ docker run -it --rm --user=0:0 -v /host/vol:/container/vol \\\n  -e RUN_AS app_user --entrypoint /entrypoint.sh \\\n  my-image:latest bash\nHave a look at the base image repo I've linked to, including the example with nginx that shows how these pieces fit together, and avoids the need to run containers in production as root (assuming production has known uid/gid's that can be baked into the image, or that you do not mount host volumes in production).",
    "Docker environmental variables from a file": "Your best options is to use either the -e flag, or the --env-file of the docker run command.\nThe -e flag allows you to specify key/value pairs of env variable,\nfor example:\ndocker run -e ENVIRONMENT=PROD\nYou can use several time the -e flag to define multiple env\nvariables. For example, the docker registry itself is configurable with -e flags, see: https://docs.docker.com/registry/deploying/#running-a-domain-registry\nThe --env-file allow you to specify a file. But each line of the file must be of type VAR=VAL\nFull documentation:\nhttps://docs.docker.com/engine/reference/commandline/run/#set-environment-variables-e-env-env-file",
    "Error while trying to ssh a docker container : System is booting up": "Solution:\nPlease edit your dockerfile like this:\nFROM centos\nRUN yum -y install openssh-server\nRUN useradd remote_user && \\\n    echo remote_user:1234 | chpasswd && \\\n    mkdir /home/remote_user/.ssh && \\\n    chmod 700 /home/remote_user/.ssh\nCOPY remote-key.pub /home/remote_user/.ssh/authorized_keys\nRUN chown remote_user:remote_user -R /home/remote_user/.ssh && \\\n    chmod 600 /home/remote_user/.ssh/authorized_keys\nRUN /usr/bin/ssh-keygen -A\nEXPOSE 22\nRUN rm -rf /run/nologin\nCMD /usr/sbin/sshd -D",
    "Docker RUN fails with \"returned a non-zero code: 6\"": "The exit code 6 means that \"Host public key is unknown. sshpass exits without confirming the new key.\"\nSo either you populate before that the ~/.ssh/known_hostswith the fingerprint of the host, or just ignore the check of the host public key by adding the StrictHostKeyChecking=no option to the scp.\nThe updated line would look like that:\nRUN sshpass -p userPassword scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -r user@server:~/data/* ./",
    "Docker: how to manage development and production settings?": "You could mount your custom nginx.conf into the container in development via e.g. --volume ./nginx/nginx.conf:/etc/nginx/nginx.conf and simply omit this parameter to docker run in production.\nIf using docker-compose, the two options I would recommend are:\nEmploy the limited support for environment variable interpolation and add something like the following under volumes in your container definition: ./nginx/nginx.${APP_ENV}.conf:/etc/nginx/nginx.conf\nUse a separate YAML file for production overrides.",
    "Copying node_modules into a dockerfile vs installing them": "I'd almost always install Node packages from inside the container rather than COPYing them from the host (probably via RUN npm ci if I was using npm).\nIf the host environment doesn't exactly match the container environment, COPYing the host's node_modules directory may not work well (or at all). The most obvious case of this using a MacOS or Windows host with a Linux container, where if there are any C extensions or other binaries they just won't work. It's also conceivable that there would be if the Node versions don't match exactly. Finally, and individual developer might have npm installed an additional package or a different version, and the image would vary based on who's building it.\nAlso consider the approach of using a multi-stage build to have both development and production versions of node_modules; that way you do not include build-only tools like the tsc Typescript compiler in the final image. If you have two different versions of node_modules then you can't COPY a single tree from the host, you must install it in the Dockerfile.\nFROM node AS build\nWORKDIR /app\nCOPY package*.json .\nRUN npm ci\nCOPY . .\nRUN npm install\n\nFROM node\nWORKDIR /app\nCOPY package*.json .\nENV NODE_ENV=production\nRUN npm ci\nCOPY --from=build /app/build /app/build\nCMD [\"node\", \"/app/build/index.js\"]",
    "docker inside docker container": "Update\nThanks to https://stackoverflow.com/a/38016704/372019 I want to show another approach.\nInstead of mounting the host's docker binary, you should copy or install a container specific release of the docker binary. Since you're only using it in a client mode, you won't need to install it as a system service. You still need to mount the Docker socket into the container so that you can easily communicate with the host's Docker engine.\nAssuming that you got a base image with a working Docker binary (e.g. the official docker image), the example now looks like this:\ndocker run\\\n  -v /var/run/docker.sock:/var/run/docker.sock\\\n  docker:1.12 docker info\nWithout actually answering your question I'd suggest you to read Using Docker-in-Docker for your CI or testing environment? Think twice.\nIt explains why running docker-in-docker should be replaced with a setup where Docker containers run as siblings of the \"outer\" or \"base\" container. The article also links to the original https://github.com/jpetazzo/dind project where you can find working examples how to run Docker in Docker - in case you still want to have docker-in-docker.\nAn example how to enable a container to access the host's Docker daemon look like this:\ndocker run\\\n  -v /var/run/docker.sock:/var/run/docker.sock\\\n  -v /usr/bin/docker:/usr/bin/docker\\\n  busybox:latest /usr/bin/docker info",
    "Running Docker pull command in Dockerfile": "You should not pull from a Dockerfile.\nYou simply can start your Dockerfile with:\nFROM docker-oracle-xe-11g\nAnd add in it any Oracle config file which you would need in your own Oracle image.\nThe docker-oracle-xe-11g Dockerfile is already based on ubuntu.\nFROM ubuntu:14.04.1",
    "How to modify the `core_pattern` when building docker image": "It's not possible to have different core_pattern in the host and in the container at the same time, as docker is sharing the kernel with its host.\nHowever, you can run the container in privileged mode and change core_pattern from inside a container during startup/runtime (modify core_pattern in CMD section or execute os command from inside).\nBut keep in mind that this setting will not be automatically restored after the container finished (until you do it programmatically).",
    "Docker WORKDIR - on my machine or the container?": "It is inside the container.\nTaken for the Dockerfile reference site https://docs.docker.com/engine/reference/builder/#workdir\nThe WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn\u2019t exist, it will be created even if it\u2019s not used in any subsequent Dockerfile instruction.\nSo rather than adding RUN cd && ... you could do:\nWORKDIR /path/to/dir\nRUN command",
    "How to append to path in dockerfile or docker run": "To append the containers $PATH try something along these lines in the Dockerfile:\nENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH\nResources:\nUpdating PATH environment variable permanently in Docker container\nIn a Dockerfile, How to update PATH environment variable?\nhttps://docs.docker.com/engine/reference/builder/#env",
    "Building Dockerfile fails when touching a file after a mkdir": "Looking at https://registry.hub.docker.com/u/library/jenkins/, it seems that /var/jenkins_home is a volume. You can only create files there while the container is running, presumably with a volume mapping like\ndocker run ... -v /your/jenkins/home:/var/jenkins_home ...\nThe docker build process knows nothing about shared volumes.",
    "nobody & nonroot user in distroless images": "There are 2 type of distroless images which can be used in production:\nwith latest tag\nThis image say, gcr.io/distroless/base by default has \"Config.User: 0\" and \"Config.WorkingDir: /\" config in it and if you don't use USER for switching the user to nonroot user which is defined in it or it will start container with root user.\nwith nonroot tag\nThis image say, gcr.io/distroless/base:nonroot by default has \"Config.User: 65532\" and \"Config.WorkingDir: /home/nonroot\" config in it and there is no need to use USER for changing user to non-root user.\nPS: maybe you need to change ownership of copied files in multistage build to nonroot user.\nnobody user\nThe purpose of nobody user is not related to distroless images and it's about Linux itself which described here very well",
    "What happens to a Docker Container when HEALTHCHECK fails": "When running HEALTHCKECKS you can specify:\n--interval=DURATION (default 30s)\n--timeout=DURATION (default 30s)\n--retries=N (default 3)\nAnd the container can have three states:\nstarting \u2013 Initial status when the container is still starting.\nhealthy \u2013 When the command succeeds.\nunhealthy \u2013 When a single run of the HEALTHCHECK takes longer than the specified timeout. When this happens it will run retries and will be declared \"unhealthy\" if it still fails.\nWhen the check fails for a specified number of times in a row, the failed container will:\nstay in \"unhealthy\" state if it is in standalone mode\nrestart if it is in Swarm mode\nOtherwise it will exit with error code 0 which means it is considered \"healthy\".\nI hope it makes things more clear.",
    "microdnf update command installs new packages instead of just updating existing packages": "I had the same or a very similar problem. Found a command-line flag that helped to lower the number of additionally installed packages. If you add install_weak_deps=0, it should help with these additional packages.\nmicrodnf upgrade \\\n  --refresh \\\n  --best \\\n  --nodocs \\\n  --noplugins \\\n  --setopt=install_weak_deps=0",
    "\\Dockerfile: The system cannot find the file specified": "The command docker -t build <my docker file name> . is being misused. It should be:\ndocker build -t <image-name> -f dockerFile .\nwhere dockerFile is the name you gave to the Dockerfile.\nThe -t option specifies the tag or name to give to the Docker image.\nThe -f must be used, if you name the Dockerfile something other than Dockerfile\nThe . specifies the docker build context.",
    "Docker - no such file or directory": "With that sort of corruption, I'd give a full docker wipe a try, rm -rf /var/lib/docker/*. Before doing that, backup any data (volumes), then shutdown docker, and you'll need to pull or rebuild all your images again. If there are still problems with aufs, try changing the filesystem driver, e.g. changing to dockerd -s overlay2 in your service startup.\nIt doesn't hurt to check for common issues, like running out of disk space or old version of the application, first.",
    "Run py.test in a docker container as a service": "I just enabled it on one of my projects recently. I use a multistage build. At present I put tests in the same folder as the source test_*.py. From my experience with this, it doesn't feel natural, I prefer tests to be in its own folder that is excluded by default.\nFROM python:3.7.6 AS build\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip3 install --compile -r requirements.txt && rm -rf /root/.cache\nCOPY src /app\n# TODO precompile\n\n# Build stage test - run tests\nFROM build AS test\nRUN pip3 install pytest pytest-cov && rm -rf /root/.cache\nRUN pytest --doctest-modules \\\n  --junitxml=xunit-reports/xunit-result-all.xml \\\n  --cov \\\n  --cov-report=xml:coverage-reports/coverage.xml \\\n  --cov-report=html:coverage-reports/\n\n# Build stage 3 - Complete the build setting the executable\nFROM build AS final\nCMD [ \"python\", \"./service.py\" ]\nIn order to exclude the test files from coverage. .coveragerc must be present.\n[run]\nomit = test_*\nThe test target runs the required tests and generates coverage and execution reports. These are NOT suitable for Azure DevOps and SonarQube. To make it suitable\nsed -i~ 's#/app#$(Build.SourcesDirectory)/app#' $(Pipeline.Workspace)/b/coverage-reports/coverage.xml\nTo run tests\n#!/usr/bin/env bash\nset -e\nDOCKER_BUILDKIT=1 docker build . --target test --progress plain",
    "How to forward all ports in docker container": "You can expose a range of ports using the -p option, for example:\ndocker run -p 2000-5000:2000-5000 -v /host/:/host appimage\nSee the docker run reference documentation for more details.",
    "Install mysql in dockerfile?": "The problem is that you've never started the database - you need to explicitly start services in most Docker images. But if you want to run two processes in Docker (the DB and your python program), things get a little more complex. You either have to use a process manager like supervisor, or be a bit cleverer in your start-up script.\nTo see what I mean, create the following script, and call it cmd.sh:\n#!/bin/bash\n\nmysqld &\npython main.py\nAdd it to the Dockerfile:\nFROM ubuntu:saucy\n\n# Install required packages\nRUN apt-get update\nRUN DEBIAN_FRONTEND=noninteractive apt-get -y install python\nRUN DEBIAN_FRONTEND=noninteractive apt-get -y install mysql-server python-mysqldb\n\n# Add our python app code to the image\nRUN mkdir -p /app\nADD . /app\nWORKDIR /app\n\n# Set the default command to execute\nCOPY cmd.sh /cmd.sh\nRUN chmod +x /cmd.sh\nCMD cmd.sh\nNow build and try again. (Apologies if this doesn't work, it's off the top of my head and I haven't tested it).\nNote that this is not a good solution; mysql will not be getting signals proxied to it, so probably won't shutdown properly when the container stops. You could fix this by using a process manager like supervisor, but the easiest and best solution is to use separate containers. You can find stock containers for mysql and also for python, which would save you a lot of trouble. To do this:\nTake the mysql installation stuff out of the Dockerfile\nChange localhost in your python code to mysql or whatever you want to call your MySQL container.\nStart a MySQL container with something like docker run -d --name mysql mysql\nStart your container and link to the mysql container e.g: docker run myapp --link mysql:mysql",
    "Make Docker build stop after if RUN fails in multi-stage build": "Your command, run-tests.sh, needs to exit with a non-zero exit code and docker will stop building. In this case, that has happened:\nThe command '/bin/bash -c cd test; ./run-tests.sh' returned a non-zero code: 1\nWhatever you run to call docker build needs to handle that exit code and stop running at that point. Docker's behavior is to give you an exit code to indicate the failure:\n$ cat df.fail\nFROM busybox\nRUN exit 1\nRUN echo still running\n\n$ docker build -f df.fail .\nSending build context to Docker daemon  23.04kB\nStep 1/3 : FROM busybox\n ---> 59788edf1f3e\nStep 2/3 : RUN exit 1\n ---> Running in 70d90fb88d6e\nThe command '/bin/sh -c exit 1' returned a non-zero code: 1\n\n$ echo $?\n1\nFrom the above example, you can see that docker does stop as soon as the command returns a non-zero exit code, it does not run the echo still running line, and there's a non-zero return code from docker build itself that you can handle with whatever you use to run the build.",
    "Docker not updating changes in directory": "In your docker file, you are using\nCOPY . .\nThis mean that, when you build your docker, you copy your current folder to the default folder of your container. Probably /root\nBut this copy isn't executed every time you RUN the container or START it, it's only when you BUILD.\nTo be able to see every change you make in real time without re BUILD, you need to create a volume, wich will be a link between your host and your container. Every content changing on the host or the container will be shared to the other.\nNote that in your dockerfile, declaring a VOLUME won't actually change anything, it's just an information. To actually make a volume you need to add -v /host/path:/container/path in your docker run command line.",
    "What are Docker COPY's rules about symlinks / how can I preserve symlinks?": "This works if you try to copy the entire directory as a unit, rather than trying to copy the files in the directory:\nCOPY ./ /foo/bar/\nNote that there are some subtleties around copying directories: the Dockerfile COPY documentation notes that, if you COPY a directory,\nNOTE: The directory itself is not copied, just its contents.\nThis is fine for your case where you're trying to copy the entire build context. If you have a subdirectory you're trying to copy, you need to make sure the subdirectory name is also on the right-hand side of COPY and that the directory name ends with /.",
    "Docker Buildx Cannot Pull From Local for Inherited Image": "There are a few different buildx drivers, and they each have tradeoffs.\nFirst is the docker driver. This is the driver for the default builder instance if you change nothing else. It's build-in to to the docker engine and should have visibility to the other images on the host. The goal is to be similar to the classic build process.\nThe second is docker-container and is the default if you create a new builder instance with docker buildx create. This is needed for specific functionality like the multi-platform images and exporting the cache. But since it's running inside a container, you won't see the other images on the docker host.\nOne big issue when trying to use the docker host for multi-architecture images is that the docker engine itself doesn't support multi-architecture images. It will only pull one of the architectures from a registry, so your image becomes a single architecture that likely can't be used in a multi-architecture build.\nThe easiest fix is to use a registry for your images. This supports the multi-architecture image formats which you can't do on a docker host. And this is portable when you run the build on another node.\nThere are other options in the buildx documentation to cache from/to other locations. But when dealing with a multi-arch base image, you'll find the external registry is much easier, and likely the one that actually works. Keep in mind this doesn't have to be Docker Hub, you can run you own registry server on the same host where you run your builds.\nSide note: buildx/buildkit also benefits from having a persistent volume if you happen to run ephemeral builders (e.g. using some kind of DinD on a CI server). Buildkit can be configured to automatically garbage collect this cache to avoid the storage problems of the past. And with that cache, you avoid the need to download the image layers on every build from the external registry.",
    "Run docker image with docker-compose": "you should build the image with this name: (registryName:RegistryPort)/imagename:version\n$ docker build -t myRegistry.example.com:5000/myApp:latest .\n$ docker build -t myRegistry.example.com:5000/myDb:latest .\nNow add these lines to the docker-compose file :\nMyapp:                                                    \n  image: myRegistry.example.com:5000/myApp:latest \n\nMyDb:                        \n  image: myRegistry.example.com:5000/myDb:latest\nAnd then push it :\n$ docker push myRegistry.example.com:5000/myApp:latest\n$ docker push myRegistry.example.com:5000/myDb:latest\nYour mate should now be able to pull it now\n$ docker pull myRegistry.example.com:5000/myApp:latest\n$ docker pull myRegistry.example.com:5000/myDb:latest",
    "Make docker container run forever while being able to gracefully stop": "You could consider using (with docker 1.9+) STOPSIGNAL in your Dockerfile.\nThe STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit.\nThis signal can be a valid unsigned number that matches a position in the kernel\u2019s syscall table, for instance 9, or a signal name in the format SIGNAME, for instance SIGKILL.\nBut for a script managing such a signal, see \"Trapping signals in Docker containers\" and its program.sh to orchestrate other non-PID1 processes.",
    "Store and Restore Inherited Dockerfile USER setting": "If you are author of parent image you can do this like this:\nENV serviceuser=foo\nRUN useradd $serviceuser\nUSER $serviceuser\nchild image:\nUSER root\nRUN apt-get install -y cool-stuff\nUSER $serviceuser",
    "docker logs <C> returns nothing": "Docker containers exit when their main process finishes. That is why you don't get any logs.\nThe docker logs command batch-retrieves logs present at the time of execution.\n(see: https://docs.docker.com/engine/reference/commandline/logs/)\nAn example:\nThe following will create a new container and start it:\ndocker run -d --name=my_test_container alpine ping -c 20 127.0.0.1\n[----run----]   [--------name--------] [image][-----command------]\nTry to use the following, before ping command stops:\ndocker logs my_test_container\ndocker logs --follow my_test_container\nThe first one shows what has been printed out by ping (until then) and the second one gives you logs as ping prints out new lines.\nAfter 20 ping requests, the ping command finishes and the container stops.\nubuntu@ubuntu:~$ docker container ls -a\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                     PORTS               NAMES\ne8f371000146        alpine              \"ping -c 20 127.0.0.1\"   29 seconds ago      Exited (0) 9 seconds ago                       my_test_container",
    "Who runs the healthcheck? Compose or the container itself?": "Dockerfiles provides a way to perform docker container health check as well as docker compose which allows to set healthcheck for the services that compose the application from the docker-compose.yml file, starting from version 2.1.\nIn both cases, from Dockerfiles or docker compose, the health checks are performed by the Docker daemon which invokes this command every 30 seconds and it determines if a container is healthy.\nIn particular, the healthcheck result is displayed in the STATUS column of the docker ps command and may have the results:\n0 healthy container\n1 unhealthy container\nPlease, refer to the official documentation https://docs.docker.com/engine/reference/builder/#healthcheck.",
    "pnpm workspace:* dependencies": "Was getting the same error while shifting from yarn to pnpm! Resolved it by just adding pnpm-workspace.yaml with following contents:\npackages:\n  - \"apps/*\"\n  - \"packages/*\"\nHope that helps!",
    "NuGet in Docker: Error NU1301: Unable to load the service index for source - Sequence contains no elements": "I was able to resolve it by adding a Personal Access Token (PAT) as the password for the artifact source.\nExample:\n# access token arg is passed in by build process                \nARG ACCESS_TOKEN=\"your PAT\"\nARG ARTIFACTS_ENDPOINT=\"https://yoursource/v3/index.json\"\n\n# Configure the environment variables\nENV NUGET_CREDENTIALPROVIDER_SESSIONTOKENCACHE_ENABLED true\nENV VSS_NUGET_EXTERNAL_FEED_ENDPOINTS \"{\\\"endpointCredentials\\\": [{\\\"endpoint\\\":\\\"${ARTIFACTS_ENDPOINT}\\\", \\\"password\\\":\\\"${ACCESS_TOKEN}\\\"}]}\"\nI also needed terminal to be running with Admin privileges.",
    "Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=container [closed]": "You should use next commands inside your docker container\nsed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-*\nsed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*",
    "How to use the ARG instruction of Dockerfile for Windows image": "As @matt9 suggested\nUse $env:FirefoxVersion in powershell\nUse %FirefoxVersion% in cmd.exe\nFROM microsoft/windowsservercore:ltsc2016\nARG FirefoxVersion\n#if using powershell\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop';\"]\nRUN Write-Host Powershell: $env:FirefoxVersion\n#if using CMD\nSHELL [\"cmd\", \"/S\", \"/C\"]\nRUN echo cmd.exe: %FirefoxVersion%\nBuild: docker build -t myimage --build-arg FirefoxVersion=61.0.1 .\nResult\nPowershell: 61.0.1\ncmd.exe: 61.0.1",
    "Docker so slow while installing pip requirements": "Probably this is because PyPI wheels don\u2019t work on Alpine. Instead of using precompile files Alpine downloads the source code and compile it. Try to use python:3.7-slim image instead:\n# Pull base image\nFROM python:3.7-slim\n\n# Set environment variables\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\n# Set work directory\nWORKDIR /code\n\n# Install dependencies\nCOPY requirements.txt /code/\nRUN pip install -r requirements.txt\n\n# Copy project\nCOPY . /code/\nCheck this article for more details: Alpine makes Python Docker builds 50\u00d7 slower.",
    "Visual Studio with docker file support - no such file or directory": "This is my fault. I should run this command from root project directory:\ndocker build -f \"DockerTest/Dockerfile\" .",
    "Pass command line args to Java app (Spring Boot) running in Docker": "You can provide all command line arguments just after name of your docker image in run command.\nExample:\ndocker run -p 8080:8080 test-image --recipient=\"World\"--greeting=\"Hello\"",
    "python based Dockerfile throws locale.Error: unsupported locale setting": "What I would do for Debian based docker image:\nFROM python:3.7.5\n\nRUN apt-get update && \\\n    apt-get install -y locales && \\\n    sed -i -e 's/# ru_RU.UTF-8 UTF-8/ru_RU.UTF-8 UTF-8/' /etc/locale.gen && \\\n    dpkg-reconfigure --frontend=noninteractive locales\n\nENV LANG ru_RU.UTF-8\nENV LC_ALL ru_RU.UTF-8\nand then in python:\nimport locale\n\nlocale.setlocale(locale.LC_ALL,'ru_RU.UTF-8')",
    "Cron and Crontab files not executed in Docker": "Cron (at least in Debian) does not execute crontabs with more than 1 hardlink, see bug 647193. As Docker uses overlays, it results with more than one link to the file, so you have to touch it in your startup script, so the link is severed:\ntouch /etc/crontab /etc/cron.*/*",
    "Docker build failed to compute cache key": "Thanks to BMitch, I found my problem. The problem is the dockerignore file contains some pattern that must not match with COPY files name.\nMy problem is the patterns inside the .dockerignore file is matched wrongly with bin/app.publish. To resolve my problem I just change the pattern in dockerignore.\nSpecifically, remove the line **/bin for .dockerignore.",
    "Docker environment variables in multi-stage builds": "So this is not a multi-stage issue.\nIt appears ENV variables are only used when running containers (docker-compose up). Not at build time (docker-compose build). So you have to use arguments:\n.env:\nTEST=11111\ndocker-compose.yaml:\nversion: '3'\nservices:\n  test:\n    build:\n      context: .\n      args:\n        TEST: ${TEST}\nDockerfile:\nFROM nginx:alpine\nARG TEST\nENV TEST ${TEST}\nCMD [\"sh\", \"-c\", \"echo $TEST\"]\ntest command:\ndocker rmi test_test:latest ; docker-compose build && docker run -it --rm test_test:latest\nSeriously the documentation is somewhat lacking.\nReference: https://github.com/docker/compose/issues/1837",
    "Bust cache bust within Dockerfile without providing external build args": "Update: Reviewing this one, it looks like you injected the cache busting option incorrectly in two ways:\nENV is not an ARG\nThe $(x) syntax is not a variable expansion, you need curly brackets (${}), not parenthesis ($()).\nTo break the cache on the next run line, the syntax is:\nARG CACHE_BUST\nRUN echo \"command with external dependencies\"\nAnd then build with:\ndocker build --build-arg CACHE_BUST=$(date +%s) .\nWhy does that work? Because during the build, the values for ARG are injected into RUN commands as environment variables. Changing an environment variable results in a cache miss on the new build.\nTo bust the cache, one of the inputs needs to change. If the command being run is the same, the cache will be reused even if the command has external dependencies that have changed, since docker cannot see those external dependencies.\nOptions to work around this include:\nPassing a build arg that changes (e.g. setting it to a date stamp).\nChanging a file that gets included into the image with COPY or ADD.\nRunning your build with the --no-cache option.\nSince you do not want to do option 1, there is a way to do option 3 on a specific line, but only if you can split up your Dockerfile into 2 parts. The first Dockerfile has all the lines as you have today up to the point you want to break the cache. Then the second Dockerfile has a FROM line to depend on the first Dockerfile, and you build that with the --no-cache option. E.g.\nDockerfile1:\nFROM base\nRUN normal steps\nDockerfile2\nFROM intermediate\nRUN curl external.jar>file.jar\nRUN other lines that cannot be cached\nCMD your cmd\nThen build with:\ndocker build -f Dockerfile1 -t intermediate .\ndocker build -f Dockerfile2 -t final --no-cache .\nThe only other option I can think of is to make a new frontend with BuildKit that allows you to inject an explicit cache break, or unique variable that results in a cache break.",
    "CMD doesn't run after ENTRYPOINT in Dockerfile": "As documented in https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact, if you combine the \"shell form\" of CMD and ENTRYPOINT, the CMD specification is ignored:\nSo you should rather use the \"exec form\" and write something like this:\n\u2026\nENTRYPOINT [\"/usr/bin/chamber\", \"exec\", \"${ENV}_${SERVICE_NAME}\", \"-r\", \"1\", \"--\", \"./entrypoint.sh\"]\nCMD [\"java -jar\", \"-Dspring.profiles.active=docker\", \"target/my.jar\"]\nHowever this won't work as is, because the ${ENV} and ${SERVICE_NAME} won't be expanded (as a shell would be required).\nSo the simplest, proper solution to apply here is to refactor your entrypoint.sh, or if ever you don't want to change it and still rely on environment variables with an \"exec form\" ENTRYPOINT, you could write instead:\n\u2026\nRUN chmod a+x entrypoint1.sh\nENTRYPOINT [\"./entrypoint1.sh\"]\nCMD [\"java -jar\", \"-Dspring.profiles.active=docker\", \"target/my.jar\"]\nwith a file\nentrypoint1.sh\n#!/bin/bash\nexec /usr/bin/chamber exec ${ENV}_${SERVICE_NAME} -r 1 -- ./entrypoint.sh \"$@\"",
    "How can I edit an existing docker image metadata?": "Its a bit hacky, but works:\nSave the image to a tar.gz file:\n$ docker save [image] > [targetfile.tar.gz]\nExtract the tar file to get access to the raw image data:\ntar -xvzf [targetfile.tar.gz]\nLookup the image metadata file in the manifest.json file: There should be a key like .Config which contains a [HEX] number. There should be an exact [HEX].json in the root of the extracted folder.\nThis is the file containing the image metadata. Edit as you like.\nPack the extracted files back into an new.tar.gz-archive\nUse cat [new.tar.gz] | docker load to re-import the modified image\nUse docker inspect [image] to verify your metadata changes have been applied\nEDIT: This has been wrapped into a handy script: https://github.com/gdraheim/docker-copyedit",
    "Docker: reload a nodejs app with nodemon": "Ah, the problem seems to be with docker-compose.yml volumes param. Changing it to\nvolumes:\n  - .:/opt/app\nmakes it work. It tells docker to mount the application folder . on the host to the /opt/app in the container.\nThen after doing docker-compose up, the server restarts in case of file changes.",
    "Putting a python script into a docker container": "CMD SOLUTION\nI would recommend switching from Entrypoint to CMD\nCMD [ \"python\", \"./my_script.py\" ]\nThis method can be seen in depth here: https://runnable.com/docker/python/dockerize-your-python-application\nSome more complexity (flags etc) can also be handled with CMD as can be seen here : how to pass command line arguments to a python script running in docker\nENTRYPOINT SOLUTION\nENTRYPOINT [\"python\", \"app.py\"]\nThis style of solution is explained in depth here: https://lostechies.com/gabrielschenker/2016/08/21/container-entrypoint/\nThe difference between the two (if you're curious and don't know)\nCMD commands can be overwritten from the command line. CMD is effectively the default value of your container's command.\nENTRYPOINT commands are not overwritten from the command line.\nCMD and ENTRYPOINT are similar, but I prefer command because it enables me to change the flags or command at run time if preferred, while keeping the same dockerfile that can be run without a command if desired.\nHere is a longer form discussion of the difference: http://goinbigdata.com/docker-run-vs-cmd-vs-entrypoint/",
    "Running redis on nodejs Docker image": "The best solution would be to use docker compose. With this you would create a redis container, link to it then start your node.js app. First thing would be to install docker compose detailed here - (https://docs.docker.com/compose/install/).\nOnce you have it up and running, You should create a docker-compose.yml in the same folder as your app's dockerfile. It should contain the following\nversion: '3'\nservices:\n  myapp:\n    build: .  \n    ports:\n     - \"3011:3011\"\n    links:\n     - redis:redis\n  redis:\n    image: \"redis:alpine\"\nThen redis will be accessible from your node.js app but instead of localhost:6379 you would use redis:6379 to access the redis instance.\nTo start your app you would run docker-compose up, in your terminal. Best practice would be to use a network instead of links but this was made for simplicity.\nThis can also be done as desired, having both redis and node.js on the same image, the following Dockerfile should work, it is based off what is in the question:\nFROM node:carbon\n\nRUN wget http://download.redis.io/redis-stable.tar.gz && \\\n    tar xvzf redis-stable.tar.gz && \\\n    cd redis-stable && \\\n    make && \\\n    mv src/redis-server /usr/bin/ && \\\n    cd .. && \\\n    rm -r redis-stable && \\\n    npm install -g concurrently   \n\nEXPOSE 6379\n\nWORKDIR /app\n\nCOPY package.json /app\n\nRUN npm install\n\nCOPY . /app\n\nEXPOSE 3011\n\nEXPOSE 6379\n\nCMD concurrently \"/usr/bin/redis-server --bind '0.0.0.0'\" \"sleep 5s; node /app/src/server.js\" \nThis second method is really bad practice and I have used concurrently instead of supervisor or similar tool for simplicity. The sleep in the CMD is to allow redis to start before the app is actually launched, you should adjust it to what suits you best. Hope this helps and that you use the first method as it is much better practice",
    "Using Docker Buildkit --mount=type=cache for caching Nuget packages for .NET 5 dockerfile": "The key is using the same --mount=type=cache argument in all of the dockerfile RUN commands that need access to the same package cache (e.g. docker restore, docker build, docker publish).\nHere's a short dockerfile example showing the same --mount=type=cache with the same id spread across separate dotnet restore/build/publish invocations. Separating the calls isn't always necessary as build will restore by default and publish will do both, but this way shows sharing the same cache across multiple commands. The cache mount declarations only appear in the dockerfile itself and don't require arguments in docker build.\nThe example also shows how you might use the BuildKit --mount=type=secret argument to pass in a NuGet.Config file that may be configured to access e.g. a private nuget feed. By default, secret files passed in this way appear in /run/secrets/<secret-id>, but you can change where they go via the target attribute in the docker build command. They only exist during the RUN invocation and don't remain in the final image.\n# syntax=docker/dockerfile:1.2\nFROM my-dotnet-sdk-image as builder\nWORKDIR \"/src\"\nCOPY \"path/to/project/src\" .\n\nRUN --mount=type=cache,id=nuget,target=/root/.nuget/packages \\\n    --mount=type=secret,id=nugetconfig \\\n    dotnet restore \"MyProject.csproj\" \\\n    --configfile /run/secrets/nugetconfig \\\n    --runtime linux-x64\n\nRUN --mount=type=cache,id=nuget,target=/root/.nuget/packages \\\n    dotnet build \"MyProject.csproj\" \\\n    --no-restore \\\n    --configuration Release \\\n    --framework netcoreapp3.1 \\\n    --runtime linux-x64\n\nRUN --mount=type=cache,id=nuget,target=/root/.nuget/packages \\\n    dotnet publish \"MyProject.csproj\" \\\n    --no-restore \\\n    --no-build \\\n    -p:PublishReadyToRun=true \\\n    -p:PublishReadyToRunShowWarnings=true \\\n    -p:TieredCompilation=false \\\n    -p:TieredCompilationQuickJit=false \\\n    --configuration Release \\\n    --framework netcoreapp3.1 \\\n    --runtime linux-x64\nA sample docker build command to pass in the nugetconfig file for a private feed might be:\ndocker build --secret id=nugetconfig,src=path/to/nuget.config -t my-dotnet-image .\nFor that command, the environment variable DOCKER_BUILDKIT=1 needs to be set.\nAlternatively, you can use buildx:\ndocker buildx build --secret id=nugetconfig,src=path/to/nuget.config -t my-dotnet-image .",
    "Why doesn't the cron service in Dockerfile run?": "Having started crond with supervisor, your cron jobs should be executed. Here are the troubleshooting steps you can take to make sure cron is running\nIs the cron daemon running in the container? Login to the container and run ps a | grep cron to find out. Use docker exec -ti CONTAINERID  /bin/bash to login to the container.\nIs supervisord running?\nIn my setup for instance, the following supervisor configuration works without a problem. The image is ubuntu:14.04. I have CMD [\"/usr/bin/supervisord\"] in the Dockerfile.\n[supervisord]\n nodaemon=true\n[program:crond]\n command = /usr/sbin/cron\n user = root\n autostart = true\nTry another simple cron job to findout whether the problem is your cron entry or the cron daemon. Add this when logged in to the container with crontab -e :\n* * * * * echo \"hi there\" >> /tmp/test\nCheck the container logs for any further information on cron:\ndocker logs CONTAINERID | grep -i cron\nThese are just a few troubleshooting tips you can follow.",
    "What is the programming language used in dockerfile and docker-compose files": "Docker is written in the GO language\nA Dockerfile is just a text file. It is a script that contains collections of commands and instructions that will be automatically executed in sequence in the docker environment for building a new docker image.\nA docker-compose.yml file is used for docker-compose, if you are using that feature of Docker.",
    "how to pass argument to dockerfile from a file": "Then you can use --build-arg, it will pass parameters with --build-arg key=value to dockerfile when build, refer to this.\nYou just need to use sed to get from your env file & combine them to the format --build-arg key=value when build the dockerfile, example as next:\nDockefile:\nFROM ubuntu:16.04\n\nARG BuildMode\nENV BuildMode=${BuildMode}\n\nRUN echo $BuildMode\ndocker.env:\nBuildMode=\"release\"\nCommand:\ndocker build -t abc:1 $(cat docker.env | sed 's@^@--build-arg @g' | paste -s -d \" \") . --no-cache\nOutput:\nshubuntu1@shubuntu1:~/1$ docker build -t abc:1 $(cat docker.env | sed 's@^@--build-arg @g' | paste -s -d \" \") . --no-cache\nSending build context to Docker daemon  3.072kB\nStep 1/4 : FROM ubuntu:16.04\n ---> 13c9f1285025\nStep 2/4 : ARG BuildMode\n ---> Running in 3bc49fbb0af4\nRemoving intermediate container 3bc49fbb0af4\nStep 3/4 : ENV BuildMode=${BuildMode}\n ---> Running in 4c253fba0b36\nRemoving intermediate container 4c253fba0b36\n ---> c70f7f535d1f\nStep 4/4 : RUN echo $BuildMode\n ---> Running in 5fef72f28975\n\"release\"\nRemoving intermediate container 5fef72f28975\n ---> 4b5555223b5b\nSuccessfully built 4b5555223b5b\nSuccessfully tagged abc:1",
    "dockerize a wpf application and use it": "You cannot run a WPF application in docker.\nHere is a snippet of the Microsoft docs\nDocker is for server applications\u2014Web sites, APIs, messaging solutions and other components that run in the background. You can\u2019t run desktop apps in Docker because there\u2019s no UI integration between the Docker platform and the Windows host. That rules out running Windows Forms or Windows Presentation Foundation (WPF) apps in containers (although you could use Docker to package and distribute those desktop apps), but Windows Communication Foundation (WCF), .NET console apps and all flavors of ASP.NET are great candidates.\nCheck out the source",
    "How to run Redis on docker with a different configuration file?": "The run command:\ndocker run --name my-redis -p 0.0.0.0:6379:6379 -d ouruser/redis redis-server --appendonly yes\nOverrides the CMD defined in the Dockerfile with redis-server --appendonly yes, so your conf file will be being ignored. Just add the path to the conf file into your run command:\ndocker run --name my-redis -p 0.0.0.0:6379:6379 -d ouruser/redis redis-server /usr/local/etc/redis/redis.conf --appendonly yes\nAlternatively, set up an entrypoint script or add --appendonly yes to the CMD instruction.",
    "Docker --ssh default Permission denied (publickey)": "Docker is not copying the file from ~/.ssh/.\nWhen using the default configuration --ssh default you need to add your keys to your local SSH agent.\nYou can check ssh-add -L locally to see if the public keys are visible to the agent.\nIf they are not, try to run ssh-add -K.\nReferences:\nhttps://medium.com/@tonistiigi/build-secrets-and-ssh-forwarding-in-docker-18-09-ae8161d066\nhttps://apple.stackexchange.com/questions/254468/macos-sierra-doesn-t-seem-to-remember-ssh-keys-between-reboots",
    "How to run two commands on Dockerfile? [duplicate]": "Try creating a script like this:\n#!/bin/sh\nnginx -g 'daemon off;' & \ngulp watch-dev\nAnd then execute it in your CMD:\nCMD /bin/my-script.sh\nAlso, notice your last line would not have worked:\nCMD [\"gulp watch-dev\"]\nIt needed to be either:\nCMD gulp watch-dev\nor:\nCMD [\"gulp\", \"watch-dev\"]\nAlso, notice that RUN is for executing a command that will change your image state (like RUN apt install curl), not for executing a program that needs to be running when you run your container. From the docs:\nThe RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.",
    "Docker - Build Arg in Source File": "According to the docs, the first instruction needs to be FROM (or technically a parser directive, but not relevant here) so this approach likely isn't going to work. Probably some shell wrapper around docker build... with some sed command or something to insert the correct version, or a template of some kind.\nGareth Rushgrove had a nice talk at DockerCon16 on image build tooling that might be interesting.\nUpdate (7/2/17): This is now possible to achieve since v17.06.",
    "How to kill used port of docker": "To expand on Robert Moskal's answer, you'll need to kill whatever's already on that port:\nkill all the containers again\nif you're on Linux, kill the process running on your port with fuser -k 9042/tcp\nif above steps don't work, reboot your computer and try again.\nHappy hunting!",
    "UserWarning: Supervisord is running as root and it is searching for its configuration file in default locations": "By design:\ndocker containers run as root if no USER is specified\nsupervisor does not allow running the daemon as root, without specifying it explicitly in the config files.\nSo you can either run supervisor as a user other than root or just add the user=root directive into the configs.\n[supervisord]\nnodaemon=true\nuser=root\n\n\n[program:uwsgi]\ncommand = /usr/local/bin/uwsgi --ini /trell-ds-framework/uwsgi.ini\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\nuser=root\n\n[program:nginx]\ncommand = /usr/sbin/nginx -g \"daemon off;\"\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes=0\nstderr_logfile=/dev/stderr\nstderr_logfile_maxbytes=0\ndaemon=off\nuser=root ;here too if you want to",
    "How to build a docker image for M1 Mac Silicon or AMD conditionally from Dockerfile?": "I believe you can use the --platform parameter on docker buildx build or docker build to set platform(s) to build the image which will be used within any FROM calls within the Dockerfile if nothing else is specified (see Dockerfile FROM), as mentioned in the documentation.\nYou can then use the TARGETPLATFORM variable within your Dockerfile to get what platform it's being built for if needed. If you want to change the default platform to build for, you can set the DOCKER_DEFAULT_PLATFORM environment variable.",
    "What is the default user for Linux Alpine?": "I believe USER root is correct. The default docker user is typically root.\nThe closest reference I could find is in the Docker docs for the USER command:\nhttps://docs.docker.com/engine/reference/builder/#user\nWarning: When the user doesn\u2019t have a primary group then the image (or the next instructions) will be run with the root group.\nHowever, it very easy to find for yourself, using a simple Dockerfile:\nFROM python:3.6-alpine\nRUN whoami\nWill output (either ifdocker is started as root or as a Docker previliged user):\nStep 2/2 : RUN whoami #\n ---> Running in 3a8d159404cd\nroot",
    "Dockerfile with copy command and relative path": "Reference: Allow Dockerfile from outside build-context\nYou can try this way\n$ cd project-console\n$ docker build -f ../project-test/Dockerfile .\nUpdate:\nBy using docker-compose\nbuild:\n  context: ../\n  dockerfile: project-test/Dockerfile\n../ will be set as the context, it should include project-console and project-test in your case. So that you can COPY project-console/*.csproj in Dockerfile.",
    "Can we include git commands in docker image?": "Yes, you can. Let me recommend some things about that.\nDefine a git token in github associated to a generic user. I like to give only read permissions to that user.\nDeclare some ARGs related to git in your Dockerfile, so you can customize your build.\nAdd Git installation to your Dockerfile.\nDo git clone cloning only needed folders, not the whole repo.\nSo, your Dockerfile could be, for example for debian/ubuntu:\nFROM <your linux distribution>\nARG GIT_USER\nARG GIT_TOKEN\nARG GIT_COMMIT\nARG SW_VERSION\nRUN apt-get update\nRUN apt-get -y install git\nRUN git clone -n https://${GIT_USER}:${GIT_TOKEN}@github.com/<your git>/your_git.git --depth 1 && cd <your_git> && git checkout ${GIT_COMMIT} <dir_you_want_to_checkout>\nCOPY myapp/ /app\nCMD /app/entry.pl /<your_git>/<dir_you_want_to_checkout>/...\nThen, you can build as you know with:\ndocker build -t --build-arg GIT_USER=<your_user> --build-arg GIT_TOKEN=d0e4467c63... --build-arg GIT_COMMIT=a14dc9f454... <your_image_name> .\nGood luck.",
    "is it a good practice to put a war file image into docker containers?": "One of the main reasons for building a docker image for your application is to provide an artefact that people can run without installing and managing external software dependencies (like an app server to run a war file).\nFrom a container user perspective, it makes no difference whether you package your code in a jar or a war file or as a fortran binary. All they do is run a container image.\nFrom your perspective, of doing the Docker build and config management, the packaging of a jar file and copying into a container would be a simpler solution than trying to setup and configure an app server for packaging, then deploying each release into the app server. That being said, users would probably prefer the more complete app server solution, because that may be easier for them the run the application.\nSee Ohmens answer for some more technical components of the java build process",
    "Dockerfile - What are the intermediate containers doing exactly?": "My guess is that the /etc/kafka-connect/jars directory is declared as a VOLUME in that image's Dockerfile.\nAnd the output of the docker inspect command confirms my guess:\n$ docker image inspect confluentinc/cp-kafka-connect:4.0.0 --format '{{.Config.Volumes}}'\nmap[/etc/kafka-connect/secrets:{} /etc/kafka/secrets:{} /var/lib/kafka/data:{} /etc/kafka-connect/jars:{}]\nCiting from The Dockerfile Specification:\nIf any build steps change the data within the volume after it has been declared, those changes will be discarded.\nSo, here are the details about your problem:\nThe base image declares VOLUME /etc/kafka-connect/jars.\nIn step 3 of your Dockerfile, you changed the contents of that directory. That's why the ls command in this step works normally.\nThen these changes are discarded.\nThe solution is to put the jar files on your host, and bind-mount the host directory to the container when running the container. As below:\ndocker run -v /path/contains/jar/files:/etc/kafka-connect/jars <IMAGE>",
    "Spring Boot in Docker": "You can use docker run Using Spring Profiles. Running your freshly minted Docker image with Spring profiles is as easy as passing an environment variable to the Docker run command\n$ docker run -e \"SPRING_PROFILES_ACTIVE=prod\" -p 8080:8080 -t springio/gs-spring-boot-docker\nYou can also debug the application in a Docker container. To debug the application JPDA Transport can can be used. So we\u2019ll treat the container like a remote server. To enable this feature pass a java agent settings in JAVA_OPTS variable and map agent\u2019s port to localhost during a container run.\n$ docker run -e \"JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,address=5005,server=y,suspend=n\" -p 8080:8080 -p 5005:5005 -t springio/gs-spring-boot-docker\nResource Link: Spring Boot with Docker\nUsing spring profile with docker for nightly and dev build:\nSimply set the environment varialbe SPRING_PROFILES_ACTIVE when starting the container. This will switch the active of the Spring Application.\nThe following two lines will start the latest Planets dev build on port 8081 and the nightly build on port 8080.\ndocker run -d -p 8080:8080 -e \\\"SPRING_PROFILES_ACTIVE=nightly\\\" --name nightly-planets-server planets/server:nightly\ndocker run -d -p 8081:8080 -e \\\"SPRING_PROFILES_ACTIVE=dev\\\" --name dev-planets-server planets/server:latest\nThis can be done automatically from a CI system. The dev server contains the latest build and nightly will be deployed once a day...",
    "How does escape character work in a Dockerfile?": "Use the escape directive on Windows to avoid these headaches, e.g.:\n# escape=`\n\nFROM microsoft/nanoserver\nCOPY testfile.txt c:\\\nRUN dir c:\\\nIn your case, the second slash is escaping the newline. Therefore two lines are running together to form: COPY testfile.txt c:\\RUN dir c:. I understand you're thinking the first slash should escape the second, but that's not how the parser behaves according to the documentation.",
    "Docker : failed to compute cache key": "In my case I missed the folder entry in .dockerignore file. Do something like that.\n**/*\n!docker-images\n!configs\n!main",
    "Dockerfile build ARG in COPY --from=": "At last check, you can't use a build arg there, but you can use it in a top level from line using the multi-stage syntax. Then, you also need to define the build arg at the top level:\nARG BUILD_CONFIG=dev\nFROM test/test-library:${BUILD_CONFIG} as test-library\n\nFROM node:12.15:0 as prod\nARG BUILD_CONFIG\nRUN echo ${BUILD_CONFIG}\n\nCOPY --from=test-library /work/dist /work/library/dist\nCMD[ \"bash\" ]",
    "Problem with connecting mongo express service in docker": "In your docker-compose file, you call the mongo service mongo. That's the name you can address it as on the docker network. In your connection string, you've said\nmongodb://root:pass@mongodb:27017/\"\nIt should be\nmongo://root:pass@mongo:27017/\"\nSince the connection string is built using the environment variables in your docker-compose file, the thing to change is\n- ME_CONFIG_MONGODB_SERVER=mongodb\nto\n- ME_CONFIG_MONGODB_SERVER=mongo",
    "install PyTorch CPU-only in Dockerfile": "Installing it via pip should work:\nRUN pip3 install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html",
    "Build a Docker image using docker build and a tmpfs?": "With BuildKit, you can use experimental features to mount a tmpfs filesystem for a single RUN line. This filesystem will not exist outside of the context of that RUN line, just as a tmpfs does not exist when a container has been stopped or deleted, so you'll need to copy any artifacts back into the container filesystem at the end of your build.\nFor BuildKit, you need at least 18.09, and you can enable it by either:\n    export DOCKER_BUILDKIT=1\nfor a single shell, or to change the default on the host you can update /etc/docker/daemon.json with:\n    {\n      \"features\": {\"buildkit\": true}\n    }\nWith BuildKit enabled, the Dockerfile would look like:\n    # syntax=docker/dockerfile:experimental\n    FROM your_base_image\n    COPY src /src\n    RUN --mount=type=tmpfs,target=/build \\\n        cp -r /src/. /build/ \\\n     && cd /build \\\n     && make your_project \\\n     && cp /build/result.bin /result.bin\n    ...\nNote that BuildKit is rather new, won't be supported in most cloud build environments, and isn't supported from docker-compose in older versions either. To see more on these experimental features, see: https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/reference.md\nDocker-compose:\nThe support for BuildKit within docker-compose was released in v1.25.0 In order to enable it, do the following configurations:\n    export DOCKER_BUILDKIT=1 # or configure in daemon.json\n    export COMPOSE_DOCKER_CLI_BUILD=1\nWith those variables set in your shell, you can now run docker-compose build using BuildKit.\nIn windows you can execute in your console:\n    setx DOCKER_BUILDKIT 1 # or configure in daemon.json\n    setx COMPOSE_DOCKER_CLI_BUILD 1\nafter will need restart your console",
    "Does LABEL add new layer to docker image or apply to all layers, and does it matter where LABEL is place in dockerfile?": "I've performed a few tests using a very simple Dockerfile, and adding a LABEL doesn't add any layers, but it does change the image's sha256 id.\nAny change to the label's value will change the sha256 and even moving it to a different part of the Dockerfile will too, but the layers remain unaffected by the label's presence or value.\nAs for whether a LABEL \"applies to all layers\", I suppose we could conclude that the label is applicable to the overall image but has no effect on its layers.",
    "can't find a suitable configuration file in this directory or any docker compose (docker-compose section not getting added in solution)": "Not really an expert on VS 2017, but you would need to create a docker-compose.yml file inside of the project.\nMore information can be found here.\nversion: '3.5'\nservices:\n  client:\n    container_name: WebApplication \n    build:\n      dockerfile: Dockerfile\n    volumes:\n        # Any volumes you would mount goes here\n    ports:\n        # Ports that need to be mapped from container to host goes here\n    environment:\n        # Any environment variables\nEdit: In order for VS 2017 to automatically create docker-compose.yml, you will need to right-click on the Project and select Add > Orchestrator Support (15.8 and greater) or Add > Docker Project Support (below 15.8).",
    "Transfer virtualenv to docker image": "We had run into this problem earlier and here are a few things we considered:\nConsider building base images that have common packages installed. The app containers can then use the one of these base containers and install the differential.\nCache the Pip packages on a local path that can be mounted on the container. That'll save the time to download the packages.\nDepending on the complexity of your project one may suit better than the other - you may also consider a hybrid approach to find maximum optimization.",
    "How to fix read-only error when adding host in Docker? [duplicate]": "From the docker docs page on /etc/hosts, they say docker itself may update the file.\nSince Docker may live update the container\u2019s /etc/hosts file, there may be situations when processes inside the container can end up reading an empty or incomplete /etc/hosts file. In most cases, retrying the read again should fix the problem.\nSo because docker may rewrite the file, any changes you make from inside the image/container would be lost. To prevent this, they don't let you change it at all, so you get the read-only FS error.",
    "why is my docker container ASP.NET core app not available after ending debugging in Visual Studio": "During the debugging, if you run this command:\ndocker exec -it containerName bash -c 'pidof dotnet'\nYou will noticed, that the dotnet process is running, and when you stop the debugging and run it again, you are going to see that, the process was finalized.\nIf you want to start your application in the container, without run the debugger again, you just need to run the start the dotnet process inside the container.\nYou could do that, running a script like this:\n#Set these 3 variables\n$containerName = \"MyContainer\"\n$appDll = \"myApp.dll\"\n$appDirectory = \"/app/bin/debug/netcoreapp3.1\"\n\n$args = \"/c docker exec -it $containerName bash -c 'cd $appDirectory;dotnet $appDll'\"\nStart-Process -FilePath \"powershell\" -ArgumentList $args -NoNewWindow\nYou can check if it worked, by running this script again:\ndocker exec -it containerName bash -c 'pidof dotnet'",
    "How to use the (short) git commit hash to tag image versions in Docker Compose file": "The build ARGS section illustrates jonrsharpe's comment\nYou need to set an environment variable first, and declare the ARGS in your docker-compose.yml\nARG commit\n...\nimage: \"myapp:${commit}\"\nSee \"variable substitution\" and also The \u201cenv_file\u201d configuration option\nYour configuration options can contain environment variables.\nCompose uses the variable values from the shell environment in which docker-compose is run.\nAny hope of running a command directly in the docker-compose.yml file was ruled out with docker/compose issue 4081.",
    "Can't connect to mongodb in the docker container": "Use host.docker.internal with exposed port : host.docker.internal:27017",
    "install mongoose in docker container": "This is just an output from node-gyp. You can ignore this messages if you don't use the MongoDB Enterprise with Kerberos Authentication.\nNevertheless the docker build command will run successfully and mongoose will also work.\nThe output above is just about some pragam directives. The pragma statement was introduced with ANSI-C to define compiler options.\nFor example have a look at:\n../lib/kerberosgss.c: In function 'authenticate_gss_client_wrap':\n../lib/kerberosgss.c:348:19: warning: variable 'server_conf_flags' set but not used [-Wunused-but-set-variable]\nchar buf[4096], server_conf_flags;\nThis just tells you, that the variable server_conf_flags defined in lib/kerberosgss.c:348:19 is not used anywhere. If you look at the source on github, this is not a problem.\nEach C-compiler handles these pragam directives slightly different which is intentionally. Maybe on your local machine you have got a different C-compiler or a completely different OS?\nSo this is nothing to worry about.",
    "Docker Compose: Stop \"depends_on\" service when \"parent\" finishes": "I do not believe this is a built in feature of docker-compose. But you can easily script a solution, e.g.:\ndocker-compose up -d db\ndocker-compose up test-backend\ndocker-compose down db\nOutside of docker-compose, there's also the docker container wait command that lets you hang until a container exits. You can lookup the container name with a docker container ls and filters on a label or other unique value, or you can rely on the predictable container names from compose ($project_$service_$replica). The docker container wait command would allow you to spin up the entire project in a detached state and then wait for one container to exit before proceeding to stop it, e.g.:\ndocker-compose up -d\ndocker container wait $(basename $(pwd))_test-backend_1\ndocker-compose down db",
    "supervisor.sock refused connection in docker container": "It's also worth checking that supervisor is actually running\nsupervisord\nRunning this will try and start supervisor, or throws an error if it is already running. For me, it's always a stale connection which breaks supervisor",
    "Run dotnet 1.1 using docker": "I changed my dockerfile to have the following COPY statement:\nCOPY out ./\nThat then made the entrypoint to work because it was then able to find out myApp.dll. I think the message could be improved here, but that's me assuming that's what happened",
    "Docker-Compose Issue with container name": "No, you can't have two containers with the same name. You'll have to pick a different name for the container_name field. The previous container needs to be removed before you can re-use the name.\nIf you wanted Compose to treat the container as if it had created it, you have to set the container labels as Compose does. The easiest way to find these would be to have compose create a container (probably by removing the container_name field), then using docker inspect to view the labels.",
    "Can Docker help build executable that work in different platform": "You still need to compile source code on different platforms. The point of the docker is to automate building and testing the code on every platform, so that you can just work on the code, and let it build and test on every platform.\nYou do have to set up the dockers and test scripts and get the code working cross-platform in the first place. But after that is done, you can basically not worry about any other platform unless you actually break it.",
    "How can I improve build time of Angular 5 project using Docker?": "[TL;DR]\nUse volumes to store node_modules and .npm\nParallelize parts of your process (e.g. tests)\nBe careful when using relative paths\nDo not copy your entire project with COPY . . . Relative path issues and possible information leaks.\nCreate a separate image containing only core dependencies for building and testing (e.g. npm, java, chrome-driver, libgconf2).\nConfigure pipelines to use this image\nLet the CI clone the repo and copy your project into the container for building and testing\nArchive built files (e.g. dist) and tag based on failure rates\nCreate a new image with just enough things to run your built files.\n[LONG VERSION]\nThere is a good chance that your npm dependencies are being re-downloaded and/or your docker images are being rebuilt for every build you run.\nRather than copying files into a docker image, it would be better to mount volumes for modules and cache so that additional dependencies included later doesn't need to be downloaded again. Typical directories that you should consider creating volumes for are npm_modules (one for global and one for local) and .npm (cache).\nYour package.json is being copied into root / and the same package.json is being copied into /web with COPY . ..\nThe initial run of npm i is installing into / and you're running it again for /web. You're downloading dependencies twice but are the modules in / going to be used for anything? Regardless, you appear to be using the same package.json in both npm i and ng build, so the same thing is being done twice, ( [EDIT] - It would seem that ng build doesn't redownload packages) but node_modules isn't available in / so the npm i command creates another one and re-downloads all packages.\nYou create a web directory in root / but there are other commands instructing to relative paths ./web. Are you certain that things are running in the right places? There is no guarantee that programs would be looking in the directories you want them to if you use relative paths. While it may appear to work for this image, the same practice will not be consistent across other images that may have different initial work directories.\n[may or may not be relevant information]\nAlthough I'm not using Bitbucket for automating builds, I faced a similar issue when running Jenkins pipelines. Jenkins placed the project in a different directory so that every time it runs, all the dependencies would be downloaded again. I initially thought the project would be in /home/agent/project but it was actually placed elsewhere. I found the directory where the project was copied to by using the pwd and npm cache verify command in a build step, then mounted the volumes to the correct places. You can view the output in the logs generated on builds.\nYou can view the output by expanding the section within the pipelines page.\nIf the image is being rebuilt on every run, build your image separately then push the image to a registry. Configure the pipeline file to use your image instead. You should try to use already available base images whenever possible unless there are other dependencies you need that are unavailable in the base image (things like alpine's apk packages and not npm. npm dependencies can be stored in volumes). If you're going to use a public registry, do not store any files that may contain sensitive data. Configure your pipeline so that things are mounted with volumes and/or uses secrets.\nA basic restructure on the test and build steps.\n       Image on Docker Hub\n              |\n              |\n           ---|-------------------------------------|\n           |                       |                |\n           V                       V                |\nCommit -- build (no test) ---> e2e tests (no build)-]--+--> archive build --> (deploy/merge/etc)\n                         |           _______________|  ^\n                         |           v                 |\n                         |-> unit tests (no build)---->|\nYou don't need to follow it entirely, but it should give you an idea on how you could use parallel steps to separate things and improve completion times.",
    "Loop/Iterate in Dockerfile": "In your case I would prepare a new plugins folder together with a (possibly generated or handcrafted) script that installs them. Add the real plugins folder to .dockerignore. Then you copy in the new folder and run the install script. This also reduces the size of the context you upload to docker before the build starts. You are hand picking the dependencies anyway, so doing the work beforehand (before you build) should work fine.\nIn the build system you you should do something like:\ncollect_plugins.sh # Creates the new plugin folder and install script\ndocker build <params>\nThe two layers could then be:\nCOPY plugins_selected/ /usr/lib/myapp/plugins/\nRUN  /usr/lib/myapp/plugins/install.sh\nIf you prepare the context you are sending to docker it will make the Dockerfile a lot more simple (a good thing). We are simply solving the problem before build.\nNormally dependencies are fetched over the network using a package manager or simply by downloading them over http(s). Copying them into the build context like you are doing is not necessarily wrong, but it gets a bit more awkward.\nLet's look at how docker processes the Dockerfile (slightly simiplified).\nWhen you build, docker will upload all the files in the context to docker engine (except the paths mentioned in .dockerignore) and start processing the Dockerfile. The purpose is to produce filesystem layers that will represent the final image.\nWhen you do operations like RUN, docker actually starts a container to execute the command(s) then adds the resulting layer to the image. The only thing that will actually run in production is what you specify in CMD and or ENTRYPOINT at the end of the Dockerfile.\nTry to create as few layers as possible.\nThe dockerfile best practices guide covers the basics.",
    "Handle EF Core migrations with container orchestration": "I'm not sure, if this is best practice, but you can trigger schema upgrade from your C# code. You must add following code to Configure method at Startup.cs:\nusing (var serviceScope = app.ApplicationServices\n    .GetRequiredService<IServiceScopeFactory>()\n    .CreateScope())\n{\n    using (var context = serviceScope.ServiceProvider.GetService<EasyAdminContext>())\n    {\n        context.Database.Migrate();\n    }\n}\nThis way the database will be updated when the application starts",
    "How to shrink size of Docker image with NodeJs": "You can certainly use my alpine-ng image if you like.\nYou can also check out the dockerfile, if you want to try and modify it in some way.\nI regret to inform you that even based on alpine, it is still 610MB. An improvement to be sure, but there is no getting around the fact that the angular compiler is grossly huge.",
    "Docker - How to get the name (user/repo:tag) of the base image used to build another image": "When you just have an image and want to recreate the Dockerfile, you can use dockerfile-from-image from\nhttps://github.com/CenturyLinkLabs/dockerfile-from-image/blob/master/dockerfile-from-image.rb\nit is some Ruby code (in a container, of course!) that find all the commands used.\nAnd, yes f35... is the id of DEBIAN:WHEEZY, as\ndocker run -v /var/run/docker.sock:/var/run/docker.sock   centurylink/dockerfile-from-image debian:wheezy\nshows\nADD file:f35a56605b9a065a14a18d0e36fdf55c1c381d3521b4fa7f11173f0025d36839 in /\nCMD [\"/bin/bash\"] `",
    "Create a user in a Dockerfile : Option d is ambiguous": "It should be RUN adduser --disabled-login myuser (or --disabled-password)",
    "Heroku run Docker image with port parameter": "The question is quite old now, but still I will write my answer here if it can be of some help to others.\nI have spring-boot App along with swagger-ui Dockerized and deployed on Heroku.\nThis is my application.yml looks like:\nserver:\n  port: ${PORT:8080}\n  forward-headers-strategy: framework\n  servlet:\n   contextPath: /my-app\n\nspringdoc:\n  swagger-ui:\n    path: '/swagger-ui.html'\nBelow is my DockerFile configuration.\nFROM maven:3.5-jdk-8 as maven_build\nWORKDIR /app\n\nCOPY pom.xml .\nRUN mvn clean package -Dmaven.main.skip -Dmaven.test.skip && rm -r target\n\nCOPY src ./src\nRUN mvn package spring-boot:repackage\n\n########run stage########\nFROM openjdk:8-jdk-alpine\nWORKDIR /app\nRUN apk add --no-cache bash\n\n\nCOPY --from=maven_build /app/target/springapp-1.1.1.jar ./\n\n#run the app\n# 256m was necessary for me, as I am using free version so Heroku was giving me memory quota limit exception therefore, I restricted the limit to 256m\nENV JAVA_OPTS \"-Xmx256m\"\nENTRYPOINT  [\"java\",\"${JAVA_OPTS}\", \"-jar\",\"-Dserver.port=${PORT}\", \"springapp-1.1.1.jar\"]\nThe commands I used to create the heroku app:\nheroku create\nheroku stack:set container\nThe commands I used to build image and deploy:\ndocker build -t app-image .\nheroku container:push web\nheroku container:release web\nFinally make sure on Heroku Dashboard the dyno information looks like this:\nweb java \\$\\{JAVA_OPTS\\} -jar -Dserver.port\\=\\$\\{PORT\\} springapp-1.1.1.jar\nAfter all these steps, I was able to access the swagger-ui via\nhttps://testapp.herokuapp.com/my-app/swagger-ui.html",
    "Docker: COPY failed: CreateFile, looking for file in strange location": "Seems to me the standard Dockerfile that comes with a new solution is bugged :/\nI moved the Dockerfile up to the solution folder, from PowerShell:\nmv Dockerfile ../Dockerfile\nRunning the docker build command from there did the trick for me...",
    "Deploy docker images to heroku": "Login in the Docker Registry before pushing\nheroku container:login",
    "How to start Nginx server within alpine image using rc-service command": "After debugging and lots of trial and error, I found a perfect solution at least for me, David Maze answer is very helpful, But I need to be able to restart the nginx service within the container.\nwhatever when I start my container using the following command :\ndocker run -it -p 80:80 -p 443:443 alpine:3.12.0\nwe access the container shell, and we need to download nginx server , and openrc to be able to use rc-service command line.\n/ # apk update\n/ # apk add nginx openrc\n#ANSWER 1\nnow we'll test If there is an error in nginx server by using the following command :\n/ # nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: [emerg] open() \"/run/nginx/nginx.pid\" failed (2: No such file or directory)\nnginx: configuration file /etc/nginx/nginx.conf test failed\nas you can see from the output that we get, It tells you that a missing file or directory shall be created, so let's create that directory :\n/ # ls -la run/\ntotal 8\ndrwxr-xr-x    2 root     root          4096 Dec 16 10:31 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:12 ..\n\n/ # mkdir /run/nginx/\nand we give that directory that we created some permissions :\n/ # chown -R nginx:nginx /run/nginx/\n/ # chmod 775 /run/nginx/\n/ # ls -la /run/\ntotal 12\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:15 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:12 ..\ndrwxrwxr-x    2 nginx    nginx         4096 Jan 16 08:15 nginx\nnow we are good with nginx :\n/ # nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: configuration file /etc/nginx/nginx.conf test is successful\nLet's test our nginx service is it started or not with rc-service command :\n/ # rc-service nginx status\n * You are attempting to run an openrc service on a\n * system which openrc did not boot.\n * You may be inside a chroot or you may have used\n * another initialization system to boot this system.\n * In this situation, you will get unpredictable results!\n * If you really want to do this, issue the following command:\n * touch /run/openrc/softlevel\nSo from the above output we see that we have two problems, openrc did not boot, and there is a missing file softlevel :\n/ # ls -la /run/\ntotal 12\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:15 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:12 ..\ndrwxrwxr-x    2 nginx    nginx         4096 Jan 16 08:22 nginx\nLet's start by booting our system with openrc simply by typing it itself :\n/ # openrc\n * Caching service dependencies ...\nService `hwdrivers' needs non existent service `dev'\n\n/ # ls -la /run/\ntotal 16\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:29 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:12 ..\ndrwxrwxr-x    2 nginx    nginx         4096 Jan 16 08:22 nginx\ndrwxr-xr-x   14 root     root          4096 Jan 16 08:29 openrc\n\n/ # ls -la /run/openrc/\ntotal 64\ndrwxr-xr-x   14 root     root          4096 Jan 16 08:29 .\ndrwxr-xr-x    1 root     root          4096 Jan 16 08:29 ..\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 daemons\n-rw-r--r--    1 root     root            11 Jan 16 08:29 depconfig\n-rw-r--r--    1 root     root          2895 Jan 16 08:29 deptree\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 exclusive\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 failed\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 hotplugged\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 inactive\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 options\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 scheduled\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 started\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 starting\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 stopping\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 tmp\ndrwxr-xr-x    2 root     root          4096 Jan 16 08:29 wasinactive\nnow we create the missing file :\n/ # touch /run/openrc/softlevel\nnow our rc-service command works perfectly :\n/ # rc-service nginx status\n * status: stopped\nlet's start our server :\n / # rc-service nginx start\n * Starting nginx ...          [ ok ]\ncheck if it is started or not :\n/ # rc-service nginx status\n * status: started\n#ANSWER 2\nOr you can simply call those two command lines instead :\n/ # openrc\n * Caching service dependencies ...\nService `hwdrivers' needs non existent service `dev'    [ ok ]\n\n/ # touch /run/openrc/softlevel\nNow you can start your nginx server :)\n/ # rc-service nginx status\n * status: stopped\n\n/ # rc-service nginx start\n * /run/nginx: creating directory\n * /run/nginx: correcting owner\n * Starting nginx ...         [ ok ]\n\n/ # rc-service nginx status\n * status: started\nHope I was clear.",
    "Docker: Installing python cryptography on alpine linux distribution": "Alpine is a headache distro for most Python packages that ship C/C++ extensions (code written in C/C++ that is compiled to a shared object and loaded in Python via a foreign function library). The reason for that is that is PEP 513 which portability definition between Linux distros, manylinux1, is based on glibc/glibcxx. Since Alpine uses musl libc, no manylinux1 compatible wheel can be installed on Alpine. So when you issue pip install cryptography, the wheel with the compiled extensions is filtered and pip tries to build the package with all the C extensions from source.\ninstalling with the system package manager\n\nThis is the preferred way and was mentioned by @GracefulRestart in the comments; use it if you don't need the bleeding edge version of the package. Install it with apk:\n$ apk add py-cryptography\ninstalling with pip\n\nShould you need the bleeding edge version, you can try building it from source by installing with pip.\nPreparing the build environment\n\nYou will need the compiler and libraries with header files: musl, OpenSSL, libffi and Python itself:\n$ apk add gcc musl-dev libffi-dev openssl-dev python3-dev\n\nBuilding\n\n$ pip install pkgname\nhides the build log by default. To see the complete build log, add -vvv to increase verbosity. (Optional) Also, you can explicitly prohibit installing manylinux1 wheels by adding -\n-no-binary=pkgname\nso the build from source will be enforced.\n$ pip install cryptography -vvv --no-binary=cryptography",
    "Run python mysql client on slim python 3.6 docker image": "I'm using python:3.7-slim and using the following command\nRUN apt-get -y install default-libmysqlclient-dev",
    "Docker compose up keep replace existing container": "Docker compose associates the container with the project name (default directory name) and the service name or container_name if specified. Thus in case both branches have the compose file under the same directory name, and thus the compose files will be interpreted as refering to the same container, which will lead to the container being recreated.\nTo avoid this situation, you can the --project-name option to override the default one (directory name).\ndocker-compose --project-name branch1 up -d\ndocker-compose --project-name branch2 up -d\nIn this case both containers will be created.\nBut note that if both compose files have the same container_name set, there will be a conflict and the second container creation will fail. To avoid that, either use different container names, or remove the container_name property, to get the default container name which is <project_name>_<service_name>_1",
    "Getting an \"operation not supported\" error when trying to RUN something while building a docker image": "Well, turns out that if your machine had a kernel update but you didn't restart yet then docker freaks out. Restarting the machine fixed that. Oops.",
    "Next.JS v13 in Docker does not respect path alias but works locally": "Found the issue.\nThe solution was to update next.config.js to set the path alias in webpack. This should be the same as your tsconfig.json.\nconst nextConfig = {\n  output: 'standalone',\n  webpack: (config, { isServer }) => {\n    config.resolve.alias['~'] = path.join(__dirname, 'src');\n    return config;\n  },\n};\nHere's my hypothesis: in Next.js 13, they moved to turbopack but still use Webpack as a bundler. Webpack runs at a different phase than the TS compilation phase, so it doesn't take into account tsconfig.json.",
    "failed to start daemon: pid file found, ensure docker is not running or delete /var/run/docker.pid": "Delete the .pid file using the below Linux command,\nrm /var/run/docker.pid\nNow the pid file will get deleted and the docker daemon can be launched newly.",
    "Dockerfile vs Docker image": "Using a Dockerfile:\nYou have an 'audit log' that describes how the image is built. For me this is fundamental if it is going to be used in a production pipeline where more people are working and maintainability should be a priority.\nYou can automate the building process of your image, being an easy way of updating the container with system updates, or if it has to take part in a continuous delivery pipeline.\nIt is a cleaner way of create the layers of your container (each Dockerfile command is a different layer)\nChanging a container and committing the changes is great for testing purposes and for fast development for a conceptual test. But if you plan to use the result image for some time, I would definitely use Dockerfiles.\nApart from this, if you have to modify a file and doing it using bash tools (awk, sed...) results very tedious, you can add any file you wish from outside during the building process.",
    "Unable to start postgres docker container from docker-compose": "The problem caused because the volume which your compose created to store your database still keep old data which initiated by PostgreSQL 9.6. That volume name is postgres-data which created when you use named volume on your docker-compose.yml. So simply to get rid of this, you can use some ways below:\nUsing docker-compose command:\nRun docker-compose down -v, this will stop all your container inside that compose and remove all named volume inside that compose.\nYou could take a look at docker-compose down command\nUsing docker volume command:\nRun docker volume ls to get list of current volumes on your machine, I think you will see your volume on that list too:\nDRIVER              VOLUME NAME\nlocal               postgres-data\nRun docker volume rm postgres-data to remove that volume, if your container still running and you couldn't remove it then you can use -f to force remove it\nHope that helps!",
    "Dockerfile build remove source code from final image": "You can do a multi-stage build and copy the artifacts on a new image from the previous one. Also install any required runtime dependencies (if any).\nFROM alpine AS builder\n\nRUN apk add --no-cache <build_dependencies>\n\nCOPY source_code /tmp/source_code\n\nRUN make -C /tmp/source_code && \\\n        mkdir /libraries/\n        cp /tmp/lib/* /libraries/\n        rm -rf /tmp/*\n\nFROM alpine\n\nRUN apk add --no-cache <runtime_dependencies>\n\nCOPY --from=builder /libraries/ /libraries/",
    "How to run cron job in docker container?": "Crontab requires additional field: user, who runs the command:\n* * * * * root python3 /code/populatePDBbackground.py >> /var/log/cron.log\n# Empty line\nThe Dockerfile is:\nFROM python:3\nRUN apt-get -y update && apt-get -y upgrade\nRUN apt-get install -y cron postgresql-client\nRUN touch /var/log/cron.log\nRUN mkdir /code\nWORKDIR /code\nADD . /code/\nCOPY crontab /etc/cron.d/cjob\nRUN chmod 0644 /etc/cron.d/cjob\nENV PYTHONUNBUFFERED 1\nCMD cron -f\nTest python script populatePDBbackground.py is:\nfrom datetime import datetime\n\nprint('Script has been started at {}'.format(datetime.now()))\nAnd finally we get:\n$ docker run -d b3fa191e8822\nb8e768b4159637673f3dc4d1d91557b374670f4a46c921e0b02ea7028f40e105\n\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\nb8e768b41596        b3fa191e8822        \"/bin/sh -c 'cron -f'\"   4 seconds ago       Up 3 seconds                            cocky_beaver\n\n$ docker exec -ti b8e768b41596 bash\nroot@b8e768b41596:/code# tail -f /var/log/cron.log\nScript has been started at 2019-03-13 00:06:01.095013\nScript has been started at 2019-03-13 00:07:01.253030\nScript has been started at 2019-03-13 00:08:01.273926",
    "\"Exec format error\" with docker run command": "The \"Exec format error\" was simply because I was copying the binary file built on OSX/MacOS into the Docker image and trying to run that binary file in the Linux container. That don't work.\nHere is the Dockerfile that worked for me:\nFROM golang:latest\n\nRUN mkdir -p /app\n\nWORKDIR /app\n\nCOPY . .\n\nENV GOPATH /app\n\nRUN go install huru\n\nENTRYPOINT /app/bin/huru\nand my project structure like so on my host fs:\n$GOPATH/\n      src/\n        huru/\n      .dockerignore\n      Dockerfile\nI run:\ndocker build -t foo .\ndocker run foo\nmy .dockerignore file contains:\n.vscode\nbin\npkg",
    "Dockerfile: create ENV variable that a USER can see?": "Any user can see the environment variables:\n$ cat Dockerfile\nFROM debian\n\nENV foo bar\nRUN groupadd -r am && useradd -r -g am am\nUSER am\n$ docker build -t test .\n...\n$ docker run test bash -c 'echo $foo'\nbar\nSo that's not what the problem is. It may be that your process forked a new environment, but I can't be sure as you haven't shared how you're checking the value.",
    "standard_init_linux.go:211: exec user process caused \"no such file or directory\"?": "The \"shebang\" line at the start of a script says what interpreter to use to run it. In your case, your script has specified #!/bin/bash, but Alpine-based Docker images don't typically include GNU bash; instead, they have a more minimal /bin/sh that includes just the functionality in the POSIX shell specification.\nYour script isn't using any of the non-standard bash extensions, so you can just change the start of the script to\n#!/bin/sh",
    "Getting ssh-keygen in Alpine docker": "Thanks to @PrasadK - which nudged me along, the answer to Node- Red new Projects feature since version 0.18.3 - in order to have a remote repo - using this function in Node-Red Projects, the underlying docker image requires ssh-keygen. Do this in the Dockerfile with:\n......\nRUN   apk update && \\\n      apk add --no-cache \\\n      openssh-keygen\n......",
    "How to create an empty file in a scratch container?": "There are no commands in scratch to run. The only options you have from scratch are COPY and ADD. And you can only copy or add files from the context (unless you want to ADD from a remote url which I wouldn't recommend). So you are left to create an empty file in your context and copy that.\nAnd then docker introduced multi stage builds, which let's you use another build as your context. So you can make an empty file in one stage and copy it to the other.\nFROM busybox AS build-env\nRUN touch /empty\n\nFROM scratch\nCOPY --from=build-env /empty /.emptyfile",
    "Building Docker image for Node application using Yarn dependency": "You should first run yarn install to generate a yarn lockfile (yarn.lock) before building the image. Then make sure to copy it along with the package.json. Your dockerfile should look like this :\nFROM node:7 \nWORKDIR /app \nCOPY package.json /app \nCOPY yarn.lock /app\nRUN yarn install \nCOPY . /app \nCMD npm run develop \nEXPOSE 8000\nWith this all dependencies should install successfully when building your image",
    "RUN echo -e \"deb http ... \" prepares a wrong contents of the target file": "Try using the bash shell for the echo command instead. Docker by default uses the sh shell and for some reason it doesn't like the -e characters and had echo'ed it as part of its input.\nTo fix this;\nUpdate to:\nRUN bash -c 'echo -e \"deb http://nginx.org/packages/mainline/ubuntu/ xenial nginx\\ndeb-src http://nginx.org/packages/mainline/ubuntu/ xenial nginx\" | tee /etc/apt/sources.list.d/nginx.list'\nOutputs:\nroot@87944d07f493:/etc/apt/sources.list.d# cat nginx.list deb http://nginx.org/packages/mainline/ubuntu/ xenial nginx deb-src http://nginx.org/packages/mainline/ubuntu/ xenial nginx\nYou can replicate this \"docker problem\" by switching to sh on your current terminal session and then do the same echo command again.",
    "Docker Official Tomcat Image Modify Server.xml and add jar to lib folder": "The OP is old, but I came across it on google searching for a way to update the server.xml from the official docker image for tomcat. I would like to add my solution.\nUse Case: My company has static html files that are generated from another application (running under Websphere Liberty) and saved to a shared directory. The shared directory is also mounted in our Tomcat container, but is not mounted in any of the web application directories, so tomcat does not have access to it by default. The official Tomcat docker image contains a default server.xml file, so I need to update this file to specify additional directories where Tomcat can search for content.\nInitial Thoughts: Initially, I thought it would be best to write a bash script that simply applied my changes to the server.xml without overwriting the entire file. After contemplating this, I came to the conclusion if the default server.xml file changed (due to an image update) then unexpected behavior could occur which would require updates to the bash script, therefore, I dropped this idea.\nSolution: Make a copy of the image's server.xml file, make my changes and add a COPY directive in the Dockerfile to overwrite the default server.xml with my changes. If the image is updated and the default server.xml file changes, then I will need to update my copy as well, but I would much rather do that than introduce more code to our repo.\nDockerfile:\nFROM tomcat:7-jre8\nBuild and run official Tomcat image:\ndocker build -t tomcat:official .\nCopy the server.xml to the local filesystem:\ndocker run tomcat:official tar -c -C /usr/local/tomcat/conf server.xml | tar x\nMake changes to the local copy of the server.xml file and make sure that it's in the same directory as the Dockerfile.\nIn my case, I had to add the following line between the \"Host\" tags\n<Context docBase=\"/mnt/html/Auto/Reports\" path=\"/Auto/Reports\" />\nUpdate the Dockerfile:\nFROM tomcat:7-jre8\nCOPY server.xml /usr/local/tomcat/conf/\nNotes: My company is currently using the 'tomcat:7-jre8' version of the Tomcat image, but a list of all versions can be found here.\nHopfully this helps someone else.",
    "DockerFile one-line vs multi-line instruction [duplicate]": "In this specific case it is important to put apt-get update and apt-get install together. More broadly, fewer layers is considered \"better\" but it almost never has a perceptible difference.\nIn practice I tend to group together \"related\" commands into the same RUN command. If I need to configure and install a package from source, that can get grouped together, and even if I change make arguments I don't mind re-running configure. If I need to configure and install three packages, they'd go into separate RUN lines.\nThe important difference in this specific apt-get example is around layer caching. Let's say your Dockerfile has\nFROM ubuntu:18.04\nRUN apt-get update\nRUN apt-get install package-a\nIf you run docker build a second time, it will decide it's already run all three of these commands and the input hasn't changed, so it will run very quickly and you'll get an identical image out.\nNow you come back a day or two later and realize you were missing something, so you change\nFROM ubuntu:18.04\nRUN apt-get update\nRUN apt-get install package-a package-b\nWhen you run docker build again, Docker decides it's already run apt-get update and can jump straight to the apt-get install line. In this specific case you'll have trouble: Debian and Ubuntu update their repositories fairly frequently, and when they do the old versions of packages get deleted. So your apt-get update from two days ago points at a package that no longer exists, and your build will fail.\nYou'll avoid this specific problem by always putting the two apt-get commands together in the same docker run line\nFROM ubuntu:18.04\nRUN apt-get update \\\n && DEBIAN_FRONTEND=noninteractive \\\n    apt-get install --assume-yes --no-install-recommends \\\n      package-a \\\n      package-b",
    "Dockerfile doesn't seem to run while debugging in Visual Studio 2019": "This is apparently by design as a \"Fast mode\" optimization in Visual Studio 2019. See the documentation for debugging in containers here.\nWhat it states is that \"Fast mode\" is the default behavior when debugging containers in VS 2019. In this mode, only the first stage (base) of a multi-stage build is built according to the Dockerfile. VS then handles the rest on the host machine, ignoring the Dockerfile, and shares the output to the container by using volume mounting. This means that any custom steps you add to other stages will be ignored when using the Debug configuration in VS 2019. (The reason given for this non-obvious, and therefore potentially frustrating, optimization is that builds are much slower in a container than on the local machine.) Note that this optimization only happens when using the Debug configuration. The Release configuration will use the entire Dockerfile.\nYour options are:\nPlace your custom steps in the first (base) step of the Dockerfile.\nor\nDisable this optimization by editing the project file like this:\n<PropertyGroup>\n   <ContainerDevelopmentMode>Regular</ContainerDevelopmentMode>\n</PropertyGroup>\nAlso keep in mind that it will try to reuse a previously built container if possible, so you may need to perform a Clean or Rebuild in order to force the build to create a new version of the container.\nGood luck!\n** EDIT **\nIt seems that there is an issue when trying to use the ContainerDevelopmentMode flag after Container Orchestration Support (in this case, Docker Compose) is added. See this issue. It is suggested in the issue discussion that this flag could be used on the docker-compose.dcproj file, but there is a bug (still not fixed) that keeps that approach from working.\nA third option, hinted at in my previous answer but not made explicit, would be:\nSwitch your solution configuration from Debug to Release.\nThis works, but clearly isn't ideal when you're trying to debug your application.",
    "Difference between --cache-to/from and --mount type=cache in docker buildx build": "They are solving two different problems.\n--cache-to/from is used to store the result of a build step and reuse it in future builds, avoiding the need to run the command again. This is stored in a persistent location outside of the builder, like on a registry, so that other builders can skip already completed steps of a build even if the image wasn't built on the local system.\n--mount type=cache creates a mount inside the temporary container that's executed in a RUN step. This mount is reused in later executions of the build when the step itself is not cached. This is useful when a step pulls down a lot of external dependencies that do not need to be in the image and can safely be reused between builds. The storage of the mount cache is local to the builder and is an empty directory on first use.",
    "Conditionally mount volumes in docker-compose for several conditions": "You can set defaults for environment variable in a .env-file shipped alongside with a docker-compose.yml [1].\nBy setting your environment variables to /dev/null by default and then handling this case in the containerized application, you should be able to achieve what you need.\nExample\n$ tree -a\n.\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 run.sh\ndocker-compose.yml\nversion: \"3\"\n\nservices:\n  test:\n    build: .\n    environment:\n      - VOL_DST=${VOL_DST}\n    volumes:\n      - \"${VOL_SRC}:${VOL_DST}\"\nDockerfile\nFROM alpine\nCOPY run.sh /run.sh\nENTRYPOINT [\"/run.sh\"]\n.env\nVOL_SRC=/dev/null\nVOL_DST=/volume\nrun.sh\n#!/usr/bin/env sh\nset -euo pipefail\n\nif [ ! -d ${VOL_DST} ]; then\n  echo \"${VOL_DST} not mounted\"\nelse\n  echo \"${VOL_DST} mounted\"\nfi\nTesting\nEnvironment variable VOL_SRC not defined:\n$ docker-compose up\nStarting test_test_1 ... done\nAttaching to test_test_1\ntest_1  | /volume not mounted\ntest_test_1 exited with code 0\nEnvironment variable VOL_SRC defined:\n$ VOL_SRC=\"./\" docker-compose up\nRecreating test_test_1 ... done\nAttaching to test_test_1\ntest_1  | /volume mounted\n[1] https://docs.docker.com/compose/environment-variables/#the-env-file",
    "How to navigate up one folder in a dockerfile": "TL;DR\nrun it from the root directory:\ndocker build . -f ./path/to/dockerfile\nthe long answer:\nin dockerfile you cant really go up.\nwhy\nwhen the docker daemon is building you image, it uses 2 parameters:\nyour Dockerfile\nthe context\nthe context is what you refer to as . in the dockerfile. (for example as COPY . /app) both of them affect the final image - the dockerfile determines what is going to happen. the context tells docker on which files it should perform the operations you've specified in that dockerfile.\nthats how the docs put it:\nA build\u2019s context is the set of files located in the specified PATH or URL. The build process can refer to any of the files in the context. For example, your build can use a COPY instruction to reference a file in the context.\nso, usually the context is the directory where the Dockerfile is placed. my suggestion is to leave it where it belongs. name your dockerfiles after their role (Dockerfile.dev,Dockerfile.prod, etc) thats ok to have a few of them in the same dir.\nthe context can still be changed:\nafter all, you are the one that specify the context. since the docker build command accepts the context and the dockerfile path. when i run:\ndocker build .\ni am actually giving it the context of my current directory, (ive omitted the dockerfile path so it defaults to PATH/Dockerfile)\nso if you have a dockerfile in dockerfiles/Dockerfile.dev, you shoul place youself in the directory you want as context, and you run:\ndocker build . -f dockerfiles/Dockerfile.dev\nsame applies to docker-compose build section (you specify there a context and the dockerfile path)\nhope that made sense.",
    "Docker variable expansion in compose entrypoint": "Variables defined in the compose environment section are passed to the container, but not used by docker-compose itself to parse your yml file. The variables in the yml file are expanded with your host shell's environment (the shell where you run the docker-compose up command from) and/or the .env file contents.\nSince you are running the entrypoint with the shell syntax, you can have the shell inside the container expand the variables instead of having docker-compose do it, by escaping the variables:\nentrypoint: \"java $${JAVA_OPTS} -Xmx$${javaMemoryLimit} -jar /app.jar\"\nYou may need to add a /bin/sh to parse those variables:\nentrypoint: \"/bin/sh -c \\\"java $${JAVA_OPTS} -Xmx$${javaMemoryLimit} -jar /app.jar\\\"\"",
    "How happens when Linux distributions are different between the docker host and the docker image?": "Docker does not use LXC (not since Docker 0.9) but libcontainer (now runc), a built-in execution driver which manipulates namespaces, control groups, capabilities, apparmor profiles, network interfaces and firewalling rules \u2013 all in a consistent and predictable way, and without depending on LXC or any other userland package.\nA docker image represents a set of files winch will run as a container in their own memory and disk and user space, while accessing the host kernel.\nThis differs from a VM, which does not access the host kernel but includes its own hardware/software stack through its hypervisor.\nA container has just to set limits (disk, memory, cpu) in the host. An actual VM has to build an entire new host.\nThat docker image (group of files) can be anything, as long as:\nit does not depends on host libraries (since it is isolated in its own disk space, it does not have access to hosts files, unless volumes are mounted)\nit does only system calls: see \"What is meant by shared kernel in Docker?\"\nThat means an image can be anything: another linux distro, or even a single executable file. Any executable compile in go (https://golang.org/) for instance, could be packaged in its own docker image without any linux distro:\nFROM scratch\nCOPY my_go_exe /\nENTRYPOINT /my_go_exe\nscratch is the \"empty\" image, and a go executable is statically linked, so it is self-contained and only depends on system calls to the kernel.",
    "Serving multiple tensorflow models using docker": "I ran into this double slash issue for git bash on windows.\nAs such I am passing the argument, mentioned by @KrisR89, in via command in the docker-compose.\nThe new docker-compose looks like this and works with the supplied dockerfile:\nversion: '3'\n\nservices:\n  serving:\n    build: .\n    image: testing-models\n    container_name: tf\n    command: --model_config_file=/config/config.conf",
    "Configure dockerfile with postgres": "When building your docker image postgres is not running. Database is started when container is starting, any sql files can be executed after that. Easiest solution is to put your sql files into special directory:\nFROM postgres:9.4\nCOPY *.sql /docker-entrypoint-initdb.d/\nWhen booting startup script will execute all files from this dir. You can read about this in docs https://hub.docker.com/_/postgres/ in section How to extend this image.\nAlso, if you need different user you should set environment variables POSTGRES_USER and POSTGRES_PASSWORD. It's easier then using custom scripts for creating user.",
    "How to copy azure pipeline artifacts to a docker image which is microsoft dotnetcore runtime image": "Publishing to $(Build.ArtifactStagingDirectory) is fine. There are several ways to do it, so here's one:\nAdd a Docker task after the publish, configured to \"Build\"\nSet the \"Build Context\" in the task to $(Build.ArtifactStagingDirectory). That's the root path Docker will use for commands like COPY in a Dockerfile.\nCommit a Dockerfile to your repo, and set the Dockerfile path in the task to match its location\nSet up the Dockerfile like this (I'm assuming .NET Core 2.2 here):\nFROM mcr.microsoft.com/dotnet/core/aspnet:2.2\nWORKDIR /app\nCOPY . .\nENTRYPOINT [\"dotnet\", \"myAppNameHere.dll\"]\nBecause you've set the Docker Build Context to $(Build.ArtifactStagingDirectory), where your app has been published, the COPY command will use that as a \"current working directory.\" The translation of the COPY is \"copy everything in $(Build.ArtifactStagingDirectory) to the /app folder inside the container.\"\nThat'll get you a basic Docker container that simply contains your pre-built and published app files.",
    "How to use an environment variable from a docker-compose.yml in a Dockerfile?": "There are two different time frames to understand. Building an image is separate from running the container. The first part uses the Dockerfile to create your image. And the second part takes the resulting image and all of the settings (e.g. environment variables) to create a container.\nInside the Dockerfile, the RUN line occurs at build time. If you want to pass a parameter into the build, you can use a build arg. Your Dockerfile would look like:\nFROM node:7\nARG GITLAB_USER=default_user_name\n# ENV is optional, without it the variable only exists at build time\n# ENV GITLAB_USER=${GITLAB_USER}\nRUN echo '${GITLAB_USER}' \nAnd your docker-compose.yml file would look like:\nversion: '2'\nservices:\n  myapp:\n    build:\n      context: .\n      args:\n      - GITLAB_USER=${GITLAB_USER}\nThe ${GITLAB_USER} value inside the yml file will be replaced with the value set inside your .env file.",
    "How make Docker find referenced projects on build?": "Is complicated to explaind, why is not posible to build Dockerfile from same project space, then I read this article [blog]: https://josiahmortenson.dev/blog/2020-06-08-aspnetcore-docker-https and finaly understud how I need to run the build.\nIn my case:\n/src\n .sln\n /project1\n   project1.csproj\n   Dockerfile\n /project2\n   project2.csproj\n   Dockerfile\n /reference1\n   reference1.csproj\n /reference2\n   reference2.csproj\nIs necesary to move to /src level for run command\ndocker build -t {nametagedcontainer} -f project1/Dockerfile .\nFinally\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS c\nWORKDIR /app   \n\n    \nCOPY [\"project1/project1.csproj\", \"project1/\"]\nCOPY [\"reference1/reference1.csproj\", \"reference1/\"]\n#others reference if necesary\n\n\n\nRUN dotnet restore \"project1/project1.csproj\" \n\nCOPY . .\nWORKDIR \"/app/project1\"\nRUN dotnet build \"project1.csproj\" -c Release -o /app/build\n\nfrom c as publish\nRUN dotnet publish \"project1.csproj\" -c Release -o /app/publish\n \n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app \nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"{dllname}.dll\"]",
    "How to pass Java_opts before an executable to entrypoint in dockerfile?": "You can declare environment variables with ENV:\n...\nADD --chown=daemon:daemon UserPrefExporter.sc /opt/docker\nUSER daemon\nENV JAVA_OPTS=\"-Xmx<limit>\"\nENTRYPOINT [\"./amm\", \"-h\", \"amm_home\", \"UserPrefExporter.sc\"]",
    "Docker run container on different port": "Please try:\ndocker run -p 70:80 kitematic/hello-world-nginx\nbinding port is probably mistyped in the command you provided.",
    "What is the location of config file in Docker 1.9?": "It sounds like you have moved from the CentOS distributed docker to the docker.com docker-engine packages as CentOS hasn't moved to 1.9 yet.\nThe CentOS packages will make use of the /etc/sysconfig standard. Dockers packages will not. You will also miss out on the docker-storage-setup program RedHat built to deal with their unique storage requirements.\nThe systemd config in /usr/lib/systemd/system/docker.service will show you the difference:\nDocker\n[Service]\nType=notify\nExecStart=/usr/bin/docker daemon -H fd://\nMountFlags=slave\nLimitNOFILE=1048576\nLimitNPROC=1048576\nCentOS\n[Service]\nType=notify\nEnvironmentFile=-/etc/sysconfig/docker\nEnvironmentFile=-/etc/sysconfig/docker-storage\nEnvironmentFile=-/etc/sysconfig/docker-network\nEnvironment=GOTRACEBACK=crash\nExecStart=/usr/bin/docker daemon $OPTIONS \\\n          $DOCKER_STORAGE_OPTIONS \\\n          $DOCKER_NETWORK_OPTIONS \\\n          $ADD_REGISTRY \\\n          $BLOCK_REGISTRY \\\n          $INSECURE_REGISTRY\nLimitNOFILE=1048576\nLimitNPROC=1048576\nLimitCORE=infinity\nMountFlags=slave\nTimeoutStartSec=1min\nRestart=on-failure\nSo then, how do I configure the docker-engine installed docker service?\nDocker have a page on setting up the systemd config. You can go either CentOS or Docker with your config\nDocker - edit the docker-engine systemd config file /usr/lib/systemd/system/docker.service with your required options\nCentos - copy the EnvironmentFile setup and then configure your options in /etc/sysconfig/docker.",
    "How to COPY files of current directory to folder in Dockerfile": "you can start a container and check.\n$ docker run -ti --rm <DOCKER_IMAGE> sh\n$ ls -l /this/folder\nIf your docker image has ENTRYPOINT setting, then run below command:\n$ docker run -ti --rm --entrypoint sh <DOCKER_IMAGE>\n$ ls -l /this/folder",
    "How to cache Python dependencies properly": "The --download-cache option was removed in pip version 8, because it's now using cache by default. So you don't need to specify this option at all. I'm not sure what the purpose of the pip download -d <dir> option is, but apparently it's not creating a cache in the destination directory. You can just leave out the -d <dir> option too. The following Dockerfile works:\nFROM python:3.7\nCOPY constraints.txt requirements.txt ./\nRUN pip3 download -d .pipcache -r requirements.txt -c constraints.txt\nCOPY test.txt ./\nRUN pip3 install -r requirements.txt -c constraints.txt\nIf you add --cache-dir <dir> to both the download and install commands, it will work as well. So the following Dockerfile also works:\nFROM python:3.7\nCOPY constraints.txt requirements.txt ./\nRUN pip3 download --cache-dir ./tmp/pipcache -r requirements.txt -c constraints.txt\nCOPY test.txt ./\nRUN pip3 install --cache-dir ./tmp/pipcache -r requirements.txt -c constraints.txt\nExample output (with only pep8 and pylint in the requirements.txt):\nFirst run:\nSending build context to Docker daemon  5.632kB\nStep 1/5 : FROM python:3.7\n ---> a4cc999cf2aa\nStep 2/5 : COPY constraints.txt requirements.txt ./\n ---> 411eaa3d36ff\nStep 3/5 : RUN pip3 download -r requirements.txt -c constraints.txt\n ---> Running in 6b489df74137\nCollecting pep8==1.7.1 (from -c constraints.txt (line 17))\n  Downloading https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl (41kB)\n  Saved /pep8-1.7.1-py2.py3-none-any.whl\nCollecting pylint==2.3.1 (from -c constraints.txt (line 22))\n  Downloading https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl (765kB)\n  Saved /pylint-2.3.1-py3-none-any.whl\nCollecting mccabe==0.6.1 (from -c constraints.txt (line 14))\n  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n  Saved /mccabe-0.6.1-py2.py3-none-any.whl\nCollecting astroid==2.2.5 (from -c constraints.txt (line 2))\n  Downloading https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl (193kB)\n  Saved /astroid-2.2.5-py3-none-any.whl\nCollecting isort==4.3.19 (from -c constraints.txt (line 10))\n  Downloading https://files.pythonhosted.org/packages/ae/ae/5ef4b57e15489754b73dc908b656b02ab0e6d37b190ac78dd498be8b577d/isort-4.3.19-py2.py3-none-any.whl (42kB)\n  Saved /isort-4.3.19-py2.py3-none-any.whl\nCollecting lazy-object-proxy==1.4.1 (from -c constraints.txt (line 12))\n  Downloading https://files.pythonhosted.org/packages/43/a5/1b19b094ad19bce55b5b6d434020f5537b424fd2b3cff0fbef23d7bb5a95/lazy_object_proxy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (49kB)\n  Saved /lazy_object_proxy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl\nCollecting wrapt==1.11.1 (from -c constraints.txt (line 39))\n  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n  Saved /wrapt-1.11.1.tar.gz\nCollecting six==1.12.0 (from -c constraints.txt (line 28))\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n  Saved /six-1.12.0-py2.py3-none-any.whl\nCollecting typed-ast==1.3.5 (from -c constraints.txt (line 37))\n  Downloading https://files.pythonhosted.org/packages/17/9e/00918af7bdd616decb5b7ad06a9cd0a4a247d2fccaa630ab448a57e68b98/typed_ast-1.3.5-cp37-cp37m-manylinux1_x86_64.whl (736kB)\n  Saved /typed_ast-1.3.5-cp37-cp37m-manylinux1_x86_64.whl\nSuccessfully downloaded pep8 pylint mccabe astroid isort lazy-object-proxy wrapt six typed-ast\nRemoving intermediate container 6b489df74137\n ---> 8ac3be432c58\nStep 4/5 : COPY test.txt ./\n ---> 5cac20851967\nStep 5/5 : RUN pip3 install -r requirements.txt -c constraints.txt\n ---> Running in 394847f09e9b\nCollecting pep8==1.7.1 (from -c constraints.txt (line 17))\n  Using cached https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl\nCollecting pylint==2.3.1 (from -c constraints.txt (line 22))\n  Using cached https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl\nCollecting astroid==2.2.5 (from -c constraints.txt (line 2))\n  Using cached https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl\nCollecting mccabe==0.6.1 (from -c constraints.txt (line 14))\n  Using cached https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\nCollecting isort==4.3.19 (from -c constraints.txt (line 10))\n  Using cached https://files.pythonhosted.org/packages/ae/ae/5ef4b57e15489754b73dc908b656b02ab0e6d37b190ac78dd498be8b577d/isort-4.3.19-py2.py3-none-any.whl\nCollecting lazy-object-proxy==1.4.1 (from -c constraints.txt (line 12))\n  Using cached https://files.pythonhosted.org/packages/43/a5/1b19b094ad19bce55b5b6d434020f5537b424fd2b3cff0fbef23d7bb5a95/lazy_object_proxy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl\nCollecting six==1.12.0 (from -c constraints.txt (line 28))\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting wrapt==1.11.1 (from -c constraints.txt (line 39))\n  Using cached https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\nCollecting typed-ast==1.3.5 (from -c constraints.txt (line 37))\n  Using cached https://files.pythonhosted.org/packages/17/9e/00918af7bdd616decb5b7ad06a9cd0a4a247d2fccaa630ab448a57e68b98/typed_ast-1.3.5-cp37-cp37m-manylinux1_x86_64.whl\nBuilding wheels for collected packages: wrapt\n  Building wheel for wrapt (setup.py): started\n  Building wheel for wrapt (setup.py): finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\nSuccessfully built wrapt\nInstalling collected packages: lazy-object-proxy, six, wrapt, typed-ast, astroid, isort, mccabe, pep8, pylint\nSuccessfully installed astroid-2.2.5 isort-4.3.19 lazy-object-proxy-1.4.1 mccabe-0.6.1 pep8-1.7.1 pylint-2.3.1 six-1.12.0 typed-ast-1.3.5 wrapt-1.11.1\nRemoving intermediate container 394847f09e9b\n ---> 68e65a214a32\nSuccessfully built 68e65a214a32\nSuccessfully tagged test:latest\nSecond run (after changing test.txt to trigger a rebuild of Docker layers 4 and 5):\nSending build context to Docker daemon  5.632kB\nStep 1/5 : FROM python:3.7\n ---> a4cc999cf2aa\nStep 2/5 : COPY constraints.txt requirements.txt ./\n ---> Using cache\n ---> 411eaa3d36ff\nStep 3/5 : RUN pip3 download -r requirements.txt -c constraints.txt\n ---> Using cache\n ---> 8ac3be432c58\nStep 4/5 : COPY test.txt ./\n ---> 7ab5814153b7\nStep 5/5 : RUN pip3 install -r requirements.txt -c constraints.txt\n ---> Running in 501da787ab07\nCollecting pep8==1.7.1 (from -c constraints.txt (line 17))\n  Using cached https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl\nCollecting pylint==2.3.1 (from -c constraints.txt (line 22))\n  Using cached https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl\nCollecting astroid==2.2.5 (from -c constraints.txt (line 2))\n  Using cached https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl\nCollecting mccabe==0.6.1 (from -c constraints.txt (line 14))\n  Using cached https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\nCollecting isort==4.3.19 (from -c constraints.txt (line 10))\n  Using cached https://files.pythonhosted.org/packages/ae/ae/5ef4b57e15489754b73dc908b656b02ab0e6d37b190ac78dd498be8b577d/isort-4.3.19-py2.py3-none-any.whl\nCollecting typed-ast==1.3.5 (from -c constraints.txt (line 37))\n  Using cached https://files.pythonhosted.org/packages/17/9e/00918af7bdd616decb5b7ad06a9cd0a4a247d2fccaa630ab448a57e68b98/typed_ast-1.3.5-cp37-cp37m-manylinux1_x86_64.whl\nCollecting six==1.12.0 (from -c constraints.txt (line 28))\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nCollecting wrapt==1.11.1 (from -c constraints.txt (line 39))\n  Using cached https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\nCollecting lazy-object-proxy==1.4.1 (from -c constraints.txt (line 12))\n  Using cached https://files.pythonhosted.org/packages/43/a5/1b19b094ad19bce55b5b6d434020f5537b424fd2b3cff0fbef23d7bb5a95/lazy_object_proxy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl\nBuilding wheels for collected packages: wrapt\n  Building wheel for wrapt (setup.py): started\n  Building wheel for wrapt (setup.py): finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\nSuccessfully built wrapt\nInstalling collected packages: typed-ast, six, wrapt, lazy-object-proxy, astroid, isort, mccabe, pep8, pylint\nSuccessfully installed astroid-2.2.5 isort-4.3.19 lazy-object-proxy-1.4.1 mccabe-0.6.1 pep8-1.7.1 pylint-2.3.1 six-1.12.0 typed-ast-1.3.5 wrapt-1.11.1\nRemoving intermediate container 501da787ab07\n ---> b377fe561e97\nSuccessfully built b377fe561e97\nSuccessfully tagged test:latest\nNB: The official documentation was quite helpful.",
    "how to reload .bashrc in dockerfile": "Each command in a Dockerfile creates a new temporary container, but without tty (issue 1870, discussed in PR 4955, but closed in favor of PR 4882).\nThe lack of tty during docker builds triggers the ttyname failed: inappropriate ioctl for device error message.\nWhat you can try instead is running a wrapper script which in it will source the .bashrc.\nDockerfile:\nCOPY myscript /path/to/myscript\nRUN /path/to/myscript\nmyscript:\n#!/bin/bash\nsource /path/to/.bashrc\n# rest of the commands    \nAbderrahim points out in the comments:\nIn my case it was for nvm: it adds an init script to .bashrc therefore it wasn't usable in the Dockerfile context.\nEnded up making an install script with it's dependent command.",
    "How to download a file from URL using Dockerfile": "I recommend using ADD, as @David Maze commented and as @nicobo commented on the first answer.\nI think that this is the best answer for many of us, since it does not force download of wget or similar into the Docker image. Here is the example I just used for CMake:\nADD https://github.com/Kitware/CMake/releases/download/v3.27.6/cmake-3.27.6-linux-x86_64.sh /tmp/cmake.sh\n\nRUN mkdir /opt/cmake && bash /tmp/cmake.sh --prefix=/opt/cmake --skip-license && rm /tmp/cmake.sh",
    "Can't send request during build time in next.js with docker?": "There are 2 solutions I can think of.\nApparently, the Next.js build fails because the service it is querying is not running. Thus why not build and start the service it depends on explicitly and build the rest like this.\ndocker-compose build some_services\ndocker-compose up -d some_services\ndocker-compose build the_rest\nThis way the Next.js app will be able to make the request. Please keep in mind that You still need to configure the ports and networks correctly. Pretty sure this will resolve the issue.\nA more 'fancy' solution would be using build-time networks which are added in the later versions, 3.4+ if I am not mistaken.\ndocker-compose.yml\n\nbuild:\n    context: ./service_directory\n    network: some_network\n   \nFor more details please see Docker-compose network",
    "Dockerfile HOSTNAME Instruction for docker build like docker run -h": "You can use docker-compose to build your image and assign the hostname, e.g.\nversion: '3'\nservices:\n  all:\n    image: testimage\n    container_name: myname\n    hostname: myhost\n    build:\n      context: .\nThe run as: docker-compose --build up.",
    "How to create docker-compose.yml file while using jib?": "Answer recommended by Google Cloud Collective",
    "What is the best approach for adding cron jobs (scheduled tasks) for a particular service in docker-compose": "You can try the following Opensource tool for scheduling crons in the docker-compose.\nhttps://github.com/mcuadros/ofelia\neg:\n [job-service-run \"service-executed-on-new-container\"]\n schedule = 0,20,40 * * * *\n image = ubuntu\n network = swarm_network\n command =  touch /tmp/example\nIn case you are planning to utilize the image in any of the cloud platforms.\nFor Eg.\nAWS: You can also have a look at ECS scheduler\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduled_tasks.html\nGCP: Kubernetes Engine CronScheduler\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/cronjobs",
    "pull access denied for oracle/serverjre": "For some time now, it's needed not only to login with the Oracle account, but also to accept the license and user agreement. So you cannot do it only from command line. You must go to the Oracle container registry:\nhttps://container-registry.oracle.com/\nThen select Java repository, then select serverjre, then signin:\nAnd accept the license:\nOnce you have done that, you'll be able to pull the docker image, but as other have said, you'll need to change the registry that is set inside the Dockerfile:\n#FROM oracle/serverjre:8\nFROM container-registry.oracle.com/java/serverjre:8\nAnd afterwards, before running the build, you must do a docker login\ndocker login container-registry.oracle.com\nusername:<SSO USERNAME>\npassword:<SSO PASSWORD>\nAt this point, you'll be able to pull the image.",
    "Using VNCserver + GUI application + Virtual Display in Docker container": "I managed to found the solution:\nChanged the script in Attempt 3 above as follows worked\n!/bin/bash\n\nXvfb :1 -screen 0 800x600x16 &\n/usr/bin/x11vnc -display :1.0 -usepw &\nDISPLAY=:1.0\nexport DISPLAY\nfirefox\nCheers.",
    "Issue - Building Docker Image - as linux/amd64 on Macbook M1 Chip": "Actually I had to delete older images that would match the image build which end up pushing the older version ARM, not AMD.\nEverything is working as expected with the steps above (Just make sure to clean your local stored images)",
    "what npm command creates a dist folder in nodejs?": "Usually the dist/ folder is created by the build tool (webpack, vite, parcel, etc). So there's no command that just says \"create this directory.\" If you're using create-react-app then it would be npm run build but I believe CRA is actually outputting to a build/ directory and not dist/.\nReally it just depends on what you're wanting to do exactly. Are you looking for a command to build the application for the docker file? Are you looking to just have a path? etc.",
    "Dockerfile - add node.js via use COPY --from": "For node and npm to work you have to copy two directories in your Dockerfile:\n# Get NodeJS\nCOPY --from=node:20-slim /usr/local/bin /usr/local/bin\n# Get npm\nCOPY --from=node:20-slim /usr/local/lib/node_modules /usr/local/lib/node_modules",
    "Dockerfile - Hide --build-args from showing up in the build time": "Update\nYou know, I was focusing on the wrong part of your question. You shouldn't be using a username and password at all. You should be using access keys, which permit read-only access to private repositories.\nOnce you've created an ssh key and added the public component to your repository, you can then drop the private key into your image:\nRUN mkdir -m 700 -p /root/.ssh\nCOPY my_access_key /root/.ssh/id_rsa\nRUN chmod 700 /root/.ssh/id_rsa\nAnd now you can use that key when installing your Python project:\nRUN pip install git+ssh://git@bitbucket.org/you/yourproject.repo\n(Original answer follows)\nYou would generally not bake credentials into an image like this. In addition to the problem you've already discovered, it makes your image less useful because you would need to rebuild it every time your credentials changed, or if more than one person wanted to be able to use it.\nCredentials are more generally provided at runtime via one of various mechanisms:\nEnvironment variables: you can place your credentials in a file, e.g.:\nUSERNAME=myname\nPASSWORD=secret\nAnd then include that on the docker run command line:\ndocker run --env-file myenvfile.env ...\nThe USERNAME and PASSWORD environment variables will be available to processes in your container.\nBind mounts: you can place your credentials in a file, and then expose that file inside your container as a bind mount using the -v option to docker run:\ndocker run -v /path/to/myfile:/path/inside/container ...\nThis would expose the file as /path/inside/container inside your container.\nDocker secrets: If you're running Docker in swarm mode, you can expose your credentials as docker secrets.",
    "Docker: Entrypoint's override involve CMD specification?": "This is one of the exception cases when inheriting values from a previous image. If the parent image defines a CMD, and your image defines an ENTRYPOINT, then the value of CMD is nulled out. In all other scenarios, you should see ENTRYPOINT and CMD inherited from parent images unchanged. For the logic behind this decision, please see issue 5147.",
    "Difference between && and `set -ex` in Dockerfiles": "This isn't specific to Docker; it's just regular shell syntax used in the RUN command. set -e causes the script to exit if any command fails, while && only runs its right-hand command if the left-hand command does not fail. So in both\nset -e\nfoo\nbar\nand\nfoo && bar\nbar will only run if foo succeeds.\nSo, the two are identical if the entire script consists of a single list command ... && ... && ... where a command only runs if every previous command succeeds. An example of how they would differ:\nset -e\necho one\nfalse\necho two\necho three\nHere, echo two and echo three would never run. But in\necho one && false && echo two\necho three\nthe echo three would still run, because only echo two was \"guarded\" by the && preceding it.",
    "Docker build not using layer cache": "I found the answer by the way :) The Dockerfile itself was ok except one problem I could then find via docker history Which shows the real shell command which are executed by docker.\nThe Problem was that ARG BUILD_VERSION lead to docker adds to every run command the environment variable like /bin/sh -c \"ARG=123 ./sbt ...\". This leads to a different call signature and a different hash every time the arg changes and therefore the run command is not applied from cache. To fix this issue just move the ARG down to the first RUN command which needs it.\nFROM openjdk:8 as workspace\n\nWORKDIR /build\n\nCOPY ./sbt ./sbt\nCOPY ./sbt-dist ./sbt-dist\nCOPY ./build.sbt ./build.sbt\nCOPY ./project/build.properties ./project/build.properties\nCOPY ./project/plugins.sbt ./project/plugins.sbt\n\nRUN ./sbt -sbt-dir ./sbt-dir -ivy ./ivy update\n\nCOPY ./ ./\n\n# Embedded postgres need to be run as non-root user\nRUN useradd -ms /bin/bash runner\nRUN chown -R runner /build\nUSER runner\n\nRUN ./sbt -sbt-dir ./sbt-dir -ivy ./ivy clean test\n\nARG BUILD_VERSION\nRUN ./sbt -sbt-dir ./sbt-dir -ivy ./ivy docker:stage -Ddocker.image.version=\"${BUILD_VERSION}\"",
    "What Docker base image (`FROM`) for Java Spring Boot?": "There's a nice documentation on how to integrate Spring-Boot with Docker: https://spring.io/guides/gs/spring-boot-docker/\nBasically you define your dockerfile in src/main/docker/Dockerfile and configure the docker-maven-plugin like this:\n<build>\n<plugins>\n    <plugin>\n        <groupId>com.spotify</groupId>\n        <artifactId>docker-maven-plugin</artifactId>\n        <version>0.4.11</version>\n        <configuration>\n            <imageName>${docker.image.prefix}/${project.artifactId}</imageName>\n            <dockerDirectory>src/main/docker</dockerDirectory>\n            <resources>\n                <resource>\n                    <targetPath>/</targetPath>\n                    <directory>${project.build.directory}</directory>\n                    <include>${project.build.finalName}.jar</include>\n                </resource>\n            </resources>\n        </configuration>\n    </plugin>\n</plugins>\nDockerfile:\nFROM frolvlad/alpine-oraclejre8:slim\nVOLUME /tmp\nADD gs-spring-boot-docker-0.1.0.jar app.jar\nRUN sh -c 'touch /app.jar'\nENV JAVA_OPTS=\"\"\nENTRYPOINT [ \"sh\", \"-c\", \"java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar\" ]\nNote that in this example FROM frolvlad/alpine-oraclejre8:slim is a small-footprinted image which is based on Alpine Linux.\nYou should also be able to use the standard Java 8 image (which is based on Debian and might have an increased footprint) as well. An extensive list of available Java Baseimages can be found here: https://github.com/docker-library/docs/tree/master/openjdk.",
    "Does a RHEL7 docker container need subscription?": "On the Docker hub, you can find some Red Hat docker images , like\nhttps://hub.docker.com/r/richxsl/rhel6.5/\nor\nhttps://hub.docker.com/r/lionelman45/rhel7/\nbut in order to update them, you will need a valid subscription\nYou will find Red Hat docker images on the Red Hat site, at\nhttps://access.redhat.com/containers\nthis article summarizes what you need in order to build a Red hat docker image\nhttp://cloudgeekz.com/625/howto-create-a-docker-image-for-rhel.html\nit begins with\nPre-requisites\nAccess to RHEL package repository.",
    "Specifying JVM Options in docker-compose File": "Switching the environment declaration from sequence style to value-map style allows to use the YAML multiline string operator '>'. It will merge all lines to a single line.\nversion: '3.1'\nservices:\n  service:\n    image: registry.gitlab.com/project/service/${BRANCH}:${TAG}\n    container_name: serviceApp\n    env_file: docker-compose.env\n    environment:\n      JVM_OPTS: >\n        -XX:NativeMemoryTracking=summary\n        -XX:+StartAttachListener\n        -XX:+UseSerialGC\n        -Xss512k\n        -Dcom.sun.management.jmxremote.rmi.port=8088\n        -Dcom.sun.management.jmxremote=true\n        -Dcom.sun.management.jmxremote.port=8088\n        -Dcom.sun.management.jmxremote.ssl=false\n        -Dcom.sun.management.jmxremote.authenticate=false\n        -Dcom.sun.management.jmxremote.local.only=false\n        -Djava.rmi.server.hostname=localhost\n\n    ports:\n        - 8088:8088\n    networks:\n        - services\n    working_dir: /opt/app\n    command: [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/service.jar\"\"]\n\nnetworks:\n  services:\n    external:\n    name: services",
    "What is the base.Docker file in VSCode's dev containers for?": "base.Dockerfile is the \"recipe\" used by Microsoft to generate the image used in Dockerfile. In this case, this is mcr.microsoft.com/vscode/devcontainers/jekyll:0-${VARIANT}, specied in the FROM section at the top of Dockerfile.\nThey state that if you want to customize the image, that you can replace the FROM with the contents of base.Dockerfile. This is described here: https://github.com/microsoft/vscode-dev-containers/tree/v0.205.2/containers/jekyll#using-this-definition",
    "Can we have a docker container without a shell?": "Yes, you can create a container from scratch, which does not contain anything even bash, it will only contain binaries that you copy during build time, otherwise, it will be empty.\nFROM scratch\nCOPY hello /\nCMD [\"/hello\"]\nYou can use Docker\u2019s reserved, minimal image, scratch, as a starting point for building containers. Using the scratch \u201cimage\u201d signals to the build process that you want the next command in the Dockerfile to be the first filesystem layer in your image.\nWhile scratch appears in Docker\u2019s repository on the hub, you can\u2019t pull it, run it, or tag any image with the name scratch. Instead, you can refer to it in your Dockerfile. For example, to create a minimal container using scratch:\nscratch-docker-image\nUsing this as a base image, you can create your custom image, for example you only node runtime not thing more, then you try form scratch-node.\nFROM node as builder\n\nWORKDIR /app\n\nCOPY package.json package-lock.json index.js ./\n\nRUN npm install --prod\n\nFROM astefanutti/scratch-node\n\nCOPY --from=builder /app /\n\nENTRYPOINT [\"node\", \"index.js\"]",
    "Understanding Docker user/uid creation": "Absent user namespace remapping, there are only two things that matter:\nWhat the numeric user ID is; and\nWhat's in the /etc/passwd file.\nRemember that each container and the host have separate filesystems, so each of these things could have separate /etc/passwd files.\nWhat happens if I run the same image multiple times i.e multiple containers? Will the same uid be assigned to all processes?\nYes, because each container gets a copy of the same /etc/passwd file from the image.\nWhat happens if I add same above command in another image and run that image as container? - will I get new uid or same uid as the previous one?\nIt depends on what adduser actually does; it could be the same or different.\nHow the uid increment happens in Container in relation with the host machine.\nThey're completely and totally independent.\nAlso remember that you can docker push/docker pull a built image to run it on a different host. That will bring the image's /etc/passwd file along with it, but the host environment could be totally different. Correspondingly, it's not a best practice to try to match some specific host's uid mapping in a Dockerfile, because it will be wrong if you try to run the same image anywhere else.",
    "docker error in windows container read tcp : wsarecv: An existing connection was forcibly closed by the remote host": "I am not sure exactly why this one worked, as I was trying to do a pull of a couple microsoft images. But in Settings > General > Expose daemon on tcp://localhost:2375 without TLS, worked for me. Following that I reverted the change but nice to have that on in the back-pocket. Might be related to firewall settings in Windows. I am using Win 10 Professional.",
    "Dockerize existing Django project": "This question is too broad. What happens with the Dockerfile you've created?\nYou don't need docker compose unless you have multiple containers that need to interact.\nSome general observations from your current Dockerfile:\nIt would be better to collapse the pip install commands into a single statement. In docker, each statement creates a file system layer, and the layers in between the pip install commmands probably serve no useful purpose.\nIt's better to declare dependencies in setup.py or a requirements.txt file (pip install -r requirements.txt), with fixed version numbers (foopackage==0.0.1) to ensure a repeatable build.\nI'd recommend packaging your Django app into a python package and installing it with pip (cd /code/; pip install .) rather than directly adding the code directory.\nYou're missing a statement (CMD or ENTRYPOINT) to execute the app. See https://docs.docker.com/engine/reference/builder/#cmd",
    "Docker COPY all files and folders except some files/folders": "You have two choices:\nList all directories you want to copy directly:\nCOPY [\"foldera\", \"folderc\", \"folderd\", ..., \"/dstPath]\nTry to exclude some paths but also make sure that all paths patterns are not including the path we want to exclude:\nCOPY [\"folder[^b]*\", \"file*\", \"/dstPath\"]\nAlso you can read more about available solutions in this issue: https://github.com/moby/moby/issues/15771",
    "Docker: bind `/uploads` directory to Amazon S3 Storage": "The S3 storage link that your posted is for Docker Registry setup and not for Docker volumes. What you need is to map a folder on your hard drive to the container. So let's assume you map /var/www/uploads on host to your uploads inside the container.\nNow what you want is /var/www/uploads to be actually mounted as an S3 backed folder. For this Amazon had launched an AWS Storage Gateway. You can use that to create a S3 backed folder on your system. Below is an article from Amazon that details how to configure the same\nhttp://docs.aws.amazon.com/storagegateway/latest/userguide/ManagingLocalStorage-common.html",
    "Disable Dockerfile POP-UP Assistance in Visual Studio Code": "This is handled by the Parameter Hints functionality of VSCode rather than the Docker extension itself.\nYou can turn these off by unchecking the Editor \u203a Parameter Hints setting in your VSCode settings file.\nYou can still access the hint using the keyboard:\nFor Linux and Windows:\nCtrl + Shift + Space\nFor MacOS:\n\u21e7 + \u2318 + Space\nSee also this related answer.",
    "Copy a file from container to host during build process": "I wouldn't run the tests during build, this will only increase the size of your image. I would recommend you to build the image and then run it mounting a host volume into the container and changing the working directory to the mount point.\ndocker run -v `pwd`/results:/results -w /results -t IMAGE test_script",
    "How Docker selects an image's os/arch": "I'm wondering what image I'll get if I just do a \"docker pull python\"\nFrom \"Leverage multi-CPU architecture support\"\nMost of the Docker Official Images on Docker Hub provide a variety of architectures.\nFor example, the busybox image supports amd64, arm32v5, arm32v6, arm32v7, arm64v8, i386, ppc64le, and s390x.\nWhen running this image on an x86_64 / amd64 machine, the amd64 variant is pulled and run.\nSo it depends on the OS used when you do your docker pull.\nand how I can choose the OS and architecture myself.\nThe same page adds:\nYou can also run images targeted for a different architecture on Docker Desktop.\nYou can run the images using the SHA tag, and verify the architecture.\nFor example, when you run the following on a macOS:\ndocker run --rm docker.io/username/demo:latest@sha256:2b77acdfea5dc5baa489ffab2a0b4a387666d1d526490e31845eb64e3e73ed20 uname -m\naarch64\ndocker run --rm docker.io/username/demo:latest@sha256:723c22f366ae44e419d12706453a544ae92711ae52f510e226f6467d8228d191 uname -m\narmv71\nIn the above example, uname -m returns aarch64 and armv7l as expected, even when running the commands on a native macOS or Windows developer machine",
    "Is sklearn compatible with Linux-alpine?": "UPDATE: Since 2020 there is an official sklearn alpine package, which can easily be installed via:\napk add py3-scikit-learn\nhttps://pkgs.alpinelinux.org/package/edge/community/x86/py3-scikit-learn\nI would even recommend this way, instead of using pip. Some people (including me) encountered problems while trying the pip-way for scipy and/or sklearn:\nScipy error in python:3.8-alpine3.11 - No lapack/blas resources found",
    "Apache with Docker Alpine Linux": "It is true that Apache uses SIGWINCH to trigger a graceful shutdown:\ndocker kill ----signal=SIGWINCH apache\ndocker-library/httpd issue 9 mentions\nEven just dropping \"-t\" should remove the sending of SIGWINCH when the window resizes.\nActually, you need just -d: see PR669.\nIn your case, you already running the image with -dit, so check if just keeping -d might help.\nThe original issue (on httpd side, not docker) is described in bug id 1212224.\nThe OP Sebi2020 confirms in the comments:\nif I don't connect a tty the signal isn't send\nSo if possible, avoid the -t and, if needed, add a docker exec -t session if you need tty.",
    "Caching Jar dependencies for Maven-based Docker builds": "There is a new instruction regarding this topic: https://github.com/carlossg/docker-maven#packaging-a-local-repository-with-the-image\nThe $MAVEN_CONFIG dir (default to /root/.m2) is configured as a volume so anything copied there in a Dockerfile at build time is lost. For that the dir /usr/share/maven/ref/ is created, and anything in there will be copied on container startup to $MAVEN_CONFIG.\nTo create a pre-packaged repository, create a pom.xml with the dependencies you need and use this in your Dockerfile. /usr/share/maven/ref/settings-docker.xml is a settings file that changes the local repository to /usr/share/maven/ref/repository, but you can use your own settings file as long as it uses /usr/share/maven/ref/repository as local repo.",
    "chown: changing ownership of '/var/lib/mysql/': Operation not permitted": "The MariaDB images on DockerHub don't follow good practice of not requiring to be run as root user.\nYou should instead use the MariaDB images provided by OpenShift. Eg:\ncentos/mariadb-102-centos7\nSee:\nhttps://github.com/sclorg/mariadb-container\nThere should be an ability to select MariaDB from the service catalog browser in the OpenShift web console, or use the mariadb template from the command line.",
    "Spring Boot can't read application.properties in Docker": "You have to add the application.properties file in the docker /app/ directory. Ur docker directory structure will be\napp\n   -main.jar\n   -application.properties\nYou can do so by using ADD /ur/local/location/application.properties /app/application.properties\nThen better write this command in your docker file\nENTRYPOINT [\"java\" ,\"-Djava.security.egd=file:/dev/./urandom --spring.config.location=classpath:file:/app/application-properties\",\"-jar\",\"/app/main.jar\"]\nYour whole dockerFile should look like this:\nFROM java:8-jre\nVOLUME /tmp\nCOPY ./mail.jar /app/mail.jar\nADD /ur/local/location/application.properties /app/application.properties\nENTRYPOINT [\"java\" ,\"-Djava.security.egd=file:/dev/./urandom --spring.config.location=classpath:file:/app/application-properties\",\"-jar\",\"/app/main.jar\"]\nEXPOSE 8080",
    "standard_init_linux.go:190: exec user process caused \"no such file or directory\" Docker with go basic web app": "File not found can mean the file is missing, a script missing the interpreter, or an executable missing a library. In this case, the net import brings in libc by default, as a dynamic linked binary. You should be able to see that with ldd on your binary.\nTo fix it, you'll need to pass some extra flags:\nCGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -tags netgo -ldflags '-w' -o mybin *.go\nThe above is from: https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671",
    "How to install hadolint on Ubuntu [closed]": "This code works and installs the prebuilt binary file :\nwget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v2.10.0/hadolint-Linux-x86_64\nI didn't know these basic commands since I am new to Ubuntu",
    "docker build failed after pip installed requirements with exit code 137": "I was getting the below error on my windows machine:\nKilled ERROR: Service 'kafka-asr' failed to build: The command '/bin/sh -c pip install --no-cache-dir -r requirements.txt' returned a non-zero code: 137\nAfter increasing the docker memory the error got resolved.\nRight click on docker -> setting -> Advance\nI increased the memory 2304 to 2816 and clicked on Apply.",
    "Permission denied in Docker build": "For me, it was the issue with the missing permission for the Docker\nI fixed the issue with the following command.\nsudo chmod -R g+rw \"$HOME/.docker\"\nFYI: I am using a Mac M1 machine.",
    "How to build a Docker image on a specific architecture with Docker Hub?": "I solved my own issue after a bit of research... First, I was making a stupid mistake and second, I was forgetting a very important thing. Here's how I fixed my issues:\nThe Stupid Mistake\nAlthough I specified different Dockerfiles for each automated build, I also had a build hook which was overwriting the docker build command and it was defaulting to Dockerfile for all builds instead of picking the right file.\nFixed build hook file:\n#!/bin/bash\n\ndocker build \\\n    --file \"${DOCKERFILE_PATH}\" \\\n    --build-arg BUILD_DATE=\"$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\" \\\n    --build-arg VCS_REF=\"$(git rev-parse --short HEAD)\" \\\n    --tag \"$IMAGE_NAME\" \\\n    .\nThe Important Thing\nLike @JanGaraj mentioned on his answer, Docker Hub runs on amd64 so it can't run binaries for other architectures. How does one build multi-arch images with Docker Hub Automated Builds? With the help of qemu-user-static and more hooks. I found the answer on this GitHub issue but I'll post here the complete answer to my specific use case:\nMy sample project tree:\n.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Dockerfile.aarch64\n\u251c\u2500\u2500 Dockerfile.armhf\n\u2514\u2500\u2500 hooks\n    \u251c\u2500\u2500 build\n    \u251c\u2500\u2500 post_checkout\n    \u2514\u2500\u2500 pre_build\nThe post_checkout hook file:\n#!/bin/bash\n\nBUILD_ARCH=$(echo \"${DOCKERFILE_PATH}\" | cut -d '.' -f 2)\n\n[ \"${BUILD_ARCH}\" == \"Dockerfile\" ] && \\\n    { echo 'qemu-user-static: Download not required for current arch'; exit 0; }\n\nQEMU_USER_STATIC_ARCH=$([ \"${BUILD_ARCH}\" == \"armhf\" ] && echo \"${BUILD_ARCH::-2}\" || echo \"${BUILD_ARCH}\")\nQEMU_USER_STATIC_DOWNLOAD_URL=\"https://github.com/multiarch/qemu-user-static/releases/download\"\nQEMU_USER_STATIC_LATEST_TAG=$(curl -s https://api.github.com/repos/multiarch/qemu-user-static/tags \\\n    | grep 'name.*v[0-9]' \\\n    | head -n 1 \\\n    | cut -d '\"' -f 4)\n\ncurl -SL \"${QEMU_USER_STATIC_DOWNLOAD_URL}/${QEMU_USER_STATIC_LATEST_TAG}/x86_64_qemu-${QEMU_USER_STATIC_ARCH}-static.tar.gz\" \\\n    | tar xzv\nThe pre_build hook file:\n#!/bin/bash\n\nBUILD_ARCH=$(echo \"${DOCKERFILE_PATH}\" | cut -d '.' -f 2)\n\n[ \"${BUILD_ARCH}\" == \"Dockerfile\" ] && \\\n    { echo 'qemu-user-static: Registration not required for current arch'; exit 0; }\n\ndocker run --rm --privileged multiarch/qemu-user-static:register --reset\nThe Dockerfile file:\nFROM amd64/alpine:3.8\n(...)\nThe Dockerfile.aarch64 file:\nFROM arm64v8/alpine:3.8\nCOPY qemu-aarch64-static /usr/bin/\n(...)\nThe Dockerfile.armhf file:\nFROM arm32v6/alpine:3.8\nCOPY qemu-arm-static /usr/bin/\n(...)\nThat's it!",
    "Dockerfile returns npm not found on build": "You need to explicitly install node / npm in your container before running npm install. Add this to your Dockerfile.\nRUN apt-get update && apt-get install -y curl\nRUN curl -sL https://deb.nodesource.com/setup_8.x | bash -\nRUN apt-get update && apt-get install -y nodejs",
    "Docker Python File Input Selector": "You need to run the container with docker run -ti image to make sure that it runs in interactive mode with the terminal attached.\nRunning X11 GUI applications is a bit more tricky since you need to give the container access to your display. This blog post describes the process in more details.",
    "How to define OpenJDK 8 in CentOS based Dockerfile?": "Seems to be as easy as this:\nFROM centos\n\nRUN yum install -y \\\n       java-1.8.0-openjdk \\\n       java-1.8.0-openjdk-devel\n\nENV JAVA_HOME /etc/alternatives/jre\n.\n.",
    "Python + Docker + No module found": "At first sight, the error you obtain in the logs\n[\u2026] Traceback (most recent call last):\napp6_1 | File \"/code/bank_transactions/main_transactions.py\", line 5, in <module>\napp6_1 | from bank_transactions import constants\napp6_1 | ModuleNotFoundError: No module named 'bank_transactions'\nsuggests the file main_transactions.py is indeed parsed, which in turn imports the constants.py file, which fails.\nActually, this is related to the way import walks directories to find python packages and I'm pretty sure your error should vanish by setting the PYTHONPATH environment variable:\nFROM python:3.6\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /code\n\nCOPY . /code/\n\nRUN pip install -r requirements.txt \n\nENV PYTHONPATH /code\n\nCMD [ \"python\", \"/code/bank_transactions/main_transactions.py\" ]\nFor additional details, see this other StackOverflow question (which dealt with Python2 \u2212 cf. the ImportError instead of ModuleNotFoundError with Python3 \u2212 but the fix should be the same).",
    "How to run script file(.sh file) inside Dockerfile? [closed]": "Steps as follows :\nCopy '.sh' file to container\nCOPY install.sh .\nExecuting '.sh' file\nRUN ./install.sh\n'install.sh' file should be in current working directory else you can specify path.",
    "linux source command not working when building Dockerfile": "From the docker builder reference, each RUN command is run independently. So doing RUN source /usr/local/rvm/scripts/rvm does not have any effect on the next RUN command.\nTry changing the operations which require the given source file as follows\n  RUN /bin/bash -c \"source /usr/local/rvm/scripts/rvm ; gem install rails\"",
    "How to export my local docker image to a tar and the load on another computer": "The best way is use save/load commands because the CMD are saved. Using import/export commands the CMD is not saved.\nSave to the disk your docker image:\ndocker save --output=\"image_name.tar\" id_image\nLoad your docker image from the disc:\ndocker load --input image_name.tar\nif after list images the repository and tag are < none >, you can rename your image setting new repository:tag\nDocker tag new_repository:new_tag",
    "Docker Images Hierarchy": "docker tree was deprecated before any good replacement was proposed (see the debate in PR 5001)\nThis is currently externalized to justone/dockviz.\n alias dockviz=\"docker run --rm -v /var/run/docker.sock:/var/run/docker.sock nate/dockviz\"\nImage info is visualized with lines indicating parent images:\ndockviz images -d | dot -Tpng -o images.png\nas a tree in the terminal:\n$ dockviz images -t\n\u2514\u2500511136ea3c5a Virtual Size: 0.0 B\n  |\u2500f10ebce2c0e1 Virtual Size: 103.7 MB\n  | \u2514\u250082cdea7ab5b5 Virtual Size: 103.9 MB\n  ...",
    "Dockerfile not executing second stage": "Buildkit uses a dependency graph. It looks at the target stage, which by default is the last one:\nFROM base as final\nRUN echo \"3\"\nFrom there it sees that base is needed to build this stage so it pulls in the base stage:\nFROM alpine as base\nRUN echo \"1\"\nAnd from there it's done, it's not needed to build the mid stage to create your target image. There's no dependencies in the FROM or a COPY --from that would require it. This behavior differs from the classic docker build which performed steps in order until the target stage was reached, and is one of the reasons buildkit is much faster.",
    "How to inject Docker container build timestamp in container?": "The output of each RUN step during a build is the changes to the filesystem. So you can output the date to a file in your image. And the logs from the container are just the stdout from commands you run. So you can cat out the date inside your entrypoint.\nIn code, you'd have at the end of your Dockerfile:\nRUN date >/build-date.txt\nAnd inside an entrypoint script:\n#!/bin/sh\n#.... Initialization steps\necho Image built: $(cat /build-date.txt)\n#.... More initialization steps\n# run the command\nexec \"$@\"",
    "Why can't I use the build arg again after FROM in a Dockerfile?": "Why can't I use the FROM_IMAGE build arg twice, on and after a FROM line?\nThere is a real difference depending on where you put ARG related to FROM line:\nany ARG before the first FROM can be used in any FROM line\nany ARG within a build stage (after a FROM) can be used in that build stage\nThis is related to build stages mechanics and some reference of actual behavior can be found here: https://github.com/docker/cli/pull/333, and a discussion on why documentation and build mechanics is a bit confusing on ARG usage is here: https://github.com/moby/moby/issues/34129",
    "Unable to change directories while building docker Image using Dockerfile": "use WORKDIR\nhttps://docs.docker.com/engine/reference/builder/#workdir\nor do all in one RUN\nyour cd is \"forgotten\" when you are in another RUN\nBy the way, group your RUN, as indicated in the Dockerfile best practices\nhttps://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/",
    "How can I grab exposed port from inspecting docker container?": "Execute the command: docker inspect --format=\"{{json .Config.ExposedPorts }}\" src_python_1\nResult: {\"8000/tcp\":{}}\nProof (using docker ps):\ne5e917b59e15        src_python:latest   \"start-server\"         22 hours ago        Up 22 hours         0.0.0.0:8000->8000/tcp                                        src_python_1",
    "Nuget package restore error in Docker Compose build": "SOLVED:\nIt turns out to be a networking issue. I am behind a corporate firewall at work that leverages TLS packet inspection to break apart SSL traffic. The build process while debugging runs as \"me\" on my local machine, however, the release build (docker-compose) actually pulls down a aspnetcore-build docker image, copies your code to the docker container, then runs dotnet restore to get fresh nuget packages for your docker image. These actions can be found in the Docker File in your project. This \"dotnet restore\" inside the container, runs under a different security context, and therefore was getting hung up. We traced the network traffic which was hard for me to get to because of how docker networking works. Fiddler was not catching the traffic. Using wireshark, we were able to catch it from a device level and see the drop. The reason it continued to fail from my home network was due to the configuration with our hypervisor & networking.\nRESOLUTIONS:\nAdd a firewall rule for https://api.nuget.org/v3/index.json (Preferred) OR Build the image from VSTS in the cloud OR Build from a different network.\nPS4 please post back if you are able to resolve this the same way? Having spent 3 days on this, I'm curious about your status.",
    "How to access the Host's machine's localhost 127.0.0.1 from docker container": "its not working, its looks like inside container its trying to connect localhost of container.\nYes, that is the all idea behind the isolation provided by container, even docker build (which builds one container per Dockerfile line, and commits it in an intermediate image).\nAs commented by the OP dhairya, and mentioned in the documentation I referred in my comments: \"Docker container networking\" and docker build, you can set to host the networking mode for the RUN instructions during build: then localhost would refer to the host localhost.\n docker build --network=\"host\"\nThis is since API 1.25+ only, in docker v.1.13.0-rc5 (January 2017)\nPOST /build accepts networkmode parameter to specify network used during build.\nBut if you don't need Git in your actual built image (for its main process to run), it would be easier to\nclone the repo locally (directly on the host) before the docker build.\nYou need to clone it where your Dockerfile is, as it must be relative to the source directory that is being built (the context of the build).\nuse a COPY directive in the Dockerfile, to copy the host folder representing the checked out Git repo.\nNote: add .git/: to your .dockerignore (file in the same folder as your Dockerfile), in order to not copy the repo_dir/.git folder of that cloned repo, if you don't have git in your target image.\nCOPY repo_dir .\n(Here '.' represent the current WORKDIR within the image being built)",
    "Using ARG in FROM in dockerfile": "Using the docs for reference, if you want to use ARG in FROM, ARG must be the first line in your Dockerfile (as reminded by koby.el's answer) and FROM must come immediately after. See this section for details.\nThis minimal Dockerfile works:\nARG url=docker-local.artifactory.com/projectA \nFROM $url\nBuilt using this command with a build arg:\ndocker build -t from --build-arg url=alpine:3.9 .\n[+] Building 0.1s (5/5) FINISHED\n => [internal] load build definition from Dockerfile                                                               0.0s\n => => transferring dockerfile: 116B                                                                               0.0s\n => [internal] load .dockerignore                                                                                  0.0s\n => => transferring context: 2B                                                                                    0.0s\n => [internal] load metadata for docker.io/library/alpine:3.9                                                      0.0s\n => CACHED [1/1] FROM docker.io/library/alpine:3.9                                                                 0.0s\n => exporting to image                                                                                             0.0s\n => => exporting layers                                                                                            0.0s\n => => writing image sha256:352159a49b502edb1c17a3ad142b320155bd541830000c02093b79f4058a3bd1                       0.0s\n => => naming to docker.io/library/from\nThe docs also show an example if you want to re-use the ARG value after the first FROM command:\nARG url=docker-local.artifactory.com/projectA \nFROM $url\nARG url\nRUN echo $url",
    "Two docker containers cannot communicate": "Do not use localhost to communicate between containers. Networking is one of the namespaces in docker, so localhost inside of a container only connects to that container, not to your external host, and not to another container. In this case, use the service name, graph-db, instead of localhost, in your app to connect to the db.",
    "error while creating mount source path mkdir /host_mnt/d: file exists": "I also experienced this issue (using Docker Desktop for Windows). For me, I simply issued a restart for Docker by using the following steps:\nLocate the Docker Desktop Icon (in the Windows Task Bar)\nRight-Click the Docker Desktop Icon and choose \"Restart...\"\nClick the \"Restart\" button on the confirmation dialog that appears\nWhen Docker Desktop had restarted, I then:\nChecked for running containers using docker-compose ps\nStopped any running containers using docker-compose down\nStarted the containers docker-compose up\nAnd Voila! All containers restarted successfully with no mount errors.",
    "running two nodejs apps in one docker image": "I recommend using pm2 as the entrypoint process which will handle all your NodeJS applications within docker image. The advantage of this is that pm2 can bahave as a proper process manager which is essential in docker. Other helpful features are load balancing, restarting applications which consume too much memory or just die for whatever reason, and log management.\nHere's a Dockerfile I've been using for some time now:\n#A lightweight node image\nFROM mhart/alpine-node:6.5.0\n\n#PM2 will be used as PID 1 process\nRUN npm install -g pm2@1.1.3\n\n# Copy package json files for services\n\nCOPY app1/package.json /var/www/app1/package.json\nCOPY app2/package.json /var/www/app2/package.json\n\n# Set up working dir\nWORKDIR /var/www\n\n# Install packages\nRUN npm config set loglevel warn \\\n# To mitigate issues with npm saturating the network interface we limit the number of concurrent connections\n    && npm config set maxsockets 5 \\\n    && npm config set only production \\\n    && npm config set progress false \\\n    && cd ./app1 \\\n    && npm i \\\n    && cd ../app2 \\\n    && npm i\n\n\n# Copy source files\nCOPY . ./\n\n# Expose ports\nEXPOSE 3000\nEXPOSE 3001\n\n# Start PM2 as PID 1 process\nENTRYPOINT [\"pm2\", \"--no-daemon\", \"start\"]\n\n# Actual script to start can be overridden from `docker run`\nCMD [\"process.json\"]\nprocess.json file in the CMD is described here",
    "Difference between nodejs v0.12 and v5.x distributions": "You should definitely not use any of the v0.x versions of Node.js as support for them are set to expire in 2016.\nYou should use either v4 (code name argon) which is the next LTS (long term support) version of Node.js or v5 which is the latest stable version.\nAlso, Node.js has an official Docker Image:\nFROM node:5",
    "How to set mysql username in dockerfile": "If you take a look at the official Docker MySQL image Dockerfile, you will discover how they did it using debconf-set-selections.\nThe relevant instructions are:\nRUN { \\\n        echo mysql-community-server mysql-community-server/data-dir select ''; \\\n        echo mysql-community-server mysql-community-server/root-pass password ''; \\\n        echo mysql-community-server mysql-community-server/re-root-pass password ''; \\\n        echo mysql-community-server mysql-community-server/remove-test-db select false; \\\n    } | debconf-set-selections \\\n    && apt-get update && apt-get install -y mysql-server\ndebconf-set-selections is a tool that allows you to prepare the answers for the questions that will be asked during the later installation.",
    "\"docker build\" requires exactly 1 argument [duplicate]": "You should provide the context, current directory for instance: ..\ndocker build -f C://Users/XXXX/XXXX/XXXX/XXXX/XXXX/Dockerfile -t something .",
    "How to set JVM settings in Dockerfile": "You can simply set the JAVA_OPTS value to the one you need at build time, in your Dockerfile :\nENV JAVA_OPTS=\"-DTOMCAT=Y -DOracle.server=1234 [...]\"\nYou may also simply set it a runtime if you don't modify the CMD from the official tomcat image:\n$ docker run -e JAVA_OPTS=\"-DTOMCAT=Y -DOracle.server=1234 [...]\" your_image:your_tag \nSee: https://github.com/docker-library/tomcat/issues/8\nConsidering the options you're providing in your example, it would be better to opt for the second version (host, port and password information should not be left in a Docker image, from a security standpoint).\nIf you're only providing minimal requirements, resource-wise, for your application, this could live in the Dockerfile.",
    "How do I setup only python 2.7 in a docker container?": "You can use python base image\nFROM python:2.7\nThis base image with have python pre-configured and you don't need to install python seperately. Hope it helps.\nHere is the list of available image\nFor quick reference please check https://blog.realkinetic.com/building-minimal-docker-containers-for-python-applications-37d0272c52f3",
    "Pass ENV in docker run command": "It looks like you might be confusing the image build with the container run. If the difference between the two isn't immediately clear, I'd recommend reviewing some other questions and docs like:\nIn Docker, what's the difference between a container and an image? https://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nRUN echo \"${FILENAME} ${animals}\" > ./entrypoint.sh\nWith the above, the variables will be expanded during the image build. The entrypoint.sh will not contain ${FILENAME} ${animals}. Instead, it will contain\nfile_to_run.zip turtle, monkey, goose\nAfter the build, the docker run command will create a container from that image and run the above script with the environment variables defined but never used since the script already has the variables expanded. To prevent the variable expansion, you need to escape the $ or use single quotes to prevent the expansion, e.g.\nRUN echo \"\\${FILENAME} \\${animals}\" > ./entrypoint.sh\nor\nRUN echo '${FILENAME} ${animals}' > ./entrypoint.sh\nI would also recommend being explicit with a #!/bin/ash at the top of this script. Then when you run the script, do not override the command with parameters after the image name. Instead set the environment variables with the appropriate flag to run:\ndocker run -it -e animals=\"mouse,rat,kangaroo\" image ",
    "Fix umask for future RUN commands in dockerfile": "Docker creates a new, minimal sh environment for every RUN step.\nThe umask is set to 0022 in runc by default when a container is started. A umask configuration option has been exposed in runc but unfortunately that is not configurable from Docker yet.\nFor now, the umask command (or the process setting the umask) will need to be chained in each RUN step where it is needed, while the subsequent chained commands run under the same shell process.\nRUN set -uex; \\\n    umask 0002; \\\n    do_something; \\\n    do_otherthing;\nRUN set -uex; \\\n    umask 0002; \\\n    do_nextthing; \\\n    do_subsequentthing;",
    "Error unknown time zone America/Los_Angeles in time.LoadLocation": "For anyone looking for an answer, this helped me.\nadding these two line to docker file , (final if it's a 2 stage build)\nADD https://github.com/golang/go/raw/master/lib/time/zoneinfo.zip /zoneinfo.zip\nENV ZONEINFO /zoneinfo.zip",
    "Adding default external network in docker-compose": "First of all, check your version of the file. For version 3.6, the following examples can meet your needs:\nExample 1:\nversion: '3.6'\nservices:\n  webserver:\n...\n#add existing database network \nnetworks:\n  default:\n    external:\n      name: proxy_host\nExample 2:\nversion: '3.6'\nservices:\n  webserver:\n...\n#add existing database network \nnetworks:\n  default:\n    name: proxy_host\n    external: true\nExample 3: This configuration creates new networks.\nversion: '3.6'\nservices:\n  webserver:\n    networks:\n      - proxy_host\n      - database_host\n...\nnetworks:\n  proxy_host: {}\n  database_host: {}",
    "Routing doesn't work after Dockerizing Angular app": "This is happening because the base nginx image will try to serve the requests from the docroot, so it will try to find a login.html when you send a request to /login.\nTo avoid this, you need nginx to serve the index.html no matter the request, and thus letting angular take care of the routes.\nTo do so, you will need to change your nginx.conf, which is currently in the image, to include the following :\ntry_files $uri $uri/ /index.html;\nInstead of the default being:\ntry_files $uri $uri/ =404;\nYou can do so in many ways, but I guess the best approach is to have an extra command in your Dockerfile, that copies over the nginx configuration like so :\nCOPY nginx/default.conf /etc/nginx/conf.d/default.conf\nAnd have that nginx/default.conf file in your directory (containing the default nginx configuration) with the command specified above replaced.\nEDIT\nThere is already images that does exactly that, so you can just use an image other than the official one, that is made specifically for that, and it should work fine: example",
    "Mounting Maven Repository to Docker": "I finally found the solution for mounting my local maven repository in docker. I changed my solution; I am mounting it in the run phase instead of build phase. This is my Dockerfile:\nFROM ubuntu\nMAINTAINER Zeinab Abbasimazar\nADD gwr $HOME\nRUN apt-get update; \\\n    apt-get install -y --no-install-recommends apt-utils; \\\n    apt-get install -y wget unzip curl maven git; \\\n    echo \\\n    \"<settings xmlns='http://maven.apache.org/SETTINGS/1.0.0\\' \\\n    xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance' \\\n    xsi:schemaLocation='http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd'> \\\n        <localRepository>/root/.m2/repository</localRepository> \\\n        <interactiveMode>true</interactiveMode> \\\n        <usePluginRegistry>false</usePluginRegistry> \\\n        <offline>false</offline> \\\n    </settings>\" \\\n    > /usr/share/maven/conf/settings.xml; \\\n    mkdir /root/.m2/; \\\n    echo \\\n    \"<settings xmlns='http://maven.apache.org/SETTINGS/1.0.0\\' \\\n    xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance' \\\n    xsi:schemaLocation='http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd'> \\\n        <localRepository>/root/.m2/repository</localRepository> \\\n        <interactiveMode>true</interactiveMode> \\\n        <usePluginRegistry>false</usePluginRegistry> \\\n        <offline>false</offline> \\\n    </settings>\" \\\n    > /root/.m2/settings.xml\nWORKDIR .\nCMD mvn  -X clean install -pl components -P profile\nAt first, I build the image using above Dockerfile:\nsudo docker build -t imageName:imageTag .\nThen, I run a container as below:\nsudo docker run -d -v /home/zeinab/.m2/:/root/.m2/ --name containerName imageName:imageTag",
    "Pass variables from .env file to dockerfile through docker-compose": "I'm posting a new answer to highlight the various assumptions related to the OP's question, in particular, the fact that there's a subtle difference between the \".env\" unique filename and *.env files (arguments for env_file:).\nBut apart from this subtlety, the process to pass arguments from docker-compose.yml to docker build -f Dockerfile . and/or docker run -e \u2026 is easy, as shown by the comprehensive example below.\nMinimal working example\nLet's consider the following files in a given directory, say ./docker.\nFile docker-compose.yml:\nservices:\n  demo-1:\n    image: demo-${ENV_NUM}\n    build:\n      context: .\n      args:\n        ARG1: \"demo-1/${ARG1}\"\n        ARG3: \"demo-1/${ARG3}\"\n  demo-2:\n    image: demo-2${ENV_FILE_NUM}\n    build:\n      context: .\n      args:\n        ARG1: \"demo-2/${ARG1}\"\n        ARG3: \"demo-2/${ARG3}\"\n    env_file:\n      - var.env\nRemark: even if we use a build: field, it appears to be a good idea to also add an image: field to automatically tag the built image; but note that these image names must be pairwise different.\nFile .env:\nKEY=\"some value\"\nENV_NUM=1\nARG1=.env/ARG1\nARG2=.env/ARG2\nARG3=.env/ARG3\nFile var.env:\nENV_FILE_NUM=\"some number\"\nARG1=var.env/ARG1\nARG2=var.env/ARG2\nARG3=var.env/ARG3\nARG4=var.env/ARG4\nFile Dockerfile:\nFROM debian:10\n\n# Read build arguments (default value if omitted at CLI)\nARG ARG1=\"default 1\"\nARG ARG2=\"default 2\"\nARG ARG3=\"default 3\"\n\n# the build args are exported at build time\nRUN echo \"ARG1=${ARG1}\" | tee /root/arg1.txt\nRUN echo \"ARG2=${ARG2}\" | tee /root/arg2.txt\nRUN echo \"ARG3=${ARG3}\" | tee /root/arg3.txt\n\n# Export part of these args at runtime also\nENV ARG1=\"${ARG1}\"\nENV ARG2=\"${ARG2}\"\n\n# exec-form is mandatory for ENTRYPOINT/CMD\nCMD [\"/bin/bash\", \"-c\", \"echo ARG1=\\\"${ARG1}\\\" ARG2=\\\"${ARG2}\\\" ARG3=\\\"${ARG3}\\\"; echo while at build time:; cat /root/arg{1,2,3}.txt\"]\nExperiment session 1\nFirst, as suggested by @SergioSantiago in the comments, a very handy command to preview the effective docker-compose.yml file after interpolation is docker-compose config:\n$ docker-compose config\n\nWARN[0000] The \"ENV_FILE_NUM\" variable is not set. Defaulting to a blank string. \nname: docker\nservices:\n  demo-1:\n    build:\n      context: /home/debian/docker\n      dockerfile: Dockerfile\n      args:\n        ARG1: demo-1/.env/ARG1\n        ARG3: demo-1/.env/ARG3\n    image: demo-1\n    networks:\n      default: null\n  demo-2:\n    build:\n      context: /home/debian/docker\n      dockerfile: Dockerfile\n      args:\n        ARG1: demo-2/.env/ARG1\n        ARG3: demo-2/.env/ARG3\n    environment:\n      ARG1: var.env/ARG1\n      ARG2: var.env/ARG2\n      ARG3: var.env/ARG3\n      ARG4: var.env/ARG4\n      ENV_FILE_NUM: some number\n    image: demo-2\n    networks:\n      default: null\nnetworks:\n  default:\n    name: docker_default\nHere, as indicated by the warning, we see there's an issue for interpolating ENV_FILE_NUM despite the fact this variable is mentioned by var.env. The reason is that env_files lines just add new environment variables for the underlying docker run -e \u2026 command, but don't interpolate anything in the docker-compose.yml.\nContrarily, one can notice that the value ARG1=.env/ARG1 taken from \".env\" is interpolated within the args: field of docker-compose.yml, cf. the output line:\nargs:\n  ARG1: demo-1/.env/ARG1\n  \u2026\nThis very distinct semantics of \".env\" vs. env_files is described in this page of the official documentation.\nExperiment session 2\nNext, let us run:\n$ docker-compose up --build\n                                                                                         \nWARN[0000] The \"ENV_FILE_NUM\" variable is not set. Defaulting to a blank string.                                                                     \n[+] Building 10.4s (13/13) FINISHED\n => [demo-1 internal] load build definition from Dockerfile\n => => transferring dockerfile: 609B\n => [demo-2 internal] load build definition from Dockerfile\n => => transferring dockerfile: 609B\n => [demo-1 internal] load .dockerignore\n => => transferring context: 2B\n => [demo-2 internal] load .dockerignore\n => => transferring context: 2B\n => [demo-2 internal] load metadata for docker.io/library/debian:10\n => [demo-2 1/4] FROM docker.io/library/debian:10@sha256:ebe4b9831fb22dfa778de4ffcb8ea0ad69b5d782d4e86cab14cc1fded5d8e761\n => => resolve docker.io/library/debian:10@sha256:ebe4b9831fb22dfa778de4ffcb8ea0ad69b5d782d4e86cab14cc1fded5d8e761\n => => sha256:85bed84afb9a834cf090b55d2e584abd55b4792d93b750db896f486680638344 50.44MB / 50.44MB\n => => sha256:ebe4b9831fb22dfa778de4ffcb8ea0ad69b5d782d4e86cab14cc1fded5d8e761 1.85kB / 1.85kB\n => => sha256:40dd1c1b1c36eac161ab63b6ce3a57d56ad79a667a37717a31721bac3f30aaf9 529B / 529B\n => => sha256:26a2b081e03207d26a105340161109ba0f00e857cbb0ff85aaeeeadd46b709c5 1.46kB / 1.46kB\n => => extracting sha256:85bed84afb9a834cf090b55d2e584abd55b4792d93b750db896f486680638344\n => [demo-2 2/4] RUN echo \"ARG1=demo-2/.env/ARG1\" | tee /root/arg1.txt\n => [demo-1 2/4] RUN echo \"ARG1=demo-1/.env/ARG1\" | tee /root/arg1.txt\n => [demo-1 3/4] RUN echo \"ARG2=default 2\" | tee /root/arg2.txt\n => [demo-2 3/4] RUN echo \"ARG2=default 2\" | tee /root/arg2.txt\n => [demo-2 4/4] RUN echo \"ARG3=demo-2/.env/ARG3\" | tee /root/arg3.txt\n => [demo-1 4/4] RUN echo \"ARG3=demo-1/.env/ARG3\" | tee /root/arg3.txt\n => [demo-2] exporting to image\n => => exporting layers\n => => writing image sha256:553f294a410ceeb3c0ac9d252d443710c804d3f7437ad7fffa586967517f5e7a\n => => naming to docker.io/library/demo-1\n => => writing image sha256:84bb2bd0ffae67ffed0e74efbf9253b6d634a6f37c6f99bc4eedea81846a9352\n => => naming to docker.io/library/demo-2\n                                     \nUse 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n[+] Running 3/3              \n \u283f Network docker_default     Created\n \u283f Container docker-demo-1-1  Created\n \u283f Container docker-demo-2-1  Created\nAttaching to docker-demo-1-1, docker-demo-2-1\n\ndocker-demo-1-1  | ARG1=demo-1/.env/ARG1 ARG2=default 2 ARG3=\ndocker-demo-1-1  | while at build time:\ndocker-demo-1-1  | ARG1=demo-1/.env/ARG1\ndocker-demo-1-1  | ARG2=default 2\ndocker-demo-1-1  | ARG3=demo-1/.env/ARG3\n\ndocker-demo-2-1  | ARG1=var.env/ARG1 ARG2=var.env/ARG2 ARG3=var.env/ARG3\ndocker-demo-2-1  | while at build time:\ndocker-demo-2-1  | ARG1=demo-2/.env/ARG1\ndocker-demo-2-1  | ARG2=default 2\ndocker-demo-2-1  | ARG3=demo-2/.env/ARG3\n\ndocker-demo-1-1 exited with code 0\ndocker-demo-2-1 exited with code 0\nHere, we can see again that the \".env\" values and those of file_env: [ filename.env ] play different roles that don't overlap.\nFurthermore:\nGiven the absence of a Dockerfile command line ENV ARG3=\"${ARG3}\", the value of build-arg ARG3 is not propagated at runtime (see the ARG3= line in the output above).\nBut the value can be exported at runtime anyway if it is defined/overidden in the environment: or env_file: sections in the docker-compose.yml file (see the ARG3=var.env/ARG3 line in the output above).\nFor more details, see the documentation of the ARG directive.\nRemarks on the docker-compose --env-file option use-case\nAs mentioned by the OP, docker-compose also enjoys a useful CLI option --env-file (which is precisely named the same way as the very different env-file: field, which is unfortunate, but nevermind).\nThis option allows for the following use-case (excerpt from OP's code):\nFile docker-compose.yml:\nservices:\n  home:\n    image: home-${ENV_NUM}\n    build:\n      args:\n        ARG1: \"${ARG1}\"\n      ...\n    labels:\n      - traefik.http.routers.home.rule=Host(`${DOMAIN}`)\n      ...\n    env_file:\n      - ${ENV}\nFile prod.env:\nDOMAIN = 'actualdomain.com'\nENV = 'prod.env'\nENV_NUM = 1\nARG1 = 'value 1'\nFile dev.env:\nDOMAIN = 'localhost'\nENV = 'dev.env'\nENV_NUM = 0\nARG1 = 'value 1'\nThen run:\ndocker-compose --env-file prod.env build,\nor docker-compose --env-file dev.env build\nAs an aside, even if most of this answer up to now, amounted to illustrating that the \".env\" filename and env_file: files enjoy a very different semantics\u2026 it is true that they can also be combined \"nicely\" this way, as suggested by the OP, to achieve this use case.\nNote in passing that the docker-compose config is also applicable to \"debug\" the Compose specification:\ndocker-compose --env-file prod.env config,\nor docker-compose --env-file dev.env config.\nNow regarding the last question:\nGetting the values from the prod.env or dev.env files to docker-compose is not the issue. The issue is getting those values to the Dockerfile.\nit can first be noted that there are two different cases:\nEither the two different deployment environments (prod.env and dev.env) can share the same image, so that the difference only lies in the runtime environment variables (not the docker build args).\nOr, depending on the file passed for --env-file, the images should be different (and then a docker-compose --env-file \u2026 build is indeed necessary).\nIt appears that most of the time, case 1. can be achieved (and it is also the case in the question's configuration, because the ARG1 values are the same in prod.env and dev.env) and can be viewed as more-interesting for the sake of reproducibility (because we are sure that the \"prod\" image will be the same as the \"dev\" image).\nYet, sometimes it's impossible to do so and we are \"in case 2.\", e.g. if the Dockerfile has a specific step, maybe related to tests or so, that has to be enabled (resp. disabled) in production mode.\nSo now, let us assume we're in case 2. How can we pass \"everything\" from the --env-file to the Dockerfile? There is only one solution, namely, extending the args: map of the docker-compose.yml and include each variable you are interested in, for example:\nservices:\n  home:\n    image: home-${ENV_NUM}\n    build: \n      context: .\\home\n      args:\n        DOMAIN: \"${DOMAIN}\"\n        ENV_NUM: \"${ENV_NUM}\"\n        ARG1: \"${ARG1}\"\n    networks:\n      - demo-net\n    env_file:\n      - ${ENV}\n    labels:\n      - traefik.enable=true\n      - traefik.http.routers.home.rule=Host(`${DOMAIN}`)\n      - traefik.http.routers.home.entrypoints=web\n    volumes:\n      - g:\\:c:\\sharedrive\nEven if there is no other solution to pass arguments at build time (from docker-compose to the underlying docker build -f Dockerfile \u2026), this has the advantage of being \"declarative\" (only the variables mentioned in args: will be actually passed to the Dockerfile).\nDrawback?\nThe only drawback I see is that you may have unneeded extra environment variables at runtime (from docker-compose to the underlying docker run -e \u2026), such as ENV=prod.env.\nIf this is an issue, you might want to split your \".env\" files like this:\nFile prod.env:\nDOMAIN = 'actualdomain.com'\nENV = 'prod-run.env'\nENV_NUM = 1\nARG1 = 'value 1'\nFile prod-run.env:\nDOMAIN = 'actualdomain.com'\nENV_NUM = 1\n(assuming you only want to export these two environment variables at runtime).\nOr alternatively, to better follow the usual Do-not-Repeat-Yourself rule, remove prod-run.env, then pass these values as docker-compose build arguments as mentioned previously:\nargs:\n  DOMAIN: \"${DOMAIN}\"\n  ENV_NUM: \"${ENV_NUM}\"\nand write in the Dockerfile:\nARG DOMAIN\nARG ENV_NUM\n\n# ... and in the end:\n\nENV DOMAIN=\"${DOMAIN}\"\nENV ENV_NUM=\"${ENV_NUM}\"\nI already gave an example of these Dockerfile directives in section \"Experiment session 2\".\n(Sorry for the significant length of this answer BTW :)",
    "Not understanding how docker build --secret is supposed to be used": "Short answer\nCould you try to combine 2 RUN coverage with RUN --mount ... ?\nI think it will be something like that :\nRUN --mount=type=secret,id=mysecret \\\n  export MYSECRET=$(cat /run/secrets/mysecret) \\\n  && coverage run --omit='src/manage.py,src/config/*,*/.venv/*,*/*__init__.py,*/tests.py,*/admin.py' src/manage.py test src \\\n  && coverage report\nDetail\nThis --secret flag persists only in one docker layer.\nThe final image built will not have the secret file\nDocumentation :\nhttps://docs.docker.com/engine/reference/builder/#run---mounttypesecret\nBlog about it:\nhttps://pythonspeed.com/articles/docker-buildkit/\neach RUN creates one intermediate layer.\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nSo, according to my understanding,\nRUN --mount=type=secret,id=mysecret export MYSECRET=$(cat /run/secrets/mysecret)\nwill not persist both in your final image and in your last RUN.\nEDIT 1\nRUN is executed by /bin/sh not /bin/bash so you have to instanciate MYSECRET and export it in 2 steps :\n/bin/sh -c [your RUN instructions]\nRUN --mount=type=secret,id=mysecret \\\n    MYSECRET=$(cat /run/secrets/mysecret) \\\n    && export MYSECRET \\\n    && coverage run --omit='src/manage.py,src/config/*,*/.venv/*,*/*__init__.py,*/tests.py,*/admin.py' src/manage.py test src \\\n    && coverage report\nHere is my working example :\ntest.py\nprint MYSECRET value\nimport os\n\nprint(os.environ[\"MYSECRET\"])\nmysecret.txt\ncontains MYSECRET value\nHello\nDockerfile\nbased on python:3.9 image\nFROM python:3.9\n\nCOPY test.py /tmp/test.py\n\nRUN --mount=type=secret,id=mysecret \\\n  MYSECRET=$(cat /run/secrets/mysecret) \\\n  && export MYSECRET \\\n  && python /tmp/test.py\nBuild output\n...\n#7 [3/3] RUN --mount=type=secret,id=mysecret     MYSECRET=$(cat /run/secrets/mysecret)     && export MYSECRET     && python /tmp/test.py\n#7 sha256:f0134fcb389100e7094a47f437f8630e67da83447daaf617756c1d8432163bae\n#7 0.374 Hello\n#7 DONE 0.4s\n...",
    "speed up docker healthcheck containers": "Edit 2023-07-17\nThere is now support implemented for setting a start-interval to check in a shorter interval during the start-period\nThe feature is expected to be released in version 25 of the docker engine.\nExample:\nHEALTHCHECK --interval=5m --start-period=3m --start-interval=10s \\\n  CMD curl -f http://localhost/ || exit 1\nIn this example, there is a start-interval of 10s, which means the first healthcheck is done 10s after the container has been started. After that, the healthcheck is repeated every 10s, until either the health state switches to healthy, or the start-period is over (3m). After that, it proceeds the healthchecks at the regular interval (5m).\nDocker Docs: https://docs.docker.com/engine/reference/builder/#healthcheck\nRelated pull request: https://github.com/moby/moby/pull/40894\nOriginal Answer as of 2021\nThere is currently no built in way to decrease the time, until the first healthcheck is performed. Docker always waits a full interval between the container start and the first healthcheck. The start-period option just defines a grace time, that allow healthchecks to fail without marking the container as unhealthy. This will only be meaningful, if the interval is lower than the start-period.\nThere is a feature request to add an option, that decreases the interval while the container is starting, to get the container faster into a healthy state: https://github.com/moby/moby/issues/33410",
    "How to install tshark on Docker?": "RUN DEBIAN_FRONTEND=noninteractive apt-get install -y tshark\nWill install without needing user interaction.",
    "Docker container works from Dockerfile but get next: not found from docker-compose container": "This is happening because your local filesystem is being mounted over what is in the docker container. Your docker container does build the node modules in the builder stage, but I'm guessing you don't have the node modules available in your local file system.\nTo see if this is what is happening, on your local file system, you can do a yarn install. Then try running your container via docker again. I'm predicting that this will work, as yarn will have installed next locally, and it is actually your local file system's node modules that will be run in the docker container.\nOne way to fix this is to volume mount everything except the node modules folder. Details on how to do that: Add a volume to Docker, but exclude a sub-folder\nSo in your case, I believe you can add a line to your compose file:\nfrontend:\n    ...\n    volumes:\n      - ./frontend:/app\n      - ./frontend/node_modules # <-- try adding this!\n    ...\nThat should allow the docker container's node_modules to not be overwritten by any volume mount.",
    "dockerfile copy from with variable": "You can do what you want, but you need to have a FROM statement identifying the image you want to copy from. Something like this\nARG MY_VERSION\nFROM my-image:$MY_VERSION as source\nFROM scratch as final\nCOPY --from=source /src /dst\nReplace scratch with the base image you want to use for your new image.\nHere's an example to show that it works. Dockerfile:\nARG MY_VERSION\nFROM ubuntu:$MY_VERSION as source\nFROM alpine:latest\nCOPY --from=source /etc/os-release /\nBuild and run with\ndocker build --build-arg MY_VERSION=20.04 -t test .\ndocker run --rm test cat /os-release\nThe output shows\nNAME=\"Ubuntu\"\nVERSION=\"20.04.2 LTS (Focal Fossa)\"\nID=ubuntu\n...\nwhich shows that it has copied a file from the Ubuntu 20.04 image to an Alpine image.",
    "What exactly does \"-Djava.security.egd=file:/dev/./urandom\" do when containerizing a Spring Boot application": "The purpose of that security property is to speed up tomcat startup. By default the library used to generate random number in JVM on Unix systems relies on /dev/random. On docker containers there isn't enough entropy to support /dev/random. See Not enough entropy to support /dev/random in docker containers running in boot2docker. The random number generator is used for session ID generation. Changing it to /dev/urandom will make the startup process faster.\nSimilar question Slow startup on Tomcat 7.0.57 because of SecureRandom",
    "How to install kerberos client in docker?": "You need the -y parameter for the apt\nFROM node:latest\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get -qq update && \\\n    apt-get -yqq install krb5-user libpam-krb5 && \\\n    apt-get -yqq clean\n\nCOPY / ./\n\nEXPOSE 3000\n\nCMD [\"npm\", \"start\"]\nAnd pay attention, that each RUN directive creates one additional layer in the image. That means, your clean command will create a new layer, but all package cache will remain in other layers. So reducing the amount of these directives will be nice. It would help you to shrink the image size.",
    "How does Docker run a command without invoking a command shell?": "If I understand your question correctly, you're asking how something can be run (specifically in the context of docker) without invoking a command shell.\nThe way things are run in the linux kernel are usually using the exec family of system calls.\nYou pass it the path to the executable you want to run and the arguments that need to be passed to it via an execl call for example.\nThis is actually what your shell (sh, bash, ksh, zsh) does under the hood anyway. You can observe this yourself if you run something like strace -f bash -c \"cat /tmp/foo\"\nIn the output of that command you'll see something like this:\nexecve(\"/bin/cat\", [\"cat\", \"/tmp/foo\"], [/* 66 vars */]) = 0\nWhat's really going on is that bash looks up cat in $PATH, it then finds that cat is actually an executable binary available at /bin/cat. It then simply invokes it via execve. and the correct arguments as you can see above.\nYou can trivially write a C program that does the same thing as well. This is what such a program would look like:\n#include<unistd.h>\n\nint main() {\n\n    execl(\"/bin/cat\", \"/bin/cat\", \"/tmp/foo\", (char *)NULL);\n\n    return 0;\n}\nEvery language provides its own way of interfacing with these system calls. C does, Python does and Go, which is what's used to write Docker for the most part, does as well. A RUN instruction in the docker likely translates to one of these exec calls when you hit docker build. You can run an strace -f docker build and then grep for exec calls in the log to see how the magic happens.\nThe only difference between running something via a shell and directly is that you lose out on all the fancy stuff your shell will do for you, such as variable expansion, executable searching etc.",
    "What does the Docker sleep command do? [closed]": "docker container run -d --name mycontainer myimage:mytag sleep infinity\nThe last part after the image name (i.e. sleep infinity) is not a docker command but a command sent to the container to override its default command (set in the Dockerfile).\nAn extract from the documentation (you can get it typing man sleep in your terminal, it may vary depending on the implementation)\nPause for NUMBER seconds. SUFFIX may be 's' for seconds (the default), 'm' for minutes, 'h' for hours or 'd' for days\nTo my surprise, the parameter infinity is not documented in my implementation but is still accepted. It's quite easy to understand: it pauses indefinitely (i.e. until the command is stopped/killed). In your own example above, it will pause for one day.\nWhat is the usual reason to use sleep as a command to run a docker container?\nA docker container will live until the command it runs finishes. This command is normally set in the Dockerfile used to build the image (in a CMD stanza) and can be overridden on the command line (as in the above examples).\nA number of base images (like base OS for debian, ubuntu, centos....) will run a shell as the default command (bash or sh in general). If you try to spawn a container from that image using its default command, it will live until the shell exits.\nWhen running such an image interactively (i.e. with docker container run -it .....), it will run until you end your shell session. But if you want to launch it in the background (i.e. with docker container run -d ...) it will exit immediately leaving you with a stopped container.\nIn this case, you can \"fake\" a long running service by overriding the default command with a long running command that basically does nothing but wait for the container to stop. Two widely used commands for this are sleep infinity (or whatever period suiting your needs) and tail -f /dev/null\nAfter you launched a container like this you can use it to test whatever you need. The most common way is to run an interactive shell against it:\n# command will depend on shells available in your image\ndocker exec -it mycontainer [bash|sh|zsh|ash|...]\nOnce you are done with your experimentation/test, you can stop and recycle your container\ndocker container stop mycontainer\ndocker container rm mycontainer",
    "Couldn't connect to Docker Aerospike from host": "The IP 172.17.0.2 is only accessible within Docker (therefore you can use another container to connect). In case you want to connect from your host you need to map the respective port.\ndocker run -d --name aerospike -p 3000:3000 aerospike/aerospike-server\nAfterwards you can use:\nAerospikeClient client = new AerospikeClient(\"localhost\", 3000);",
    "Docker ONBUILD COPY doesn't seem to copy files": "You have to use COPY to build your image. Use ONBUILD if your image is kind of template to build other images.\nSee Docker documentation:\nThe ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile.",
    "Parse a variable with the result of a command in DockerFile": "I had same issue and found way to set environment variable as result of function by using RUN command in dockerfile.\nFor example i need to set SECRET_KEY_BASE for Rails app just once without changing as would when i run:\ndocker run  -e SECRET_KEY_BASE=\"$(openssl rand -hex 64)\"\nInstead it i write to Dockerfile string like:\nRUN bash -l -c 'echo export SECRET_KEY_BASE=\"$(openssl rand -hex 64)\" >> /etc/bash.bashrc'\nand my env variable available from root, even after bash login. or may be\nRUN /bin/bash -l -c 'echo export SECRET_KEY_BASE=\"$(openssl rand -hex 64)\" > /etc/profile.d/docker_init.sh'\nthen it variable available in CMD and ENTRYPOINT commands\nDocker cache it as layer and change only if you change some strings before it.\nYou also can try different ways to set environment variable.",
    "Running a longer command from docker": "I was fiddling with crossbuild* and was wondering about how to use here documents to pipe commands to a Docker container. Here's the solution.\n$ docker run --rm --interactive --volume $(pwd):/workdir --env CROSS_TRIPLE=x86_64-apple-darwin multiarch/crossbuild /bin/bash -s <<EOF\nmkdir build && cd build\ncmake ..\nmake\nEOF\nQuick rundown of what's happening.\n--rm tells Docker to remove the container when it finished execution, otherwise it would show up in the output docker ps -a (not mandatory to use of course)\n--interactive, -i is needed, otherwise /bin/bash would not run in an interactive environment and would not accept the here document from stdin as its input\nabout the -s flag passed to /bin/bash\nIf the -s option is present, or if no arguments remain after option processing, then commands are read from the standard input.\n--volume $(pwd):/workdir, just -v will mount the current working directory on the host to /workdir in the container\n--env CROSS_TRIPLE=x86_64-apple-darwin, or simple -e tells the crossbuild container about the target platform and architecture (the container's entry point is /usr/bin/crossbuild, which is a shell script and based on the environment variable it's symlink the right toolchain components to the right places for the cross compilation to work)\nmultiarch/crossbuild the name of the Docker container to run (available in Docker Hub)\nThe commands can be fed to Docker like this as well.\n$ cat a.sh\nmkdir build && cd build\ncmake ..\nmake\n$ docker run --rm -i -v $(pwd):/workdir -e CROSS_TRIPLE=x86_64-apple-darwin multiarch/crossbuild /bin/bash -s < a.sh\nHoped this helps.\nUpdate\nActually it seems you don't even need to use /bin/bash -s, it can be ommited, at least for the crossbuild container, YMMV.\n*Linux based container used to produce multi-arch binaries: Linux, Windows and OS X, very cool.",
    "Dockerfile COPY and keep folder structure": "After a lot of searching around, I managed to find a solution that works for me\nI ended up creating a tar of all the package.json files\ntar --exclude node_modules -cf package.json.tar  */package.json\nAnd using the docker add command to unpack that tar inside the image\nADD package.json.tar .\nThe add command will detect that the input is a tar file, and will extract it inside the image.\nThe down side of using this approach is you have to run the tar command sometime before you build the image.",
    "Installing python in Dockerfile without using python image as base": "just add this with any other thing you want to apt-get install:\nRUN apt-get update && apt-get install -y \\\n    python3.6 &&\\\n    python3-pip &&\\\nin alpine it should be something like:\nRUN apk add --update --no-cache python3 && ln -sf python3 /usr/bin/python &&\\\n    python3 -m ensurepip &&\\\n    pip3 install --no-cache --upgrade pip setuptools &&\\",
    "Go server empty response in Docker container": "Don't use localhost (basically an alias to 127.0.0.1) as your server address within a Docker container. If you do this only 'localhost' (i.e. any service within the Docker container's network) can reach it.\nDrop the hostname to ensure it can be accessed outside the container:\n// Addr:         \"localhost:\" + port, // unreachable outside container\nAddr:         \":\" + port, // i.e. \":3000\" - is accessible outside the container",
    "Build 2 images with Docker from a single Dockerfile while using different base images": "You could use build-time configuration with ARG.\nFROM instructions support variables that are declared by any ARG instructions that occur before the first FROM. An ARG declared before a FROM is outside of a build stage, so it can\u2019t be used in any instruction after a FROM. To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside of a build stage:\nDockerfile:\nARG imagename\nFROM $imagename\nRUN echo $imagename\nRun with two different base images:\ndocker build --build-arg imagename=alpine .\noutputs:\nStep 1/3 : ARG imagename\nStep 2/3 : FROM $imagename\nlatest: Pulling from library/alpine\nff3a5c916c92: Pull complete \nDigest: sha256:e1871801d30885a610511c867de0d6baca7ed4e6a2573d506bbec7fd3b03873f\nStatus: Downloaded newer image for alpine:latest\n ---> 3fd9065eaf02\nStep 3/3 : RUN echo $imagename\n ---> Running in 96b45ef959c3\n\nRemoving intermediate container 96b45ef959c3\n ---> 779bfc103e9e\nSuccessfully built 779bfc103e9e\nAlternatively:\ndocker build --build-arg imagename=busybox .\nresults in:\nStep 1/3 : ARG imagename\nStep 2/3 : FROM $imagename\nlatest: Pulling from library/busybox\n07a152489297: Pull complete \nDigest: sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47\nStatus: Downloaded newer image for busybox:latest\n ---> 8c811b4aec35\nStep 3/3 : RUN echo $imagename\n ---> Running in 6027fe4f5b7b\n\nRemoving intermediate container 6027fe4f5b7b\n ---> 28640f123967\nSuccessfully built 28640f123967\nSee also this blogpost for more ideas.",
    "Docker creating multiple images": "Images are immutable, so any change you make results in a new image being created. Since your compose file specifies the build command, it will rerun the build command when you start the containers. If any files you are including with a COPY or ADD change, then the existing image cache is no longer used and it will build a new image without deleting the old image.\nNote, I'd recommend naming your image in the compose file so it's clear which image is being rebuilt. And you can watch the compose build output to the the first step that doesn't report using the cache to see what is changing. If I was to guess, the line that breaks your cache and causes a new image is this one in nodejs:\nCOPY . /usr/src/app\nIf the files being changed and causing the rebuild are not needed in your container, then use a .dockerignore file to exclude the unnecessary files.",
    "Docker: cannot open port from container to host": "Please check the program in container is listening on interface 0.0.0.0.\nIn container, run command:\nss -lntp\nIf it appears like:\nLISTEN  0   128   127.0.0.1:5000  *:*\nthat means your web app only listen at localhost so container host cannot access to your web app. You should make your server listen at 0.0.0.0 interface by changing your web app build setting.\nFor example if your server is nodejs app:\nvar app = connect().use(connect.static('public')).listen(5000, \"0.0.0.0\");\nIf your server is web pack:\n \"scripts\": {\n    \"dev\": \"webpack-dev-server --host 0.0.0.0 --port 5000 --progress\"\n  }",
    "Does docker reuse images when multiple containers run on the same host?": "Dockers Understand images, containers, and storage drivers details most of this.\nFrom Docker 1.10 onwards, all the layers that make up an image have an SHA256 secure content hash associated with them at build time. This hash is consistent across hosts and builds, as long as the content of the layer is the same.\nIf any number of images share a layer, only the 1 copy of that layer will be stored and used by all images on that instance of the Docker engine.\nA tag like debian can refer to multiple SHA256 image hash's over time as new releases come out. Two images that are built with FROM debian don't necessarily share layers, only if the SHA256 hash's match.\nAnything that runs the Docker Engine underneath will use this storage setup.\nThis sharing also works in the Docker Registry (>2.2 for the best results). If you were to push images with layers that already exist on that registry, the existing layers are skipped. Same with pulling layers to your local engine.",
    "How can I set $PS1 with Dockerfile?": "What is happening here is that PS1 is being redefined by the file ~/.bashrc that is in your image and automatically sourced on start up of your container (it could be on another file - I am not sure if PS1 always get defined in ~/.bashrc on all linux distros).\nAssuming it is defined in ~/.bashrc, then you could write in your Dockerfile a RUN command that could look like:\nRUN echo PS1=\\\"\\\\h:\\\\W \\\\u$ \\\" >> ~/.bashrc\nEt voila!",
    "ArchLinux docker CI-failed to initialise alpm library :: returned a non-zero code: 255": "This workaround has worked for me. It requires patching glibc to an older version.\nRUN patched_glibc=glibc-linux4-2.33-4-x86_64.pkg.tar.zst && \\\ncurl -LO \"https://repo.archlinuxcn.org/x86_64/$patched_glibc\" && \\\nbsdtar -C / -xvf \"$patched_glibc\"\nhttps://github.com/qutebrowser/qutebrowser/commit/478e4de7bd1f26bebdcdc166d5369b2b5142c3e2",
    "Does cleaning the user yarn cache will affect docker image?": "yarn Version 1\nYou can safely remove cache files, it will not affect your application. There's even a dedicated command for that:\n$ yarn cache clean\nyarn Version 2\nWith Plug'n'Play, however, clearing the cache will very likely break your application because dependencies are no longer placed in node_modules. Here's what the documentation says:\nIn this install mode (now the default starting from Yarn v2), Yarn generates a single .pnp.js file instead of the usual node_modules. Instead of containing the source code of the installed packages, the .pnp.js file contains a map linking a package name and version to a location on the disk, and another map linking a package name and version to its set of dependencies. Thanks to this efficient system, Yarn can tell Node exactly where to look for files being required - regardless of who asks for them!\nThe location on the disk is cache.\nYou can get the old behavior back by placing this in your .yarnrc.yml file.\nnodeLinker: node-modules\nRead more about Plug'n'Play here",
    "Password does not match for user \"postgres\"": "The Postgres password is set on the first initialization if the db (when the var/lib/postgresql/data folder is empty). Since you probably ran docker-compose before with a different password, the new one will not take effect. To fix this, remove the volume in your host machine and have docker-compose automatically recreate it next time you run docker-compose up. Note that removing this volume will delete all data stored in PostgreSQL",
    "How to set username and password for our own docker private registry?": "How to set username and password for our own docker private registry?\nThere are couple ways to implement basic auth in DTR. The simplest way is to put the DTR behind a web proxy and use the basic auth mechanism provided by the web proxy.\nTo enable basic auth in DTR directly? This is how.\nCreate a password file containing username and password: mkdir auth && docker run --entrypoint htpasswd registry:2 -Bbn your-username your-password > auth/htpasswd.\nStop DTR: docker container stop registry.\nStart DTR again with basic authentication, see commands below.\nNote: You must configure TLS first for authentication to work.\ndocker run -d \\\n  -p 5000:5000 \\\n  --restart=always \\\n  --name registry \\\n  -v `pwd`/auth:/auth \\\n  -e \"REGISTRY_AUTH=htpasswd\" \\\n  -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\\n  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\\n  -v `pwd`/certs:/certs \\\n  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\\n  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\\n  registry:2\nHow to use them when we want to use the image from that repository?\nBefore pulling images, you need first to login to the DTR:\ndocker login your-domain.com:5000\nAnd fill in the username and password from the first step.",
    "How to get version value of package.json inside of Dockerfile?": "If you just need the version from inside of your node app.. require('./package.json').version will do the trick.\nOtherwise, since you're already building your own container, why not make it easier on yourself and install jq? Then you can run VERSION=$(jq .version package.json -r).\nEither way though, you cannot simply export a variable from a RUN command for use in another stage. There is a common workaround though:\nFROM node:8-alpine\nRUN apk update && apk add jq\nCOPY package.json .\nCOPY server.js .\nRUN jq .version package.json -r > /root/version.txt\nCMD VERSION=$(cat /root/version.txt) node server.js\nResults from docker build & run:\n{ NODE_VERSION: '8.11.1',\n  YARN_VERSION: '1.5.1',\n  HOSTNAME: 'xxxxx',\n  SHLVL: '1',\n  HOME: '/root',\n  VERSION: '1.0.0',\n  PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n  PWD: '/' }",
    "Conditional Block in Docker": "If you don't want to use all those RUN if statements, you can instead create a bash script with the setup procedure and call it from the Dockerfile. For example:\nFROM centos:centos7\nMAINTAINER Someone <someone@email.com>\n\nARG BUILD_TOOL\n\nCOPY setup.sh /setup.sh\n\nRUN ./setup.sh\n\nRUN rm /setup.sh\nAnd the setup.sh file (don't forget to make it executable):\nif [ \"${BUILD_TOOL}\" = \"MAVEN\" ]; then\n    echo \"Step 1 of MAVEN setup\";\n    echo \"(...)\";\n    echo \"Done MAVEN setup\";\nelif [ \"${BUILD_TOOL}\" = \"ANT\" ]; then\n    echo \"Step 1 of ANT setup\";\n    echo \"(...)\";\n    echo \"Done ANT setup\";\nfi\nYou can then build it using docker build --build-arg BUILD_TOOL=MAVEN . (or ANT).\nNote that I used a shell script here, but if you have other interpreters available (ex: python or ruby), you can also use them to write the setup script.",
    "Gracefully Stopping Docker Containers": "Finally solved the problem.\nTcsh shell doesn't receive most of the signals like SIGTERM which is the signal sent by docker when stopping the container.\nSo I changed the script to use bash shell and whenever I want to run a tcsh command I just do it like this:\n/bin/tcsh ./my-command\nSo, my docker-entrypoint.sh is like this:\n#!/bin/bash\n\n# SIGTERM-handler this funciton will be executed when the container receives the SIGTERM signal (when stopping)\nterm_handler(){\n   echo \"***Stopping\"\n   /bin/tcsh ./my-cleanup-command\n   exit 0\n}\n\n# Setup signal handlers\ntrap 'term_handler' SIGTERM\n\necho \"***Starting\"\n/bin/tcsh ./my-command\n\n# Running something in foreground, otherwise the container will stop\nwhile true\ndo\n   #sleep 1000 - Doesn't work with sleep. Not sure why.\n   tail -f /dev/null & wait ${!}\ndone",
    "How to get all available environment-variables of docker image?": "docker image inspect {image_name} has a section called \"Env\" where you can see this info.",
    "How to create a solr core using docker-solrs image extension mechanism?": "Provide precreate-core file location which is to be executed, so edit create-a12core.sh as given below\n #!/bin/bash\n /opt/docker-solr/scripts/precreate-core  A12Core /A12Core\nTested and Works !!!",
    "Docker user namespacing map user in container to host": "No, if you want to use user-namespaces, there's currently no way to remap the user for bind-mounted directories and files.\nBasically, the issue you're running into is the exact goal of user namespaces; preventing a privileged user inside a container to get access to files on the host. This protects you from a process being able to escape the container from doing damage on the host.\nHowever it looks like your goal is to give the postgres user access to files on your host, which are owned by a local user. For that situation, there may be another approach; run the container with the same uid:gid as the user that owns the files on the host. This may require changes to the image (because some parts in the images are currently created with the postgres user, which currently has a different uid:gid (also see some information about this in this answer)\nCurrently, doing so with the postgres official image requires some manual changes, but a pull request was recently merged for the official postgres image that makes this work out of the box (see docker-entrypoint.sh#L30-L41)\nYou can find details on the pull request; https://github.com/docker-library/postgres/pull/253, and the associated documentation; https://github.com/docker-library/docs/pull/802. This change should be available soon, but in the meantime you can create a custom image that extends the official PostgreSQL image.",
    "How to install a specific version of Java 8 using Dockerfile": "As most PPA packages pack the latest stable version, I would recommend installing Java manually from Oracle, just like in this answer.\nYou can do all the work in the script too, the steps are:\nget the tarball with wget,\nuntar it with tar -xz,\nuse update-alternatives to set is as default",
    "How do I use command substition in Dockerfile": "What went wrong\nThe $( ... ) command substitution you attempted is for Bash, whereas the Dockerfile is not Bash. So docker doesn't know what to do with that, it's just plain text to docker, docker just spews out what you wrote as-is.\nRecommendation\nTo avoid hard-coding values into a Dockerfile, and instead, to dynamically change a setting or custom variable as PYTHONPATH during the build, perhaps the ARG ... , --build-arg docker features might be most helpful, in combination with ENV ... to ensure it persists.\nWithin your Dockerfile:\nARG PYTHON_PATH_ARG\n\nENV PYTHONPATH ${PYTHON_PATH_ARG}\nIn Bash where you build your container:\npython_path=\"/usr/local$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\"\n\ndocker build --build-arg PYTHON_PATH_ARG=$python_path .\nExplanation\nAccording to documentation, ARG:\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nSo, in Bash we first:\npython_path=\"/usr/local$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\"\n$(...) Bash command substitution is used to dynamically put together a Python path value\nthis value is stored temporarily in a Bash variable $python_path for clarity\ndocker build --build-arg PYTHON_PATH_ARG=$python_path .\nBash variable $python_path value is passed to docker's --build-arg PYTHON_PATH_ARG\nWithin the Dockerfile:\nARG PYTHON_PATH_ARG\nso PYTHON_PATH_ARG stores the value from --build-arg PYTHON_PATH_ARG...\nARG variables are not equivalent to ENV variables, so we couldn't merely do ARG PYTHONPATH and be done with it. According to documentation about Using arg variables:\nARG variables are not persisted into the built image as ENV variables are.\nSo finally:\nENV PYTHONPATH ${PYTHON_PATH_ARG}\nWe use Dockerfile's ${...} convention to get the value of PYTHON_PATH_ARG, and save it to your originally named PYTHONPATH environment variable\nDifferences from original code\nYou originally wrote:\nENV PYTHONPATH /usr/local/$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\nI re-wrote the Python path finding portion as a Bash command, and tested on my machine:\n$ python_path=\"/usr/local/$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\"\n\n$ echo $python_path\n/usr/local//usr/lib/python3/dist-packages\nNotice there is a double forward slash ... local//usr ... , not sure if that will break anything for you, depends on how you use it in your code.\nInstead, I changed it to:\n$ python_path=\"/usr/local$(python3 -c 'from distutils import sysconfig; print(sysconfig.get_python_lib())')\"\nResult:\n$ echo $python_path\n/usr/local/usr/lib/python3/dist-packages\nSo this new code will have no double forward slashes.",
    "libGL error: failed to load driver swrast in docker container": "Figured it out. I had to build the gui with hardware accelerated OpenGL support. Theres a repo (https://github.com/gklingler/docker3d) that contains docker images with nvidia or other graphics drivers support.\nThe other catch was, it didn't work for me unless the host and the container had the exact same driver. In order to resolve this, you can run the following shell script if you're running on linux:\n#!/bin/bash\nversion=\"$(glxinfo | grep \"OpenGL version string\" | rev | cut -d\" \" -f1 | rev)\"\nwget http://us.download.nvidia.com/XFree86/Linux-x86_64/\"$version\"/NVIDIA-Linux-x86_64-\"$version\".run\nmv NVIDIA-Linux-x86_64-\"$version\".run NVIDIA-DRIVER.run",
    "Dockerfile FROM Instruction": "#Note: image1 and image2 can be same\n\nFROM image1\n.. any commands for image1\nFROM image2\n.. any commands for image2\nIt will create two images. It will return latest image id after the build(as the doc says). So this usage is possible(I didn't see that usage yet.), but in my opinion it can be used on exceptional cases. It doesn't seem a nice usage to build two different images and reaching first image id.\nMay be your requirement is building mass applications and able to build once a time together. So it's up to your requirement. Do you really need this usage is the main question.",
    "Self-hosted alternative to hub.docker.com?": "You need to seperately setup the registry and the build server separately. This way when you make a push to GitLab, it notifies the build system (via a POST) and builds the image. After the build is complete, the final image gets pushed to the registry (either self-hosted or to hub.docker.com).\nSetting up the Registry\nFirst make sure that you have docker installed.\nThen run the following command, which will start an instance of the registry.\nsudo docker run --restart='always' -d -p 5000:5000 --name=registry \\\n-e GUNICORN_OPTS=[\"--preload\"] \\\n-v /srv/registry:/tmp/registry \\\nregistry\nTo expose a Web UI for the above registry, run the following. (Replace with the IP of the registry)\nsudo docker run  -d -P --restart='always' \\\n-e ENV_DOCKER_REGISTRY_HOST=<REGISTRY_IP> \\\n-e ENV_DOCKER_REGISTRY_PORT=5000 \\\nkonradkleine/docker-registry-frontend\nSetting up the Build Server\nThe ubiquitous Jenkins build server can fill in this gap.\nYou'll need to install the GitLab CI plugin (for Jenkins) which partially emulates the GitLab CI API. Note than you need to also configure the CI plugin after installation from \"Manage Jenkins\" -> \"Configure System\". Note that the private token functionality is not implemented. So enter something random in that field.\nNow you can configure your GitLab repo to fire up a CI event after a PUSH to the repo using Services -> GitLab CI.\nPlease Note: I have tried this out on GitLab v7.7.2. AFAIK the newer GitLab release has interated the earlier seperate GitLab CI.\nOn the jenkins server, create a new freestyle project or edit an existing project. Then check Build on Push Events.\nNow for the final step, execute the following code snippet as a shell script. Note that you will need to start your docker daemon with the insecure registry option. Refer: https://docs.docker.com/registry/insecure/\n# Build and push image\ncd $WORKSPACE\ndocker build -t <REGISTRY_IP>:5000/<PROJECT_NAME>:latest .\ndocker push <REGISTRY_IP>:5000/<PROJECT_NAME>:latest\nAlternatively\nHave a look at tarzan. It works quite similar to docker hub but it needs to be triggered from a GitHub event (not GitLab). Also because I haven't tried it out, I can't vouch for it.\nI suspect that even though tarzan is said to work only with GitHub, it might also work with GitLab.",
    "Advantages of a Dockerfile": "Dockerfile is used for automation of work by specifying all step that we want on docker image.\nA Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession.\nyes , we can create Docker images but every time when we want to make any change then you have to change manually and test and push it .\nor if you use Dockerfile with dockerhub then it will rebuild automatically and make change on every modification and if something wrong then rebuild will fail.\nAdvantages of Dockerfile\nDockerfile is automated script of Docker images\nmanual image creation will become complicated when you want to test same setup on different OS flavor then you have to create image for all flavor but by small changing in dockerfile you can create images for different flavor\nit have simple syntax for image and do many change automatically that will take more time while doing manually.\nDockerfile have systematic step that can be understand by others easily and easy to know what exact configuration changed in base image.\nAdvantage of Dockerfile with dockerhub\nDocker Hub provide private repository for Dockerfile.\nDockerfile can share among team and organization.\nAutomatic image builds\nWebhooks that are attached to your repositories that allow you to trigger an event when an image or updated image is pushed to the repository\nwe can put Dockerfile on Github or Bitbucket\nDifference between committed image and Dockerfile image\nCommitted image : it commit a container\u2019s file changes or settings into a new image.\nUsage:  docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\nCreate a new image from a container's changes\n\n  -a, --author=       Author (e.g., \"John Hannibal Smith <hannibal@a-team.com>\")\n  -c, --change=[]     Apply Dockerfile instruction to the created image\n  --help=false        Print usage\n  -m, --message=      Commit message\n  -p, --pause=true    Pause container during commit\nIt is good option to debug container and export changed setting into another image.but docker suggest to use dockerfile see here or we can say commit is versioning of docker or backup of image.\nThe commit operation will not include any data contained in volumes mounted inside the container.\nBy default, the container being committed and its processes will be paused while the image is committed. This reduces the likelihood of encountering data corruption during the process of creating the commit. If this behavior is undesired, set the \u2018p\u2019 option to false.\nDockerfile based image:\nit always use base image for creating new image. let suppose if you made any change in dockerfile then it will apply all dockerfile steps on fresh image and create new image. but commit use same image.\nmy point of view we have to use dockerfile that have all step that we want on image but if we create image from commit then we have to document all change that we made that may be needed if we want to create new image and we can say dockerfile is a documentation of image.",
    "Docker Compose: executable file not found in $PATH: unknown": "As pointed out in @derpirscher's comment and mine, one of the issues was the permission of your script(s) and the way they should be called as the ENTRYPOINT (not CMD).\nConsider this alternative code for your Dockerfile :\nFROM node:16\n\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\nCOPY wait-for-it.sh ./\nCOPY docker-deploy.sh ./\n\n# Use a single RUN command to avoid creating multiple RUN layers\nRUN chmod +x wait-for-it.sh \\\n  && chmod +x docker-deploy.sh \\\n  && npm install --legacy-peer-deps\n\nCOPY . .\n\nRUN npm run build\n\nENTRYPOINT [\"./docker-deploy.sh\"]\ndocker-deploy.sh script :\n#!/bin/sh\n\n# call wait-for-it with args and then start node if it succeeds\nexec ./wait-for-it.sh -h \"${DB_HOST}\" -p \"${DB_PORT}\" -t 300 -s -- node start\nSee this other SO question for more context on the need for the exec builtin in a Docker shell entrypoint.\nAlso, note that the fact this exec ... command line is written inside a shell script (not directly in an ENTRYPOINT / CMD exec form) is a key ingredient for using the parameter expansion.\nIn other words: in the revision 2 of your question, the \"${DB_HOST}:${DB_PORT}\" argument was understood literally because no shell interpolation occurs in an ENTRYPOINT / CMD exec form.\nRegarding the docker-compose.yml :\n# version: '3.7'\n# In the Docker Compose specification, \"version:\" is now deprecated.\n\nservices:\n  my-service:\n    build: .\n    # Add \"image:\" for readability\n    image: some-optional-fresh-tag-name\n    # Pass environment values to the entrypoint\n    environment:\n      DB_HOST: postgres\n      DB_PORT: ${DB_PORT}\n      # etc.\n    # Add network spec to make it explicit what services can communicate together\n    networks:\n      - postgres-network\n    # Add \"depends_on:\" to improve \"docker-run scheduling\":\n    depends_on:\n      - postgres\n\n  postgres:\n    # container_name: postgres # unneeded\n    image: postgres:14.3\n    environment:\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_DB: my-service-db\n      PG_DATA: /var/lib/postgresql2/data\n    volumes:\n      - pgdata:/var/lib/postgresql2/data\n    networks:\n      - postgres-network\n    # ports:\n    #   - ${DB_PORT}:${DB_PORT}\n    # Rather remove this line in prod, which is a typical weakness, see (\u00a7)\n\nnetworks:\n  postgres-network:\n    driver: bridge\n\nvolumes:\n  pgdata:\n    # let's be more explicit\n    driver: local\nNote that in this Docker setting, the wait-for-it host should be postgres (the Docker service name of your database), not 0.0.0.0 nor localhost. Because the wait-for-it script acts as a client that tries to connect to the specified web service in the ambient docker-compose network.\nFor a bit more details on the difference between 0.0.0.0 (a server-side, catch-all special IP) and localhost in a Docker context, see e.g. this other SO answer of mine.\n(\u00a7): last but not least, the ports: [ \"${DB_PORT}:${DB_PORT}\" ] lines should rather be removed because they are not necessary for the Compose services to communicate (the services just need to belong to a common Compose network and use the other Compose services' hostname), while exposing one such port directly on the host increases the attack surface.\nLast but not least:\nTo follow-up this comment of mine, suggesting to run ls -l docker-deploy.sh; file docker-deploy.sh in your myapp/ directory as a debugging step (BTW: feel free to do this later on then comment for the record):\nAssuming there might be an unexpected bug in Docker similar to this one as pointed by @Lety:\nI'd suggest to just replacing (in the Dockerfile) the line\nRUN chmod +x wait-for-it.sh \\\n  && chmod +x docker-deploy.sh \\\n  && npm install --legacy-peer-deps\nwith\nRUN npm install --legacy-peer-deps\nand running directly in a terminal on the host machine:\ncd myapp/\nchmod -v 755 docker-deploy.sh\nchmod -v 755 wait-for-it.sh\n\ndocker-compose --env-file .env up --build\nIf this does not work, here is another useful information you may want to provide: what is your OS, and what is your Docker package name? (e.g. docker-ce or podman\u2026)",
    "Docker Error response from daemon: OCI runtime create failed container_linux.go:380: starting container process caused": "It looks like you're trying to replace the application code in the image with different code using Docker bind mounts. Docker's general model is that a container runs a self-contained image; you shouldn't need to separately install the application code on the host.\nIn particular, these two volumes: blocks cause the error you're seeing, and can safely be removed:\nservices:\n  react-app:\n    build:             # <-- This block builds an image with the code\n      context: ./\n      dockerfile: ./client/Dockerfile\n    # volumes:         # <-- So delete this block\n    #   - ./client:/usr/src/client:ro\n    #   - /usr/src/client/node_modules\n  node-app:\n    build: \n      context: ./\n      dockerfile: ./server/Dockerfile\n    # volumes:         # <-- And this one\n    #   - ./server:/usr/src/server:ro\n    #   - /usr/src/server/node_modules\nMechanically, the first volumes: line replaces the image's code with different code from the host, but with a read-only mount. The second volumes: line then further tries to replace the node_modules directory with an old copy from an anonymous volume. This will create the node_modules directory if it doesn't exist yet; but the parent directory is a read-only volume mount, resulting in the error you're seeing.",
    "Why is a folder created by WORKDIR owned by root instead of USER": "I failed to find detail documents for this, but I'm interested on this, so I just had a look for docker source code, I guess we can get the clue from sourcecode:\nmoby/builder/dockerfile/dispatcher.go (Line 299):\n// Set the working directory for future RUN/CMD/etc statements.\n//\nfunc dispatchWorkdir(d dispatchRequest, c *instructions.WorkdirCommand) error {\n    ......\n    if err := d.builder.docker.ContainerCreateWorkdir(containerID); err != nil {\n        return err\n    }\n\n    return d.builder.commitContainer(d.state, containerID, runConfigWithCommentCmd)\n}\nAbove, we can see it will call ContainerCreateWorkdir, next is the code:\nmoby/daemon/workdir.go:\nfunc (daemon *Daemon) ContainerCreateWorkdir(cID string) error {\n    ......\n    return container.SetupWorkingDirectory(daemon.idMapping.RootPair())\n}\nAbove, we can see it call SetupWorkingDirectory, next is the code:\nmoby/container/container.go (Line 259):\nfunc (container *Container) SetupWorkingDirectory(rootIdentity idtools.Identity) error {\n    ......\n    if err := idtools.MkdirAllAndChownNew(pth, 0755, rootIdentity); err != nil {\n        pthInfo, err2 := os.Stat(pth)\n        if err2 == nil && pthInfo != nil && !pthInfo.IsDir() {\n            return errors.Errorf(\"Cannot mkdir: %s is not a directory\", container.Config.WorkingDir)\n        }\n\n        return err\n    }\n\n    return nil\n}\nAbove, we can see it call MkdirAllAndChownNew(pth, 0755, rootIdentity), next is the code:\nmoby/pkg/idtools/idtools.go (Line 54):\n// MkdirAllAndChownNew creates a directory (include any along the path) and then modifies\n// ownership ONLY of newly created directories to the requested uid/gid. If the\n// directories along the path exist, no change of ownership will be performed\nfunc MkdirAllAndChownNew(path string, mode os.FileMode, owner Identity) error {\n    return mkdirAs(path, mode, owner, true, false)\n}\nAbove will setup folder in intermediate build container & also change the ownership of the folder with rootIdentity.\nFinally, what is rootIdentity here?\nIt's passed here as daemon.idMapping.RootPair(), next is the declare:\nmoby/pkg/idtools/idtools.go (Line 151):\n// RootPair returns a uid and gid pair for the root user. The error is ignored\n// because a root user always exists, and the defaults are correct when the uid\n// and gid maps are empty.\nfunc (i *IdentityMapping) RootPair() Identity {\n    uid, gid, _ := GetRootUIDGID(i.uids, i.gids)\n    return Identity{UID: uid, GID: gid}\n}\nSee the function desc:\nRootPair returns a uid and gid pair for the root user\nYou can continue to see what GetRootUIDGID is, but I think it's enough now from the function desc. It will finally use change the ownership of WORKDIR to root.\nAnd, additional to see what USER do?\n__moby/builder/dockerfile/dispatcher.go (Line 543):__\n\n// USER foo\n//\n// Set the user to 'foo' for future commands and when running the\n// ENTRYPOINT/CMD at container run time.\n//\nfunc dispatchUser(d dispatchRequest, c *instructions.UserCommand) error {\n    d.state.runConfig.User = c.User\n    return d.builder.commit(d.state, fmt.Sprintf(\"USER %v\", c.User))\n}\nAbove, just set user to run config and directly commit for further command, but did nothing related to WORKDIR setup.\nAnd, if you want to change the ownership, I guess you will have to do it by yourself use chown either in RUN or ENTRYPOINT/CMD.",
    "\"ModuleNotFoundError: No module named <package>\" in my Docker container": "Inside the container, when I pip install bugsnag, I get the following:\nroot@af08af24a458:/app# pip install bugsnag\nRequirement already satisfied: bugsnag in /usr/local/lib/python2.7/dist-packages\nRequirement already satisfied: webob in /usr/local/lib/python2.7/dist-packages (from bugsnag)\nRequirement already satisfied: six<2,>=1.9 in /usr/local/lib/python2.7/dist-packages (from bugsnag)\nYou probably see the problem here. You're installing the package for python2.7, which is the OS default, instead of python3.6, which is what you're trying to use.\nCheck out this answer for help resolving this issue: \"ModuleNotFoundError: No module named <package>\" in my Docker container\nAlternatively, this is a problem virtualenv and similar tools are meant to solve, you could look into that as well.",
    "Templating config file with docker": "Found a solution using simple bash script, so I didn't even alter the template.\n#!/bin/sh\n\ncat > /etc/sphinxsearch/sphinx.conf\n\nfor LOCALE in ru en de ;\ndo\n    sed \"s/{{ locale }}/${LOCALE}/g\" ./template/index.conf.template >> /etc/sphinxsearch/sphinx.conf\ndone",
    "Extending docker official postgres image": "If you define a new ENTRYPOINT in your Dockerfile it will override the inherited ENTRYPOINT. So in that case, Postgres will not be able to be initialized automatically (unless yo write the same ENTRYPOINT).\nhttps://docs.docker.com/engine/reference/builder/#entrypoint\nAlso, the official postgres image let you add .sql/.sh files in the /docker-entrypoint-initdb.d folder, so they can be executed once the database is initialized.\nFinally, if you don't want that Postgres remove your data, you can mount a volume between the /var/lib/postgresql/data folder and a local folder in each docker run ... command to persist them.",
    "Restart Docker container during build process": "OK, looks like you're trying to install VisualStudio 2019. That's how I solved the problem. The first approach is to use multi-stage build as stated above:\nFROM mcr.microsoft.com/windows/servercore:1809 as baseimage\nRUN powershell -NoProfile -ExecutionPolicy Bypass -Command \\    \n     $Env:chocolateyVersion = '0.10.15' ; \\\n     $Env:chocolateyUseWindowsCompression = 'false' ; \\\n     \"[Net.ServicePointManager]::SecurityProtocol = \\\"tls12, tls11, tls\\\"; iex ((New-Object System.Net.WebClient).DownloadString('http://chocolatey.org/install.ps1'))\" && SET \"PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\"\n\n# suppress the \"restart required\" error code (3010)\nRUN choco install -y --ignore-package-exit-codes=3010 dotnetfx\n\n# emulate the required restart after installing dotnetfx\nFROM baseimage\nRUN choco install -y visualstudio2019buildtools --package-parameters \\\n    \"--norestart --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64\"\nThe problem with that approach is that dotnetfx package seems to be broken - some other packages fail to install due to the missing 'alink.dll' library. Also, I didn't check that --ignore-package-exit-codes=3010 suppresses only one error or all errors (choco doc says nothing about the possibility to specify the exact code).\nThe second approach is to install visual studio from the MS website (works perfectly):\nFROM mcr.microsoft.com/windows/servercore:1809\n\nRUN powershell -NoProfile -ExecutionPolicy Bypass -Command \\\n    Invoke-WebRequest \"https://aka.ms/vs/16/release/vs_community.exe\" \\\n    -OutFile \"%TEMP%\\vs_community.exe\" -UseBasicParsing\n\nRUN \"%TEMP%\\vs_community.exe\"  --quiet --wait --norestart --noUpdateInstaller \\\n    --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 \\\n    --add Microsoft.VisualStudio.Component.Windows10SDK.18362\nNote that components might be different in you case.",
    "How to configure proxy for NuGet while using Docker?": "Check my answer here.\nBasically, you should instruct the docker build environment to use the proxy by adding build arguments like:\ndocker build --build-arg HTTP_PROXY=<proxy URL> --build-arg HTTPS_PROXY=<proxy URL> -t <application name>",
    "How to COPY library files between stages of a multi-stage Docker build while preserving symlinks?": "This is more of a workaround than an answer.\nYou could tar the files, copy the tarball to the second container and then untar them.\nTar maintains symbolic links by default.",
    "defining multiple services on Dockerfile with single CMD": "It is best practice to run a single service in each container, but it is not always practical. I run CMD [\"sh\", \"-c\", \"/usr/sbin/crond && php-fpm\"] so I can use the laravel scheduler. So in answer to the question, yes the above does work.",
    "How to Containerize a Vue.js app?": "In a default vue-cli setup, npm start (the command you are using) runs npm run dev.\nAnd, again, by default, npm run dev binds to localhost only.\nAdd --host 0.0.0.0 to your webpack-dev-server line in package.json so you can access it from outside the docker container:\nFrom something like:\n  \"scripts\": {\n    \"dev\": \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js\",\nTo something like (add --host 0.0.0.0):\n    \"dev\": \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js --host 0.0.0.0\",\nNote: I'm assuming, because you used CMD [\"npm\", \"start\"], you are creating a container for development or debugging purposes. If you are targeting production, you should really consider generating the bundle (npm run build) and serving the generated files directly on a HTTP server like nginx (which could be created in a docker as well).",
    "Run SQL scripts inside Dockerfile": "======================== Correct Answer Start ========================\nAccording to the Postgres image documentation you can extend the original image by running a script or an SQL file.\nIf you would like to do additional initialization in an image derived from this one, add one or more *.sql, *.sql.gz, or *.sh scripts under /docker-entrypoint-initdb.d (creating the directory if necessary). After the entrypoint calls initdb to create the default postgres user and database, it will run any *.sql files and source any *.sh scripts found in that directory to do further initialization before starting the service.\nFor your Dockerfile, you could do something like this.\nFROM postgres:9.6.5\n\nENV POSTGRES_USER postgres\nENV POSTGRES_PASSWORD xxx\nENV POSTGRES_DB postgres\n\nADD create-role.sh /docker-entrypoint-initdb.d\nADD localDump.sql /docker-entrypoint-initdb.d\nYou will also need the create-role.sh script to execute the plsql command\n#!/bin/bash\nset -e\n\npsql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" <<-EOSQL\n    CREATE ROLE read_only;\nEOSQL\nNote that I'm not a SQL DBA, the plsql command is just an example.\n======================== Correct Answer End ========================\nOriginal answer left for posterity :\nCan you use psql ? Something like that.\nFROM postgres:9.6.5\n\nENV POSTGRES_USER postgres\nENV POSTGRES_PASSWORD xxx\nENV POSTGRES_DB postgres\n\nRUN psql -U postgres -d database_name -c \"SQL_QUERY\"\n\nADD localDump.sql /docker-entrypoint-initdb.d\nYou should find how to run SQL Query for Postgres from bash, and then add a RUN instruction in your Dockerfile.",
    "What's the difference between RUN and bash script in a dockerfile?": "The primary difference is that when you COPY the bash script into the image it will be available for inspection in the running container, whereas the RUN command is a little more opaque. Putting your commands in a file like that is arguably more manageable for other reasons: changes in your VCS history will be a little more clear, and for longer or more complex scripts you will probably find it easier to format things cleanly with the script in a separate file rather than embedded in your Dockerfile in a RUN command.\nOtherwise the result is the same (in both cases, you are executing the same set of commands), although the COPY and RUN will result in an extra image layer (vs. just the RUN by itself).",
    "How can I install python modules in a docker image?": "Yes, the best thing is to build your image in such a way it has the python modules are in there.\nHere is an example. I build an image with the build dependencies:\n$ docker build -t oz123/alpine-test-mycoolapp:0.5 - < Image\nSending build context to Docker daemon  2.56 kB\nStep 1 : FROM alpine:3.5\n ---> 88e169ea8f46\nStep 2 : ENV MB_VERSION 3.1.4\n ---> Running in 4587d36fa4ae\n ---> b7c55df49803\nRemoving intermediate container 4587d36fa4ae\nStep 3 : ENV CFLAGS -O2\n ---> Running in 19fe06dcc314\n ---> 31f6a4f27d4b\nRemoving intermediate container 19fe06dcc314\nStep 4 : RUN apk add --no-cache python3 py3-pip gcc python3-dev py3-cffi    file git curl autoconf automake py3-cryptography linux-headers musl-dev libffi-dev openssl-dev build-base\n ---> Running in f01b60b1b5b9\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.5/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.5/community/x86_64/APKINDEX.tar.gz\n(1/57) Upgrading musl (1.1.15-r5 -> 1.1.15-r6)\n(2/57) Upgrading zlib (1.2.8-r2 -> 1.2.11-r0)\n(3/57) Installing m4 (1.4.17-r1)\n(4/57) Installing perl (5.24.0-r0)\n(5/57) Installing autoconf (2.69-r0)\n(6/57) Installing automake (1.15-r0)\n(7/57) Installing binutils-libs (2.27-r1)\n...\nNote, I am installing Python's pip inside the image, so later I can download packages from pypi. Packages like numpy might require a C compiler and tool chain, so I am installing these too.\nAfter building the packages which require the build tools chain I remove the tool chain packages:\nRUN apk del file pkgconf autoconf m4 automake perl g++ libstdc++\nAfter you have your base image, you can run your application code in an image building on top of it:\n$ cat Dockerfile\n\n    FROM oz123/alpine-test-mycoolapp\n    ADD . /code\n    WORKDIR /code\n    RUN pip3 install -r requirements.txt -r requirements_dev.txt\n    RUN pip3 install -e .\n    RUN make clean\n    CMD [\"pytest\", \"-vv\", \"-s\"]\nI simply run this with docker.",
    "Unable to display GUI application from Windows container : Windows 2019 Server": "The short answer here is no. Windows containers don't have the underlying GUI components available, so you can't run any desktop application on Windows containers.\nThe longer answer is that this depends on what you're trying to achieve. The actual components for the GUI (GUI APIs) are present on the Server Core and Server image (not on the Nano Server one). However, the GUI itself is not present. What that means is you can run an application that depends on the GUI APIs, for UI test automation, for example. I blogged about this topic a while ago here: https://techcommunity.microsoft.com/t5/containers/nano-server-x-server-core-x-server-which-base-image-is-the-right/ba-p/2835785",
    "Why does the ASP.NET Core Multi-Stage Dockerfile use 4 Stages": "The file effectively equivalent to below\nFROM microsoft/aspnetcore-build:2.0 AS build\nWORKDIR /src\nCOPY WebApplication1.sln ./\nCOPY WebApplication1/WebApplication1.csproj WebApplication1/\nRUN dotnet restore\nCOPY . .\nWORKDIR /src/WebApplication1\nRUN dotnet build -c Release -o /app\nRUN dotnet publish -c Release -o /app\n\nFROM microsoft/aspnetcore:2.0 AS base\nEXPOSE 80\nWORKDIR /app\nCOPY --from=build /app .\nENTRYPOINT [\"dotnet\", \"WebApplication1.dll\"]\nNow the reason they may have chosen 4 build stage might be any of the two\nPresentational\nFuture changes\nSo it may be that it depicts a\nbase -> build -> publish -> deploy the build\nThe size with 2 build stage would also the same as this one. So there is no obvious difference between the 2 stage and the 4 stage. It becomes a matter preference, representation and all. Nothing technologically different",
    "Error CTC1014 Docker command failed with exit code 1 with dotnet core api": "I ran into the very same error (CTC1014) in the past. I found out that my project was being run in \"Release\" mode, when it should have been running in \"Debug\" mode. So I would like to suggest this as a workaround.\nHere's how you change it in VS2019\nOf course, running your application in Release mode shouldn't be a problem. So I assumed it must have been related to some optimization employed by most release build configurations, in conflict with Docker. Not exactly sure which one, though.\nCheers! o/",
    "How to correctly initialize git submodules in Dockerfile for Docker Cloud": "See the answer I gave here: https://stackoverflow.com/a/59640438/1021344\nReproduced here for simplicity: You need to use hooks: https://docs.docker.com/docker-hub/builds/advanced/#custom-build-phase-hooks\nTL;DR: Place this in hooks/post_checkout:\n#!/bin/bash\n# Docker hub does a recursive clone, then checks the branch out,\n# so when a PR adds a submodule (or updates it), it fails.\ngit submodule update --init",
    "How do I run Elixir Phoenix Docker in production mode using example from Phoenix Guides?": "There is a good sample repo is configured for production-ready build and deploy cycle. It contains an ansible setup that will maintain docker image, build phoenix app in docker image, and do automated versioned releases on your production server.\nAnd I recommend you to read the blog post guide",
    "Couldn't use data file .coverage: unable to open database file": "Problem\nI ran into the same error when running pytest with coverage using docker-compose in the github-hosted ubuntu-latest image in GitHub Actions. This is an instance of the Docker host file system owner matching problem.\nIn short, the user on the host (the github action runner) and the user in on the container (where my pytest suite runs) have different UIDs. The mounted directory app is owned by the user on the host. When the user in the container attempts to write to the app/.coverage, permission is denied (since this user is not the owner).\nThe fix\nIn my case, I solved the issue by matching the UID of my docker image's default user with that of the github actions runner user, 1001. I added this to my Dockerfile to accomplish this:\n# Make the default user have the same UID as the github actions \"runner\" user.\n# This to avoid permission issues when mounting volumes.\nUSER root\nRUN usermod --uid 1001 <image_default_user>\nUSER <image_default_user>\nAll relevant files\napp/test_utils/docker-compose.yml:\nversion: \"3.9\"\n\nservices:\n  app:\n    build:\n      context: ../\n      dockerfile: ./test_utils/Dockerfile\n    container_name: app\n    volumes:\n      - ..:/app\napp/test_utils/Dockerfile:\nFROM <my base image>\n\n# Make the default user have the same UID as the github actions \"runner\" user.\n# This to avoid permission issues when mounting volumes, see\nUSER root\nRUN usermod --uid 1001 <image_default_user>\nUSER <image_default_user>\n\nCOPY . /app\nWORKDIR /app\n\nRUN pip3 install -r requirements.txt -r requirements_test.txt\napp/.github/unittests.yml:\nname: Run unit tests, Report coverage\n\non:\n  pull_request:\n    paths:\n      - app/*\n      - .github/workflows/unittests.yml\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checks out the repo\n        uses: actions/checkout@v2\n\n      - name: Build docker image\n        run: docker-compose -f test_utils/docker-compose.yml build\n\n      - name: Run unit tests & produce coverage report\n        # Adapted from the docker example in\n        # https://github.com/MishaKav/pytest-coverage-comment?tab=readme-ov-file#example-usage\n        run: |\n          docker-compose \\\n            -f test_utils/docker-compose.yml \\\n            run app \\\n            pytest \\\n              --cov-report=term-missing:skip-covered \\\n              --junitxml=/app/pytest.xml \\\n              --cov=/app \\\n              /app \\\n            | tee pytest-coverage.txt\n\n      - name: Pytest coverage comment\n        uses: MishaKav/pytest-coverage-comment@main\n        with:\n          pytest-coverage-path: pytest-coverage.txt\n          junitxml-path: pytest.xml",
    "Override ENV variable in base docker image": "If you cannot build another image, as described in \"Dockerfile Overriding ENV variable\", you at least can modify it when starting the container with docker run -e\nSee \"ENV (environment variables)\"\nthe operator can set any environment variable in the container by using one or more -e flags, even overriding those mentioned above, or already defined by the developer with a Dockerfile ENV\n$ docker run -e \"deep=purple\" -e today --rm alpine env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=d2219b854598\ndeep=purple   <=============",
    "mkdir: cannot create directory '/ffa_app': Permission denied": "The user set by the base image is jboss, so you have 2 options:\ncreate and work in the user's home folder mkdir -p ~/ffa_app\nset USER root at the top of your Dockerfile, after the FROM statement\nNeedless to say, I'd recommend sticking to a user with lower privileges.",
    "Docker COPY and keep directory": "You should consider ADD instead of COPY: see Dockerfile ADD\nIf <src> is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory.\nThat means you can wrap your docker build step into a script which would first tar -cvf dirs.tar dir1 dir2 dir3\nYour Dockerfile can then ADD dirs.tar: you will find your folders in your image.\nSee also Dockerfile Best Practices: ADD or COPY.",
    "difference between building a docker from ubuntu base image and python base image?": "With ubuntu you can run a django app. you just have to install the dependencies for it (with instructions in your Dockerfile for example).\nIn your Dockerfile, add something like :\nRUN apt-get install python\nRUN apt-get install django\nYou may also have to replace some commands by their equivalent if they're not implemented in the new base image (replace apt-get by pacman if you use archlinux instead of ubuntu for example).\nBut if you use django, you also can install and use pip.",
    "qemu-x86_64: Could not open '/lib/ld-musl-x86_64.so.1': No such file or directory": "It seems azul/zulu-openjdk-alpine:11 doesn't have arm64 image needed to run on Apple silicon architecture.\nTry updating to jdk 17 image with arm64 support https://hub.docker.com/r/azul/zulu-openjdk-alpine/",
    "Where docker named volumes are stored? [duplicate]": "You can inspect docker volumes and see detailed informations.\nSee docker volume reference\nEdit (commented suggestion):\nAlternatively (for exact answer) see the already answered question.",
    "X11 forwarding of GUI app in Docker container": "To be able to communicate with the X server, the user that runs the app has to be allowed to communicate with the X server. So I think you have two options:\nAllow the user you have in the container to connect to the X server. If your app is run with user root inside the container, you can use:\n$ xhost +SI:localuser:root\n(I don't know the security implications of this, but root should be able to connect either way...)\nAdd a user in the container that matches your user session. If the user you are using in the host system has UID = 1000, you can create a dummy user inside the container:\n$ useradd -u 1000 my_user\nAnd then use that user to run your app inside the container. This doesn't require any change in the accepted hosts (as user 1000 is already capable of connection).\nLooking at the two options, the second seems better, because it does not require any change in the host system, and if you need to use this container in other systems that the main user could not match UID=1000, you can make the container receive the correct uid from an env var, and then set up the correct user (useradd + chown program files).",
    "How do I build docker images without docker?": "Part of the problem could be that you're missing the --privileged flag, but in general, your questions can probably be answered here: https://hub.docker.com/_/docker/\nAnd you might take the time to read the blog linked there detailing some of the pitfalls of using docker-in-docker.",
    "Using docker image to store data files": "You're trying to address a configuration management problem with an application management approach. For shared configurations, you generally want to take the \"centralized location approach\". Basic examples of this would be a git repository or an S3 bucket. Both solutions have native document storage and can be appropriately shared between services with fine-grained access control.\nA docker image isn't the docker approach to store/share configuration. By having an image, you can basically do two things:\n1. Base other images off your initial image\n2. Run a container\nGiven that your image here is just Scratch and files, there are no executables and nothing to run. The hello world example copied a directory with an actual script in it, which is why they ended up with a runnable container.\nAs a base image, configuration doesn't make sense coming before things like dependencies.\nIf you really want to use a docker tool for this, you're looking for docker volumes. That can persist a file system and is easy to share around different containers. https://docs.docker.com/engine/userguide/containers/dockervolumes/",
    "Dockerfile COPY wildcard to only match files, not folders": "That's a tricky one. Given what you said about not being able to .dockerignore the directories, I think I'd probably either:\nlist out all the files explicitly, or\nuse something to generate the Dockerfile from a template .. this is kind of just a special case of the first option",
    "Extracting gz in Dockerfile": "The ADD directive will unpack the .gz automatically on build\nADD target/test_app.tar.gz /service/app_lib\n# Clean up\nRUN rm -rf /service/app_lib/conf \\\n           /service/app_lib/bin",
    "ERROR: Cannot find \"/config/config.json\". Have you run \"sequelize init\"?": "You can create a .sequelizerc config file for Sequlize CLI command for your project which tells Sequlize where to look for config files.\n    var path = require('path')\n\nmodule.exports = {\n  'config':          path.resolve('server', 'config', 'database.json'),\n  'migrations-path': path.resolve('server', 'migrations'),\n  'models-path':     path.resolve('server', 'models'),\n  'seeders-path':    path.resolve('server', 'seeders'),\n}",
    "error NETSDK1064: Package DnsClient, 1.2.0 was not found": "When solutions like using --no-cache are not restoring at all work, it usually indicates that artifact from the host machine are getting copied into the build container. This can cause problems when the host machine and build container have different RIDs. This can commonly occur on Windows machines when running linux containers. The .NET Core Docker Samples suggest using a .dockerignore file to avoid coping the bin and obj directories with your Dockerfiles.",
    "How to start another bash in Dockerfile": "To expand on @user2915097's answer here is a working example using devtoolset-7 and rh-python36 instead of devtoolset-1.1\nFROM centos:7\n\n# Default version of GCC and Python\nRUN gcc --version && python --version\n\n# Install some developer style software collections with intent to\n# use newer version of GCC and Python than the OS provided\nRUN yum install -y centos-release-scl && yum install -y devtoolset-7 rh-python36\n\n# Yum installed packages but the default OS-provided version is still used.\nRUN gcc --version && python --version\n\n# Okay, change our shell to specifically use our software collections.\n# (default was SHELL [ \"/bin/sh\", \"-c\" ])\n# https://docs.docker.com/engine/reference/builder/#shell\n#\n# See also `scl` man page for enabling multiple packages if desired:\n# https://linux.die.net/man/1/scl\nSHELL [ \"/usr/bin/scl\", \"enable\", \"devtoolset-7\", \"rh-python36\" ]\n\n# Switching to a different shell has brought the new versions into scope.\nRUN gcc --version && python --version",
    "Dockerfile: COPY folder if it exists (conditional COPY)": "TL;DR;\nCOPY ./migration[s]/ /app/migrations/\nMore detailed answer\nWe can use glob patterns to achieve such behavior, Docker will not fail if it won't find any valid source, there are 3-most used globs:\n? - any character, doesn't match empty character\n* - any characters sequence of any length, matches empty character\n[abc] - sequence of characters to match, just like ?, but it matches only characters defined in brackets\nSo there are 3 ways to solve this\nCOPY ./migration?/ /app/migrations/ - will also match migrationZ, migration2 and so on..\nCOPY ./migrations*/ /app/migrations/ - will also match migrations-cool, migrations-old\nCOPY ./migration[s]/ /app/migrations/ - will match only migrations, because we are using glob that is saying match any character from 1-item sequence [s] and it just can't anything except letter \"s\"\nMore about globs: https://en.wikipedia.org/wiki/Glob_(programming)",
    "safe way to use build-time argument in Docker": "With docker 18.09+, that will be: docker build --secret id=mysecret,src=/secret/file (using buildkit).\nSee PR 1288, announced in this tweet.\n--secret is now guarded by API version 1.39.\nExample:\nprintf \"hello secret\" > ./mysecret.txt\n\nexport DOCKER_BUILDKIT=1\n\ndocker build --no-cache --progress=plain --secret id=mysecret,src=$(pwd)/mysecret.txt -f - . <<EOF\n# syntax = tonistiigi/dockerfile:secrets20180808\nFROM busybox\nRUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret\nRUN --mount=type=secret,id=mysecret,dst=/foobar cat /foobar\nEOF\nIt sure would help if you could please extend this Dockerfile with an example of making the secret available in a Docker variable so it can be used, like to compose a URL for an artifact repository that requires credentials.\nI tried this but it did not work: RUN export MYSECRET=$(cat /foobar)\nThe issue with setting the secret as an environment variable inside the Dockerfile (e.g., RUN export MYSECRET=$(cat /foobar)) is that environment variables do not persist between RUN instructions by default.\nAnd using ENV to store the secret would bake it into the image layers, making it visible to anyone who inspects the image: see docker inspect and \"How to get ENV variable when doing docker inspect\"\nThe recommended approach is to use the secret in the same RUN instruction where it is read. That way, the secret is never stored as a persistent environment variable or in a layer\u2014only used transiently during that particular build step.\nFor example, if you want to use the secret to form a URL and fetch some artifact, you could do this:\n# syntax = tonistiigi/dockerfile:secrets20180808\nFROM busybox\n\nRUN --mount=type=secret,id=mysecret \\\n    sh -c 'MYSECRET=$(cat /run/secrets/mysecret) && \\\n           echo \"Secret read, now using it in a command\" && \\\n           curl -fSL \"https://myrepo.com/artifact?token=${MYSECRET}\" -o /artifact.tar.gz'\nMYSECRET is defined and used within the same sh -c command sequence. Once the RUN step completes, MYSECRET and the secret file are gone.",
    "docker-compose to create replication in mongoDB": "Here is how I did this using a 4th container to initialize the replica set after 3 mongodb containers are running:\nDocker-compose file\n    version: '3'\n    services:\n    \n      mongodb1:\n        image: mongo:latest\n        networks:\n          - alphanetwork\n        volumes:\n          - data1:/data/db\n          - config1:/data/configdb\n        ports:\n          - 30001:27017\n        entrypoint: [ \"/usr/bin/mongod\", \"--bind_ip_all\", \"--replSet\", \"rs0\" ]\n    \n      mongodb2:\n        image: mongo:latest\n        networks:\n          - alphanetwork\n        ports:\n          - 30002:27017\n        entrypoint: [ \"/usr/bin/mongod\", \"--bind_ip_all\", \"--replSet\", \"rs0\" ]\n    \n      mongodb3:\n        image: mongo:latest\n        networks:\n          - alphanetwork\n        ports:\n          - 30003:27017\n        entrypoint: [ \"/usr/bin/mongod\", \"--bind_ip_all\", \"--replSet\", \"rs0\" ]\n    \n      mongoclient:\n        image: mongo\n        networks:\n          - alphanetwork\n        depends_on:\n          - mongodb1\n          - mongodb2\n          - mongodb3\n        volumes:\n          - ./deployment_scripts:/deployment_scripts\n        entrypoint:\n          - /deployment_scripts/initiate_replica.sh\n    \n    networks:\n      alphanetwork:\n    \n    volumes:\n      data1:\n      config1:\ninitiate_replica.sh\n    #!/bin/bash\n    \n    echo \"Starting replica set initialize\"\n    until mongo --host mongodb1 --eval \"print(\\\"waited for connection\\\")\"\n    do\n        sleep 2\n    done\n    echo \"Connection finished\"\n    echo \"Creating replica set\"\n    mongo --host mongodb1 <<EOF\n    rs.initiate(\n      {\n        _id : 'rs0',\n        members: [\n          { _id : 0, host : \"mongodb1:27017\" },\n          { _id : 1, host : \"mongodb2:27017\" },\n          { _id : 2, host : \"mongodb3:27017\" }\n        ]\n      }\n    )\n    EOF\n    echo \"replica set created\"\nAnd don't forget to chmod +x ./deployment_scripts/initiate_replica.sh to give docker the permission to execute it.",
    "Possible to add kaniko to alpine image or add jq to kaniko image": "Official Kaniko Docker image is built from scratch using standalone Go binaries (see Dockerfile from Kaniko's GitHub repository). You can re-use the same binaries from official image and copy them in your image such as:\n# Use this FROM instruction as shortcut to use --copy=from kaniko below\n# It's also possible to use directly COPY --from=gcr.io/kaniko-project/executor\nFROM gcr.io/kaniko-project/executor AS kaniko\n\nFROM alpine:3.14.2\n\nRUN apk --update add \\\n  bash \\\n  curl \\\n  git \\\n  jq \\\n  npm\nRUN curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.21.4/bin/linux/amd64/kubectl\nRUN chmod u+x kubectl && mv kubectl /bin/kubectl\n\n#\n# Add kaniko to this image by re-using binaries and steps from official image\n#\nCOPY --from=kaniko /kaniko/executor /kaniko/executor\nCOPY --from=kaniko /kaniko/docker-credential-gcr /kaniko/docker-credential-gcr\nCOPY --from=kaniko /kaniko/docker-credential-ecr-login /kaniko/docker-credential-ecr-login\nCOPY --from=kaniko /kaniko/docker-credential-acr-env /kaniko/docker-credential-acr-env\nCOPY --from=kaniko /etc/nsswitch.conf /etc/nsswitch.conf\nCOPY --from=kaniko /kaniko/.docker /kaniko/.docker\n\nENV PATH $PATH:/usr/local/bin:/kaniko\nENV DOCKER_CONFIG /kaniko/.docker/\nENV DOCKER_CREDENTIAL_GCR_CONFIG /kaniko/.config/gcloud/docker_credential_gcr_config.json\nEDIT: for the debug image, Dockerfile would be:\nFROM gcr.io/kaniko-project/executor:debug AS kaniko\n\nFROM alpine:3.14.2\n\nRUN apk --update add \\\n  bash \\\n  curl \\\n  git \\\n  jq \\\n  npm\nRUN curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.21.4/bin/linux/amd64/kubectl\nRUN chmod u+x kubectl && mv kubectl /bin/kubectl\n\n#\n# Add kaniko to this image by re-using binaries and steps from official image\n#\nCOPY --from=kaniko /kaniko/ /kaniko/\nCOPY --from=kaniko /kaniko/warmer /kaniko/warmer\nCOPY --from=kaniko /kaniko/docker-credential-gcr /kaniko/docker-credential-gcr\nCOPY --from=kaniko /kaniko/docker-credential-ecr-login /kaniko/docker-credential-ecr-login\nCOPY --from=kaniko /kaniko/docker-credential-acr /kaniko/docker-credential-acr\nCOPY --from=kaniko /kaniko/.docker /kaniko/.docker\nCOPY --from=busybox:1.32.0 /bin /busybox\n\nENV PATH $PATH:/usr/local/bin:/kaniko:/busybox\nENV DOCKER_CONFIG /kaniko/.docker/\nENV DOCKER_CREDENTIAL_GCR_CONFIG /kaniko/.config/gcloud/docker_credential_gcr_config.json\nNote that you need to use gcr.io/kaniko-project/executor:debug (for latest version) or gcr.io/kaniko-project/executor:v1.6.0-debug as source (or another tag)\nTested building a small image, seems to work fine:\n# Built above example with docker build . -t kaniko-alpine\n# And ran container with docker run -it kaniko-alpine sh\necho \"FROM alpine\" > Dockerfile\necho \"RUN echo hello\" >> Dockerfile\necho \"COPY Dockerfile Dockerfile\" >> Dockerfile\n\nexecutor version\nexecutor -c . --no-push\n\n# Output like:\n#\n# Kaniko version :  v1.6.0\n#\n# INFO[0000] Retrieving image manifest alpine             \n# INFO[0000] Retrieving image alpine from registry index.docker.io \n# INFO[0000] GET KEYCHAIN                                 \n# [...] \n# INFO[0001] RUN echo hello                               \n# INFO[0001] Taking snapshot of full filesystem...        \n# INFO[0001] cmd: /bin/sh                                 \n# INFO[0001] args: [-c echo hello]                        \n# INFO[0001] Running: [/bin/sh -c echo hello]             \n# [...]\nNote that using Kaniko binaries outside of their official image is not recommended, even though it may still work fine:\nkaniko is meant to be run as an image: gcr.io/kaniko-project/executor. We do not recommend running the kaniko executor binary in another image, as it might not work.",
    "ModuleNotFoundError in Docker": "So I finally fixed the issue. For those who may be wondering how was it that I fixed it. You need to define a PYTHONPATH environment variable either in the Dockerfile or docker-compose.yml.",
    "npm run doesn't pass --configuration to build task": "Finally I found the solution of my problem in this post.\nMy sintax of npm run wasn't correct. The correct way to pass params to a npm command is adding -- operator.\nIn my case I need to change\nRUN npm run ng build --output-path=dist --configuration=$ENVIROMENT --verbose\nto\nRUN npm run ng build -- --output-path=dist --configuration=$ENVIROMENT --verbose",
    "Docker: Dockerfile vs docker-compose.yml": "You could put this docker-compose.yaml next to your Dockerfile:\nversion: '2'\nservices:\n  docker-whale:\n    image: docker-whale\n    build: .\nAnd then execute the following commands:\n# build docker image\ndocker-compose build\n\n# bring up one docker container\ndocker-compose up -d\n\n# scale up to three containers\ndocker-compose scale docker-whale=3\n\n# overview about these containers\ndocker-compose ps\n\n# view combined logs of all containers\n# use <ctrl-c> to stop viewing\ndocker-compose logs --follow\n\n# take down all containers\ndocker-compose down",
    "Docker-compose with nginx reverse, a website and a restful api?": "I wrote a tutorial specifically about reverse proxies with nginx and docker.\nCreate An Nginx Reverse Proxy With Docker\nYou'd basically have 3 containers and two without exposed ports that would be communicated through a docker network and each attached to the network.\nBash Method:\ndocker create my-network;\n# docker run -it -p 80:80 --network=my-network ...\nor\nDocker Compose Method:\nFile: docker-compose.yml\nversion: '3'\nservices:\n   backend:\n      networks:\n         - my-network\n   ...\n   frontend:\n      networks:\n         - my-network\n   proxy:\n      networks:\n         - my-network\nnetworks:\n   my-network:\nA - Nginx Container Proxy - MAPPED 80/80\nB - REST API - Internally Serving 80 - given the name backend\nC - Website - Internally Serving 80 - given the name frontend\nIn container A you would just have an nginx conf file that points to the different services via specific routes:\nFile: /etc/nginx/conf.d/default.conf\nserver {\n    listen       80;\n    server_name  localhost;\n    #charset koi8-r;\n    #access_log  /var/log/nginx/host.access.log  main;\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n        proxy_pass http://frontend;\n    }\n    location /api {\n        proxy_pass http://backend:5000/;\n    }\n    //...\n}\nThis makes it so that when you visit:\nhttp://yourwebsite.com/api = backend\nhttp://yourwebsite.com = frontend\nLet me know if you have questions, I've built this a few times, and even added SSL to the proxy container.\nThis is great if you're going to test one service for local development, but for production (depending on your hosting provider) it would be a different story and they may manage it themselves with their own proxy and load balancer.\n===================== UPDATE 1: =====================\nThis is to simulate both backend, frontend, a proxy and a mysql container in docker compose.\nThere are four files you'll need in the main project directory to get this to work.\nFiles:\n- backend.html\n- frontend.html\n- default.conf\n- docker-compose.yml\nFile: ./backend.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Backend API</title>\n</head>\n<body>\n    <h1>Backend API</h1>\n</body>\n</html>\nFile: ./frontend.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Frontend / Website</title>\n</head>\n<body>\n    <h1>Frontend / Website</h1>\n</body>\n</html>\nTo configure the proxy nginx to point the right containers on the network.\nFile: ./default.conf\n# This is a default site configuration which will simply return 404, preventing\n# chance access to any other virtualhost.\n\nserver {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n\n    # Frontend\n    location / {\n        proxy_pass http://frontend-name; # same name as network alias\n    }\n\n    # Backend\n    location /api {\n        proxy_pass http://backend-name/;  # <--- note this has an extra /\n    }\n\n    # You may need this to prevent return 404 recursion.\n    location = /404.html {\n        internal;\n    }\n}\nFile: ./docker-compose.yml\nversion: '3.5'\nservices:\n    frontend:\n        image: nginx:alpine\n        volumes:\n            - $PWD/frontend.html:/usr/share/nginx/html/index.html\n        networks:\n            my-network-name:\n                aliases:\n                    - frontend-name\n    backend:\n        depends_on:\n            - mysql-database\n        image: nginx:alpine\n        volumes:\n            - $PWD/backend.html:/usr/share/nginx/html/index.html\n        networks:\n            my-network-name:\n                aliases:\n                    - backend-name\n    nginx-proxy:\n        depends_on:\n            - frontend\n            - backend\n        image: nginx:alpine\n        volumes: \n            - $PWD/default.conf:/etc/nginx/conf.d/default.conf\n        networks:\n            my-network-name:\n                aliases:\n                    - proxy-name\n        ports:\n            - 1234:80\n    mysql-database:\n        image: mysql\n        command: --default-authentication-plugin=mysql_native_password\n        restart: always\n        environment:\n            MYSQL_DATABASE: 'root'\n            MYSQL_ROOT_PASSWORD: 'secret'\n        ports:\n            - '3306:3306'\n        networks:\n            my-network-name:\n                aliases:\n                    - mysql-name\nnetworks:\n    my-network-name:\nCreate those files and then run:\ndocker-compose -d up;\nThen visit:\nFrontend - http://localhost:1234\nBackend - http://localhost:1234/api\nYou'll see both routes now communicate with their respective services. You can also see that the fronend and backend don't have exposed ports. That is because nginx in them default port 80 and we gave them aliases within our network my-network-name) to refer to them.\nAdditionally I added a mysql container that does have exposed ports, but you could not expose them and just have the backend communicate to the host: mysql-name on port 3306.\nIf you to walkthrough the process a bit more to understand how things work, before jumping into docker-compose, I would really recommend checking out my tutorial in the link above.\nHope this helps.\n===================== UPDATE 2: =====================\nHere's a diagram:",
    "Change ImageMagick policy on a Dockerfile": "This change to your Dockerfile works\nARG imagemagic_config=/etc/ImageMagick-6/policy.xml\n\nRUN if [ -f $imagemagic_config ] ; then sed -i 's/<policy domain=\"coder\" rights=\"none\" pattern=\"PDF\" \\/>/<policy domain=\"coder\" rights=\"read|write\" pattern=\"PDF\" \\/>/g' $imagemagic_config ; else echo did not see file $imagemagic_config ; fi",
    "Docker Image > 1GB in size from python:3.8.3-alpine": "welcome to Docker! It can be quite the thing to wrap one's head around, especially when beginning, but you're asking really valid questions that are all pertinent\nReducing Size\nHow to\nA great place to start is Docker's own Dockerfile best practices page:\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/\nThey explain neatly how your each directve (COPY, RUN, ENV, etc) all create additional layers, increasing your containers size. Importantly, they show how to reduce your image size by minimising the different directives. They key to alot of minimisation is chaining commands in RUN statements with the use of &&.\nSomething else I note in your Dockerfile is one specific line:\nCOPY . $APP_HOME\nNow, depending on how you build your container (Specifically, what folder you pass to Docker as the context), this will copy EVERYTHING in that it has available to it. Chances are, this will be bringing in your venv folder etc if you have one. I feel that this may be the largest perpetrator of size for you. You can mitigate this by adding an explicit COPY in, or using a .dockerignore file.\nI built your image (Without any source code, and without copying in entrypoint.sh), and it came out to 710MB as a base. It could be a good idea to check the size of your source code, and see if anything else is getting in there. After I re-arranged some of the commands to reuse directives, the image was 484MB, which is considerably smaller! If you get stuck, I can pop it into a gist on Github for you and walk you through it, however, the Docker documentation should hopefully get you going\nWhy?\nWell, larger applications / images aren't inherently bad, but with any increase in data, some operations may be slower.\nWhen I say operations, I tend to mean pulling images from a registry, or pushing them to publish. It will take longer to transfer 1GB than it will 50MB.\nThere's also a consideration to be made when you scale your containers. While the image size does not necessarily correlate directly to how much disk you will use when you start a container, it will certainly increase the requirements for the machine you're running on, and limit others on smaller devices\nDocker\nThe advantages of using Docker are widespread, and I can't cover them all here without submitting my writing for thesis defence ;-)\nBut it mainly boils down to the following points:\nAlot of providers support running your applications in docker\nDockerfiles help you to build your application in a consisten environment, meaning you dont have to configure each host your app runs on, or worry about version clashes\nContainers let you develop and run your application in a consistent (And the same) environment\nContainers usually provide really nice networking capabilities. An example you will have encountered is within docker compose, you can reach other containers simply through their hostname\nNginx\nYou've set things up well there, from what I can gather! I imagine nginx is 'telling you' (Via the logs?) to navigate to 0.0.0.0 because that is what it will have bound to in the container. Now, you've forwarded traffic from 1337:80. Docker follows the format of host:container, so this means that traffic on localhost:1337 will be directed to the containers port 80. You may need to swap this around based on your nginx configuration, but rest assured you will be able to navigate to localhost in your browser and see your website once everything is set up\nLet me know if you need help with any of the above, or want more resources to aid you. Happy to correspond and walk you through anything anytime given we seem to be in the same timezone \ud83e\udd19",
    "Add sudo permission (without password ) to user by command line": "First, you are not suggested to use sudo in docker. You could well design your behavior using USER + gosu.\nBut, if you insist for some uncontrolled reason, just add next line after you setup normal user:\nRUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\nSo for your scenario, the workable one is:\nFROM ubuntu:bionic\n\nENV DEBIAN_FRONTEND noninteractive\n\n# Get the basic stuff\nRUN apt-get update && \\\n    apt-get -y upgrade && \\\n    apt-get install -y \\\n    sudo\n\n# Create ubuntu user with sudo privileges\nRUN useradd -ms /bin/bash ubuntu && \\\n    usermod -aG sudo ubuntu\n# New added for disable sudo password\nRUN echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n\n# Set as default user\nUSER ubuntu\nWORKDIR /home/ubuntu\n\nENV DEBIAN_FRONTEND teletype\n\nCMD [\"/bin/bash\"]\nTest the effect:\n$ docker build -t abc:1 .\nSending build context to Docker daemon  2.048kB\nStep 1/9 : FROM ubuntu:bionic\n......\nSuccessfully built b3aa0793765f\nSuccessfully tagged abc:1\n\n$ docker run --rm abc:1 cat /etc/sudoers\ncat: /etc/sudoers: Permission denied\n\n$ docker run --rm abc:1 sudo cat /etc/sudoers\n#\n# This file MUST be edited with the 'visudo' command as root.\n#\n# Please consider adding local content in /etc/sudoers.d/ instead of\n# directly modifying this file.\n#\n# See the man page for details on how to write a sudoers file.\n#\nDefaults        env_reset\n......\n#includedir /etc/sudoers.d\n%sudo ALL=(ALL) NOPASSWD:ALL\nYou could see with sudo, we could already execute a root-needed command.",
    "How to keep a docker container run for ever": "From HOW TO KEEP DOCKER CONTAINERS RUNNING, we can know that docker containers, when run in detached mode (the most common -d option), are designed to shut down immediately after the initial entrypoint command (program that should be run when container is built from image) is no longer running in the foreground. So to keep docker container run even the inside program has done\njust add CMD tail -f /dev/null as last line to dockerfile\nWhat's more important is that we should understand what's docker is intended for and how to use it properly. Try to use docker as environment foundation for applications in host machine is not a good choice, docker is designed to run applications environment-independent. Applications should be placed into docker image via docker build and run in docker container in runtime.",
    "Can't set environment variables with docker run -e or --env-file option": "Rearrange the docker run command, as the default entrypoint for node base docker image, is node, so the container considers --env-file .env as an argument to node process.\ndocker run -d -p 3000:3000  --env-file .env chatapp-back\nAlso, you can verify this before running the main process.\ndocker run -it -p 3000:3000 --env-file .env chatapp-back -e \"console.log(process.env)\"",
    "Unsupported config option for services.web: 'dockerfile'": "The best way to troubleshoot these type of issues is checking the docker-compose reference for the specific version - https://docs.docker.com/compose/compose-file/.\nWithout testing, the issue is because you need to put dockerfile under build.\nOld:\nweb:\n  dockerfile: Dockerfile\nNew:\nweb:\n  build:\n    context: .\n    dockerfile: Dockerfile\nSince you are using the standard Dockerfile filename, you could also use:\nbuild:\n  context: .\nFrom reference page:\nDOCKERFILE Alternate Dockerfile.\nCompose uses an alternate file to build with. A build path must also be specified.\nbuild:\n  context: .\n  dockerfile: Dockerfile-alternate",
    "Install and using pip and virtualenv in Docker": "On Debian-based platforms, including Ubuntu, the command installed by python3-pip is called pip3 in order for it to peacefully coexist with any system-installed Python 2 and its pip.\nSomewhat similarly, the virtualenv command is not installed by the package python3-virtualenv; to get that, you need apt-get install -y virtualenv.\nNote that venv is included in the Python 3 standard library, so you don't really need to install anything at all.\npython3 -m venv newenv\nWhy would you want a virtualenv inside Docker anyway, though? (There are situations where it makes sense but in the vast majority of cases, you want the Docker container to be as simple as possible, which means, install everything as root, and rebuild the whole container if something needs to be updated.)\nAs an aside, you generally want to minimize the number of RUN statements. Making many layers while debugging is perhaps defensible, but layers which do nothing are definitely just wasteful. Perhaps also discover that apt-get can install more than one package at a time.\nRUN apt-get update -y && \\\n    apt-get install -y python3 python3-pip && \\\n    ...\nThe && causes the entire RUN sequence to fail as soon as one of the commands fails.",
    "Dockerfile CMD `command not found`": "You are using wrong quotes. It should be:\nCMD [\"bash\", \"npm run lint\"]",
    "Scripts in the /docker-entrypoint-initdb.d folder are ignored": "Postgres only initializes the database if no database is found, when the container starts. Since you have a volume mapping on the database directory, chances are that a database already exists.\nIf you delete the db_data volume and start the container, postgres will see that there isn't a database and then it'll initialize one for you using the scripts in docker-entrypoint-initdb.d.",
    "EACCES: permission denied, open \"file-path\"": "It is clear that node_modules folder in container is built by root user during the step npm install, therefore has root as user. This is the reason we don't have access to that folder when we set up our node user. To resolve this what we have to do is firstly using the root user we have to give permission to the node user while copying files from local directory to image and then later set up node as the user as shown below:\nCOPY --chown=node:node package.json .\nRUN npm install\n\nCOPY --chown=node:node . .\nUSER node",
    "How do I set up my Dockerfile to use cpanm to install a specific version of a Perl module?": "For anyone who's searching for this same answer in the future, another option can be found here in the documentation for cpanm:\ncpanm Plack@0.9990\nIf you have a long list of modules, consider feeding a cpanfile into cpanm rather than listing them all in the Dockerfile.\nThe easiest way to specify a particular version number for a module in a cpanfile is like this:\nrequires 'Text::ParseWords', '==3.1';\nThe syntax for requesting the latest version of a module is this:\nrequires 'Text::ParseWords';\nRequesting a minimum version: (note the lack of '==')\nrequires 'Text::ParseWords', '3.1';\nThe syntax for requesting specific versions in other ways is fairly well-documented here.\nAnother great write-up of the use of cpanm and a cpanfile can be found in Installation of cpan modules by cpanm and cpanfile.",
    "Call From quickstart.cloudera/172.17.0.2 to quickstart.cloudera:8020 failed on connection exception: java.net.ConnectException: Connection refused": "Port 8020 is for the hdfs-namenode service, so my guess is that service not started or has failed.\nCan you try to restart it?\ncommand: sudo  service hadoop-hdfs-namenode restart\nYou can also check the status of the namenode service.\nCommand: sudo  service hadoop-hdfs-namenode status\nAlso, check the hadoop-hdfs-datanode service as it may also need to be restarted.\ncommand: sudo  service hadoop-hdfs-datanode restart\nIf you still get the error then check the NameNode logs in /var/log/hadoop-hdfs and add it to your question for further analysis.",
    "$GOPATH/go.mod exists but should not when building docker container, but works if I manually run commands": "I have same problem. You need set WORKDIR /go/delivery",
    "How do I run TypeScript `tsc` before `COPY` in Dockerfile.template?": "You can use a multi-stage build for this. The first stage includes all of the development dependencies, including tsc; the second stage only includes the files that are needed to run the built application.\n(I'm not familiar with the specific build environment you're using so this will be in terms of the standard node image.)\n# First stage: compile things.\nFROM node:12 AS build\nWORKDIR /usr/src/app\n\n# (Install OS dependencies; include -dev packages if needed.)\n\n# Install the Javascript dependencies, including all devDependencies.\nCOPY package.json .\nRUN npm install\n\n# Copy the rest of the application in and build it.\nCOPY . .\n# RUN npm build\nRUN npx tsc -p ./tsconfig.json\n\n# Now /usr/src/app/dist has the built files.\n\n# Second stage: run things.\nFROM node:12\nWORKDIR /usr/src/app\n\n# (Install OS dependencies; just libraries.)\n\n# Install the Javascript dependencies, only runtime libraries.\nCOPY package.json .\nRUN npm install --production\n\n# Copy the dist tree from the first stage.\nCOPY --from=build /usr/src/app/dist dist\n\n# Run the built application when the container starts.\nEXPOSE 3000\nCMD [\"npm\", \"run\", \"serve\"]",
    "Cloning a git repo in Dockerfile and working off it": "You can do it by three ways.\nHere is the Dockerfile.\nFROM node:alpine\nRUN apk add --no-cache git\nRUN apk add --no-cache openssh\nWORKDIR /data\nRUN git clone https://github.com/jahio/hello-world-node-express.git /data/app\nWORKDIR /data/app\nEXPOSE 3000\nBuild:\ndocker build -t node-test .\nUpdate:\nDamn, I am crazy about docker :D Another solution easiest and good\nCreate an empty directory in host and container and mount that one\n/home/adiii/Desktop/container_Data:/to_host\nCopy the cloned repo to to_host at the entry point with -u flat so only will new file will be paste and host data will be persistent.\nand entrypoint.sh\n#!/bin/ash\ncp -r -u /data/app /to_host && /bin/ash\ndockerfile update section.\nADD entrypoint.sh /usr/bin/entrypoint.sh\nRUN chmod +x /usr/bin/entrypoint.sh\nRUN WORKDIR /\nRUN mkdir -p to_host\n\n# so we will put the code in to_host after container bootup boom im crazy about docker so no need to make it complex..simple easy :D\n\nENTRYPOINT [ \"/usr/bin/entrypoint.sh\" ]\n1: using docker volume\nCreate volume named code\ndocker volume create code\nNow run that container with mounting this volume.\ndocker run -p 3000:3000 -v myvol:/data/app --rm -it node-test ash\nNow terminate the container or stopping it will data still preserved in volume.\nYou can find if OS is Linux.\n/var/lib/docker/volumes/code/_data\nyou will see three\napp.js  node_modules  package.json\n2: using bash see comments in the script\n  #!/bin/bash\nimage_name=node-test\ncontainer_name=git_code\n\n\n# for first time use first_time\nif [ $1 == \"first_time\" ] ; then\n# remove if exist\ndocker rm -f $container_name\n#run contianer for first time to copy code\ndocker run --name $container_name -dit $image_name ash\nfi\n# check if running\nif docker inspect -f '{{.State.Running}}' $container_name ; then\n# copy code from container to /home/adiii/desktop\ndocker cp $container_name:/data/app /home/adil/Desktop/app\nfi\n\n# for normal runing using run \nif [ $1 == \"run\" ]; then\n# remove old container if running\ndocker rm -f $container_name\ndocker run --name $container_name -v /home/adil/Desktop/app:/data/app -dit $image_name\nfi\nNow run the command in the container\ndocker exec -it git_code ash\n3: By mounting the empty directory of the host with code directory of the container at the runtime. So when you run next time with mount directory it will contain your update code which you made any change from host OS. But make sure the permission of that directory after container run and terminated data will be there but the behaviour of this method is not constant.\ndocker run -p 3000:3000 -v /home/adiii/code/app:/data/app --rm -it node-test ash\nHere /home/adiii/code/app is an empty directory of host and after termination of containers its still have cloned code but I said its behavior varies.",
    "Unable to build docker from root": "Move your dockerfile out of your root directory.\nThere are a whole lot of reasons to do this, not the least of which is permissions issues. You should nearly never be using your root directory as your working directory.",
    "\"CMD ['/home/user/script.sh']\" in docker file doesn't work with docker-compose": "I simplified your example a bit\nsee https://github.com/BITPlan/docker-stackoverflowanswers/tree/master/33229581\nand used:\ndocker-compose.yml\nweb:\n  build: .\n  ports:\n   - \"8888:8888\"\nfind .\n.\n./docker-compose.yml\n./Dockerfile\n./myproject\ndocker build & run\ndocker-compose build\ndocker-compose run web\nand of course I get\n/bin/sh: 1: [/home/root/myproject/uwsgi.sh,: not found\nassuming this is since there is no uwsgi.sh in the directory myproject.\nIf I add uwsgi.sh with\necho 'echo $0 is there and called with params $@!' > myproject/uwsgi.sh\nchmod +x myproject/uwsgi.sh\nand test it with\ndocker-compose run web /bin/bash\nls\ncat uwsgi.sh\n./uwsgi.sh start\nit's there and behaves as expected:\nroot@9f06f8ff8c3b:/home/root/myproject# ls\nuwsgi.sh\nroot@9f06f8ff8c3b:/home/root/myproject# cat uwsgi.sh \necho $0 is there and called with params $@!\nroot@9f06f8ff8c3b:/home/root/myproject# ./uwsgi.sh start\n./uwsgi.sh is there and called with params start!\n root@9f06f8ff8c3b:/home/root/myproject# \nbut for docker-compose run web I still get\n/bin/sh: 1: [/home/root/myproject/uwsgi.sh,: not found\nIf I add a single blank to the Dockerfile CMD line:\nCMD [ '/home/root/myproject/uwsgi.sh', 'start' ] \nthe result is: /bin/sh: 1: [: /home/root/myproject/uwsgi.sh,: unexpected operator\nwhich brings us closer. As a next step I am leaving out the \"start\" parameter.\nCMD [ '/home/root/myproject/uwsgi.sh' ] \nthis now leads to no output at all ...\nIf i change the CMD line to:\nCMD [ \"/home/root/myproject/uwsgi.sh\", \"start\" ] \nI get\n Cannot start container \n 0b9da138c43ef308ad70da4a7718cb96fbfdf6cda113e2ae0ce5e24de06f07cd: [8]  \n System   error: exec format error\nand now you could continue in the Edison like approach:\nI have not failed. I've just found 10,000 ways that won't work. until you find\nCMD [ \"/bin/bash\", \"/home/root/myproject/uwsgi.sh\", \"start\" ]\nwhich brings you closer with the result:\n /home/root/myproject/uwsgi.sh is there and called with params start!\nCMD expects an executable as the first parameter e.g.\nhttps://docs.docker.com/compose/\nhas\nCMD python app.py\nas an example. To run your shell script you'll need a shell like bash. See also https://stackoverflow.com/a/33219131/1497139",
    "docker-compose yaml - option to pass the 'ulimit' parameters 'rtprio' and 'memlock'": "There's a per-service dictionary called ulimits:.\nversion: '3'\nservices:\n  my_proj:\n    image: image/my_image\n    ulimits:\n      rtprio: 95\n      memlock: -1\n    ...\nNote that Docker Compose works better with non-interactive services that stay running; I would use it to launch your service proper and not necessarily to get an interactive shell in a temporary container.",
    "Where to see the Dockerfile for a docker image?": "Use\ndocker history --no-trunc IMAGE_NAME_OR_ID\nThis will show all commands run in the image building process in reverse order. It's not exactly a Dockerfile, but you can find all essential content.",
    "Creating a table in single user mode in postgres": "@a_horse_with_no_name got me on the right track with his comment. I decided to ditch the single user mode even if it was \"recommended\". Instead I start postgres with pg_ctl, load some sql files containing my table creations, and stop the server with pg_ctl.\nMy shell script looks like this:\n#!/bin/bash\necho \"******CREATING DOCKER DATABASE******\"\n\necho \"starting postgres\"\ngosu postgres pg_ctl -w start\n\necho \"bootstrapping the postgres db\"\ngosu postgres psql -h localhost -p 5432 -U postgres -a -f /db/bootstrap.sql\n\necho \"initializing tables\"\ngosu postgres psql -h localhost -p 5432 -U postgres -d orpheus -a -f /db/setup.sql\n\necho \"stopping postgres\"\ngosu postgres pg_ctl stop\n\necho \"stopped postgres\"\n\n\necho \"\"\necho \"******DOCKER DATABASE CREATED******\"",
    "Cannot change permissions for 'RUN chmod +x /app-entrypoint.sh' in Dockerfile": "The COPY step will create the file with the uid/gid of 0:0 (root:root) within the / directory where normal users have no access. And the selected base image is configured to run as uid 1001. Probably the easiest is to switch back to root temporarily to run that step.\nFROM docker.io/bitnami/jasperreports:7-debian-10\nUSER root\nCOPY custom-entrypoint.sh /app-entrypoint.sh\nUSER 1001\nRUN  chmod +x /app-entrypoint.sh\nAlternatively, you can copy the script to a directory where the user has access with the right ownership on the file. Without pulling the image, I suspect /opt/bitnami/scripts may have different permissions:\nFROM docker.io/bitnami/jasperreports:7-debian-10\nCOPY --chown=1001 custom-entrypoint.sh /opt/bitnami/scripts/app-entrypoint.sh\nRUN  chmod +x /opt/bitnami/scripts/app-entrypoint.sh",
    "Unable to run docker image due to libGl error": "I was able to make the docker container run by making following changes to the dockerfile\nFROM python:3.6.8\nCOPY . /app\nWORKDIR /app\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y\nRUN apt install libgl1-mesa-glx -y\nRUN apt-get install 'ffmpeg'\\\n    'libsm6'\\\n    'libxext6'  -y\nRUN pip3 install --upgrade pip\n\nRUN pip3 install opencv-python==4.3.0.38\nRUN pip3 install -r requirements.txt\nEXPOSE 80\nCMD [\"python3\", \"server.py\"]\nThe lines required for resolving the libGl error\nRUN apt install libgl1-mesa-glx -y\nRUN apt-get install 'ffmpeg'\\\n    'libsm6'\\\n    'libxext6'  -y\nwere not able to run without updating the ubuntu environment. Moreover creating the docker image as noninteractive helped to skip any interactive command line inputs",
    "Docker Image Timestamp Issue": "That's from Google Jib. For reproducibility they don't set a date, or they explicitly set the date to the zero value, which is the epoch in 1970.\nThere is a FAQ entry for this: https://github.com/GoogleContainerTools/jib/blob/master/docs/faq.md#why-is-my-image-created-48-years-ago\nFor reproducibility purposes, Jib sets the creation time of the container images to the Unix epoch (00:00:00, January 1st, 1970 in UTC). If you would like to use a different timestamp, set the jib.container.creationTime / <container><creationTime> parameter to an ISO 8601 date-time. You may also use the value USE_CURRENT_TIMESTAMP to set the creation time to the actual build time, but this sacrifices reproducibility since the timestamp will change with every build.\nSetting creationTime parameter\nMaven:\n<configuration>\n  <container>\n    <creationTime>2019-07-15T10:15:30+09:00</creationTime>\n  </container>\n</configuration>\nGradle:\njib.container.creationTime = '2019-07-15T10:15:30+09:00'",
    "dotnet build with version is not working in docker": "Yes, it should be a trivial task but I also ran into a few issues before I finally could do it. Spent a few hours to solve the issue, hope you can save time. I recommend to read this post it helped.\nMy final solution was to use a Docker ARG, you must be declare it before the first FROM:\n#Declare it at the beginning of the Dockerfile\nARG BUILD_VERSION=1.0.0\n...\n#Publish your project with \"-p:Version=${BUILD_VERSION}\" it works also with \"dotnet build\"\nRUN dotnet publish \"<xy.csproj>\" -c Release -o /app/publish --no-restore -p:Version=${BUILD_VERSION}\nOne very important thing to Note: is if you are using multistage build (multiple FROM in your file) you have to \"re-declare\" the ARG in that stage. See a similar question here.\nFinally you can call your Docker build with:\ndocker build --build-arg BUILD_VERSION=\"1.1.1.1\"",
    "How to automatically delete intermediate stage Docker containers from my system?": "You should build it like this:\ndocker build --rm -t kube-test-app .\nIf dockerfile is in direcory were you located or specify path to dockerfile\ndocker build --rm -t kube-test-app -f path/to/dockerfile .\n-t is tag, name of your builded docker image\nFor remove all except images and runing containers use docker system prune\nFor remove image docker rmi -f image_name",
    "URL Rewrite 2.0 installation fails on Docker": "I finally figured it out thanks to this article. Using PowerShell to run msiexec with the appropriate switches works. Oddly, it threw \"Unable to connect to the remote server\" when trying to also download the MSI using PowerShell, so I resorted to using ADD.\nHere's the relevant portion of my Dockerfile:\nWORKDIR /install\nADD https://download.microsoft.com/download/C/9/E/C9E8180D-4E51-40A6-A9BF-776990D8BCA9/rewrite_amd64.msi rewrite_amd64.msi\nRUN Write-Host 'Installing URL Rewrite' ; \\\n    Start-Process msiexec.exe -ArgumentList '/i', 'rewrite_amd64.msi', '/quiet', '/norestart' -NoNewWindow -Wait",
    "docker build purely from command line": "Update for 2017-05-05: Docker just released 17.05.0-ce with this PR #31236 included. Now the above command creates an image:\n$ docker build -t test-no-df -f - . <<EOF\nFROM busybox:latest\nCMD echo just a test\nEOF\nSending build context to Docker daemon  23.34MB\nStep 1/2 : FROM busybox:latest\n ---> 00f017a8c2a6\nStep 2/2 : CMD echo just a test\n ---> Running in 45fde3938660\n ---> d6371335f982\nRemoving intermediate container 45fde3938660\nSuccessfully built d6371335f982\nSuccessfully tagged test-no-df:latest\nThe same can be achieved in a single line with:\n$ printf 'FROM busybox:latest\\nCMD echo just a test' | docker build -t test-no-df -f - .\nOriginal Response\ndocker build requires the Dockerfile to be an actual file. You can use a different filename with:\ndocker build -f Dockerfile.temp .\nThey allow the build context (aka the . or current directory) to be passed by standard input, but attempting to pass a Dockerfile with this syntax will fail:\n$ docker build -t test-no-df -f - . <<EOF\nFROM busybox:latest\nCMD echo just a test\nEOF\n\nunable to prepare context: unable to evaluate symlinks in Dockerfile path: lstat /home/bmitch/data/docker/test/-: no such file or directory",
    "Docker multi-stage builds: how to set WORKDIR globally for every stage?": "A FROM line inherits almost all of the settings from its base image, including its WORKDIR. Since your later stages derive from the first stage you only need to include WORKDIR in the base stage.\nFROM base AS builder\nWORKDIR /app\n\nFROM builder AS test\n# already in /app directory\n\nFROM builder AS deploy\n# already in /app directory",
    "Why copy package.json and install dependencies before copying the rest of the project within docker? [duplicate]": "It's done because of Docker layer caching. If you run docker build when none of the copied files have changed, the image layers will just be read from the cache. If a COPY file has changed, all layers after that in the image will be rebuilt.\nRUN yarn install is an expensive operation and we don't want to need to execute it again when any random source files in the project change. This way, it's only re-executed if the package.json or package-lock.json files have changed.",
    "How to log docker healthcheck status in docker logs?": "OK, this is super hacky and I'm not proud of it. The issue is that the healthcheck runs in a different process than your main process, so it's hard to write to the main process' stdout.\nBut you can exploit that the main process (usually) runs as process #1 and write to /proc/1/fd/1 which is that process' stdout. Something like this\nFROM ubuntu\nHEALTHCHECK --interval=1s --timeout=30s --retries=3 CMD echo {'health': 'healthy'} | tee /proc/1/fd/1\nCMD tail -f /dev/null\nThe tail -f /dev/null is just a dummy command that keeps the container running.",
    "Adding SSL certificate when using Google Jib and Kubernetes": "Answer recommended by Google Cloud Collective",
    "public key is not available: NO_PUBKEY F76221572C52609D": "There are several issues here:\n1) W: GPG error: https://apt.dockerproject.org debian-jessie InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY F76221572C52609D W: There is no public key available for the following key IDs: AA8E81B4331F7F50\nSolution:\nMove the keyserver add actions to the place before RUN echo 'deb http://deb.debian.org/debian jessie-backports main' > /etc/apt/sources.list.d/jessie-backports.list, meanwhile add AA8E81B4331F7F50 also as next:\nRUN apt-get install -y --no-install-recommends apt-transport-https ca-certificates\nRUN apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\nRUN apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys AA8E81B4331F7F50\n2) W: Failed to fetch http://deb.debian.org/debian/dists/jessie-backports/main/binary-amd64/Packages 404 Not Found\nE: Some index files failed to download. They have been ignored, or old ones used instead.\nSolution:\nmicrosoft/aspnetcore-build:1.0.1 base on debian8, and you want to use openjdk8 which was default not in apt repository. So you use deb http://deb.debian.org/debian jessie-backports main.\nUnfortunately, if you check http://ftp.debian.org/debian/dists/, you will find jessie-backports had been removed. So you had to switch to archived url like next (Comment the old url, just use the url next):\n#RUN echo 'deb http://deb.debian.org/debian jessie-backports main' > /etc/apt/sources.list.d/jessie-backports.list\nRUN echo 'deb http://archive.debian.org/debian jessie-backports main' > /etc/apt/sources.list.d/jessie-backports.list\nMeanwhile, you had to add next after doing above to resolve release-file-expired-problem:\nRUN echo \"Acquire::Check-Valid-Until \\\"false\\\";\" > /etc/apt/apt.conf.d/100disablechecks\n3) ENV JAVA_VERSION 8u111\nENV JAVA_DEBIAN_VERSION 8u111-b14-2~bpo8+1\nSolution:\nNot sure how you get this version, but in fact after change to archive jessie backports, what you could get is something like next:\nroot@2ecaeffec483:/etc/apt# apt-cache policy openjdk-8-jdk\nopenjdk-8-jdk:\n  Installed: (none)\n  Candidate: 8u171-b11-1~bpo8+1\n  Version table:\n     8u171-b11-1~bpo8+1 0\n        100 http://archive.debian.org/debian/ jessie-backports/main amd64 Packages\nSo, you had to change to next:\nENV JAVA_VERSION 8u171\nENV JAVA_DEBIAN_VERSION 8u171-b11-1~bpo8+1",
    "Install certificate in dotnet core docker container": "Is your Docker container running on Linux?\nI assume that it is. Then your base image should be microsoft/aspnetcore, which is based on Ubuntu.\nYou should add this in your DOCKERFILE:\nCOPY ca_bundle.crt /usr/local/share/ca-certificates/your_ca.crt\nRUN update-ca-certificates\nFirst line copies your CA bundle into the image, the second line updates the CA list.\nThe CA bundle (the list of authorities that signed your certificate) can be extracted from PFX, just Google for it. This is the first link I found.\nIf your container is running on Windows, then Powershell command should work as-is (I'm not sure about that)",
    "Docker container exits when using -it option": "This behaviour is caused by Apache and it is not an issue with Docker. Apache is designed to shut down gracefully when it receives the SIGWINCH signal. When running the container interactively, the SIGWINCH signal is passed from the host to the container, effectively signalling Apache to shut down gracefully. On some hosts the container may exit immediately after it is started. On other hosts the container may stay running until the terminal window is resized.\nIt is possible to confirm that this is the source of the issue after the container exits by reviewing the Apache log file as follows:\n# Run container interactively:\ndocker run -it <image-id>\n\n# Get the ID of the container after it exits:\ndocker ps -a\n\n# Copy the Apache log file from the container to the host:\ndocker cp <container-id>:/var/log/apache2/error.log .\n\n# Use any text editor to review the log file:\nvim error.log\n\n# The last line in the log file should contain the following:\nAH00492: caught SIGWINCH, shutting down gracefully\nSources:\nhttps://bz.apache.org/bugzilla/show_bug.cgi?id=50669\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1212224\nhttps://github.com/docker-library/httpd/issues/9",
    "Volumes and docker-compose": "Aug. 2022:\nbrandt points out in the comments to the updated docker-compose documentation.\nNote August 2017: with docker-compose version 3, regarding volumes:\nThe top-level volumes key defines a named volume and references it from each service\u2019s volumes list.\nThis replaces volumes_from in earlier versions of the Compose file format. See Use volumes and Volume Plugins for general information on volumes.\nExample:\nversion: \"3.2\"\nservices:\n  web:\n    image: nginx:alpine\n    volumes:\n      - type: volume\n        source: mydata\n        target: /data\n        volume:\n          nocopy: true\n      - type: bind\n        source: ./static\n        target: /opt/app/static\n\n  db:\n    image: postgres:latest\n    volumes:\n      - \"/var/run/postgres/postgres.sock:/var/run/postgres/postgres.sock\"\n      - \"dbdata:/var/lib/postgresql/data\"\n\nvolumes:\n  mydata:\n  dbdata:\nThis example shows a named volume (mydata) being used by the web service, and a bind mount defined for a single service (first path under db service volumes).\nThe db service also uses a named volume called dbdata (second path under db service volumes), but defines it using the old string format for mounting a named volume.\nNamed volumes must be listed under the top-level volumes key, as shown.\nFebruary 2016:\nThe docs/compose-file.md mentions:\nMount all of the volumes from another service or container, optionally specifying read-only access(ro) or read-write(rw).\n(If no access level is specified, then read-write will be used.)\nvolumes_from:\n - service_name\n - service_name:ro\n - container:container_name\n - container:container_name:rw\nFor instance (from this issue or this one)\nversion: \"2\"\n\nservices:\n...\n  db:\n    image: mongo:3.0.8\n    volumes_from:\n      - dbdata\n    networks:\n      - back\n    links:\n      - dbdata\n\n dbdata:\n    image: busybox\n    volumes:\n      - /data/db",
    "How can I fix the 'stream terminated by RST_STREAM' error when building a Docker image for a Vue app?": "This issue has to do with the \"Dockerfile\" text encoding, change it to UTF-8, you can do that in notepad++ or any other editor like VS Code. This fixed the issue for me.",
    "Unknown Instruction ln Docker File in RUN": "You need to add an &&\\ to link RUN commands togethers\nOtherwise Dockerfile won't be able to interpret ln as a Dockerfile command (like RUN, COPY, ADD, ...)\nRUN mv /maven /opt/maven &&\\    <============== missing\n    ln -s /opt/maven/bin/mvn /usr/bin/mvn && \\\nOr at least add a second run\nRUN mv /maven /opt/maven \nRUN ln -s /opt/maven/bin/mvn /usr/bin/mvn && \\\n ^  ...\n |\n --- missing",
    "How can I run a docker container on localhost over the default IP?": "The default network is bridged. The 0.0.0.0:49166->443 shows a port mapping of exposed ports in the container to high level ports on your host because of the -P option. You can manually map specific ports by changing that flag to something like -p 8080:80 -p 443:443 to have port 8080 and 443 on your host map into the container.\nYou can also change the default network to be your host network as you've requested. This removes some of the isolation and protections provided by the container, and limits your ability to configure integrations between containers, which is why it is not the default option. That syntax would be:\ndocker run --name nginx1 --net=host -d nginx\nEdit: from your comments and a reread I see you're also asking about where the 10.0.75.2 ip address comes from. This is based on how you launch the docker daemon. That IP binding is assigned when you pass the --ip flag to the daemon documentation here. If you're running docker in a vm with docker-machine, I'd expect this to be the IP of your vm.",
    "Run dotnet tests in docker which use testcontainers": "I could manage to do it, with two major differences:\nThe tests do not run on the docker image, but rather on the docker container.\nI am using docker compose now.\ndocker-compose-tests.yml:\nversion: '3.4'\n\nservices:\n  myproject.authentication.api.tests: # docker compose -f docker-compose-tests.yml up myproject.authentication.api.tests\n    build:\n      context: .\n      dockerfile: Authentication.Api/MyProject.Authentication.Api/Dockerfile\n      target: build\n    command: >\n        sh -cx \"\n                dotnet test /src/Authentication.Api/MyProject.Authentication.Api.IntegrationTests/MyProject.Authentication.Api.IntegrationTests.csproj -c Release --results-directory /testresults --logger \\\"trx;LogFileName=testresults_authentication_api_it.trx\\\" /p:CollectCoverage=true /p:CoverletOutputFormat=json%2cCobertura /p:CoverletOutput=/testresults/coverage/ -p:MergeWith=/testresults/coverage/coverage.json\"\n    environment:\n      - TESTCONTAINERS_HOST_OVERRIDE=host.docker.internal # Needed in Docker Desktop (Windows), needs to be removed on linux hosts. Can be done with a override compose file.\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - coverage:/testresults/coverage\n    container_name: myproject.authentication.api.tests\n(\"sh\" command is useful if more test projects are expected to run.)\nAuthentication.Api/MyProject.Authentication.Api/Dockerfile:\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\nWORKDIR /src\nCOPY [\"Authentication.Api/MyProject.Authentication.Api/MyProject.Authentication.Api.csproj\", \"Authentication.Api/MyProject.Authentication.Api/\"]\nCOPY [\"Authentication.Api/MyProject.Authentication.Api.IntegrationTests/MyProject.Authentication.Api.IntegrationTests.csproj\", \"Authentication.Api/MyProject.Authentication.Api.IntegrationTests/\"]\nRUN dotnet restore \"Authentication.Api/MyProject.Authentication.Api/MyProject.Authentication.Api.csproj\"\nRUN dotnet restore \"Authentication.Api/MyProject.Authentication.Api.IntegrationTests/MyProject.Authentication.Api.IntegrationTests.csproj\"\nCOPY . .\n\nWORKDIR \"/src/Authentication.Api/MyProject.Authentication.Api\"\nRUN dotnet build \"MyProject.Authentication.Api.csproj\" -c Release -o /app/build\n\nWORKDIR \"/src/Authentication.Api/MyProject.Authentication.Api.IntegrationTests\"\nRUN dotnet build -c Release\nAuthentication.Api/MyProject.Authentication.Api.IntegrationTests/Factory/CustomWebApplicationFactory.cs: same as in the question.\n{shared library path}/MsSqlDatabaseProvider.cs:\npublic class MsSqlDatabaseProvider\n{\n    private const string DbImage = \"mcr.microsoft.com/mssql/server:2019-latest\";\n    private const string DbUsername = \"sa\";\n    private const string DbPassword = \"my_dummy_password#123\";\n    private const ushort MssqlContainerPort = 1433;\n\n\n    public readonly TestcontainerDatabase Database;\n\n    public MsSqlDatabaseProvider() =>\n        Database = new TestcontainersBuilder<MsSqlTestcontainer>()\n            .WithDatabase(new MsSqlTestcontainerConfiguration\n            {\n                Password = DbPassword,\n            })\n            .WithImage(DbImage)\n            .WithCleanUp(true)\n            .WithPortBinding(MssqlContainerPort, true)\n            .WithEnvironment(\"ACCEPT_EULA\", \"Y\")\n            .WithEnvironment(\"MSSQL_SA_PASSWORD\", DbPassword)\n            .WithWaitStrategy(Wait.ForUnixContainer().UntilCommandIsCompleted(\"/opt/mssql-tools/bin/sqlcmd\", \"-S\", $\"localhost,{MssqlContainerPort}\", \"-U\", DbUsername, \"-P\", DbPassword))\n            .Build();\n}\nAnd I can run the tests in docker with docker compose -f docker-compose-tests.yml up myproject.authentication.api.tests.",
    "Dynamic Docker base image": "For that, you need to define Global ARGs and better to have some default value and override it during build time.\nARG sample_TAG=test\nFROM maven:3.6.1-jdk-8 as maven-build\nARG sample_TAG\nWORKDIR /apps/sample-google\nRUN echo \"image tag is ${sample_TAG}\"\nFROM $sample_TAG\nVOLUME /apps\nRUN mkdir /apps/sample-google",
    "Multistage dockerfile skip stages": "Skipping stages only works with BuildKit. See the discussion here and article here.\nAs for the error you're getting, you should be getting it with or without BuildKit as you cannot use build arguments in COPY instruction. The difference being that with BuildKit Docker will refuse to even start the build and without it the build will fail on COPY instruction.\nWhat you need to do is to create an additional alias for the image you want to copy from using the fact that FROM instruction resolves the build args:\nARG GIT_TOKEN=abc:1a2b3\nARG EXECUTION_ENV=local\n\n# get dependencies from github\nFROM alpine/git as gitclone-ci\nWORKDIR /usr/src/\nRUN git clone https://{GIT_USER_TOKEN}@github.com/something.git \\\n    && git clone https://{GIT_USER_TOKEN}@github.com/somethingelse.git\n\n## in local dependencies are already available in the parent folder\nFROM alpine/git as gitclone-local\nWORKDIR /usr/src/\nCOPY ../something /usr/src/something \nCOPY ../somethingelse /usr/src/somethingelse\n\nFROM gitclone-${EXECUTION_ENV} as intermediate\n\nFROM node:latest as builder\nWORKDIR /usr/src\nCOPY --from=intermediate /usr/src .\nCOPY package* ./\nCOPY src/ src/\nRUN [\"npm\", \"install\"]",
    "How to craft a Dockerfile for an image with NVIDIA driver/CUDA (support for tensorflow-gpu) and Python3 with pip?": "you can use this :\nFROM nvidia/driver:418.40.04-ubuntu18.04\nRUN apt-get -y update \\\n    && apt-get install -y software-properties-common \\\n    && apt-get -y update \\\n    && add-apt-repository universe\nRUN apt-get -y update\nRUN apt-get -y install python3\nRUN apt-get -y install python3-pip",
    "config alpine's proxy in dockerfile": "It seems like you're required to set http_proxy in your Dockerfile. If you do (e.g. for a specific, temporary reason - say you're building your container behind a corporate proxy) and subsequently don't need it anymore I'd suggest something like the following:\nRUN export \\\n  http_proxy=\"http://some.custom.proxy:8080/\u201d \\\n  https_proxy=\"https://some.custom.proxy:8080/\" \\\n  \\\n  && < E.G. pip install requirements.txt> \\\n  \\\n  && unset http_proxy https_proxy\nYou can also use a more permanent solution in your Dockerfile by invoking ENV, but be aware that these are persisted and can lead to problems further down the road if you push/deploy your images somewhere else - Reference.",
    "How to install gulp on a docker with docker-compose": "You need to run npm install gulp AFTER WORKDIR /app, so that gulp is installed locally in node_modules/gulp. But you already did that and having the same error. It's because in your docker-compose-dev.yml, you are mounting host directory as /app volume inside docker container. So local changes in /app directory is lost when you are running the container.\nYou can either remove volumes from docker-compose-dev.yml or run npm install gulp in host machine.",
    "Docker: Expose a range of ports": "Here is a similar question that caters your requirement as well. Docker expose all ports or range of ports from 7000 to 8000\nTo summarize here, Yes it is possible since Docker version 1.5. You can expose port range when executing docker run command like this:\ndocker run -p 192.168.0.10:8000-9000:8000-9000\nor\ndocker run -p 8000-9000:8000-9000\nI have verified that its working fine on my machine using Docker version 1.6.",
    "Is it possible to add pgvector extension on top of postgres:15.3-alpine images": "This extension of the Alpine docker image worked for me:\n$ cat Dockerfile\nFROM postgres:14.4-alpine AS pgvector-builder\nRUN apk add git\nRUN apk add build-base\nRUN apk add clang\nRUN apk add llvm13-dev\nWORKDIR /home\nRUN git clone --branch v0.4.4 https://github.com/pgvector/pgvector.git\nWORKDIR /home/pgvector\nRUN make\nRUN make install\n\nFROM postgres:14.4-alpine\nCOPY --from=pgvector-builder /usr/local/lib/postgresql/bitcode/vector.index.bc /usr/local/lib/postgresql/bitcode/vector.index.bc\nCOPY --from=pgvector-builder /usr/local/lib/postgresql/vector.so /usr/local/lib/postgresql/vector.so\nCOPY --from=pgvector-builder /usr/local/share/postgresql/extension /usr/local/share/postgresql/extension\n\n$ docker build -t postgres:14.4-alpine-pgvector .",
    "How can I correctly specify the platform for my dockerfile?": "On Mac, I add this line to the .zprofile and that takes care of everything at once. Adding it to the .zprofile, for me, ensures the DOCKER_DEFAULT_PLATFORM is present in the environ on accessing the terminal.\nexport DOCKER_DEFAULT_PLATFORM=linux/amd64",
    "Docker set user password non-interactively": "Instead of using passwd, there is another utility for the: chpasswd. I've resolved this by using the following command in my Dockerfile (after creation of the user):\nRUN echo \"${USER}:pass\" | chpasswd\nworks like a charm!",
    "ModuleNotFoundError and import errors in Docker container": "I've just come across another thread on StackOverflow which seems to have resolved my issue. I can leave the import statements as I indicated above in my question, and by setting the PYTHONPATH in the docker container correctly, I am able to get the imports working correctly in docker.\nHow do you add a path to PYTHONPATH in a Dockerfile\nMy updated (working) Dockerfile is as follows:\nFROM python:3.8.5-alpine\n\nRUN pip install pipenv\nCOPY Pipfile /usr/src/\nWORKDIR /usr/src\nRUN pipenv lock --requirements > requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY app /usr/src/app\n\nENV PYTHONPATH \"${PYTHONPATH}:/usr/src/app\"\n\nCMD [\"python\", \"app/api.py\"]",
    "What causes a cache invalidation when building a Dockerfile?": "Let's focus on your original problem (regarding apt-get update) to make things easier. The following example is not based on any best practices. It just illustrates the point you are trying to understand.\nSuppose you have the following Dockerfile:\nFROM ubuntu:18.04\n\nRUN apt-get update\nRUN apt-get install -y nginx\nYou build a first image using docker build -t myimage:latest .\nWhat happens is:\nThe ubuntu image is pulled if it does not exist\nA layer is created and cached to run apt-get update\nA layer is created an cached to run apt install -y nginx\nNow suppose you modify your Docker file to be\nFROM ubuntu:18.04\n\nRUN apt-get update\nRUN apt-get install -y nginx openssl\nand you run a build again with the same command as before. What happens is:\nThere is already an ubuntu image locally so it will not be pulled (unless your force with --pull)\nA layer was already created with command apt-get update against the existing local image so it uses the cached one\nThe next command has changed so a new layer is created to install nginx and openssl. Since apt database was created in the preceding layer and taken from cache, if a new nginx and/or openssl version was released since then, you will not see them and you will install the outdated ones.\nDoes this help you to grasp the concept of cached layers ?\nIn this particular example, the best handling is to do everything in a single layer making sure you cleanup after yourself:\nFROM ubuntu:18.04\n\nRUN apt-get update  \\\n    && apt-get install -y nginx openssl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*",
    "Dockerfile setup for both production and development?": "A good aproach is to use Docker's multi-stage builds, since they will allow you to build your artifact inside an image that contains your dev dependencies and only use your artifact and runtime dependencies in the final image.\nI'd generally advise against using different Dockerfiles for different environments, this should normally be achieved using configuration parameters (environment variables are a good solution).\nHaving a faster development feedback cycle depends IMO heavily on the used language. Many people will develop using an IDE, in a more classic fashion and only build the image for integration testing and so on (in my experience this is for example often the case for Java developers). Other interpreted languages might indeed profit from mounting your sources into a development environment image. In this case, you might incorperate this image into your multi-stage build.",
    "Where is the docker file located?": "The Dockerfile isn't on your machine. Once the image is built, it exists independently of the Dockerfile, just like a compiled C program doesn't need its source code kept around to function. You can partially recover the Dockerfile via the docker history command, but this won't show you files added with ADD or COPY, and various \"image squishing\" programs can further obfuscate things. The recommended way to get an image's Dockerfile is to go to the repository from which the image was built (hopefully linked from the image's Docker Hub page). If there's no public repository, you're out of luck.",
    "Docker COPY behaving inconsistently": "Generally, COPY instruction copy files and/or directories from \"/app\" and adds them to the container at path \"/www/\". If you want to have a \"app\" directory inside of \"/www/\" then your COPY instruction should looks like:\nCOPY /app /www/app/\n\nYou can read more about COPY instruction in documentation. Here I paste explanation of this behaviour from it:\nIf is a directory, the entire contents of the directory are copied, including filesystem metadata. Note:The directory itself is not copied, just its contents.\n\n\nRegarding to your update:\nCOPY . /target and COPY app /target behaves the same. It takes entire contents from source directory ( from app or from . directory ) and copy it to the /target directory.\n\nYou can see slightly different behaviour when you use wildcards, eg. COPY app/* /www/. It copy all files from app/ to /www/ but then it treat every single directory from app/ like a source and copy its contents to /www/.\n\nWhy in Docker COPY is not implemented in the same way like it is in UNIX's cp command? I don't know, but if you want you can create pull request with own implementation :)",
    "Dockerfile, persist data with VOLUME": "What is not very obvious is that you are creating a brand new container every time you do a \"docker run\". Each new container would then have a fresh volume.\nSo your data is being persisted, but you're not reading the data from the container you wrote it to.\nExample to illustrate the problem\nSample Dockerfile\nFROM ubuntu\nVOLUME /data\nbuilt as normal\n$ docker build . -t myimage\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM ubuntu\n ---> bd3d4369aebc\nStep 2 : VOLUME /data\n ---> Running in db84d80841de\n ---> 7c94335543b8\nNow run it twice\n$ docker run -ti myimage echo hello world\n$ docker run -ti myimage echo hello world\nAnd take a look at the volumes\n$ docker volume ls\nDRIVER              VOLUME NAME\nlocal               078820609d31f814cd5704cf419c3f579af30672411c476c4972a4aad3a3916c\nlocal               cad0604d02467a02f2148a77992b1429bb655dba8137351d392b77a25f30192b\nThe \"docker rm\" command has a special \"-v\" option that will cleanup any volumes associated with containers.\n$ docker rm -v $(docker ps -qa)\nHow to use a data container\nUsing the same docker image, built in the previous example create a container whose sole purpose is to persist data via it's volume\n$ docker create --name mydata myimage\nLaunch another container that saves some data into the \"/data\" volume\n$ docker run -it --rm --volumes-from mydata myimage bash\nroot@a1227abdc212:/# echo hello world > /data/helloworld.txt\nroot@a1227abdc212:/# exit\nLaunch a second container that retrieves the data\n$ docker run -it --rm --volumes-from mydata myimage cat /data/helloworld.txt\nhello world\nCleanup, simply remove the container and specify the \"-v\" option to ensure its volume is cleaned up.\n$ docker rm -v mydata\nNotes:\nThe \"volumes-from\" parameter means all data is saved into the underlying volume associated with the \"mydata\" container\nWhen running the containers the \"rm\" option will ensure they are automatically removed, useful for once-off containers.",
    "Docker: Best practice for development and production environment": "So the way I handle it is I have 2 Docker files (Dockerfile and Dockerfile.dev).\nIn the Dockerfile.dev I have:\nFROM node:6\n\n# Update the repository\nRUN apt-get update\n\n# useful tools if need to ssh in or used by other tools\nRUN apt-get install -y curl net-tools jq\n\n# app location\nENV ROOT /usr/src/app\n\nCOPY package.json /usr/src/app/\n\n# copy over private npm repo access file\nADD .npmrc /usr/src/app/.npmrc\n\n# set working directory\nWORKDIR ${ROOT}\n\n# install packages\nRUN npm install\n\n# copy all other files over\nCOPY . ${ROOT}\n\n# start it up\nCMD [ \"npm\", \"run\", \"start\" ]\n\n# what port should I have\nEXPOSE 3000\nMy NPM scripts look like this\n\"scripts\": {\n    ....\n    \"start\": \"node_modules/.bin/supervisor -e js,json --watch './src/' --no-restart-on error ./index.js\",\n    \"start-production\": \"node index.js\",\n    ....\n},\nYou will notice it uses supervisor for start so any changes to any file under src will cause it to restart the server without requiring a restart to docker.\nLast is the docker compose.\ndev:\n  build: .\n  dockerfile: Dockerfile.dev\n  volumes:\n    - \"./src:/usr/src/app/src\"\n    - \"./node_modules:/usr/src/node_modules\"\n  ports:\n    - \"3000:3000\"\n\nprod:\n  build: .\n  dockerfile: Dockerfile\n  ports:\n    - \"3000:3000\"\nSo you see in a dev mode it loads and mounts the current directory's src folder to the container at /usr/src/app/src and also the node_modules directory to the /usr/src/node_modules.\nThis makes it so that I can make changes locally and save, the volume will update the container's file, then supervisor will see that change and restart the server.\n** Note as it doesn't watch the node_modules folder you have to change another file in the src directory to do the restart **",
    "Shared folder in Docker. With Windows. Not only \"C/user/\" path": "This question and this question have a similar root problem, mounting a non C:/ drive folder in boot2docker. I wrote an in-depth answer to the other question that provide the same information that is in the first half of @VonC's answer.\nFrom Docker Docs:\nAll other paths come from your virtual machine\u2019s filesystem. [...] In the case of VirtualBox you need to make the host folder available as a shared folder in VirtualBox. Then, you can mount it using the Docker -v flag.\nTo get your folder mounted in a container:\nThis mounts your entire D:\\ drive, you can simply change the file paths to be more granular and specific.\nShare the directory with VBox:\nThis only needs to be done once.\nIn windows CMD:\nVBoxManage sharedfolder add \"boot2docker-vm\" --name \"d-share\" --hostpath \"D:\\\"\nMount the shared directory in your VM:\nThis will need to be done each time you restart the VM.\nIn the Boot2Docker VM terminal:\nmount -t vboxsf -o uid=1000,gid=50 d-share /d\nTo see sources and explanation for how this works see my full answer to the other similar question\nAfter this you can use the -v/--volume flag in Docker to mount this folder or any sub-folders or files into containers. If you mounted your whole D:\\ drive you can use that exact docker run command from your question and it should now work. If you mounted a specific part of you drive you will have to change the paths to match.\nTo edit in windows, run in docker:\nAlso from Docker Docs:\nMounting a host directory can be useful for testing. For example, you can mount source code inside a container. Then, change the source code and see its effect on the application in real time.\nAs a VBox shared directory you should be able to see changes made from the Windows side reflected in the boot2docker vm.\nYou may need to restart containers to see the changes actually appear, this depends on how the program running inside the container, in your case ruby, uses the files. If the files are compiled into an app when the container starts, for example, you will definitely need to restart the container to see the changes.\nNote:\nBeware the CR LF vs. LF line ending difference when writing files in Windows and reading them in Linux. Make sure your text editor is saving files with Unix line endings or else you may start to see errors caused by '^M' appended to the end of all your lines.",
    "Installing Google Chrome on Ubuntu via Dockerfile hitting Geographic Area [duplicate]": "I solved that issue by configuring the timezone before installing Chrome\nENV TZ=Europe/Madrid \nRUN echo \"Preparing geographic area ...\"\nRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone",
    "Setting security_opt in docker-compose.yaml doesnt work": "Answering \"How can i check security_opt settings been applied?\"\n$ docker inspect [CONTAINER-NAME] --format '{{ .Id }}: SecurityOpt={{ .HostConfig.SecurityOpt }}'\nTo do it for all running containers:\n$ docker ps --quiet --all | xargs docker inspect --format '{{ .Id }}: SecurityOpt={{ .HostConfig.SecurityOpt }}'",
    "How to copy files from a docker image - dockerfile cmd": "Copying files from the image to the host at build-time is not supported.\nThis can easily be achieved during run-time using volumes.\nHowever, if you really want to work-around this by all means, you can have a look in the custom build outputs documentation, that introduced support for this kind of activity.\n\nHere is a simple example inspired from the official documentation:\nDockerfile\nFROM alpine AS stage-a\nRUN mkdir -p /opt/temp/\nRUN touch /opt/temp/file-created-at-build-time\nRUN echo \"Content added at build-time\" > /opt/temp/file-created-at-build-time\n\nFROM scratch as custom-exporter\nCOPY --from=stage-a /opt/temp/file-created-at-build-time .\nFor this to work, you need to launch the build command using these arguments:\nDOCKER_BUILDKIT=1 docker build --output out .\nThis will create on your host, aside the Dockerfile, a directory out with the file you need:\n.\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 out\n    \u2514\u2500\u2500 file-created-at-build-time\ncat out/file-created-at-build-time \nContent added at build-time",
    "pass external config file to docker container at runtime": "I'd recommend instead of passing application.conf file with overrides, go with overrides based on environment variables, because of reasons like:\nMounting volume with configuration file might be tricky from deployment tools perspective;\nNot all configurations management tools (for instance HashiCorp Consul) provide HOCON support, but managing environment variables is almost a standard. Especially if it contains secrets which needs to be protected;\nSo, you can do next: In your application conf set overrides via environment variable:\nfoo=default\nfoo={?FOO}\nAnd run application docker container with specific override:\ndocker run ...\n -e foo=bar \\\n ...\nPlease, see for more details:\nHOCON Optional system or env variable overrides: https://github.com/lightbend/config#optional-system-or-env-variable-overrides\nRun docker container with environment variables: https://docs.docker.com/engine/reference/run/#env-environment-variables\nHope this helps!",
    "ENTRYPOINT: first run command as root, then start shell for non-root user": "You start your container as root. This runs your entrypoint as root. Perform all the steps you need, then make the last step look like:\nexec gosu username /bin/bash\nTo launch /bin/bash as the user username. You can find gosu in this github repo. It has the advantage of running an su command with an implicit exec which avoids leaving the parent process around which can break signal handling.\nIf you make /bin/bash the value of CMD, you can make this more flexible with:\nexec gosu username \"$@\"\nMake sure to use the JSON syntax for ENTRYPOINT and CMD to avoid issues with the merged commands and cli args.\nThis is preferable over sudo since it avoids any option to go back from the user to root.",
    "docker ERROR: for nginx Cannot start service nginx: driver failed programming external connectivity on": "You can't allocate port 9010 of your host for both services. This is what you're doing in the ports section of declaration of service nginx and web.\nMoreover, by default nginx will listen to port 80 and 443 for https.\nYou can keep it like that and publish to a different port on your host. See how to use port keyword in docker-compose :\nhttps://docs.docker.com/compose/compose-file/#ports\nMaybe you want something more like that:\nversion: '3'\n\nservices:\n  nginx:\n    image: nginx:latest\n    ports:\n      - \"10080:80\"\n      - \"10443:443\"\n    volumes:\n      - .:/app\n      - ./config/nginx:/etc/nginx/conf.d\n      - ./static_cdn:/static\n   depends_on:\n      - web\n\n  web:\n    build: .\n    command: ./start.sh\n    container_name: \"web-app\"\n    volumes:\n      - .:/app\n      - ./static_cdn:/static\n    expose:\n      - \"9010\"\n    depends_on:\n      - db\n\n  db: \n    image: postgres\ncontents of config/nginx/nginx.conf\nupstream web {\n  ip_hash;\n  server web-app:9010;\n}\n\nserver {\n    location /static {\n        autoindex on;\n        alias /static/\n     }\n\nlocation / {\n    proxy_pass http://web;\n}\n\nlisten 80;\nserver_name localhost;\n}\nConcerning your last question, you could go for an official Python image from the Docker hub Python repository or start from any other base image like debian:jessie-slim from Debian official repository or keep the Ubuntu 18.04 image",
    "Validate yaml schema with golang (semantic check)": "Here is what you could try.\nModel a struct after the shape of the expected yaml data:\ntype Config struct {\n        Version struct {\n                Required bool\n        }\n        ID struct {\n                Required bool\n                Pattern string\n        }\n        ReleaseVersion struct {\n                Required bool\n        }\n        Type interface{}\n        Builds struct {\n                Type []interface{} `yaml:\"type\"`\n                Sequence struct {\n                        Type string\n                }\n                Mapping struct {\n                        Name map[string]interface{}\n                        Params struct {\n                                Type string `yaml:\"type\"`\n                                Mapping struct {\n                                        To map[string]string `yaml:\"=\"`\n                                }\n                        }\n                } `yaml:\"mapping\"`              \n        }\n}\nThe yaml flag yaml:\"somefield\" is added to label the field name of the yaml the data we're interested in.\nAlso many fields with unknown/undetermined type can be declared as empty interface (interface{}) or if you want to \"enforce\" that the underlying form is a key-value object you can either declare it as a map[string]interface{} or another struct.\nWe then unmarshal the yaml data into the struct:\ncfg := Config{}\nerr := yaml.Unmarshal([]byte(data), &cfg)\nif err != nil {\n        log.Fatalf(\"error: %v\", err)\n}\nSince we have modeled fields as either anonymous structs or maps, we can test if a particular field has a \"key-value\" value by checking its equality to nil.\n// Mapping is a key value object\nif (Mapping != nil) {\n        // Mapping is a key-value object, since it's not nil.\n}\n\n\n// type any is and key value\n// Mapping.To is declared with map[string]string type\n// so if it's not nil we can say there's a map there.\nif (Mapping.To != nil) {\n        // Mapping.To is a map\n}\nIn marshaling/unmarshaling, maps and structs are pretty interchangeable. The benefit of a struct is you can predefine the field's names ahead of time while unmarshaling to a map it won't be clear to you what the keys are.",
    "Changing /proc/sys/kernel/core_pattern file inside docker container": "The kernel does not support per-container patterns. There is a patch for this, but it is unlikely to go in any time soon. The basic problem is that core patterns support piping to a dedicated process which is spawned for this purpose. But the code spawning it does not know how to handle containers just yet. For some reason a simplified pattern handling which requires a target file was not deemed acceptable.",
    "How do I write a dockerfile to execute a simple bash script?": "You need to make the script part of the container. To do that, you need to copy the script inside using the COPY command in the Docker file, e.g. like this\nFROM ubuntu:14.04\nCOPY run_netcat_webserver.sh /some/path/run_netcat_webserver.sh\nCMD /some/path/run_netcat_webserver.sh\nThe /some/path is a path of your choice inside the container. Since you don't need to care much about users inside the container, it can be even just /.\nAnother option is to provide the script externally, via mounted volume. Example:\nFROM ubuntu:14.04\nVOLUME /scripts\nCMD /scripts/run_netcat_webserver.sh\nThen, when you run the container, you specify what directory will be mounted as /scripts. Let's suppose that your script is in /tmp, then you run the container as\ndocker run --volume=/tmp:/scripts (rest of the command options and arguments)\nThis would cause that your (host) directory /tmp would be mounted under the /scripts directory of the container.",
    "mkdir is not executing in Dockerfile/build": "There's an interesting character between RUN and mkdir in your Dockerfile. Replacing it with a space makes your Dockerfile build.\ndocker build output with yours:\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM ubuntu:14.04\n ---> c4bea91afef3\nStep 2 : RUN\u2002MKDIR\nUnknown instruction: RUN\u2002MKDIR\ndocker build output with fixed:\nSending build context to Docker daemon 2.048 kB\nStep 1 : FROM ubuntu:14.04\n ---> c4bea91afef3\nStep 2 : RUN mkdir -p /home/developer\n ---> Using cache\n ---> 1ac57f7c9ccd\nSuccessfully built 1ac57f7c9ccd",
    "How to write a Dockerfile which I can start a service and run a shell and also accept arguments for the shell?": "You have to use supervisord inside a Docker container able to use more complex shell syntax when you creating containers.\nDocker documentation about supervisord: https://docs.docker.com/engine/articles/using_supervisord/\nYOU CAN use more complex shell syntax (that you want to use) when you create a new container with $ docker run command, however this will not work within systemd service files (due to limitation in systemd) and docker-compose .yml files and the Dockerfiles also.\nFirst, you have to install supervisord in your Dockerfile:\nRUN apt-get -y update && apt-get -y dist-upgrade \\\n    && apt-get -y install \\\n        supervisor\nRUN mkdir -p /var/log/supervisord\nThan place this at the end of the Dockerfile:\nCOPY etc/supervisor/conf.d/supervisord.conf /etc/supervisor/conf.d/\nCMD [\"/usr/bin/supervisord\", \"-c\", \"/etc/supervisor/supervisord.conf\"]\nCreate a file in etc/supervisor/conf.d/supervisord.conf next to your Dockerfile:\n[unix_http_server]\nfile=/var/run/supervisord.sock\nchmod=0777\nchown=root:root\nusername=root\n\n[supervisord]\nnodaemon=true\nuser=root\nenvironment=HOME=\"/root\",USER=\"root\"\nlogfile=/var/log/supervisord/supervisord.log\npidfile=/var/run/supervisord.pid\nchildlogdir=/var/log/supervisord\nlogfile_maxbytes=10MB\nloglevel=info\n\n[program:keepalive]\ncommand=/bin/bash -c 'echo Keep Alive service started... && tail -f /dev/null'\nautostart=true\nautorestart=true\nstdout_events_enabled=true\nstderr_events_enabled=true\nstdout_logfile=/var/log/supervisord/keepalive-stdout.log\nstdout_logfile_maxbytes=1MB\nstderr_logfile=/var/log/supervisord/keepalive-stderr.log\nstderr_logfile_maxbytes=1MB\n\n[program:dcheck]\ncommand=/bin/bash -c 'chmod +x /root/dcheck/repo/dcheck.sh && cd /root/dcheck/repo && ./dcheck.sh'\nautostart=true\nautorestart=true\nstdout_events_enabled=true\nstderr_events_enabled=true\nstdout_logfile=/var/log/supervisord/dcheck-stdout.log\nstdout_logfile_maxbytes=10MB\nstderr_logfile=/var/log/supervisord/dcheck-stderr.log\nstderr_logfile_maxbytes=1MB\nThis is a more complex supervisord.conf and probably you don't need many of the commands here, plus you have to change the file locations to your needs. However you can see how to create log files from the bash output of the script.\nLater on you have to docker exec in that container and you can watch real-time the log with:\ndocker exec -it your_running_container /bin/bash -c 'tail -f /var/log/supervisord/dcheck-stdout.log'\nYou have the option to show subprocess log in the main supervisord log with loglevel=debug, however this is full of timestamps and comments, not the pure bash output like when you run the script directly.\nAs you can see in my scipt, I keeping alive the container with tail -f /dev/null, however this is a bad practice. The .sh script should keep alive your container on their own.\nWhen you sending your scipt to ENTRYPOINT as ENTRYPOINT [\"sudo service docker start\", \"&&\", \"/home/user/che/bin/che.sh run\"], you want to change the default docker ENTRYPOINT from /bin/sh -c to sudo (also, use full location names).\nThere are two ways to change docker ENTRYPOINT in Dockerfile. One is to place this in the head section of your Dockerfile:\nRUN ln -sf /bin/bash /bin/sh && ln -sf /bin/bash /bin/sh.distrib\nOr place this at the bottom:\nENTRYPOINT ['/bin/bash', '-c']\nAfter when you send any CMD to this Dockerfile, it will be run by /bin/bash -c command.\nOne more thing to note is that the first command takes PID1, so if you want to run the .sh script without tail -f /dev/null in my supervisord script, it will take PID1 process place and CTRL+C command will not gonna work. You have to shut down the container from another shell instance.\nBut if you run the command with:\n[program:dcheck]\ncommand=/bin/bash -c 'echo pid1 > /dev/null && chmod +x /root/dcheck/repo/dcheck.sh && cd /root/dcheck/repo && ./dcheck.sh'\necho pid1 > /dev/null will take PID1 and SIGTERM, SIGKILL and SIGINT will work again with your shell script.\nI try to stay away running Docker with --privileged flag. You have many more options to get away on the limitations.\nI don't know anything about your stack, but generally good idea to not dockerise Docker in a Container. Is there a specific reason why sudo service docker start is in your Dockerfile?\nI don't know anything about this container, is it have to be alive? Because if doesn't, there is a more simple solution, only running the container when it has to process something from the command line. Place this file on the host with the name of run let's say in /home/hostuser folder and give it chmod +x run:\n#!/bin/bash\ndocker run --rm -it -v /home/hostuser/your_host_shared_folder/:/root/your_container_shared_folder/:rw your_docker_image \"echo pid1 > /dev/null && chmod +x /root/script.sh && cd  /root && ./script.sh\"\nIn this case, ENTRYPOINT is preferred to be ENTRYPOINT ['/bin/bash', '-c'].\nRun this script on the host with:\n$ cd /home/hostuser\n$ ./run -flag1 -flag2 args1 args2 args3",
    "docker-compose - Expose linked service port": "You don't need to: an EXPOSE port from one service is directly visible from another (linking to the first).\nNo port mapping necessary (as you do for 9000 from SonarQube and 3306)\nPort mapping is necessary for accessing a container from the host.\nBut from container to a (linked) container (both managed by the same docker daemon), any port declared in EXPOSE in its Dockerfile is directly accessible.\nI want to expose both ports to my localhost. I need access to both ports from my machine, as I SonarQube runner needs access to the database\nWell then,... the db section should have its own port mapping section:\ndb:\n  ports:\n    - \"xxx:yyyy\"",
    "If I use EXPOSE $PORT in a Dockerfile, can I un-expose the port it when I use `docker run`?": "Even a couple of years later, the situation hasn't changed much. There is no UNEXPOSE but atleast there is a workaround with \"docker save\" and \"docker load\" allowing to edit the metadata of a docker image. Atleast to remove volumes and ports, I have created a little script that can automate the task, see docker-copyedit.",
    "yaml: line 8:did not find expected key": "It happens when we do our own yml file for docker, you need to indent two spaces for sub entries under image details:\nversion: '1'\nservices:\n    mariadb-ikg:\n      image: bitnami/mariadb:10.3\n      ports:\n        - 3306:3306\n      volumes:\n        - D:/docker/bitnami-mariadb/databases:/bitnami/mariadb\n      environment:\n        - MARIADB_ROOT_PASSWORD=123456\n    phpfpm-ikg:\n      image: wyveo/nginx-php-fpm:php80\n      ports:\n        - 80:80\n      volumes:\n        - D:/docker/wyveo-nginx-php-fpm/wordpress:/usr/share/nginx/html\n      depends_on:\n        - mariadb-ikg",
    "How can I create a Docker container whose timezone matches that of my local machine?": "using docker run command:\n-e TZ=`ls -la /etc/localtime | cut -d/ -f8-9`\nSource\nif you still want to use volumes you need to share /etc form the Docker UI in your MAC \"Prefernces --> Resources --> FILE SHARING\"\nUpdate\nfor docker-compose :\nunder build section use:\nargs:\n  - TZ\nand then:\nenvironment:\n    - TZ=${TZ}\nand then start it like - after re-build -:\nexport TZ=`ls -la /etc/localtime | cut -d/ -f8-9` && docker-compose up -d --build",
    "Is there a way to create Docker volume and prepopulate it with data?": "If you want to populate a volume with data before using it, you can first create it:\ndocker volume create myapplicationdata\nAnd then attach it to an ephemeral container in order to populate it with data:\ntar -C /path/to/my/files -c -f- . | docker run --rm -i -v myapplicationdata:/data alpine tar -C /data -xv -f-\nTo use a pre-existing volume in your docker-compose.yml, declare it as an external volume:\nversion: \"3\"\n\nservices:\n  myapplication:\n    image: image-name\n    container_name: container-name\n    volumes:\n      - myapplicationdata:/app/data\n    ports:\n      - \"3000:3000\"\n\nvolumes:\n  myapplicationdata:\n    external: true",
    "Installing homebrew packages during Docker build": "You have to set the PATH environment variable in the Dockerfile with:\nENV PATH=~/.linuxbrew/bin:~/.linuxbrew/sbin:$PATH\nHere is a complete working Dockerfile:\nFROM debian\nRUN apt-get update && apt-get install -y git curl binutils clang make\nRUN git clone https://github.com/Homebrew/brew ~/.linuxbrew/Homebrew \\\n&& mkdir ~/.linuxbrew/bin \\\n&& ln -s ../Homebrew/bin/brew ~/.linuxbrew/bin \\\n&& eval $(~/.linuxbrew/bin/brew shellenv) \\\n&& brew --version \\\n&& brew tap aws/tap && brew install aws-sam-cli \\\n&& sam --version\nENV PATH=~/.linuxbrew/bin:~/.linuxbrew/sbin:$PATH",
    "Dockerfile - Docker directive to switch home directory": "You can change user directory using WORKDIR in the dockerfile, this will become the working directory. So whenever you created the container the working directory will be the one that is pass to WORKDIR instruction in Dockerfile.\nWORKDIR\nDockerfile reference for the WORKDIR instruction\nFor clarity and reliability, you should always use absolute paths for your WORKDIR. Also, you should use WORKDIR instead of proliferating instructions like RUN cd \u2026 && do-something, which are hard to read, troubleshoot, and maintain.\nFROM buildpack-deps:buster\n\nRUN groupadd -r someteam --gid=1280 && useradd -r -g someteam --uid=1280 --create-home --shell /bin/bash someteam\n\n# Update and allow for apt over HTTPS\nRUN apt-get update && \\\n  apt-get install -y apt-utils\nRUN apt-get install -y apt-transport-https\nRUN apt update -y \nRUN apt install python3-pip -y\n\n# switch user from 'root' to \u2018someteam\u2019 and also to the home directory that it owns \nUSER someteam\nWORKDIR /home/someteam\nusing $HOME will cause error.\nWhen you use the USER directive, it affects the userid used to start new commands inside the container.Your best bet is to either set ENV HOME /home/aptly in your Dockerfile, which will work dockerfile-home-is-not-working-with-add-copy-instructions",
    "How to use local nuget package sources for Dockerfile dotnet restore [duplicate]": "To have all packages ready you need restore before building. To have all packages during the build you need to copy the packages.\nHere is an example in form of an experiment:\nPreparation:\nHave the sdk ready: docker pull microsoft/dotnet:2.2-sdk.\nHave src/src.csproj ready:\n<Project Sdk=\"Microsoft.NET.Sdk\">\n  <PropertyGroup>\n    <TargetFramework>netstandard2.0</TargetFramework>\n  </PropertyGroup>\n  <ItemGroup>\n    <PackageReference Include=\"Newtonsoft.Json\" Version=\"12.0.2\" />\n  </ItemGroup>\n</Project>\nHave src/Dockerfile ready:\nFROM microsoft/dotnet:2.2-sdk AS byse\nCOPY packages /root/.nuget/packages\nCOPY src src\nRUN ls /root/.nuget/packages\nWORKDIR /src\nRUN dotnet restore\nRUN ls /root/.nuget/packages\nExecution:\nRestore the Packages:\ndocker run --rm -v $(pwd)/src:/src -v $(pwd)/packages:/root/.nuget/packages -w /src  microsoft/dotnet:2.2-sdk dotnet restore\nBuild the Image:\ndocker build -t test -f src/Dockerfile .\nExpectation:\nSending build context to Docker daemon  13.77MB\nStep 1/7 : FROM microsoft/dotnet:2.2-sdk AS byse\n ---> e4747ec2aaff\nStep 2/7 : COPY packages /root/.nuget/packages\n ---> 76c3e9869bb4\nStep 3/7 : COPY src src\n ---> f0d3f8d9af0a\nStep 4/7 : RUN ls /root/.nuget/packages\n ---> Running in 8323a9ba8cc6\nnewtonsoft.json\nRemoving intermediate container 8323a9ba8cc6\n ---> d90056004474\nStep 5/7 : WORKDIR /src\n ---> Running in f879d52f81a7\nRemoving intermediate container f879d52f81a7\n ---> 4020c789c338\nStep 6/7 : RUN dotnet restore\n ---> Running in ab62a031ce8a\n  Restore completed in 44.28 ms for /src/src.csproj.\nRemoving intermediate container ab62a031ce8a\n ---> 2cd0c01fc25d\nStep 7/7 : RUN ls /root/.nuget/packages\n ---> Running in 1ab3310e2f4c\nnewtonsoft.json\nRemoving intermediate container 1ab3310e2f4c\n ---> 977e59f0eb10\nSuccessfully built 977e59f0eb10\nSuccessfully tagged test:latest\nNote that the ls steps are cached and would not print on a subsequent call. Run docker rmi test to reset.\nStep 4/7 runs before the restore and the packages are already cached.\nStep 4/7 : RUN ls /root/.nuget/packages\n ---> Running in 8323a9ba8cc6\nnewtonsoft.json\nThis can solves excessive restore times for example during automated builds.\nTo solve your network issue you can try to mount the network patch instead of the local path during the resolve step or robocopy files from your corp network into a local cache first.",
    "How to specify in Dockerfile that the image is interactive?": "Many of the docker run options can only be specified at the command line or via higher-level wrappers (shell scripts, Docker Compose, Kubernetes, &c.). Along with port mappings and network settings, the \u201cinteractive\u201d and \u201ctty\u201d options can only be set at run time, and you can\u2019t force these in the Dockerfile.",
    "Docker container/image running but there is no port number": "This line from the question helps reveal the problem;\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\n01cc8173abfa        test_1              \"python manage.py ru\u2026\"   13 seconds ago      Exited (1) 11 seconds ago                       testing_first\nExited (1) (from the STATUS column) means that the main process has already exited with a status code of 1 - usually meaning an error. This would have freed up the ports, as the docker container stops running when the main process finishes for any reason.\nYou need to view the logs in order to diagnose why.\ndocker logs 01cc will show the logs of the docker container that has the ID starting with 01cc. You should find that reading these will help you on your way. Knowing this command will help you immensely in debugging weirdness in docker, whether the container is running or stopped.\nAn alternative 'quick' way is to drop the -d in your run command. This will make your container run inline rather than as a daemon.",
    "COPY package.json - Dockerfile": "Do not ever run nodemon in production (if that's what you tried to do). You should configure your restart in case if app crashes. Preferably, set it to always in docker-compose.yml\nThe best way to structure Dockerfile in your case:\nFROM node:latest\nWORKDIR ./app\n# please note, you already declared a WORKDIR, \n# therefore your files will be automaticaly pushed to ./app\nCOPY package.json ./\nRUN npm install -g\nCOPY ./ ./ \nEXPOSE 3000\nCMD [\"npm\", \"start\"]\nHope, that helps.",
    "apt-get commands doesn't work in docker containers": "If apt-get is working fine on the VM, you can build the container using network host mode.\ndocker build --network=host ...",
    "Docker Caching, how does it really work?": "Yes, the two images will share the same layers if you meet the prerequisites. Docker layers are reused independently of the resulting image name. The requirements to use a cached layer instead of creating a new one are:\nThe build command needs to be run against the same docker host where the previous image's cache exists.\nThe previous layer ID must match between the cache layer and the running build step.\nThe command currently being run, or the source context if you are running a COPY or ADD, must be identical. Docker does not know if you are running a command that pulls from an external changing resource (e.g. git clone or apt-get update), which can result in a false cache hit.\nYou cannot have disabled caching in your build command.\nKeep in mind that layers are immutable, once created they are never changed, just replaced with different layers with new ID's when you run a different build. When you run a container, it uses a copy-on-write RW layer specific to that container, which allows multiple containers and images to point to the same image layers when they get a cache hit.\nIf you are having problems getting the cache to match in the two builds, e.g. importing a large file and something like the file timestamp doesn't match, consider creating an intermediate image that contains the common files. Then each project can build FROM that intermediate image.",
    "Is it possible to pause a Docker image build?": "Is it possible to pause a Docker image\nno, you cannot pause the docker build command.\nYou could give a try to the Scroll Lock key, but depending on your terminal that might fail.\nYou could pipe the result of the docker build command to less -R:\ndocker build -t test . | less -R\nOnce built, you can then use the arrow keys to go up and down, use / to search for test, etc.\n-R is to keep colors\n-r  -R  ....  --raw-control-chars  --RAW-CONTROL-\n                Output \"raw\" control characters.\nAlso you can record the output to a file (I know you explicitly said you don't want this solution, but it can suit others):\ndocker build -t test . | tee build.log",
    "Failed to calculate checksum of ref [duplicate]": "All COPY commands must refer to files available in the build context, which is the directory containing your Dockerfile.\nTherefore, you should ensure that all necessary files are placed in the Dockerfile directory so that the COPY instructions in the Dockerfile can locate the source files and copy them into the Docker image.",
    "How to create tun interface inside Docker container image?": "The /dev directory is special, and Docker build steps cannot really put anything there. That also is mentioned in an answer to question 56346114.\nApparently a device in /dev isn't a file with data in it, but a placeholder, an address, a pointer, a link to driver code in memory that does something when accessed. Such driver code in memory is not something that a Docker image would hold.\nI got device creation working in a container by putting your command line code in an .sh script wrapping the app we really want to run.",
    "Can docker image choose the OS?": "Docker is composed of layers. At the beginning of any Dockerfile you specify the OS by typing e.g. FROM python:3 My belief is that if you were to add another OS. The image would retain the environment from the first OS and install the env of the second OS over that. So essentially, your image would have both environments.\nIf you create a python image from the command above and name it docker build -t 'this_python' . then make a new Dockerfile with the first line: FROM this_python so the new image has python already, and you can install anything over this.\nBest practice is to keep your docker image as small as possible. Install only what is required.\nA quick example\nFROM python:3\nFROM ubuntu:latest\n\nRUN apt-get update\nThe above Dockerfile gives you an image with Python and Ubuntu installed. But this is not how you should do it. Better is to use FROM ubuntu:latest and then install python over it.",
    "Making an Oracle JDK docker image": "Here's an Atlassian blog post that describes how to build an Oracle JDK docker container.\nAnd here is an existing Oracle JDK image on docker hub, and another.\nNote the license disclaimers in the READMEs, though. I'm not sure that is enough to bypass the \"Limitations on Redistribution\" clause in the Oracle JDK License. Probably safer to use their Dockerfiles and the blog as a basis to create your own. Just put it in a private repo rather than on docker hub.\nFor completeness, here is the Dockerfile content from the blog post:\n# AlpineLinux with a glibc-2.21 and Oracle Java 8\n\nFROM alpine:3.2\nMAINTAINER Anastas Dancha [...]\n\n# Install cURL\nRUN apk --update add curl ca-certificates tar && \\\n    curl -Ls https://circle-artifacts.com/gh/andyshinn/alpine-pkg-glibc/6/artifacts/0/home/ubuntu/alpine-pkg-glibc/packages/x86_64/glibc-2.21-r2.apk > /tmp/glibc-2.21-r2.apk && \\\n    apk add --allow-untrusted /tmp/glibc-2.21-r2.apk\n\n# Java Version\nENV JAVA_VERSION_MAJOR 8\nENV JAVA_VERSION_MINOR 45\nENV JAVA_VERSION_BUILD 14\nENV JAVA_PACKAGE       jdk\n\n# Download and unarchive Java\nRUN mkdir /opt && curl -jksSLH \"Cookie: oraclelicense=accept-securebackup-cookie\"\\\n  http://download.oracle.com/otn-pub/java/jdk/${JAVA_VERSION_MAJOR}u${JAVA_VERSION_MINOR}-b${JAVA_VERSION_BUILD}/${JAVA_PACKAGE}-${JAVA_VERSION_MAJOR}u${JAVA_VERSION_MINOR}-linux-x64.tar.gz \\\n    | tar -xzf - -C /opt &&\\\n    ln -s /opt/jdk1.${JAVA_VERSION_MAJOR}.0_${JAVA_VERSION_MINOR} /opt/jdk &&\\\n    rm -rf /opt/jdk/*src.zip \\\n           /opt/jdk/lib/missioncontrol \\\n           /opt/jdk/lib/visualvm \\\n           /opt/jdk/lib/*javafx* \\\n           /opt/jdk/jre/lib/plugin.jar \\\n           /opt/jdk/jre/lib/ext/jfxrt.jar \\\n           /opt/jdk/jre/bin/javaws \\\n           /opt/jdk/jre/lib/javaws.jar \\\n           /opt/jdk/jre/lib/desktop \\\n           /opt/jdk/jre/plugin \\\n           /opt/jdk/jre/lib/deploy* \\\n           /opt/jdk/jre/lib/*javafx* \\\n           /opt/jdk/jre/lib/*jfx* \\\n           /opt/jdk/jre/lib/amd64/libdecora_sse.so \\\n           /opt/jdk/jre/lib/amd64/libprism_*.so \\\n           /opt/jdk/jre/lib/amd64/libfxplugins.so \\\n           /opt/jdk/jre/lib/amd64/libglass.so \\\n           /opt/jdk/jre/lib/amd64/libgstreamer-lite.so \\\n           /opt/jdk/jre/lib/amd64/libjavafx*.so \\\n           /opt/jdk/jre/lib/amd64/libjfx*.so\n\n# Set environment\nENV JAVA_HOME /opt/jdk\nENV PATH ${PATH}:${JAVA_HOME}/bin",
    "Docker tomcat editing configuration files through dockerfile": "I figured it out. The problem was that the user with which I was executing the build tasks did not have sufficient rights to write stuff in those folders. What I did was add a USER root task to the Dockerfile so it executes all the commands as root.\nMy Dockerfile now looks like this:\nFROM tomcat\nUSER root\nCOPY tomcat-users.xml /usr/local/tomcat/conf/\nCOPY context.xml /usr/local/tomcat/webapps/manager/META-INF/\nCMD [\"catalina.sh\",\"run\"]",
    "How to have two JARs start automatically on \"docker run container\"": "The second CMD instruction replaces the first, so you need to use a single instruction for both commands.\nEasy (not so good) Approach\nYou could add a bash script that executes both commands and blocks on the second one:\n# start.sh\n/usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar first.jar &\n/usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar second.jar\nThen change your Dockerfile to this:\n# base image is java:8 (ubuntu)\nFROM java:8\n\n# add files to image \nADD first.jar .\nADD second.jar .\nADD start.sh .\n\n# start on run\nCMD [\"bash\", \"start.sh\"]\nWhen using docker stop it might not shut down properly, see: https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/\nBetter Approach\nTo solve this, you could use Phusion: https://hub.docker.com/r/phusion/baseimage/\nIt has an init-system that is much easier to use than e.g. supervisord.\nHere is a good starting point: https://github.com/phusion/baseimage-docker#getting_started\nInstructions for using phusion\nSadly there is not official openjdk-8-jdk available for Ubuntu 14.04 LTS. You could try with an inofficial ppa, which is used in the following explanation.\nIn your case you would need to bash scripts (which act like \"services\"):\n# start-first.sh (the file has to start with the following line!):\n#!/bin/bash\nusr/lib/jvm/java-8-openjdk-amd64/bin/java -jar /root/first.jar\n\n# start-second.sh\n#!/bin/bash\nusr/lib/jvm/java-8-openjdk-amd64/bin/java -jar /root/second.jar\nAnd your Dockerfile would look like this:\n# base image is phusion\nFROM phusion/baseimage:latest\n\n# Use init service of phusion\nCMD [\"/sbin/my_init\"]\n\n# Install unofficial openjdk-8\nRUN add-apt-repository ppa:openjdk-r/ppa && apt-get update && apt-get dist-upgrade -y && apt-get install -y openjdk-8-jdk\n\nADD first.jar /root/first.jar\nADD second.jar /root/second.jar\n\n# Add first service\nRUN mkdir /etc/service/first\nADD start-first.sh /etc/service/first/run\nRUN chmod +x /etc/service/first/run\n\n# Add second service\nRUN mkdir /etc/service/second\nADD start-second.sh /etc/service/second/run\nRUN chmod +x /etc/service/second/run\n\n# Clean up\nRUN apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\nThis should install two services which will be run on startup and shut down properly when using docker stop.",
    "How to make Dockerfile COPY work for windows absolute paths?": "All the resources need to be in the dir that you run the build, i.e. where your Dockerfile is. You cant use an absolute path from elsewhere, think of it from the build perspective - where is the build happening - in the Dockerfile? It can run commands but those resources need to be public.\nhttps://github.com/moby/moby/issues/4592\nhttps://github.com/docker/compose/issues/4857",
    "using nuget cache inside a docker build with .net core when offline": "One suggestion I have to look into Docker BuildKit if you have not done so already. BuildKit adds support for Dockerfile mounts. It supports various types of mounts one being a cache intended for this exact scenario - build cache artifacts such as NuGet packages.",
    "\"Application startup exception: System.Net.Sockets.SocketException (111): Connection refused\" with docker-compose": "You can call services by it's name, 127.0.0.0.1 or localhost is the client container itself. Try changing the connection string to below -\n\"ConnectionStrings\": {\n    \"DbContext\": \"@Host=postgres;Port=5432;Database=kitchen_table;UserID=<user>Password=<password>;\"\n  }",
    "docker host port and container port": "3306/tcp -> 127.0.0.1:3666 means port 3306 inside container is exposed on to port 3666 of host.\nMore info here.\nIf you think output of docker port command is confusing then use docker inspect command to retrieve port mapping. As mentioned here in official doc.\ndocker ps docker port docker inspect are useful commands to get the info about port mapping.\n[user@jumphost ~]$ docker run -itd -p 3666:3306 alpine sh\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\n050382585609: Pull complete \nDigest: sha256:6a92cd1fcdc8d8cdec60f33dda4db2cb1fcdcacf3410a8e05b3741f44a9b5998\nStatus: Downloaded newer image for alpine:latest\n428c80bfca4e60e474f82fc5fe9c1c0963ff2a2f878a70799dc5da5cb232f27a\n[user@jumphost ~]$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES\n428c80bfca4e        alpine              \"sh\"                3 seconds ago       Up 3 seconds        0.0.0.0:3666->3306/tcp   fervent_poitras\n[user@jumphost ~]$ docker port 428c80bfca4e\n3306/tcp -> 0.0.0.0:3666\n[user@jumphost ~]$ docker inspect --format='{{range $p, $conf := .NetworkSettings.Ports}} {{$p}} -> {{(index $conf 0).HostPort}} {{end}}' 428c80bfca4e\n 3306/tcp -> 3666 \n[user@jumphost ~]$\ndocker inspect comtainer-id also gives a clear mapping of the ports.\n$ docker inspect 428c80bfca4e\n     |\n     |\n\"Ports\": {\n                \"3306/tcp\": [\n                    {\n                        \"HostIp\": \"0.0.0.0\",\n                        \"HostPort\": \"3666\"\n                    }\n                ]\n            },\n     |\n     |\nHope this helps.",
    "Dockerfile entrypoint unable to switch user": "I think that your su command should be something like\nsu $USERNAME --command \"/doit.sh\"\nb/c your entrpoiny script is switching user, doing nothing, and then switching back to root.",
    "Docker, copy files in production and use volume in development": "My problem is that with only one dockerfile, for the development command a volume will be mounted at /app and also files copied with ADD . /app. I haven't tested what happens in this scenario, but I am assuming it is incorrect to have both for the same destination.\nFor this scenario, it will do as follows:\na) Add your code in host server to app folder in container when docker build.\nb) Mount your local app to the folder in the container when docker run, here will always your latest develop code.\nBut it will override the contents which you added in dockerfile, so this could meet your requirements. You should try it, no need for any complex solution.",
    "How to fix permissions for an Alpine image writing files using Cron as non root user into accessible volume": "crond or cron should be used as root, as described in this answer.\nCheck out instead aptible/supercronic, a crontab-compatible job runner, designed specifically to run in containers. It will accomodate any user you have created.",
    "What does each sha mean in a docker image": "Those are digests of image layers. The same image might be tagged with different names. But the SHA256 digest is a unique and immutable identifier you can reference.\nIf you pull an image specifying the digest, you have a guarantee that the image you\u2019re using is always the same.\nThe more details can be found in docs, here: Pull an image by digest (immutable identifier) and here: Understand images, containers, and storage drivers",
    "How-to: Dockerfile layer(s) reuse / reference": "Based on further reading:\nThere is no INCLUDE like feature currently. https://github.com/docker/docker/issues/735\nBest practices encourage to use \"RUN apt-get update && apt-get install -y\" for all package installation. But it doesn't mean that you can not use that same technique to separate package installs (e.g. package-foo and package-bar) due to maintainability. It is a tradeoff with minimizing the number of layers. https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#/minimize-the-number-of-layers (see also how the build cache operates, identifying it as different layers)\nThank you @Sri for some lead pointers.",
    "Docker: save - produces no output": "Since you are new to docker I'd like to suggest some things that might make your life easier:\nI committed the container which resulted in a 15GB image.\ncommit is generally not recommended for standard workflows. It is not really reproducible. I would suggest creating a Dockerfile using FROM <oracle image> and doing the work in the Dockerfile.\nIf you have large datasets you probably want to manage those in volumes or a host bind mount (aka volume mount).\nI need to upload this onto a server,\nThe recommended way of doing this is using docker push to push to a registry.\nit just hangs and produces no output.\nIt could be that it's just very slow? You can also check the docker daemon log file to see if there are any warnings.\nHow to restore or attach the ENV/CMD metadata (..the dockerfile?) to a exported/imported image?\nI don't think this is possible.",
    "From Golang, how to build/run a docker-compose project?": "I'm a complete Go noob, so take this with a grain of salt, but I was looking through the Docker compose implementation of their up command - they use Cobra for their cli tooling (which seems quite common), and Viper for argument parsing. The link provided is to the Cobra command implementation, which is different from the up internals, which the command uses.\nIf you were to want to add a command which would invoke the docker compose \"up\" command as part of your golang command (which is what I think you're going for - it's what I was looking to try to do), I think you'd have to accept the fact that you'd have to basically implement your own versions of the Cobra commands they have there, replicating their existing logic. That was my take.",
    "Docker tini no such file or direcory": "I updated your COPY and ENTRYPOINT commands.\nTry the following Dockerfile:\nFROM ubuntu:latest\n\nWORKDIR /vault\n\nCOPY ./run.sh ./run.sh\nCOPY ./docker-entrypoint.sh ./docker-entrypoint.sh\nCOPY ./config/local.json ./config/local.json\nCOPY ./logs ./logs\nCOPY ./file ./file\n\nENV TINI_VERSION v0.19.0\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini\nRUN chmod +x /tini\nRUN chmod +x run.sh\nRUN chmod 777 docker-entrypoint.sh\n\nRUN apt-get update && apt-get install -y software-properties-common curl gnupg2 && \\\n  curl -fsSL https://apt.releases.hashicorp.com/gpg | apt-key add - && \\\n  apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" && \\\n  apt-get update && apt-get install -y \\\n  vault bash && \\\n  setcap cap_ipc_lock= /usr/bin/vault\n\nENTRYPOINT [\"/tini\", \"--\", \"bash\", \"/vault/docker-entrypoint.sh\"]",
    "I Can't build my docker file to create my docker image using Docker build": "It's hard to be a 100% sure of how to guide you with the information provided, but it looks like you may have directly provided the file path instead of the folder path (you can read more in the doc):\n# this is used to build a Dockerfile file inside directory /PATH/TO/FOLDER\ndocker build /PATH/TO/FOLDER\nIf you want to give the Dockerfile path instead of the folder path, you may need to do (again read the doc for more):\n# this is used to build a Dockerfile file given its content\ndocker build - < /PATH/TO/FOLDER/Dockerfile\n(note that there a more subtleties because in the first case you build with a context and not in the second, I let you read Docker's doc which is fairly good).",
    ".NET Core Windows authentication in docker container": "By nature, your container is isolated and does not belong to your domain, which makes Windows Authentication a well known issue. The way to achieve this is by using a technology Microsoft introduced recently called gMSA, https://learn.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-R2-and-2012/hh831782(v=ws.11)\nAbout how to use it with Docker: https://www.axians-infoma.de/techblog/windows-authentication-in-docker-containers-just-got-a-lot-easier/ https://artisticcheese.wordpress.com/2017/09/09/enabling-integrated-windows-authentication-in-windows-docker-container/\nPer Mark request, you could also use a piece of code using LDAP:\nprivate bool VerifyServerCertificateCallback(LdapConnection connection, X509Certificate certificate)\n{\n    return new X509Certificate2(certificate).Verify();\n}\n\npublic bool ValidateCredentials(string userName, string password) \n{\n    try\n    {\n        var ldapDirectoryIdentifier = new ldapDirectoryIdentifier(ldapServer.ServerAddress);\n\n        var ldapConnection = new LdapConnection(ldapDirectoryIdentifier)\n        {\n            AuthType = AuthType.Basic\n        };\n        ldapConnection.SessionOptions.ProtocolVersion = 3;\n        ldapConnection.SessionOptions.SecureSocketLayer = true;\n        ldapConnection.SessionOptions.VerifyServerCertificate = VerifyServerCertificateCallback;\n\n        ldapConnection.Bind(new NetworkCredential(string.Format(ldapServer.UserLocation, userName), password));\n\n        ldapConnection.Dispose();\n    }\n    catch (Exception exception) {\n        continue;\n    }\n    return true;\n}\nAnd in your controller:\nif (ValidateCredentials(username, password))\n{\n    ClaimsPrincipal principal = new ClaimsPrincipal(new ClaimsIdentity(\n        new List<Claim>()\n        {\n            new Claim(ClaimTypes.Name, username),\n            //...\n        },\n        /*\"...\"*/));\n\n    await HttpContext.SignInAsync(AuthSchemeName, principal);\n}",
    ".net core build error when building docker image missing Newtonsoft.Json": "In my case, When used JsonProperty in a class,\nVisual Studio intellicode auto-filled,\nusing Newtonsoft.Json;\nThen during docker build,\nwarning MSB3245: Could not resolve this reference. Could not locate the assembly \"Newtonsoft.Json\". Check to make sure the assembly exists on disk.\nWhen I checked, found out that the Visual Studio added an Assembly Reference to Newtonsoft.Json (you can find this by expanding Dependencies node in the solution explorer in Visual Studio). And I was using Linux images.\nSo in order to solve this, I removed the Assembly Reference, and added nuget package Newtonsoft.Json, then the docker build was successful.",
    "How to speed up the COPY from one image to other in Dockerfile": "It is taking time because the number of files are large . If you can compress the data folder as tar and then copy and extract will be helpful in your situation.\nOtherwise If you can take this step to running containers it will be very fast. As per your requirement you need to copy an image of your application that is created already in another image. You can use volume sharing functionality that will share a volume in between 2 or more docker containers.\nCreate 1st container:\ndocker run -ti --name=Container -v datavolume:/datavolume ubuntu\n2nd container:\ndocker run -ti --name=Container2 --volumes-from Container ubuntu\nOr you can use -v option , so with v option create your 1st and 2nd container as:\ndocker run -v docker-volume:/data-volume --name centos-latest -it centos\n\ndocker run -v docker-volume:/data-volume --name centos-latest1 -it centos\nThis will create and share same volume folder that is data-volume in both the containers. docker-volume is the volume name and data-volume is folder name in that container that will be pointing to docker-volume volume Same way you can share a volume with more than 2 containers.",
    "Modify docker image in portainer": "Portainer do not allow you to edit an image from a Dockerfile as it does not store the Dockerfile.\nI'd recommend to version your Dockerfile in a CVS, this would allow to version any changes to your Dockerfile, and then update your image via the upload method inside Portainer when needed.",
    "No route to host within docker container": "It seems it's the container having connectivity issues so your proposed solution is likely to not work, as that is only mapping a host port to a container port (considering your target URL is not the actual host).\nCheck out https://docs.docker.com/compose/compose-file/#network_mode and try setting it to host.",
    "How to connect frontend to backend via docker-compose networks": "In fact your traffic is as next:\nUser browser request page from angular container, then all pages will rendered to user's browser.\nThe front javascript code using angular HttpClient to fetch the data from express container.\nAt that time, although docker-compose setup a customized network for you which afford auto-dns to resolve angular & express, but this dns just works among containers, not effect for host.\nBut, your augular HttpClient which call http://express was happened on user's browser which not in container, so the auto-dns defintly not work for you to resolve the name express.\nFor you, if you just want to open browser from your docker host, you can use localhost, but if you also want the user from other machine to visit your web, you had to use the ip of your dockerhost.\nThen, in angular HttpClient you need to use something like http://your_dockerhost_ip:8000 to visit express container.\nIf interested, you can visit this to see User-defined bridges provide automatic DNS resolution between containers.",
    "Nuget command fails in docker file but not when run manually": "I tried it and got it working like this:\nIf you change the content of your Dockerfile like this:\nFROM sixeyed/msbuild:netfx-4.5.2-webdeploy AS build-agent\nSHELL [\"powershell\"]\nRUN [\"nuget\", \"sources\" , \"Add\", \"-Name\", \"\\\"Common Source\\\"\", \"-Source\" ,\"http://CompanyPackages/nuget/common\"]\nAnd when you then run docker build -t yourImageName .\nYou end up with this:\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM sixeyed/msbuild:netfx-4.5.2-webdeploy AS build-agent\n ---> 0851a5b495a3\nStep 2/3 : SHELL [\"powershell\"]\n ---> Running in 10082a1c45f8\nRemoving intermediate container 10082a1c45f8\n ---> c00b912c6a8f\nStep 3/3 : RUN [\"nuget\", \"sources\" , \"Add\", \"-Name\", \"\\\"Common Source\\\"\", \"-Source\" ,\"http://CompanyPackages/nuget/common\"]\n ---> Running in 797b6c055928\nPackage Source with Name: \"Common Source\" added successfully.\nRemoving intermediate container 797b6c055928\n ---> 592db3be9f8b\nSuccessfully built 592db3be9f8b\nSuccessfully tagged nuget-tester:latest",
    "Why use label in docker-compose.yml, can't environment do the job?": "Another reason to use labels in docker-compose is to flag your containers as part of this docker-compose suite of containers, as opposed to other purposes each docker image might get used for.\nHere's an example docker-compose.yml that shares labels across two services:\nx-common-labels: &common-labels\n  my.project.environment: \"my project\"\n  my.project.maintainer: \"me@example.com\"\n\nservices:\n  s1:\n    image: somebodyelse/someimage\n    labels:\n      <<: *common-labels\n    # ...\n  s2:\n    build:\n      context: .\n    image: my/s2\n    labels:\n    <<: *common-labels\n    # ...\nThen you can do things like this to just kill this project's containers.\ndocker rm -f $(docker container ls --format \"{{.ID}}\" --filter \"label=my.project.environment\")\nre: labels vs. environment variables\nLabels are only available to the docker and docker-compose commands on your host.\nEnvironment variables are also available at run-time inside the docker container.",
    "Persisting MySQL data in Docker": "The problem:\nWhat's going on here is that you create an empty database while the Docker image is being built:\nmysql_install_db --user=mysql --verbose=1 --basedir=/usr --datadir=/var/lib/mysql --rpm > /dev/null && \\\nBut when you create a container from this image, you mount a volume on the /var/lib/mysql folder. This hides your containers data, to expose your host's folder. Thus, you're seeing an empty folder.\nThe solution:\nIf you take a look at https://hub.docker.com/_/mysql/, you'll see that this problem is addressed by creating the database when the container actually starts, not when the image is created. This answers both your questions:\nIf you start your container with your mounted volume, then the database init will be executed afterwards, and your data will actually be written in your host's FS\nYou have to pass those information as env variables\nIn other words, init your DB with a script in an ENTRYPOINT, rather than directly in the image.\nWarnings:\nSome warnings, though. The way you did your image is not really recommended, because Docker's philosophy is \"one process per container\". The difficulty you have here is that you'll have to start multiple services on the same container (apache, Mysql, etc.), so you may have to do things on both in your entrypoint, which is confusing. Also, is one service fails, your container will still be up, but not working as expected.\nI then would suggest to split what you did in as 1 image per process, then either start them all with raw Docker, or user something like docker-compose.\nAlso, this will benefit you in the way that you'll be able to use already existing and highly configurable images, from the Docker Hub : https://hub.docker.com. Less job for you and less error prone.",
    "How to run command during Docker build which requires a tty?": "Short answer : You can't do it straightly because docker build or either buildx didn't implement [/dev/tty, /dev/console]. But there is a hacky solution where you can achieve what you need but I highly discourage using it since it break the concept of CI. That's why docker didn't implement it.\nHacky solution\nFROM ubuntu:14.04\nRUN echo yes | read  #tty requirement command\nAs mentioned in docker reference document the RUN consist of two stage, first is execution of command and the second is commit to the image as a new layer. So you can do the stages manually on your own where we will provide tty to first stage(execution) and then commit the result.\nCode:\n  cd\n  cat >> tty_wrapper.sh << EOF\n  echo yes | read  ## Your command which needs tty\n  rm /home/tty_wrapper.sh \n  EOF\n  docker run --interactive --tty --detach --privileged --name name1 ubuntu:14.04\n  docker cp tty_wrapper.sh name1:/home/\n  docker exec name1 bash -c \"cd /home && chmod +x tty_wrapper.sh && ./tty_wrapper.sh \"\n  docker commit name1 your:tag\nYour new image is ready. Here is a description about the code. At first we make a bash script which wrap our tty to it and then remove itself after fist execute. Then we run a container with provided tty option(you can remove privileged if you don't need). Next step we copy wrapped bash script inside container and do the execution & commit stage on our own.",
    "Reuse steps in docker build image. Cache?": "Source\nYou need to explicitly specify the image that you want to refer Docker for caching. Try this:\ndocker build --cache-from my-registry/my-docker-cache:latest -t new_img .",
    "Docker Compose build command using Cache and not picking up changed files while copying to docker": "You can force the build to ignore the cache by adding on the --no-cache option to the docker-compose build",
    "Getting reproducible docker layers on different hosts": "After some extra googling, I found a great post describing solution to this problem.\nStarting from 1.13, docker has --cache-from option that can be used to tell docker to look at another images for layers. Important thing - image should be explicitly pulled for it to work + you still need point what image to take. It could be latest or any other \"rolling\" image you have.\nGiven that, unfortunately there is no way to produce same layer in \"isolation\", but cache-from solves root problem - how to eventually reuse some layers during ci build.",
    "debconf: delaying package configuration, since apt-utils is not installed": "Using DEBIAN_FRONTEND=noninteractive apt-get -yq install {your-pkgs} instead of apt-get -yq install {your-pkgs} should resolve the problem.\nRefer https://stackoverflow.com/a/56569081/12782026 for details.",
    "Locked package.json files in Docker container using docker-compose": "I encountered the same issue and I solved this by mounting the folder where package.json is located instead of package.json itself.\nversion: \"3\"\n\nservices:\n  node:\n    build:\n      context: ./\n      dockerfile: ./docker/Dockerfile\n    volumes:\n      - .:/Example\nMounting package.json directly as a volume seems to lock the file.",
    "How to use quotes in Dockerfile CMD": "I have had a similar problem when using CMD in the exec form, aka JSON array.\nThe TL;DR is that there seems to be a fallback mechanism that introduces /bin/sh when the CMD items cannot be parsed correctly.\nI could not find the exact rules but I have examples:\nENTRYPOINT [\"reflex\"]\nCMD [\"-r\", \".go$\"]\n\n$ docker inspect --format '{{.Config.Cmd}}' inventories_service_dev_1 \n[-r .go$]\nENTRYPOINT [\"reflex\"]\nCMD [\"-r\", \"\\.go$\"]\n\n$ docker inspect --format '{{.Config.Cmd}}' inventories_service_dev_1 \n[/bin/sh -c [\"-r\", \"\\.go$\"]]\nNotice that \\ in above.",
    "Appending to base image's ENTRYPOINT": "You can not have multiple ENTRYPOINTs, but you could get this to work by putting both commands into a start-up.ps1 and running that as your ENTRYPOINT.\nADD start-up.ps1\n\nENTRYPOINT ['powershell', '.\\start-up.ps1']",
    "Assembly specified in the dependencies manifest was not found while running docker with dotnet-core project": "To fix this error I have to changed my WORKDIR. It was reported in GitHub\nWORKDIR /app",
    "Docker History Base Image Add:sha256hash": "The docker brew debian image is made of intermediate containers, as described in \"Understand images, containers, and storage drivers\".\nSee issue 25925: each layer being stored in (for instance) /var/lib/docker/aufs/mnt/.\nSo ADD file:89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da95 would add all files found in /var/lib/docker/aufs/mnt/89ecb642d662ee7edbb868340551106d51336c7e589fdaca4111725ec64da95.\n(Note: I mentioned the (nop) part in \"Docker missing layer IDs in output\")",
    "Activate conda env and run pip install requirements in the Dockfile": "It has to be in the same line. To install in dev1 using pip:\nRUN . /code/miniconda/etc/profile.d/conda.sh && conda activate dev1\\\n    && pip install -r requirements.txt\nTo install in dev1 using conda itself for packages that are in coda channels\nRUN conda install -n dev1 packagname",
    "Assigning a command output to ARG variable in a Dockerfile": "Although not a direct answer to the question, the way I solved this problem was by creating a bash script that is responsible for starting the Dockerfile build. The script runs the command I need and then passes it as an argument to the Dockerfile. Example:\n#!/usr/bin/env bash\n\nMY_VAR=$(echo 'hello')\ndocker build --build-arg MY_VAR=${MY_VAR} -t myapp .\nAnd then the Dockerfile gets it:\nARG MY_VAR\n# MY_VAR now equals 'hello'",
    "Error when opening a VSCode remote-container from a project using a Dockerfile and devcontainer.json": "This solved the error for me:\nsudo snap install docker",
    "arm64 docker image for apple silicon M1 macbook air": "This github is the source for pre-built arm64 (Apple Silicon) docker images with PyTorch. That said, you'll still need to install fastai inside this image.\nI am currently working on a new version of my \"native\" fastai image (which uses the above image as a base image) that will not require sonisa's base image and will include Ruby (so you can run a Ruby on Rails server that can access pickle files created by fastai). However, that's still a work in progress. Watch this repo as I hope to publish the new docker file in the coming days.",
    "Install Bash on scratch Docker image": "When you start a Docker image FROM scratch you get absolutely nothing. Usually the way you work with one of these is by building a static binary on your host (or these days in an earlier Dockerfile build stage) and then COPY it into the image.\nFROM scratch\nCOPY mybinary /\nENTRYPOINT [\"/mybinary\"]\nNothing would stop you from creating a derived image and COPYing additional binaries into it. Either you'd have to specifically build a static binary or install a full dynamic library environment.\nIf you're doing this to try to debug the container, there is probably nothing else in the image. One thing this means is that the set of things you can do with a shell is pretty boring. The other is that you're not going to have the standard tool set you're used to (there is not an ls or a cp). If you can live without bash's various extensions, BusyBox is a small tool designed to be statically built and installed in limited environments that provides minimal versions of most of these standard tools.",
    "Docker: Insert certificate into ketstore": "Just like someone already stated in the comment - if you want to use the crt file that gets mounted at deployment, you have to add the keytool command to the deployment.\nThe crt you are trying to access when building the container does not exist yet.",
    "Why SHA256 digest of Docker image changes if we push the same image with same tag multiple times to the same docker repository": "Since my comment answered your question, original credit goes to the post here: https://windsock.io/explaining-docker-image-ids/\nLayers are identified by a digest in this form: algorithm:hex which looks like sha256:abcd.....\nThe hex is calculated by applying the algorithm (sha256) to the layers content. If the content changes, then the digest changes.",
    "When creating pod it go into CrashLoopBackoff. Logs show \"exec /usr/local/bin/docker-entrypoint.sh: exec format error.\"": "As mentioned in the comment above by David Maze, the issue is due to building the image on Mac M1 Pro.\nTo fix this I need to add FROM --platform=linux/amd64 <image>-<version> in the Dockerfile and build or you can run the below command while running the build\ndocker build --platform=linux/amd64 <image>-<version>\nBoth solutions will work. I added FROM --platform=linux/amd64 to the Dockerfile and it's fixed now.",
    "Docker - Couldn't create the mpm-accept mutex": "Thanks to @Bets, the problem was solved with adding the following into Dockerfile:\nRUN echo \"Mutex posixsem\" >> /etc/apache2/apache2.conf",
    "Database Permission denied after running docker-compose up": "You can try to edit the db service in docker-compose.yml with the following changes:\nvolumes:\n  # - ./init-db:/docker-entrypoint-initdb.d\n  - ./data-db:/var/lib/mysql:rw # <- permissions\nuser: mysql # <- user, should be 'mysql'",
    "Cant create image/ the Dockerfile (Dockerfile) cannot be empty": "I saw that you, don't save your dockerfile, try save changes and run docker build again.\nAlso saw same issue on github, it could be helpful\nTry to change COPY to ADD\nhttps://github.com/docker/compose/issues/5170",
    "Docker container with neo4j and password set": "You can override the login-password in the docker file:\nFROM neo4j:3.2\nENV NEO4J_AUTH=neo4j/newPassword",
    "docker run entrypoint with multiple commands": "As a style point, this gets vastly easier if your image has a CMD that can be overridden. If you only need to run one command with no initial setup, make it be the CMD and not the ENTRYPOINT:\nCMD ./some_command  # not ENTRYPOINT\nIf you need to do some initial setup and then launch the main command, make the ENTRYPOINT be a shell script that ends with the special instruction exec \"$@\". The CMD will be passed into it as parameters, and this line replaces the shell script with that command.\n#!/bin/sh\n# entrypoint.sh\n... do first time setup, run database migrations, set variables ...\nexec \"$@\"\n# Dockerfile\n...\nENTRYPOINT [\"./entrypoint.sh\"] # MUST be JSON-array syntax\nCMD ./some_command             # as before\nIf you do these things, then you can use your initial docker run form. This will replace the CMD but leave the ENTRYPOINT intact. In the wrapper-script case, your alternate command will be run as the exec \"$@\" command, so all of the first-time setup will be done first.\n# Assuming the image correctly honors the CMD\ndocker run ... \\\n  image-name \\\n  sh -c 'echo \"foo is $FOO\" && echo \"bar is $BAR\"'\nIf you really can't do this, you can override the docker run --entrypoint. This runs instead of the image's entrypoint (if you want the image's entrypoint you have to run it yourself), and the syntax is awkward:\n# Run a shell command instead of the entrypoint\ndocker run ... \\\n  --entrypoint /bin/sh \\\n  image-name \\\n  -c 'echo \"foo is $FOO\" && echo \"bar is $BAR\"'\nNote that the --entrypoint option comes before the image name, and its arguments come after the image name.",
    "Create docker image without source image (OS)": "You said \"we want to separate base image(centos or rhel) and application images and link them during run time.\" That is essentially what FROM rhel does, thanks to the layered file systems used by Docker.\nThat is, the FROM image does not become part of your image -- it remains in a separate layer. Your new image points to that rhel (or other FROM'd base layer) and then adds itself on top of it at runtime.\nSo go ahead and use FROM -- you'll get the behavior you wanted.\nFor those that find this question looking for a way to build their own base image (so you don't have to use anything as a base), you can use FROM scratch and you should read about Creating a base image.\nAnd, to be completely pedantic, the reason why Docker needs a FROM and a base including a Linux distribution's root file system is that without a root file system, there is nothing. You can't even RUN mkdir /opt/cassandra because mkdir is a program provided by the root file system.",
    "How to edit standalone.xml file dynamically in keycloak": "First, it seems in a docker container by default standalone-ha.xml is used. You can find this in /opt/jboss/tools/docker-entrypoint.sh. Second, I think after changing configuration file you'll have to restart keycloak server (container).\nNot sure what do you mean by \"dynamically\". But it will be easier to modify the file locally and build a custom docker image. Dockerfile may look like:\nFROM jboss/keycloak:6.0.1\nADD <path on your system>/standalone-ha.xml /opt/jboss/keycloak/standalone/configuration/standalone-ha.xml",
    "\"Docker is not supported on this Mac\"": "I had this error as well, and then I realized that I downloaded the wrong installation file. On the Docker site, there are two options - 'Mac with Intel Chip' or 'Mac with Apple Chip'. I have an Intel Chip and found that I accidentally downloaded the Apple Chip version. The Intel Chip version worked fine for me.",
    "Is `FROM` clause required in Dockerfile?": "Based on the official documentation it's required:\nThe FROM instruction initializes a new build stage and sets the Base Image for subsequent instructions. As such, a valid Dockerfile MUST start with a FROM instruction. The image can be any valid image \u2013 it is especially easy to start by pulling an image from the Public Repositories.\nhttps://docs.docker.com/engine/reference/builder/#from",
    "SQL Server in Docker CREATE INDEX failed because the following SET options have incorrect settings: \u2018QUOTED_IDENTIFIER\u2019": "The SQLCMD utility unfortunately defaults to QUOTED_IDENTIFIER OFF for backwards compatibility reasons. Specify the -I argument so that QUOTED_IDENTIFIER ON is used.\n/opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P $SA_PASSWORD -i $file -I",
    "How to keep Docker container volume files?": "When you add a VOLUME at docker run, what you are saying is to use the main host filesystem instead of the copy-on-write filesystem that Docker uses for images. There are two main options here:\nYou bind-mount an actual filesystem location into the image. This is what you are doing here.\nYou let Docker handle the location... in this case, Docker creates the location on the main filesystem, and then copies the contents of the image into that location to get things started.\nYou are looking to get both- you want a fixed location on your filesystem, but you want files from your image to be there. Now, there is a reason it doesn't work this way! What happens if 'auto.conf' already exists in that folder and you launch your container? What happens if you run two containers with different versions of that file pointed at the same location? That is why if you pick a real location, it does not attempt to guess what to do with conflicts between the image and the filesystem, it just goes with the filesystem.\nYou CAN achieve what you want though. There are really two options. The better one would be to have your app read from two seperate folders- one that is populated inside the image, and one that is on your filesystem. That completely avoids this problem ;) The second option is to go in and tell Docker how to handle individual files in your image.\nversion: '2'\nservices:\n  app:\n    container_name: mono\n    build: .\n    volumes:\n      # save .composer files on host to keep cache warmed up\n      - '/srv/mono/mydir:/root/mydir'\n      # Marking a volume this way will tell Docker to use THIS file \n      # from the image, even though the parent directory is a regular\n      # volume.  If you have an auto.cnf file in your directory, it\n      # will be ignored. \n      - /root/mydir/auto.cnf\n    command: sleep infinity\n......",
    "Error when installing google-chrome-stable on python image in Dockerfile": "As per the instructions here you need to add the chrome repo. So changing your dockerfile to something like the following works:\nFROM python:3.8\nWORKDIR /test\nRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\nRUN echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list\nRUN apt-get update && apt-get install -y google-chrome-stable",
    "Docker isn't caching Alpine apk add command": "Reorder your Dockerfile and it should work.\nFROM golang:1.13.5-alpine\nRUN apk add --update docker\n\nWORKDIR /go/src/app\nCOPY src .\nRUN go get -d -v ./...\nRUN go install -v ./...\nCMD [\"app\"]\nAs you are copying before installation, so whenever you change something in src the cache will invalidate for docker installtion.",
    "Docker RUN does NOT persist files": "The reason those changes are not persisted, is that they are inside a volume the Jenkins Dockerfile marks /var/jenkins_home/ as a VOLUME.\nInformation inside volumes is not persisted during docker build, or more precisely; each build-step creates a new volume based on the image's content, discarding the volume that was used in the previous build step.\nHow to resolve this?\nI think the best way to resolve this, is to;\nAdd the files you want to modify inside jenkins_home in a different location inside the image, e.g. /var/jenkins_home_overrides/\nCreate a custom entrypoint based on, or \"wrapping\", the default entrypoint script that copies the content of your jenkins_home_overrides to jenkins_home the first time the container is started.\nActually...\nAnd just when I wrote that up; It looks like the official Jenkins image already support this out of the box; https://github.com/jenkinsci/docker/blob/683b0d6ed17016ee3211f247304ef2f265102c2b/jenkins.sh#L5-L23\nAccording to the documentation, you need to add your files to the /usr/share/jenkins/ref/ directory, and those will be copied to /var/jenkins/home upon start.\nAlso see https://issues.jenkins-ci.org/browse/JENKINS-24986",
    "Docker ARG variables not working (always empty)": "ARG steps are scoped. Before the first FROM step, the ARG only applies to FROM steps. And within each FROM step, it only applies to the lines after that ARG step until the next FROM (in a multistage build).\nTo fix this, reorder your steps:\nFROM node:current AS build-node\nARG APP_NAME='ground-station'\nWORKDIR /${APP_NAME}\nRUN echo \"APP_NAME=${APP_NAME}\"",
    "EACCES: permission denied mkdir ... while trying to use docker volumes in a node project": "So, what I gathered from looking around and seeing other people's Dockerfiles and a bit of running commands in a bash terminal in my container, is that the node:alpine image I used as base, creates a user named node inside the container.\nThe npm run start is executed as a process of node user(process with the UID of node) which does not have root privileges (which I found out while looking in the htop), so in order to make node user the owner of /app directory I added this in the docker file-\nRUN chown -R node.node /app\nThe updated Dockerfile.dev is-\nFROM node:alpine\n\nWORKDIR '/app'\n\nCOPY package.json .\nRUN npm install\nRUN chown -R node.node /app\n\nCOPY . .\n\nCMD [\"npm\", \"run\", \"start\"]\nThis has fixed the problem for me, hope it can help someone else too. :)",
    "Dockerfile: copy zip and open it": "Found a way to do it using .tar file insead of .zip and using \"ADD\" instead of \"COPY\":\nThe DOCKERFILE. now looks like this:\nFROM mcr.microsoft.com/windows/servercore:ltsc2019\nADD test3.tar c:/\nThe ADD command extracts the archive.",
    "Dockerfile CMD not accepting variables for substitution": "When you write the arguments to CMD (or ENTRYPOINT) as a JSON string, as in...\nCMD [\"/opt/jdk/bin/java\", \"-jar\", \"ssltools-domain-LATEST.jar\"]\n...the command is executed directly with the exec system call and is not processed by a shell. That means no i/o redirection, no wildcard processing...and no variable expansion. You can fix this in a number of ways:\nYou can just write it as a plain string instead, as in:\n  CMD /opt/jdk/bin/java -jar ${ARTIFACTID}-${VERSION}.${PACKAGING}\nWhen the argument is not a JSON construct, it gets passed to sh -c.\nYou can do as suggested by Philip, and pass the arguments to sh -c yourself:\n  CMD [\"sh\", \"-c\", \"/opt/jdk/bin/java -jar ${ARTIFACTID}-${VERSION}.${PACKAGING}\"]\nThose two options are basically equivalent.\nA third option is to put everything into a shell script, and then run that:\n  COPY start.sh /start.sh\n  CMD [\"sh\", \"/start.sh\"]\nThis is especially useful if you need to perform more than a simple command line.",
    "Is there any cache advantage to using ADD <url> vs RUN wget/curl <url> in a Dockerfile": "Certainly.\nThe RUN instruction will not invalidate the cache unless its text changes. So if the remote file is updated, you won't get it. Docker will use the cached layer.\nThe ADD instruction will always download the file and the cache will be invalidated if the checksum of the file no longer matches.\nI would recommend using ADD instead of RUN wget ... or RUN curl .... I imagine people use the latter as its more familiar, but the ADD instruction is quite powerful. It can untar files and set ownership. It's also considered best practice to avoid downloading any packages that are not necessary for your process to run (though there are multiple ways to accomplish this, like using multi-stage builds).\nDocs on cache invalidation:\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#leverage-build-cache",
    "Docker healthcheck in composer file": "I believe this is similar to Docker Compose wait for container X before starting Y\nYour db_image needs to support curl.\nTo do that, create your own db_image as:\nFROM base_image:latest\nRUN apt-get update\nRUN apt-get install -y curl \nEXPOSE 3306\nThen all you should need is a docker-compose.yml that looks like this:\nversion: '2'\nservices:\n  db:\n    image: db_image\n    restart: always\n    dns:\n      - 10.\n    ports:\n      - \"${MYSQL_EXTERNAL_PORT}:${MYSQL_INTERNAL_PORT}\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:${MYSQL_INTERNAL_PORT}\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n    environment:\n      TZ: Europe/Berlin\n  main_application:\n    image: application_image\n    restart: always\n    depends_on:\n      db:\n        condition: service_healthy\n    links: \n        - db\n    dns:\n      - 10.\n    ports:\n      - \"${..._EXTERNAL_PORT}:${..._INTERNAL_PORT}\"\n    environment:\n      TZ: Europe/Berlin\n    volumes:\n      - ${HOST_BACKUP_DIR}:/...\n    volumes_from:\n      - data\n      - db",
    "Dockerfile `RUN --mount=type=ssh` doesn't seem to work": "The error has nothing to do with your private key; it is \"host key verification failed\". That means that ssh doesn't recognize the key being presented by the remote host. It's default behavior is to ask if it should trust the hostkey, and when run in an environment when it can't prompt interactively, it will simply reject the key.\nYou have a few options to deal with this. In the following examples, I'll be cloning a GitHub private repository (so I'm interacting with github.com), but the process is the same for any other host to which you're connecting with ssh.\nInject a global known_hosts file when you build the image.\nFirst, get the hostkey for the hosts to which you'll be connecting and save it alongside your Dockerfile:\n$ ssh-keyscan github.com > known_hosts\nConfigure your Dockerfile to install this where ssh will find it:\nCOPY known_hosts /etc/ssh/ssh_known_hosts\nRUN chmod 600 /etc/ssh/ssh_known_hosts; \\\n  chown root:root /etc/ssh/ssh_known_hosts\nConfigure ssh to trust unknown host keys:\nRUN sed /^StrictHostKeyChecking/d /etc/ssh/ssh_config; \\\n  echo StrictHostKeyChecking no >> /etc/ssh/ssh_config\nRun ssh-keyscan in your Dockerfile when building the image:\nRUN ssh-keyscan github.com > /etc/ssh/ssh_known_hosts\nAll three of these solutions will ensure that ssh trusts the remote host key. The first option is the most secure (the known hosts file will only be updated by you explicitly when you run ssh-keyscan locally). The last option is probably the most convenient.",
    "Run a command line when starting a docker container": "Docker execute RUN command when you build the image.\nDocker execute ENTRYPOINT command when you start the container. CMD goes as arguments to ENTRYPOINT. Both of these can be overridden when you create a container from an image. Their purpose in Dockerfile is to provide defaults for future when you or someone else will be creating containers from this image.\nConsider the example:\nFROM debian:buster\n\nRUN apt update && apt install procps\n\nENTRYPOINT [\"/usr/bin/ps\"]\nCMD [\"aux\"]\nThe RUN command adds ps command to the image, ENTRYPOINT and CMD are not executed but they will be when you run the container:\n# create a container named 'ps' using default CMD and ENTRYPOINT\ndocker run --name ps my_image\n# equivalent to /usr/bin/ps aux\n\n# start the existing container 'ps'\ndocker start ps\n# equivalent to /usr/bin/ps aux\n\n# override CMD\ndocker run my_image au\n# equivalent to /usr/bin/ps au\n\n# override both CMD and ENTRYPOINT\ndocker run --entrypoint=/bin/bash my_image -c 'echo \"Hello, world!\"'\n# will print Hello, world! instead of using ps aux\n\n# no ENTRYPOINT, only CMD\ndocker run --entrypoint=\"\" my_image /bin/bash -c 'echo \"Hello, world!\"'\n# the output is the same as above\nEach time you use docker run you create a container. The used ENTRYPOINT and CMD are saved as container properties and executed each time you start the container.",
    "How to apt-get install firefox on an openjdk:11 docker base image without \"no installation candidate\" error?": "The firefox package is only available under the Debian Unstable Repository (codename \"Sid\"). Debian Stable only has firefox-esr. To include the Sid repository in the package index update, you must add deb http://deb.debian.org/debian/ unstable main contrib non-free as a repository source for apt.\necho \"deb http://deb.debian.org/debian/ unstable main contrib non-free\" >> /etc/apt/sources.list.d/debian.list\napt-get update\napt-get install -y --no-install-recommends firefox\nIf the Sid repository does not have an up-to-date version of Firefox, the next best place to check are the Firefox PPAs (Personal Package Archive) operated by the Mozilla team themselves. PPAs are just repositories, and are added the exact same way as the Sid repository above:\nFirefox PPA\nFirefox Next PPA\nFor example,\nsudo add-apt-repository ppa:mozillateam/firefox-next\nsudo apt-get update",
    "Permission denied docker-entrypoint.sh": "I know I'm a little late but seconding @ARK, you need to give execute permissions to the entrypoint.sh. But use the following command after COPY entrypoint-base.sh /sbin/docker-entrypoint.sh (note the lowercase chmod and a RUN command) -\nRUN chmod +x /sbin/docker-entrypoint.sh",
    "Can we replace dockerfile by docker-compose.yml?": "1) Here are the official docs between COPY and ADD:\nAlthough ADD and COPY are functionally similar, generally speaking, COPY is preferred. That\u2019s because it\u2019s more transparent than ADD. COPY only supports the basic copying of local files into the container, while ADD has some features (like local-only tar extraction and remote URL support) that are not immediately obvious. Consequently, the best use for ADD is local tar file auto-extraction into the image, as in ADD rootfs.tar.xz /.\nYou can't do this in a docker-compose.yml, you'd have to use a Dockerfile if you want to add files to the image.\n2) docker-compose.yml has both an entrypoint and command override. Similar to how you'd pass a CMD at runtime with docker, you can do with a docker-compose run\ndocker run -itd mystuff/myimage:latest bash -c 'npm install && node server.js'\nYou can do this with docker-compose, assuming the service name here is myservice:\ndocker-compose run myservice bash -c 'npm install && node server.js'\nIf your use case is only running one container, it's probably hard to see the benefits of docker-compose as a beginner. If you were to add a mongodb an nginx container to your dev stack you'd start to see where docker-compose really picks up and gives you benefits. It works best when used to orchestrate several containers that need to run and communicate with each other.",
    "Jekyll site running inside docker container but localhost:4000 is not working on host": "Try to bind against 0.0.0.0. Then your site is reachable from an external ip.\nCMD [\"bundle\", \"exec\", \"jekyll\", \"serve\", \"--livereload\", \"--host\", \"0.0.0.0\"]",
    "Access logs from docker container": "You can always check direct docker output of your service with:\ndocker logs my-container-instance\nTo check the log path file you can use:\ndocker inspect my-cintainer-instance\nand find for the LogPath key on the json output.",
    "Dockerfile: Copied file not found": "TL;DR\nDon't rely on shell expansions like ~ in COPY instructions. Use COPY conf/phantomjs.sh /path/to/user/home/phantomjs.sh instead!\nDetailed explanation\nUsing ~ as shortcut for the user's home directory is a feature offered by your shell (i.e. Bash or ZSH). COPY instructions in a Dockerfile are not run in a shell; they simply take file paths as an argument (also see the manual).\nThis issue can easily be reproduced with a minimal Dockerfile:\nFROM alpine\nCOPY test ~/test\nThen build and run:\n$ echo foo > test\n$ docker built -t test\n$ docker run --rm -it test /bin/sh\nWhen running ls -l / inside the container, you'll see that docker build did not expand ~ to /root/, but actually created the directory named ~ with the file test in it:\n/ # ls -l /~/test.txt \n-rw-r--r--    1 root     root             7 Jan 16 12:26 /~/test.txt",
    "error invalid windows mount type: 'bind' on Windows 10 docker build of Windows container": "In docker settings, make sure to have buildkit set to false as below:\n  \"features\": {\n    \"buildkit\": false\n  }\nAfter this is applied and successfully restarted, the build should proceed.\nIn the docs, it's currently mentioned BuildKit is a feature \"only supported for building Linux containers\" : https://docs.docker.com/develop/develop-images/build_enhancements/\nUnfortunately in a fresh install it's value defaults to true and doesn't change to false when switching to Windows Containers.",
    "How do I prevent root access to my docker container": "As David mentions, once someone has access to the docker socket (either via API or with the docker CLI), that typically means they have root access to your host. It's trivial to use that access to run a privileged container with host namespaces and volume mounts that let the attacker do just about anything.\nWhen you need to initialize a container with steps that run as root, I do recommend gosu over something like su since su was not designed for containers and will leave a process running as the root pid. Make sure that you exec the call to gosu and that will eliminate anything running as root. However, the user you start the container as is the same as the user used for docker exec, and since you need to start as root, your exec will run as root unless you override it with a -u flag.\nThere are additional steps you can take to lock down docker in general:\nUse user namespaces. These are defined on the entire daemon, require that you destroy all containers, and pull images again, since the uid mapping affects the storage of image layers. The user namespace offsets the uid's used by docker so that root inside the container is not root on the host, while inside the container you can still bind to low numbered ports and run administrative activities.\nConsider authz plugins. Open policy agent and Twistlock are two that I know of, though I don't know if either would allow you to restrict the user of a docker exec command. They likely require that you give users a certificate to connect to docker rather than giving them direct access to the docker socket since the socket doesn't have any user details included in API requests it receives.\nConsider rootless docker. This is still experimental, but since docker is not running as root, it has no access back to the host to perform root activities, mitigating many of the issues seen when containers are run as root.",
    "Docker : java.net.ConnectException: Connection refused - Application running at port 8083 is not able to access other application on port 3000": "# The problem\nYou run with:\ndocker run -p 8083:8083 myrest-app\nBut you need to run like:\ndocker run --network \"host\" --name \"app\" myrest-app\nSo passing the flag --network with value host will allow you container to access your computer network.\nPlease ignore my first approach, instead use a better alternative that does not expose the container to the entire host network... is possible to make it work, but is not a best practice.\nA Better Alternative\nCreate a network to be used by both containers:\ndocker network create external-api\nThen run both containers with the flag --network external-api.\ndocker run --network \"external-api\" --name \"app\" -p 8083:8083 myrest-app\nand\ndocker run -d --network \"external-api\" --name \"api\" -p 3000:3000 dockerExternalId/external-rest-api\nThe use of flag -p to publish the ports for the api container are only necessary if you want to access it from your computers browser, otherwise just leave them out, because they aren't needed for 2 containers to communicate in the external-api network.\nTIP: docker pull is not necessary, once docker run will try to pull the image if does not found it in your computer. Let me know how it went...\nCall the External API\nSo in both solutions I have added the --name flag so that we can reach the other container in the network.\nSo to reach the external api from my rest app you need to use the url http://api:3000/externalrestapi/testresource.\nNotice how I have replaced localhost by api that matches the value for --name flag in the docker run command for your external api.",
    "How can I use Chinese in Alpine headless chrome?": "Install wqy-zenhei package at testing channel will fix it.\necho @edge http://nl.alpinelinux.org/alpine/edge/testing >> /etc/apk/repositories && apk add wqy-zenhei@edge",
    "Unable to locate file in docker container": "You're copying your local /app/ folder to the /app/ folder in the running Docker container (as mentioned in the comments) creating /app/app/server.py in the Docker container.\nHow to resolve\nA simple fix will be to change\nCOPY . /app\nto\nCOPY ./app/server.py /app/server.py\nExplanation\nThe command COPY works as follows:\nCOPY <LOCAL_FROM> <DOCKER_TO>\nYou're selecting everything in the folder where the Dockerfile resides, by using . in your first COPY, thereby selecting the local /app folder to be added to the Docker's folder. The destination you're allocating for it in the Docker container is also /app and thus the path in the running container becomes /app/app/.. explaining why you can't find the file.\nHave a look at the Docker docs.",
    "\"localhost didn't send data. ERR_EMPTY_RESPONSE\" and \"curl: (52) Empty reply from server\"": "CMD [\"ng\", \"serve\", \"--host=0.0.0.0\"]\nWhen an HTTP client like a web browser (or in this case, curl) sends an HTTP request, the server needs the client\u2019s IP address, so it knows where to send the response. When you run a local development server, it inspects that IP address, and won\u2019t even respond to requests that aren\u2019t coming from 127.0.0.1 (localhost). That keeps other folks on your network (say, sitting in the same Starbucks) from using the dev server as a way to hack your box.\nWhen you run a dev server in a guest container or VM, and run the client (curl) on the host machine, the server sees a different IP address, and refuses to respond. Most servers let you specify a mask of what IP addresses are valid clients. 0.0.0.0 means fugetaboutit, accept all client IPs. It gives the bouncer the night off.",
    "keytool error: java.io.FileNotFoundException (Permission denied) while calling from docker file": "The default user in docker is root. I believe it has been set to a user other than root by your organisation for security purposes. You need to change to user root and then change back to whatever user had been set by your organisation.\nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64\n\nCOPY app-module/src/main/resources/certificates/A.crt /etc/ssl/certs/\nCOPY app-module/src/main/resources/certificates/B.crt /etc/ssl/certs/\n\n#change to user root to install certificates\nUSER root\nRUN $JAVA_HOME/bin/keytool -import -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit -noprompt -file /etc/ssl/certs/A.crt -alias A\nRUN $JAVA_HOME/bin/keytool -import -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit -noprompt -file /etc/ssl/certs/B.crt -alias B\n\n#change to user oldUser to comply with organisation standards\nUSER oldUser",
    "Error trying to create a scheduled task within Windows 2016 Core container": "The issue has to do with the Container user. By default a scheduled task is created with the current user. It's possible the container user is a special one that the Scheduled Task command cannot parse into XML.\nSo you have to pass the user /ru (and if needed the password /rp) to the schtasks command in a Windows Container.\nThis works\nFROM microsoft/windowsservercore\nRUN schtasks /create /tn \"hellotest\" /sc daily /tr \"echo hello\" /ru SYSTEM\nIt will run the command under the system account.\nIf you are a fan of Powershell (like me), you can use this\nFROM microsoft/windowsservercore\n\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\n\nRUN $action = New-ScheduledTaskAction -Execute 'echo \"\"Hello World\"\"'; \\\n    $trigger = New-ScheduledTaskTrigger -Daily -At '1AM'; \\\n    Register-ScheduledTask -TaskName 'Testman' -User 'SYSTEM' -Action $action -Trigger $trigger -Description 'Container Scheduled task test';",
    "Merge two docker images": "You can't just merge the images. You have to recreate your own based on what was in each of the images you want. You can download both images and re-create the Docker files for each like this:\ndocker history --no-trunc=true image1 > image1-dockerfile\ndocker history --no-trunc=true image2 > image2-dockerfile\nSubstitute the image1 and image2 with the images you want to see the history for. After this you can use those dockerfiles to build your own image that is the combination of the two.\nThe fly in the ointment here is that any ADD or COPY commands will not reveal what was copied because you don't have access to the local file system from which the original images were created. With any luck that won't be necessary or you can get any missing bits from the images themselves.",
    "Can I use Docker buildkit to provide an ssh key to a non-root user?": "It turns out, buildkit's --mount syntax takes a uid= parameter.\nSo I was able to discover the uid for my non-root user like so:\n$ docker run -it --rm apache/airflow bash -c 'id -u $(whoami)'\n    50000\nAnd then adjust my Dockerfile to mount the socket to the ssh agent with that uid as the owner:\nFROM docker.io/apache/airflow\nUSER root\nRUN apt update && apt install -y git openssh-client && rm -rf /var/lib/apt/lists/*\nUSER airflow\nRUN mkdir -m 700 ~/.ssh\nRUN ssh-keyscan github.com > ~/.ssh/known_hosts\nRUN --mount=type=ssh,uid=50000 ssh -vvvT git@github.com\nFollowing this, I was able to authenticate over ssh with the externally-supplied key, as a non-root user.",
    "Docker Compose: Port mapping by selecting a random port within given port range": "There's no way to do this in Linux in general. You can ask the OS to use a specific port or you can ask the OS to pick a port for you, but you can't constrain the automatically-chosen port.\nIn a Compose context you can omit the host port to let the OS choose the port\nports:\n  - \"8001\" # container port only\nand you will need to use docker-compose port to find out what the port number actually is. That will be any free port number, and there is no way to limit it to a specific range.",
    "Environment variables inside Dockerfile": "You need to declare them as an argument and not as an environment variable, since you are passing an argument to the docker build command as a --build-arg argument value.\nThe Dockerfile should be something like this:\nFROM mongo:4.4.9\n\nARG MONGO_UID\nARG MONGO_GID\n\n# setup folder before switching to user\nRUN mkdir /var/lib/mongo\nRUN usermod -u ${MONGO_UID} mongodb\nRUN groupmod -g ${MONGO_GID} mongodb\nRUN chown mongodb:mongodb /var/lib/mongo\nRUN chown mongodb:mongodb /var/log/mongodb\n\nUSER mongodb\nIf you need to have them as an environment variable inside the container, then you would need to say that the ENV is equal to the ARG:\nARG MONGO_UID\nENV MONGO_UID=${MONGO_UID}\nIt's also possible to give the argument (same to ENV) a default value, in case you are interested:\nARG MONGO_UID=default\nNot having a default value will force the user to supply a value to the argument.",
    "Docker running script in entrypoint": "The problem is with your interpreter: sh Try exec form: ENTRYPOINT [\"/bin/bash\", \"-c\", \"./docker-entrypoint.sh\"]",
    "'docker build: how does version pinning invalidate the build cache?": "All docker does is look at the string you run, along with the environment you pass in, and compare it to other images in the build cache. If you have a Dockerfile that doesn't pin a package, like:\nRUN apt-get update && apt-get install -y \\\n    package-foo\nand you first run this when version 1.2 is the current latest, and then run the same command again after 1.3 gets released, docker will see the command is identical, and reuse the build cache from the previous build of the image, rather than pulling the newer version of that package.\nIf instead you specify a version like:\nRUN apt-get update && apt-get install -y \\\n    package-foo=1.2.*\nAnd then rebuild with an updated Dockerfile containing a different version pinned:\nRUN apt-get update && apt-get install -y \\\n    package-foo=1.3.*\nThe second build will be a different command to run, and therefore force docker to rerun the build without the cache from the previous run. This also has the advantage that a 1.4 release doesn't get pulled in unexpectedly (e.g. if an earlier line breaks the cache or the cache gets removed/disabled).\nIf you just want the most recent versions of these packages, regardless of the cache, then you can skip version pinning and either intentionally break the cache or disable the cache. To disable the cache, you can build with the flag --no-cache. To intentionally break the cache, you can pass a changing build arg with something like:\ndocker build --build-arg \"TIMESTAMP=$(date +%s)\" .\nAnd a Dockerfile that defines that build arg before you want to break the cache:\nARG TIMESTAMP\nRUN apt-get update && apt-get install -y \\\n    package-foo",
    "How to install libwebp in alpine Linux Docker image": "Maybe I didn't understand you right way, but you can install libwebp and libwebp-tools packages as other packages described in your question.\nThe final Dockerfile is:\nFROM openjdk:8-jdk-alpine\nRUN apk update && \\\n    apk upgrade -U && \\\n    apk add ca-certificates ffmpeg libwebp libwebp-tools && \\\n    rm -rf /var/cache/*\nNow you can find dwebp binary file by the following path:\n/ # which dwebp\n/usr/bin/dwebp\nEDIT:\nIf you want to install another libwebp version on alpine platform you need to add package repository from the previous alpine versions and define version of package you need to install.\nFor your particular case there are the following versions of libwebp package in alpine package repositories:\n0.4.4-r0 - alpine v3.3\n0.5.0-r0 - alpine v3.4\n0.5.2-r0 - alpine v3.5\n0.6.0-r0 - alpine v3.6\n0.6.0-r1 - alpine v3.7\nFor example you want to install libwebp version 0.4.4-r0. The Dockerfile is:\nFROM openjdk:8-jdk-alpine\nRUN apk update && \\\n    apk upgrade -U && \\\n    apk add ca-certificates ffmpeg && rm -rf /var/cache/*\n\nRUN echo \"http://dl-cdn.alpinelinux.org/alpine/v3.3/main\" >> /etc/apk/repositories\n\nRUN apk add --no-cache libwebp=0.4.4-r0 libwebp-tools=0.4.4-r0",
    "Start two servers in one docker container": "You can use supervisord when you want to run multiple processes in one container - like in your case npm and python server.\nCheck documentation for supervisord for more information.\nI just pick some important parts.\n1, You will need to install supervisord for docker image\nSomething like:\nCentOS: `yum install supervisor`\n\nUbuntu: `apt-get install -y supervisor`\n2, copy configuration for supervisord (supervisord.conf) to docker image .\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\nIn your case supervisord.conf should looks like example below (not tested) :\n[supervisord]\nnodaemon=true\n\n[program:npm]\ncommand=npm start\n\n[program:python]\ncommand=python server.py\nand run supervisord as CMD command in Dockerfile:\n...\n# install supervisord\n# copy supervisord configuration\n...\n# run supervisord\nCMD [\"/usr/bin/supervisord\"]",
    "pass parameters to docker entrypoint": "You can append your dynamic parameters at the end of the docker run .... You haven't specified any CMD instruction, so it'll work.\nWhat is actually run without specifying any command at the end, when running the docker run ..., is this:\nENTRYPOINT CMD (it's concatenated and there is a space in between)\nSo you can also use something like\n...\nENTRYPOINT [\"java\", \"-jar\", \"my_app.jar\"]\nCMD [\"--spring.config.location=classpath:/srv/app/configs/application.properties\"]\nwhich means, when using\ndocker run mycontainer the\njava -jar my_app.jar --spring.config.location=classpath:/srv/app/configs/application.properties\nwill be invoked (the default case), but when running\ndocker run mycontainer --spring.config.location=classpath:/srv/app/configs/some_other_application.properties -Dversion=$version\nit'll be run w/ different property file and with the system property called version (overriding the default case)",
    "Docker config : Celery + RabbitMQ": "I have similar Celery exiting problem while dockerizing the application. You should use rabbit service name ( in your case it's rabbitmq) as host name in your celery configuration.That is,\nuse broker_url = 'amqp://guest:guest@rabbitmq:5672//' instead of broker_url = 'amqp://guest:guest@localhost:5672//' .\n\nIn my case, major components are Flask, Celery and Redis.My problem is HERE please check the link, you may find it useful.",
    "Docker Default Operating System": "You always should have a FROM instruction for a Dockerfile as per documentation as mentioned by Munir. However, you can choose variety of base images, which does not have to be an OS for your Dockerfile. For example, if you are creating a docker image for your java application, you can use java image as your base images.\nFROM library/java\nHowever, at the end, if you traverse thorough those image's Dockerfile, you will end up in one or the other OS. Java is based on Debian.",
    "Dockerfile: COPY folder inside folder": "You have to define your foldername inside the directory\nCOPY folder /usr/share/nginx/html/folder\nor\nADD folder /usr/share/nginx/html/folder",
    "Build container with docker-compose, but run /etc/bash with -it option later?": "Looks like docker-compose run will become your friend. From the docker compose docs:\nRuns a one-time command against a service. For example, the following command starts the web service and runs bash as its command.\n$ docker-compose run web bash\nThe command docker-compose run <service-name> also ensures that all required containers (e.g. those providing volumes and links) will be started prior to the container <service-name>, if they are not already running.\nIn case you use any port-mappings, you might consider using docker-compose run --service-ports <service-name>. Without that option docker-compose will only map ports when using docker-compose up.\nLike with docker run you can also use the --rm option to remove containers once you are done with them.\nIf your image uses ENTRYPOINT, you should consider overriding this in your docker-compose.yml with entrypoint: /bin/bash.",
    "How to install Node.js version 16.x.x in a Debian based image (Dockerfile)? (why so hard?)": "You can use:\nFROM python:buster\n\nRUN apt-get update && \\\n apt-get install -y \\\n    nodejs npm\n\nRUN curl -fsSL https://deb.nodesource.com/setup_current.x | bash - && \\\n apt-get install -y nodejs",
    "Dockerfile FROM Insecure Registry": "Depending on your version, you may need to include the scheme in the insecure registry definition. Newer versions of buildkit should not have this issue, so an upgrade may also help.\n   ...\n   \"insecure-registries\" : [\n     \"insecure.registry.local\",\n     \"http://insecure.registry.local\"\n   ]\n   ...",
    "docker run interactive with conda environment already activated": "Each RUN statement (including docker run) is executed in a new shell, so one cannot simply activate an environment in a RUN command and expect it to continue being active in subsequent RUN commands.\nInstead, you need to activate the environment as part of the shell initialization. The SHELL command has already been changed to include --login, which is great. Now you simply need to add conda activate my_env to .profile or .bashrc:\n...\n# Create and activate the environment.\nRUN conda env create --force -f environment.yml\nRUN echo \"conda activate my_env\" >> ~/.profile\nand just be sure this is after the section added by Conda.",
    "Docker build stops on software-properties-common when run with `apt update`": "Add the following line in your Dockerfile and build again.\nRUN DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata",
    "How to use a docker file with terraform": "Terraform is a provisioning tool rather than a build tool, so building artifacts like Docker images from source is not really within its scope.\nMuch as how the common and recommended way to deal with EC2 images (AMIs) is to have some other tool build them and Terraform simply to use them, the same principle applies to Docker images: the common and recommended path is to have some other system build your Docker images -- a CI system, for example -- and to publish the results somewhere that Terraform's Docker provider will be able to find them at provisioning time.\nThe primary reason for this separation is that it separates the concerns of building a new artifact and provisioning infrastructure using artifacts. This is useful in a number of ways, for example:\nIf you're changing something about your infrastructure that doesn't require a new image then you can just re-use the image you already built.\nIf there's a problem with your Dockerfile that produces a broken new image, you can easily roll back to the previous image (as long as it's still in the registry) without having to rebuild it.\nIt can be tempting to try to orchestrate an entire build/provision/deploy pipeline with Terraform alone, but Terraform is not designed for that and so it will often be frustrating to do so. Instead, I'd recommend treating Terraform as just one component in your pipeline, and use it in conjunction with other tools that are better suited to the problem of build automation.\nIf avoiding running a separate registry is your goal, I believe that can be accomplished by skipping using docker_image altogether and just using docker_container with an image argument referring to an image that is already available to the Docker daemon indicated in the provider configuration.\ndocker_image retrieves a remote image into the daemon's local image cache, but docker build writes its result directly into the local image cache of the daemon used for the build process, so as long as both Terraform and docker build are interacting with the same daemon, Terraform's Docker provider should be able to find and use the cached image without interacting with a registry at all.\nFor example, you could build an automation pipeline that runs docker build first, obtains the raw id (hash) of the image that was built, and then runs terraform apply -var=\"docker_image=$DOCKER_IMAGE\" against a suitable Terraform configuration that can then immediately use that image.\nHaving such a tight coupling between the artifact build process and the provisioning process does defeat slightly the advantages of the separation, but the capability is there if you need it.",
    "Run sqlcmd in Dockerfile": "The MS SQL containers can have SQL statements run against them inside a dockerfile that uses them.\nI think your problem is just that double quotes are being stripped from your RUN command.\nI couldn't quite decide if its a bug based on this github issue but escaping them as \\\" will work around it.\nYou can also avoid setting the SA password in your docker file by defaulting to a trusted connection:\nrun sqlcmd -Q \\\"select name from master.dbo.sysdatabases\\\"\nThis way the variables can be set at container run time like this:\ndocker run -e ACCEPT_EULA=Y -e SA_PASSWORD=bobleponge -p 1433:1433 <imageid>",
    "Using variables across multi-stage docker build": "To send variable we can use \"ARG\" solution, the \"base\" solution, and \"file\" solution.\nARG version_default=v1\n\nFROM alpine:latest as base1\nARG version_default\nENV version=$version_default\nRUN echo ${version}\nRUN echo ${version_default}\n\nFROM alpine:latest as base2\nARG version_default\nRUN echo ${version_default}\nanother way is to use base container for multiple stages:\nFROM alpine:latest as base\nARG version_default\nENV version=$version_default\n\nFROM base\nRUN echo ${version}\n\nFROM base\nRUN echo ${version}\nYou can find more details here: https://github.com/moby/moby/issues/37345\nAlso you could save the hash into a file in the first stage, and copy the file in the second stage and then read it and use it there.\nFrom what I understand you want to copy the built program into the new docker for multistage build that the output size is smaller. Basically you do not need to send a variable you need to know were you built it in the first image and copy it from there\nFROM golang:alpine as gobuilder\nRUN apk update && apk add git\n\nCOPY sources/src/ $GOPATH/src/folder/\nWORKDIR $GOPATH/src/folder/\n#get dependencies\nRUN go get -d -v\n#build the binary\nRUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -installsuffix cgo -ldflags=\"-w -s\" -o /go/bin/myGoProgram myGoSource.go\n\nFROM alpine:latest\nCOPY --from=gobuilder /go/bin/myGoProgram /usr/local/bin/myGoProgram\nENTRYPOINT [\"myGoProgram\"] # or ENTRYPOINT [\"/usr/local/bin/myGoProgram\"]",
    "how can I configure the default admin user:password with docker if i am launching docker-compose up of a docker(portainer in my case)?": "Portainer allows you to specify an encrypted password from the command line for the admin account. You need to generate the hash value for password.\nFor example, this is the hash value of password - $$2y$$05$$arC5e4UbRPxfR68jaFnAAe1aL7C1U03pqfyQh49/9lB9lqFxLfBqS\nIn your docker-compose file make following modification\nversion: '3.3'\n services:\n   portainer:\n    image: portainer/portainer\n    volumes:\n     - /var/run/docker.sock:/var/run/docker.sock\n     - ./portainer/portainer_data:/data\n    command: --admin-password \"$$2y$$05$$arC5e4UbRPxfR68jaFnAAe1aL7C1U03pqfyQh49/9lB9lqFxLfBqS\"\n    ports:\n     - \"9000:9000\"\n--admin-password This flag is used to specify an encrypted password in Portainer.\nMore information can be found in documentation - Portainer\nHope this will help you.",
    "How can I add a customized kong plugin into dockerized kong": "I tried to do same thing but could not found well-describe answer yet. You can configure simple helloworld plugin as below: (https://github.com/brndmg/kong-plugin-hello-world)\nLocal 'plugin' directory structure on Docker host:\nThen you can mount local /plugins directory and let kong load custom 'helloworld' plugin from /plugins directory\n1) using environment variables\n$ docker run -d --name kong --network=kong-net \\\n-e \"KONG_DATABASE=cassandra\" \\\n-e \"KONG_PG_HOST=kong-database\" \\ \n-e \"KONG_CASSANDRA_CONTACT_POINTS=kong-database\" \\\n**-e \"KONG_LUA_PACKAGE_PATH=/plugins/?.lua\" \\\n-e \"KONG_CUSTOM_PLUGINS=helloworld\" \\ ** \n...\n-e \"KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl\" \\\n**-v \"/plugins:/plugins\" \\**\n-p 8080:8000 -p 8443:8443 -p 8001:8001 -p 8444:8444 kong:latest\nThen, you can see configured custom plugin on http://[kong-url]:8001/\n..\n\"custom_plugins\": [\n\"helloworld\"\n],\n..\n2) Or, you can simple mount your custom kong.conf file which describes plugins you want.\n/etc/kong/kong.conf\nplugins = bundled,helloworld,jwt-crafter\n(It seems that second option is better for latest version of Kong because 'kong_custom_plugin' configuration prints 'deprecation' warning)\nFor the JWT crafter, https://github.com/foodora/kong-plugin-jwt-crafter it seems that the plugin is not maintained well so that installation using luarocks failed with errors.\n$ luarocks install kong-plugin-jwt-crafter\n....\nkong-plugin-jwt-crafter 1.0-0 depends on lua-resty-jwt ~> 0.1.10-1 (not installed)\n\nError: Could not satisfy dependency lua-resty-jwt ~> 0.1.10-1: No results matching query were found.\nInstead, you can directly to add 'resty-jwt' to official docker image, to resolve dependency, which is not included in official image. and copy \"JWT crafter\" into \"/plugins\" directory, and load.\n(Inside docker container)\n luarocks install lua-resty-jwt\nHope this helps.",
    "Why can't I connect to my local docker-compose container on Windows 10?": "Just change:\nexpose:\n  - \"8000\"\nBy\nports:\n  - \"8000:8000\"\nBtw http://localhost:80 is not working?\nRegards",
    "Is it possible to export a file while inside a docker image?": "Run your container as this:\ndocker run -v $(PWD)/local-dir/:/path/to/results/dir (...rest of the command..)\nSo any file that is created inside the container into /path/to/results/dir gets automatically reflected in your host, inside ./local-dir.\nAlternatively, you can copy any file from container to host:\ndocker cp <container-id>:/path/to/file ./local-dir",
    "Docker for windows - Internal server error": "I finally found the problem. I was using url rewrite 2.0 functionality, but forgot to install it in the docker image.\nAdding the following to my docker file solved the problem:\n# Install Url Rewrite\nADD https://download.microsoft.com/download/C/9/E/C9E8180D-4E51-40A6-A9BF-776990D8BCA9/rewrite_amd64.msi /install/rewrite_amd64.msi\nRUN msiexec.exe /i c:\\install\\rewrite_amd64.msi /passive",
    "How can I combine ENV statement in Dockerfile?": "Example from the Dockerfile reference\nhttps://docs.docker.com/engine/reference/builder/#env\nENV myName=\"John Doe\" myDog=Rex\\ The\\ Dog \\\n    myCat=fluffy",
    "How am I supposed to use a Postgresql docker image/container?": "Short and simple:\nWhat you get from the official postgres image is a ready-to-go postgres installation along with some gimmicks which can be configured through environment variables. With docker run you create a container. The container lifecycle commands are docker start/stop/restart/rm Yes, this is the Docker way of things.\nEverything inside a volume is persisted. Every container can have an arbitrary number of volumes. Volumes are directories either defined inside the Dockerfile, the parent Dockerfile or via the command docker run ... -v /yourdirectoryA -v /yourdirectoryB .... Everything outside volumes is lost with docker rm. Everything including volumes is lost with docker rm -v\nIt's easier to show than to explain. See this readme with Docker commands on Github, read how I use the official PostgreSQL image for Jira and also add NGINX to the mix: Jira with Docker PostgreSQL. Also a data container is a cheap trick to being able to remove, rebuild and renew the container without having to move the persisted data.\nCongratulations, you have managed to grasp the basics! Keep it on! Try docker-compose to better manage those nasty docker run ...-commands and being able to manage multi-containers and data-containers.\nNote: You need a blocking thread in order to keep a container running! Either this command must be explicitly set inside the Dockerfile, see CMD, or given at the end of the docker run -d ... /usr/bin/myexamplecommand command. If your command is NON blocking, e.g. /bin/bash, then the container will always stop immediately after executing the command.",
    "Installing maven in a docker build overrides JAVA 8 with JAVA 7(!)": "I found a minimal-delta solution although the point about not using apt-get for maven installs is noted. Here is the solution as the code\nFROM java:8\n\n# preserve Java 8  from the maven install.\nRUN mv /etc/alternatives/java /etc/alternatives/java8\nRUN apt-get update -y && apt-get install maven -y\n\n# Restore Java 8\nRUN mv -f /etc/alternatives/java8 /etc/alternatives/java\nRUN ls -l /usr/bin/java && java -version\nObviously, the last line is unnecessary but does confirm that the result is java 8.",
    "What are some strategies to invalidate the Dockerfile instruction cache while Downloading resources": "Solution\nDocker will NOT look at any caching mechanism before downloading using \"RUN curl\" nor ADD. It will repeat the step of downloading. However, Docker invalidates the cache if the mtime of the file has been changed https://stackoverflow.com/a/26612694/433814, among other things. https://github.com/docker/docker/blob/master/pkg/tarsum/versioning.go#L84\nHere's a strategy that I've been working on to solve this problem when building Dockerfiles with dependencies from File storage or repository such as Nexus, Amazon S3 is to retrieve the ETag from the resource, caching it, and modifying the mdtime of a cache-flag file. (https://gist.github.com/marcellodesales/721694c905dc1a2524bc#file-s3update-py-L18). It follows the approach performed in Python (https://stackoverflow.com/a/25307587), Node.js (http://bitjudo.com/blog/2014/03/13/building-efficient-dockerfiles-node-dot-js/) projects.\nHere's what we can do:\nGet the ETag of the resource and save it outside of Dockerfile\nUse an ADD instruction to add the cacheable file prior to download\nDocker will check the mtime metadata of the file to whether invalidate the cache or not.\nUse a RUN instruction as usual to download the content\nIf the previous instruction was invalidated, Docker will re-download the file. If not, the cache will be used.\nHere's a setup to demo this strategy:\nExample\nCreate a Web Server that handles HEAD requests and return an ETag header, usually returned by servers.\nThis simulates the Nexus or S3 storage of files.\nBuild an image and verify that the dependent layer will download the resource for the first time\nCaching the current value of the ETag\nRebuild the image and verify that the dependent layer will use the Cached value.\nChanging the ETag value returned by Web Server handler to simulate a change.\nIn addition, persist the change IFF the file has changed. In this cause yes...\nRebuild the image and verify that the dependent layer will be invalidated, triggering a download.\nRebuild the image again and verify that the cache was used.\n1. Node.js server\nSuppose you have the following Node.js server serving files. Let's implement a HEAD operation and return a value.\n// You'll see the client-side's output on the console when you run it.\n\nvar restify = require('restify');\n\n// Server\nvar server = restify.createServer({\n  name: 'myapp',\n  version: '1.0.0'\n});\n\nserver.head(\"/\", function (req, res, next) {\n  res.writeHead(200, {'Content-Type': 'application/json; charset=utf-8',\n        'ETag': '\"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"'});\n  res.end();\n  return next();\n});\n\nserver.get(\"/\", function (req, res, next) {\n  res.writeHead(200, {'Content-Type': 'application/json; charset=utf-8',\n        'ETag': '\"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"'});\n  res.write(\"The file to be downloaded\");\n  res.end();\n  return next();\n});\n\nserver.listen(80, function () {\n  console.log('%s listening at %s', server.name, server.url);\n});\n\n// Client\nvar client = restify.createJsonClient({\n  url: 'http://localhost:80',\n  version: '~1.0'\n});\n\nclient.head('/', function (err, req, res, obj) {\n  if(err) console.log(\"An error ocurred:\", err);\n  else console.log('HEAD    /   returned headers: %j', res.headers);\n});\nExecuting this will give you:\nmdesales@ubuntu [11/27/201411:10:49] ~/dev/icode/fuego/interview (feature/supportLogAuditor *) $ node testserver.js \nmyapp listening at http://0.0.0.0:8181\nHEAD    /   returned headers: {\"content-type\":\"application/json; charset=utf-8\",\n            \"etag\":\"\\\"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\\\"\",\n            \"date\":\"Thu, 27 Nov 2014 19:10:50 GMT\",\"connection\":\"keep-alive\"}\n2. Build an image based on ETag value\nConsider the following build script that caches the ETag Header in a file.\n#!/bin/sh\n\n# Delete the existing first, and get the headers of the server to a file \"headers.txt\"\n# Grep the ETag to a \"new-docker.etag\" file\n# If the file exists, verify if the ETag has changed and/or move/modify the mtime of the file\n# Proceed with the \"docker build\" as usual\nrm -f new-docker.etag\ncurl -I -D headers.txt http://192.168.248.133:8181/ && \\\n  grep -o 'ETag[^*]*' headers.txt > new-docker.etag && \\\n  rm -f headers.txt\n\nif [ ! -f docker.etag ]; then\n  cp new-docker.etag docker.etag\nelse\n  new=$(cat docker.etag)\n  old=$(cat new-docker.etag)\n  echo \"Old ETag = $old\"\n  echo \"New ETag = $new\"\n  if [ \"$old\" != \"$new\" ]; then\n    mv new-docker.etag docker.etag\n    touch -t 200001010000.00 docker.etag\n  fi\nfi\n\ndocker build -t platform.registry.docker.corp.intuit.net/container/mule:3.4.1 .\n3. Rebuilding and using cache\nBuilding this would result as follows, considering I'm using the current cache.\nmdesales@ubuntu [11/27/201411:54:08] ~/dev/github-intuit/docker-images/platform/mule-3.4 (master) $ ./build.sh \nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"\nDate: Thu, 27 Nov 2014 19:54:16 GMT\nConnection: keep-alive\n\nOld ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"\nNew ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"\nSending build context to Docker daemon 51.71 kB\nSending build context to Docker daemon \nStep 0 : FROM core.registry.docker.corp.intuit.net/runtime/java:7\n ---> 3eb1591273f5\nStep 1 : MAINTAINER Marcello_deSales@intuit.com\n ---> Using cache\n ---> 9bb8fff83697\nStep 2 : WORKDIR /opt\n ---> Using cache\n ---> 3e3c96d96fc9\nStep 3 : ADD docker.etag /tmp/docker.etag\n ---> Using cache\n ---> db3f82289475\nStep 4 : RUN cat /tmp/docker.etag\n ---> Using cache\n ---> 0d4147a5f5ee\nStep 5 : RUN curl -o docker https://get.docker.com/builds/Linux/x86_64/docker-latest\n ---> Using cache\n ---> 6bd6e75be322\nSuccessfully built 6bd6e75be322\n4. Simulating the ETag change\nChanging the value of the ETag on the server and restarting the server to simulate the new update will result in updating the cache-flag file and invalidation of the Cache. For instance, the Etag was changed to \"465fb0d9b9f143ad691c7c3bcf3801b47284f8333\". Rebuilding will trigger a new download because the ETag file was updated, and Docker will verify that during the \"ADD\" instruction. Here, step #5 will run again.\nmdesales@ubuntu [11/27/201411:54:16] ~/dev/github-intuit/docker-images/platform/mule-3.4 (master) $ ./build.sh \nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nDate: Thu, 27 Nov 2014 19:54:45 GMT\nConnection: keep-alive\n\nOld ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nNew ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8555}}\"\nSending build context to Docker daemon 50.69 kB\nSending build context to Docker daemon \nStep 0 : FROM core.registry.docker.corp.intuit.net/runtime/java:7\n ---> 3eb1591273f5\nStep 1 : MAINTAINER Marcello_deSales@intuit.com\n ---> Using cache\n ---> 9bb8fff83697\nStep 2 : WORKDIR /opt\n ---> Using cache\n ---> 3e3c96d96fc9\nStep 3 : ADD docker.etag /tmp/docker.etag\n ---> ac3b200c8cdc\nRemoving intermediate container 4cf0040dbc43\nStep 4 : RUN cat /tmp/docker.etag\n ---> Running in 4dd38d30549a\nETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\n ---> 4fafbeac2180\nRemoving intermediate container 4dd38d30549a\nStep 5 : RUN curl -o docker https://get.docker.com/builds/Linux/x86_64/docker-latest\n ---> Running in de920c7a2e28\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 13.5M  100 13.5M    0     0  1361k      0  0:00:10  0:00:10 --:--:-- 2283k\n ---> 95aff324da85\nRemoving intermediate container de920c7a2e28\nSuccessfully built 95aff324da85\n5. Reusing the Cache again\nConsidering that the ETag hasn't changed, the cache-flag file will continue being the same and Docker will do a super fast build using the cache.\nmdesales@ubuntu [11/27/201411:54:56] ~/dev/github-intuit/docker-images/platform/mule-3.4 (master) $ ./build.sh \nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nDate: Thu, 27 Nov 2014 19:54:58 GMT\nConnection: keep-alive\n\nOld ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nNew ETag = ETag: \"{SHA1{465fb0d9b9f143ad691c7c3bcf3801b47284f8333}}\"\nSending build context to Docker daemon 51.71 kB\nSending build context to Docker daemon \nStep 0 : FROM core.registry.docker.corp.intuit.net/runtime/java:7\n ---> 3eb1591273f5\nStep 1 : MAINTAINER Marcello_deSales@intuit.com\n ---> Using cache\n ---> 9bb8fff83697\nStep 2 : WORKDIR /opt\n ---> Using cache\n ---> 3e3c96d96fc9\nStep 3 : ADD docker.etag /tmp/docker.etag\n ---> Using cache\n ---> ac3b200c8cdc\nStep 4 : RUN cat /tmp/docker.etag\n ---> Using cache\n ---> 4fafbeac2180\nStep 5 : RUN curl -o docker https://get.docker.com/builds/Linux/x86_64/docker-latest\n ---> Using cache\n ---> 95aff324da85\nSuccessfully built 95aff324da85\nThis strategy has been used to build Node.js, Java and other App servers or pre-built dependencies.",
    "How to organize multiple Dockerfiles, docker-compose.yaml and .dockerignore": "When you build an image, you send the Docker daemon a build context; in your Compose setup this is the directory named in the build: { context: } setting. The .dockerignore file must be in that exact directory and nowhere else. Its actual effect is to cause files to be excluded from the build context, which can result in a faster build sequence.\nThe build context's other important effect is that all Dockerfile COPY directives are considered relative to that directory; you cannot COPY from parent or sibling directories. So if files are shared between projects, you must set the context directory to some ancestor directory of all of the files that will be included, and COPY directives will be relative to that directory (even if the Dockerfiles are in per-project directories). See also How to include files outside of Docker's build context?\nIf your projects are completely separate: maybe there's a front-end and a back-end project, or in your case a producer and a consumer that share a message format but not any actual code. Then in this case:\nPut a Dockerfile, named exactly Dockerfile, in each project subdirectory\nPut a .dockerignore file in each project subdirectory (it cannot be in the parent directory)\nCOPY directives are relative to the project subdirectory\nCOPY requirements.txt ./\nIn the Compose file, you can use the shorthand build: directory syntax, since you have the standard (default) dockerfile: name\nversion: '3.8'\nservices:\n   producer:\n     build: ./python_producer\n     environment:\n       - RABBITMQ_HOST=rabbitmq\n   consumer:\n     build: ./python_consumer\n     environment:\n       - RABBITMQ_HOST=rabbitmq\n   rabbitmq:\n     image: rabbitmq:3\n     hostname: rabbitmq # RabbitMQ is very unusual in needing to set this\nIf your projects share code or other files: in your example maybe you define Python data structures for the message format in shared code. This in this case:\nPut a Dockerfile, named exactly Dockerfile, in each project subdirectory\nPut a single .dockerignore file in the project root\nCOPY directives are relative to the project root directory\nCOPY python_producer/requirements.txt ./\nIn the Compose file you need to specify context: . and dockerfile: pointing at a per-component Dockerfile\nversion: '3.8'\nservices:\n   producer:\n     build:\n       context: .\n       dockerfile: python_producer/Dockerfile\n     environment:\n       - RABBITMQ_HOST=rabbitmq\n   consumer:\n     build:\n       context: .\n       dockerfile: python_consumer/Dockerfile\n     environment:\n       - RABBITMQ_HOST=rabbitmq\n   rabbitmq:\n     image: rabbitmq:3\n     hostname: rabbitmq # RabbitMQ is very unusual in needing to set this",
    "Multiple docker-compose file with different context path": "Option 1: Use absolute paths for the contexts in both docker-compose files\nOption 2: Create a docker-compose.override.yml with the absolute paths:\nversion: \"3\"\n\nservices:\n  service1:\n    build:\n      context: /home/project1\n  service2:\n    build:\n      context: /home/project2\nand include it in the docker-compose command:\ndocker-compose -f /home/project1/docker-compose.yml -f /home/project2/docker-compose.yml -f /home/docker-compose.override.yml config\nOn linux, to avoid hard-coding of the base path in the docker-compose.override.yml, you can use PWD environment variable:\nservices:\n  service1:\n    build:\n      context: ${PWD}/project1\n  service2:\n    build:\n      context: ${PWD}/project2",
    "How do I have multiple docker images for one project in one directory?": "There are two straightforward answers in pure-Docker space.\nThe first thing you can do is package all of your code into a single image:\nFROM python:3\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"./main.py\"]\nYou can override the command part at the docker run command or in a docker-compose.yml file, so run multiple containers off the same image.\nversion: '3'\nservices:\n  crawler:\n    build: .\n    command: ./crawl.py\n  indexer:\n    build: .\n    command: ./indexer.py\nOr, you can have multiple Dockerfiles in the same directory\nversion: '3'\nservices:\n  crawler:\n    build:\n      context: .\n      dockerfile: Dockerfile.crawler\n  indexer:\n    build:\n      context: .\n      dockerfile: Dockerfile.indexer\nand those Dockerfiles could be more limited\nFROM python:3\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY common common\nCOPY crawler crawler\nCOPY crawler.py .\nCMD [\"./crawler.py\"]\nWhich approach to use depends on your code base. If it's in an interpreted language, almost all of the code is shared, and the only real difference is the command you're running, then it's probably better to just have one image. But if the library dependencies are significantly different for each dependency, or if you need to build a separate binary for each process, a separate image for each makes more sense.",
    "Docker: \"not found\" for an existing file": "If app is a shell script, then the error refers to the first line of that script, e.g. #!/bin/bash if /bin/bash doesn't exist locally. Can also be windows linefeeds breaking that line.\nIf app is an executable, then the error is from linked libraries. Use ldd app to see what that binary is linked against. With alpine, the issue is often glibc.",
    "Docker-compose port range forward": "I agree that the docker-compose ports documentation does not provide sufficient info on the syntax for the range mapping of ports. To understand the syntax, check the docker run documentation on ports.\nIn particular,\n- \"20000-20100\" means: Expose the container ports in the range 20000 to 20100 into random ports on the host machine\n- \"10000-10100:20000-20100\" means: Expose the container ports in the range 20000 to 20100 into random ports on the host machine in the range of 10000 to 10100\n- \"20000-20100:20000-20100\" similar to the above\nIn your case, all these should allow you to access the containerized application",
    "Console.log not working in a dockerized Node.js / Express app": "Remove the -d from the command you're using to run the container:\ndocker run -p 49160:8080\nThe -d option runs the container in the background, so you won't be able to see its output in your console.\nIf you want to keep the container running in the background and you want to access that container's shell, you can run the following command once your container is up and running:\ndocker exec -it <container-name> bash",
    "dotnet docker /bin/sh: 1: [dotnet,: not found": "I faced similar issue. In my case the problem was in the entry point statement in Dockerfile. I was missing a comma between two parameters \"dotnet\" and \"url for dll\".\nBefore:\nCMD [\"dotnet\" \"url to dll\"]\nAfter fix applied:\nCMD [\"dotnet\", \"url for dll\"]",
    "Why I cannot get output of tail of a file created by docker build": "The same issue occurs for me using the overlay2 storage driver on the Docker 4.9.8-moby Alpine release.\nIt seems like CMD tail is opening the /var/log/mylog.log file from the overlay layer that RUN touch /var/log/mylog.log creates.\nWhen you append to the log, a \"new\" file is created in the topmost overlay layer that the container uses for any file system changes made on top of the image while running, and this new file is actually being appended to. tail is not able to pick up the changeover correctly though, with either -f or -F.\nThe docker start and docker stop resolves the problem as the tail process starts again after /var/log/mylog.log has been updated and is then pointing at the \"new\" file in the container overlay layer. Using a slightly different CMD would workaround the issue in a similar way:\nCMD [\"sh\", \"-c\", \"touch /var/log/mylog.log && tail -f /var/log/mylog.log\"]\nThe debian:testing image includes coreutils-8.26-2 with the fix for supporting overlays magic number to remove that warning message, but still exhibits the same behaviour.\nIt's most likely an overlay issue to be fixed in the kernel. coreutils might be able to work around the issue when using -F.\nWhat you are attempting is a bit of an edge case in Docker though. Containers that use tail as the foreground process usually complete a bunch of work in a script before running tail, which includes running the commands that create the log file to be tailed. Might be why not many people have picked this up.",
    "Configure Dockerfile to use impdp command when the container is created": "Ok, I have now figured how to make it happen, after much of experimenting,reading how the cmd works (finally) and the help/inputs provided by the above comments from other users.\nBasically Docker runs only one CMD (from the docs). So if I create a dockerfile from wnameless/oracle-xe-11g as\nFrom wnameless/oracle-xe-11g\n...\n...\nCMD [\"impdp\", \"....\"]\nthen this will inherently override the CMD command described by the wnameless/oracle-xe-11g's docker file.\nSo here are the steps to be done to achieve it\nStep 1: copy the CMD's executed from the parent image (from the Dockerfile)\nIn my case that would be\n/usr/sbin/startup.sh\nStep 2: append your own CMD to the above CMD using && operation.\nhere it would be\nbin/bash  -c \"/u01/app/oracle/product/11.2.0/xe/bin/impdp system/oracle NOLOGFILE=Y\nNote that you need to include the entire path of the impdp and the whole operation inside blockquotes\nStep 3: If the parent Dockerfile contains a background running process make sure that it goes in the last\nHere it would be\n/usr/sbin/sshd -D\nThe final output should be something like this\nCMD /usr/sbin/startup.sh \n&& bin/bash  -c \"/u01/app/oracle/product/11.2.0/xe/bin/impdp\nsystem/oracle NOLOGFILE=Y ...\" \n&& /usr/sbin/sshd -D\nThat's it. This should work\nOther things to keep in mind especially when using the above oracle dockerfile is you need to set the ENV for oracle_home and also export it to the bash.bashrc as this is not done by default.\n# Add env variables for oracle_home and related\nENV ORACLE_HOME=/u01/app/oracle/product/11.2.0/xe \\\nORACLE_SID=XE\n\n#Export oracle_home and related\nRUN echo 'export ORACLE_HOME=/u01/app/oracle/product/11.2.0/xe' >> etc/bash.bashrc\nRUN echo 'export PATH=$ORACLE_HOME/bin:$PATH' >> /etc/bash.bashrc\nRUN echo 'export ORACLE_SID=XE' >> /etc/bash.bashrc",
    "How to serve static files from a Dockerized Python web app?": "If you know your app will always-and-forever have the same static assets, then just containerize them with the app and be done with it.\nBut things change, so when you need it I would recommend a Docker Volume Container approach: put your static assets in a DVC and mount that DVC in the main container so it's all pretty much \"just one app container\". You could use Docker Compose something like this:\nappdata:\n    image: busybox\n    volumes:\n        - /path/to/app/static\n    command: echo \"I'm just a volume container\"\napp:\n    build: .\n    volumes_from:\n        - appdata\n    command: \u2026\nYou can expand further by starting your container with a bootstrap script that copies initial static files into the destination path on startup. That way your app is guaranteed to always have a default set to get started, but you can add more static files as the app grows. For an example of this, pull the official Jenkins container and read /usr/local/bin/jenkins.sh.",
    "How can I use a docker image saved as tar file in my Dockerfile as parent image": "If you have a docker save tarball, you need to docker load it before it can be used. When you do, that will print out the name(s) and tag(s) of the image(s) that were loaded. You can then use that in the FROM line of your Dockerfile, like any other local image.\n$ docker load -i myimage.tar.gz\n\nLoaded image: my/image:and-its-tag\n$ head -1 Dockerfile\nFROM my/image:and-its-tag\nIf you docker push or docker save the resulting image, it will have a complete copy of the original image.\n(In normal operation you shouldn't need docker save; prefer a registry service like Docker Hub, something cloud-hosted like GCR/ACR/ECR, or running your own. You can't really directly use the saved image tarfile for anything.)",
    "I can't install specific version (1.0.2g) of openssl in docker": "What base image do you use to build an image?\nIt works pretty fine with ubuntu:16.04 base image and the same Dockerfile you provided:\nFROM ubuntu:16.04\nRUN apt-get update\nRUN apt-get install -y build-essential cmake zlib1g-dev libcppunit-dev git subversion wget && rm -rf /var/lib/apt/lists/*\n\nRUN wget https://www.openssl.org/source/openssl-1.0.2g.tar.gz -O - | tar -xz\nWORKDIR /openssl-1.0.2g\nRUN ./config --prefix=/usr/local/openssl --openssldir=/usr/local/openssl && make && make install",
    "Running simple Java Gradle app in Docker": "The fix was to specify --chown=gradle permissions on the /code directory in the Dockerfile. Many Docker images are designed to run as root, the base Gradle image runs as user gradle.\nFROM gradle:4.3-jdk-alpine\nADD --chown=gradle . /code\nWORKDIR /code\nCMD [\"gradle\", \"--stacktrace\", \"run\"]\nEthan Davis suggested using /home/gradle rather than code. That would probably work as well, but I didn't think of that.\nThe docker image maintainer should have a simple getting started type reference example that shows the recommended way to get basic usage.",
    "Secure Admin must be enabled to access the DAS remotely - Acess Glassfish Admin Console with Docker": "There are a couple of ways to do this, but the best way is probably to copy the method used in the Payara Server dockerfile. (Payara Server is derived from GlassFish and therefore the dockerfile is compatible with GlassFish too)\nTo summarise, this method creates 2 files: a tmpfile which contains the default (empty) password and the desired new password, and a pwdfile which contains just the newly changed file.\nIf the contents of the tmpfile are:\nAS_ADMIN_PASSWORD=\nAS_ADMIN_NEWPASSWORD=MyNewPassword\nThen the contents of pwdfile should be:\nAS_ADMIN_PASSWORD=MyNewPassword\nto change the password using asadmin, the first file must be used with the change-admin-password command, and the second with all future commands.\nIn docker terms, this looks like this (taken directly from the dockerfile linked above):\nENV PAYARA_PATH /opt/payara41\nENV ADMIN_USER admin\nENV ADMIN_PASSWORD admin\n\n# set credentials to admin/admin \n\nRUN echo 'AS_ADMIN_PASSWORD=\\n\\\nAS_ADMIN_NEWPASSWORD='$ADMIN_PASSWORD'\\n\\\nEOF\\n'\\\n>> /opt/tmpfile\n\nRUN echo 'AS_ADMIN_PASSWORD='$ADMIN_PASSWORD'\\n\\\nEOF\\n'\\\n>> /opt/pwdfile\n\nRUN \\\n $PAYARA_PATH/bin/asadmin start-domain && \\\n $PAYARA_PATH/bin/asadmin --user $ADMIN_USER --passwordfile=/opt/tmpfile change-admin-password && \\\n $PAYARA_PATH/bin/asadmin --user $ADMIN_USER --passwordfile=/opt/pwdfile enable-secure-admin && \\\n $PAYARA_PATH/bin/asadmin restart-domain\n\n# cleanup\nRUN rm /opt/tmpfile",
    "jar file with arguments in docker": "Set the jar file as your entrypoint and the args as your command\nAn example:\nENTRYPOINT [\"/path/to/my/java.jar\"]\nCMD [\"my\", \"default\", \"args\"]\nYou can then override the args whenever you run the container, using:\ndocker run <my-docker-image> some custom args\nMore information here: http://goinbigdata.com/docker-run-vs-cmd-vs-entrypoint/",
    "How to use COPY command in Docker build?": "COPY copies files from your host filesystem into the container. It looks like you want to copy files from one directory in the container to another. For that you need to use RUN and cp\nRUN cp -r built/* /data/\nSince you will be removing the /tempDir/ directory, you can speed things up a bit by renaming the directory:\nRUN mv built /data\nThis way you don't have to copy data around and then delete the originals.",
    "Docker: how to execute a batch file when container starts and keep the user in cmd / session": "instead of the ENTRYPOINT you can try putting something like this in your Dockerfile:\nCMD C:\\init\\init.bat && cmd",
    "creating a docker image with nginx compile options for Optional HTTP modules": "I'm somewhat of a noob with Docker, but I had to solve this same problem. I used this Dockerfile as a starting point.\nFROM centos:centos7\n\nWORKDIR /tmp\n\n# Install prerequisites for Nginx compile\nRUN yum install -y \\\n        wget \\\n        tar \\\n        openssl-devel \\\n        gcc \\\n        gcc-c++ \\\n        make \\\n        zlib-devel \\\n        pcre-devel \\\n        gd-devel \\\n        krb5-devel \\\n    openldap-devel \\\n        git\n\n# Download Nginx and Nginx modules source\nRUN wget http://nginx.org/download/nginx-1.9.3.tar.gz -O nginx.tar.gz && \\\n    mkdir /tmp/nginx && \\\n    tar -xzvf nginx.tar.gz -C /tmp/nginx --strip-components=1 &&\\\n    git clone https://github.com/kvspb/nginx-auth-ldap.git /tmp/nginx/nginx-auth-ldap\n\n# Build Nginx\nWORKDIR /tmp/nginx\nRUN ./configure \\\n        --user=nginx \\\n        --with-debug \\\n        --group=nginx \\\n        --prefix=/usr/share/nginx \\\n        --sbin-path=/usr/sbin/nginx \\\n        --conf-path=/etc/nginx/nginx.conf \\\n        --pid-path=/run/nginx.pid \\\n        --lock-path=/run/lock/subsys/nginx \\\n        --error-log-path=/var/log/nginx/error.log \\\n        --http-log-path=/var/log/nginx/access.log \\\n        --with-http_gzip_static_module \\\n        --with-http_stub_status_module \\\n        --with-http_ssl_module \\\n        --with-http_spdy_module \\\n        --with-pcre \\\n        --with-http_image_filter_module \\\n        --with-file-aio \\\n        --with-ipv6 \\\n        --with-http_dav_module \\\n        --with-http_flv_module \\\n        --with-http_mp4_module \\\n        --with-http_gunzip_module \\\n        --add-module=nginx-auth-ldap && \\\n    make && \\\n    make install\n\nWORKDIR /tmp\n\n# Add nginx user\nRUN adduser -c \"Nginx user\" nginx && \\\n    setcap cap_net_bind_service=ep /usr/sbin/nginx\n\nRUN touch /run/nginx.pid\n\nRUN chown nginx:nginx /etc/nginx /etc/nginx/nginx.conf /var/log/nginx /usr/share/nginx /run/nginx.pid\n\n# Cleanup after Nginx build\nRUN yum remove -y \\\n        wget \\\n        tar \\\n        gcc \\\n        gcc-c++ \\\n        make \\\n        git && \\\n    yum autoremove -y && \\\n    rm -rf /tmp/*\n\n# PORTS\nEXPOSE 80\nEXPOSE 443\n\nUSER nginx\nCMD [\"/usr/sbin/nginx\", \"-g\", \"daemon off;\"]",
    "Dockerfile issue - Why is the binary dlv not being found - No such file or directory": "You built dlv in alpine-based distro. dlv executable is linked against libc.musl:\n# ldd dlv \n        linux-vdso.so.1 (0x00007ffcd251d000)\n        libc.musl-x86_64.so.1 => not found\nBut then you switched to glibc-based image debian:buster-slim. That image doesn't have the required libraries.\n# find / -name libc.musl*                                        \n<nothing found>\nThat's why you can't execute dlv - the dynamic linker fails to find the proper lib.\nYou need to build in glibc-based docker. For example, replace the first line\nFROM golang:bullseye AS builder\nBTW. After you build you need to run the container in the priviledged mode\n$ docker build . -t try-dlv\n...\n$ docker run --privileged --rm try-dlv\nAPI server listening at: [::]:40000\n2022-10-30T10:51:02Z warning layer=rpc Listening for remote connections (connections are not authenticated nor encrypted)\nIn non-priviledged container dlv is not allowed to spawn a child process.\n$ docker run --rm try-dlv\nAPI server listening at: [::]:40000\n2022-10-30T10:55:46Z warning layer=rpc Listening for remote connections (connections are not authenticated nor encrypted)\ncould not launch process: fork/exec /app/fooapp: operation not permitted\nReally Minimal Image\nYou use debian:buster-slim to minimize the image, it's size is 80 MB. But if you need a really small image, use busybox, it is only 4.86 MB overhead.\nFROM golang:bullseye AS builder\n\n# Build Delve for debugging\nRUN go install github.com/go-delve/delve/cmd/dlv@latest\n\n# Create and change to the app directory.\nWORKDIR /app\nENV CGO_ENABLED=0\n\n# Retrieve application dependencies.\nCOPY go.* ./\nRUN go mod download\n\n# Copy local code to the container image.\nCOPY . ./\n\n# Build the binary.\nRUN go build -o fooapp .\n\n# Download certificates\nRUN set -x && apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n    ca-certificates \n\n# Use the official Debian slim image for a lean production container.\nFROM busybox:glibc\n\nEXPOSE 8000 40000\n\n# Copy the binary to the production image from the builder stage.\nCOPY --from=builder /app/fooapp /app/fooapp \n# COPY --from=builder /app/ /app\n\nCOPY --from=builder /go/bin/dlv /dlv\n\nCOPY --from=builder /etc/ssl /etc/ssl\n\n# Run dlv as pass fooapp as parameter\nCMD [\"/dlv\", \"--listen=:40000\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"/app/fooapp\"]\n# ENTRYPOINT [\"/bin/sh\"]\nThe image size is 25 MB, of which 18 MB are from dlv and 2 MB are from Hello World application.\nWhile choosing the images care should be taken to have the same flavors of libc. golang:bullseye links against glibc. Hence, the minimal image must be glibc-based.\nBut if you want a bit more comfort, use alpine with gcompat package installed. It is a reasonably rich linux with lots of external packages for just extra 6 MB compared to busybox.\nFROM golang:bullseye AS builder\n\n# Build Delve for debugging\nRUN go install github.com/go-delve/delve/cmd/dlv@latest\n\n# Create and change to the app directory.\nWORKDIR /app\nENV CGO_ENABLED=0\n\n# Copy local code to the container image.\nCOPY . ./\n\n# Retrieve application dependencies.\nRUN go mod tidy\n\n# Build the binary.\nRUN go build -o fooapp .\n\n# Use alpine lean production container.\n# FROM busybox:glibc\nFROM alpine:latest\n\n# gcompat is the package to glibc-based apps\n# ca-certificates contains trusted TLS CA certs\n# bash is just for the comfort, I hate /bin/sh\nRUN apk add gcompat ca-certificates bash\n\nEXPOSE 8000 40000\n\n# Copy the binary to the production image from the builder stage.\nCOPY --from=builder /app/fooapp /app/fooapp \n# COPY --from=builder /app/ /app\n\nCOPY --from=builder /go/bin/dlv /dlv\n\n# Run dlv as pass fooapp as parameter\nCMD [\"/dlv\", \"--listen=:40000\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"/app/fooapp\"]\n# ENTRYPOINT [\"/bin/bash\"]",
    "Docker is pushing all layers instead of the last one": "Docker invalidates COPY layers once the context changes -- regardless of what the next steps actually depend on. Copy files at the last possible moment -- in your case, copy requirements.txt first, and the rest later. Like this:\nFROM dr_prof_patrick/my_app:my_app_base_image\n\nWORKDIR /my_app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python3\", \"main.py\"]\nAlso take a look at your .dockerignore and don't copy useless files. The best strategy I see used is to use .dockerignore as a whitelist, not a blacklist, by ignoring everything first and then un-ignoring the files you need:\n*\n!requirements.txt",
    "Use GPU on python docker image": "TensorFlow image split into several 'partial' Dockerfiles. One of them contains all dependencies TensorFlow needs to operate on GPU. Using it you can easily create a custom image, you only need to change default python to whatever version you need. This seem to me a much easier job than bringing NVIDIA's stuff into Debian image (which AFAIK is not officially supported for CUDA and/or cuDNN).\nHere's the Dockerfile:\n# TensorFlow image base written by TensorFlow authors.\n# Source: https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/tools/dockerfiles/partials/ubuntu/nvidia.partial.Dockerfile\n# -------------------------------------------------------------------------\nARG ARCH=\nARG CUDA=10.1\nFROM nvidia/cuda${ARCH:+-$ARCH}:${CUDA}-base-ubuntu${UBUNTU_VERSION} as base\n# ARCH and CUDA are specified again because the FROM directive resets ARGs\n# (but their default value is retained if set previously)\nARG ARCH\nARG CUDA\nARG CUDNN=7.6.4.38-1\nARG CUDNN_MAJOR_VERSION=7\nARG LIB_DIR_PREFIX=x86_64\nARG LIBNVINFER=6.0.1-1\nARG LIBNVINFER_MAJOR_VERSION=6\n\n# Needed for string substitution\nSHELL [\"/bin/bash\", \"-c\"]\n# Pick up some TF dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        build-essential \\\n        cuda-command-line-tools-${CUDA/./-} \\\n        # There appears to be a regression in libcublas10=10.2.2.89-1 which\n        # prevents cublas from initializing in TF. See\n        # https://github.com/tensorflow/tensorflow/issues/9489#issuecomment-562394257\n        libcublas10=10.2.1.243-1 \\ \n        cuda-nvrtc-${CUDA/./-} \\\n        cuda-cufft-${CUDA/./-} \\\n        cuda-curand-${CUDA/./-} \\\n        cuda-cusolver-${CUDA/./-} \\\n        cuda-cusparse-${CUDA/./-} \\\n        curl \\\n        libcudnn7=${CUDNN}+cuda${CUDA} \\\n        libfreetype6-dev \\\n        libhdf5-serial-dev \\\n        libzmq3-dev \\\n        pkg-config \\\n        software-properties-common \\\n        unzip\n\n# Install TensorRT if not building for PowerPC\nRUN [[ \"${ARCH}\" = \"ppc64le\" ]] || { apt-get update && \\\n        apt-get install -y --no-install-recommends libnvinfer${LIBNVINFER_MAJOR_VERSION}=${LIBNVINFER}+cuda${CUDA} \\\n        libnvinfer-plugin${LIBNVINFER_MAJOR_VERSION}=${LIBNVINFER}+cuda${CUDA} \\\n        && apt-get clean \\\n        && rm -rf /var/lib/apt/lists/*; }\n\n# For CUDA profiling, TensorFlow requires CUPTI.\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n# Link the libcuda stub to the location where tensorflow is searching for it and reconfigure\n# dynamic linker run-time bindings\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 \\\n    && echo \"/usr/local/cuda/lib64/stubs\" > /etc/ld.so.conf.d/z-cuda-stubs.conf \\\n    && ldconfig\n# -------------------------------------------------------------------------\n#\n# Custom part\nFROM base\nARG PYTHON_VERSION=3.7\n\nRUN apt-get update && apt-get install -y --no-install-recommends --no-install-suggests \\\n          python${PYTHON_VERSION} \\\n          python3-pip \\\n          python${PYTHON_VERSION}-dev \\\n# Change default python\n    && cd /usr/bin \\\n    && ln -sf python${PYTHON_VERSION}         python3 \\\n    && ln -sf python${PYTHON_VERSION}m        python3m \\\n    && ln -sf python${PYTHON_VERSION}-config  python3-config \\\n    && ln -sf python${PYTHON_VERSION}m-config python3m-config \\\n    && ln -sf python3                         /usr/bin/python \\\n# Update pip and add common packages\n    && python -m pip install --upgrade pip \\\n    && python -m pip install --upgrade \\\n        setuptools \\\n        wheel \\\n        six \\\n# Cleanup\n    && apt-get clean \\\n    && rm -rf $HOME/.cache/pip\nYou can take from here: change python version to one you need (and which is available in Ubuntu repositories), add packages, code, etc.",
    "Running Jest test with Dockerfile": "RUN and CMD aren't commands, they're instructions to tell Docker what do when building your container. So e.g.:\nRUN if [ \"$runTests\" = \"True\" ]; then \\\n    RUN npm test; fi\ndoesn't make sense, RUN <command> runs a shell command but RUN isn't defined in the shell, it should just be:\nARG runTests  # you need to define the argument too\nRUN if [ \"$runTests\" = \"True\" ]; then \\\n    npm test; fi\nThe cleaner way to do this is to set up npm as the entrypoint, and start as the specific command:\nENTRYPOINT [ \"npm\" ]\nCMD [ \"start\" ]\nThis allows you to build the container normally, it doesn't require any build arguments, then run an NPM script other than start in the container, e.g. to run npm test:\ndocker run <image> test\nHowever, note that this means all of the dev dependencies need to be in the container. It looks (from ENV NODE_ENV=production) like you intend this to be a production build, so you shouldn't be running the tests in the container at all. Also despite having as builder this isn't really a multi-stage build. The idiomatic script for this would be something like:\n# stage 1: copy the source and build the app\n\nFROM node:10-alpine as builder\nARG TOKEN\n\nWORKDIR /app\n\nCOPY .npmrc-pipeline .npmrc\n\nCOPY package*.json ./\nRUN npm ci\n\nCOPY . .\nRUN npm run build\n\n# stage 2: copy the output and run it in production\n\nFROM node:10-alpine\n\nWORKDIR /app\n\nENV PORT=3000\nENV NODE_ENV=production\n\nCOPY --from=builder /app/package*.json ./\nRUN npm ci\n\nCOPY --from=builder /* your build output */\n\nEXPOSE 3000\n\nENTRYPOINT [ \"npm\" ]\nCMD [ \"start\" ]\nSee e.g. this Dockerfile I put together for a full-stack React/Express app.",
    "Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections": "You can't access service which on the host using localhost from container, you had to use the ip address of your host to access.\nThis because: default docker will use bridge which will setup a internal network for your container, so when container use localhost, it doesn't mean the host, it mean the container self's network.\nIf insist on, a ugly solution is use --net=host.\nSomething like next:\nsudo docker run --net=host -p 8080:8080 -t djtijare/a2i-web:v1\nThen you can use localhost to visit host's service from container.",
    "Docker RUN multiple instance of a image with different parameters": "Docker containers are started with an entrypoint and a command; when the container actually starts they are simply concatenated together. If the ENTRYPOINT in the Dockerfile is structured like a single command then the CMD in the Dockerfile or command: in the docker-compose.yml contains arguments to it.\nThis means you should be able to set up your docker-compose.yml as:\nservices:\n  my.app1:\n    image: ${DOCKER_REGISTRY}my/app\n    ports:\n     - 5000:80\n    command: [80, db1.db]\n  my.app2:\n    image: ${DOCKER_REGISTRY}my/app\n    ports:\n     - 5001:80\n    command: [80, db2.db]\n(As a side note: if one of the options to the program is the port to listen on, this needs to match the second port in the ports: specification, and in my example I've chosen to have both listen on the \"normal\" HTTP port and remap it on the hosts using the ports: setting. One container could reach the other, if it needed to, as http://my.app2/ on the default HTTP port.)",
    "How to run copy command via docker RUN": "This is because RUN cp ... is not the same as COPY. COPY copies files from your host machine to the image, RUN runs inside the container during it's build process, and fails because there really is \"No such file or directory\" in there.\nAnd looking at the COPY documentation, there really is not a way to copy multiple files to multiple destinations with one COPY, only multiple sources to one destination.\nWhat you probably can do, if you really want, is to COPY everything first to one directory, for example /tmp, and then use the RUN cp /tmp/rdkafka.ini /etc/php/7.0/mods-available/ && ....",
    "Why && rather than a new RUN": "It is optimisation for docker image layer. I also recommend to read Best practices for writing Dockerfiles\nThere is also interesting presentation from DockerCon EU 2017.",
    "Advantage of using docker-compose file version 3 over a shellscript?": "Readability\nCompare your sample shell script to a YAML version of same:\nservices:\n  api_cntr:\n    image: api_img\n    network: net1\n    ports:\n      - 5000:5000\n  message_service:\n    image: redis\n    network: net1\n    ports:\n      - 6379:6379\n  celery_worker1:\n    image: celery_worker_img\n    network: net1\n  flower_hud:\n    image: flower_hud_img\n    network: net1\n    ports:\n      - 5555:5555    \nTo my eye at least, it is much easier to determine the overall architecture of the application from reading the YAML than from reading the shell commands.\nCleanup\nIf you use docker-compose, then running docker-compose down will stop and clean up everything, remove the network, etc. To do that in your shell script, you'd have to separately write a remove section to stop and remove all the containers and the network.\nMultiple inheriting YAML files\nIn some cases, such as for dev & testing, you might want to have a main YAML file and another that overrides certain values for dev/test work.\nFor instance, I have an application where I have a docker-compose.yml as well as docker-compose.dev.yml. The first contains all of the production settings for my app. But the \"dev\" version has a more limited set of things. It uses the same service names, but with a few differences.\nAdds a mount of my code directory into the container, overriding the version of the code that was built into the image\nExposes the postgres port externally (so I can connect to it for debugging purposes) - this is not exposed in production\nUses another mount to fake a user database so I can easily have some test users without wiring things up to my real authentication server just for development\nNormally the service only uses docker-compose.yml (in production). But when I am doing development work, I run it like this:\ndocker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d\nIt will load the normal parameters from docker-compose.yml first, then read docker-compose.dev.yml second, and override only the parameters found in the dev file. The other parameters are all preserved from the production version. But I don't require two completely separate YAML files where I might need to change the same parameters in both.\nEase of maintenance\nEverything I described in the last few paragraphs can be done using shell scripts. It's just more work to do it that way, and probably more difficult to maintain, and more prone to mistakes.\nYou could make it easier by having your shell scripts read a config file and such... but at some point you have to ask if you are just reimplementing your own version of docker-compose, and whether that is worthwhile to you.",
    "How to use a python library that is constantly changing in a docker image or new container?": "During development it is IMO perfectly fine to map/mount the hostdirectory with your ever changing sources into the Docker container. The rest (the python version, the other libraries you are dependent upon you can all install in the normal way in the the docker container.\nOnce stabilized I remove the map/mount and add the package to the list of items to install with pip. I do have a separate container running devpi so I can pip-install packages whether I push them all the way to PyPI or just push them to my local devpi container.\nDoing speed up container creation even if you use the common (but more limited) python [path_to_project/setup.py] develop. Your Dockerfile in this case should look like:\n # the following seldom changes, only when a package is added to setup.py\n COPY /some/older/version/of/project/plus/dependent/packages /older/setup\n RUN pip /older/setup/your_package.tar.gz\n\n # the following changes all the time, but that is only a small amount of work\n COPY /latest/version/of/project     \n RUN python [path_to_project/setup.py] develop\nIf the first copy would result in changes to files under /older/setup then the container gets rebuilt from there.\nRunning python ... develop still makes more time and you need to rebuild/restart the container. Since my packages all can also be just copied in/linked to (in addition to be installed) that is still a large overhead. I run a small program in the container that checks if the (mounted/mapped) sources change and then reruns anything I am developing/testing automatically. So I only have to save a new version and watch the output of the container.",
    "Where does dockerized jetty store its logs?": "Why you aren't seeing logs\n2 things to note:\nRunning docker run -it jetty bash will start a new container instead of connecting you to your existing daemonized container.\nAnd it would invoke bash instead of starting jetty in that container, so it won't help you to get logs from either container.\nSo this interactive container won't help you in any case.\nBut also...\nJettyLogs are disabled anyways\nAlso, you won't see the logs in the standard location (say, if you tried to use docker exec to read the logs, or to get them in a volume), quite simply because the Jetty Docker file is aptly disabling logging entirely.\nIf you look at the jetty:9.2.10 Dockerfile, you will see this line:\n&& sed -i '/jetty-logging/d' etc/jetty.conf \\\nWhich nicely removes the entire line referencing the jetty-logging.xml default logging configuration.\nWhat to do then?\nReading logs with docker logs\nDocker gives you access to the container's standard output.\nAfter you did this:\ndocker run --name='abc' -d -p 10908:8080 -v /var/log/abc:/var/log/jetty me/abc:latest\nYou can simply do this:\ndocker logs abc\nAnd be greeted with somethig similar to this:\nRunning Jetty: \n2015-05-15 13:33:00.729:INFO::main: Logging initialized @2295ms\n2015-05-15 13:33:02.035:INFO:oejs.SetUIDListener:main: Setting umask=02\n2015-05-15 13:33:02.102:INFO:oejs.SetUIDListener:main: Opened ServerConnector@73ec519{HTTP/1.1}{0.0.0.0:8080}\n2015-05-15 13:33:02.102:INFO:oejs.SetUIDListener:main: Setting GID=999\n2015-05-15 13:33:02.106:INFO:oejs.SetUIDListener:main: Setting UID=999\n2015-05-15 13:33:02.133:INFO:oejs.Server:main: jetty-9.2.10.v20150310\n2015-05-15 13:33:02.170:INFO:oejdp.ScanningAppProvider:main: Deployment monitor [file:/var/lib/jetty/webapps/] at interval 1\n2015-05-15 13:33:02.218:INFO:oejs.ServerConnector:main: Started ServerConnector@73ec519{HTTP/1.1}{0.0.0.0:8080}\n2015-05-15 13:33:02.219:INFO:oejs.Server:main: Started @3785ms\nUse docker help logs for more details.\nCustomize\nObviously your other option is to revert what the default Dockerfile for jetty is doing, or to create your own dockerized Jetty.",
    "docker-compose Equivalent to Docker Build --secret Argument": "Turns out I was a bit ahead of the times. docker compose v.2.5.0 brings support for secrets.\nAfter having modified the Dockerfile as explained above, we must then update the docker-compose to defined secrets.\ndocker-compose.yml\nservices:\n  my-cool-app:\n    build:\n      context: .\n      secrets:\n        - github_user\n        - github_token\n...\nsecrets:\n  github_user:\n    file: secrets_github_user\n  github_token:\n    file: secrets_github_token\nBut where are those files secrets_github_user and secrets_github_token coming from? In your CI you also need to export the environment variable and save it to the default secrets file location. In our project we are using Tasks so we added these too lines.\nNote that we are running this task from our CI, so you could do it differently without Tasks for example.\n- printenv GITHUB_USER > /root/project/secrets_github_user\n- printenv GITHUB_TOKEN > /root/project/secrets_github_token\nWe then update the CircleCI config and add two environment variable to our job:\n.config.yml\n  name-of-our-job:\n    environment:\n      DOCKER_BUILDKIT: 1\n      COMPOSE_DOCKER_CLI_BUILD: 1\nYou might also need a more recent Docker version, I think they introduced it in a late 19 release or early 20. I have used this and it works:\n    steps:\n      - setup_remote_docker:\n          version: 20.10.11\nNow when running your docker-compose based commands, the secrets should be successfully mounted through docker-compose and available to correctly build or run your Dockerfile instructions!",
    "invalid empty ssh agent socket, make sure SSH_AUTH_SOCK is set How to set SSH_AUTH_SOCK for docker build?": "It seems that either the --ssh argument doesn't accept empty values as an argument.\neval $(ssh-agent)\nset \"DOCKER_BUILDKIT=1\" && docker build --ssh default=${SSH_AUTH_SOCK} -f docker/Dockerfile -t basketball_backend_api_core .\nor you may need to run ssh-add to add private key identities to the authentication agent first for this to work.\nbefore_script:\n  ##\n  ## Install ssh-agent if not already installed, it is required by Docker.\n  ## (change apt-get to yum if you use an RPM-based image)\n  ##\n  - 'command -v ssh-agent >/dev/null || ( apt-get update -y && apt-get install openssh-client -y )'\n\n  ##\n  ## Run ssh-agent (inside the build environment)\n  ##\n  - eval $(ssh-agent -s)\n\n  ##\n  ## Add the SSH key stored in SSH_PRIVATE_KEY variable to the agent store\n  ## We're using tr to fix line endings which makes ed25519 keys work\n  ## without extra base64 encoding.\n  ## https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556\n  ##\n  - echo \"$SSH_PRIVATE_KEY\" | tr -d '\\r' | ssh-add -\nfrom Gitlab's docs.",
    "dial tcp 127.0.0.1:8080: connect: connection refused. go docker app": "For communicating between multiple docker-compose clients, you need to make sure that the containers you want to talk to each other are on the same network.\nFor example, (edited for brevity) here you have one of the docker-compose.yml\n# sport_app docker-compose.yml\nversion: '3'\nservices:\n  go-sports-entities-hierarchy:\n    ...\n    networks:\n      - some-net\n  go-sports-events-workflow\n    ...\n    networks:\n      - some-net\nnetworks:\n  some-net:\n    driver: bridge\nAnd the other docker-compose.yml\n# user_management app docker-compose.yml\nversion: '3'\nservices:\n  postgres:\n    ...\n    networks:\n      - some-net\n  go-user-management\n    ...\n    networks:\n      - some-net\nnetworks:\n  some-net:\n    external: true\nNote: Your app\u2019s network is given a name based on the project name, which is based on the name of the directory it lives in, in this case a prefix user_ was added.\nThey can then talk to each other using the service name, i.e. go-user-management, etc.\nYou can, after running the docker-compose up --build commands, run the docker network ls command to see it, then docker network inspect bridge, etc.",
    "How do I detect the interactive flag in a container?": "At build time, you won't know what the runtime environment will look like. So there's no way to modify the RUN statements based on the -it flags or lack there of, we don't have time travel to look into the future for that, and the same image could be run multiple times with different flags.\nYou could do this within the entrypoint script. /dev/stdin which is mapped for interactive containers points to /proc/self/fd/0 which will point to different things based on whether you have -i, -it, or neither:\n$ docker run -it --rm busybox ls -al /dev/stdin\nlrwxrwxrwx    1 root     root            15 Jun 18 14:43 /dev/stdin -> /proc/self/fd/0\n\n$ docker run -it --rm busybox ls -al /proc/self/fd/0\nlrwx------    1 root     root            64 Jun 18 14:43 /proc/self/fd/0 -> /dev/pts/0\n\n$ docker run -i --rm busybox ls -al /proc/self/fd/0\nlr-x------    1 root     root            64 Jun 18 14:50 /proc/self/fd/0 -> pipe:[9858991]\n\n$ docker run --rm busybox ls -al /proc/self/fd/0\nlrwx------    1 root     root            64 Jun 18 14:43 /proc/self/fd/0 -> /dev/null\nSo if you stat the link to see if it's going to /dev/null or not, that would give you the answer to the question you asked.\nSide note, if you only care about the -t, there's another check in shell you can run, [ -t 0 ], e.g.:\n$ docker run -it --rm busybox /bin/sh -c 'if [ -t 0 ]; then echo tty; else echo no tty; fi'\ntty\n\n$ docker run -i --rm busybox /bin/sh -c 'if [ -t 0 ]; then echo tty; else echo no tty; fi'\nno tty\n\n$ docker run --rm busybox /bin/sh -c 'if [ -t 0 ]; then echo tty; else echo no tty; fi'\nno tty\nHowever, best practice with docker is to run the app itself in the foreground, as pid 1. And if you need to enter the container for debugging in a development environment, use docker exec for that. Your entrypoint then becomes:\nENTRYPOINT [ \"/usr/local/apache-tomcat-v8.5.55/bin/catalina.sh\", \"run\" ]\nAnd by doing that, you avoid an extra shell in pid 1 that could block container stop signals from gracefully stopping the app.\nThen to debug, you'd exec after the run, using the container name:\ndocker container run -dp 8080:8080 -n tomcat-app <mydockername>\ndocker container exec -it tomcat-app /bin/sh",
    "Access docker within Dockerfile?": "No, you can't do this.\nYou need access to your host's Docker socket somehow. In a standalone docker run command you'd do something like docker run -v /var/run/docker.sock:/var/run/docker.sock, but there's no way to pass that option (or any other volume mount) into docker build.\nFor running unit-type tests (that don't have external dependencies) I'd just run them in your development or core CI build environment, outside of Docker, and run run docker build until they pass. For integration-type tests (that do) you need to set up those dependencies, maybe with a Docker Compose file, which again will be easier to do outside of Docker. This also avoids needing to build your test code and its additional dependencies into your image.\n(Technically there are two ways around this. The easier of the two is the massive security disaster that is opening up a TCP-based Docker socket; then your Dockerfile could connect to that [\"remote\"] Docker daemon and launch containers, stop them, kill itself off, impersonate the host for inbound SSH connections, launch a bitcoin miner that lives beyond the container build, etc...actually it allows any process on the host to do any of these things. The much harder, as @RaynalGobel suggests in a comment, is to try to launch a separate Docker daemon inside the container; the DinD image link there points out that it requires a --privileged container, which again you can't have at build time.)",
    "Docker-compose: Set a variable in env file and use it in Dockerfile": "You need to pass the build argument in docker compose\nversion '2'\n\nservices:\n    php:\n        build: \n          dockerfile: php7-fpm\n          args:\n            TIMEZONE: ${TIMEZONE}\n        volumes:\n            - ${APP_PATH}:/var/www/app\n            - ./logs:/var/www/logs\nThe environment are passed to the running container and not to the buildfile. For the you need to pass args in the build section",
    "Installing specific version of node.js and npm in ubuntu image with Dockerfile": "You can just follow the usual Ubuntu install instructions, just within the RUN statement in your Dockerfile\nRUN curl -sL https://deb.nodesource.com/setup_6.x | bash - \\\n    && apt-get install -y nodejs\nDocs",
    "How to create a copy of exisiting docker image": "Simply write a Dockerfile starting with:\nFROM debian:latest\n...\n(using the FROM directive)\nThat will create a local image based on debian, and since debian is already downloaded, it won't be downloaded again.\nNote: it is best to avoid the \"latest\" tag: see \"Docker: The latest Confusion\" by Stack Overflow contributor Adrian Mouat.\nUsing actual labels is more precise:\ndocker pull debian:7.8\ndocker pull debian:wheezy\nIf wanted to do something in ubuntu is there a way when: I just execute command docker copy \"image_name\" and then do whatever I want to (run image, clone some git repo, install some packages, test it) , and then just delete it docker rmi \"image_name\" (when I'm done with image) .\nYes: you can docker run --it <image> bash (for images which includes bash), and exit that bash: your container will be exited: you can then docker commit <containerrid> newimage, and you will get a copy of the original image.",
    "Docker - Multiple duplicate volume declarations; what happens?": "Take a look at https://docs.docker.com/reference/builder/#volume - the VOLUME command is declaring a mount point so it can be used by other hosts with the --volumes-from as well the VOLUME command tells docker that the contents of this directory is external to the image. While the -v /dir1/:/dir2/ will mount dir1 from the host into the running container at dir2 location.\nIn other words, you can use both together and docker will mount the -v properly.",
    "Docker bundle install cache issues when updating gems": "I found two possible solutions that use external data volume for gem storage: one and two.\nBriefly,\nyou specify an image that is used to store gems only\nin your app images, in docker-compose.yml you specify the mount point for BUNDLE_PATH via volumes_from.\nwhen your app container starts up, it executes bundle check || bundle install and things are good to go.\nThis is one possible solution, however to me it feels like it goes slightly against the docker way. Specifically, bundle install to me sounds like it should be part of the build process and shouldn't be part of the runtime. Other things, that depend on the bundle install like asset:precompile are now a runtime task as well.\nThis is a vaiable solution but I'm looking forward to something a little more robust.",
    "forward udp multicast from eth0 to docker0": "After a lot of frustrating days of trying out a number of things... finally something worked:\nUsing Pipework (https://github.com/jpetazzo/pipework), the following command worked but there is a catch -\npipework eth2 $(docker run -d hipache /usr/sbin/hipache) 50.19.169.157/24\nrunning a docker container by only running the above command did not quite help me. I had to run tcpdump -i eth2 on my host to capture packets on eth2 interface, which then started to forward the packets to the docker container.\nAny idea why is worked and not just running the command??",
    "How to run a shell script using dockerfiles CMD": "A Docker container will stop when its main process completes. In your case, this means the two Java applications will be forked to the background (because of the nohup call) then the script will immediately complete and the container will exit.\nThere are a few solutions:\nThe quickest and easiest solution is to just remove nohup call from the second java call. That way the script won't exit until the second Java application exits.\nUse a process manager such as runit or supervisord to manage the processes.\nPut the jars in separate containers and call Java directly (this would seem to be the best solution to me).",
    "How to use docker to test multiple compiler versions": "I would separate the parts of preparing the compiler and doing the calculation, so the source doesn't become part of the docker container.\nPrepare Compiler\nFor preparing the compiler I would take the ARG approach but without copying the data into the container. In case you wanna fast retry while having enough resources you could spin up multiple instances the same time.\nARG COMPILER=gcc:4.8\nFROM ${COMPILER}\nENV DEBIAN_FRONTEND noninteractive\n\n# Install tools (cmake, ninja, etc)\n# this will cause bloat if the FROM layer changes\nRUN <<EOF\n  apt update\n  apt install -y cmake ninja-build\n  rm -rf /var/lib/apt/lists/*\nEOF\n\n# Set the work directory\nVOLUME /src\nWORKDIR /src\nCMD [\"cmake\"]\nBuild it\nHere you have few options. You could either prepare a volume with the sources or use bind mounts together with docker exec like this:\n#bash style \nfor compiler in gcc:4.9 gcc:4.8 gcc:5.1\ndo \n  docker build -t mytag-${compiler} --build-arg COMPILER=${compiler} .\n  # place to clean the target folder\n  docker run -v $(pwd)/src:/src  mytag-${compiler} \ndone\nAnd because the source is not part of the docker image you don't have bloat. You can also have two mounts, one for a readonly source tree and one for the output files.\nNote: If you remove the CMake command you could also spin up the docker containers in parallel and use docker exec to start the build. The downside of this is that you have to take care of out of source builds to avoid clashes on the output folder.",
    "In Dockerfile, COPY all contents of current directory except one directory": "I don't think there is an easy solution to this problem.\nIf you need vendor for RUN composer install and you're not using a multistage build then it doesn't matter if you remove the vendor folder in the copy command. If you've copied it into the build earlier then it's going to be present in your final image, even if you don't copy it over in your COPY step.\nOne way to get around this is with multi-stage builds, like so:\nFROM debian as base\nCOPY . /var/task/\nRUN rm -rf /var/task/vendor\nFROM debian\nCOPY --from=base /var/task /var/task\nIf you can use this pattern in your larger build file then the final image will contain all the files in your working directory except vendor.\nThere's still a performance hit though. You're still going to have to copy the entire vendor directory into the build, and depending on what docker features you're using that will still take a long time. But if you need it for composer install then there's really no way around this.",
    "Why isn't docker reusing docker-compose's cache layers?": "With Docker-compose 1.25+ (Dec. 2019), try and use:\nCOMPOSE_DOCKER_CLI_BUILD=1 docker-compose build\nThat is what is needed to enable the docker-cli, instead of the own internal docker-compose build.\nSee also \"Faster builds in Docker Compose 1.25.1 thanks to BuildKit Support\".\nBut be aware of docker-compose issue 7336, when using it with DOCKER_BUILDKIT=1 (in addition of COMPOSE_DOCKER_CLI_BUILD=1)",
    "Cypress could not verify that this server is running when using Docker and Docker Compose": "localhost in Docker is always \"this container\". Use the names of the service blocks in the docker-compose.yml as hostnames, i.e., http://web:8080\n(Note that I copied David Maze's answer from the comments)",
    "docker-compose use environment variables from .env file": "So what you are doing is wrong. Reason being there is environment variable to be passed to Dockerfile while building. So there are two possible solutions\nUse Builder Arguments\nFROM mybaseimage\nMAINTAINER Zeinab Abbasimazar\nARG IP\nARG JMX_PORT\nARG WS_PORT\nRUN echo ${IP}\nRUN echo ${JMX_PORT}\nRUN echo ${WS_PORT}\nAnd then pass these build arguments using compose\nversion: \"2\"\nservices:\n  mynode:\n    build:\n      context: .\n      dockerfile: Dockerfile-mynode\n      args:\n        - IP=${IP}\n        - JMX_PORT=${JMX_PORT}\n        - WS_PORT=${WS_PORT}\n    environment_file:\n      - .env\n    ports:\n      - \"${JMX_PORT}:${JMX_PORT}\"\n      - \"${WS_PORT}:${WS_PORT}\"\nAlso you can load environment variables using environment_file. now this is only available when the container is started and they don't apply to your compose file or to your build file.\nAlso adding the build arguments will only make them available during the build and not during the container start. If you need to do that you need to define the variable twice\nFROM mybaseimage\nMAINTAINER Zeinab Abbasimazar\nARG IP\nARG JMX_PORT\nARG WS_PORT\nENV IP=${IP} JMX_PORT=${JMX_PORT} WS_PORT=${WS_PORT}\nRUN env\nCopying the environment file\n.env\nexport NAME=TARUN\nDockerfile\nFROM alpine\nCOPY .env /tmp/.env\nRUN cat /tmp/.env > /etc/profile.d/tarun.sh && chmod +x /etc/profile.d/tarun.sh\nSHELL [\"sh\",\"-lc\"]\nRUN env\nIn the RUN cat /tmp/.env > /etc/profile.d/tarun.sh && chmod +x /etc/profile.d/tarun.sh we create profile file which should be loaded when a shell is executed with profile.\nIn SHELL [\"sh\",\"-lc\"] we change default shell from sh -c to sh -l so it can load our profile also.\nOutput of the last step is\nStep 5/5 : RUN env\n ---> Running in 329ec287a5a6\nHOSTNAME=e1ede117fb1e\nSHLVL=1\nHOME=/root\nPAGER=less\nPS1=\\h:\\w\\$\nNAME=TARUN\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/\nCHARSET=UTF-8\n ---> 6ab95f46e940\nAs you can see the environment variable is there.\nNote: This .env format cannot be loaded in docker-compose as we have an export statement which is a bash command",
    "How should I handle Perl module updates when maintaining docker images?": "You could use the standard module installer for your underlying OS on your docker image.\nFor example, if its RedHat then use yum and only use CPAN when they are not available\nFROM centos:centos7\n  RUN  yum -y install cpanm gcc perl perl-App-cpanminus perl-Config-Tiny &&  yum clean all\n  RUN cpanm install Some::Module; rm -fr root/.cpanm; exit 0\ntaken from here and modified\nI would try to have a base image which the actual applications use\nI would also avoid doing things interactively (e.g. script a dockerfile) as you want to be able to repeat the build when upstream dependencies change, which docker hub does for you.\nEDIT You can convert perl modules into your own packages using dh-make-perl\nYou can load these into your own Ubuntu repo using reprepro or a paid solution of Artifactory\nThese can then be installed using apt-get when you use your repo as a source from within a dockerfile.\nWhen I have tried a similar thing before There are a few problems\nYour apps don't work with the latest version of modules\nThere are far more dependencies than you expected\nSome modules wont package\nBenefits are\nYou keep the build tools (gcc, etc) off the app servers\nYou know much more about your dependencies",
    "Dockerfile - How to copy files from a local folder? [duplicate]": "The key thing you are missing is the build's context, relevant from the COPY part of the docs:\nThe path must be inside the context of the build; you cannot COPY ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\nhttps://docs.docker.com/engine/reference/builder/#copy\nDescription of \"context\" here.\nhttps://docs.docker.com/engine/reference/commandline/build/\nBut essentially, when you say \"docker build directory-with-docker-file\" COPY only sees files in (and below) the directory with the Dockerfile.\nWhat you probably want to do is compile \"swagger\" during the docker build, and then put it in the path you want.\nA key thing to remember is that a Dockerfile is generally meant to be reproducible such that if you run docker build on ten different hosts, the exact same image will be produced. Copying files from arbitrary locations on the host wouldn't lead to that.",
    "Dockerfile - How to append PATH using ENV instruction?": "Apparently Docker doesn't let you use environment variables defined outside of your Dockerfile within an ENV or ARG declaration.\nAs a workaround, you can pass the names/directories to your Dockerfile explicitly using ARG:\nFROM golang:1.14.10\n    \n# set default to `root`\nARG USERNAME=root\n\nENV PATH=$PATH:/$USERNAME/go/bin\n\nRUN echo $PATH\nYou can then pass the USERNAME via docker build --build-arg USERNAME=myuser\nDepending on your usecase you can also do this using a RUN or ENTRYPOINT.",
    "How to run `httpd` in Docker in detached mode using CMD in Dockerfile?": "You should delete CMD [\"httpd\"], see this:\nCMD [\"httpd-foreground\"]\nThere is already a foreground httpd there.\nFinally, Why CMD [\"httpd\"] won't work?\nThe CMD defined in Dockerfile would be acting as PID1 of your container. In docker, if PID1 exits, then, the container will also exit.\nIf use CMD [\"httpd-foreground\"], the apache process will always be in front, so the process will not exit, then the container is alive.\nIf use CMD [\"httpd\"], the httpd will directly exit after executing, then PID1 exits, so the container exits.",
    "Why is my final docker image in this multi-stage build so large?": "Buried deep in the Docker documentation I found that my ARG and ENV definitions were cleared when I started the final FROM. Redefining them solved the issue:\n# Configure environment and build settings.\nFROM golang:alpine AS buildstage\nARG name=ddmnh\nENV GOPATH=/gopath\n\n# Create the working directory.\nWORKDIR ${GOPATH}\n\n# Copy the repository into the image.\nADD . ${GOPATH}\n\n# Move to GOPATH, install dependencies and build the binary.\nRUN cd ${GOPATH} && go get ${name}\nRUN CGO_ENABLED=0 GOOS=linux go build ${name}\n\n# Multi-stage build, we just use plain alpine for the final image.\nFROM alpine:latest\nARG name=ddmnh\nENV GOPATH=/gopath\n\n# Copy the binary from the first stage.\nCOPY --from=buildstage ${GOPATH}/${name} ./${name}\nRUN chmod u+x ./${name}\n\n# Expose Port 80.\nEXPOSE 80\n\n# Set the run command.\nCMD ./ddmnh",
    "Import-Module in the Docker PowerShell image": "Because it happens in a wrong \"context\". For this to work the way you want it to work you need to use both these commands in 1 powershell session:\npwsh -command \"Import-Module Microsoft.PowerShell.Management; Get-Module\"\nelse it creates a layer where it ran that command, but when powershell stops all the imports are gone (and when the layer is done, it shuts down the container, so it doesnt preserve session state, only os state).\nMy dockerfile (working example):\nFROM microsoft/powershell:ubuntu16.04\nRUN pwsh -c \"Install-Module AzureRM.netcore -Force\"\n\nCMD [ \"pwsh\" ]",
    "Docker - [Internal] load build context: when try to build an Image": "Same issue, but I made a bandaid with:\nsudo chown $USER:$USER *\nThe problem was really me being too fast and loose with docker volumes which caused some of the files/directories in the directory with Dockerfile to be owned by root instead of \"me\".\nThe permanent fix for me is presumably to stop playing fast and loose with volume mounts.",
    "Specify multiple files in ARG to COPY in Dockerfile": "because it treats src/hi src/there as a single item.\nHow can I \"expand\" the files argument into multiple files to copy?\nThat seems unlikely considering the Dockerfile format mentions, regarding arguments:\nwhitespace in instruction arguments, such as the commands following RUN, are preserved\nAnd that is not limited to RUN.\nCOPY, however, also includes:\nEach <src> may contain wildcards and matching will be done using Go\u2019s filepath.Match rules.\nIt would not work in your case.\nRemain the multi-stage builds approach\nCOPY src (the all folder)\nRemove everything you don't want:\nRUN find pip ${files} -maxdepth 1 -mindepth 1 -print | xargs rm -rf\nBuild your actual image based on the resulting state of the first image.\nYou can pass as one argument the folders you want to preserve\ndocker build --build-arg=\"files=! -path \"src/hi\" ! -path \"src/there\"\" .\nSee an example in \"Docker COPY files using glob pattern?\".",
    "VS Code Remote-Containers: cannot create directory \u2018/home/appuser\u2019:": "What works for me is to create a non-root user in my Dockerfile and then configure the VS Code dev container to use that user.\nStep 1. Create the non-root user in your Docker image\nARG USER_ID=1000\nARG GROUP_ID=1000\nRUN groupadd --system --gid ${GROUP_ID} MY_GROUP && \\\n    useradd --system --uid ${USER_ID} --gid MY_GROUP --home /home/MY_USER --shell /sbin/nologin MY_USER\nStep 2. Configure .devcontainer/devcontainer.json file in the root of your project (should be created when you start remote dev)\n\"remoteUser\": \"MY_USER\" <-- this is the setting you want to update\nIf you use docker compose, it's possible to configure VS Code to run the entire container as the non-root user by configuring .devcontainer/docker-compose.yml, but I've been happy with the process described above so I haven't experimented further.\nYou might get some additional insight by reading through the VS Code docs on this topic.",
    "RUN command in dockerfile produces different result than manually running same commands inside container": "First of all, a little bit of background: the platform detection script which runs during the build uses uname(1) utility (thus uname(2) system call) to identify the hardware it runs on:\nroot@6e4b69adfd4c:/gcc-4.8.5# grep 'uname -m' config.guess \nUNAME_MACHINE=`(uname -m) 2>/dev/null` || UNAME_MACHINE=unknown\nOn your 64-bit machine uname -m returns x86_64. However, there is a system call which allows overriding this result: personality(2). When the process calls personality(2), it and its subsequent forks (children) start seeing the fake results when calling uname(2). So, there is the possibility to ask the kernel to provide the fake hardware information in uname(2).\nThe base image you use (jnickborys/i386-ubuntu:12.04) contains the 32-bit binaries and defines the entrypoint /usr/bin/linux32, which calls personality(PER_LINUX32) to ask the kernel to pretend that it runs on 32-bit hardware and to return i686 in uname(2) (this may be checked using docker inspect and strace respectively). This makes possible to pretend that the containerized process runs in 32-bit environment.\nWhat is the difference between executing the build in RUN directive and manually in the container?\nWhen you execute the build in RUN, Docker does not use the entrypoint to run the commands. It uses what is specified in the SHELL directive instead (default is /bin/sh -c). This means that the personality of the shell running the build is not altered, and it (and the child processes) sees the real hardware information - x86_64, thus, you get x86_64-unknown-linux-gnu build system type in 32-bit environment and the build fails.\nWhen you run the build manually in the container (e.g. after starting it using docker run -it jnickborys/i386-ubuntu:12.04 and then performing the same steps as in the Dockerfile), the entrypoint is called, thus, the personality is altered, and the kernel starts reporting that it runs on 32-bit hardware (i686), so you get i686-pc-linux-gnu build system type, and the build runs correctly.\nHow to fix this? Depends on what do you want. If your goal is to build gcc for 64-bit environment, just use the 64-bit base image. If you want to build for 32-bit environment, one of your options is to alter the SHELL being used for RUNs before these RUNs:\nSHELL [\"/usr/bin/linux32\", \"/bin/sh\", \"-c\"]\nThis will make Docker execute RUNs with altered personality, so, the build system type will be detected correctly (i686-pc-linux-gnu) and the build will succeed. If required, you may change the SHELL back to /bin/sh -c after the build.",
    "Teradata Database Docker Image": "As mention by dnoeth. Currently there's only a VMWare image.",
    "Using a dockerfile argument in a RUN statement": "Well, I found a solution that works but it's definitely not ideal. It appears variables simply aren't expanded in RUN statements (at least on Windows, haven't tried Linux). However, the COPY statement will expand them. So, I can copy my file to a temp file with a hard coded name, and then use that:\nCOPY ./ /inetpub/wwwroot/\nCOPY ${transform} /inetpub/wwwroot/Web.Current.config\nRUN powershell -executionpolicy bypass /Build/Transform.ps1 -xml \"/inetpub/wwwroot/web.config\" -xdt \"/inetpub/wwwroot/Web.Current.config\"\nIn this particular situation, it works for me.\nUpdate: Found another method that works\nThis is perhaps a better method using environment variables:\nFROM someimage\nARG transform\nENV TransformFile ${transform}\n\nCOPY ./ /inetpub/wwwroot/\nRUN powershell -executionpolicy bypass /Build/Transform.ps1 -xml \"/inetpub/wwwroot/web.config\" -xdt \"/inetpub/wwwroot/$ENV:TransformFile\"\nIn this case, Docker will evaluate the parameter transform and set it to the environment variable TransformFile. When I use it in the PowerShell script, it's no longer Docker that's evaluating the argument, but Powershell itself. Thus, the Powershell syntax for interpolating an environment variable must be used.",
    "Dockerfile can't copy files. (\"cp\" command)": "Approach #1\nI believe that the below statements of the Dockerfile can be changed\nFrom:\nADD . /var/www\nWORKDIR /var/www\nRUN cp config.php.sample config.php\nRUN cp shared_config.php.sample shared_config.php\nRUN cp vendor/propel/propel1/build.properties.sample vendor/propel/propel1/build.properties\nTo:\nADD config.php.sample /var/www/config.php\nADD shared_config.php.sample /var/www/shared_config.php\nADD vendor/propel/propel1/build.properties.sample /var/www/vendor/propel/propel1/build.properties \nOf course, you also use COPY instead of ADD as well in the above.\nApproach #2\nChange below statements from:\nRUN cp config.php.sample config.php\nRUN cp shared_config.php.sample shared_config.php\nRUN cp vendor/propel/propel1/build.properties.sample vendor/propel/propel1/build.properties\nTo:\nRUN [\"cp\",  \"config.php.sample\", \"config.php\"]\nRUN [\"cp\", \"shared_config.php.sample\", \"shared_config.php\"]\nRUN [\"cp\", \"vendor/propel/propel1/build.properties.sample\", \"vendor/propel/propel1/build.properties\"]\nFor more details, refer documentation.\nHope this is helpful.",
    "Cannot run PlayWright inside docker image": "I think this is a known issue in Playwright. You can see the detailed discussion here, and particularly see this comment\nThe solution I've seen mostly working is to either delete the node_modules entirely and then re-install it again, or in case of a docker image, try the solution mentioned in this link, by setting the playwright browsers to a specific path",
    "ARM64 Geckodriver for Linux": "anyone having this same issue try this process step by step.\nA) Install Firefox\nsudo apt install firefox\nfirefox --version\nB) geckodriver - For arm64\nsudo apt install firefox-geckodriver\nC) Install selenium\npip3 install selenium\npip3 install --upgrade requests\nD) Script to test\nimport time\nfrom selenium import webdriver\nfrom selenium.webdriver import FirefoxOptions\nfrom selenium.webdriver.common.keys import Keys\n\nopts = FirefoxOptions()\nopts.add_argument(\"--headless\")\nbrowser = webdriver.Firefox(options=opts)\nbrowser.get('https://google.com/')\nprint('Title: %s' % browser.title)\ntime.sleep(2)\nbrowser.quit()\n** Tested & working on Ubuntu v20 & arm64",
    "Docker cannot find Resource on classpath": "Seems like you aren't building Fat Jar. You can utilize spring-boot-maven-plugin for that.\n<plugin>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-maven-plugin</artifactId>\n    <version>2.0.1.RELEASE</version>\n</plugin>\nThen change your Dockerfile like:\nFROM java:8\nVOLUME /tmp\nENV tom=dev\nCOPY build/libs/demo-0.0.1-SNAPSHOT.jar /app/app.jar\nWORKDIR /app\nENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-Dprofile=${tom}\",\"-jar\",\"app.jar\"]",
    "Create Docker Image For JRE FROM scratch": "The hotspot sources do not currently support statically linking. See http://mail.openjdk.java.net/pipermail/hotspot-dev/2013-September/010810.html for more info.",
    "Docker container fails to run, Error : python3: can't open file 'flask run --host=0.0.0.0': [Errno 2] No such file or directory": "The best use for ENTRYPOINT is to set the image\u2019s main command, allowing that image to be run as though it was that command (and then use CMD as the default flags).\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#entrypoint\nlots of people seem to miss this point about the ENTRYPOINT and CMD Dockerfile-instructions.\nthe ENTRYPOINT instruction supposed to run some executable, which should run every time you start the container, such as starting your server.\nthe CMD supposed to include the flags provided to that executable, so they can be easily overridden when running the container.\ni am not sure you are supposed to have more then one CMD instruction. if you need to run commands during the build process, you can use the RUN instruction - for example:\nRUN mkdir some/dir\nnow:\nrun.py is the main python flask file for execution\nhence i suggest you define it as your entrypoint:\nENTRYPOINT [ \"./run.py\" ]\ncommands that you may also want to run every time the container starts, such as flask run --host=0.0.0.0 you can:\nmove that command to sit inside the run.py file\nor\nkeep the CMD [ \"flask\", \"run\", \"--host=0.0.0.0\" ] line. this command will be passed as an argument to the run.py entrypoint, so you may execute it in there. that way you can easily override the command when running the container with alternative arguments.\nthis stuff is also in the docs:\nUnderstand how CMD and ENTRYPOINT interact\nBoth CMD and ENTRYPOINT instructions define what command gets executed when running a container. There are few rules that describe their co-operation.\nDockerfile should specify at least one of CMD or ENTRYPOINT commands.\nENTRYPOINT should be defined when using the container as an executable.\nCMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container.\nCMD will be overridden when running the container with alternative arguments.",
    "yum update / apk update / apt-get update not working behind proxy": "It's quite bad idea to set http(s)_proxy as system wide variable. U only need to make that package manager work's over proxy. If u still want to set http(s)_proxy don't forget about no_proxy or all your traffic will try to go via proxy host. For ubuntu i prefer to use something like this\nFROM ubuntu\nARG PROXY=false\nARG PROXY_URL=\"http://proxy:8080\"\n\nRUN if [ \"$PROXY\" = true ] ; then echo 'Acquire::http::Proxy \"'$PROXY_URL'\";' >> /etc/apt/apt.conf ; fi && \\\n  apt-get update && \\\n  apt-get install -y vim\nAnd execute it like this on server without internet connection, but local execute will work without proxy\ndocker build -t ubuntu-with-proxy --build-arg PROXY=true .\nCentos also can handle proxy configuration inside yum.conf\nFROM centos\nARG PROXY=false\nARG PROXY_URL=\"http://proxy:8080\"\n\nRUN if [ \"$PROXY\" = true ] ; then echo 'proxy=\"$PROXY_URL\";' >> /etc/yum.conf ; fi && \\\n  yum install -y vim\nAnd execute it like this on server without internet connection, but local execute will work without proxy\ndocker build -t centos-with-proxy --build-arg PROXY=true .\nBut i can't find such solution for alpine\nI think that something like for Centos/Ubuntu could be achieved in Alpine with this, but i haven't test this yet.\nFROM alpine\nARG PROXY=false\nARG PROXY_URL=\"http://proxy:8080\"\n\nRUN if [ \"$PROXY\" = true ] ; then echo \"http_proxy = $PROXY_URL\" > /etc/wgetrc && echo \"use_proxy = on\" >> /etc/wgetrc ; fi && \\\n  apk add -U vim\nAnd again execution\ndocker build -t alpine-with-proxy --build-arg PROXY=true .",
    "How to read docker env variables into Java code": "This is weird but, restarting the docker container just worked fine for me. Turns out , i have to restart the container when ever I update the network connection using \"--network\". Thanks",
    "Docker build leads to \"no space left on device\" on Windows 10": "I had the same problem on Windows. One of this two settings should solve this problem:\nIncrease Memory that Docker is using. If is it 2GB, add more\nIncrease \"Disk image max size\" - initially is 60, move it to 80 GB\nThis should be enough, but depends on complexity of what you are building. In some cases increase of Swap would be needed. The initial Swap of Docker is 1024 MB\nThanks",
    "docker run with volume changes directory permission to root": "This is because you use the root user inside the container.\nYou can use the -u param to set the user you use outside (e.g. -u `id -u $USER`), but if you need root inside the container, you have to chown it manually.",
    "Unable to run flask app in debug mode using Docker": "A sample (simplifed) runthru demostrating file edits with no need for container restarts outlined below for your reference.\napp.py\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return \"Hello, World\"\n\nif __name__ == '__main__':\n    app.run(debug=True,host='0.0.0.0',port=80)\nYou will need to specify the port for the flask development server in order to match the exposed container port of 80.\nscreenshot can be viewed here\nSummary of Steps in screenshot (MAC OS X):\nstarting with Empty directory\nCreate app.py\ndocker run\ncurl localhost (this will display Hello, World)\nedit app.py\ncurl localhost (this should display the new edits)",
    "ERROR [internal] load metadata for docker.io/library/python.alpine3.8:latest": "change docker daemon.json \"buildkit\" on the last line to FALSE, like so:\n  {\n        \"builder\": { \"gc\": { \"defaultKeepStorage\": \"20GB\", \"enabled\": true } },\n        \"experimental\": false,\n        \"features\": { \"buildkit\": false }\n    }",
    "Dockerfile COPY vs \"docker run ... cp\"": "Without knowing what the files are for, we can only take wild guesses.\nVolumes are used for data persistence, whereas COPY is used for data that is needed in running the process, but may be destroyed.\nOne possible valid scenario for why data is copied into the volume instead of using the COPY command is that persistent data needs to be initialized and they don't want that initialization data to add bloat to the container image. Like I said, it's just a wild guess.\nAnother guess is that the Dockerfile is shared between developers and the initialization data between groups may vary, so one set of developers might copy different data into the volume.\nWhatever the data is, if you shut down and remove the container, data created via COPY just vanishes with the container, but data moved into the volume via cp on the host stays in the directory that was mounted. That data may have changed while the container was running from what was originally placed in it, but it doesn't reset when you remove the container and spawn a new container from the image.\nYou should ask the developer what all the files are for, and whether the files need to persist or whether they can just be \"ephemeral\". This will probably answer your questions as to why they are copied the way they are.",
    "Exclude a folder from a docker build COPY command without .dockerignore": "As long as .dockerignore is not an option, there are 3 methods that will work:\nExclude the folder from COPY by multiple copy instructions (one for each letter):\n COPY ./file_that_always_exists.txt ./[^s]* .        # All files that don't start with 's'\n COPY ./file_that_always_exists.txt ./s[^o]* .       # All files that start with 's', but not 'so'\n COPY ./file_that_always_exists.txt ./so[^l]* .      # All files that start with 'so', but not 'sol'\n COPY ./file_that_always_exists.txt ./sol[^v]* .     # All files that start with 'sol', but not 'solv'\n COPY ./file_that_always_exists.txt ./solv[^e]* .    # All files that start with 'solv', but not 'solve'\n COPY ./file_that_always_exists.txt ./solve[^r]* .   # All files that start with 'solve', but not 'solver'\nDisadvantage: This will flatten the folders structures, also, imagine that you have more than one folder to do that for :(\nNote, the requirement for file_that_always_exists.txt (e.g. can be Dockerfile) is to avoid the error COPY failed: no source files were specified if there are no files that match the copy step.\nCopy all folders and then in a different layer delete the unwanted folder:\n COPY . .\n RUN rm -rf ./solver\nDisadvantage: The content of the folder will still be visible in the Docker image, and if you are trying to decrease the image size this will not help.\nManually specify the files and folders you would like to copy (\ud83d\ude2d):\n COPY [\"./file_to_copy_1.ext\", \"file_to_copy_2.ext\", \"file_to_copy_3.ext\", \".\"]\n COPY ./folder_to_copy_1/ ./folder_to_copy_1/\n # ...\n COPY ./folder_to_copy_n/ ./folder_to_copy_n/\nDisadvantage: You have to manually write all files and folders, but more annoyingly, manually update the list when the folder hierarchy changes.\nEach method has it's own advantages and disadvantages, choose the one that most fit your application requirements.",
    "setfacl in Dockerfile has no effect": "Any idea why docker does not correctly apply the acl changes when running setfacl in the Dockerfile?\nDon't take this as an authoritative answer, because I'm just guessing.\nDocker images have to run on a variety of distributions, with different storage backends (possibly even more when you facter in image registries, like hub.docker.com). Even those that are filesystem based may be backed by different filesystems with different capabilities.\nThis means that in order for Docker images to run reliably and reproducibly in all situations, they have to minimize the number of extended filesystem features they preserve.\nThis is probably why the extended attributes necessary to implement filesystem ACLs are not preserved as part of the image.\nIt works in a container because at this point the files are stored on a specific local filesystem, so you can take advantage of any features supported by that filesystem.",
    "Docker-compose: error when trying to run mongo image": "change your docker-compose.yml file to:\nversion: '2'\nservices:\n    db:\n        image: mongo:3.4.10\n        command: mongod --dbpath /data/db\n        volumes:\n            - ./data/mongodb:/data/db",
    "Build go dependencies in separate Docker layer": "I've got a nasty hack that seems to work:\nFROM golang:1.12\n\nWORKDIR /src\n\nCOPY go.mod go.sum ./\nCOPY vendor/ ./vendor\nRUN go build -mod=vendor $(cat deps|grep -v mypackage | grep -v internal)\n\nCOPY . .\nRUN go build -mod=vendor\n...\ngo list -f '{{join .Deps \"\\n\"}}'  > deps\ndocker build .",
    "Access raspistill / pi camera inside a Docker container": "I've had the same problem trying to work with camera interface from docker container. With suggestions in this thread I've managed to get it working with the below dockerfile.\nFROM node:12.12.0-buster-slim\n\nEXPOSE 3000\n\nENV PATH=\"$PATH:/opt/vc/bin\"\n\nRUN echo \"/opt/vc/lib\" > /etc/ld.so.conf.d/00-vcms.conf\n\nCOPY \"node_modules\" \"/usr/src/app/node_modules\"\nCOPY \"dist\" \"/usr/src/app\"\n\nCMD ldconfig && node /usr/src/app/app.js\nThere are 3 main points here:\nAdd /opt/vc/bin to your PATH so that you can call raspistill without referencing the full path.\nAdd /opt/vc/lib to your config file so that raspistill can find all dependencies it needs.\nReload config file (ldconfig) during container's runtime rather than build-time.\nThe last point is the main reason why Anton's solution didn't work. ldconfig needs to be executed in a running container so either use similar approach to mine or go with entrypoint.sh file instead.",
    "curl doesn't work during docker build": "I could workaround the problem doing something like this:\ndocker build --add-host central.maven.org:151.101.56.209 .\nbut I'm not happy with that. I would like to say Docker to use my DNS instead of set fixed IP. It would be more elegant.",
    "unable to access environment variables from docker compose env file": "You are not setting ENV at build time, so docker-compose will able to read env file as you set configuration in the compose file, but docker run will not read the env as you need to specify env file\nYou need to specify env-file in docker run command\ndocker run --env-file .env my-app-test app.py\nor to just check ENV\ndocker run --env-file .env --entrypoint printenv my-app-test\nor run the stack\ndocker-compose up",
    "Docker image search using SHA hash": "You could list all the images with docker images and find a particular one:\ndocker images --no-trunc -q | grep <image_hash>\nOr you want to search via a chunk of hash number:\ndocker images -q | grep <image_hash>",
    "Dockerfile parse error line 7: COPY requires at least two arguments, but only one was provided. Destination could not be determined": "The full, proper way to do it is:\nCOPY --chown=node:node . ./\nThat is because COPY expects 2 arguments (as the error message says), it just doesn't like the second bare dot.",
    "Keeping node_modules up to date in docker": "This is an old question but in case anyone else comes here with the same issue, I struggled with this too and this is how I could achieve the desired behaviour:\ndocker-compose up --build --renew-anon-volumes\nAccording to the documentation on docker-compose up https://docs.docker.com/engine/reference/commandline/compose_up/\nOption Short Description\n--renew-anon-volumes -V Recreate anonymous volumes instead of retrieving data from the previous containers.\nWith this, you not only build a new image, but also override the original volume used to store node_modules. Note that it doesn't work with named volumes. If you use named volumes, you'd need to manually remove that volume with docker volume rm <volume-name> before running docker-compose up again.\nThe understanding of how volumes work from the original question seems sound. Readers may also find the following discussion in the Github issue beneficial to understanding it.\nhttps://github.com/moby/moby/issues/30647#issuecomment-276882545",
    "How to set container id when I run docker-compose up?": "the -t option to docker build doesn't set something called CONTAINER ID. In fact, it has nothing to do with a container. The output of docker build is an image, which is named based on the -t option. docker build -t myorg:myimage . creates an image called myorg:myimage that you can use later to build containers, or push to the docker registry so that you can later use it to build a container.\nThe equivalent in docker-compose is docker-compose build, not docker-compose up. To specify an image tag in docker-compose build, you use both the build and the image tags on a service in the compose file- in that case, using docker-compose build will build the image based on the build directive, and tag the output using the image tag.",
    "Failed to solve with frontend xxx: rpc error: code = Unknown desc = (...) out: `exit status 2: gpg: decryption failed: No secret key`": "try download first the docker image and run command for build image, it worked me",
    "Enable gpu support by default on docker containers": "Here's the right config to set in /etc/docker/daemon.json :\n{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"args\": [],\n            \"path\": \"nvidia-container-runtime\"\n        }\n    },\n    \"default-runtime\": \"nvidia\"\n}\nDon't forget to sudo service docker restart",
    "How to set multiple commands in one yaml file with Kubernetes?": "command: [\"/bin/sh\",\"-c\"]\nargs: [\"command one; command two && command three\"]\nExplanation: The command [\"/bin/sh\", \"-c\"] says \"run a shell, and execute the following instructions\". The args are then passed as commands to the shell. In shell scripting a semicolon separates commands, and && conditionally runs the following command if the first succeed. In the above example, it always runs command one followed by command two, and only runs command three if command two succeeded.\nAlternative: In many cases, some of the commands you want to run are probably setting up the final command to run. In this case, building your own Dockerfile is the way to go. Look at the RUN directive in particular.",
    "Docker compose build error - Project file does not exist": "you have issues with your working directory try using absolute path instead of relative path in your working directory in docker file",
    "Missing `node_modules` in Docker Container": "Then why don't you create this directory at build (ie adding the following line in your Dockerfile before RUN npm install) ?\nRUN mkdir <workdir>/node_modules\n(Replace <workdir> with the actual default working dir of the keymetrics/pm2 image)",
    "How to override docker-compose values in multiple combined files?": "I would actually strongly suggest just not using the \"target\" command in your compose files. I find it to be extremely beneficial to build a single image for local/staging/production - build once, test it, and deploy it in each environment. In this case, you change things using environment variables or mounted secrets/config files.\nFurther, using compose to build the images is... fragile. I would recommend building the images in a CI system, pushing them to a registry, and then using the image version tags in your compose file- it is a much more reproducible system.",
    "Docker stuck at npm install": "So, this might be a temporary solution till this is properly addressed, but you can use\nnpm config set registry http://registry.npmjs.org/\nI have used it for docker environment and it worked fine.",
    "Setup git via windows docker file": "I've solved issue with GUI through usage of MinGit and by putting information about mingit into environment/path variable. I've used following approach:\nRUN Invoke-WebRequest 'https://github.com/git-for-windows/git/releases/download/v2.12.2.windows.2/MinGit-2.12.2.2-64-bit.zip' -OutFile MinGit.zip\n\nRUN Expand-Archive c:\\MinGit.zip -DestinationPath c:\\MinGit; \\\n$env:PATH = $env:PATH + ';C:\\MinGit\\cmd\\;C:\\MinGit\\cmd'; \\\nSet-ItemProperty -Path 'HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment\\' -Name Path -Value $env:PATH",
    "MySQL in Docker frozen at root password config": "The accepted answer may be true in some abstract sense, but it's completely irrelevant to the matter at hand. You need a way to specify the password statically. And unless you are using the official image, you'll need that whether or not you follow the \"one process, one container\" dogma.\nThe answer here tells how, but it leaves out a key setting: you still have to tell debconf to use the Noninteractive front-end, as described here.\nHere's an example of a working Dockerfile based on the above.\nFROM ubuntu:latest\nMAINTAINER Jonathan Strange <jstrange@norrell.edu>\nRUN apt-get update \\\n    && apt-get install -y apt-utils \\                                           \n    && { \\\n        echo debconf debconf/frontend select Noninteractive; \\\n        echo mysql-community-server mysql-community-server/data-dir \\\n            select ''; \\\n        echo mysql-community-server mysql-community-server/root-pass \\\n            password 'JohnUskglass'; \\\n        echo mysql-community-server mysql-community-server/re-root-pass \\\n            password 'JohnUskglass'; \\\n        echo mysql-community-server mysql-community-server/remove-test-db \\\n            select true; \\\n    } | debconf-set-selections \\\n    && apt-get install -y mysql-server apache2 python python-django \\\n        python-celery rabbitmq-server git\nThis is not too terribly different from what the official Dockerfile does -- though they handle the actual password config somewhat differently.\nSome people have had success by setting the DEBIAN_FRONTEND environment variable to noninteractive, like so:\nENV DEBIAN_FRONTEND noninteractive\nHowever, that doesn't seem to work in all cases. Using debconf directly has proven more reliable for me.",
    "Docker building issue: Error checking context: can't stat": "I had a similar issue until recently . On 18.04, Ubuntu installs Docker via Snap.\nBy default, SNAP packages are not allowed to access /media, or any other / root folder. The SNAP sandbox will deny access unless you have explicitly granted access to removable-media. ( /mnt or /media )\nTo grant docker access to removable-media:\nsudo snap connect docker:removable-media\nAlternatively, move your project into your /home/user directory and try building from there.",
    "/bin/sh: 1: [\u201cnpm\u201d,: not found on docker-compose up [duplicate]": "You have the wrong quotes in your dockerfile:\napp    | /bin/sh: 1: [\u201cnpm\u201d,: not found\ndoesn't match the quotes in the example you pasted:\nCMD [\"npm\", \"start\"]\nDouble check your Dockerfile to correct your quotes.",
    ".bash_profile does not work with docker php image": "Had a similar issue and the easiest solution was to use the -l option to bash to make bash act as if it had been invoked as a login shell.\ndocker run --rm -it $IMAGE /bin/bash -l\nbash will then read in ~/.bash_profile",
    "Restore a SQL Server DB.bak in a Dockerfile": "A friend and I puzzled through this together and eventually found this solution. Here's what the docker file looks like:\nFROM microsoft/mssql-server-linux\nENV MSSQL_SA_PASSWORD=myPassword\nENV ACCEPT_EULA=Y\n\nCOPY ./My_DB.bak /var/opt/mssql/backup/My_DB.bak\nCOPY restore.sql restore.sql\nRUN (/opt/mssql/bin/sqlservr --accept-eula & ) | grep -q \"Starting database restore\" && /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P 'myPassword' -d master -i restore.sql\n*Note that I moved the SQL restore statement to a .sql file.",
    "Docker: Set export PATH in Dockerfile": "$PATH is a list of actual directories, e.g., /bin:/usr/bin:/usr/local/bin:/home/dmaze/bin. No expansion ever happens while you're reading $PATH; if it contains ~/bin, it looks for a directory named exactly ~, like you might create with a shell mkdir \\~ command.\nWhen you set $PATH in a shell, PATH=\"~/bin:$PATH\", first your local shell expands ~ to your home directory and then sets the environment variable. Docker does not expand ~ to anything, so you wind up with a $PATH variable containing a literal ~.\nThe best practice here is actually to avoid needing to set $PATH at all. A Docker image is an isolated filesystem space, so you can install things into the \"system\" directories and not worry about confusing things maintained by the package manager or things installed by other users; the Dockerfile is the only thing that will install anything.\nRUN pip install awscli\n# without --user\nBut if you must set it, you need to use a Dockerfile ENV directive, and you need to specify absolute paths. ($HOME does seem to be well-defined but since Docker containers aren't usually multi-user, \"home directory\" isn't usually a concept.)\nENV PATH=\"$HOME/.local/bin:$PATH\"\n(In a Dockerfile, Docker will replace $VARIABLE, ${VARIABLE}, ${VARIABLE:-default}, and ${VARIABLE:+isset}, but it doesn't do any other shell expansion; path expansion of ~ isn't supported but variable expansion of $HOME is.)",
    "Vert.x based application crashes on docker container": "Judging by FileResolver.java, vert.x tries to create a \".vertx\" directory in the current working directory by default. You have configured a user called \"daemon\", are you sure that this user has write access to the working dir in the docker image? If not, change the permissions as outlined in docker-image-author-guidance, or revert to using the root user.",
    "How to fix 'Hash Sum Mismatch' in Docker on mac": "Found this answer from here https://forums.docker.com/t/hash-sum-mismatch-writing-more-data-as-expected/45940/2 This bothered me for a day\nRUN echo \"Acquire::http::Pipeline-Depth 0;\" > /etc/apt/apt.conf.d/99custom && \\\n    echo \"Acquire::http::No-Cache true;\" >> /etc/apt/apt.conf.d/99custom && \\\n    echo \"Acquire::BrokenProxy    true;\" >> /etc/apt/apt.conf.d/99custom\n\nRUN apt-get update && apt-get upgrade -y \\\n    && apt-get install -y \\",
    "set NODE_ENV variable in production via docker file": "Remove the equal sign\nENV NODE_ENV production",
    "Docker: Can't read class path resource from spring boot application": "Since you are using springboot you can try to use the following annotation for loading your classpath resource. Worked for me because I had the same exception. Be aware that the directory \"location\" must be under the src/main/resources folder:\n@Value(\"classpath:/location/GeoLite2-City.mmdb\")\nprivate Resource geoLiteCity;\nWithout springboot you could try:\ntry (InputStream inputStream = getClass().getClassLoader().getResourceAsStream(\"/location/GeoLite2-City.mmdb\")) {\n... //convert to file and other stuff\n}\nAlso the answers before were correct as the use of \"/\" is not good at all and File.separator would be the best choice.",
    "Cant access my Docker DotNet core website": "Finally.\nI found this blog post: http://dotnetliberty.com/index.php/2015/11/26/asp-net-5-on-aws-ec2-container-service-in-10-steps/\nEven though it used an old version of the dotnet core there was an important point I had overseen;\nNotice that I\u2019ve provided an extra parameter to the dnx web command to tell it to serve on 0.0.0.0 (rather than the default localhost). This will allow our web application to serve requests that come in from the port forwarding provided by Docker which defaults to 0.0.0.0.\nWhich is pretty damn important.\nSolution:\nvar host = new WebHostBuilder()\n            .UseKestrel()\n            .UseStartup<Startup>()\n            .UseUrls(\"http://0.0.0.0:5000\")\n            .Build();\nOld code:\nvar host = new WebHostBuilder()\n            .UseKestrel()\n            .UseStartup<Startup>()\n            .UseUrls(\"http://localhost:5000\")\n            .Build();\nWhich is wery frustrating since it seemed to work perfect in Linux and windows and app starting up in Docker, but never getting any requests. Hope this helps some other poor souls out there :)",
    "Visual Studio generated Dockerfile does not work with manual docker build": "Your command fails, because the build context is wrong. Visual studio is using the solution root folder as build context, you are (probably) using the project's dockerfile's location. You can read more about the docker build command here.\nYour command should look similar to this:\ndocker build -f \"<path-to-your-dockerfile>\" -t some-name \"<!!!path-to-your-solution-folder!!!>\"\nYou can see the exact command executed by visual studio in the output window, with \"Container Tools\" selected from the dropdown box.",
    "Docker includes invalid characters \"${PWD}\" for a local volume name": "Path expansion is different in each shell.\nFor PowerShell use: ${pwd} \nFor cmd.exe \"Command Prompt\" use: %cd%\nbash, sh, zsh, and Docker Toolbox Quickstart Terminal use: $(pwd) \nNote, if you have spaces in your path, you'll usually need to quote the path.\nAlso answered here: Mount current directory as a volume in Docker on Windows 10",
    "Docker : execute commands as a non-root user": "1- Execute docker command with non-root user\nIf this is your case and don't want to run docker command with root user, follow this link . create a docker group and add your current user to it.\n$ sudo groupadd docker\n$ sudo usermod -aG docker $USER\n2- Execute commands inside docker! with non-root user\nIf I'm right you want to use a non-root user inside docker not the root!\nThe uid given to your user in the docker is related to the root docker images you are using, for example alphine or ubuntu:xenial as mentioned in this article\nBut you can simple change the user inside docker by changing a little bit as follow in your Dockerfile and add a new user and user it. like this:\n RUN adduser -D myuser\n USER myuser\n ENTRYPOINT [\u201csleep\u201d]\n CMD [\u201c1000\u201d]\nthen in the docker file, if you gain the /bin/bash and execute id command in it, you will see that the id of user inside docker is changed.\nUpdate:\nIf you have a ready to use Dockerfile, then create a new Dockerfile, for example it's name is myDocker, and put code below in it:\n from myDockerfile\n RUN adduser -D myuser\n USER myuser\n ENTRYPOINT [\u201csleep\u201d]\n CMD [\u201c1000\u201d]\nthen save this file,and build it:\n$ docker build -t myNewDocker .\n$ docker run myNewDocker <with your options>",
    "invalid header field value \"oci runtime error while running docker image": "Your docker-entrypoint.sh isn't executable, you need to add a RUN chmod 755 /docker-entrypoint.sh after the COPY command in your Dockerfile and rebuild the image.",
    "nginx: [emerg] open() \"/run/nginx.pid\" failed (13: Permission denied)": "UPDATE\nIn order to fix my \"/var/run/nginx.pid\" permission denied error.\nI had to add nginx.pid permission errors inside my dockerfile for the new user to work.\nBelow are the changes i made in my dockerfile\nRUN touch /run/nginx.pid \\\n && chown -R api-gatway:api-gatway /run/nginx.pid /cache/nginx",
    "Docker-compose args are not passed to Dockerfile": "There are two problems here. ARG is only used at build time, when creating the image, and CMD defines a step at run time, when running your container. ARG gets implemented as an environment variable for RUN steps, so it is up to the shell to expand the environment variable. And the json syntax doesn't run a shell. So to do this with CMD, you need to make two changes.\nFirst, you need to save your ARG as an ENV value that gets saved to the image metadata and used to setup the environment when creating the container.\nAnd second, you need to switch from an exec/json syntax for running CMD to run a shell that will expand these variables. Docker does this for you with the string syntax.\nThe end result looks like:\nFROM golang:1.11\n\n// stuff...\n\nARG broker\nENV broker=${broker}\nARG queue\nENV queue=${queue}\n\nCMD go run /go/src/github.com/org/project/cmd/some-service/some-service.go --broker \"$broker\" --queue \"$queue\"\nAs an aside, you should also note that every argument in exec syntax needs to be a separate array entry, e.g.:\nCMD [\"go\", \"run\", \"/go/src/github.com/org/project/cmd/some-service/some-service.go\", \"--broker $broker\", \"--queue $queue\"]\nis similar to running:\ngo run /go/src/github.com/org/project/cmd/some-service/some-service.go \"--broker $broker\" \"--queue $queue\"\nwhen you really wanted to run:\nCMD [\"go\", \"run\", \"/go/src/github.com/org/project/cmd/some-service/some-service.go\", \"--broker\", \"your_broker\", \"--queue\", \"your_queue\"]\nwhich would be similar to:\ngo run /go/src/github.com/org/project/cmd/some-service/some-service.go --broker \"your_broker\" --queue \"your_queue\"\n(Note I removed the variables from my example because they do not work in the exec syntax.)",
    "Docker build image - glob error { Error: EPERM: operation not permitted, scandir": "The map_files directory is a representation of the files a process currently has memory mapped by the kernel. This info is also contained in the maps file in the same directory.\nAs these files are a representation of memory, they change frequently. If a process creates a directory listing and then processes the list, the files might not exist by the time the process gets to them.\nIf the build is reporting files in /proc, a search has likely started from the / directory in the container and is recursively searching everything on the filesystem.\nUse a directory other than / as the WORKDIR in your Dockerfile\nFROM node:latest\n\nWORKDIR /app\nCOPY package.json npm-shrinkwrap.json /app/\nRUN npm install --production\nCOPY . /app/\nEXPOSE 8080\n\nRUN npm run deploy",
    "How does Java application know it is running within a Docker container": "Solution\nCheck control group of the init process simply by /proc/1/cgroup .\nif it is initiated normally all hierarchies have in / value\nif it is initiated from docker container they have /docker/<container_id> value.\nWhen running inside docker /proc/1/cgroup has values similar to :\n11:perf_event:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n10:memory:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n9:cpuset:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n8:net_cls,net_prio:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n7:pids:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n6:cpu,cpuacct:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n5:blkio:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n4:freezer:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n3:hugetlb:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n2:devices:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\n1:name=systemd:/docker/897df2a033d6ab07c357c1ac1f75741bd16474487de83c6d4d98518e5ef52249\nNote: As @JanisKirsteins informed me, If you run your application in amazon ec2 you might want to change the condition to line.contains(\"/ecs\") instead. because in /proc/1/cgroups you will find pattern similar to: /ecs/<uuid>/<uuid>\nIn Java\npublic static Boolean isRunningInsideDocker() {\n\n        try (Stream < String > stream =\n            Files.lines(Paths.get(\"/proc/1/cgroup\"))) {\n            return stream.anyMatch(line -> line.contains(\"/docker\"));\n        } catch (IOException e) {\n            return false;\n        }\n    }\nLive code checking\noutside docker: running outside docker\ninside docker : running inside docker\nMore Info\nhttps://tuhrig.de/how-to-know-you-are-inside-a-docker-container/\nHow to determine if a process runs inside lxc/Docker?\nHow to check if a process is running inside docker container",
    "Docker Error response from daemon: Error processing tar file(exit status 1): no space left on device": "first run docker image prune to clean up all dangling images\nif this didn't help you might need to check this answer\nDocker error : no space left on device",
    "how to include class library reference into docker file": "You need to move the Dockerfile to the solution level, and reference the projects using [Folder]/[Project].csproj. Here is my Dockerfile (located in the solution level):\nFROM mcr.microsoft.com/dotnet/aspnet:3.1 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:3.1 AS build\nWORKDIR /src\nCOPY [\"SumApi/SumApi.csproj\", \"SumApi/\"]\nCOPY [\"Shared/Shared.csproj\", \"Shared/\"]\nRUN dotnet restore \"SumApi/SumApi.csproj\"\nCOPY . .\nWORKDIR \"/src/SumApi\"\nRUN dotnet build \"SumApi.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"SumApi.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app`enter code here`\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"SumApi.dll\"]\nLearned it from here: https://imranarshad.com/dockerize-net-core-web-app-with-dependent-class-library/",
    "Failed to parse 'app.server' as an attribute name or function call when running docker": "I fixed this error on a coworker's app where they had the following in app.py:\nserver = Flask(__name__) # define flask app.server\napp = dash.Dash(\n    __name__,\n    server=server,\n)\nAnd then in an index.py they were doing:\nfrom app import app\n\nserver = app.server # I add this part here\nAnd then the Docker CMD was:\nCMD [\"gunicorn\", \"index:server\", \"-b\", \":8050\"]\nAnd it worked.",
    "Dockerfile for tesseract 4.0": "The solution was to upgrade to Ubuntu 18.04:\nFROM ubuntu:18.04\nRUN apt-get update \\\n    && apt-get install tesseract-ocr -y \\\n    python3 \\\n    #python-setuptools \\\n    python3-pip \\\n    && apt-get clean \\\n    && apt-get autoremove\n\nADD . /home/App\nWORKDIR /home/App\nCOPY requirements.txt ./\nCOPY . .\n\nRUN pip3 install -r requirements.txt\n\nVOLUME [\"/data\"]\nEXPOSE 5000 5000\nCMD [\"python3\",\"OCRRun.py\"]",
    "How to synchronize host folder in container folder with Docker": "You have to map volume of your docker container directory with host directory.\nFor example :\ndocker run -v <host_dir>:<container_dir> -other options imagename \nHere both directory synchronised vice or versa.\nHost directory and container directory must be available.",
    "pip install from custom whl file in Dockerfile": "Install all wheels without listing them explicitly:\npip install /path/to/*.whl",
    "E: Unable to locate package redis-server": "I am able to install redis-server as well as python. I added RUN apt-get update in Dockerfile. It updated and got redis installed. And there was one more thing in my case. I had already run 'apt-get update' which created an image before. All the time it was referring to the image and was not updating. Hence I used --no-cache=True and made it.\nFROM ubuntu:14.04\n\nRUN apt-get update\n\nRUN apt-get -y install redis-server",
    "How can I set the time zone in Dockerfile using gliderlabs/alpine:3.3": "The usual workaround is to mount /etc/localtime, as in issue 3359\n$ docker run --rm busybox date\nThu Mar 20 04:42:02 UTC 2014\n$ docker run --rm -v /etc/localtime:/etc/localtime:ro  busybox date\nThu Mar 20 14:42:20 EST 2014\n$ FILE=$(mktemp) ; echo $FILE ; echo -e \"Europe/Brussels\" > $FILE ; docker run --rm -v $FILE:/etc/timezone -v /usr/share/zoneinfo/Europe/Brussels:/etc/localtime:ro busybox date\n/tmp/tmp.JwL2A9c50i \nThu Mar 20 05:42:26 CET 2014\nThe same thread mentions (for ubuntu-based image though), but you already tried it.\nRUN echo Europe/Berlin > /etc/timezone && dpkg-reconfigure --frontend noninteractive tzdata\n(And I referred before to a similar solution)\nAnother option would be to build your own gliderlabs/docker-alpine image with builder/scripts/mkimage-alpine.bash.\nThat script allows you to set a timezone.\n    [[ \"$TIMEZONE\" ]] && \\\n        cp \"/usr/share/zoneinfo/$TIMEZONE\" \"$rootfs/etc/localtime\"\nYou can see that image builder script used in Digital Ocean: Alpine Linux:\nGenerate Alpine root file system\nEnsure Docker is running locally.\nDownload and unzip gliderlabs/docker-alpine.\nwget -O docker-alpine-master.zip https://github.com/gliderlabs/docker-alpine/archive/master.zip\nunzip docker-alpine-master.zip\nBuild the builder (export the right timezone first).\nexport TIMEZONE=xxx\ndocker build -t docker-alpine-builder docker-alpine-master/builder/\nBuild the root file system (change v3.3 to the Alpine version you want to build).\ndocker run --name alpine-builder docker-alpine-builder -r v3.4\nCopy the root file system from the container.\ndocker cp alpine-builder:/rootfs.tar.gz .\nOnce you have the rootfs.tar.gz on your own filesystem, you can use it (as mentioned here) to build your own Alpine image, with the following Dockerfile:\nFROM SCRATCH\nADD rootfs.tar.gz /\nOnce built, you can use that Alpine image with the right timezone.",
    "How can I print the value of a variable, either env or arg, at \"docker build\" time?": "Maybe I'm missing something, but can't you just echo it?\nIf you take this Dockerfile\nFROM ubuntu\nARG test\nRUN echo $test\nand build it with\ndocker build --build-arg test=hello . --progress=plain\nit prints hello.",
    "M1 Mac Docker Issues with apt-get update": "The package list grabbed by the curl command references the architecture of your system, and as there are no mssql-tools or msodbcsql17 packages for arm64 architecture at the moment, I was getting the unable to locate error. Current solution is changing the first line of the Dockerfile to specify the platform with FROM --platform=linux/amd64 python:3.7",
    "How to import a mysql dump file into a Docker mysql container": "The answer to your question is given in the docker hub page of MySQL.\nInitializing a fresh instance\nWhen a container is started for the first time, a new database with the specified name will be created and initialized with the provided configuration variables. Furthermore, it will execute files with extensions .sh, .sql and .sql.gz that are found in /docker-entrypoint->initdb.d. Files will be executed in alphabetical order. You can easily populate your mysql services by mounting a SQL dump into that directory and provide custom images with contributed data. SQL files will be imported by default to the database specified by the MYSQL_DATABASE variable.\nIn your docker-compose.yml use:\nvolumes:\n  - ${PWD}/config/start.sql:/docker-entrypoint-initdb.d/start.sql\nand that's it.",
    "NestJS minimize dockerfile": "For reducing docker image size you can use\nMulti-stage build\nNpm prune\nWhile using multi-stage build you should have 2(or more) FROM directives, as usual, the first stage does build, and the second stage just copies build from the first temporary layer and have instructions for run the app. In our case, we should copy dist & node_modules directories.\nThe second important moment its correctly split dependencies between 'devDependencies' & 'dependencies' in your package.json file.\nAfter you install deps in the first stage, you should use npm prune --production for remove devDependencies from node modules.\nFROM node:12.14.1-alpine AS build\n\n\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . ./\n\nRUN npm run build && npm prune --production\n\n\nFROM node:12.14.1-alpine\n\nWORKDIR /app\nENV NODE_ENV=production\n\nCOPY --from=build /app/dist /app/dist\nCOPY --from=build /app/node_modules /app/node_modules\n\nEXPOSE 3000\nENTRYPOINT [ \"node\" ]\nCMD [ \"dist/main.js\" ]\nIf you have troubles with node-gyp or just want to see - a full example with comments in this gist:\nhttps://gist.github.com/nzvtrk/cba2970b1df9091b520811e521d9bd44\nMore useful references:\nhttps://docs.docker.com/develop/develop-images/multistage-build/\nhttps://docs.npmjs.com/cli/prune",
    "Java Testcontainers - Cannot connect to exposed port": "There's some good advice in other answers; I'll augment these with a couple of other tips:\nAs already suggested:\nabsolutely do add the LogConsumer so that you can see the container's log output - maybe something useful will appear there, now or in the future. It's always good to have.\nset a breakpoint after the container is started, just before you start your client.\nAdditionally, I'd hope the following things make the difference. While paused at the breakpoint:\nrun docker ps -a in the terminal\nfirstly, check that your container is running and hasn't exited. If it has exited, have a look at the logs for the container from the terminal.\nsecondly, check the port mapping in the docker ps output. You should see something like 0.0.0.0:32768->24999/tcp (first port number is random, though).\nevaluate container.getFirstMappedPort() in your IDE and check that the port number you get back is the same as the randomised exposed port. Unless you have a very unusual Docker installation on your local machine, this container should be reachable at localhost: + this port.\nif you've got this far then it's likely that there's something wrong with either the container or the client code. You could try connecting a different client to the running container - even something like nc could help if you don't have another POP3 client handy.\nAnother thing to try is to run the container manually, just to reduce the amount of indirection taking place. The Testcontainers code snippet you've given is equivalent to:\ndocker run -p 24999 immerfroehlich/emailfilter:latest\nYou might find that this helps you divide up the problem space into smaller pieces.",
    "Docker - Writing python output to a csv file in the current working directory": "You can bind your host directory, I would suggest using a WORKDIR & replace ADD with COPY -\nDOCKERFILE\nFROM python:3\nWORKDIR /mydata\nCOPY scraper.py ./\nRUN pip install pandas\nCMD [\"python3\",\"./scraper.py\"]\nRun it -\ndocker run -v ${PWD}:/data ex_scraper\nYou should now be able to see the CSV in your current directory on host.",
    "How to do --rm and --restart in dockerfile?": "Dockerfile is used to specify instructions to build an image.\nOnce the image is built, you can start a container from that image using docker run command. --rm and --restart are options for docker run, which means those commands apply to a container. Using the --restart flag you can specify a restart policy for a container. --rm flag is used to remove the file system on the container when it exits.\nI hope you can see why the functionality provided by those two flags doesn't belong in the Dockerfile. If not, you should really read more about Docker (esp. images vs containers).\nADDITION:\n--rm removes the file system and cleans up the container. restart is used only to restart a container, and the file system disappearing between restarts would be extremely unpleasant. Also note that a restart after file system removal is more like a \"fresh start\" than a \"restart\". So basically they're mutually exclusive. Using them together will result in an error. Doesn't matter where you do it.",
    "Docker Ubuntu image missing prompt, color and completion?": "You're missing -t flag to allocate a pseudo-tty for your container:\ndocker run -it --name=\"TEST\" ubuntu:14.04 /bin/bash",
    "Windows Docker: Issues installing Visual C++ Redistributable in Windows Docker": "I found I was able to install this onto a docker image with the following code:\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\nRUN [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12; `\n    Invoke-WebRequest \"https://aka.ms/vs/17/release/vc_redist.x64.exe\" -OutFile \"vc_redist.x64.exe\"; `\n    Start-Process -filepath C:\\vc_redist.x64.exe -ArgumentList \"/install\", \"/passive\", \"/norestart\" -Passthru | Wait-Process; `\n    Remove-Item -Force vc_redist.x64.exe;\nSource: https://github.com/microsoft/dotnet-framework-docker/issues/15",
    "Docker build cache keeps growing": "The build cache is part of buildkit, and isn't visible as images or containers in docker. Buildkit itself talks directly to containerd, and only outputs the result to docker. You can prune the cache with:\ndocker builder prune\nAnd there are flags to keep storage based on size and age. You can see more in the docker documentation: https://docs.docker.com/engine/reference/commandline/builder_prune/",
    "How to fix error occurring during Docker image build: \"E: Unsupported file /tmp given on commandline\"": "you need to edit the last line in apt-get command change less \\ to less\ndocker thinks that RUN cd \"/tmp\" is a parameter for apt-get\nanyway you should use WORKDIR if you want to use /tmp for further steps",
    "I am getting max depth exceeded while building the docker image for Microsoft SQL stored procedure": "This happens because your Dockerfile has exceeded the 125 layers limit for docker images.\nIf you add the Dockerfile to the question I can help you to simplify it. As a first suggestion, try to group commands in only one RUN",
    "What if I change a Dockerfile while a build is running out of it?": "Not until you re-run it. The first version of the Dockerfile would be in memory and have no awareness of your changes.",
    "Error: Cannot find module @rollup/rollup-linux-x64-musl on docker container": "I encountered the same issue when trying to dockerize a react app using vite.\nI managed to solve the issue by adding a named volume to the docker-compose.yml file, like this:\ndashboard:\n    container_name: dashboard\n    image: dashboard\n    depends_on:\n      - postgres\n    build:\n      context: ../../packages/dashboard\n      dockerfile: Dockerfile\n    ports:\n      - \"5173:5173\"\n    volumes:\n      - ../../packages/dashboard:/app\n      - node_modules:/app/node_modules\nvolumes:\n  pgdata: {}\n  node_modules: {}\nTo be completely honest, I don't know why it helped. I queried Perplexity (the LLM tool) and it suggested that it has something to do with ARM64 processors (I'm using M2 Pro Mackbook Pro).\nPerplexity's explanation goes like this:\nThe root cause seems to be that when you mount your local node_modules directory into the Docker container using the volumes section, it tries to use the binaries from your local machine, which are likely compiled for a different architecture (e.g., x86_64) and are incompatible with the ARM64 architecture inside the container.\nOne of its references was this GitHub discussions about the same problem.",
    ".net 6 minimal api docker issue: error CS5001: Program does not contain a static 'Main' method suitable for an entry point": "The error message is not explicative.\nThe real error is on the line of COPY ./src/ ./. With this cmd you copy the CONTENT of the src folder (test) into the ROOT of the container.\nIt should be COPY ./src/ /src/ or (better) ./src /src\nTry to comment out ALL except the first 9 rows, build the container and run\ndocker run --rm -it <imagename> /bin/sh to see yourself.\nSince you try to build an empty folder the compiler raise the error that not found the main method (in reality it not find anything...)",
    "Set today's date as environment variable": "ARG should be what you are looking for:\nFROM base\n\n# to be able to use in Dockerfile\nARG now\n\n# to store the value as environment variable in the image\nENV build_date=$now\nNow you can build this with\n# pass value explicitly\ndocker build --build-arg now=\"$(date +%Y%m%d)\" .\n\n# pass value from environment\nexport now=\"$(date +%Y%m%d)\"\ndocker build --build-arg now .\nThis still requires to run date on the host since doing this inside the Dockerfile is not possible unfortunately:\nThe only way to execute arbitrary commands in the build is within a RUN statement; but\nThe only way to persist a variable into the environment of an image/container is with an ENV statement which can only use environment variables from outside the build\nYou could use a custom ENTRYPOINT tough and inject the date to the environment from a file:\nFROM base\n\nRUN date +%Y%m%d > /build-timestamp\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT /entrypoint.sh\nentrypoint.sh:\n#!/bin/bash\n\nexport BUILD_TIMESTAMP=\"$(cat /build-timestamp)\"\nexec \"$@\"",
    "How do I change the dll name when I dotnet publish?": "You can use this command to perform the build and rename the assembly:\ndotnet msbuild -r -p:Configuration=Release;AssemblyName=foo\nOn Linux/macOS you will have to add quotes around the command, like this:\ndotnet msbuild -r '-p:Configuration=Release;AssemblyName=foo'\nHowever, there can be unintended side effects due to a global property being set. You should read this open issue from Sep 2019 as it speaks directly to your question regarding Docker and renaming the output: https://github.com/dotnet/msbuild/issues/4696\nAlso, I know you wanted to avoid editing the .csproj file, but in case you aren't aware, you can add the AssemblyName property and set the output name in that manner.\nSuch as:\n  <PropertyGroup>\n    <OutputType>Exe</OutputType>\n    <TargetFramework>netcoreapp3.1</TargetFramework>\n    <AssemblyName>foo</AssemblyName>\n  </PropertyGroup>\nThis will create foo.dll (and other files as well, e.g. .pdb, .deps.json, etc.)",
    "Run Laravel app in Docker container, but autoload.php file does not exist": "I just had the same issue.\nFor some reason the vendor folder can not be created or written to when your apps root volume is mounted to mirror your local environment (- ./:/var/www).\nAdd the volume \"/var/www/vendor\" to your application. This will create the folder in the container and prevent that folder from being mirrored. This allows Laravel to control that folder and create what it needs in it.\nIf someone has a explanation of why the vendor folder can not be created or written to when mirrored it would be appreciated!\n  #PHP Service\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: digitalocean.com/php\n    container_name: bumapp\n    restart: unless-stopped\n    volumes:\n       - ./:/var/www\n       -  /var/www/vendor\n       - ./docker/php/local.ini:/usr/local/etc/php/conf.d/local.ini\n    tty: true\n    environment:\n      SERVICE_NAME: app\n      SERVICE_TAGS: dev\n    working_dir: /var/www\n    networks:\n      - app-network",
    "Require environment variables to be given to image when run using `-e`": "In detach mode it not possible to print message that env is required, in your word when running with -d, but you can try a workaround:\nDockerfile\nFROM alpine\nCOPY entrypoint.sh /usr/bin/\nRUN chmod +x /usr/bin/entrypoint.sh\nENTRYPOINT [\"entrypoint.sh\"]\nentrypoint.sh\n#!/bin/sh\necho \"starting container $hostname\"\nif [ -z \"$REQUIRED_ENV\" ]; then\n  echo \"Container failed to start, pls pass -e REQUIRED_ENV=sometest\"\n  exit 1\nfi\necho \"starting container with $REQUIRED_ENV\"\n#your long-running command from CMD\nexec \"$@\"\nSo when you run with\ndocker run -it --name envtest  --rm env-test-image \nit will exit with the message\nstarting container \nContainer failed to start, pls pass -e REQUIRED_ENV=sometest\nThe workaround with detach mode\ndocker run -it --name envtest  -d --rm env-test-image && docker logs envtest",
    "unhealthy docker container not restarted by docker native health check": "Docker only reports the status of the healthcheck. Acting on the healthcheck result requires an extra layer running on top of docker. Swarm mode provides this functionality and is shipped with the docker engine. To enable:\ndocker swarm init\nThen instead of managing individual containers with docker run, you would declare your target state with docker service or docker stack commands and swarm mode will manage the containers to achieve the target state.\ndocker service create -d  --net=host applicationname:temp\nNote that host networking and publishing ports are incompatible (they make no logical sense together), net requires two dashes to be a valid flag, and changing the pid namespace is not supported in swarm mode. Many other features should work similar to docker run.\nhttps://docs.docker.com/engine/reference/commandline/service_create/",
    "kubernetes deployment with args": "The problem was in your case is container is not found after finishing it's task. You told to execute a shell script to your conatainer. And after doing that the container is finished. That's why you can't see whether the files were created or not. Also it didn't put any logs. So you need to keep alive the container after creating the files. You can do that by putting a infinite while loop. Here it comes:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: hello\nlabels:\n    app: hi\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: hi\ntemplate:\n    metadata:\n    labels:\n        app: hi\n    spec:\n    containers:\n    - name: hi\n        image: busybox\n        args:\n        - /bin/sh\n        - \"-c\"\n        - \"touch /tmp/healthy; touch /tmp/liveness; while true; do echo .; sleep 1; done\"\n        ports:\n        - containerPort: 80\nSave it to hello-deployment.yaml and run,\n$ kubectl create -f hello-deployment.yaml\n$ pod_name=$(kubectl get pods -l app=hi -o jsonpath='{.items[0].metadata.name}')\n$ kubectl logs -f $pod_name\n$ kubectl exec -it -f $pod_name -- ls /tmp",
    "How to always use the newest version of a Docker Base Image?": "solution:\ndocker build --pull\nexplanation:\n--pull Always attempt to pull a newer version of the image\nhttps://docs.docker.com/engine/reference/commandline/build/",
    "Use private npm repo in Docker": "I'm guessing the package utilities@0.1.9 is your private package? If so, it would seem your auth token either isn't being used or doesn't have access to that package for some reason.\nYou could try writing the ~/.npmrc file rather than using the config set, this would just be a case of using:\nRUN echo -e \"//private.repo/:_authToken=... > ~/.npmrc\nThis will cause your docker user to then authenticate using that token against the registry defined. This is how we setup auth tokens for npm for the most part.\nOn a side note, you might want to consider not using multiple RUN commands one after another. This causes a new image layer to be created for every single command and can bloat the size of your container massively. Try using && \\ at the end of your commands and then placing the next command on a new line without the RUN bit. For example:\nFROM keymetrics/pm2:latest-alpine\n\nRUN mkdir -p /app\n\nWORKDIR /app\n\nCOPY package.json ./\nCOPY .npmrc ./\n\nRUN npm config set registry http://private.repo/:_authToken=$AUTH_TOKEN && \\\n  npm install utilities@0.1.9 && \\\n  apk update && apk add yarn python g++ make && rm -rf /var/cache/apk/* && \\\n  set NODE_ENV=production && \\\n  npm config set registry https://registry.npmjs.org/ && \\\n  npm install\n\nCOPY . /app\n\nRUN ls -al -R\n\nEXPOSE 51967\n\nCMD [ \"pm2-runtime\", \"start\", \"pm2.json\" ]\nIt should be just as readable but the final image should be smaller and potentially a bit faster to build.",
    "Issues with COPY when using multistage Dockerfile builds -- no such file or directory": "To follow-up my comment, the path you set with the WORKDIR is absolute and should be specified in the same way in the COPY --from=build command.\nSo this could lead to the following Dockerfile:\nFROM golang:latest AS build\n\nENV SRC_DIR=/go/src/github.com/grafana/grafana/\nENV GIT_SSL_NO_VERIFY=1\n\nCOPY . $SRC_DIR\nWORKDIR $SRC_DIR\n\n# Building of Grafana\nRUN \\\n  npm run build && \\\n  go run build.go setup && \\\n  go run build.go build\n\n# Create final stage containing only required artifacts\nFROM scratch\n\nENV SRC_DIR=/go/src/github.com/grafana/grafana/\nWORKDIR $SRC_DIR\n\nCOPY --from=build ${SRC_DIR}/bin/grafana-server ${SRC_DIR}/bin/grafana-server\n\nEXPOSE 3001\n\nCMD [\"./bin/grafana-server\"]\n(only partially tested)",
    "deploy stack to remote swarm": "You are trying to reach the remote Docker Daemon to push your compose.yml. But the problem is by default Docker Daemon is only bound to unix socket.\nTo do so, on your remote server, you will have to alter /usr/lib/systemd/system/docker.service file and change ExecStart to...\nExecStart=/usr/bin/docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\nthen,\nsystemctl daemon-reload\nand restart\nservice docker restart\nI wouldn't recommend you this setup without securing the Daemon with TLS. If you don't use TLS anyone can reach out to your server and deploy containers.\nHope it helps!",
    "Dockerfile: create and mount disk image during build": "You would need the --privileged or --cap-add functionality that docker run does have, but that is not supported for docker build. So, as of current Docker version, you can't.\nSee this comment:\nA significant number of docker users want the ability to --cap-add or --privileged in the build command, to mimic what is there in the run command.\nThat's why this ticket has been open for 3 years with people constantly chiming in even though the maintainers aren't interested in giving the users what they want in this specific instance.\nAs an alternative you can move that RUN commands to a script that should run when container starts (and adding the mentioned --privileged flag, or --cap-add=SYS_ADMIN)",
    "ADD and COPY to merge the contents of a dir with the one already on the build": "Docker has multi-stage builds since 17.05: Use multi-stage builds\nFROM busybox as builder\nADD build/image-base.tgz /tmproot/\n\nFROM alpine:latest  \n...\nCOPY --from=builder /tmproot /\n...\nAnother example:\nFROM busybox as builder\nCOPY src/etc/app /tmproot/etc/app\nCOPY src/app /tmproot/usr/local/app\n\nFROM alpine:latest  \n...\nCOPY --from=builder /tmproot /\n...",
    "How to copy a file from container to host using copy in docker-py": "copy is a deprecated method in docker and the preferred way is to use put_archive method. So basically we need to create an archive and then put it into the container. I know that's sounds weird, but that's what the API supports currently. If you, like me, think this can be improved, feel free to open an issue/feature request and I'll upvote it.\nHere is a code snippet on how to copy a file to the container :\ndef copy_to_container(container_id, artifact_file):\n    with create_archive(artifact_file) as archive:\n        cli.put_archive(container=container_id, path='/tmp', data=archive)\n\ndef create_archive(artifact_file):\n    pw_tarstream = BytesIO()\n    pw_tar = tarfile.TarFile(fileobj=pw_tarstream, mode='w')\n    file_data = open(artifact_file, 'r').read()\n    tarinfo = tarfile.TarInfo(name=artifact_file)\n    tarinfo.size = len(file_data)\n    tarinfo.mtime = time.time()\n    # tarinfo.mode = 0600\n    pw_tar.addfile(tarinfo, BytesIO(file_data))\n    pw_tar.close()\n    pw_tarstream.seek(0)\n    return pw_tarstream",
    "Change entrypoint of a k8s Pod, but keep the CMD": "That's not a thing.\nENTRYPOINT (in Dockerfile) is equal to command: (in PodSpec)\nCMD (in Dockerfile) equals args: (in PodSpec)\nSo just override command but not args.",
    "How to avoid start of Gradle daemon inside Dockerfile": "This question has already an extensive answer on the Gralde forums: Using \u2013no-daemon, but still see a process called \u201cGradle Worker Daemon 1\u201d.\nIn short: The Gradle daemon process is the one executing the build and started always no matter on what has been specified on the command line. If --no-daemon is specified, the process is terminated after build completion.\nOriginal answer from the Gradle forums:\nMy question is why is the daemon being created when we specify --no-daemon?\nThe process run by Gradle to execute a build is the same whether or not you enable or disable the daemon. The behavior of the process after a build completes is the difference.\nWith the daemon enabled, the process will continue running in the background and can reused for a subsequent build. With the daemon disabled, the process is terminated at the end of the build. Even with the daemon disabled, you will still see a process labeled as a daemon. It doesn\u2019t mean it will continue running in the background like a daemon.",
    "How to expose docker container port and call from postman?": "Looks like your server.js is listening on port 6000 of your container. You need to bind port 6000 of your container to port 6000 of your host (You're currently binding port 80 of the container to your host's port 6000)\ndocker run -p 6000:6000 ... <image>\nAlso make sure your process is listening on host 0.0.0.0 (instead of localhost). Container's localhost is not the same as your host's localhost\napp.listen('0.0.0.0',port, () => console.log(`Example app listening on port ${port}!`));",
    "How to access Docker build context path inside Dockerfile": "You don't need docker build context location known inside your dockerfile.\nWhat you need to know is:\nLocation of you build context. (say C:\\work\\Personal\\mycontext which contains files and folders that you need to copy inside docker container)\nLocation of dockerfile (say C:\\work\\Personal\\API\\api-service\\Dockerfile)\nAlso you need to know relative file path structure of your context. Like\n- C:\\work\\Personal\\mycontext\n  |\n   - scripts\n     |\n     - start.sh\n  |\n  - create.py\n  |\n  - target\n    |\n    - maven\n      |\n      - abc.jar\nIn this case your dockerfile will contain appropriate commands like COPY scripts /scripts that copy these files assuming its running from the context folder C:\\work\\Personal\\mycontext\nYour docker build command will be\ndocker build -f C:\\work\\Personal\\API\\api-service\\Dockerfile -t image:version1 C:\\work\\Personal\\mycontext\nNote: Here -f option specify location of dockerfile in this case its C:\\work\\Personal\\API\\api-service\\Dockerfile and C:\\work\\Personal\\mycontext specify location of docker build context.\nIrrespective of location from where the docker build commands runs, it will work as long as you provide exact location of dockerfile and docker build context.\nMore info here.",
    "Does docker EXPOSE refer to the container port or the host port?": "The EXPOSE instruction documents the port on which an application inside the container is expected to be listening. The important word there is \"documents\". It does not change the behavior of docker running your container, it does not publish the port, and does not impact the ability to connect between containers.\nWhether or not you expose the port, you need to separately publish the port to access it from outside the container network. And whether or not you expose the port, you can connect between containers on the same docker network.\nThere are various tools that can use this image metadata to automatically discover your application. This includes the -P flag to publish all container ports on random high numbered host ports. You will also see reverse proxies (like traefik) use this when querying the docker engine to determine the default port to use for your container.",
    "How to get docker architecture, like amd64, arm32v7, in alpine linux?": "Image recipes for different architectures have their own Dockerfiles, so it's just a matter of picking the right one to work with (hope I got your question right).\nThere are available Node images for Alpine targeting various architectures, for example:\ndocker pull ppc64le/node:8-alpine\nIs the Node.js 8.12.0 image for PPC64LE on Alpine 3.8.\nEdit after clarification:\nFor a multiarch Dockerfile that builds differently depending on the target architecture, you could resolve arch name on runtime, by checking the result of uname -m and using shell conditionals, for example:\nRUN /bin/ash -c 'set -ex && \\\n    ARCH=`uname -m` && \\\n    if [ \"$ARCH\" == \"x86_64\" ]; then \\\n       echo \"x86_64\" && \\\n       apk add some-package; \\\n    else \\\n       echo \"unknown arch\" && \\\n       apk add some-other-package; \\\n    fi'",
    "Preventing access to code inside of a docker container": "Anybody who has your image can always do\ndocker run -u root imagename sh\nAnybody who can run Docker commands at all has root access to their system (or can trivially give it to themselves via docker run -v /etc:/hostetc ...) and so can freely poke around in /var/lib/docker to see what's there. It will have all of the contents of all of the images, if scattered across directories in a system-specific way.\nIf your source code is actually secret, you should make sure you're using a compiled language (C, Go, Java kind of) and that your build process doesn't accidentally leak the source code into the built image, and it will be as secure as anything else where you're distributing binaries to end users. If you're using a scripting language (Python, JavaScript, Ruby) then intrinsically the end user has to have the code to be able to run the program.",
    "Run docker as root verus non-root": "The docker group grants privileges equivalent to the root user\nBy default yes. This is also true for any user that can run a docker container on the machine.\nThe reason is that by default when you are running as root inside the container, this will map to root on the host machine. Thus you can bind some sensitive folders from the host onto the container, and execute privileged actions on those mounts since the user inside the container is root (pid 0).\nThe solution for that is to enable the user-namespace that basically would map the root user inside the container into a non-root user on the machine.\nSecond question (run as root user): assume I followed the steps above (create docker group and add user to it). Yet I specify \"USER root\" in a Dockerfile (example below). When I run this container, it will run as root regardless of the setting above, correct?\nThere are several points here:\nBy default, USER root is the default, so you don't have to specify it. (Unless the base image explicitly sets a user other than root)\nFrom the perspective of the host machine, a docker container is just a normal process. Every process has an owner. This owner is the host machine user that executed the docker runcommand. The USER root instruction has nothing to do with this owner. The USER instruction only specifies the user inside the container that will start the process inside the container that is different from the owner of the container.",
    "Docker build: error: ER_NOT_SUPPORTED_AUTH_MODE: MySQL client": "version: '3'\n\nservices:\n\n  db:\n    image: mysql\n    command: --default-authentication-plugin=mysql_native_password\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWOR=example\nI found solution on Docker hub in mysql image. Its set \"command\" for docker-compose.yml that change mode of auth.",
    "What is a Docker build stage?": "A stage is the creation an image. In a multi-stage build, you go through the process of creating more than one image, however you typically only tag a single one (exceptions being multiple builds, building a multi-architecture image manifest with a tool like buildx, and anything else docker releases after this answer).\nEach stage, building a distinct image, starts from a FROM line in the Dockerfile. One stage doesn't inherit anything done in previous stages, it is based on its own base image. So if you have the following:\nFROM alpine as stage1\nRUN apk add your_tool\n\nFROM alpine as stage2\nRUN your_tool some args\nyou will get an error since your_tool is not installed in the second stage.\nWhich stage do you get as output from the build? By default the last stage, but you can change that with the docker image build --target stage1 . to build the stage with the name, stage1 in this example. The classic docker build will run from the top of the Dockerfile until if finishes the target stage. Buildkit builds a dependency graph and builds stages concurrently and only if needed, so do not depend on this ordering to control something like a testing workflow in your Dockerfile (buildkit can see if nothing in the test stage is needed in your release stage and skip building the test).\nWhat's the value of multiple stages? Typically, its done to separate the build environment from the runtime environment. It allows you to perform the entire build inside of docker. This has two advantages.\nFirst, you don't require an external Makefile and various compilers and other tools installed on the host to compile the binaries that then get copied into the image with a COPY line, anyone with docker can build your image.\nAnd second, the resulting image doesn't include all the compilers or other build time tooling that isn't needed at runtime, resulting in smaller and more secure images. The typical example is a java app with maven and a full JDK to build, a runtime with just the jar file and the JRE.\nIf each stage makes a separate image, how do you get the jar file from the build stage to the run stage? That comes from a new option to the COPY command, --from. An oversimplified multi-stage build looks like:\nFROM maven as build\nCOPY src /app/src\nWORKDIR /app/src\nRUN mvn install\n\nFROM openjdk:jre as release\nCOPY --from=build /app/src/target/app.jar /app\nCMD java -jar /app/app.jar\nWith that COPY --from=build we are able to take the artifact built in the build stage and add it to the release stage, without including anything else from that first stage (no layers of compile tools like JDK or Maven get added to our second stage).\nHow is the FROM x as y and the COPY --from=y /a /b working together? The FROM x as y is defining an image name for the duration of this build, in this case y. Anywhere later in the Dockerfile that you would put an image name, you can put y and you'll get the result of this stage as your input. So you could say:\nFROM upstream as mybuilder\nRUN apk add common_tools\n\nFROM mybuilder as stage2\nRUN some_tool arg2\n\nFROM mybuilder as stage3\nRUN some_tool arg3\n\nFROM minimal_base as release\nCOPY --from=stage2 /bin2 /\nCOPY --from=stage3 /bin3 /\nNote how stage2 and stage3 are each FROM mybuilder that is the output of the first stage.\nThe COPY --from=y allows you to change the context where you are copying from to be another image instead of the build context. It doesn't have to be another stage. So, for example, you could do the following to get a docker binary in your image:\nFROM alpine\nCOPY --from=docker:stable /usr/local/bin/docker /usr/local/bin/\nFurther documentation on this is available at: https://docs.docker.com/develop/develop-images/multistage-build/",
    "PHP cannot write in docker container": "Apache runs PHP with the user www-data, this user needs to have write access on your host directory ./public_html\nTo fix that, go to your docker-compose directory and execute the following command to change the owner of your public_htmldirectory and all files inside.\nsudo chown -R www-data:www-data public_html\nAfter that you need to allow users in the group \"www-data\" to edit files\n# To change all the directories to 775 \n# (write for user & group www-data, read for others):\nfind public_html -type d -exec chmod 775 {} \\;\n\n# To change all the files to 664\n# (write for user & group www-data, read for others):\nfind public_html -type f -exec chmod 664 {} \\;\nIn order for your current user to edit these files you need to add it to the www-data group :\nsudo usermod -aG www-data $USER",
    "Docker build argument": "Docker has ARG that you can use here\nFROM    ubuntu:14.04\n\nMAINTAINER Karl Morrison\n\nARG myVarHere\n\nRUN do-something-here myVarHere\nAnd then build using --build-arg:\ndocker build --build-arg myVarHere=value",
    "How to run Redis docker container for Mac Silicon?": "You should use arm64v8/redis instead of the default. So, replace:\nFROM redis:alpine\nFor:\nFROM arm64v8/redis:alpine\nMore info here: https://hub.docker.com/r/arm64v8/redis\nAlternatively, you can use the --platform arg or use the TARGETPLATFORM, as explained here:\nhttps://nielscautaerts.xyz/making-dockerfiles-architecture-independent.html",
    "Install homebrew using Dockerfile": "The eval happens in the first RUN statement, but is not persisted through to the next one. You want to join the two.\nRUN git clone https://github.com/Homebrew/brew ~/.linuxbrew/Homebrew \\\n&& mkdir ~/.linuxbrew/bin \\\n&& ln -s ../Homebrew/bin/brew ~/.linuxbrew/bin \\\n&& eval $(~/.linuxbrew/bin/brew shellenv) \\\n&& brew --version\nGenerally speaking, any environment changes you perform in one shell instance (one RUN statement, or your ENTRYPOINT) will be lost as soon as that instance terminates. See also Global environment variables in a shell script",
    "Connect to VPN with Podman": "It turns out that you are blocked by SELinux: after running the client container and trying to access /dev/net/tun inside it, you will get the following AVC denial in the audit log:\ntype=AVC msg=audit(1563869264.270:833): avc:  denied  { getattr } for  pid=11429 comm=\"ls\" path=\"/dev/net/tun\" dev=\"devtmpfs\" ino=15236 scontext=system_u:system_r:container_t:s0:c502,c803 tcontext=system_u:object_r:tun_tap_device_t:s0 tclass=chr_file permissive=0\nTo allow your container configuring the tunnel while staying not fully privileged and with SELinux enforced, you need to customize SELinux policies a bit. However, I did not find an easy way to do this properly.\nLuckily, there is a tool called udica, which can generate SELinux policies from container configurations. It does not provide the desired policy on its own and requires some manual intervention, so I will describe how I got the openvpn container working step-by-step.\nFirst, install the required tools:\n$ sudo dnf install policycoreutils-python-utils policycoreutils udica\nCreate the container with required privileges, then generate the policy for this container:\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z --name ovpn peque/vpn\n$ podman inspect ovpn | sudo udica -j - ovpn_container\n\nPolicy ovpn_container created!\n\nPlease load these modules using: \n# semodule -i ovpn_container.cil /usr/share/udica/templates/base_container.cil\n\nRestart the container with: \"--security-opt label=type:ovpn_container.process\" parameter\nHere is the policy which was generated by udica:\n$ cat ovpn_container.cil \n(block ovpn_container\n    (blockinherit container)\n    (allow process process ( capability ( chown dac_override fsetid fowner mknod net_raw setgid setuid setfcap setpcap net_bind_service sys_chroot kill audit_write net_admin ))) \n\n    (allow process default_t ( dir ( open read getattr lock search ioctl add_name remove_name write ))) \n    (allow process default_t ( file ( getattr read write append ioctl lock map open create  ))) \n    (allow process default_t ( sock_file ( getattr read write append open  ))) \n)\nLet's try this policy (note the --security-opt option, which tells podman to run the container in newly created domain):\n$ sudo semodule -i ovpn_container.cil /usr/share/udica/templates/base_container.cil\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z --security-opt label=type:ovpn_container.process peque/vpn\n<...>\nERROR: Cannot open TUN/TAP dev /dev/net/tun: Permission denied (errno=13)\nUgh. Here is the problem: the policy generated by udica still does not know about specific requirements of our container, as they are not reflected in its configuration (well, probably, it is possible to infer that you want to allow operations on tun_tap_device_t based on the fact that you requested --device /dev/net/tun, but...). So, we need to customize the policy by extending it with few more statements.\nLet's disable SELinux temporarily and run the container to collect the expected denials:\n$ sudo setenforce 0\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z --security-opt label=type:ovpn_container.process peque/vpn\nThese are:\n$ sudo grep denied /var/log/audit/audit.log\ntype=AVC msg=audit(1563889218.937:839): avc:  denied  { read write } for  pid=3272 comm=\"openvpn\" name=\"tun\" dev=\"devtmpfs\" ino=15178 scontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tcontext=system_u:object_r:tun_tap_device_t:s0 tclass=chr_file permissive=1\ntype=AVC msg=audit(1563889218.937:840): avc:  denied  { open } for  pid=3272 comm=\"openvpn\" path=\"/dev/net/tun\" dev=\"devtmpfs\" ino=15178 scontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tcontext=system_u:object_r:tun_tap_device_t:s0 tclass=chr_file permissive=1\ntype=AVC msg=audit(1563889218.937:841): avc:  denied  { ioctl } for  pid=3272 comm=\"openvpn\" path=\"/dev/net/tun\" dev=\"devtmpfs\" ino=15178 ioctlcmd=0x54ca scontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tcontext=system_u:object_r:tun_tap_device_t:s0 tclass=chr_file permissive=1\ntype=AVC msg=audit(1563889218.947:842): avc:  denied  { nlmsg_write } for  pid=3273 comm=\"ip\" scontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tcontext=system_u:system_r:ovpn_container.process:s0:c138,c149 tclass=netlink_route_socket permissive=1\nOr more human-readable:\n$ sudo grep denied /var/log/audit/audit.log | audit2allow\n\n\n#============= ovpn_container.process ==============\nallow ovpn_container.process self:netlink_route_socket nlmsg_write;\nallow ovpn_container.process tun_tap_device_t:chr_file { ioctl open read write };\nOK, let's modify the udica-generated policy by adding the advised allows to it (note, that here I manually translated the syntax to CIL):\n(block ovpn_container\n    (blockinherit container)\n    (allow process process ( capability ( chown dac_override fsetid fowner mknod net_raw setgid setuid setfcap setpcap net_bind_service sys_chroot kill audit_write net_admin )))\n\n    (allow process default_t ( dir ( open read getattr lock search ioctl add_name remove_name write )))\n    (allow process default_t ( file ( getattr read write append ioctl lock map open create  )))\n    (allow process default_t ( sock_file ( getattr read write append open  )))\n\n    ; This is our new stuff.\n    (allow process tun_tap_device_t ( chr_file ( ioctl open read write )))\n    (allow process self ( netlink_route_socket ( nlmsg_write )))\n)\nNow we enable SELinux back, reload the module and check that the container works correctly when we specify our custom domain:\n$ sudo setenforce 1\n$ sudo semodule -r ovpn_container\n$ sudo semodule -i ovpn_container.cil /usr/share/udica/templates/base_container.cil\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z --security-opt label=type:ovpn_container.process peque/vpn\n<...>\nInitialization Sequence Completed\nFinally, check that other containers still have no these privileges:\n$ podman run -it --cap-add NET_ADMIN --device /dev/net/tun -v $PWD:/vpn:Z peque/vpn\n<...>\nERROR: Cannot open TUN/TAP dev /dev/net/tun: Permission denied (errno=13)\nYay! We stay with SELinux on, and allow the tunnel configuration only to our specific container.",
    "How to accept the license agreement when building rti-connext-dds-5.3.1 with docker build?": "You can use the env variable \"RTI_NC_LICENSE_ACCEPTED=yes\". Your dockerfile will look something like this:\nFROM ubuntu:bionic\n\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt-get update && \\\n    apt-get install -y apt-utils debconf-utils gnupg2 lsb-release && \\\n    apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 421C365BD9FF1F717815A3895523BAEEB01FA116 && \\\n    echo \"deb http://packages.ros.org/ros2/ubuntu `lsb_release -sc` main\" > /etc/apt/sources.list.d/ros2-latest.list && \\\n    apt-get update \nRUN RTI_NC_LICENSE_ACCEPTED=yes apt-get install rti-connext-dds-5.3.1\n\nWORKDIR /home",
    "Unable to execute *catkin* commands using *RUN* in Dockerfile": "Your second case will build because CMD is not executed during the build. It simply defines the default command to execute when starting the container. See the docker CMD docs for more information.\nThe root of your problem is that ~/.bashrc is not being called in the shell used by the RUN instruction. As a result, the environment variables are not present to allow catkin_make or other ros commands to work. I address this issue in my images by running a command similar to the below.\nRUN . /opt/ros/kinetic/setup.sh && \\\n    catkin_make\nYou will need to activate the environment in each RUN instruction that needs those environment variables because the shell is not re-used between RUN commands.\nEdited to include the improvement from David Maze.",
    "Can i reference a Dockerfile in a Dockerfile?": "Generally things in Docker space like the docker run command and the FROM directive will use a local image if it exists; it doesn't need to be pushed to a repository. That means you can build your first image and refer to it in the later Dockerfile by name. (There's no way to refer to the other Dockerfile per se.)\nNewer versions of Docker have an extended form of the Dockerfile COPY command which\naccepts a flag --from=<name|index>.... In case a build stage with a specified name can\u2019t be found an image with the same name is attempted to be used instead.\nSo if you ahead of time run\ndocker build -t build-env ~/build\nthen the exact syntax you show in your proposed Dockerfile will work\nFROM some-image\nCOPY --from=build-env /built .\nand it doesn't matter that the intermediate build image isn't actually pushed anywhere.",
    "Docker bind mount volumes do not propagate changes events watched by angular `ng serve` execution": "In order to make the example working, replace step 3 by the commands below:\n// Create the new app\ndocker run --rm --mount type=bind,src=$PWD,dst=/volumes my_angular_image new my-app --directory app --style scss\n// Change ownership of the generated app\nsudo chown -R $USER:$USER .\n// Configure angular-cli polling:\nsed -i 's/\\\"styleExt\\\": \\\"scss\\\",/\"styleExt\": \"scss\", \"poll\": 1000,/g' $PWD/app/.angular-cli.json\nCredits:\n@PavelAgarkov's answer and its usefull links.",
    "How to prevent Docker from re-running pip installs every time I modify code [duplicate]": "This question, Docker how to run pip requirements.txt only if there was a change?, seems to pertain to my situation. Every time I modify the code I invalidate the Docker build cache, even though requirements.txt is unchanged. So to avoid having to re-run pip installs every time, it is recommended to COPY the requirements.txt and RUN pip install -r requirements.txt in a separate step.",
    "making docker containers communicate through port": "I think you have 3 options here to get this working:\nCreate a docker network to connect the hosts:\ndocker network create --driver bridge sample-sendr-rcv-test\ndocker run  --name=\"testListen\" --env LISTEN_HOST=\"0.0.0.0\" --env LISTEN_PORT=\"5555\" --network=sample-sendr-rcv-test -d docker.io/ayonnayihan/sample-sendr-rcv-test:receiver0.1\ndocker run --name=\"testTalk\" --env SEND_HOST=\"testListen\" --env SEND_PORT=\"5555\" --network=sample-sendr-rcv-test -d docker.io/ayonnayihan/sample-sendr-rcv-test:sender0.1\nUse docker-compose with a docker-compose.yml like:\nversion: '2'\nservices:\n  sender:\n    image: docker.io/ayonnayihan/sample-sendr-rcv-test:sender0.1\n    # build: sender\n    environment:\n      SEND_HOST: receiver\n      SEND_PORT: 5555\n  receiver:\n    image: docker.io/ayonnayihan/sample-sendr-rcv-test:receiver0.1\n    # build: receiver\n    environment:\n      LISTEN_HOST: '0.0.0.0'\n      LISTEN_PORT: 5555\nUse host networking:\ndocker run  --name=\"testListen\" --env LISTEN_HOST=\"127.0.0.1\" --env LISTEN_PORT=\"5555\" --net=host -d docker.io/ayonnayihan/sample-sendr-rcv-test:receiver0.1\ndocker run --name=\"testTalk\" --env SEND_HOST=\"localhost\" --env SEND_PORT=\"5555\" --net=host -d docker.io/ayonnayihan/sample-sendr-rcv-test:sender0.1\nThe third option is the most similar to what you are currently doing but I would not recommend it for reasons explained below. Either of the other options will work but may not be worth learning docker-compose if you are just starting with docker.\nThe reason you are having an issue is that the containers each have their own idea of 'localhost' because they are in a different network namespace. This means that 'localhost' on your 'testTalk' container does not resolve to the host where your listener is running. When you use --net=host (option 3 above) you are removing the separate namespace for the containers, thus removing some of the security benefits of using docker.",
    "Creating bash script from Dockerfile strips comments": "You're right, it's a little weird that Docker interprets that as a Dockerfile comment instead of a comment inside a string. As a workaround, I got the following to work\nFROM ubuntu:latest\n\nRUN printf \"#!/bin/bash \\\n\\n# This is a bash comment inside the script \\\n\\nls -l\\n\" > /script.sh\n\nRUN cat /script.sh\nResults in this output\nStep 3 : RUN cat /script.sh\n ---> Running in afc19e228656\n#!/bin/bash \n# This is a bash comment inside the script \nls -l\nIf you move \\n to the beginning of the comment line it still generates the correct output but no longer treats that line as a Dockerfile comment line.\nAssuming I found the right command parsing code, and I'm reading it correctly, Docker strips comments out before attempting to parse the line to see if it has any commands on it.",
    "build docker image from local (unpublished) image": "I think you have to specify the version explicitly. When I did, I was able to build. When I did not, I got the same error.\ndocker build -t munchkin/base:latest -f baseimage .\nAnd then you can use this images.",
    "npm WARN config only Use `--omit=dev`": "It's a weird message to say \"Don't use A, use B instead.\" Changing from the one to the other is what I did to get rid of the warnings on my four containers.\nIt appears that npm-ci no longer includes an \"only\" option, however the --omit=dev seems to do the same thing:\nhttps://docs.npmjs.com/cli/v9/commands/npm-ci#omit\nI think the --only=prod may date back to node 12-14ish, I couldn't find any official references to date it, just that the syntax was used in blog posts dating around 2020 at the latest.",
    "How to solve network connection when RUN yarn install in docker image build?": "I drop this answer here, because nothing about proxy helped in my case.\nTL;DR The version check URL redirects and that causes confusion to yarn\nSo, I went on to examine typical archaic tools, like verbosity check, curl and dig.\nSo, first thing first, I tried to check verbose output:\nBingo #1.\nverbose 0.181073051 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".\n[1/4] Resolving packages...                                                       \nsuccess Already up-to-date.                                                                                                                                          \nDone in 0.12s.                                                                                                                                                       \ninfo There appears to be trouble with your network connection. Retrying...                                                                                           \nverbose 33.254841183 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".                                                                               \ninfo There appears to be trouble with your network connection. Retrying...                                                                                           \nverbose 66.292530305 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".                                                                               \ninfo There appears to be trouble with your network connection. Retrying...                                                                                           \nverbose 99.329186881 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".                                                                               \ninfo There appears to be trouble with your network connection. Retrying...                                                                                           \nverbose 132.366749502 Performing \"GET\" request to \"https://yarnpkg.com/latest-version\".\nWhy not getting an answer from yarnpkg.com? This is insane... So, let's see if this resolves:\n$ dig yarnpkg.com\n\n; <<>> DiG 9.18.7-1+b1-Debian <<>> yarnpkg.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 55602\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n; COOKIE: 9cf3b6f8b290069dbbe8ca8763501b11ea79617323c673e6 (good)\n;; QUESTION SECTION:\n;yarnpkg.com.                   IN      A\n\n;; ANSWER SECTION:\nyarnpkg.com.            300     IN      A       104.18.126.100\nyarnpkg.com.            300     IN      A       104.16.171.99\n\n;; Query time: 11 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1) (UDP)\n;; WHEN: Wed Oct 19 18:43:13 EEST 2022\n;; MSG SIZE  rcvd: 100\nIt does!!! (?)\nSo, let's see what I get from it, by using curl\n$ curl https://yarnpkg.com/latest-version\nRedirecting to https://classic.yarnpkg.com/latest-version\nWHAT? A redirect? Ok, this is new..\nI tried to set the url to check version not to be yarnpkg.com, but classic.yarnpkg.com, but I couldn't find the yarn configuration variable to use.\nSo, I used /etc/hosts.\n; <<>> DiG 9.18.7-1+b1-Debian <<>> classic.yarnpkg.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 35092\n;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n; COOKIE: 72636a6924283d51c4127a8463501bbb42e53d34835e8402 (good)\n;; QUESTION SECTION:\n;classic.yarnpkg.com.           IN      A\n\n;; ANSWER SECTION:\nclassic.yarnpkg.com.    300     IN      CNAME   yarnpkg.netlify.com.\nyarnpkg.netlify.com.    20      IN      A       3.64.200.242\nyarnpkg.netlify.com.    20      IN      A       34.141.11.154\n\n;; Query time: 59 msec\n;; SERVER: 192.168.1.1#53(192.168.1.1) (UDP)\n;; WHEN: Wed Oct 19 18:46:03 EEST 2022\n;; MSG SIZE  rcvd: 138\nSet the first IP to yarnpkg.com\n# /etc/hosts\n34.141.48.9     yarnpkg.com\nBINGO!!! Yarn command finished instantly.\n$ yarn \nyarn install v1.22.19\n[1/4] Resolving packages...\nsuccess Already up-to-date.\nDone in 0.12s.\n$",
    "Docker SDK for Python: how to build an image with custom Dockerfile AND custom context": "Even if the documentation refers to the \"path within the build context to the Dockerfile\", it works for a Dockerfile outside the build context if an absolute path is specified.\nUsing my project tree:\nclient.images.build(\n    path = '/opt/project/src/',\n    dockerfile = '/opt/project/dockerfile/Dockerfile.name',\n    tag = 'image:version'\n)",
    "Use latest curl version on docker": "You can use the downloaded packages directly to solve this problem by installing with the make command.\nFROM debian:10.7\n\nRUN apt-get update && \\\n    apt-get install --yes --no-install-recommends wget build-essential libcurl4 && \\\n    wget https://curl.se/download/curl-7.74.0.tar.gz && \\\n    tar -xvf curl-7.74.0.tar.gz && cd curl-7.74.0 && \\\n    ./configure && make && make install\nNote that it requires running ./configure.\nAfter installation curl will work perfectly in the version you need, in this case, version 7.74.0.\nIf you want to optimize your container, remove the build-essential package, it alone will consume more than 200MB of storage. To do this, add at the end of the compilation:\napt-get autoremove build-essential",
    "Permission denied to run npm install in alpine-chrome docker image": "Try to use the user root\nFROM zenika/alpine-chrome:86-with-node\nUSER root",
    "Azure Devops not supporting build-args with Docker@2": "Is there any other way to pass arguments from Azure Devops through to a dockerfile?\nYes. The BuildAndPush Command doesn't support adding argument.\nBut the Build command support it.\nYou could split buildandpush into Docker build task and docker push task.\nHere is an example:\nsteps:\n- task: Docker@2\n  displayName: Docker Build \n  inputs:\n    command: build\n    repository: $(imageRepository)\n    containerRegistry: $(dockerRegistryServiceConnection)\n    dockerfile: $(dockerfilePath)\n    tags: 5.12.4\n    arguments: '--build-arg test_val=\"$(build.buildid)\" '\n- task: Docker@2\n  displayName: Docker Push\n  inputs:\n    command: push\n    repository: $(imageRepository)\n    containerRegistry: $(dockerRegistryServiceConnection)\n    tags: 5.12.4\nDocker Build Step Result:",
    "systemctl not found while building mongo image": "To be precise\nRUN ln -s /bin/echo /bin/systemctl\nRUN apt-get -qqy install  mongodb-org",
    "Docker: Error code 127 when executing shell script": "Docker is executing the install-deps.sh script. The issue is with a command inside install-deps.sh that is not recognized when docker attempts to run the script.\nAs you can see the script returns an error code of 127 meaning that a command within the file does not exist.\nFor instance - try this:\ntouch test.sh\necho \"not-a-command\" >> test.sh\nchmod 755 test.sh\n/bin/sh -c \"./test.sh\"\nOutput:\n/root/test.sh: line 1: not-a-command: command not found\nNow check the exit code:\necho $?\n127\nI would suggest running the script inside an interactive environment to identify/install the command that is not found.",
    "How to mount a host directory with docker-compose, with the \"~/path/on/host\" to be specified when running the host, not in the docker-compose file": "version: '3'\n\nservices:\n  my-app:\n    build:\n      context: .\n      dockerfile: ./path/to/Dockerfile\n    volumes:\n      - ~/path/on/host:/path/on/container\n    ports:\n      - \"3000:3000\"\nThen you can start your service with docker-compose up -d. Make sure to stop and remove first the container you started using docker commands otherwise you will get conflicts.\nEDIT BASED ON COMMENT:\nCreate a .env file with the contents:\nHOST_PATH=~/path/on/host\nand change your docker-compose.yml:\nversion: '3'\n\nservices:\n  my-app:\n    build:\n      context: .\n      dockerfile: ./path/to/Dockerfile\n    volumes:\n      - ${HOST_PATH}:/path/on/container\n    ports:\n      - \"3000:3000\"",
    "what is the difference between creating dockerfile for x86, armv7 32 and arm 64": "Every Dockerfile starts with a\nFROM <base_image>\ndeclaration, so you will have to choose a base image that will be able to run on your system/architecture and build on top of it.\nFrom here:\nDocker Official Images\nSee Docker's documentation for a good high-level overview of the program.\nArchitectures other than amd64?\nSome images have been ported for other architectures, and many of these are officially supported (to various degrees).\nArchitectures officially supported by Docker, Inc. for running Docker: (see download.docker.com) - IBM z Systems (s390x): https://hub.docker.com/u/s390x/ - ARMv7 32-bit (arm32v7): https://hub.docker.com/u/arm32v7/ - Windows x86-64 (windows-amd64): https://hub.docker.com/u/winamd64/ - Linux x86-64 (amd64): https://hub.docker.com/u/amd64/\nOther architectures built by official images: (but not officially supported by Docker, Inc.)\nIBM POWER8 (ppc64le): https://hub.docker.com/u/ppc64le/\nx86/i686 (i386): https://hub.docker.com/u/i386/\nARMv8 64-bit (arm64v8): https://hub.docker.com/u/arm64v8/\nARMv6 32-bit (arm32v6): https://hub.docker.com/u/arm32v6/ (Raspberry Pi 1, Raspberry Pi Zero)\nARMv5 32-bit (arm32v5): https://hub.docker.com/u/arm32v5/\n\nYou may also find other users/sources that use Docker Hub to upload their images. While doing some tests with ffmpeg on a Raspberry Pi, I decided to trust the images provided by resin.io (update: now they are called balena.io and here is their Docker hub: hub.docker.com/u/balena)\nIf you are interested in learning how an image is created, you can check its Dockerfile. For example for Node.js on arm64v8 see the Dockerfiles here",
    "How to clone git repo using Dockerfile": "If you don't want to install git you can use multi stage builds in your Dockerfile,\nFROM alpine/git:latest\nWORKDIR /clone-workspace\nRUN git clone https://github.com/umairnow/LocalizableGenerator.git\n\nFROM mattes/hello-world-nginx\nCOPY --from=0 /clone-workspace/LocalizableGenerator /path/to/file",
    "How to set RAM memory of a Docker container by terminal or DockerFile": "Don't rely on /proc/meminfo for tracking memory usage from inside a docker container. /proc/meminfo is not containerized, which means that the file is displaying the meminfo of your host system.\nYour /proc/meminfo indicates that your Host system has 2G of memory available. The only way you'll be able to make 6G available in your container without getting more physical memory is to create a swap partition.\nOnce you have a swap partition larger or equal to ~4G, your container will be able to use that memory (by default, docker imposes no limitation to running containers).\nIf you want to limit the amount of memory available to your container explicitly to 6G, you could do docker run -p 5311:5311 --memory=2g --memory-swap=6g my-linux, which means that out of a total memory limit of 6G (--memory-swap), upto 2G may be physical memory (--memory). More information about this here.\nThere is no way to set memory limits in the Dockerfile that I know of (and I think there shouldn't be: Dockerfiles are there for building containers, not running them), but docker-compose supports the above options through the mem_limit and memswap_limit keys.",
    "How to get a docker container to the state: dead for debugging?": "You've asked for a dead container.\nTL;DR: This is how to create a dead container\nDon't do this at home:\nID=$(docker run --name dead-experiment -d -t alpine sh)\ndocker kill dead-experiment\ntest \"$ID\" != \"\" && chattr +i -R /var/lib/docker/containers/$ID\ndocker rm -f dead-experiment\nAnd voila, docker could not delete the container root directory, so it falls to a status=dead:\ndocker ps -a -f status=dead\nCONTAINER ID        IMAGE         COMMAND       CREATED             STATUS        PORTS         NAMES\n616c2e79b75a        alpine        \"sh\"          6 minutes ago       Dead                        dead-experiment\nExplanation\nI've inspected the source code of docker and saw this state transition:\ncontainer.SetDead()\n// (...)\nif err := system.EnsureRemoveAll(container.Root); err != nil {\n    return errors.Wrapf(err, \"unable to remove filesystem for %s\", container.ID)\n}\n// (...)\ncontainer.SetRemoved()\nSo, if docker cannot remove the container root directory, it remain as dead and does not continue to the Removed state. So I've forced the file permissions to not permit root remove files (chattr -i).\nPS: to revert the directory permissions do this: chattr -i -R /var/lib/docker/containers/$ID",
    "Change hostname after running a container": "Some discussions here: https://github.com/docker/docker/issues/8902\nWhat I got from above discussion is\nadd SYS_ADMIN cap when run the container: https://github.com/docker/docker/issues/8902#issuecomment-218911749\nuse nsenter https://github.com/docker/docker/issues/8902#issuecomment-241129543",
    "Cannot change owner of Docker Volume directory to non-root user": "When you declare a directory as a VOLUME, you effectively can't use it in a Dockerfile any more. The basic reason is that volumes are set up when the container is run, not built.\nIn this case, you could simply move the VOLUME statement to the end of the Dockerfile. Any data in the image at that directory will be copied into the volume when the container is started.",
    "Pip3 is unable to install requirements.txt during docker build": "pip3 freeze outputs the package and its version installed in the current environment, no matter the package installed by pip or with other methods.\nIn fact, apt-clone==0.2.1 comes from debian package repo, not from pypi.org, see next:\n$ pip3 freeze | grep apt-clone\n$ apt-get install -y apt-clone\n$ dpkg -l apt-clone\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name           Version      Architecture Description\n+++-==============-============-============-=================================\nii  apt-clone      0.4.1        all          Script to create state bundles\n$  dpkg -L apt-clone\n/.\n/usr\n/usr/share\n/usr/share/doc\n/usr/share/doc/apt-clone\n/usr/share/doc/apt-clone/copyright\n/usr/share/doc/apt-clone/changelog.gz\n/usr/share/man\n/usr/share/man/man8\n/usr/share/man/man8/apt-clone.8.gz\n/usr/lib\n/usr/lib/python3\n/usr/lib/python3/dist-packages\n/usr/lib/python3/dist-packages/apt_clone.py\n/usr/lib/python3/dist-packages/apt_clone-0.2.1.egg-info\n/usr/bin\n/usr/bin/apt-clone\n$ pip3 freeze | grep apt-clone\napt-clone==0.2.1\nYou could see from above, the apt_clone.py & apt_clone-0.2.1.egg-info are installed by debian package apt-clone, the 0.4.1 is just the debian package version, while 0.2.1 is the python package version.\nSo, for apt-clone similar, you need to install them with apt although they are seen in pip3 freeze.",
    "If I am using docker-compose.yml why would I still need a Dockerfile \u2013 an if so, how many?": "In your example, you are using already built containers (node:15.8.0 / redis), that's why docker-compose doesn't need any Dockerfile.\nThe Dockerfile is needed when you want to build your own image. In that case, docker-compose will need instructions on how to build your image.\nSo just to summarize - if you want to use some existing container \"as is\" - you don't need Docekrfile. docker-compose will pull the image from docker registry and it'll be ready to use without any modifications. On the other hand, if you want to build your own container (e.g. building an application), you'll need Dockerfile.",
    "Leading spaces inside Dockerfile for readability": "You can indent lines in Dockerfile, but usually it's used only when breaking long command lines, like:\nRUN export ADMIN_USER=\"mark\" \\\n    && echo $ADMIN_USER > ./mark \\\n    && unset ADMIN_USER\nYou can use indenting for instructions, but i, personally, wouldn't do that -- each instruction creates new layer and it's logical to place them with equal indent. As extra indenting like:\nFROM python:3.8-buster\n  RUN pip --no-cache-dir install poetry gunicorn\nwould look like it introduces sub-layers(and Docker doesn't have such concept).\nBut again, that's personal, and if you and your team agrees on that formatting standard -- there's a bunch of linters that would allow you to use any formatting standard with little(or no) tweaking:\nHaskell Dockerfile Linter -- check it online\nFROM:latest -- check it online\ndockerfile-lint",
    "Setting alias in Dockerfile not working: command not found": "The problem is that the alias only exists for that intermediate layer in the image. Try the following:\nFROM ubuntu\n\nRUN apt-get update && apt-get install python3-pip -y\n\nRUN alias python=python3\nTesting here:\n\u2770mm92400\u2759~/sample\u2771\u2714\u227b docker build . -t testimage\n...\nSuccessfully tagged testimage:latest\n\n\u2770mm92400\u2759~/sample\u2771\u2714\u227b docker run -it testimage bash\nroot@78e4f3400ef4:/# python\nbash: python: command not found\nroot@78e4f3400ef4:/#\nThis is because a new bash session is started for each layer, so the alias will be lost in the following layers.\nTo keep a stable alias, you can use a symlink as python does in their official image:\nFROM ubuntu\n\nRUN apt-get update && apt-get install python3-pip -y \n\n# as a quick note, for a proper install of python, you would\n# use a python base image or follow a more official install of python,\n# changing this to RUN cd /usr/local/bin \n# this just replicates your issue quickly \nRUN cd \"$(dirname $(which python3))\" \\\n    && ln -s idle3 idle \\\n    && ln -s pydoc3 pydoc \\\n    && ln -s python3 python \\ # this will properly alias your python\n    && ln -s python3-config python-config\n\nRUN python -m pip install -r requirements.txt\nNote the use of the python3-pip package to bundle pip. When calling pip, it's best to use the python -m pip syntax, as it ensures that the pip you are calling is the one tied to your installation of python:\npython -m pip install -r requirements.txt",
    "Error: cannot find automatically a registry where to push images - Kamel Kubernetes": "You need to set the container registry where kamel can pull/push images\nFor example\nkamel install --registry=https://index.docker.io/v1/",
    "docker-compose env file set by command line": "The -e flag is meant to pass the environment variables to the container. Add your environment before your docker run command to assign the environment variable to the docker engine so that the environment variable interpolation can be done\ndocker-compose run -e ENVIROMENT=local spring-app\n[...]\nERROR: Couldn't find env file: /Users/sabhat/code/scratch/.env\nENVIROMENT=local docker-compose run spring-app\n[...]\nStarting scratch_docker-mariadb_1\nAs an aside, hope you are aware that docker-compose run is meant to run a one-time command for a service - and it doesn't map the ports and also overrides the run commands defined in the service. You should be using docker-compose up to bring up the entire set of containers",
    "Does the hash '#' in a Dockerfile comment need to be in column 1?": "Looking at the Docker CLI files:\nOn the file parser line 45 we find\nline := strings.TrimLeftFunc(string(scannedBytes), unicode.IsSpace)\nIt trims empty spaces from the left. So if the non-first space character would be a # that would count as a comment for any code that follows the left trim.\nThe isSpace function checks for the following characters\n'\\t', '\\n', '\\v', '\\f', '\\r', ' ', U+0085 (NEL), U+00A0 (NBSP).\nThese would all be removed by the code on line 45 until they encounter a character that does not fit these specifications.\n# Nothing trimmed\n           # 1 tab 7 spaces trimmed\n    0 # 4 spaces trimmed\nThen on line 48 we find where it tests if it's a comment\n  if len(line) > 0 && !strings.HasPrefix(line, \"#\") {\nSo any space characters that are stripped by strings.TrimLeftFunc will not \"invalidate\" a comment.\nSo in conclusion on your question Does the hash '#' in a Dockerfile comment need to be in column 1? the answer is no, it can be preceded by space characters and still remain a comment.\n# Nothing trimmed   < -- comment\n# 1 tab 7 spaces trimmed < -- comment\n0 # 4 spaces trimmed  < -- not a comment",
    "Copy root folder to docker root path?": "Docker daemon runs within the context of the current directory. So you will need to copy the files to the directory from where your are running the Dockerfile.\nRefer: https://github.com/moby/moby/issues/4592",
    "Dockerfile pass environments on docker compose build": "You can set build arguments with docker compose as described here:\ndocker-compose build [--build-arg key=val...]\ndocker-compose build --build-arg REP_USER=myusername --build-arg REP_PASS=mypassword\nBtw, AFAIK build arguments are a compromise between usability and deterministic building. Docker aims to build in a deterministic fashion. That is, wherever you execute the build the produced image should be the same. Therefore, it appears logical that the client ignores the environment (variables) it is executed in.",
    "What's the best way to setup playwright in Apache Airflow in Docker?": "The best way to do it is using playwright docker image as base image, then you won't need to install its dependencies. Take a look to the documentation here: https://playwright.dev/docs/docker",
    "docker-compose up gives 'failed to read dockerfile: error from sender'": "The error is telling you that the Dockerfile was not found, because the path doesn't exist. That's because it is trying to enter the path as folder.\nThe system cannot find the path specified.\nThis comes because you made a mistake in the compose build syntax. There are 2 ways it can be used.\n1. The simple form:\nThis is using ./users/ as context, expecting a Dockerfile to be in this directory.\nuser:\n  build: ./user\n2. The complex form:\nuser:\n  build:\n    context: ./\n    dockerfile: ./users/Dockerfile\nThis lets you separate the context and where the Dockerfile is. In this example, the current folder is used as context, and the Dockerfile is taken from ./users/Dockerfile. It is also useful when you have a different name for your Dockerfile. I.E. Dockerfile.dev.\nNote that this is just an example, I don't know if this would make sense in your project. You need to know what context is the correct one.\nWhat do I mean by context?\nThe docker build command builds Docker images from a Dockerfile and a \u201ccontext\u201d. A build\u2019s context is the set of files located in the specified PATH or URL. The build process can refer to any of the files in the context. For example, your build can use a COPY instruction to reference a file in the context.\nAs example:\ndocker build --file /path/to/Dockerfile /path/to/context",
    "Disadvantage of using --allow-releaseinfo-change for apt-get update": "Potentially - \"Yes, it does add a risk.\"\nThis is evident from reading the documentation on the --allow-releaseinfo-change option in man apt-get. Specifically, man apt-secure describes and discusses its role in ensuring the integrity of the archives from which updates & upgrades are drawn. This risk applies to both signed and unsigned repositories.\nThe documentation further suggests that this risk may be mitigated through the use of speciality options that limit acceptable changes to certain fields (labels) in the repo; for example, suite. See the documentation for all the details.",
    "Failed to install llvm-lite with a Dockerfile": "Firstly, LLVM-Lite requires LLVM.\nSo, it's necessary to install the LLVM correctly, to do this:\nRUN apt-get update && apt-get install -y \\\n    build-essential \\\n    libedit-dev  \\\n    llvm-{version} \\\n    llvm-{version}-dev\nThen, set the environment variable used when building LLVM-Lite and install pip package:\nRUN LLVM_CONFIG=/usr/bin/llvm-config-{version} pip install llvmlite=={version}\nThis will solve your problem.\nTo find out which version of LLVM is compatible with each version of LLVM-Lite go to:\nhttps://github.com/numba/llvmlite#compatibility",
    "Why do I need to mount Docker volumes for both php and nginx? And how to copy for production?": "The reason you need to provide your sources for both nginx and php is rather simple. The webserver needs all of your assets (images, stylesheets, javascript etc.) so that it can serve them to the client. PHP is interpreted serverside hence it requires your PHP source files and any referenced file (configs etc.) in order to execute them. In this case you are using PHP-FPM which is decoupled from the webserver and runs in a standalone fashion.\nMany projects cleanly seperate frontend and backend code and only provide the frontend sources to the webserver and the backend code to the PHP container.\nAnother quick tip regarding docker: It is generally a good idea to compact RUN statements in the Dockerfile to avoid excessive layers in the image. Your Dockerfile could be compacted to:\nFROM php:7.2-fpm-alpine\n\nRUN docker-php-ext-install pdo pdo_mysql \\\n    && chown -R www-data:www-data /var/www \\\n    && chmod 755 /var/www",
    "I am getting a returned a non-zero code: 8 when building my docker file": "If you go to this link, then 3.4.13 doesn't exist anymore\nhttps://www.apache.org/dist/zookeeper/\nYou can change to ENV ZOOKEEPER_VERSION 3.4.14, or just use an existing Zookeeper Docker image",
    "How to create a docker container which every night make a backup of mysql database?": "You could use the cron service from your host system to run the following command as described in the documentation for the mysql docker image:\ncrontab example for running the command every night at 2:00 am:\n00 02 * * * /usr/bin/docker exec db-mysql sh -c 'exec mysqldump --all-databases -uroot -p\"my-secret-pw\"' > /some/path/on/your/host/all-databases.sql\nAlternatively you could run another container designed just for this task such as deitch/mysql-backup:\ndocker run --name db-mysql -d \\\n    -e MYSQL_ROOT_PASSWORD=my-secret-pw \\\n    -e MYSQL_USER=my-user \\\n    -e MYSQL_PASSWORD=my-user-password \\\n    -e MYSQL_DATABASE=my-db \\\n    mysql:latest\n\ndocker run -d --restart=always \\\n    --name=db-backup \\\n    -e DB_DUMP_BEGIN=0200 \\\n    -e DB_SERVER=db-mysql \\\n    -e DB_USER=my-user \\\n    -e DB_PASS=my-user-password \\\n    -e DB_NAMES=my-db \\\n    -e DB_DUMP_TARGET=/db \\\n    -v /somewhere/on/your/host/:/db \\\n    databack/mysql-backup\nYou also need to make sure the /somewhere/on/your/host/ folder is writable by users of group 1005:\nsudo chgrp 1005 /somewhere/on/your/host/\nsudo chmod g+rwX /somewhere/on/your/host/\nBut this container must have a mean to connect to your db-mysql container. For that you create a docker network and connect both containers to it:\ndocker network create mysql-backup-net\ndocker network connect mysql-backup-net db-backup\ndocker network connect mysql-backup-net db-mysql",
    "How to create a file using touch in Dockerfile or docker-compose?": "I am not sure about the docker-compose.yml but the dockerfile that you have seems to be working for me.\nThe Dockerfile looks like this,\nFROM python:3.6-slim\n\nRUN mkdir /app\nWORKDIR /\nRUN touch /app/modbus.db\nBuild the dockerfile,\ndocker build -t test .\nSending build context to Docker daemon  2.048kB\nStep 1/4 : FROM python:3.6-slim\n ---> 903e8a0f0681\nStep 2/4 : RUN mkdir /app\n ---> Using cache\n ---> c039967bf463\nStep 3/4 : WORKDIR /\n ---> Using cache\n ---> c8c81ac01f50\nStep 4/4 : RUN touch /app/modbus.db\n ---> Using cache\n ---> 785916fe4cea\nSuccessfully built 785916fe4cea\nSuccessfully tagged test:latest\nBuild the container,\ndocker run -dit test\n52cde500cda015f170140ae9e7174a0367b29265a49a3742173946b686179fb3\nI ssh'ed into the container and was able to find the file.\ndocker exec -it 52cde500cda015f170140ae9e7174a0367b29265a49a3742173946b686179fb3 /bin/bash\nroot@52cde500cda0:/# ls\napp  bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\nroot@52cde500cda0:/# cd app\nroot@52cde500cda0:/app# ls\nmodbus.db ",
    "Docker: Is the server running on host localhost error": "Your application tries to connect to PostgreSQL running on localhost. PostgreSQL, though, is obviously not running on localhost. You'll have to add a container to your docker compose configuration that starts PostgreSQL. Then, configure your Python application to use that name instead of localhost.",
    "Different process are running as PID 1 when running CMD/ENTRYPOINT in shell form when the base images is centos vs ubuntu:trusty": "This is the behavior of bash. Docker is still running the command with a shell which you can identify with an inspect:\n$ docker inspect test-centos-entrypoint --format '{{.Config.Entrypoint}}'\n[/bin/sh -c ping localhost]\nYou can see the version of /bin/sh (note the GNU bash part):\n$ docker exec -it quicktest /bin/sh --version\nGNU bash, version 4.2.46(2)-release (x86_64-redhat-linux-gnu)\nCopyright (C) 2011 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\n\nThis is free software; you are free to change and redistribute it.                               \nThere is NO WARRANTY, to the extent permitted by law.\nThe ubuntu version of /bin/sh (possibly dash) doesn't even support the --version flag and is not linked to bash. But if you change the ubuntu image to use bash instead of /bin/sh, you'll see the behavior matching centos:\n$ cat df.ubuntu-entrypoint\nFROM ubuntu:trusty\nENTRYPOINT [ \"/bin/bash\", \"-c\", \"ping localhost\" ]\n\n$ DOCKER_BUILDKIT=0 docker build -t test-ubuntu-entrypoint -f df.ubuntu-entrypoint .\nSending build context to Docker daemon  23.04kB\nStep 1/2 : FROM ubuntu:trusty\n ---> 67759a80360c\nStep 2/2 : ENTRYPOINT [ \"/bin/bash\", \"-c\", \"ping localhost\" ]\n ---> Running in 5c4161cafd6b\nRemoving intermediate container 5c4161cafd6b\n ---> c871fe2e2063\nSuccessfully built c871fe2e2063\nSuccessfully tagged test-ubuntu-entrypoint:latest\n\n$ docker run -d --name quicktest2 --rm test-ubuntu-entrypoint\n362bdc75e4a960854ff17cf5cae62a3247c39079dc1290e8a85b88114b6af694\n\n$ docker exec -it quicktest2 ps -ef\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 13:05 ?        00:00:00 ping localhost\nroot         8     0  0 13:05 pts/0    00:00:00 ps -ef",
    "Possible to get the git branch name in a dockerfile?": "Dockerfile supports build arguments and environment variables. So its possible to specify which branch to use and then decide command based upon the branch. For example, to pass build args use docker build--build-arg =. Then you can set a variable based upon the git_branch variable that decides the env to use.\nFROM busybox\nARG git_branch\nRUN echo \u201cBuilding $git_branch\u201d",
    "How to user docker exec with zsh": "It looks like zsh is not installed on that image as /bin/zsh would likely be the path. You can create a new Dockerfile that uses the base image and installs zsh, or you can install it within the container temporarily and launch from bash.",
    "Docker Python script can't find file": "There are two issues I've identified so far. Maya G points out a third in the comments below.\nIncorrect conditional logic\nYou need to replace:\nif len(sys.argv) >= 2:\n    sys.exit('ERROR: Received 2 or more arguments. Expected 1: Input file name')\nWith:\nif len(sys.argv) > 2:\n    sys.exit('ERROR: Received more than two arguments. Expected 1: Input file name')\nBear in mind that the first argument given to the script is always its own name. This means you should be expecting either 1 or 2 arguments in sys.argv.\nIssues with locating the default file\nAnother problem is that your docker container's working directory is /home/aws, so when you execute your Python script it will try to resolve paths relative to this.\nThis means that:\nwith open('inputfile.txt') as f:\nWill be resolved as /home/aws/inputfile.txt, not /home/aws/myapplication/inputfile.txt.\nYou can fix this by either changing the code to:\nwith open('myapplication/inputfile.txt') as f:\nOr (preferred):\nwith open(os.path.join(os.path.dirname(__file__), 'inputfile.txt')) as f:\n(Source for the above variation)\nUsing CMD vs. ENTRYPOINT\nIt also seems like your script apparently isn't receiving myapplication/inputfile.txt as an argument. This might be a quirk with CMD.\nI'm not 100% clear on the distinction between these two operations, but I always use ENTRYPOINT in my Dockerfiles and it's given me no grief. See this answer and try replacing:\nCMD [\"python\", \"/myapplication/script.py\", \"/myapplication/inputfile.txt\"]\nWith:\nENTRYPOINT [\"python\", \"/myapplication/script.py\", \"/myapplication/inputfile.txt\"]\n(thanks Maya G)",
    "Docker compose doesn't recognize 'env_file'": "Like many other version related issues, updating to v1.7.1 of docker-compose resolved the issue, works like a charm!",
    "MySQL bind-address in a Docker container": "sed is usually the weapon of choice for such tasks. Taken from the official mysql dockerfile:\nRUN sed -Ei 's/^(bind-address|log)/#&/' /etc/mysql/my.cnf\nThe command comments out lines starting with bind-address or log in my.cnf or conf.d/*.",
    "In Dockerfile how to copy file from network drive": "Not easily, considering ADD or COPY uses the Dockerfile context (the current folder or below) to seek their resources.\nIt would be easier to cp that file first to the Dockerfile folder (before a docker build .), and leave in said Dockerfile a COPY myfile /opt/files/file directive.\nOr you could run the container, and use a docker cp //somenetwork/somefiles/myfile /opt/files/file to add that file at rintime",
    "Setting conditional variables in a Dockerfile": "Ok, was not complete. Here a full working solution:\nDockerfile:\nFROM ubuntu:18.04\n\nARG TARGETARCH\n\nARG DOWNLOAD_amd64=\"x86_64\"\nARG DOWNLOAD_arm64=\"aarch64\"\nWORKDIR /tmp\nARG DOWNLOAD_URL_BASE=\"https://download.url/path/to/toolkit-\"\nRUN touch .env; \\\n    if [ \"$TARGETARCH\" = \"arm64\" ]; then \\\n    export DOWNLOAD_URL=$(echo $DOWNLOAD_URL_BASE$DOWNLOAD_arm64) ; \\\n    elif [ \"$TARGETARCH\" = \"amd64\" ]; then \\\n    export DOWNLOAD_URL=$(echo $DOWNLOAD_URL_BASE$DOWNLOAD_amd64) ; \\\n    else \\\n    export DOWNLOAD_URL=\"\" ; \\\n    fi; \\\n    echo DOWNLOAD_URL=$DOWNLOAD_URL > .env; \\\n    curl ... #ENVS JUST VALID IN THIS RUN!\n \nCOPY ./entrypoint.sh ./entrypoint.sh\nENTRYPOINT [\"/bin/bash\", \"entrypoint.sh\"]\nentrypoint.sh\n#!/bin/sh\n\nENV_FILE=/tmp/.env\nif [ -f \"$ENV_FILE\" ]; then\n    echo \"export \" $(grep -v '^#' $ENV_FILE | xargs -d '\\n') >> /etc/bash.bashrc\n    rm $ENV_FILE\nfi\n\ntrap : TERM INT; sleep infinity & wait\nTest:\n# bash\nroot@da1dd15acb64:/tmp# echo $DOWNLOAD_URL\nhttps://download.url/path/to/toolkit-aarch64\nNow for Alpine:\nDockerfile\nFROM alpine:3.13\nRUN apk add --no-cache bash\nEntrypoint.sh\nENV_FILE=/tmp/.env\nif [ -f \"$ENV_FILE\" ]; then\n    echo \"export \" $(grep -v '^#' $ENV_FILE) >> /etc/profile.d/environ.sh\n    rm $ENV_FILE\nfi\nAlpine does not accept xargs -d. But not that interesting here due to the fact URL does not contain any blank..\nTesting: Alpine just uses that for login shells.. so:\ndocker exec -it containername sh --login\necho $DOWNLOAD_URL",
    "Docker context on remote server \u201cError response from daemon: invalid volume specification\u201d": "You are getting invalid volume specification: \u2018C:\\Users\\user\\fin:/fin:rw\u2019 in your production environment is because, the host path C:\\Users\\user\\fin isn't available. You can remove it when you are deploying or change it to an absolute path which is available in your production environment as below.\nvolumes:\n    - '/root:/fin:rw'\nwhere /root is a directory available in my production environment.\n /path:/path/in/container mounts the host directory, /path at the /path/in/container\n\n path:/path/in/container creates a volume named path with no relationship to the host.\nNote the slash at the beginning. if / is present it will be considered as a host directory, else it will be considered as a volume",
    "Docker-compose invalid. Additional properties are not allowed": "I think the problem is resulted from the fact that you don't indent the \"networks\" property.\nI used to get \"(root) Additional property agent is not allowed\", but indent solves the problem.",
    "Unable to install sklearn when building docker image": "Try to downgrade your Python version, because Scikit Learn don't support Python 3.9 yet.",
    "Docker downloads newer image for supposedly-cached digest": "Given that this doesn't happen for every run, and likely wouldn't happen if you tested locally, the issue doesn't appear to be with your Dockerfile or FROM line. Docker does not automatically clean the cache, so you'll want to investigate what external processes are deleting the cache. Since you are running your builds in Jenkins with a kubernetes plugin, the issue appears to be from that plugin cleaning up build agents after a timeout. From the documentation, you can see various settings to tune this builder:\npodRetention Controls the behavior of keeping slave pods. Can be 'never()', 'onFailure()', 'always()', or 'default()' - if empty will default to deleting the pod after activeDeadlineSeconds has passed.\nactiveDeadlineSeconds If podRetention is set to 'never()' or 'onFailure()', pod is deleted after this deadline is passed.\nidleMinutes Allows the Pod to remain active for reuse until the configured number of minutes has passed since the last step was executed on it.\nOne method to workaround ephemeral build agents is to use the --cache-from option in the docker build command. With the classic build (vs buildkit) you need to first pull this image locally. That image would be from a previous build, and you can use multiple images for your cache, which is particularly useful for multi-stage builds since you'll need to pull a cache for each stage. This flag tells docker to trust the image pulled from a registry since normally only locally built images are trusted (there's a risk someone could inject a malicious image that claims to have run steps in a popular image but includes malware in the tar of that layer).",
    "Why does docker create containers while building images?": "For each line daemon creates a new image and each instruction runs independently. Docker Daemon uses intermediate images to accelerate the docker build process. Build cache indicates this.",
    "OSX Docker is nearly always consuming about 4GB of RAM with no active containers": "Docker runs natively only on Linux. On OSX there is a LinuxKit VM for Docker Desktop for Mac underneath to emulate Linux. This of course adds some overhead. It's meant to be used for development and not for production.\nHere is some explanation about the memory usage.",
    "Is the lowercase \"dockerfile\" file name supported and how long has it been supported?": "I guess this was the change -\nhttps://github.com/spf13/docker/commit/7b1a2bbf701dfc961d9e2e00cc2e56544bb162b4\nThis change was pushed on 17 Feb 2015. Probably we didn't realise it because it looks for a Dockerfile and falls back to dockerfile.",
    "How do I answer install prompts (other than with \"yes\") automatically?": "If you want to script a terminal interaction, you could use expect on Linux (that might not be very easy; you need to predict the interactions).\nRemember that terminal emulators are complex and arcane things (because terminals like VT100 have been complex). See termios(3), pty(7) and read The Tty demystified.",
    "Docker for Windows building added prefix `/var/lib/docker/tmp/` for COPY?": "When you run the build command\ndocker build .\nThe . in that command indicates the \"build context\" that gets sent to the (possibly remote) docker engine. All of the COPY and ADD commands must be from inside this directory (or from a previous stage with multi stage builds). You can also exclude files from this context using the .dockerignore file. The tmp directory is a uniquely generated internal directory of docker's consisting of this build context.\nTo fix your issue, you need to make sure ProcessFiles/ProcessFiles.csproj exists in the directory where you run your build from, and that you didn't exclude it from the context with the .dockerignore file.\nEdit: based on your comments, change your copy command to:\nCOPY ProcessFiles.csproj ProcessFiles/",
    "Docker. Spring application. set & get environment variable": "In java code you are using java system property, but not the system environment variable. In order to pass system property to java process you need to specify -Dkey=value in running command.\nSo if this is tomcat you can set in $JAVA_OPTS=\"... -DJDBC_CONNECTION_STRING=$JDBC_CONNECTION_STRING\"",
    "Why do Docker official images not use USER as required by \"best practices\"": "Fundamentally, USER is not possible in official images. It conflicts with the requirement that \"A beginning user should be able to docker run official-image bash without needing to learn about --entrypoint\". If you don't have root, you can't edit config files, install packages like strace... or particularly, fixup UIDs in volumes. Realistically, the official image style is considered (a) best practice. (So the Docker userguide should put the emphasis on running daemons as non-root and less on USER specifically)\nIMO this is a problem. The popular examples you can learn from don't show the need to set fixed UIDs. Otherwise if you update with a base image that adds another user, you'll have to intervene manually. The Best Practices say you should consider setting fixed UIDs, but they don't even show an example of it. So prominent examples of simple Dockerfiles that use USER aren't setting fixed UIDs. The official images don't set fixed UIDs either - pretending like this isn't a problem - but then brute-force data volumes with chown, because their entrypoint scripts run as root. Not very impressive.\nTechnically, the official Dockerfiles could be fixed by adding even more chown and UID swapping to the Dockerfile, but that seems undesirable.\nI suppose the other alternative would be a path-dependent update. That is, keep the chown around until everyone's done their automatic updates to fixed UIDs (a couple of months?), and then drop it.",
    "\"correct\" way to manage database schemas in docker": "We use Postgres and Docker where I work and we ended up doing the following:\nCopy the Dockerfile from the official Postgres repo so you can make your own image.\nModify docker-entrypoint.sh (https://github.com/docker-library/postgres/blob/8f80834e934b7deaccabb7bf81876190d72800f8/9.4/docker-entrypoint.sh), which is what is called when the container starts.\nAt the top of docker-entrypoint.sh, I put in the following:\n# Get the schema\nurl=$(curl -s -u ${GIT_USER}:${GIT_PASSWORD} \"${SQL_SCRIPT_URL}\" | python -c 'import sys, json; print json.load(sys.stdin)[\"download_url\"]')\ncurl ${url} > db.sh\nchmod +x db.sh\ncp db.sh ./docker-entrypoint-initdb.d\nThis basically downloads a shell script from Github that initializes the schema for the database. We do this to manage versions of the schema, so when you start your container you can tell it which schema to use via an ENV variable.\nSome notes about the code:\nWe need to refactor to pull stuff from Github using a private key instead of user credentials.\nThe ./docker-entrypoint-initdb.d directory is a place where docker-entrypoint.sh will look to run init scripts for the database. You can move files to that location however you want. Do this if downloading from Github is not applicable.",
    "Set condition based on CPU-Arch in Dockerfile": "As Daniel mentioned, Docker provides predefined environmental variables that are accessible at container build time.\nWe don't need to do anything Docker specific with these however, we can simply use them in a bash script condition:\nFROM python:3-bullseye\n\nRUN apt-get update && apt-get install -y ffmpeg firefox-esr npm\nARG TARGETARCH\nRUN if [ $TARGETARCH = \"arm64\" ]; then \\\n        apt-get install -y libavformat-dev libavdevice-dev python3-av \\\n    ; fi\nRUN npm install -g webdriver-manager\nRUN webdriver-manager update --gecko\nNote the necessary specification of ARG TARGETARCH to make that environmental variable accessible by dispatched processes at build time.",
    "Is there a tool to convert an ansible-playbook to a Dockerfile? [closed]": "Not to convert into a Dockerfile bit to spit out a container image on the other side of the build pipeline, you could use ansible-bender.",
    "Get image tag inside Dockerfile": "Use a bash script with the tag wanted in the first positional argument of the script:\n#!/bin/bash\ndocker build --build-arg sw-ver=$1 -t my_image:$1 .\nThen tun the script:\nscript.sh 1.2.3",
    "Why Docker build need to use /dev/shm?": "The --shm-size option of docker build sets the /dev/shm size for intermediate containers that are started as part of the build process.",
    "How to CMD Powershell script in Dockerfile": "See this discussion:\nMicrosoft removed powershell and other pieces from base nanoserver image, You need to use image with built in poweshell.\nAnd, also, I found the offical doc:\nhttps://learn.microsoft.com/en-us/windows-server/get-started/nano-in-semi-annual-channel\nPowerShell Core, .NET Core, and WMI are no longer included by default, but you can include PowerShell Core and .NET Core container packages when building your container.\nAnd this is the Dockerfile which could guide you on how to install powershell in nanoserver:\n# escape=`\n# Args used by from statements must be defined here:\nARG fromTag=1709\nARG InstallerVersion=nanoserver\nARG InstallerRepo=mcr.microsoft.com/powershell\nARG NanoServerRepo=mcr.microsoft.com/windows/nanoserver\n\n# Use server core as an installer container to extract PowerShell,\n# As this is a multi-stage build, this stage will eventually be thrown away\nFROM ${InstallerRepo}:$InstallerVersion  AS installer-env\n\n# Arguments for installing PowerShell, must be defined in the container they are used\nARG PS_VERSION=6.2.0-rc.1\n\nARG PS_PACKAGE_URL=https://github.com/PowerShell/PowerShell/releases/download/v$PS_VERSION/PowerShell-$PS_VERSION-win-x64.zip\n\nSHELL [\"pwsh\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\n\nARG PS_PACKAGE_URL_BASE64\n\nRUN Write-host \"Verifying valid Version...\"; `\n    if (!($env:PS_VERSION -match '^\\d+\\.\\d+\\.\\d+(-\\w+(\\.\\d+)?)?$' )) { `\n        throw ('PS_Version ({0}) must match the regex \"^\\d+\\.\\d+\\.\\d+(-\\w+(\\.\\d+)?)?$\"' -f $env:PS_VERSION) `\n    } `\n    $ProgressPreference = 'SilentlyContinue'; `\n    if($env:PS_PACKAGE_URL_BASE64){ `\n        Write-host \"decoding: $env:PS_PACKAGE_URL_BASE64\" ;`\n        $url = [System.Text.Encoding]::Unicode.GetString([System.Convert]::FromBase64String($env:PS_PACKAGE_URL_BASE64)) `\n    } else { `\n        Write-host \"using url: $env:PS_PACKAGE_URL\" ;`\n        $url = $env:PS_PACKAGE_URL `\n    } `\n    Write-host \"downloading: $url\"; `\n    [Net.ServicePointManager]::SecurityProtocol = [Net.ServicePointManager]::SecurityProtocol -bor [Net.SecurityProtocolType]::Tls12; `\n    New-Item -ItemType Directory /installer > $null ; `\n    Invoke-WebRequest -Uri $url -outfile /installer/powershell.zip -verbose; `\n    Expand-Archive /installer/powershell.zip -DestinationPath \\PowerShell\n\n# Install PowerShell into NanoServer\nFROM ${NanoServerRepo}:${fromTag}\n\nARG VCS_REF=\"none\"\nARG PS_VERSION=6.2.0-rc.1\nARG IMAGE_NAME=mcr.microsoft.com/powershell\n\nLABEL maintainer=\"PowerShell Team <powershellteam@hotmail.com>\" `\n      readme.md=\"https://github.com/PowerShell/PowerShell/blob/master/docker/README.md\" `\n      description=\"This Dockerfile will install the latest release of PowerShell.\" `\n      org.label-schema.usage=\"https://github.com/PowerShell/PowerShell/tree/master/docker#run-the-docker-image-you-built\" `\n      org.label-schema.url=\"https://github.com/PowerShell/PowerShell/blob/master/docker/README.md\" `\n      org.label-schema.vcs-url=\"https://github.com/PowerShell/PowerShell-Docker\" `\n      org.label-schema.name=\"powershell\" `\n      org.label-schema.vcs-ref=${VCS_REF} `\n      org.label-schema.vendor=\"PowerShell\" `\n      org.label-schema.version=${PS_VERSION} `\n      org.label-schema.schema-version=\"1.0\" `\n      org.label-schema.docker.cmd=\"docker run ${IMAGE_NAME} pwsh -c '$psversiontable'\" `\n      org.label-schema.docker.cmd.devel=\"docker run ${IMAGE_NAME}\" `\n      org.label-schema.docker.cmd.test=\"docker run ${IMAGE_NAME} pwsh -c Invoke-Pester\" `\n      org.label-schema.docker.cmd.help=\"docker run ${IMAGE_NAME} pwsh -c Get-Help\"\n\n# Copy PowerShell Core from the installer container\nENV ProgramFiles=\"C:\\Program Files\" `\n    # set a fixed location for the Module analysis cache\n    LOCALAPPDATA=\"C:\\Users\\ContainerAdministrator\\AppData\\Local\" `\n    PSModuleAnalysisCachePath=\"$LOCALAPPDATA\\Microsoft\\Windows\\PowerShell\\docker\\ModuleAnalysisCache\" `\n    # Persist %PSCORE% ENV variable for user convenience\n    PSCORE=\"$ProgramFiles\\PowerShell\\pwsh.exe\" `\n    # Set the default windows path so we can use it\n    WindowsPATH=\"C:\\Windows\\system32;C:\\Windows\"\n\n    # Set the path\nENV PATH=\"$WindowsPATH;${ProgramFiles}\\PowerShell;\"\n\nCOPY --from=installer-env [\"\\\\PowerShell\\\\\", \"$ProgramFiles\\\\PowerShell\"]\n\n# intialize powershell module cache\nRUN pwsh `\n        -NoLogo `\n        -NoProfile `\n        -Command \" `\n          $stopTime = (get-date).AddMinutes(15); `\n          $ErrorActionPreference = 'Stop' ; `\n          $ProgressPreference = 'SilentlyContinue' ; `\n          while(!(Test-Path -Path $env:PSModuleAnalysisCachePath)) {  `\n            Write-Host \"'Waiting for $env:PSModuleAnalysisCachePath'\" ; `\n            if((get-date) -gt $stopTime) { throw 'timout expired'} `\n            Start-Sleep -Seconds 6 ; `\n          }\"\n\nCMD [\"pwsh.exe\"]",
    "How to activate a conda environment before running Docker commands": "Do this with an ENTRYPOINT in your Dockerfile.\nsrc/entrypoint.sh\n#!/bin/bash\n\n# enable conda for this shell\n. /opt/conda/etc/profile.d/conda.sh\n\n# activate the environment\nconda activate my_environment\n\n# exec the cmd/command in this process, making it pid 1\nexec \"$@\"\nsrc/Dockerfile\n# ...\nCOPY ./entrypoint.sh ./entrypoint.sh\nRUN chmod +x ./entrypoint.sh\nENTRYPOINT [\"./entrypoint.sh\"]",
    "Why do we build and publish as two steps when publish also builds?": "According to the book .NET Microservices: Architecture for Containerized .NET Applications (Microsoft EBook), the first build instruction is redundant because the publish instruction also builds, and it is right after the first build instruction. Page 94 (86), line 10.\nHere is a short excerpt from the book:\n1 FROM microsoft/dotnet:2.1-aspnetcore-runtime AS base\n2 WORKDIR /app\n3 EXPOSE 80\n4\n5 FROM microsoft/dotnet:2.1-sdk AS build\n6 WORKDIR /src\n7 COPY src/Services/Catalog/Catalog.API/Catalog.API.csproj \u2026\n8 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.AspNetCore.HealthChecks \u2026\n9 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions.HealthChecks \u2026\n10 COPY src/BuildingBlocks/EventBus/IntegrationEventLogEF/ \u2026\n11 COPY src/BuildingBlocks/EventBus/EventBus/EventBus.csproj \u2026\n12 COPY src/BuildingBlocks/EventBus/EventBusRabbitMQ/EventBusRabbitMQ.csproj \u2026\n13 COPY src/BuildingBlocks/EventBus/EventBusServiceBus/EventBusServiceBus.csproj \u2026\n14 COPY src/BuildingBlocks/WebHostCustomization/WebHost.Customization \u2026\n15 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions \u2026\n16 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions \u2026\n17 RUN dotnet restore src/Services/Catalog/Catalog.API/Catalog.API.csproj\n18 COPY . .\n19 WORKDIR /src/src/Services/Catalog/Catalog.API\n20 RUN dotnet build Catalog.API.csproj -c Release -0 /app\n21\n22 FROM build AS publish\n23 RUN dotnet publish Catalog.API.csproj -c Release -0 /app\n24\n25 FROM base AS final\n26 WORKDIR /app\n27 COPY --from=publish /app\n28 ENTRYPOINT [\"dotnet\", \"Catalog.API.dll\"]\nFor the final optimization, it just happens that line 20 is redundant, as line 23 also builds the application and comes, in essence, right after line 20, so there goes another time-consuming command.",
    "How to build my own custom Ubuntu ISO with docker": "So incase anyone finds this post. The way to resolve the dns issue is to make sure your resolv.conf file in the chroot is actually pointing to a proper dns servers. Some apps like cubic already do this for you.",
    "Cannot access nodejs app on browser at localhost:4200 (docker run -p 4200:4200 ....)": "EDIT\nhttps://stackoverflow.com/a/48286174/2663059\nHas the solution.\nEDIT\nFirst of all . Why you have done this .\n#EXPOSE 4200\nin Dockerfile. # in front means comment in dockerfile . Use this\n EXPOSE 4200\n#EXPOSE 4200 means you have not expose the port .\nAnd next check that container running your node server inside the Docker or not. How can you check is this. docker exec -it nept0 bash or can try\ndocker exec -it nept0 /bin/bash\nThen you can run\ncurl localhost:4200 or the get api of the node.js if that working fine your node is working fine #EXPOSE 4200 was culprit .\nYou can read for docker exec here more https://docs.docker.com/engine/reference/commandline/exec/\nLet me know if any issue.",
    "Stop a Docker container after executing code": "Note: the default CMD for python:3 is python3.\nexit code 143 means SIGTERM as mentioned here. That is what docker sends.\nSo you need for your python3 application to process SIGTERM signal gracefully\nDon't forget that your python app, once completed and exited the main function, would cause the container to automatically stop and exit.\nThe OP adds in the comments:\nIn the meantime, I have found out that handling the SIGTERM in the Docker environment works perfectly fine.\nHowever using the same code in Docker on CloudFoundry does not prevent the container from crashing.\nIn CloudFoundry you need an application that is always running and not just doing a task and then stopping like a script does.\nEven stopping without errors is detected as a crash in CloudFoundry.\nI transformed my script into a REST server by using the flask framework. Now it is always running but only doing its task when being called via its url.",
    "Passing a list of arguments to docker at build / run time": "Build time\nThe best I can think of is to pass in a comma-separated list:\n docker build --build-arg repos=repo1,repo2\nDocker won't parse this into a list for you, but the scripts run during the build could split the string into a list.\nRun time\nIf you define your command using ENTRYPOINT then any trailing parameters to docker run get appended to the entry point command.\nSo if your Dockerfile contains:\n   ENTRYPOINT echo hello\nThen:\n   docker run myimage glorious world \n... will run the command\n   echo hello glorious world",
    "Daemonized buildbot start": "Buildbot bootstrap is based on Twisted's \".tac\" files, which are expected to be started using twistd -y buildbot.tac. The buildbot start script is actually just a convenience wrapper around twistd. It actually just run twistd, and then watches for the logs to confirm buildbot successfully started. There is no value added beyond this log watching, so it is not strictly mandatory to start buildbot with buildbot start. You can just start it with twistd -y buildbot.tac.\nAs you pointed up the official docker image is starting buildbot with twistd -ny buildbot.tac If you look at the help of twistd, -y means the Twisted daemon will run a .tac file, and the -n means it won't daemonize. This is because docker is doing process watching by itself, and do not want its entrypoint to daemonize.\nThe buildbot start command also has a --nodaemon option, which really only is 'exec'ing to twistd -ny. So for your dockerfile, you can as well us twistd -ny or buildbot start --nodaemon, this will work the same.\nAnother Docker specific is that the buildbot.tac is different. It configured the twistd logs to output to stdout instead of outputing to twisted.log. This is because docker design expects logs to be in stdout so that you can configure any fancy cloud log forwarder independently from the application's tech.",
    "Cannot Compile the typescript in Docker": "In the best practice when you create a Dockerfile, you should only user one CMD and one ENTRYPOINT.\nIn your case, It should be:\nCOPY code/ /usr/local/lib\nWORKDIR /usr/local/lib\n\nRUN npm install -g koa@2\nRUN npm install -g sequelize\nRUN npm install -g mysql\nRUN npm install -g typescript\n\nRUN sh -c rm webApp.js\nRUN sh -c tsc webApp.ts\nCMD [\"node\", \"webApp.js\"]",
    "What is the difference b/w a docker image generated by docker-compose build vs docker build?": "There are no differences between the actual image that gets built between docker-compose build and a \"manual\" docker build in terms of the contents of the image.\nThe difference is only in naming/tagging of the build result, which docker-compose does automatically for you. Other than that, the docker-compose build is no different behind the scenes and simply a wrapper for the normal docker build.",
    "Docker with ngnix and angular stuck at start worker process 30": "The container is up and running, it didn't stuck there. The container is run in attached mode by default. You can either run the container in detached mode using command -\ndocker run -d -p 8080:80 -v $(pwd)/dist:/usr/share/nginx/html nginx-angular\nOr leave it open and try checking container status in new terminal using command -\nsudo docker ps",
    "Dockerfile fails when installing a python local package with permission denied": "I hit this issue as well. I wound up building a wheel and then adding and installing the wheel in the image.\nlocal\npython setup.py bdist_wheel\nDockerfile\nFROM jupyter/scipy-notebook\nRUN conda install ipdb lxml -y\nADD dist/my_package-0.0.1-py3-none-any.whl my_package-0.0.1-py3-none-any.whl\nRUN pip install my_package-0.0.1-py3-none-any.whl",
    "How to NOT inherit labels from base images in Docker?": "I don't think there is a way to remove labels. I'm using my own 'namespace' when I add labels in my image so that I can find the ones I want easily later. Example label can be com.mycompanyname.foo set to value bar.",
    "Chrome/Chromium inside Docker and the sandbox": "Enable user namespaces in your kernel. >>another relevant thread<<",
    "Docker build fails on `add-apt-repository: not found`": "You can build an image using the current Dockerfile you have. I am assuming you are having an issue when you try to build 2 separate images.\nThat is because add-apt-repository will not be recognizable until there is software-properties-common or python-software-properties installed.\nIf both the runs are in a Dockerfile and you build an image using\ndocker build -t mydockerimage .\nYou will not see any issue because the second layer/run will build on top of the first, so the add-apt-repository is recognizable and you will not have any issue.\nI hope this answers your query.",
    "installing mongodb in a docker container": "I'm hoping you know that you don't have to go to the effort because there is already an official image on Docker Hub.\nHowever, your error message gives a hint to what went wrong:\nmongodb-org/3.4/multiverse/binary-amd64/Packages.gz     Hash Sum mismatch\nYou can correct this if you are determined to make your own image.\nOne place I would start is looking at the Docker best practices for run. You have split some related to the same dependency over multiple lines in your code, which can cause caching to mismatch versions and hashes. This is likely the problem.\nIf you aren't satisfied with using the official image directly you can at least look at how it was built here: https://github.com/docker-library/mongo/blob/c02ca4cce8c69e5069b75cb574d1b99d7b4edaeb/3.4/Dockerfile",
    "Simple docker containers: Build dedicated image or mount config as volume?": "If you 'bake in' the nginx config (your second approach)\nADD nginx.conf /etc/nginx/\nit makes your docker containers more portable - i.e. they can be downloaded and run on any server capable of running docker and it will just work.\nIf you use option 1, mounting the config file at run time, then you are transferring one of your dependencies to outside of your container. This makes it a dependency that must be managed outside of docker.\nIn my opinion it is best to put as many dependencies inside your dockerfiles as possible because it makes them more portable and more automated (great for CI Pipelines for example)\nThere are reasons for mounting files at run time and these are usually centred around environmentally specific settings (although these can largely be overcome within docker too) or 'sensitive' files that application developers shouldn't or couldn't have access to. For example ssl certificates, database passwords, etc",
    "wkhtmltopdf not working with .net 8 and docker file": "Official NET8 docker images are based on Debian 12 (\"bookworm\") which doesn't support old libssl1, this means that wkhtmltopdf binaries for older Debian (10,11) will not work.\nFortunatelly wkhtmltopdf build for \"bookworm\" is available, it can be downloaded here: https://github.com/wkhtmltopdf/packaging/releases\nTo include wkhtmltopdf add these lines to 'Dockerfile':\nRUN apt-get update && apt-get install -y --no-install-recommends wget ca-certificates fontconfig libc6 libfreetype6 libjpeg62-turbo libpng16-16 libssl3 libstdc++6 libx11-6 libxcb1 libxext6 libxrender1 xfonts-75dpi xfonts-base zlib1g\nRUN wget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6.1-3/wkhtmltox_0.12.6.1-3.bookworm_amd64.deb\nRUN dpkg -i wkhtmltox_0.12.6.1-3.bookworm_amd64.deb",
    "'403 Forbidden' apt-get update Ubuntu Dockerfile": "Can you add:\napt-get --allow-releaseinfo-change update\nbefore apt-update command",
    "\"Permission denied\" on file when running a docker container": "You may try this simple one.\nFROM alpine\nCOPY . /home/guestuser/bin/gateway\nRUN apk add libressl-dev\nRUN apk add libffi-dev\nWORKDIR /home/guestuser/bin/\nRUN chmod -R 755 /home/guestuser\nCMD [\"/bin/bash\", \"/home/guestuser/bin/gateway\"]\nOtherwise, run sleep command login to container and see your commands works manually",
    "How to connect to remote host from a docker container": "It could be different reasons why it doesn't work.\nNetworking misconfiguration: docker container runs in an isolated network environment, it knows nothing about postgres server on your localhost. If the connection to abc.com doesn't work either, there could be a problem with dns resolution, you should try to use an ip of the host to troubleshoot it.\nPostgres server misconfiguration: take a look to pg_hba.conf and postgresql.conf, there are settings for access restrictions. Check the connectivity from container with the commands pg_isready and psql.",
    "DOCKER How do I set an password and username on docker run with an windows image?": "The default shell for Windows images is cmd.exe. Therefore the ARG and ENV should be dereferenced the same way as in any windows cmd: %myarg%.\nSo in your case dereferencing should be done like: RUN net USER /ADD %user% %password% && RUN net localgroup Administrators %user% /ADD\nAlso, ENV statement should be placed after FROM statement, in order to have the environment variables available inside the container.\nOne can also change the shell to powershell using: SHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue'; $verbosePreference='Continue';\"]\nIn this case dereferencing would have the syntax: $env:myenv",
    "ERROR: unsatisfiable constraints: curl (missing): while building for jmeter dockerfile": "The error indicates that Alpine apk package management tool wasn't able to install curl, fontconfig and other packages due to not being able to connect to http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/ host and get the files from there.\nEnsure that your host machine has Internet and if it does follow recommendations from the My docker container has no internet answers.\nAlso be aware that currently JMeter 5.2 is out so I would recommend at least changing this line:\nENV MIRROR https://www-eu.apache.org/dist/jmeter/binaries\nto this one:\nENV MIRROR https://archive.apache.org/dist/jmeter/binaries\notherwise your Dockerfile will not work even if you resolve Internet connectivity issues.\nOptionally you can ramp-up JMETER_VERSION to match the latest stable JMeter release",
    "How to run Symfony console command inside the docker container": "Copy from https://docs.docker.com/compose/reference/exec/\nTo disable this behavior, you can either the -T flag to disable pseudo-tty allocation.\ndocker-compose exec -T nginx <command>\nOr, set COMPOSE_INTERACTIVE_NO_CLI value as 1\nexport COMPOSE_INTERACTIVE_NO_CLI=1\nFor php bin/console to run you need to run from app container like below.\ndocker-compose exec -T app php bin/console",
    "Conda not found when trying to build Docker image": "# Install Peddy\nRUN INSTALL_PATH=~/anaconda \\\n    && wget http://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh \\\n    && bash Miniconda2-latest* -fbp $INSTALL_PATH \\\n    && PATH=$INSTALL_PATH/bin:$PATH\nThe last part of the above updates a PATH variable that will only exist in the shell running the command. That shell exits immediately after setting the PATH variable, and the temporary container used to execute the RUN command exits. The result of the RUN command is to gather the filesystem changes into a layer of the docker image being created. Any environment variable changes, background processes launched, or anything else not part of the container filesystem is lost.\nInstead, you'll want to update the image environment with:\n# Install Peddy\nRUN INSTALL_PATH=~/anaconda \\\n    && wget http://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh \\\n    && bash Miniconda2-latest* -fbp $INSTALL_PATH \\\nENV PATH=/root/anaconda/bin:$PATH\nIf the software permits it, I would avoid installing in the /root home directory and instead install it somewhere like /usr/local/bin, making it available if you change the container to run as a different user.",
    "docker build - Avoid ADDing files only needed at build time": "Relying on docker commit indeed amounts to a hack :) and its use is thus mentioned as inadvisable by some references such as this blog article.\nI only see one possible approach for the kind of use case you mention (copy a one-time .deb package, install it and remove the binary immediately from the image layer):\nYou could make remotely available to the docker engine that builds your image, the .deb you'd want to install, and replace the COPY + RUN directives with a single one, e.g., relying on curl:\nRUN curl -OL https://example.com/foo.deb && dpkg -i foo.deb && rm -f foo.deb\nIf curl is not yet installed, you could run beforehand the usual APT commands:\nRUN apt-get update -y -q \\\n  && DEBIAN_FRONTEND=noninteractive apt-get install -y -q --no-install-recommends \\\n    ca-certificates \\\n    curl \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nMaybe there is another possible solution (but I don't think the multi-staged builds Docker feature would be of some help here, as all perms would be lost by doing e.g. COPY --from=build / /).",
    "Not able to run source command from Dockerfile": "This is very similar to this question: How to solve 'ttyname failed: Inappropriate ioctl for device' in Vagrant?\nIn ~/.profile there is a line:\nmesg n || true\nThat is incompatible. In order to fix it, make it conditional upon if tty is available (credit Gogowitsch):\nRUN sed -i ~/.profile -e 's/mesg n || true/tty -s \\&\\& mesg n/g'\nThis sed command replaces mesg n || true (try it, and ignore if it fails) with tty -s && mesg n, (only try it if it will succeed) which makes the error message go away",
    "Add exchanges in rabbitmq with Dockerfile or docker-compose": "There seems to be two ways to address this issue:\nby referencing rabbitmq.config file via volume\nvia temporary configuration container and entrypoint script\nOption #2 appeals to me more, as the configuration is performed via REST calls.",
    "Dockerfile Public Key Permission Denied using Git (Bitbucket)": "Could not open a connection to your authentication agent.\nThat seems expected: the agent started in your Dockerfile in one layer would not be running in the next layer created by the next line of the Dockerfile: each container run from each line is then stopped and committed as an image.\nEven if you put both commands on the same line, the agent would still be running after said unique line.\nThat agent starting + ssh-add command should be part of your CMD script, which will run as well a foreground process.\nMeaning the Dockerfile should end with CMD script, with 'script' being the path of a (COPY'ed) script which includes what you want to run in your container, and that would start with the ssh agent and the ssh-add command.\nThe OP Chris points out in the comments:\nlayers are executed serially, with the current layer not having any context to prior ones.\nBased on that \"oh snap\" moment, I went on to consolidate all RUN commands into a single RUN command using \"&& \\\".\nEverything is working as expected.",
    "Reusable docker image for AngularJS": "EDIT : Better option is to use build args\nInstead of passing URL at docker run command, you can use docker build args. It is better to have build related commands to be executed during docker build than docker run.\nIn your Dockerfile,\nARG URL \nAnd then run\ndocker build --build-arg URL=<my-url> .\nSee this stackoverflow question for details",
    "External properties file using Spring Boot and Docker": "I think you only need to mount the volume to the conf folder e.g.\ndocker run -d -p 8080:8080 -v /opt/gpm/config:/conf --name gpm gpm-web:1.0",
    "Docker run results in \"host not found in upstream\" error": "I have solved this. There are two things at play.\nOne is how it works locally and the other is how it works in Docker Cloud.\nLocal workflow\ncd into root of project, where Dockerfile is located\nbuild image: docker build -t media-saturn:dev .\nrun the builded image: docker run -it --add-host=\"my-server-address.com:123.45.123.45\" -p 80:80   media-saturn:dev\nDocker cloud workflow\nAdd extra_host directive to your Stackfile, like this\nand then click Redeploy in Docker cloud, so that changes take effect\nextra_hosts:\n'my-server-address.com:123.45.123.45'\nOptimization tip\nignore as many folders as possible to speed up process of sending data to docker deamon\nadd .dockerignore file\ntypically you want to add folders like node_modelues, bower_modules and tmp\nin my case the tmp contained about 1.3GB of small files, so ignoring it sped up the process significantly",
    "Efficient Dockerfile for many apt-get packages": "I think you need to run apt-get update only once within the Dockerfile, typically before any other apt-get commands.\nYou could just first have the large list of known programs to install, and if you come up with a new one then just add a new RUN apt-get install -y abc to you Dockerfile and let docker continue form the previously cached command. Periodically (once a week, one a month?) you could re-organize them as you see fit or just run everything in a single command.\nI suppose I could combine them when dependencies are more stable, but I find I'm always overly optimistic about when that is.\nOh you actually mentioned this solution already, anyway there is no harm doing these \"tweaks\" every now and then. Just run apt-get update only once.",
    "/home/web/.gem/ruby/2.2.0/gems/redis-3.2.1/lib/redis/connection/ruby.rb:152:in `getaddrinfo': getaddrinfo: Name or service not known (SocketError)": "The REDIS_URL environment variable can be used to pass the Redis url to both Sidekiq and the Redis gem.\nThis environment variable can be set in the docker-compose.yml like this:\ndb:  \n  image: postgres\n  ports:\n    - \"5432\"\n\nredis:  \n  image: redis\n  ports:\n    - \"6379\"\n\nweb:  \n  build: .\n  command: bundle exec rails s -b 0.0.0.0\n  volumes:\n    - .:/app\n  ports:\n    - \"3000:3000\"\n  links:\n    - db\n    - redis\n  environment:\n    REDIS_URL: \"redis://redis:6379\"\nAs Sidekiq and Redis gems will use REDIS_URL by default, you also need to make sure that you are not overriding this default behavior in your config files. You don't need the file config/initializers/redis.rb anymore, and your config/app-config.yml should contain only your namespace:\ndefault: &default\n  redis_namespace: 'RAILS_CACHE'\n\ndevelopment:\n  <<: *default",
    "How to copy local wp-content files to Wordpress container using Dockerfile": "Today I ran into the same problem. I wanted to add theme when building the image by using COPY. However, it did not work. This was because I already set my wp-content folder as a volume. It seems you cannot COPY into a volume.\nThe discussion in the following link helped me realise this:\nhttps://github.com/docker-library/wordpress/issues/146\nBelow I have added my WordPres Dockerfile and docker-compose file. After commenting out the volume everything worked as expected.\nI sure hope you do not need it anymore after two years, haha.\nReference: https://docs.docker.com/samples/wordpress/\nDockerfile\nFROM wordpress:latest\n\nWORKDIR /var/www/html\n\nCOPY ./wp-content/ ./wp-content/\nRUN chmod -R 755 ./wp-content/\ndocker-compose.yml\nversion: \"3\"\nservices:\n  db:\n    build:\n      context: .\n      dockerfile: ./compose/local/db/Dockerfile\n    image: new_db_image_name\n    container_name: new_db_image_name\n    command: '--default-authentication-plugin=mysql_native_password'\n    volumes:\n      - db_data:/var/lib/mysql\n    restart: always\n    environment:\n      - MYSQL_ROOT_PASSWORD=somewordpress\n      - MYSQL_DATABASE=wordpress\n      - MYSQL_USER=wordpress\n      - MYSQL_PASSWORD=wordpress\n    expose:\n      - 3306\n      - 33060\n  wordpress:\n    build:\n      context: .\n      dockerfile: ./compose/local/wordpress/Dockerfile\n    image: new_wordpress_image_name\n    container_name: new_wordpress_image_name\n#    volumes:\n#      - wp-content:/var/www/html/wp-content\n    ports:\n      - 80:80\n    restart: always\n    environment:\n      - WORDPRESS_DB_HOST=db\n      - WORDPRESS_DB_USER=wordpress\n      - WORDPRESS_DB_PASSWORD=wordpress\n      - WORDPRESS_DB_NAME=wordpress\n\nvolumes:\n  db_data:\n#  wp-content:",
    "Issue creating OSM tile server using Docker": "I would recommend using a different volume name from the example provided as this worked for me.",
    "Conditionally expose port in Dockerfile": "I also looking for the solution.\nI known EXPOSE just some kind of docs for image. but I think it's really useful.\ndirectly NO\nseems the answer is NO for now.\nSO I have to create 2 (or more) dockerfile for that purpose.\nworkaround\nmaybe build from STDIN (generate dockerfile by other program) can be a workaround.\nBut it's really not that straightforward and effective\n// It is no different from dynamically creating a dockerfile.\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#pipe-dockerfile-through-stdin\nhttps://docs.docker.com/build/building/context/#text-files\nhttps://docs.docker.com/engine/reference/builder/#environment-replacement:~:text=Dockerfile through STDIN",
    "Dynamically get JAR filename in DockerFile": "Instead of copying the jar file inside the dockerfile, you can mount the folder in which jar is created. And you will not be needed to be worried with the filename.\nWhat you want can be done using environment variables. You can create an --env-file and keep your version number there. And inside dockerfile, refer to value of variable declared in env file with {VERSIION}. By following this process, you may want to use this same version in your pom file. For that, take a look at this question.\nThe answer links environment variables by setting env.path and detailed guide is here.",
    "docker-compose build download few pom dependencies each time": "My previous answer didn't make sense (I mistakenly thought that Maven dependencies were handled at run time and they are handled at ONBUILD instructions instead). But I'll try to give a second explanation of why these specific dependencies are not cached.\nThe downloaded dependencies listed in the question are from the builtin plugins for Maven clean. In the parent Dockerfile we download the project dependencies at https://github.com/heroku/docker-java/blob/master/Dockerfile#L14 (everything in your POM). Then, later on in the Dockerfile it runs the clean at https://github.com/heroku/docker-java/blob/master/Dockerfile#L18. Since the maven-clean-plugin isn't part of your POM, it gets downloaded automatically at the clean step (it isn't cached from the previous Dockerfile instructions).\nSo, if you wanted to cache the maven-clean-plugin as well, you may need to add it as a dependency in your POM (you might be able to get away with just <scope>import</scope>).",
    "Docker running out of memory when loading large sql dump": "Surprisingly, it can be a bad dump, try to recreate it and retry. This helps to me.",
    "Build a Docker Image from private git repository": "Replacing git:// with ssh:// will solve the problem.",
    "Making a Docker build stage depend on another stage without using anything from it": "Can I have stage X fail if stage Y fails, even though X doesn't use anything from Y?\nUsing the modern BuildKit engine, if stage X is the final image and it doesn't use anything from Y, Docker won't even run the earlier stage.\nI'd suggest running your unit-test sequence outside of Docker, before you build an image. You should be able to configure your CI system to do the equivalent of:\nyarn install\nyarn test\ndocker build .\nThis also would let you omit the test code and dependencies from the final image, resulting in a smaller image and marginally faster build.\nIf it's really important to you to use no tool other than Docker, a trick from classic Makefiles is to have your test stage create an empty file on success, and then have the later stage copy that file. Now there is \"a dependency\" even though it's not actually a meaningful file.\nFROM workspace AS test\nRUN yarn test \\\n && touch /.tests-successful\n\nFROM nginx:1.23.1-alpine\nCOPY --from=test /.tests-successful /\nCOPY --from=compile /app/build /usr/share/nginx/html",
    "Spring boot apps port mapping in docker container": "Rather than providing an exact succinct solution, let me also explain why doesn't it work for you, I think it will be more valuable for our colleagues who will ever read this post.\nSo Starting with a spring boot part.\nSpring boot knowns nothing about the docker.\nWhen you put server.port: 8080 it only means that spring boot application starts the web server that is ready to get accept connections on that port.\nIf you want to change that mapping externally without modifying the application.yaml you can do that by passing the additional parameter:\njava -jar myapp.jar --server.port=8081\nThis will effectively override the definition in yaml. There are other ways as well (environment variable for example). Spring boot supports many different ways of configurations and there are ways that take precedence over the configuration specified in yaml. Among them SERVER_PORT environment variable for exmaple.\nYou can read the official spring boot documentation chapter for more information\nAll the above happens regardless the presence of docker in the setup.\nNow, as for the docker part.\nWhen you run the container with -p A:B this means that it will forward the port A in the host machine to port B inside the container.\nSo this is why it doesn't work: you run\ndocker run -d -p 8081:8081 -it user-service\nBut no-one is ready to accept the connections on port 8081 inside the container. So its not precise to say that your docker service doesn't work - it starts but you can't call it.\nSo the simplest way wound be running with something like -p 8081:8080 so that from the host machine's standpoint the ports won't clash, but the container will be accessible.\nIf you want to also change the port in the docker container (for whatever reason) you can read the above about the spring boot part (with --server.port and everything)",
    "How to install vim in a docker image based on mysql version 8?": "If you take a look at the Tag for 8.0 you can see that the base uses a different version of Oracle linux (8 vs 7). Yum is not installed in 8. Instead, there's a minimal installer (microdnf). So this substitution should work for you:\nmicrodnf install -y vim",
    "Cannot start apache automatically with docker": "Docker services have to be running in the foreground. In your Dockerfile, RUN service apache2 restart will start apache as background process. Hence the container will exit.\nTo run apache in the foreground, add the following to the Dockerfile.\nCMD [\"/usr/sbin/apachectl\", \"-D\", \"FOREGROUND\"]\nFROM ubuntu:latest\nRUN apt-get update && apt-get install -y apache2 php libapache2-mod-php php-mcrypt php-mysql php-cli php-curl php-xml php-intl php-mbstring git vim composer curl\n\nCOPY . /var/www/example\nCOPY vhost.conf /etc/apache2/sites-available/example.conf\n\nRUN a2ensite example\nRUN chown -R www-data:www-data /var/www/example/logs\nCMD [\"/usr/sbin/apachectl\", \"-D\", \"FOREGROUND\"]",
    "running netcat inside docker container": "That's never going to work. There are several problems with your Dockerfile.\n1\nSetting ENTRYPOINT to /bin/bash means that docker run ... is simply going to start bash. Read this question about ENTRYPOINT and CMD.\n2\nSince you're in non-interactive mode, bash is going to exit immediately. Consider:\nhost$ docker run nc-ubuntu\nhost$\nVs:\nhost$ docker run -it nc-ubuntu\nroot@e3e1a1f4e453:/# \nThe latter, because of the -it (which allocates a tty device, which bash requires in interactive mode), gets a bash prompt.\n3\nNeither invocation will cause the container to run netcat...and even if it did, nothing in your Dockerfile would generate the hello daemon response you're expecting.\n4\nThe nc command line is incorrect. The syntax is:\nnc -l -p <port>\nSo you would need:\nCMD [\"nc\", \"-l\", \"-p\", \"1234\"]\n5\nIf you actually want nc to provide you with the hello daemon response, you would need to add an appropriate -c command to your nc command line, as in:\nCMD [\"nc\", \"-l\", \"-p\", \"1234\", \"-c\", \"echo hello daemon\"]\nThis makes the final Dockerfile look like:\nFROM ubuntu\nRUN apt-get update \\\n  && DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n    net-tools \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nRUN apt-get update \\\n  && DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n    netcat \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nEXPOSE 1234\nCMD [\"nc\", \"-l\", \"-p\", \"1234\", \"-c\", \"echo hello daemon\"]\nAnd if I build that:\ndocker build -t nc-ubuntu .\nAnd run that:\ndocker run -d  -i -p 1234:1234 --name daemon  nc-ubuntu\nI can then telnet to port 1234 on my host and see the expected response:\nhost$ telnet localhost 1234\nTrying ::1...\nConnected to localhost.\nEscape character is '^]'.\nhello daemon\nConnection closed by foreign host.\nAt this point, the container will have exited because nc exits after accepting a single connection (without additional parameters), and a Docker contain exits when the foreground process exits.\nI don't have access to the book so I can't tell if this is do to a problem with the book or if you have made a mistake in your implementation, but I would suggest that there are a number of online Docker tutorials that are probably at least as good.",
    "Can not install package within docker debian:jessie": "I don't what's causing the 'unable to locate package' error, but your apt-get invocations are missing -y, which means you're going to get:\nAfter this operation, 33.8 MB of additional disk space will be used.\nDo you want to continue? [Y/n] Abort.\nThe command '/bin/sh -c apt-get install git-core' returned a non-zero code: 1\nOtherwise, your Dockerfile worked just fine for me:\nStep 1 : RUN apt-get -qq update\n ---> Running in 0430a990fa81\n ---> 54f88a02d81e\nRemoving intermediate container 0430a990fa81\nStep 2 : RUN apt-get install git-core\n ---> Running in 0fdad2e3c35b\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following extra packages will be installed:\n  ca-certificates git git-man less libcurl3-gnutls liberror-perl libidn11\n  librtmp1 libssh2-1 libx11-6 libx11-data libxau6 libxcb1 libxdmcp6 libxext6\n  libxmuu1 openssh-client patch rsync xauth\nSuggested packages:\n  gettext-base git-daemon-run git-daemon-sysvinit git-doc git-el git-email\n  git-gui gitk gitweb git-arch git-cvs git-mediawiki git-svn ssh-askpass\n  libpam-ssh keychain monkeysphere ed diffutils-doc openssh-server\nRecommended packages:\n  ssh-client\nThe following NEW packages will be installed:\n  ca-certificates git git-core git-man less libcurl3-gnutls liberror-perl\n  libidn11 librtmp1 libssh2-1 libx11-6 libx11-data libxau6 libxcb1 libxdmcp6\n  libxext6 libxmuu1 openssh-client patch rsync xauth\n0 upgraded, 21 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 8,059 kB of archives.\nAfter this operation, 33.8 MB of additional disk space will be used.\nDo you want to continue? [Y/n] Abort.\nThe command '/bin/sh -c apt-get install git-core' returned a non-zero code: 1\nERROR: failed to build larsks/sodocker:latest",
    "Facing the following error while trying to create a Docker Container using a DockerFile -> \"error from sender: open .Trash: operation not permitted\"": "The Dockerfile should be inside a folder. Navigate to that folder and then run docker build command. I was also facing the same issue but got resovled when moved the docker file inside a folder",
    "apt-get not found in Docker": "as tkausl said: you can only use one base image (one FROM).\nalpine's package manager is apk not apt-get. you have to use apk to install packages. however, pip is already available.\nthat Dockerfile should work:\nFROM python:3.6-alpine\n\nRUN apk update && \\\n    apk add --virtual build-deps gcc python-dev musl-dev\n\nWORKDIR /app\nADD . /app\nRUN pip install -r requirements.txt\nEXPOSE 5000\nCMD [\"python\", \"main.py\"]",
    "Invalid framework identifier Dotnet restore, docker build": "The problem was that i needed to copy my Directory.Build.props file and now is working. Just added this line to my dockerfile\nCOPY Directory.Build.props ./",
    "Non-root user in Docker": "Add RUN useradd -s /bin/bash user before the USER directive.",
    "Install nodejs in python slim buster": "This should work:\nFROM python:3.7-slim-buster\n\n# setup dependencies\nRUN apt-get update\nRUN apt-get install xz-utils\nRUN apt-get -y install curl\n\n# Download latest nodejs binary\nRUN curl https://nodejs.org/dist/v14.15.4/node-v14.15.4-linux-x64.tar.xz -O\n\n# Extract & install\nRUN tar -xf node-v14.15.4-linux-x64.tar.xz\nRUN ln -s /node-v14.15.4-linux-x64/bin/node /usr/local/bin/node\nRUN ln -s /node-v14.15.4-linux-x64/bin/npm /usr/local/bin/npm\nRUN ln -s /node-v14.15.4-linux-x64/bin/npx /usr/local/bin/npx\nTo run node start it with docker run -it <containerName> /bin/bash Then node, npm and npx are available",
    "Docker-compose volume doesn't save state of container for Keycloak": "Using default database location you may try this option with docker-compose:\nkeycloak:\n    image: quay.io/keycloak/keycloak:14.0.0\n    container_name: keycloak\n    environment:\n      KEYCLOAK_USER: admin\n      KEYCLOAK_PASSWORD: admin\n    ports:\n      - \"8082:8080\"\n    restart: always\n    volumes:\n      - .local/keycloak/:/opt/jboss/keycloak/standalone/data/\nFound similar answer with plain docker https://stackoverflow.com/a/60554189/6916890\ndocker run --volume /root/keycloak/data/:/opt/jboss/keycloak/standalone/data/",
    "Visual studio docker tools publish port when debugging": "Port mapping must be specified during docker run command. EXPOSE command within dockerfile normally used for documentation purposes.\nSolution: in your Visual Studio project file add the following:\n  <PropertyGroup>\n    <DockerfileRunArguments>-p 5000:5000</DockerfileRunArguments>\n  </PropertyGroup>\nReferences:\nhttps://learn.microsoft.com/en-us/visualstudio/containers/container-msbuild-properties?view=vs-2019 https://docs.docker.com/engine/reference/builder/#expose",
    "Angular Development Docker Container": "As I suspected, it was an Angular config issue. All I needed to do was change my package.json file.\nFrom:\n\"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\"\nTo:\n\"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve --host 0.0.0.0 --disable-host-check\"\nMy Dockerfile stayed the same.",
    "Go's time doesn't work under the docker image from scratch": "I think the proper way to solve this is to import tzdata as the library wants it to be. You can make use of the TZ environment variable.\nThe solution is indicated in a Dockerfile which I found on GitHub here\nTo wrap up everything:\nFROM golang:alpine AS build\nRUN apk update && apk add ca-certificates && apk add tzdata\nWORKDIR /app\nADD . .\nRUN CGO_ENABLED=0 GOOS=linux go build -o myapp\n\nFROM scratch AS final\nCOPY --from=build /usr/share/zoneinfo /usr/share/zoneinfo\nCOPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\nCOPY --from=build /app/myapp /\n\nENV TZ Australia/Sydney\nENTRYPOINT [\"/myapp\"]",
    "Installing curl inside nginx docker image": "For Nginx image use Debian as OS use below : -\nFROM nginx\nRUN apt-get update && apt-get install -y curl && apt-get clean",
    "How to set Docker ENTRYPOINT To NPM?": "The issue seems to be due to the single quotes in the ENTRYPOINT. Could you check if using the double quotes the issue solves?\nENTRYPOINT [\"/usr/local/bin/npm\", \"run\"]\nAccording to the Docker guide, there are only two forms to write the ENTRYPOINT:\nexec form (preferred): ENTRYPOINT [\"executable\", \"param1\", \"param2\"]\nshell form: ENTRYPOINT command param1 param2",
    "how to resolve warning ''using a subshell to avoid having to cd back'": "You can invoke the commands in a subshell (...):\n(\n   cd ./folder_02\n   docker build . -t image_name\n)\n(\n   cd ./folder_03\n   something else\n)\nIn scripts that I want to preserve environment I use pushd+popd.\npushd ./folder_02 >/dev/null\ndocker build . -t image_name\npopd >/dev/null\n\npushd ./folder_03 >/dev/null\nsomething soemthing\npopd >/dev/null",
    "apache not starting in alpine image docker": "Try this :\nFROM alpine:latest\nRUN \\\n    apk add --no-cache \\\n    apache2-proxy \\\n    apache2-ssl \\\n    apache2-utils \\\n    curl \\\n    git \\\n    logrotate \\\n    openssl\n\nENV APACHE_RUN_USER www-data\nENV APACHE_RUN_GROUP www-data\nENV APACHE_LOG_DIR /var/log/apache2\n\nWORKDIR /var/www/localhost/htdocs\nCOPY  index.html  /var/www/localhost/htdocs \n\nEXPOSE 80\n\nCMD [\"/usr/sbin/httpd\", \"-D\", \"FOREGROUND\"]",
    "Docker getting exited just after start": "Edit: In my original post I mention: \"try to think like with VMs\". I recently fell into this, which says not to do so:\nStop thinking about a container as a mini-VM and instead start thinking about it as just a process.\nalso, worth-reading article: Containers are not VMs\nOriginal post:\nThe logic with Docker containers is that they are supposed to have a service up and running. If this service stops, they exit and go to \"stopped\" state. (As you learn more about Docker, you will understand how this works and you will be able to use ENTRYPOINT and CMD). But let's skip this for a while and try to think like with VMs, run a new container and get inside to type some commands...\nthis succeeds:\ndocker container create -it --name test ubuntu\n445cad0a3afea97494635361316e5869ad3b9ededdd6db46d2c86b4c1461fb75\n$ docker container start test\ntest\n$ docker container exec -it test bash\nroot@445cad0a3afe:/# your are inside, you can type your commands here!\nwhy yours failed...\nwhen you created the container, you didn't use the -i flag which helps Keep STDIN open even if not attached. This practically means that when the container starts, it uses the CMD set in the official ubuntu Dockerfile, which is bash and then exits immediately.\ndocker attach VS docker exec --it bash\nYou can test this with an image like nginx. If you run a new nginx container and try to attach to it, you will see that logs from nginx are being printed out and you are not able to type any command in the shell. This happens because the CMD of the image is the following:\n# Define default command.\nCMD [\"nginx\"]\nTo be able to \"attach\" to a container like that but also be able to use the shell (some others may also mention this like doing something equivalent to ssh to the container), you will have to run:\ndocker container exec -it your_container_name bash\nI suggest you also read:\nIs it possible to start a shell session in a running container (without ssh)\nDocker - Enter Running Container with new TTY\nHow do you attach and detach from Docker's process?\nWhy docker container exits immediately\n~jpetazzo: If you run SSHD in your Docker containers, you're doing it wrong!",
    "Dockerfile fails to build": "If your host is an Ubuntu VM, it could be an invalid /etc/resolve.conf. Look at the /etc/resolv.conf on the host Ubuntu VM. If it contains nameserver 127.0.1.1, that is wrong.\nRun these commands on the host Ubuntu VM to fix it:\nsudo vi /etc/NetworkManager/NetworkManager.conf\n# Comment out the line `dns=dnsmasq` with a `#`\n\n# restart the network manager service\nsudo systemctl restart network-manager\n\ncat /etc/resolv.conf\nNow /etc/resolv.conf should have a valid value for nameserver, which will be copied by the docker containers.",
    "OpenShift 3.1 - Prevent Docker from caching curl resource": "According to the OpenShift docs (https://docs.openshift.com/enterprise/3.1/dev_guide/builds.html#no-cache) you can force builds to not be cached using the following syntax:\nstrategy:\n  type: \"Docker\"\n  dockerStrategy:\n    noCache: true\nThis will mean that no steps are cached, which will make your builds slower but will mean you have the correct artifact version in your build.",
    "Dockerized Vue app - hot reload does not work": "After many days I managed to add hot reload by adding in the webpack configuration file this config:\ndevServer: {\n      public: '0.0.0.0:8080'\n    }\nAfter digging to the official vue js repo, specifically to serve.js file found the public option which:\nspecify the public network URL for the HMR client\nIf you do not want to edit your webpack config, you can do this directly from docker-compose file in the command:\ncommand: npm run serve -- --public 0.0.0.0:8080",
    "what \"--from=builder\" do in Dockerfile": "builder is the name of a previous stage in the multi-stage Dockerfile. It is probably building the application manager. Once the builder is complete, this current stage copies the manager file into the current image /workspace directory. The entrypoint simply gives the program that will run when the container starts.\nYou can find more detailed explanations here:\nhttps://docs.docker.com/engine/reference/builder/",
    "How do I run Prisma migrations in a Dockerized GraphQL + Postgres setup?": "Late answer to my own question: Like @Athir suggested I now separated the two processes and created two docker-compose.yml files: one named docker-compose.migrate.yml that's responsible for running the migration and one named docker-compose.yml which is the main application.\nMy docker-compose.migrate.yml:\nversion: '3'\nservices:\n  prisma-migrate:\n    container_name: prisma-migrate\n    build: ./api/prisma\n    env_file:\n      - .env\n    environment:\n      DB_HOST: <secret>\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13\n    container_name: db\n    restart: always\n    env_file:\n      - .env\n    environment:\n      DB_PORT: 5432\n    ports:\n      - ${DB_PORT}:5432\n    volumes:\n      - ${POSTGRES_VOLUME_DIR}:/var/lib/postgresql/data\nWith the following Prisma Dockerfile:\nFROM node:14\n\nRUN echo $DATABASE_URL\n\nWORKDIR /app\n\nCOPY ./package.json ./\nCOPY . ./prisma/\n\nRUN chmod +x ./prisma/wait-for-postgres.sh\n\nRUN npm install\nRUN npx prisma generate\n\nRUN apt update\nRUN apt --assume-yes install postgresql-client\n\n# Git will replace the LF line-endings with CRLF, causing issues while executing the wait-for-postgres shell script\n# Install dos2unix and replace CRLF (\\r\\n) newlines with LF (\\n)\nRUN apt --assume-yes install dos2unix\nRUN dos2unix ./prisma/wait-for-postgres.sh\n\nCMD sh ./prisma/wait-for-postgres.sh ${DB_HOST} ${POSTGRES_USER} npx prisma migrate deploy && npx prisma db seed --preview-feature\nEDIT: wait-for-postgres.sh:\n#!/bin/sh\n# wait-for-postgres.sh\n\nset -e\n  \nhost=\"$1\"\nuser=\"$2\"\nshift\nshift\ncmd=\"$@\"\n  \nuntil PGPASSWORD=$POSTGRES_PASSWORD psql -h \"$host\" -U \"$user\" -c '\\q'; do\n  >&2 echo \"Postgres is unavailable - sleeping\"\n  sleep 1\ndone\n  \n>&2 echo \"Postgres is up - executing command\"\n\nexec $cmd\nEDIT: example .env file:\n# ---- DB ----\nDB_HOST=localhost\nDB_PORT=5432\nDB_SCHEMA=example\n\nPOSTGRES_DB=example\nPOSTGRES_USER=example\nPOSTGRES_VOLUME_DIR=/path/where/you/want/to/store\nPOSTGRES_PASSWORD=example\n\nDATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${DB_HOST}:${DB_PORT}/${POSTGRES_DB}?schema=${DB_SCHEMA}",
    "'cannot normalize nothing' error on Docker Hub multi-stage build with shared ARG [duplicate]": "It turns out that I need to renew ARG on each step I want to reuse it. Quoting docs:\nAn ARG instruction goes out of scope at the end of the build stage where it was defined. To use an arg in multiple stages, each stage must include the ARG instruction.\nThe fixed Dockerfile is:\nARG BUILD_DIR=/build\n\nFROM alpine:latest as build\nARG BUILD_DIR # <----------------------------------\nRUN apk add --no-cache gcc musl-dev\nWORKDIR $BUILD_DIR\nRUN echo -e '\\n\\\n#include <stdio.h>\\n\\\nint main (void)\\n\\\n{ puts (\"Hello, World!\");}'\\\n>> hello.c\nRUN cc -o hello hello.c\n\nFROM alpine:latest\nARG BUILD_DIR # <----------------------------------\nCOPY --from=build $BUILD_DIR/hello /app\nCMD [\"/app\"]\nIt means that an ARG value from the code in my initial question does not even work. Interesting, that my Docker For Mac ignores the problem and silently uses empty BUILD_DIR argument for WORKDIR producing successful build. While Docker Hub automated builder fails with an error explicitly.",
    "Passing arguments to a docker container when calling docker-compose": "There are two \"halves\" to the command line Docker ultimately runs, the entrypoint and the command. There are corresponding Docker Compose entrypoint: and command: directives to set them. Their interaction is fairly straightforward: they're just concatenated together to produce a single command line.\nGiven what you show, in your Dockerfile you could write\nENTRYPOINT [\"dotnet\", \"myproj.dll\"]\nand then in your docker-compose.yml\ncommand: myArg1=hallo myArg2=world\nand that would ultimately launch that process with those arguments.\nIf you want to be able to specify this from the command line when you spin up the Compose stack, Compose supports ${VARIABLE} references everywhere so you could write something like\ncommand: myArg1=${MY_ARG1:-hallo} world\nand then launch this with a command like\nMY_ARG1=greetings docker-compose up\nA very typical pattern in Docker is to use ENTRYPOINT for a wrapper script that does some first-time setup, and then runs the command as the main container process with exec \"$@\". The entrypoint gets the command-line arguments as parameters and can do whatever it likes with them, including reformatting them or ignoring them entirely, and you could also use this to implement @Kevin's answer of configuring this primarily with environment variables, but translating that back to var=value type options.",
    "Problems running conda update in a dockerfile": "You have to add anaconda to the PATH during build time with the ENV variable before executing anaconda inside the Dockerfile.\nRUN bash Anaconda3-2018.12-Linux-x86_64.sh -b && \\\n    echo \"export PATH=\"/root/anaconda3/bin:$PATH\"\" >> ~/.bashrc && \\\n    /bin/bash -c \"source ~/.bashrc\"\nENV PATH /root/anaconda3/bin:$PATH\nRUN conda update --all\nUpdating PATH in .bashrc will make it possible to call conda inside the container when run with docker run, but not in other RUN statements in the docker file.",
    "Docker-compose copy config": "You need to add a volumes reference in your docker-compose.yml.\nversion: 2\nservices:\n  myService:\n    image: serviceA from registry\n     # Would like to modify a config file's content...\n    volumes:\n      - /path/to/host/config.conf:/path/to/container/config/to/overwrite/config.conf\n\n  mainAPI:\n     ...\nSince 3.3, you can also create your own docker config file to overwrite the existing one. This is similar to above, except the docker config is now a copy of the original instead of binding to the host config file.\nversion: 3.3\nservices:\n  myService:\n    image: serviceA from registry\n     # Would like to modify a config file's content...\n    configs:\n      - source: myconfigfile\n        target: /path/to/container/config/to/overwrite/config.conf\n\n  mainAPI:\n     ...\n\nconfigs:\n  myconfigfile:\n    file: ./hostconfigversion.conf\nSee https://docs.docker.com/compose/compose-file/#long-syntax for more info.",
    "What Docker scratch contains by default?": "The scratch image contains nothing. No files. But actually, that can work to your advantage. It turns out, Go binaries built with CGO_ENABLED=0 require absolutely nothing, other than what they use. There are a couple things to keep in mind:\nWith CGO_ENABLED=0, you can't use any C code. Actually not too hard.\nWith CGO_ENABLED=0, your app will not use the system DNS resolver. I don't think it does by default anyways because it's blocking and Go's native DNS resolver is non-blocking.\nYour app may depend on some things that are not present:\nApps that make HTTPS calls (as in, to other services, i.e. Amazon S3, or the Stripe API) will need ca-certs in order to confirm HTTPS certificate authenticity. This also has to be updated over time. This is not needed for serving HTTPS content.\nApps that need timezone awareness will need the timezone info files.\nA nice alternative to FROM scratch is FROM alpine, which will include a base Alpine image - which is very tiny (5 MiB I believe) and includes musl libc, which is compatible with Go and will allow you to link to C libraries as well as compile without setting CGO_ENABLED=0. You can also leverage the fact that alpine is regularly updated, using its tzinfo and ca-certs.\n(It's worth noting that the overhead of Docker layers is amortized a bit because of Docker's deduplication, though of course that is negated by how often your base image is updated. Still, it helps sell the idea of using the quite small Alpine image.)\nYou may not need tzinfo or ca-certs now, but it's better to be safe than sorry; you can accidentally add a dependency without realizing it breaks your build. So I recommend using alpine as your base. alpine:latest should be fine.\nBonus: If you want the advantages of reproducible builds inside Docker, but with small image sizes, you can use the new Docker multi-stage builds available in Docker 17.06+.\nIt works a bit like this:\nFROM golang:alpine\nADD . /go/src/github.com/some/gorepo  # may need some go getting if you don't vendor\nRUN go build -o /app github.com/some/gorepo\n\nFROM scratch  # or alpine\nCOPY --from=0 /app /app\nENTRYPOINT [\"/app\"]\n(I apologize if I've made any mistakes, I'm typing that from memory.)\nNote that when using FROM scratch you must use the exec form of ENTRYPOINT, because the shell form won't work (it depends on the Docker image having /bin/sh, which it won't.) This will work fine in Alpine.",
    "Answer '29' to apt-get install prompt for xorg": "There are quite a few issues I see with the Dockerfile provided.\nDefining a volume inside the Dockerfile provides little to no value. It's much better to define this in your docker-compose.yml or as part of your run command. I've got a blog post going through the issues with this.\nSplitting up the apt-get update from the later apt-get install commands can result in situations where the apt-get install will fail. There's a section in the best practices about this.\nFor your error message, I'd run apt-get in the noninteractive mode. You can also preconfigure debconf if you need a non-default value set during install.\nSplitting each apt-get into a separate RUN command is creating lots of excessive layers, these should be merged where possible. This is also described in the best practices documentation.\nHere's a sample of an install command that works for me taking the above into account:\nFROM ubuntu:16.04\n\nRUN apt-get update \\\n && DEBIAN_FRONTEND=noninteractive apt-get install -y \\\n      build-essential \\\n      xorg \\\n      libssl-dev \\\n      libxrender-dev \\\n      wget \\\n      gdebi \\\n      libxrender1 \\\n      xfonts-utils \\\n      xfonts-base \\\n      xfonts-75dpi",
    "Docker echo to /etc/hosts not working": "Docker manages /etc/hosts. It does this to make container linking work. You can ask docker to append to the hosts file when starting the container with\ndocker run -it --add-host foo:192.168.99.100",
    "How to use Laravel docker container & MySQL DB with a Vue one?": "According to this article you will need to follow the steps below.\nMake your project folder look like this: (d: directory, f: file)\nd: backend\nd: frontend\nd: etc\n    d: nginx\n        d: conf.d\n            f: default.conf.nginx\n    d: php\n        f: .gitignore\n    d: dockerize\n        d: backend\n            f: Dockerfile\nf: docker-compose.yml\nAdd docker-compose.yml\n    version: '3'\n    services:\n    www:\n        image: nginx:alpine\n        volumes:\n            - ./etc/nginx/conf.d/default.conf.nginx:/etc/nginx/conf.d/default.conf\n        ports:\n            - 81:80\n        depends_on:\n            - backend\n            - frontend\n\n    frontend:\n        image: node:current-alpine\n        user: ${UID}:${UID}\n        working_dir: /home/node/app\n        volumes:\n            - ./frontend:/home/node/app\n        environment:\n            NODE_ENV: development\n        command: \"npm run serve\"\n\n    backend:\n        build:\n            context: dockerize/backend\n        # this way container interacts with host on behalf of current user.\n        # !!! NOTE: $UID is a _shell_ variable, not an environment variable!\n        # To make it available as a shell var, make sure you have this in your ~/.bashrc (./.zshrc etc):\n        # export UID=\"$UID\"\n        user: ${UID}:${UID}\n\n        volumes:\n            - ./backend:/app\n            # custom adjustments to php.ini\n            # i. e. \"xdebug.remote_host\" to debug the dockerized app\n            - ./etc/php:/usr/local/etc/php/local.conf.d/\n        environment:\n            # add our custom config files for the php to scan\n            PHP_INI_SCAN_DIR: \"/usr/local/etc/php/conf.d/:/usr/local/etc/php/local.conf.d/\"\n        command: \"php artisan serve --host=0.0.0.0 --port=8080\"\n\n    mysql:\n        image: mysql:5.7.22\n        container_name: mysql\n        restart: unless-stopped\n        tty: true\n        ports:\n            - \"4306:3306\"\n        volumes:\n            - ./etc/mysql:/var/lib/mysql\n        environment:\n            MYSQL_DATABASE: tor\n            MYSQL_USER: root\n            MYSQL_PASSWORD: root\n            MYSQL_ROOT_PASSWORD: root\n            SERVICE_TAGS: dev\n            SERVICE_NAME: mysql\nAdd default.conf.nginx\n    server {\n        listen 81;\n        server_name frontend;\n        error_log  /var/log/nginx/error.log debug;\n\n        location / {\n            proxy_pass http://frontend:8080;\n        }\n\n        location /sockjs-node {\n            proxy_pass http://frontend:8080;\n            proxy_set_header Host $host;\n            # below lines make ws://localhost/sockjs-node/... URLs work, enabling hot-reload\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"Upgrade\";\n        }\n\n        location /api/ {\n            # on the backend side, the request URI will _NOT_ contain the /api prefix,\n            # which is what we want for a pure-api project\n            proxy_pass http://backend:8080/;\n            proxy_set_header Host localhost;\n        }\n    }\nAdd Dockerfile\nFROM php:fpm-alpine\n\nRUN apk add --no-cache $PHPIZE_DEPS oniguruma-dev libzip-dev curl-dev \\\n    && docker-php-ext-install pdo_mysql mbstring zip curl \\\n    && pecl install xdebug redis \\\n    && docker-php-ext-enable xdebug redis\nRUN mkdir /app\nVOLUME /app\nWORKDIR /app\n\nEXPOSE 8080\nCMD php artisan serve --host=0.0.0.0 --port=8080\nDON'T FORGET TO ADD vue.config.js to your frontend folder\n// vue.config.js\nmodule.exports = {\n    // options...\n    devServer: {\n        disableHostCheck: true,\n        host: 'localhost',\n        headers: {\n            'Access-Control-Allow-Origin': '*',\n            'Access-Control-Allow-Headers': 'Origin, X-Requested-With, Content-Type, Accept'\n        },\n        watchOptions: {\n            poll: true\n        },\n        proxy: 'http://localhost/api',\n    } \n}\nRun sudo docker-compose up\nIf you want to do migrations run this: sudo docker-compose exec backend php artisan migrate",
    "How to remove <none> images after building successfull": "docker rmi `docker images | grep \"<none>\" | awk {'print $3'}`",
    "Start redis container with backup dump.rdb": "You can use docker-compose.yml like :\nversion: '3'\nservices:\n  redis:\n    image: redis:alpine\n    container_name: \"redis\"\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - /data/redis:/data\n\n  server:\n    build: ./src\n    image: hubName:imageName\n    container_name: containerName\n    links: \n      - redis\n    depends_on:\n      - \"redis\"\n    ports:\n      - \"8443:8443\"\n    restart: always\nLet's copy your dump.rdb to /data/redis folder on your host machine then start docker-compose.\nAbout redis persistance,you must have docker volume and have two types for redis persinstance: RDB and AOF\nRDB: The RDB persistence performs point-in-time snapshots of your dataset at specified intervals ( example: 60 seconds or if there're at least 10000 keys have been changed)\nAOF: logs every write operation received by the server(eg: SET command) , that will be played again at server startup, reconstructing the original dataset\nFor more: https://redis.io/topics/persistence\nYou should decide base on your critical data level. In this case you have rdb dump so you can use RDB, it's default option",
    "How to delete files sent to Docker daemon build context": "What happens when you run docker build . command:\nDocker client looks for a file named Dockerfile at the same directory where your command runs. If that file doesn't exists, an error is thrown.\nDocker client looks a file named .dockerignore. If that file exists, Docker client uses that in next step. If not exists nothing happens.\nDocker client makes a tar package called build context. Default, it includes everything in the same directory with Dockerfile. If there are some ignore rules in .dockerignore file, Docker client excludes those files specified in the ignore rules.\nDocker client sends the build context to Docker engine which named as Docker daemon or Docker server.\nDocker engine gets the build context on the fly and starts building the image, step by step defined in the Dockerfile.\nAfter the image building is done, the build context is released.\nSo, your build context is not replicated anywhere but in the image you just created if only it needs all the build context. You can check image sizes by running this: docker images. If you see some unused or unnecessary images, use docker rmi unusedImageName.\nIf your image does'nt need everything in the build context, I suggest you to use .dockerignore rules, to reduce build context size. Exclude everything which are not necessary for the image. This way, the building process will be shorter and you will see if there is any misconfigured COPY or ADD steps in the Dockerfile.\nFor example, I use something like this:\n# .dockerignore\n* # exclude everything\n!build/libs/*.jar # include just what I need in the image\nhttps://docs.docker.com/engine/reference/builder/#dockerignore-file\nhttps://docs.docker.com/engine/docker-overview/",
    "Expand ARG value in CMD [Dockerfile]": "The problem here is that ARG params are available only during image build.\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\nhttps://docs.docker.com/engine/reference/builder/#arg\nCMD is executed at container startup where ARG variables aren't available anymore.\nENV variables are available during build and also in the container:\nThe environment variables set using ENV will persist when a container is run from the resulting image.\nhttps://docs.docker.com/engine/reference/builder/#env\nTo solve your problem you should transfer the ARG variable to an ENV variable.\nadd the following line before your CMD:\nENV RUNTIME_ENV ${RUNTIME_ENV}\nIf you want to provide a default value you can use the following:\nENV RUNTIME_ENV ${RUNTIME_ENV:default_value}\nHere are some more details about the usage of ARG and ENV from the docker docs.",
    "How to copy files and folders to a working directory in Dockerfile?": "Solution\nIn order to copy files and folders to a working directory, you can use the following in your Dockerfile:\nWORKDIR /working/directory/path\nCOPY . .\nThis is an easy way to change to a working directory and copy everything from your host source.\nPlease Note\nThis will copy everything from the source path --more of a convenience when testing locally.",
    "How to use RUN clone git in dockerfile": "The best solution is to change docker working directory using WORKDIR. So your Dockerfile should look like this:\nFROM python:3\nRUN git clone https://github.com/user/mygit.git\nWORKDIR mygit\nCMD [ \"python3\", \"./aa.py\" ]",
    "Ubuntu dockerfile - mailutils install": "Use the following dockerfile:\nFROM ubuntu:latest\nENV DEBIAN_FRONTEND=\"noninteractive\"\nRUN apt-get update && apt-get install -y mailutils\nThe important part is setting debconf to noninteractive mode.",
    "'DEBIAN_FRONTEND=noninteractive' not working inside shell script with apt-get": "Drop sudo in your script, there is point to use it if you're running as root. This is also the reason that DEBIAN_FRONTEND has no effect - sudo drops your current user's environment for security reasons, you'd have to use with -E option to make it work.",
    "Cannot start service app: OCI runtime create failed: container_linux.go:349": "Set the permission to your executable it should work.\nRUN chmod +x ./main\n# Command to run the executable\nCMD [\"./main\"]",
    "how to run .sh file when container is running using dockerfile": "At a purely mechanical level, the quotes are causing trouble. When you say\nRUN \"sh test.sh\"\nit tries to run a single command named sh\\ test.sh; it does not try to run sh with test.sh as a parameter. Any of the following will actually run the script\nRUN [\"sh\", \"test.sh\"]\nRUN sh test.sh\nRUN chmod +x test.sh; ./test.sh\nAt an operational level you'll have a lot of trouble running that command in the server container at all. The big problem is that you need to run that command after the server is already up and running. So you can't run it in the Dockerfile at all (no services are ever running in a RUN command). A container runs a single process and you need that process to be the Elasticsearch server itself, so you can't do this directly in ENTRYPOINT or CMD either.\nThe easiest path is to run this command from the host:\ndocker build -t my/elasticsearch .\ndocker run -d --name my-elasticsearch -p 9200:9200 my/elasticsearch\ncurl http://localhost:9200  # is it alive?\n./test.sh\nIf you have a Docker Compose setup, you could also run this from a separate container, or you could run it as part of the startup of your application container. There are some good examples of running database migrations in an ENTRYPOINT script for your application container running around, and that's basically the pattern you're looking for.\n(It is theoretically possible to run this in an entrypoint script. You have to start the server, wait for it to be up, run your script, stop the server, and then finally exec \"$@\" to run the CMD. This is trickier for Elasticsearch, where you might need to connect to other servers in the same Elasticsearch cluster lest your state get out of sync. The official Docker Hub mysql does this, for a non-clustered database server; see its rather involved entrypoint script for ideas.)",
    "Can't build Docker multi-stage image using ARG in COPY instruction": "OK. I discovered it's an open issue on Docker side. https://github.com/moby/moby/issues/35018\n--> ARG/ENV substitution is NOT working for -- values in ADD and COPY.\nFor my case, there is a work around while waiting for correction in Docker. https://github.com/docker/cli/issues/996\nARG HELLO_VERSION\nFROM hello-world:${HELLO_VERSION:-latest} as hello\n\nFROM ubuntu:18.04\nCOPY --from=hello /hello /hello",
    "How to nuke everything inside my Docker containers and start a new?": "So apparently docker system prune has some additional options, and the proper way to nuke everything was docker system prune --all --volumes. The key for me was probably --volumes, as those would probably hold cached packages that had to be rebuilt.\nThe segmentation fault is gone now \\o/",
    "How to build an image of a project using docker-compose with Dockerfile?": "TL;DR\nA single Dockerfile is usually not enough to replace a whole containers orchestration made with docker-compose and is not necessarly a good choice.\nAbout converting a docker-compose.yml file to a Dockerfile :\nYou can pass some informations from your docker-compose.yml file to your Dockefile (the command to run for instance) but that wouldn't be equivalent and you can't do that with all the docker-compose.yml file content.\nYou can replace your docker-compose.yml file with commands lines though (as docker-compose is precisely to replace it).\nBUT\nKeep in mind that Dockerfiles and docker-compose serve two whole different purposes.\nDockerfile are meant for image building, to define the steps to build your images.\ndocker-compose is a tool to start and orchestrate containers to build your applications (you can add some informations like the build context path or the name for the images you'd need, but not the Dockerfile content itself).\nSo asking to \"convert a docker-compose.yml file into a Dockerfile\" isn't really relevant.\nThat's more about converting a docker-compose.yml file into one (or several) command line(s) to start containers by hand.\nThe purpose of docker-compose is precisely to get rid of these command lines to make things simpler (it automates it).\nMultiple processes in a single container :\nFrom the docker documentation :\nIt\u2019s ok to have multiple processes, but to get the most benefit out of Docker, avoid one container being responsible for multiple aspects of your overall application\nSo you can if your entrypoint permits you to launch several processes, or if you use a supervisor, but maybe that's not necessarly the best idea.\nEDIT\nSince I'm not sure it's clear for you either, here is the difference between a container and an image.\nYou really should check this out and try to understand this before working with Docker since it's a very necessary thing to know.",
    "What happens when the base image of my image gets updated?": "Your image, as downloaded by someone, will always remain the same. An image relies on specific layers to give the image it's SHA256 checksum. Modifying parent layers would modify the checksum used to reference the image, so that would become a new image. The only way for that image to change is if the image is referenced by a tag and the local tag changes, either manually or by pulling the image tag again.\ndocker build will use the a local image first by default. You either need to run docker build --pull, separately docker pull or docker rmi IMAGE for the build to use the latest tagged image.\nThe Docker Hub build service has a build feature to automatically rebuild when any specified image's are updated in the hub.",
    "Waiting for a Docker container to be ready": "This will successfully wait for Postgres to start. (Specifically line 6)\nservices:\n  practice_docker: \n    image: dockerhubusername/practice_docker\n    ports: \n      - 80:3000\n    command: bash -c 'while !</dev/tcp/db/5432; do sleep 1; done; npm start'\n    depends_on:\n      - db\n    environment:\n      - DATABASE_URL=postgres://postgres:password@db:5432/practicedocker\n      - PORT=3000   \n  db:\n    image: postgres\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=password\n      - POSTGRES_DB=practicedocker",
    "docker container can't use `service sshd restart`": "The build process only builds an image. Processes that are run at that time (using RUN) are no longer running after the build, and are not started again when a container is launched using the image.\nWhat you need to do is get sshd to start at container runtime. The simplest way to do that is using an entrypoint script.\nDockerfile:\nCOPY entrypoint.sh /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD [\"whatever\", \"your\", \"command\", \"is\"]\nentrypoint.sh:\n#!/bin/sh\n\n# Start the ssh server\n/etc/init.d/ssh restart\n\n# Execute the CMD\nexec \"$@\"\nRebuild the image using the above, and when you use it to start a container, it should start sshd before running your CMD.\nYou can also change the base image you start from to something like Phusion baseimage if you prefer. It makes it easy to start some services like syslogd, sshd, that you may wish the container to have running.",
    "Is there a way to set the Docker container's mac address in docker-compose.yml file?": "You can use mac_address: xyz to configure the service.\nservices:\n  my_container:\n    mac_address: 00-50-56...\nBoth forms of MAC address (00:50:56... and 00-50-56...) should work.\nThis used to be a \"legacy\" option but appears to now be fully supported.",
    "Ollama in Docker pulls models via interactive shell but not via RUN command in the dockerfile": "Try this:\nDockerfile:\nFROM ollama/ollama\n\nCOPY ./run-ollama.sh /tmp/run-ollama.sh\n\nWORKDIR /tmp\n\nRUN chmod +x run-ollama.sh \\\n    && ./run-ollama.sh\n\nEXPOSE 11434\nrun-ollama.sh:\n#!/usr/bin/env bash\n\nollama serve &\nollama list\nollama pull nomic-embed-text\n\nollama serve &\nollama list\nollama pull qwen:0.5b-chat",
    "how to resolve failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount3554908678/Dockerfile: [duplicate]": "Ah, got it!\nYou ran the following command: C:\\Users\\User>docker build -t javaapp1 from C:\\Users\\User directory, which don't contain your Dockerfile. To get it working, first change the directory to one that contains your Dockerfile, which is C:\\Users\\User\\Desktop\\New folder 1 and then run docker build command\nHope it helped! \u270c\ufe0f",
    "Is it possible to build AOSP inside a docker container with alpine flavour?": "Android team has provided a Dockerfile, docker build command to build the image, and docker run command to run a container : https://android.googlesource.com/platform/build/+/master/tools/docker\nAs mentioned there, you can mount your local AOSP source tree to the container using -v option in docker run command.\nI have not tried to build AOSP in a Docker container. But in this question, @VasileM has mentioned that he uses the Android provided Docker instance to build AOSP.",
    "Best practice - Anonymous volume vs bind mount": "You're missing a key third option, named volumes. If you declare:\nversion: '3'\nvolumes:\n  build: {}\nservices:\n  cache:\n    image: ...\n    volumes:\n      - build:/build\nDocker Compose will create a named volume for you; you can see it with docker volume ls, for example. You can explicitly manage named volumes' lifetime, and set several additional options on them which are occasionally useful. The Docker documentation has a page describing named volumes in some detail.\nI'd suggest that named volumes are strictly superior to anonymous volumes, for being able to explicitly see when they are created and destroyed, and for being able to set additional options on them. You can also mount the same named volume into several containers. (In this sequence of questions you've been asking, I'd generally encourage you to use a named volume and mount it into several containers and replace volumes_from:.)\nNamed volumes vs. bind mounts have advantages and disadvantages in both directions. Bind mounts are easy to back up and manage, and for content like log files that you need to examine directly it's much easier; on MacOS systems they are extremely slow. Named volumes can run independently of any host-system directory layout and translate well to clustered environments like Kubernetes, but it's much harder to examine them or back them up.\nYou almost never need a VOLUME directive. You can mount a volume or host directory into a container regardless of whether it's declared as a volume. Its technical effect is to mount a new anonymous volume at that location if nothing else is mounted there; its practical effect is that it prevents future Dockerfile steps from modifying that directory. If you have a VOLUME line you can almost always delete it without affecting anything.",
    "How can I run a cron in MariaDB container?": "Elaborating on @k0pernikus's comment, I would recommend to use a separate container that runs cron. The cronjobs in that container can then work with your mysql database.\nHere's how I would approach it:\n1. Create a Cron Docker Container\nYou can set up a cron container fairly simply. Here's an example Dockerfile that should do the job:\nFROM alpine\nCOPY ./crontab /etc/crontab\nRUN crontab /etc/crontab\nRUN touch /var/log/cron.log\nCMD crond -f\nJust put your crontab into a crontab file next to that Dockerfile and you should have a working cron container.\nAn example crontab file:\n* * * * * mysql -h mysql --execute \"INSERT INTO database.table VALUES 'v';\"\n2. Add the cron container to your docker-compose.yml as a service\nMake sure you add your cron container to the docker-compose.yml, and put it in the same network as your mysql service:\nnetworks:\n    my_network:\nservices:\n    mysql:\n        image: mariadb\n        networks:\n          - my_network\n    cron:\n        image: my_cron\n        depends_on: \n          - mysql\n        build:\n            context: ./path/to/my/cron-docker-folder\n        networks:\n          - my_network",
    "Mysql installation for alpine linux in docker": "First of all - why do you want to install mysql-server on that specific image? I would suggest using 2 separate containers - one for the mysql database and one for the application you are developing using that openjdk image.\nYou can simply use this gist https://gist.github.com/karlisabele/d0bebe3d27fc44a57d1db9a9abdff45a to create a setup where your Java application can connect to mysql database using database (the service name) as hostname for mysql server.\nSee https://hub.docker.com/_/mysql for additional configurations on mysql image (you should define MYSQL user, password, db name etc...)\nUPDATE\nI updated the gist and added the environment variables necessary for the mysql to work... Replace the qwerty values with your own and you should be able to access the database from your Java application via database:3306 using the username and password provided in the environment variables\nThe volumes definition at the end of the file tells docker daemon that we want to create a persistent volume somewhere in the host file system and use mysql as an alias. So that when we define the database service volumes we can simply use the mysql:/var/lib/mysql and it will actually mount an obscure folder that docker created on your filesystem. This will allow the mysql database to have persistent state everytime you start the service (because it will mount the data from your host machine) and you don't have to worry about the actual location of the volume\nYou can see all volumes by running docker volumes ls",
    "Use console output in Dockerfile": "Dockerfiles don't support shell syntax in general, except for some very limited environment variable expansion.\nThey do support ARGs that can be passed in from the command line, and an ARG can be used to define the image FROM. So you could start your Dockerfile with\nARG tag\nFROM company:${tag:-latest}\nand then build the image with\ndocker build --build-arg tag=$(cd $PWD/../;  echo ${PWD##*/}) .\n(which is involved enough that you might want to write it into a shell script).\nAt a very low level, it's also worth remembering that docker build works by making a tar file of the current directory, sending it across an HTTP connection to the Docker daemon, and running the build there. Once that process has happened, any notion of the host directory name is lost. In other words, even if the syntax worked, docker build also doesn't have the capability to know which host directory the Dockerfile is in.",
    "Dockerfile ~ Correct Syntax for Adding Windows Folders": "Apparently, the problem was that Docker does not support absolute paths as input paths.\nI was finally able to get it to work by putting the \"Bar\"-Folder in the same directory as the Dockerfile and then using the following ADD Statement in the Dockerfile:\nADD Bar C:/Bar/\nIf I am mistaken, and it is possible to use absolute paths as the source path, please correct me",
    "yum -y install not assuming yes in docker build": "It looks like you're missing the -y on the yum update.\nAlso, you should split those commands out to separate RUN commands. In this case, it doesn't make too much difference, but splitting the echos onto different lines will make it clearer.\nYou should keep the update and installs in the same command though\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#run",
    "Install JDK 8 update 172 in dockerfile with ubuntu image": "I think oracle has fixed broken web8upd.\nSo now dockerfile specified on github works perfectly !\nJust copy-pasting same dockerfile with some modifications :\nFROM ubuntu:16.04\n\n# To solve add-apt-repository : command not found\nRUN apt-get -y install software-properties-common\n\n# Install Java\nRUN \\\n  echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections && \\\n  add-apt-repository -y ppa:webupd8team/java && \\\n  apt-get update && \\\n  apt-get install -y oracle-java8-installer --allow-unauthenticated && \\\n  rm -rf /var/lib/apt/lists/* && \\\n  rm -rf /var/cache/oracle-jdk8-installer\n\n\n# Define commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\nPlease note : rm -rf /var/lib/apt/lists/* will remove all lists fetched by apt-get update.\nSo if you want to install more things after installing Java, remove rm -rf /var/lib/apt/lists/* otherwise you have to run apt-get update again.",
    "Docker containers serving different subdomains on port 80": "Yes you can. using a proxy.\nThere is a project by jwilder/nginx-proxy which allows you to give your hostname via an enviroment variable which will than route your request to the appropriate container.\nA good example of this implemented is given here: https://blog.florianlopes.io/host-multiple-websites-on-single-host-docker/",
    "confused with CMD and ENTRYPOINT in dockerfile": "The mongod Dockerfile references docker-entrypoint.sh.\nThat script docker-entrypoint.sh starts by testing if the first parameter begins with '-' (as in '--auth'):\nif [ \"${1:0:1}\" = '-' ]; then\n    set -- mongod \"$@\"\nfi\nSo the arguments become mongod --auth, not just --auth.\nMeaning:\nif you have to pass any argument after mongod, you don't have to type mongod first when using docker run: it will be added for you in docker-entrypoint.sh\nif you don't have any argument to pass after mongod, you don't have to type mongod either: CMD will provide it for you to the ENTRYPOINT docker-entrypoint.sh.",
    "docker-compose: console does not return to host when creating database container": "To run the container in the background with compose, use the detach option:\ndocker-compose up -d",
    "Docker: Error response from daemon: no such id:": "In order to facilitate the usage of docker exec, make sure you run your container with a name:\ndocker run -d --name aname.cont ...\nI don't see an entrypoint or exec directove in the Dockerfile, so do mention what you want to run when using docker run -d\n(I like to add '.cont' as a naming convention, to remember that it is a container name, not an image name)\nThen a docker exec aname.cont bash should work.\nCheck that the container is still running with a docker ps -a",
    "CUDA to docker-container": "You can start with a CUDA Docker image and then install Python, for example:\nFROM nvidia/cuda:12.1.1-runtime-ubuntu20.04\n\n# Install Python\nRUN apt-get update && \\\n    apt-get install -y python3-pip python3-dev && \\\n    rm -rf /var/lib/apt/lists/*\nNote: User @chronoclast has suggested additionally installing python-is-python3 to fix the broken symlink to the default Python, in which case the Python installation step would instead be:\nRUN apt-get update && \\\n    apt-get install -y python3-pip python3-dev python-is-python3 && \\\n    rm -rf /var/lib/apt/lists/*",
    "How to install curl from Dockerfile": "I just tested the RUN apt-get update\n&& apt-get install -y curl line and it works well.\nLook at how i did it,\nDefine a stage with .NET core image as base, which will end up being used as final image.\nDefine an other stage based on dotnet SDK image as build in order to build and publish the application, then reuse the base image as the final image.\nHere's my dockerfile which works fine for building and packaging dotnet apps while installing packages like curl and jq.\n#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM dotnet/aspnet:5.0 AS base\n\nWORKDIR /app\n\nRUN apt-get update \\\n    && apt-get install -y curl jq \n    \nEXPOSE 80\nEXPOSE 443\n\nFROM dotnet/sdk:5.0 AS build\n\nWORKDIR /src\n\n\nCOPY [\"PROJECT.csproj\", \"project/\"]\n\nRUN dotnet restore \"project/project.csproj\" \nCOPY . .\nWORKDIR \"/src/API\"\n\nRUN dotnet build \"project.csproj\" -c Release -o /app/build\nFROM build AS publish\n\nRUN dotnet publish \"project.csproj\" -c Release -o /app/publish\n\nRUN dotnet test \"../project.Test\" \\ \n    /p:CollectCoverage=true \\ \n    /p:CoverletOutputFormat=opencover \\ \n    /p:CoverletOutput=\"./../coverage\" \\\n    /p:Include=\\\"[project.BLL]*,[project.Model]*\\\"\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"project.dll\"]\n\n\nYou can see that the final image is the one that depends on the base image with installed curl and jq\nPlease tick the answer if it helped you !",
    "Docker: WORKDIR create a layer?": "This seems to be a documentation \"bug\" in the best practices document. A better way to have phrased this is that only those commands you mention will create layers that increase the total build size.\nNote that the layers you're seeing are 0 bytes, as opposed to the FROM, ADD, COPY which each have an associated size (the amount they contribute to the total filesystem growth).\nCmp   Size  Command                                                   \n    5.6 MB  FROM 8792aa27a60beb9                                       \n    2.1 MB  ADD add.tar.gz / # buildkit                                \n    1.0 MB  COPY copy /copy # buildkit                                 \n       0 B  RUN /bin/sh -c rm /bin/arch # buildkit                     \n       0 B  WORKDIR /etc\nA layer exists for WORKDIR - but it's metadata. A layer exists for the rm command, but all it adds is a \"file deleted\" marker - not actual file contents. So they contribute nothing to the eventual filesystem size in the image.\nAs for FROM, that doesn't, in and of itself, create a layer - it imports a starting image. The reference could be clearer about that, I guess, but either way it would confuse someone. They must assume it's obvious that FROM contributes to the size (unless you are using FROM scratch).\nI wasn't able to find a definitive reference about this. A reading of the source code might be the only such reference available.",
    "How to change php-fpm default port?": "I think the problem is not the sed command itself, it's related to the wrong file you mentioned for it.\n/usr/local/etc/php-fpm.d/zz-docker.conf\nthis is the file you are trying to change the port in it but inside your docker-compose file you are mapping something else\n./docker/php-fpm/config/www.conf:/usr/local/etc/php-fpm.d/www.conf",
    "Docker multi-stage build fails if we use CMD in dockerfile": "In a multistage build, you may copy files from a previous step. Each step is considered as an individual, private image(in the scope of the multistage build).\nCMD instruction however is not invoked at build time, it only applies at runtime as clearly stated in the official docs:\nThe main purpose of a CMD is to provide defaults for an executing container.\nSince you are currently building the result image, CMD is never executed thus you get the error you have reported.\nIn the other hand, RUN instruction executes during build time making its result available for the next step. Quoting again from docs:\nThe RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.\nIt should be clear by now why the multistage build completes successfully when RUN is used in contrast to CMD instruction.\nYour confusion started from wrongly assuming that the below is true:\nI thought in the intermittent container, the cmd would execute which should make both the commands equivalent right?",
    "Next.js. Required files for docker image": "I know it has been over 5 months since this question was asked, but I was running in the same issue.\nI solved it by setting up a multistage build in docker and only copy the required files to run a production nextjs app.\nMy Dockerfile\n# Build the app\nFROM node:14-alpine as build\nWORKDIR /app\n\nCOPY . .\nRUN npm ci\nRUN npm run build\nCOPY ./.next ./.next\n\n\n# Run app\nFROM node:14-alpine\n\n# Only copy files required to run the app\nCOPY --from=build /app/.next ./\nCOPY --from=build /app/package.json ./\nCOPY --from=build /app/package-lock.json ./\n\nEXPOSE 3000\n\n# Required for healthcheck defined in docker-compose.yml\n# If you don't have a healthcheck that uses curl, don't install it\nRUN apk --no-cache add curl\n\n# By adding --production npm's devDependencies are not installed\nRUN npm ci --production\nRUN ./node_modules/.bin/next telemetry disable\n\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nextjs -u 1001\n\nUSER nextjs\nCMD [\"npm\", \"start\"]\nAn excerpt from my package.json\n  \"dependencies\": {\n    \"next\": \"^9.5.5\",\n    \"next-compose-plugins\": \"^2.2.0\",\n    \"react\": \"^16.12.0\",\n    \"react-dom\": \"^16.12.0\"\n  },",
    "Permanently change the tomcat port from Dockerfile": "With lots of effort, I found the solution to change the internal port of tomcat container\nmy Dockerfile is\nFROM tomcat:7.0.107\nRUN sed -i 's/port=\"8080\"/port=\"4287\"/' ${CATALINA_HOME}/conf/server.xml\nADD ./tomcat-cas/war/ ${CATALINA_HOME}/webapps/\nCMD [\"catalina.sh\", \"run\"]\nHere ADD ./tomcat-cas/war/ ${CATALINA_HOME}/webapps/ part is not necessary unless you want to initially deploy some war files. And also I don't add EXPOSE 4287, because if I did so, the tomcat server not binding to the port 4287 then it always binding to the 8080 default port.\nJust build the image and run\ndocker build -f Dockerfile -t test/tomcat-test:1.0 .\ndocker run -d -p 4287:4287 --name tomcat-test test/tomcat-test:1.0",
    "pip search finds tensorflow, but pip install does not": "It looks like tensorflow only publishes wheels (and only up to 3.6), and Alpine linux is not manylinux1-compatible due to its use of musl instead of glibc. Because of this, pip cannot find a suitable installation candidate and fails. Your best options are probably to build from source or change your base image.",
    "Django - Mysql Database is not created in Docker": "I'm not sure what you mean by \"I logged in mysql image shell but didn't find mywebsite database\"\nYou are migrated the DB successfully, which means, the DB connections are valid and working.\n\nIn your docker-compose.yml file, the port mapping done like this, '33060:3306', which means the db's port 3306 is mapped to host machine's port 33060. So, this may be the issue (it's not an issue, kind of typo)\n\nHow to check the DB contents?\nMETHOD-1: check through django-shell of web container\n1. run docker-compose up\n2. open a new terminal in the same path and run docker ps\nyou'll get something like below\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                     NAMES\n795093357f78        django_1_11_web     \"python manage.py ru\u2026\"   34 minutes ago      Up 11 minutes       0.0.0.0:8000->8000/tcp    django_1_11_web_1\n4ae48f291e34        mysql:5.7           \"docker-entrypoint.s\u2026\"   34 minutes ago      Up 12 minutes       0.0.0.0:33060->3306/tcp   django_1_11_db_1\n3.Get into the web container by docker exec -it 795093357f78 bash command, where 795093357f78 is the respective container id\n4. now you're inside the container. Then, run the command python manage.py dbshell. Now you will be in MYSQL shell of mywebsite (Screenshot)\n5. run the command show tables;. It will display all the tables inside the mywebsite DB\n\nMETHOD-2: check through db container\n1. repeat the steps 1 and 2 in above section\n2. get into db container by docker exec -it 4ae48f291e34 bash\n3. Now you'll be in bash terminal of MYSQL. Run the following commmand mysql -u root -p and enter the password when prompt\n4. now you're in MYSQL server. run the command, show databases;. This will show all the databases in the server.",
    "docker build is failing when snowflake dependency is included in the pip requirements": "alpine linux is very tiny and they had to remove everything so you will have to install it yourself. Some pip packages require to be compiled/build on your machine but alpine is missing these depedencies, so install them, then do your pip install and then remove them cause they are not required anymore and you will reduce the image size.\nThe snowflake docs also say you have to install libffi-dev and openssl-dev\nFROM python:alpine3.6\n\nCOPY . /sample-app\nWORKDIR /sample-app\n\nRUN apk add --update --no-cache --virtual build-deps gcc python3-dev musl-dev libc-dev linux-headers libxslt-dev libxml2-dev \\\n&& apk add libffi-dev openssl-dev \\\n&& pip install --upgrade pip setuptools \\\n&& pip install -r requirements.txt \\\n&& apk del build-deps\n\nCMD [ \"python\", \"runner.py\" ]",
    "What is the best practice to get the file name that was defined in the pom.xml, from within the docker file?": "Usually you define the <finalName> in your pom file to keep the name static..\nThe default for the final name is defined like this:\n <build>\n    <finalName>${project.artifactId}-${project.version}</finalName>\n    ..\n </build>\nThis means if you release your artifact etc. you have to change the Dockerfile...The simplest solution is to change the definition in your pom ilke this:\n <build>\n    <finalName>${project.artifactId}</finalName>\n    ..\n </build>\nThan you need to change the Dockerfile only if you change your artifactId which usually does not happen very often...\nUpdate\nWhat you could do is to provide arguments to your Dockerfile like:\n#!/bin/bash\nPOM_VERSION=$(mvn -q help:evaluate -Dexpression=project.version -DforceStdout=true)\necho \"POM Version: $POM_VERSION\"\ndocker build --no-cache \\\n    --build-arg APPVERSION=$POM_VERSION \\\n    --rm=true -t user-registration .\nOne word about the line: POM_VERSION=.. Starting with maven-help-plugin version 3.1.0 it is possible to extract things from the pom file like this in particular without any grep/awk vodoo.\nThe Dockerfile can look like this:\n# FROM alpine:3.6 (plus Open JDK?)\nFROM openjdk:8u131-jre-alpine\nARG APPVERSION\nRUN echo \"Building ${APPVERSION}\"\nRUN mkdir /usr/local/service/\nCOPY target/user-registration-${APPVERSION}.jar /usr/local/service/user-registration.jar\n# 8080 application port\n# 8081 admin port.\nEXPOSE 10080 10081\nCMD [\"java\", \"-XX:MaxRAM=128m\", \"-jar\", \"/usr/local/service/user-registration.jar\"]\nThe problem here is simply that CMD does not support ENV,ARGS expanding which means you need to do the copy by using a version as above. You could use the ARG at several points but not at all locations...",
    "Docker Run Script to catch interruption signal": "Yes it is possible to achieve what you want, but before presenting the corresponding code I have to comment on your question's code which contained some issues:\nThe line trap 'echo \"Exiting with a 137 signal.\"' 137 0 9 is incorrect because 137 is not a valid signal number (see for example the Wikipedia article on signals).\nMaybe you just encountered 137 as it is the exit code corresponding to the signal 9 (given that 137 = 128 + 9, see this appendix in the bash doc.)\n0 (EXIT) and 9 (KILL) are valid signal numbers, but in practice it is better to only trap 2 (INT) and 15 (TERM), as suggested in this SE/Unix answer.\nIndeed while the INT and TERM signals can be used for \"graceful termination\", the KILL signal means the process must be killed immediately and as mentioned in man trap:\nSetting a trap for SIGKILL or SIGSTOP produces undefined results. [\u2026] Trapping SIGKILL or SIGSTOP is syntactically accepted by some historical implementations, but it has no effect. Portable POSIX applications cannot attempt to trap these signals.\nSetting the trap at the end of the entrypoint script is a bad strategy, as it is useless in this place. Instead, I suggest that you define a cleanup function (the last instruction of which being exit), then set a trap on this function at the beginning of the script, and run your (non-terminating) application afterwards.\nHence the following proof-of-concept:\nDockerfile\nFROM debian:latest\nWORKDIR /app\n\nCOPY entrypoint.bash ./\nENTRYPOINT [\"/bin/bash\", \"./entrypoint.bash\"]\nentrypoint.bash\n#!/bin/bash\n\ncleanup() {\n    echo \"Cleaning up...\"\n    exit\n}\n\ntrap cleanup INT TERM\n\nwhile :; do\n    echo \"Hello! ${SECONDS} secs elapsed...\"\n    sleep 1s\ndone\nTo test it, you just need to run:\n$ docker build -t test-trap .\n$ docker run -d --name=TEST-TRAP test-trap\n  # wait a few seconds\n$ docker stop TEST-TRAP\n$ docker logs -f TEST-TRAP\nHello! 0 secs elapsed...\nHello! 1 secs elapsed...\nHello! 2 secs elapsed...\nHello! 3 secs elapsed...\nCleaning up...",
    "Unable to connect outside database from Docker container App": "Container's default network is \"bridge\",you should choose macvlan or host network.\nmethod 1\ndocker run -d --net host image\nthis container will share your host IP address and will be able to access your database.\nmethod 2\nUse docker network create command to create a macvlan network,refrence here\nthen create your container by\ndocker run -d --net YOURNETWORK image\nThe container will have an IP address which is the same gateway with its host.",
    "Moving node.js to a docker container, receiving a cannot file module error": "During your image build, you install some things into /usr/src/app, and then you set it as the working directory. Here are the lines from Dockerfile where that happens:\nRUN mkdir -p /usr/src\nRUN mkdir -p /usr/src/app && cp -a /tmp/node_modules /usr/src/app && cp -a /tmp/package-lock.json /usr/src/app\nRUN cp -a /tmp/dist/application /usr/src/app && cp -a /tmp/dist/config /usr/src/app && cp -a /tmp/dist/domain /usr/src/app && cp -a /tmp/dist/infrastructure /usr/src/app\nRUN cp -a /tmp/dist/routes /usr/src/app && cp -a /tmp/dist/types /usr/src/app && cp -a /tmp/dist/webapp.js /usr/src/app && cp -a /tmp/package.json /usr/src/app\nRUN chmod 755 /usr/src/app/webapp.js\nRUN mkdir -p /usr/src/app/bin\n\nCOPY Account/bin /usr/src/app/bin\nCOPY Account/.env /usr/src/app/.env\n\nWORKDIR /usr/src/app\nAfter the build phase, all of that is baked into your image and ready to be used. But at runtime, you told docker-compose:\nvolumes:\n  - ./Account:/usr/src/app/\nThis is an overlay mount. Whatever is built into the image at /usr/src/app is completely ignored and replaced by the contents of ./Account from the directory where docker-compose.yml is located.\nI don't know enough about your project to tell you how to properly fix this, but that's where your error is probably coming from. All the work done during build to construct /usr/src/app is being undone by mounting another directory on top of it at runtime.\nIf you remove that volume mount, all of /usr/src/app is still there and ready to use. But that may have other side effects that you will need to account for to make your app do its job.",
    "couldn't start Celery with docker-compose": "First of all the celery image is deprecated in favour of standard python image more info here.\nWORKDIR sets the working directory for all the command after it is defined in the Dockerfile, which means the command which you are try to run will run from that directory. Docker image for celery sets the working directory to /home/user.\nSince your code is mounted on /celery_smaple and the working directory is /home/user, Celery is not able to find your python module.\nOne alternative is to cd into the mounted directory and execute the command:\ncelery:\n    image: celery:3.1.25\n    command: \"cd /celery_sample && celery worker -A my_celery -l INFO\"\n    volumes:\n      - .:/celery_sample\n    networks:\n      - webnet\nnotice the command\nAnd another one is to create your own image with WORKDIR set to /celery_sample eg:\nFROM python:3.5\nRUN pip install celery==3.1.25\nWORKDIR /celery_sample\nafter building you own image you can use the compose file by changing the image of celery service\nEdit\nYou need to link the services to one another in order to communicate:\nversion: \"3\"\nservices:\n  web:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    command: \"python my_celery.py\"\n    ports:\n      - \"8000:8000\"\n    networks:\n      - webnet\n    volumes:\n      - .:/celery_sample\n    links:\n      - redis\n\n  redis:\n    image: redis\n    networks:\n      - webnet\n\n  celery:\n    image: celery:3.1.25\n    command: \"celery worker -A my_celery -l INFO\"\n    volumes:\n      - .:/home/user\n    networks:\n      - webnet\n    links:\n      - redis\n\nnetworks:\n  webnet:\nand your configuration file should be:\n## Broker settings.\nBROKER_URL = 'redis://redis:6379/0'    \n## Using the database to store task state and results.\nCELERY_RESULT_BACKEND = 'redis://redis:6379/0'\nonce you have linked the services in compose file you can access the service by using the service name as the hostname.",
    "Docker cannot connect application to MySQL": "docker-compose will by default create virtual network were all the containers/services in the compose file can reach each other by an IP address. By using links, depends_on or network aliases they can reach each other by host name. In your case the host name is the service name, but this can be overridden. (see: docs)\nYour script in my_common_package container/service should then connect to mysql on port 3306 according to your setup. (not localhost on port 3306)\nAlso note that using expose is only necessary if the Dockerfile for the service don't have an EXPOSE statement. The standard mysql image already does this.\nIf you want to map a container port to localhost you need to use ports, but only do this if it's necessary.\nservices:\n   mysql:\n     image: mysql:5.6\n     container_name: test_mysql_container\n     environment:\n       - MYSQL_ROOT_PASSWORD=test\n       - MYSQL_DATABASE=My_Database\n       - MYSQL_USER=my_user\n       - MYSQL_PASSWORD=my_password\n     volumes:\n       - db_data:/var/lib/mysql\n     ports:\n       - \"3306:3306\"\nHere we are saying that port 3306 in the mysql container should be mapped to localhost on port 3306.\nNow you can connect to mysql using localhost:3306 outside of docker. For example you can try to run your testsql.py locally (NOT in a container).\nContainer to container communication will always happen using the host name of each container. Think of containers as virtual machines.\nYou can even find the network docker-compose created using docker network list:\n1b1a54630639        myproject_default             bridge              local\n82498fd930bb        bridge                        bridge              local\n.. then use docker network inspect <id> to look at the details.\nAssigned IP addresses to containers can be pretty random, so the only viable way for container to container communication is using hostnames.",
    "How to _directly_ COPY files to \"Program Files (x86)\" folder in a Windows container?": "You have to use double backslashes inside the braces\nFROM microsoft/windowsservercore\n\nCOPY [\"test.txt\", \"c:\\\\program files\\\\WindowsPowerShell\\\\Modules\\\\test.txt\"]\nIt is interpreted as JSON.",
    "Docker unable to start an interactive shell if the image has an entry script": "When you set and entry point in a docker container. It is the only thing it will run. It's the one and only process that matters (PID 1). Once your entry_point.sh script finishes running and returns and exit code, docker thinks the container has done what it needed to do and exits, since the only process inside it exits.\nIf you want to launch a shell inside the container, you can modify your entry point script like so:\n#!/bin/bash\n\necho \"UPDATING GIT REPO\";\n\ncd /home/tool/cloned_github_tools_root\ngit pull\ngit submodule init\ngit submodule update\n\necho \"Entrypoint ended\";\n\n/bin/bash \"$@\"\nThis starts a shell after the repo update has been done. The container will now exit when the user quits the shell.\nThe -i and -t flags will make sure the session gives you an stdin/stdout and will allocate a psuedo-tty for you, but they will not automatically run bash for you. Some containers don't even have bash in them.",
    "How will a Docker application with ubuntu as base image work on Windows?": "Now to run this hello-world application, Is docker going to install the whole ubuntu to run the application?\nNo, the ubuntu image used is not \"the whole ubuntu\". It is a trimed-down version, without the all X11 graphic layer. Still 180 MB though: see \"Docker Base Image OS Size Comparison\".\nThese days, you would rather use an Alpine image (5 MB): see \"Docker Official Images are Moving to Alpine Linux\"\nRegarding the hello-world application specifically, there is no Ubuntu or Alpine involved. Just 1.8 KB of C machine-code, which makes only direct calls to the Linux kernel of the host.\nThat Linux host is used by docker container through system calls: see \"What is meant by shared kernel in Docker?\"\nOn Windows, said Linux host was provided by VirtualBox VM running a boot2docker VM, built from a TinyCore distro.\nWith the more recent \"Docker for Windows\", that same VM is run through the Hyper-V Windows feature.",
    "Passing Tomcat parameters to Docker": "The typical method for docker containers is passing via environment variables.\nExpanding on a solution to pass the port via command line the server.xml needs to be modified so it takes in properties from JAVA_OPTS\nFor example in server.xml\n<GlobalNamingResources>\n    <Resource Name=\"jdbc/Addresses\"\n        auth=\"Container\"\n        type=\"javax.sql.Datasource\"\n        username=\"auser\"\n        password=\"Secret\"\n        driverClassName=\"com.mysql.jdbc.Driver\"\n        description=\"Global Address Database\"\n        url=\"${jdbc.url}\" />\n</GlobalNamingResources>\nThen you can pass value of ${jdbc.url} from properties on the command line.\nJAVA_OPTS=\"-Djdbc.url=jdbc:mysql:mysqlhost:3306/\"\nWhen running the docker image you use the -e flag to set this environment variable at run time\n$ docker run -it -e \"JAVA_OPTS=-Djdbc.url=jdbc:mysql:mysqlhost:3306/\" --rm myjavadockerimage /opt/tomcat/bin/deploy-and-run.sh\nOptionally also add a --add-host if you need to map mysqlhost to a specific ip address.",
    "Subversion export/checkout in Dockerfile without printing the password on screen": "The Dockerfile RUN command is always executed and cached when the docker image is build so the variables that svn needs to authenticate must be provided at build time. You can move the svn export call when the docker run is executed in order to avoid this kind of problems. In order to do that you can create a bash script and declare it as a docker entrypoint and pass environment variables for username and password. Example\n# base image\nFROM ubuntu\n\nENV REPOSITORY_URL http://subversion.myserver.com/path/to/directory\n\n# install subversion client\nRUN apt-get -y update && apt-get install -y subversion\n\n# make it executable before you add it here otherwise docker will coplain\nADD docker-entrypoint.sh /enrypoint.sh\n\nENTRYPOINT /entrypoint.sh\ndocker-entrypoint.sh\n#!/bin/bash\n\n# maybe here some validation that variables $REPO_USER $REPO_PASSOWRD exists.\n\n\nsvn export --username=\"$REMOTE_USER\" --password=\"$REMOTE_PASSWORD\" \"$REPOSITORY_URL\"\n\n# continue execution\npath/to/file.sh\nRun your image:\ndocker run -e REPO_USER=jane -e REPO_PASSWORD=secret your/image\nOr you can put the variables in a file:\n.svn-credentials\nREPO_USER=jane\nREPO_PASSWORD=secret\nThen run:\ndocker run --env-file .svn-credentials your/image\nRemove the .svn-credentials file when your done.",
    "How to copy files from shared directory in multiple Dockerfile?": "You could define a container dedicated to keep your script in a volume (as a data volume container)\nscripts:\n  volumes:\n    - /path/to/scripts\napplication1:\n  volumes_from:\n    - scripts\napplication2:\n  volumes_from:\n    - scripts\nThe /path/to/scripts folder will be shared in each application.\nThe scripts Dockerfile should create /path/to/scripts and COPY the script.sh in it.",
    "Use wget instead of curl for healthchecks in ASP.NET Core docker images": "It's possible to specify a healthcheck via the docker run CLI, or in a docker-compose.yml. I prefer to do it in the Dockerfile.\nConfigure\nFirst note that the ASP.NET Core docker images by default expose port 80, not 5000 (so the docs linked in the question are incorrect).\nThis is the typical way using curl, for a non-Alpine image:\nHEALTHCHECK --start-period=30s --interval=5m \\\n  CMD curl --fail http://localhost:80/healthz || exit 1\nBut curl is unavailable in an Alpine image. Instead of installing it, use wget:\nHEALTHCHECK --start-period=30s --interval=5m \\\n  CMD wget --spider --tries=1 --no-verbose http://localhost:80/healthz || exit 1\nHEALTHCHECK switches documented here.\nwget switches documented here. --spider prevents the download of the page (similar to an HTTP HEAD), --tries=1 allows docker to control the retry logic, --no-verbose (instead of --quiet) ensures errors are logged by docker so you'll know what went wrong.\nTest\nFor full status:\n$ docker inspect --format '{{json .State.Health }}' MY_CONTAINER_NAME | jq\nOr:\n$ docker inspect --format '{{json .State.Health }}' MY_CONTAINER_NAME | jq '.Status'\n# \"healthy\"\n\n$ docker inspect --format '{{json .State.Health }}' MY_CONTAINER_NAME | jq '.Log[].Output'\n# \"Connecting to localhost:80 (127.0.0.1:80)\\nremote file exists\\n\"",
    "executable file not found in $PATH: unknown": "Two things: Make sure the file is marked as executable. And since /mydir isn't in your path, you need to tell Docker to look for the script in the current directory by adding ./ in front of the name.\nFROM mcr.microsoft.com/powershell:ubuntu-focal\nRUN apt-get -qq -y update && \\\n    apt-get -qq -y upgrade && \\\n    apt-get -qq -y install curl ca-certificates python3-pip exiftool mkvtoolnix\nRUN pip3 install gallery-dl yt-dlp\nWORKDIR /mydir\nCOPY gallery-dl.ps1 .\nRUN chmod +x gallery-dl.ps1\nENTRYPOINT [\"./gallery-dl.ps1\"]",
    "Can\u2019t install Choclatey into windows docker container": "I managed to fix this frustrating problem today, here is what I did for when the next person has this issue, as Docker is not going to fix it any time soon.\nWhat I did, is in the desktop app on Windows / Mac you can edit the Daemon file. Under Settings in the Docker App under Docker Engine, I added the line at the bottom of the file just above the last curly brace. \"dns\": [ \"Your DNS Address Here\", \"8.8.8.8\" ]\nThis then allows the Docker Containers all that you now build to use your host's DNS server. Technically if you can access: https://chocolatey.org/install.ps1 then you should be able to access the choco repository.\nI have also built the image in https://github.com/jasric89/vsts-agent-docker/tree/master/windows/servercore/10.0.14393 and labeled it in the repo:\nmicrosoft/windowsservercore:10.0.14393.1358\nI then set: RUN choco feature enable --name allowGlobalConfirmation before my first Choco Install command, this enables choco to install all the files and not error.\nWith all that set my Docker File Ran and built the image. Well in my Test Env now testing in my prod env. :)\nLinks that helped me:\nhttps://github.com/moby/moby/issues/24928 https://github.com/jasric89/vsts-agent-docker/blob/master/windows/servercore/10.0.14393/standard/VS2017/Dockerfile https://docs.chocolatey.org/en-us/troubleshooting https://github.com/moby/moby/issues/25537\nUPDATE:\nI ran into this problem again on a fresh build and after following my own instructions it didn't quite work.\nI realised after a couple more hours of testing that Docker on Windows uses the Hyper-V Network setup.\nWithin my Hyper V Switch Manager, I did not have a network, with Internet Access. Also when I tried to change the default switch it would not let me. Therefore I had to create a new network within the Hyper-V Network.\nI then had to edit the docker Daemon file, in the Docker Settings to tell it to use the right network, and also I put in the DNS settings I specified before in my Anwser.\nHere is my full docker daemon file:\n{\n  \"registry-mirrors\": [],\n  \"insecure-registries\": [],\n  \"bridge\": \"Internet\",\n  \"dns\": [\n    \"YOUR Local DNS Address Here\",\n    \"8.8.8.8\"\n  ],\n  \"debug\": false,\n  \"experimental\": false\n}\nRun code snippetExpand snippet",
    "Reuse user in multi-stage Dockerfile": "TL;DR;\nIt is not possible to re-use the same user in multiple stages of the docker build without re-creating the user (same UID and GID at least) in each stage as each FROM is starting from a clean slate FROM image in which a user UID=42000 and GID=42000 is unlikely to already exist.\nI am not aware of any recommendation against building as the root user inside a container. It is recommended to run services as unprivileged users however certain containers processes must be run as the root user (i.e. sshd):\nThe best way to prevent privilege-escalation attacks from within a container is to configure your container\u2019s applications to run as unprivileged users. For containers whose processes must run as the root user within the container, you can re-map this user to a less-privileged user on the Docker host. The mapped user is assigned a range of UIDs which function within the namespace as normal UIDs from 0 to 65536, but have no privileges on the host machine itself.\nTip: The Haskell Dockerfile Linter will complain if the last user is root which you can configure as a git pre-commit hook to catch things like that before committing teh codez.",
    "docker-compose - Containers have no internet during build process": "my problem was solved by adding network: host to build section.\nstill no idea why my docker bridge network is not working.\nversion: '3.7'\nservices:\n  admin-panel:\n    network_mode: host\n    container_name: react-admin\n    build:\n      context: Admin-Panel\n      # the line below fixed it\n      network: host\n    volumes:\n      - ./Admin-Panel:/app\n      - /app/node_modules\n    ports:\n      - '5000:5000'\n    stdin_open: true\n    tty: true",
    "Ubuntu and Docker: Error response from daemon: error while creating mount source path": "It was just a permissions issue. I moved the source directory to /home/myuser/directory/ and worked.",
    "Dockerfile using variable built using Shell Command": "ARG is your friend.\nDockerfile ARG\nDocker compose args\nDockerfile\nARG VER=latest\nFROM ubuntu:${VER}\nThe above dockerfile defines a build argument named VER, which is default to latest.\ndocker-compose.yml\nversion: '3'\nservices:\n  demo:\n    image: enix223/demo\n    build:\n      context: .\n      args:\n        VER: ${VERSION}\nWe substitute image build arg VER with environment variable VERSION.\nStart container using shell env variable\nVERSION=\"$(cat /etc/lsb-release | grep -o 'DISTRIB_RELEASE.*' |  cut -f2- -d=)\" docker-compose up -d\nStart container using .env file\ncat > .env << EOF\nVERSION=\"$(cat /etc/lsb-release | grep -o 'DISTRIB_RELEASE.*' |  cut -f2- -d=)\"\nEOF\n\ndocker-compose up -d",
    "I need to create a kafka image with topics already created": "I had to do it too! What if I did not want to use wurstmeister images? I decided to make a custom script which will do the job, and run this script in a separate container.\nRepository\nhttps://github.com/yan-khonski-it/kafka-compose\nNote, it will work with kafka versions that use zookeeper. Is Zookeeper a must for Kafka?\nTo start kafka with all your topics and zookeeper - docker-compose up -d.\nImplementation details.\ndocker-compose.yml\n# These services are kafka related. This docker-compose allows to start kafka locally quickly.\n\nversion: '2.1'\n\nnetworks:\n  demo-network:\n    name: demo-network\n    driver: bridge\n\nservices:\n  zookeeper:\n    image: \"confluentinc/cp-zookeeper:${CONFLUENT_PLATFORM_VERSION}\"\n    container_name: zookeeper\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 32181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - 32181:32181\n    hostname: zookeeper\n    networks:\n      - demo-network\n\n  kafka:\n    image: \"confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION}\"\n    container_name: kafka\n    hostname: kafka\n    ports:\n      - 9092:9092\n      - 29092:29092\n\n    environment:\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n      KAFKA_BROKER_ID: 1\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_HOST://kafka:29092\n      LISTENERS: PLAINTEXT://0.0.0.0:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    depends_on:\n      - \"zookeeper\"\n    networks:\n      - demo-network\n\n  # Automatically creates required kafka topics if they were not created.\n  kafka-topics-creator:\n    build:\n      context: kafka-topic-creator\n      dockerfile: Dockerfile\n    container_name: kafka-topics-creator\n    depends_on:\n      - zookeeper\n      - kafka\n    environment:\n      ZOOKEEPER_HOSTS: \"zookeeper:32181\"\n      KAFKA_TOPICS: \"topic_v1 topic_v2\"\n    networks:\n      - demo-network\nThen I have a directory kafka-topics-creator. Here, I have three files create-kafka-topics.sh, Dockerfile, README.md.\nDockerfile\n# It is recommened to use same version as kafka broker is used.\n# So no additional images are pulled.\nFROM confluentinc/cp-kafka:4.1.2\n\nWORKDIR usr/bin\n\n# Once it is executed, this container is not needed.\nCOPY create-kafka-topics.sh create-kafka-topics.sh\nENTRYPOINT [\"./create-kafka-topics.sh\"]\ncreate-kafka-topics.sh\n#!/bin/bash\n\n# Simply wait until original kafka container and zookeeper are started.\nsleep 15.0s\n\n# Parse string of kafka topics into an array\n# https://stackoverflow.com/a/10586169/4587961\nkafkatopicsArrayString=\"$KAFKA_TOPICS\"\nIFS=' ' read -r -a kafkaTopicsArray <<< \"$kafkatopicsArrayString\"\n\n# A separate variable for zookeeper hosts.\nzookeeperHostsValue=$ZOOKEEPER_HOSTS\n\n# Create kafka topic for each topic item from split array of topics.\nfor newTopic in \"${kafkaTopicsArray[@]}\"; do\n  # https://kafka.apache.org/quickstart\n  kafka-topics --create --topic \"$newTopic\" --partitions 1 --replication-factor 1 --if-not-exists --zookeeper \"$zookeeperHostsValue\"\ndone\nREADME.md - so other people know how to use it.Always document your stuff - good advise.\n# Creates kafka topics automatically.\n\n## Parameters\n`ZOOKEEPER_HOSTS` - zookeeper hosts,  I used value `\"zookeeper:32181\"` to run it locally.\n\n`KAFKA_TOPICS` - space separated list of kafka topics. Example, `topic_1, topic_2, topic_3`.\n\nNote, this container should run only **after** your original kafka broker and zookeeper are running.\nAfter this container creates topics, it is not needed anymore.\nHow to check that the topics were created.\nOne solution is to check logs of kafka-topics-creator container.\ndocker logs kafka-topics-creator should print\n$ docker logs kafka-topics-creator\nWARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\nCreated topic \"topic_v1\".\nWARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\nCreated topic \"topic_v2\".",
    "--pid=host To Set through DockerFile": "You can put pid: \"host\" in your docker-compose.yml file to make it work. It's documented here.\npid: \"host\"\nSets the PID mode to the host PID mode. This turns on sharing between container and the host operating system the PID address space. Containers launched with this flag can access and manipulate other containers in the bare-metal machine\u2019s namespace and vice versa.",
    "How can i decrease Spring Boot application memory usage in Docker?": "You need to pass something like -m 50m to limit memory available for the container along with -Xmx and Xms for JVM.\nFollowing article explains it well.\nJVM Memory Allocation in Docker Container",
    "cannot execute RUN mkdir in a Dockerfile with space in directory name": "Came across same problem. None of the answers worked for me. I finally got it working by escaping space with `\nRUN mkdir \"C:\\Program` Files\\Microsoft` Passport` RPS\"\nCOPY . \"C:\\Program` Files\\Microsoft` Passport` RPS\"\nAnother approach is to use Shell, and declare escape explictly\nWhile the JSON form is unambiguous and does not use the un-necessary cmd.exe, it does require more verbosity through double-quoting and escaping. The alternate mechanism is to use the SHELL instruction and the shell form, making a more natural syntax for Windows users, especially when combined with the escape parser directive\n# escape=`\n\nFROM microsoft/nanoserver\nSHELL [\"powershell\",\"-command\"]\nRUN New-Item -ItemType Directory C:\\Example\nADD Execute-MyCmdlet.ps1 c:\\example\\\nRUN c:\\example\\Execute-MyCmdlet -sample 'hello world'",
    "How to run `git clone` in docker file?": "You can pass credentials as arguments to container. This should work\nFROM alpine:3.8\n\nRUN apk update && apk upgrade && \\\n    apk add --no-cache bash git openssh\n\nARG username\nARG password\n\nRUN git clone https://${username}:${password}@github.com/username/repository.git\n\nENTRYPOINT [\"sleep 10\"]\nbut it might be unsafe if you want to distribute that image\nthen build\ndocker build \\\n    --no-cache \\\n    -t git-app:latest \\\n    --build-arg username=user \\\n    --build-arg password=qwerty \\\n    .",
    "How to bind docker container ports to the host using helm charts": "EXPOSE informs Docker that the container listens on the specified network ports at runtime but does not actually make ports accessible. only -p as you already mentioned will do that:\ndocker run -p :$HOSTPORT:$CONTAINERPORT\nOr you can opt for a docker-compose file, extra file but also do the thing for you:\nversion: \"2\"\nservices:\n  my_service:\n    build: .\n    name: my_container_name\n    ports:\n      - 80:8080\n    .....\nEdit:\nIf you are using helm you have just to use the exposed docker port as your targetPort :\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ template \"fullname\" . }}\n  labels:\n    chart: \"{{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\"\nspec:\n  type: {{ .Values.service.type }}\n  ports:\n  - port: {{ .Values.service.externalPort }}\n    targetPort: {{ .Values.service.internalPort }} #8080\n    protocol: TCP\n    name: {{ .Values.service.name }}\n  selector:\n    app: {{ template \"fullname\" . }}",
    "How to set system-wide environment variables in dockerfile from a file inside the docker container?": "When you run this command:\ndocker run -it xyz:latest echo $SDKTARGETSYSROOT\nThe $SDKTARGETSYSROOT is expanded by your local shell. In no case were you actually testing environment variables inside of your container. This would work:\ndocker run -it xyz:latest sh -c 'echo $SDKTARGETSYSROOT'\nThe single quotes inhibit variable expansion in your local shell, and we need to explicitly call sh -c ... because Docker does not, by default, execute commands using a shell. Compare:\n$ docker run -it alpine echo $HOME\n/home/lars\nWith:\n$ docker run -it alpine echo '$HOME'\n$HOME\nWith:\n$ docker run -it alpine sh -c 'echo $HOME'\n/root\nOtherwise, several of your approaches look like a reasonable way to do what you want.",
    "Does Docker execute the entrypoint when a container is used in a multistage build?": "From official documentation:\nOnly the last ENTRYPOINT instruction in the Dockerfile will have an effect.",
    "Docker: basic example dockerfile to run html file": "To server a html file on http you will need a web server, so to do this all you need to do is get an docker image of httpd server, put your html file in the root directory of the webserver and expose the service on some port(suppose 8080), Let's do it one by one.\n1.) Create a docker file with this content\nFROM httpd:2.4\n\nCOPY ./public-html/ /usr/local/apache2/htdocs/\n2.)\ndocker build -t my-apache2 .\n3.)\ndocker run -dit -p 8080:80 --name my-running-app my-apache2 \nThat's it. Your html page should be now available at http://yourip:8080/public-html",
    "Docker-compose error: invalid reference format: repository name must be lowercase": "It turns out that by repository it meant \u2018service\u2019. I updated service name as show bellow and it works.\n services:\n      Pricing.api: => pricing.api with lowercase \u2018p\u2019\n        environment:\n          - ASPNETCORE_ENVIRONMENT=Development\n        ports:\n          - \"80\"\nIt should have said\ninvalid reference format. Service name must be lowercase .\nThis is a confusing error message. This is something Docker team has to fix imo.",
    "Node App Can't Read File System in Docker Image": "The challenge is identifying the expected location of node script execution and the actual working directory in docker. This difference will account for discrepancies between what you expect the file path of \".\" to be and what it actually is.\nThe easiest way to determine if you are executing with the context you think you are is to console out the Fs.realpathSync(\".\") I would expect that the location is not where you thought you were executing from (but it should match with what your WORKDIR is set to in your Docker image). (You can also prove this is your problem by changing your \".\" paths to absolute ones temporarily.)\nChange your working directory to point to where you expect your \".\" to be (in your case WORKDIR /src) and you should be able to use the \".\" the way you expect.",
    "Docker Compose: Accessing my webapp from the browser": "Fairly new here as well, but did you publish your ports on your docker-compose file?\nYour Dockerfile will simply expose the ports but not open access to your host. Publishing (with the -p flag on a docker run command or ports on a docker-compose file. Will enable access from outside the container (see ports in the docs) Something like this may help:\nports:\n  - \"8080:8080\"",
    "How to use STOPSIGNAL instruction within Docker?": "A SIGKILL is a signal which stops the process immediately, without letting the process exit cleanly, so SIGKILL will not allow Tomcat to shutdown gracefully and remove it's PID file.\nSIGTERM and SIGINT both tell Tomcat to run the shutdown hook (deleting the PID file) and shutting down gracefully.\nSIGTERM is equivalent to running kill <pid> and is also the default for docker.\nSIGINT is equivalent to pressing ctrl-C.",
    "How to set environment variables via env-file": "I think\n docker run\ntakes all parameters before the image and the command. If I do\ndocker run -t --env-file=env.list ubuntu sh -c \"while true; do echo world; sleep 100 ;done\"\nand then\ndocker exec -it container_id env\nI get\nHOSTNAME=195f18677a91\nTERM=xterm\nACCOUNT_ID=my_account_id\nACCOUNT_PASSWORD=my_secret_password\nHOME=/root\nTry\ndocker run -p 8080:8080 --env-file=~/env.list -t myname/myapplication",
    "Installing libraries in ubuntu image using Docker": "You need to run apt-get update e.g:\nRUN apt-get update && apt-get install -y sl\nYou can also tidy up after yourself to save a bit of disk space:\nRUN apt-get update && apt-get install -y sl && rm -r /var/lib/apt/lists/*",
    "Docker's heredoc example for RUN is not working": "You need to use the buildkit toolkit for that:\n$ DOCKER_BUILDKIT=1 docker build .",
    "Caching Go Depencencies in Docker": "I think you can just start doing that and see. For me, if I had to use this, I would choose a different mount path, just to isolate the local environment.\nRUN --mount=type=cache,target=/Users/me/Library/Caches go mod download\nI don't see any problem mounting downloaded packages. In this example, it is used for apt.\nHowever, I think we can also take a step back and consider different approaches to not to be bothered by waiting for Docker build.\nWe can have scheduled builds (nightly build and etc), I don't see any reason to do local docker build that frequent, especially your dependencies shouldn't change that often. Maybe I am wrong, but I don't think we need to.\nWe can also further break down go mod download and leverage Docker build cache E.g.\nRUN go mod download package-never-change\nRUN go mod download package-changes-frequently\nEven\nRUN go build ./package1\nRUN go build ./package2\nIt maybe look a little bit tedious, but under certain circumstances it could be useful, for example, when BuildKit is not supported.",
    "nestjs Docker build error: can not find tsconfig.build.json": "In your last step, you never copy over the tsconfig.build.json file or the tsconfig.json. Though, I don't see why you're using start:dev when you've already built the server in the docker image. You should just be calling node dist/main",
    "Create SQL Server docker image with restored backup database using purely a Dockerfile": "If you know where the data directory is in the image, and the image does not declare that directory as a VOLUME, then you can use a multi-stage build for this. The first stage would set up the data directory as you show. The second stage would copy the populated data directory from the first stage but not the backup file. This trick might depend on the two stages running identical builds of the underlying software.\nFor SQL Server, the Docker Hub page and GitHub repo are both tricky to find, and surprisingly neither talks to the issue of data storage (as @HansKillian notes in a comment, you would almost always want to store the database data in some sort of volume). The GitHub repo does include a Helm chart built around a Kubernetes StatefulSet and from that we can discover that a data directory would be mounted on /var/opt/mssql.\nSo I might write a multi-stage build like so:\n# Put common setup steps in an initial stage\nFROM mcr.microsoft.com/mssql/server:2019-latest AS setup\nENV MSSQL_PID=Developer\nENV SA_PASSWORD=Password1?  # (weak password, easily extracted with `docker inspect`)\nENV ACCEPT_EULA=Y           # (legally probably the end user needs to accept this not the image builder)\n\n# Have a stage specifically to populate the data directory\nFROM setup AS data\n# (copy-and-pasted from the question)\nUSER mssql\nCOPY rmsdev.bak /  # not under /var/opt/mssql\nRUN ( /opt/mssql/bin/sqlservr & ) | grep -q \"Service Broker manager has started\" \\\n    && /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P $SA_PASSWORD -Q 'RESTORE DATABASE rmsdev FROM DISK = \"/rmsdev.bak\" WITH MOVE \"rmsdev\" to \"/var/opt/mssql/data/rmsdev.mdf\", MOVE \"rmsdev_Log\" to \"/var/opt/mssql/data/rmsdev_log.ldf\", NOUNLOAD, STATS = 5' \\\n    && pkill sqlservr\n\n# Final stage that actually will actually be run.\nFROM setup\n# Copy the prepopulated data tree, but not the backup file\nCOPY --from=data /var/opt/mssql /var/opt/mssql\n# Use the default USER, CMD, etc. from the base SQL Server image\nThe standard Docker Hub open-source database images like mysql and postgres generally declare a VOLUME in their Dockerfile for the database data, which forces the data to be stored in a volume. The important thing this means is that you can't set up data in the image like this; you have to populate the data externally, and then copy the data tree outside of the Docker image system.",
    "Dockerfile Healthcheck with environment variable": "Just use ${PORT} - no workarounds needed\nTo reference an environment variable in a Dockerfile, just use wrap it inside a ${\u2026} and for then the runtime value is used e.g. for the healthcheck.\nYou can also set a default like this ${PORT:-80} I don't remember where I saw/read this. But it works :)\nSo my Dockerfile looks like this\nFROM node:lts-alpine\nRUN apk add --update curl\n\nENV NODE_ENV production\nENV HOST '0.0.0.0'\nEXPOSE ${PORT:-80}\n\n# Other steps\n\nHEALTHCHECK --interval=5m --timeout=3s \\\n  CMD curl -f http://localhost:${PORT}/health || exit 1\n\nCMD [\"node\", \"server.js\"]",
    "Docker run Error: /bin/sh: 1: python: not found": "Since you are only install python3 inside the docker image as shown here\nRUN apt-get update && apt-get install -y python3 python3-pip\nSo you will need to run python3 instead of python in this line: CMD python .main.py\nAnd you have a typo in the script name. It should be main.py instead of .main.py. Or it should be ./main.py\nSo change it to CMD python3 ./main.py\nAnd if you still have error, you probably need to add this line in the Dockerfile above line of EXPOSE 5000:\nWORKDIR /opt/MyApp-test",
    "Docker: Cannot launch .Net Core 3.1 Web Api on browser after docker run": "In my case it was due to having SSL enabled inthe project, but after removing everything related to https it started working as expected.\nHaving SSL enabled implies installing a SSL certificate inside the docker container and enabling it, things I wasn't able to do as I don't have a certificate and I'm just learning docker.\nSteps I've done:\nIn Startup.cs I've disabled https redirection\n//app.UseHttpsRedirection();\nAnd in project properties under \"Debug\" section I unchecked \"Enable SSL\".\nThen I was able to run the container as expected.",
    "ENV, RUN produce layers, images or containers? (understanding docs question)": "The Docker as containerization system based on two main concepts of image and container. The major difference between them is the top writable layer. When you generate a new container the new writable layer will be put above the last image layer. This layer is often called the container layer.\nAll the underlying image content remains unchanged and each change in the running container that creating new files, modifying existing files and so on will be copied in this thin writable layer.\nIn this case, Docker only stores the actual containers data and one image instance, which decreases storage usage and simplifies the underlying workflow. I would compare it with static and dynamic linking in C language, so Docker uses dynamic linking.\nThe image is a combination of layers. Each layer is only a set of differences from the layer before it.\nThe documentation says:\nOnly the instructions RUN, COPY, ADD create layers. Other instructions create temporary intermediate images, and do not increase the size of the build.\nThe description here is neither really clear nor accurate, and generally speaking these aren't the only instructions that create layers in the latest versions of Docker, as the documentation outlines.\nFor example, by default WORKDIR creates a given path if it does not exist and change directory to it. If the new path was created WORKDIR will generate a new layer.\nBy the way, ENV doesn't lead to layer creation. The data will be stored permanently in image and container config and there is no easy way to get rid of it. Basically, there are two options, how to organize workflow:\nTemporal environment variables, they will be available until the end of the current RUN directive:\nRUN export NAME='megatron' && echo $NAME # 'megatron'\nRUN echo $NAME # blank\nClean environment variable, if there is no difference for you between the absence of env or blank content of it, then you could do:\nENV NAME='megatron'\n# some instructions\nENV NAME=''\nRUN echo $NAME\nIn the context of Docker, there is no distinction between commands and instructions. For RUN any commands that don't change filesystem content won't trigger permanent layers creation. Consider the following Dockerfile:\nFROM alpine:latest\nRUN echo \"Hello World\" # no layer\nRUN touch file.txt # new layer\nWORKDIR /no/existing/path # new layer\nIn the end, the output would be:\nStep 1/4 : FROM alpine:latest\n ---> 965ea09ff2eb\nStep 2/4 : RUN echo \"Hello World\"\n ---> Running in 451adb70f017\nHello World\nRemoving intermediate container 451adb70f017\n ---> 816ccbd1e8aa\nStep 3/4 : RUN touch file.txt\n ---> Running in 9edc6afdd1e5\nRemoving intermediate container 9edc6afdd1e5\n ---> ea0040ec0312\nStep 4/4 : WORKDIR /no/existing/path\n ---> Running in ec0feaf6710d\nRemoving intermediate container ec0feaf6710d\n ---> f2fe46478f7c\nSuccessfully built f2fe46478f7c\nSuccessfully tagged envtest:lastest\nThere is inspect command for inspecting Docker objects:\ndocker inspect --format='{{json .RootFS.Layers}}' <image_id>\nWhich shows us the list of SHA of three layers getting FROM, second RUN and WORKDIR directives, I would recommend using dive for exploring each layer in a docker image.\nSo why does it say removing intermediate container and not removing intermediate layer? Actually to execute RUN commands Docker needs to instantiate a container with the intermediate image up to that line of the Dockerfile and run the actual command. It will then \"commit\" the state of the container as a new intermediate image and continue the building process.",
    "Why does docker have to create an image from a dockerfile then create a container from the image instead of creating a container from a Dockerfile?": "the Dockerfile is the recipe to create an image\nthe image is a virtual filesystem\nthe container is the a running process on a host machine\nYou don't want every host to build its own image based on the recipe. It's easier for some hosts to just download an image and work with that.\nCreating an image can be very expensive. I have complicated Dockerfiles that may take hours to build, may download 50 GB of data, yet still only create a 200 MB image that I can send to different hosts.\nSpinning up a container from an existing image is very cheap.\nIf all you had was the Dockerfile in order to spin up image-containers, the entire workflow would become very cumbersome.",
    "How run jboss-cli on start docker container with Dockerfile": "This works for me:\n   RUN /bin/sh -c '$JBOSS_HOME/bin/standalone.sh -c=standalone-full.xml &' && \\\n      sleep 10 && \\\n      cd /tmp && \\\n      $JBOSS_HOME/bin/jboss-cli.sh --connect --command=\"module add --name=org.postgresql --resources=$JBOSS_HOME/standalone/configuration/postgresql-42.2.5.jar --dependencies=javax.api,javax.transaction.api,javax.servlet.api\" && \\\n      $JBOSS_HOME/bin/jboss-cli.sh --connect --command=:shutdown \n\n\n    # User root to modify war owners\n    USER root\n\n    CMD [\"/opt/eap/bin/standalone.sh\", \"-c\", \"standalone-full.xml\", \"-b\", \"0.0.0.0\",\"-bmanagement\",\"0.0.0.0\"] ",
    "Include .env file in `go build` command": "You cannot include non-go files in the go build process. The Go tool doesn't support \"embedding\" arbitrary files into the final executable.\nYou should use go build to build your executable then, any non-go files, e.g. templates, images, config files, need to be made available to that executable. That is; the executable needs to know where the non-go files are on the filesystem of the host machine on which the go program is running, and then open and read them as needed. So forget about embeding .env into main, instead copy .env together with main to the same location from which you want to run main.\nThen the issue with your dockerfile is the fact that the target host only copies the final executable file from go-compile (COPY --from=go-compile /app/main /app/main), it doesn't copy any other files that are present in the go-compile image and therefore your main app cannot access .env since they are not on the same host.\nAs pointed out in the comments by @mh-cbon, there do exist 3rd-party solutions for embedding non-go files into the go binary, one of which is gobuffalo/packr.",
    "Receiving pull access denied error while trying to run docker-compose.yml file": "You have an extra 'b' in rabbitmq image name, it should be rabbitmq:latest and not rabbbitmq:latest.",
    "The command returned a non-zero code: 127": "This is a PATH related issue and profile. When you use sh -c or bash -c the profile files are not loaded. But when you use bash -lc it means load the profile and also execute the command. Now your profile may have the necessary path setup to run this command.\nEdit-1\nSo the issue with the original answer was that it cannot work. When we had\nENTRYPOINT [\"/bin/bash\", \"-lc\", \"ocp-indent\"]\nCMD [\"--help\"]\nIt finally translates to /bin/bash -lc ocp-indent --help while for it to work we need /bin/bash -lc \"ocp-indent --help\". This cannot be done by directly by using command in entrypoint. So we need to make a new entrypoint.sh file\n#!/bin/sh -l\nocp-indent \"$@\"\nMake sure to chmod +x entrypoint.sh on host. And update the Dockerfile to below\nFROM ocaml/opam\n\nWORKDIR /workdir\n\nRUN opam init --auto-setup\nRUN opam install --yes ocp-indent\nSHELL [\"/bin/sh\", \"-lc\"]\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD [\"--help\"]\nAfter build and run it works\n$ docker run f76dda33092a\nNAME\n       ocp-indent - Automatic indentation of OCaml source files\n\nSYNOPSIS\nOriginal answer\nYou can easily test the difference between both using below commands\ndocker run -it --entrypoint \"/bin/sh\" <image id> env\ndocker run -it --entrypoint \"/bin/sh -l\" <image id> env\ndocker run -it --entrypoint \"/bin/bash\" <image id> env\ndocker run -it --entrypoint \"/bin/bash -l\" <image id> env\nNow either you bash has correct path by default or it will only come when you use the -l flag. In that case you can change the default shell of your docker image to below\nFROM ocaml/opam\n\nWORKDIR /workdir\n\nRUN opam init --auto-setup\nRUN opam install --yes ocp-indent\nSHELL [\"/bin/bash\", \"-lc\"]\nRUN ocp-indent --help\n\nENTRYPOINT [\"/bin/bash\", \"-lc\", \"ocp-indent\"]\nCMD [\"--help\"]",
    "Dockerfile ADD doesn't work": "Your source directory is correct \u2014 '.'. Your destination dir may lead to issues (~/var). I would specify the absolute path of the home dir intended e.g.\n/home//var. Furthermore, you can inspect the Docker image at the layer the build broke \u2014 in your case:\ndocker run --rm -it 64f1a5a8e039 bash  \nFurther details here: How can I inspect the file system of a failed `docker build`?",
    "Stop Solr gracefully when it running in docker": "When running $SOLR_HOME/bin/solr stop, the kernel sends a SIGQUIT signal to the process. To duplicate this in docker, you need to run docker kill --signal=\"SIGQUIT\" <ContainerName> rather than docker stop. Docker Stop sends a SIGTERM signal by default.",
    "Can't npm install dependencies when building docker image": "It's a problem of nodejs installation which was covered here: what are the differences between node.js and node?\nBreifly, there are three options to fix this: creating symlink yourself, using nvm, or installing nodejs-legacy instead of nodejs:\nRUN apt-get -y install nodejs-legacy",
    "boot2docker / docker \"Error. image library/.:latest not found\"": "You should run docker build first (which actually uses your Dockerfile):\ndocker build --tag=imagename .\nOr\ndocker build --tag=imagename -f yourDockerfile .\nThen you would use that image tag to docker run it:\ndocker run imagename",
    "How can i include volume data in docker image": "Depending on the nature of the data, you might also see it as the part of the image. In such case you can carry these files with the image. You could structure this as a base image and an image for a particular build, which would be built on a host that has access to the files needed (e.g. CI build node):\nFROM mybase\nADD <source of installation> /usr/local/data\nADD <source of the home data> /var/local/data\nThis new image (possibly versioned per build) would be pulled with the contents of the /usr/local/data and /var/local/data onto the target environment.\nWhen running on environment (production) you might still use the data container technique if needed:\ndocker run --it -v /usr/local/data -v /var/local/data --name my_app_data_container <my_repo>/<my_app>:<build> /bin/false\ndocker run -d --volumes-from my_app_data_container --name my_app_daemon <my_repo>/<my_app>:<build>",
    "How does the \"RUN\" instruction actually work in a Dockerfile": "The RUN statement executes commands in a container running the current image layer with the result becoming the next layer.\nConsider these two scenarios...\nDockerfile\nFROM alpine:3\nRUN apk add vim\ndocker run\n% docker run -it --rm alpine:3\n/ # apk add vim\nBoth do exactly the same thing but the first commits the change to the next image layer. The second is ephemeral only.",
    "How to install node in .NET 6 docker container": "publish stage doesn't have base stage. Try install node in build stage\ntry adding RUN apt-get... and RUN curl -sL de... after \"as build\" stage\n#base stage\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\n...\n\n#build stage\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\n\nRUN apt-get update -yq && apt-get upgrade -yq && apt-get install -yq curl git nano\nRUN curl -sL https://deb.nodesource.com/setup_16.x | bash - && apt-get install -yq nodejs build-essential\n\n\nWORKDIR /src\n\n...",
    "How to copy contents of folder to /app via dockerfile?": "The Dockerfile documentation for the COPY directive notes that it has two forms, a space-separated form and a JSON-array form, and it notes\nThis latter [JSON array] form is required for paths containing whitespace.\nSo applying that to your specific path, you would get\nCOPY [\"src/Shared Settings\", \"/app\"]\nThis is broadly true in a Dockerfile in general: the only quoting in native Dockerfile syntax is to write things in a JSON array. Wrapping a WORKDIR or a COPY command in single quotes isn't documented as having an effect. A string-form RUN or CMD is an apparent exception to this, but only because these commands are run via a shell and there the shell's quoting rules apply.",
    "How to specify and use a cert file during a docker build": "I finally figured it out.\nAll works with this -\nFROM alpine:3.14.1\n\nCOPY trusted-certs.pem /root/trusted-certs.pem\nRUN SSL_CERT_FILE=~/trusted-certs.pem apk add ca-certificates\nRUN update-ca-certificates\nRUN apk update && apk upgrade\nRUN apk add curl\nRUN curl https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip\nEven cleaner :\nFROM alpine\n\nCOPY ./trusted-certs.pem /usr/local/share/ca-certificates/\nRUN cat /usr/local/share/ca-certificates/trusted-certs.pem >> /etc/ssl/certs/ca-certificates.crt\n\nRUN apk update && apk add --no-cache jq\nRUN apk add curl\nRUN curl https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip",
    "How to use \"bind mount\" in Docker": "The binding -v \"/WorkSpace/d/data-volumes-03-adj-node-code:/app\" ruins everything! It mounts contents from host directory on /app path of the container, so all modules installed during building of the docker image will be lost.\nWhat can be done? Install node modules somewhere else and change default node path to it. So your Dockerfile becomes (Removed COPY . . because you want bind mount your project files on the container so it is unnecessary):\nFROM node:14\n \nWORKDIR /modules\n \nCOPY package.json .\n \nRUN npm install \n\nENV NODE_PATH=/modules/node_modules\n \nEXPOSE 80\n\nWORKDIR /app\n \nCMD [\"node\", \"server.js\"]\nAnd to run your docker image:\ndocker run -d -p 3000:80 --name feedback-app -v feedback:/app/feedback -v \"/c/WorkSpace/d/data-volumes-03-adj-node-code:/app\" -v /modules/node_modules feedback-node:volumes",
    "microk8s Kubernetes Service connection refused": "It's due to you are using the ClusterIP which is only accessible internally from CLuster.\nHere you require the Nginx ingress controller to expose the service. Or you can try with the Host IP once to connect with the service.\nYou can also try the command\nkubectl port-forward svc/hello 8080:8080\nonce the port is forwarded you can hit the curl on localhost:8080.\nHowever, for production use-case it's always suggested to use the ingress for managing the cluster traffic.\nIngress basically work as the Proxy it's same as Nginx.\nIngress is configuration object which managed by the Ingress controller. When you enable the ingress you require the Ingress controller in minikube it will be there however for other Cluster on GKE & EKS you have to setup manually.\nHere is good example for the implementation of the ingress in minikube :\nhttps://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/\ningress example\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\nspec:\n  rules:\n    - host: hello-world.info\n      http:\n        paths:\n          - path: /\n            backend:\n              service:\n                name: hello\n                port:\n                  number: 8080",
    "permission denied in Dockerfile": "Solved. the solution was in 2 steps:\nre-install docker.\nchanging owner to node, like this:\nFROM node:10\nRUN mkdir -p /home/node/app && chown -R node:node /home/node/app\nWORKDIR /home/node/app\nCOPY package.json .\nUSER node\nRUN npm install\n...",
    "Getting connection reset when accessing running docker container": "I finally was able to get everything working.\nChanged docker subnet settings, cause somehow subnets created by docker-compose were interfering with host's public interface. Added code below to /etc/docker/daemon.json\n{ \"dns\": [\"10.10.1.222\"], \"default-address-pools\": [ {\"base\":\"10.10.0.0/16\",\"size\":24} ] }\nRestart docker service\nForced docker compose to use default bridge network. In docker-compose.yaml\nnetwork_mode: bridge\nDown containers created by compose, then re-up\nRemoved all networks created by compose\ndocker network rm\nNote: From my question, docker routes were deleted because somehow they were making host unreachable on reboot. Changing docker subnets and removing all other networks except the default solved that problem.\ndocker network ls \n\n\n\nNETWORK ID          NAME                DRIVER              SCOPE\n15d807ad77d4        bridge              bridge              local\n6cd415046a23        host                host                local\n6e9b8dfc500e        none                null                local\nEdit\nAs per @OlivierMehani comment, while using 20.10.0.0/16 subnet worked for me, it might cause problems as 20.10.0.0/16 is public address range. To be safe, use IP ranges 10.0.0.0 - 10.255.255.255, 172.16.0.0 to 172.31.255.255 and 192.168.0.0 to 192.168.255.255. Just make sure the subnet you pick is not being used in your organization especially by the docker host.",
    "How to mount the folder as volume that docker image build creates?": "You are using a Bind Mount which will \"hide\" the content already existing in your image as you describe - /host_path/configs being empty, /docker_container/configs will be empty as well.\nYou can use named Volumes instead which will automatically populate the volume with content already existing in the image and allow you to perform updates as you described:\nservices:\n  backend:\n    # ...\n    #\n    # content of /docker_container/configs from the image\n    # will be copied into backend-volume\n    # and accessible at runtime\n    volumes:\n    - backend-volume:/docker_container/configs\n\nvolumes:\n  backend-volume:\nAs stated in the Volume doc:\nIf you start a container which creates a new volume [...] and the container has files or directories in the directory to be mounted [...] the directory\u2019s contents are copied into the volume",
    "How to Install Node 8.15 on alpine:3.9?": "Why you are installing with NVM when we have nodejs in alpine offical repository? each Docker image should represent a version of nodejs. So I will not suggest NVM in this case also will keep the image small.\nYou can find version alpine-pacakge-nodejs v8.x.\nFROM alpine:3.9\n\nENV METEOR_VERSION=1.8.1\nENV METEOR_ALLOW_SUPERUSER true\nENV NODE_VERSION 8.15\nRUN apk add --no-cache --repository=http://dl-cdn.alpinelinux.org/alpine/v3.8/main/ nodejs=8.14.0-r0 npm \n\nRUN node --version\noutput\nStep 6/6 : RUN node --version\n ---> Running in 9652a49223fa\nv8.14.0",
    "How to run/host: Multiple environments on same machine with docker-compose": "That was really silly on my part. I missed out an important point in the documentation on docker-compose. You need to specify COMPOSE_PROJECT_NAME environment variable, if not specified then it will pick up the folder name where your compose file resides. Just name this environment variable differently for your environment and you are good to go.",
    "How to install scikit-learn, pandas and numpy in a docker image?": "Simply add numpy and scikit-learn to PySEAL's requirements file.\nYour final requirements file should be:\npybind11\ncppimport\njupyter\nnumpy\nscikit-learn\nAnd run build-docker.sh again.",
    "Go build: build output \"api\" already exists and is a directory": "Your default go build is attempting to output the same name as the directory. You could change your build and ENTRYPOINT line to refer to \"go build -o apiserver\".",
    "What is docker node:9-stretch?": "The node 9 build was dropped after this commit https://github.com/nodejs/docker-node/commit/b22fb6c84e3cac83309b083c973649b2bf6b092d. You can find Dockerfile in diff.\nThe node:9-stretch image you can pull build before the commit, and persisted in docker hub. The 9-stretch tag exists in Tags page, as for now https://hub.docker.com/_/node?tab=tags&page=18.",
    "Difference between CMD echo 'Hello world' and CMD [\"echo\", ''Hello world'] in a dockerfile?": "There is not much difference in your simple example, but there will be a difference if you need shell features such as variable substitution e.g. $HOME.\nThis is the shell form. It will invoke a command shell:\nCMD echo 'Hello world'\nThis is the exec form. It does not invoke a command shell:\nCMD [\"/usr/bin/echo\", \"Hello World\"]\nThe exec form is parsed as a JSON array, which means that you should be using double-quotes around words, not single-quotes, and you should give the full path to an executable. The exec form is the preferred format of CMD.",
    "CORS blocking API call from Dockerized React project": "Your node application should support CORS, if you are using express, you should add the following lines in the app.js file\nconst cors = require('cors');\napp.use(cors());\napp.options('*', cors());",
    "Dockerizing Postfix Relay server": "Postfix v3.3.0 added support for container:\nContainer support: Postfix 3.3 will run in the foreground with \"postfix start-fg\".\nIf you are using a lower version, you might need to use supervisord or an infinite loop or an infinite sleep just to stop the container from exiting.",
    "Dockerfile: Inherit environmental variables from shell": "You could define default values with ARG:\nARG build_var=default_value\nENV ENV_VAR=$build_var\nand then override at build time:\ndocker build --build-arg build_var=$HOST_VAR",
    "Mount a local directory as volume in container, from the Dockerfile": "You do not have access to control things like host volume mounts inside the Dockerfile or image build process. Allowing this would allow malicious image creators to make an image that mounts directories on the host without the permission of the admin of that host. A security breach that allowed a popular base image to mount the filesystem could be used to send private data off-site and inject login credentials on countless machines. The only way to mount a volume is at run time at the explicit request of the admin running the container, and to the directory they provide.",
    "Docker: adding rsa keys to image from outside of build context": "What about using a build argument? Do something like this in your Dockerfile:\nARG rsakey\nRUN test -n \"${rsakey}\" && { \\\n      mkdir -p -m 700 /root/.ssh; \\\n      echo \"${rsakey}\" > /root/.ssh/id_rsa; \\\n      chmod 600 /root/.ssh/id_rsa; \\\n    } || :\nThen, when you build the image, use the --build-arg option:\ndocker build -t sshtest --build-arg rsakey=\"$(cat /path/to/id_rsa)\" .\nThis will inject the key into the image at build time without requiring it to live in your build context.",
    "Docker secrets in build-time": "Note: Docker secrets are only available to swarm services, not to standalone containers. To use this feature, consider adapting your container to run as a service with a scale of 1.\nThe answer is no. Secretes are only available in docker swarm. Swarm doesn't build images and it doesn't accept a Dockerfile.",
    "Running Wildfly Swarm with KeyCloak on docker image": "It looks like the Swarm Keycloak Server reads the keycloak*.db in the dir executed java(means user.dir) in default. The swarm process in container doesn't read /opt/keycloak*.db because java runs on /.\nYou can change the data dir with wildfly.swarm.keycloak.server.db sysprop. https://github.com/wildfly-swarm/wildfly-swarm/blob/2017.6.1/fractions/keycloak-server/src/main/java/org/wildfly/swarm/keycloak/server/runtime/KeycloakDatasourceCustomizer.java#L52\nPlease give it a try in Dockerfile;\nENTRYPOINT [\"java\", \"-jar\", \"/opt/login-service-swarm.jar\", \"-Dwildfly.swarm.keycloak.server.db=/opt/keycloak\"]\nOr, you can also use -w option with docker run.\n$ docker run --help\n-w, --workdir string              Working directory inside the container\nThe following command is supposed to work as well.\ndocker run -p 8180:8180 -w /opt login-service-swarm-v1\nP.S.\nI recommend using Volume or Volume Container instead of adding the data files in Dockerfile. https://docs.docker.com/engine/tutorials/dockervolumes/",
    "docker run complains file not found": "The /usr/src/testapp/ directory is not in the PATH environment variable, so /bin/sh complains.\nChange the last line to:\nCMD ./TestAppStart",
    "Docker Elasticsearch Plugin with Request": "We were able to get this working. We just had to add the script to the CMD command at the end of our Dockerfile so it ran after the ElasticSearch startup script.\nIt looks like you can only have one command per file so we had to look at the base Elastic image (ElasticSearch Docker GitHub) and add to it.\nCMD [\"/bin/bash\", \"bin/es-docker\", \"search-guard/run-sgadmin.sh\"]",
    "Using COPY on dockerfile for apache build": "Understand that you do not want to mount the volumes and instead wanted have those files part of the image so that it can be shared. At least assuming this.\nAs per the Docker documentation\nCOPY obeys the following rules:\n- The path must be inside the context of the build; you cannot COPY ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\n- If is a directory, the entire contents of the directory are copied, including filesystem metadata.\nYou might got the the problem by now. In your Dockerfile i.e., COPY statement is the problem since it is referring to absolute path which is not following the 1st rule from the above. So, htdocs should be available in the local directory from which you execute the docker build command.\nThe following changes need to made before building the image:\nHope you might have already created a directory(which you are building image ) and this directory contains Dockerfile, httpd-custom.conf files.\nNow, go into above directory and copy /opt/mw/apache-2.2.31-instance1/htdocs to current directory. So, that htdocs directory can be now part of context ( as mentioned in the docs) while building the image.\nChange the contents of Dockerfile to the following(mainly COPY command):\nFROM httpd:2.2.31\n\nRUN mkdir -p /opt/mw/apache-test/logs\n\nADD ./httpd-custom.conf /usr/local/apache2/conf/httpd.conf\nCOPY htdocs /usr/local/apache2/htdocs\nNow you should be able to build it successfully.\nFor just demo, used a light weight busybox and create a directory in the same context (to simulate your case) and it does as you see below:\n$ more Dockerfile \nFROM busybox\nCOPY data_folder  /opt/data_folder\nCMD [\"ls\", \"/opt/data_folder\"]\n\n$ ls\ndata_folder  Dockerfile\n$ ls data_folder/\ntest.txt\nBuild Image:\n$ sudo docker build  -t dirtest .\nSending build context to Docker daemon 3.584 kB\nStep 1 : FROM busybox\n ---> e02e811dd08f\nStep 2 : COPY data_folder /opt/data_folder\n ---> b6b1a9555825\nRemoving intermediate container b682e0467803\nStep 3 : CMD ls /opt/data_folder\n ---> Running in 3b05f08ceafc\n ---> b73190fc1fd9\nRemoving intermediate container 3b05f08ceafc\nSuccessfully built b73190fc1fd9\nRunning above Image in a Container which shows directory 'data_folder' is copied and showing its contents. In your case, it is htdocs\n$ sudo docker run -it --rm --name testdirtest dirtest\ntest.txt",
    "Docker SIGTERM not being delivered to node.js/coffee app when started with flags": "TL;DR Use a Javascript loader file instead of the coffee executable when you need to use extended node options to avoid the technicalities of signals with forked processes under Docker.\nrequire('coffee-script').register();\nrequire('./whatever.coffee').run();\nThen\nnode --max_old_space_size=384 app.js\nNow, onto the technicalities...\n\nDocker and signals\nThe initial process in a container is PID 1 in the containers namespace. PID 1 (or the init process) is treated as a special case by the kernel with regards to signal handling.\nIf the init process does not install a signal handler, that signal won't be sent to it.\nSignals do not propagate automatically from an init process, the process must manage this.\nSo a docker process is expected to handle signals itself.\n\nCoffeescripts --nodejs option\nAs you have noted, coffee will fork a child node process when it has the --nodejs option to be able to pass the extra options on.\nThis initially presents some odd behaviour outside of docker with signal handling (on osx at least). A SIGINT or SIGTERM will be forwarded onto the child but also kill the parent coffee process immediately, no matter how you handle the signal in your code (which is running in the child).\nA quick example\nprocess.on 'SIGTERM', -> console.log 'SIGTERM'\nprocess.on 'SIGINT', -> console.log 'SIGINT'\n\ncb = -> console.log \"test\"\nsetTimeout cb, 5000\nWhen you run this and ctrl-c, the signal is forwarded on to the child process and handled. The parent process closes immediately though and returns to the shell.\n$ coffee --nodejs --max_old_space_size=384 block_signal_coffee.coffee \n^C\nSIGINT\n$ <5ish second pause> test\nThen the child process with your code continues running in the background for 5 seconds and eventually outputs test.\n\nDocker and coffee --nodejs\nThe main problem is the parent coffee process does not handle any signals in code, so the signals don't arrive and are not forwarded onto the child. This probably requires a change to coffeescript's launcher code to fix.\nThe signal quirk coffee --nodejs presents outside of Docker could also be bad if it happened under Docker. If the main container process (the parent of the fork) exits before your signal handlers have a chance to complete in the child, the container will close around them. This scenario is unlikely to happen if the above problem is fixed by just forwarding signals onto the child.\nAn alternative to using the suggested javascript loader or fixing coffee scripts loader, would be to use an actual init process, like runit or supervisor but that adds another layer of complexity in between docker and your service.",
    "connect to mysql database from docker container": "At first you shouldn't expose mysql 3306 port if you not want to call it from host machine. At second links are deprecated now. You can use network instead. I not sure about compose v.1 but in v.2 all containers in common docker-compose file are in one network (more about networks) and can be resolved by name each other. Example of docker-compose v.2 file:\nversion: '2'\nservices:\n  web:\n    build: .\n    restart: always\n    volumes:\n      - .:/app/\n    ports:\n      - \"8000:8000\"\n      - \"80:80\"\n  mysql1:\n    image: mysql:latest\n    volumes:\n      - \"/var/lib/mysql:/var/lib/mysql\"\n    environment:\n      MYSQL_ROOT_PASSWORD: secretpass\nWith such configuration you can resolve mysql container by name mysql1 inside web container.",
    "Docker - how to add new python dependencies to the existing docker image?": "Once your container is in the right state (scikit-learn is installed, the script is executed), stop it (docker stop) and commit it as a new image.\nSee docker commit in order to commit a container\u2019s file changes or settings into a new image.\nThen you can run that new image (with the same parameters as before), except the container created from that new image will have the previous steps already there.\nBut the other approach is to build your image from the tenserflow udacity Dockerfile.\nFROM gcr.io/tensorflow/tensorflow:latest\nMAINTAINER Vincent Vanhoucke <vanhoucke@google.com>\nRUN pip install scikit-learn\nRUN rm -rf /notebooks/*\nADD *.ipynb /notebooks/\nWORKDIR /notebooks\nCMD [\"/run_jupyter.sh\"]\nThat image, by default, will execute the right command.",
    "How to use curl command to get manifest v2 schema version 2": "I believe the Accept type for a v2 schema is \"application/vnd.docker.distribution.manifest.v2+json\" according to the docker registry code, see:\nhttps://github.com/docker/distribution/blob/f4b6fc8d681c42137b7d2bb544b087462bc34d47/manifest/schema2/manifest.go#L15",
    "Dockerfile and dev/test/prod environment": "Why not copy in your Dockerfile all 3 config files, and then docker run -e config=file1 (or file2 or file3) and use the value of config environment variable to get the required config file .Check the doc http://docs.docker.com/reference/commandline/cli/#run\nYou can also use the --env-file=[]  Read in a line delimited file of environment variables of the docker run command.\nYou can check that environment variable is available with docker run -it -e mytag=abc123 ubuntu:latest env | grep mytag shows mytag=abc123",
    "Docker npm install --production not working": "I suspect your Dockerfile probably has something like COPY . . but you don't set .dockerignore correctly, e.g. you didn't add node_modules to your .dockerignore (check COPY with docker but with exclusion for the further information about .dockerignore)\nI made the same mistake too and it should have nothing do with nodejs14 or nodejs16\nBTW, npm install --only=prod[uction] is npm 6.x format and npm install --production is npm 8.x format. One difference is that in npm 8.x if you set NODE_ENV production npm 8.x will only install dependencies even if you run npm install but for npm6.x npm install --only will ignore NODE_ENV",
    "How to create index.html using dockerfile?": "The file already exists and is owned by root:\nRUN ls -al index.html\n ---> Running in 27f9d0ae6240\nlrwxrwxrwx 1 root root 25 Dec 23 12:08 index.html \n-> ../../doc/HTML/index.html\nremove it and re-create it:\nFROM centos:7\n#update and install nginx section\nRUN yum update -y\nRUN yum install -y epel-release\nRUN yum install -y nginx\nRUN yum install -y vim\n#create path and add index.html\nWORKDIR /usr/share/nginx/html\n\nRUN rm index.html\nRUN touch index.html\n\nEXPOSE 80/tcp\n\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\nDo note that you should, in general, combine multiple RUN commands together.",
    "H2 console not working on Docker (remote connections ('webAllowOthers') are disabled)": "i had the same problem. I found this answer after researching this problem.\nBefore starting jar in the Dockerfile, you can make it start by giving the following arguments. (\"-web -webAllowOthers -tcp -tcpAllowOthers -browser\")\nMy Dockerfile;\nFROM openjdk:8-jdk-alpine\nADD target/*.jar app.jar\nENTRYPOINT [\"java\", \"-jar\", \"/app.jar\", \"-web -webAllowOthers -tcp -tcpAllowOthers -browser\"]\nEXPOSE 8080\nIn this way, I hope there will be no problems.",
    "Can we control case sensitivity of contents in .dockerignore file?": "It seems like the .dockerignore file is case-insensitive on Windows and case-sensitive on Linux (and thus likely MacOS).\nFrom the Docker documentation:\nThe CLI interprets the .dockerignore file as a newline-separated list of patterns similar to the file globs of Unix shells. [...] Matching is done using Go\u2019s filepath.Match rules. A preprocessing step removes leading and trailing whitespace and eliminates . and .. elements using Go\u2019s filepath.Clean.\nIt seems like aforementioned functions make use of Go's glob, which is case-insensitive or case-sensitive depending on your operating system",
    "How to use maven local repository in Multi-stage docker build?": "The short answer is that you cannot. The only way you could use the maven repository during build would be to copy it inside the image in the first stage. But you cannot do that because normally the location of the maven repository is outside your build context. Of course you can change that for your project (place the .m2 folder in your current project) and then this approach might work. I am not recommending this approach, I am merely mentioning it as an option.\nHowever, I think you can solve your issue by following the best practice of not using Docker during development. While Docker is an awesome tool, it does slow down development. The build and push of the image should be delegated to your CI/CD pipeline (Jenkins, Gitlab CI, etc.). During day to day activities it is better and faster to just run your maven builds locally.",
    "Building rust project in docker causes Cargo to get stuck downloading dependencies": "So turns out the issue was with the COPY . . command, as it just copied everything into /, which was then attempted to be compiled (At least that's my belief).",
    "creating a separate docker-compose configuration for production and development": "The exact list would depend on your environment/ops team requirements, but this is what seems to be useful besides ports/existing volumes:\nNetworks\nThe default network might not work for your prod environment. As an example, your ops team might decide to put nginx/php-fpm/mariadb on different networks like in the following example (https://docs.docker.com/compose/networking/#specify-custom-networks) or even use a pre-existing network\nMysql configs\nThey usually reside in a separate dir i.e. /etc/my.cnf and /etc/my.cnf.d. These configs are likely to be different between prod/dev. Can\u2019t see it in your volumes paths\nPhp-fpm7\nHaven\u2019t worked with php-fpm7, but in php-fpm5 it also had a different folder with config files (/etc/php-fpm.conf and /etc/php-fpm.d) that is missing in your volumes. These files are also likely to differ once your handle even a moderate load (you\u2019ll need to configure number of workers/timeouts etc)\nNginx\nSame as for php-fpm, ssl settings/hostnames/domains configurations are likely to be different\nLogging\nThink on what logging driver might fit your needs best. From here:\nDocker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers.\nYou can easily configure it in docker-compose, here's an example bring up a dedicated fluentd container for logging:\nversion: \"3\"\n\nservices:\n  randolog:\n    image: golang\n    command: go run /usr/src/randolog/main.go\n    volumes:\n      - ./randolog/:/usr/src/randolog/\n    logging:\n      driver: fluentd\n      options:\n        fluentd-address: \"localhost:24224\"\n        tag: \"docker.{{.ID}}\"\n\n  fluentd:\n    build:\n      context: ./fluentd/\n    ports:\n      - \"24224:24224\"\n      - \"24224:24224/udp\"",
    "How to use bash profile in dockerfile": "You shouldn't try to edit shell dotfiles like .bash_profile in a Dockerfile. There are many common paths that don't go via a shell (e.g., CMD [\"python\", \"myapp.py\"] won't launch any sort of shell and won't read a .bash_profile). If you need to globally set an environment variable in an image, use the Dockerfile ENV directive.\nFor a Python application, you should just install your application into the image's \"global\" Python using pip install. You don't specifically need a virtual environment; Docker provides a lot of the same isolation capabilities (something you pip install in a Dockerfile won't affect your host system's globally installed packages).\nA typical Python application Dockerfile (copied from https://hub.docker.com/_/python/) might look like\nFROM python:3\nWORKDIR /usr/src/app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\nCMD [\"python\", \"./your-daemon-or-script.py\"]\nOn your last question, source is a vendor extension that only some shells provide; the POSIX standard doesn't require it and the default /bin/sh in Debian and Ubuntu doesn't provide it. In any case since environment variables get reset on every RUN command, RUN source ... (or more portably RUN . ...) is a no-op if nothing else happens in that RUN line.",
    "Bind .m2 file to docker on build stage": "You should mount the content of your project into the docker image and the $HOME/.m2/ into the image instead of copying everything into the image and building a new image..\nThe $PWD is the local directory where your pom.xml file is located and the src directory exists...\ndocker run -it --rm \\\n  -v \"$PWD\":/usr/src/mymaven \\ (1)\n  -v \"$HOME/.m2\":/root/.m2 \\ (2)\n  -v \"$PWD/target:/usr/src/mymaven/target\" \\ (3)\n  -w /usr/src/mymaven \\ (4)\n  maven:3.5-jdk-8-alpine \\ (5)\n  mvn clean package\ndefines the location of your working directory where pom.xml is located.\ndefines the location where you have located your local cache.\ndefines the target directory to map it into the image under the given path\ndefines the working directory.\ndefines the name of the image to be used.\nSo you don't need to create an new image to build your stuff with Maven. Simply run an existing image via the following command:\ndocker run -it --rm \\\n  -v \"$PWD\":/usr/src/mymaven \\\n  -v \"$HOME/.m2\":/root/.m2 \\\n  -v \"$PWD/target:/usr/src/mymaven/target\" \\ \n  -w /usr/src/mymaven \\\n  maven:3.5-jdk-8-alpine mvn clean package",
    "Printing output of shell script running inside a docker container": "Given a Docker image whose tag is $DOCKER_IMAGE:\ndocker container run -it --rm $DOCKER_IMAGE\n-i keeps STDIN open\n-t allocates a pseudo-TTY\n--rm automatically removes the container when it exits\nSee docker container run for all the options.",
    "Putting files in a Docker image": "#Base image\nFROM centos:latest\n\n#Update image\nRUN yum install -y git # and so on\n\nCOPY /home/username/lyve-SET-1.1.4f/ /lyve-SET-1.1.4f/\n\nENTRYPOINT <entrypoint of your image>\nCMD <arguments of your entrypoint>\nNow, if you build this image, everything will be executed except ENTRYPOINT and CMD\nThat means when you will build your image, your local data will be copied into your container. And it will be always in that image.\nNow push this image into registry.\nWhen user will pull this image, that image will contain data you have copied. No worries.\nIf you need to change your data, you need to build again and push.\nNote: In this way, you need to build & push every time you change code\nAnother option:\nIf you do not want to build & push every time you change code, you need to do copy-part in run-time.\nWrite a script that will clone git repository, when user will run docker.\nThar means, use this\nCOPY script.sh /script.sh\nENTRYPOINT [\"./script.sh\"]\nIn this script.sh shell, do you git clone and other things you need to do.\nIn this way, when user will run your image, they will always get the latest code you have in git repository",
    "Running Multiple python processes in Docker": "Docker docs has examples of how to do this. If you're using Python, then using supervisord is a good option.\nFROM ubuntu:latest\nRUN apt-get update && apt-get install -y supervisor\nRUN mkdir -p /var/log/supervisor\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\nCOPY my_first_process my_first_process\nCOPY my_second_process my_second_process\nCMD [\"/usr/bin/supervisord\"]\nThe advantage of this over running a bunch of background processes is you get better job control and processes that exit prematurely will be restarted.",
    "Is it possible to create container in multiple host using a single docker compose file?": "No, docker-compose alone won't achieve this. Managing containers across multiple hosts is generally the job of schedulers. The Docker Ecosystem: Scheduling and Orchestration is an older article, but should give an introduction.\nThe scheduler provided by Docker is called Swarm. Use Compose with Swarm is a good place to start to understand how to use them together.\nThis part of the answer is time limited, but during Mentor Week there are a few free courses you can take to learn more about Swarm. In your case Operations Beginner and Intermediate may be interesting.",
    "can't open fuse device in a docker container when mounting a davfs2 volume": "Thank's, successfully mounted by adding\n--privileged --cap-add=SYS_ADMIN --device /dev/fuse\nin to docker run command.",
    "Docker - how to automatically POST a request on a REST container after launch?": "You can create another container that wait for the the rabbitmq to expose its REST API and then perform the actions you want (Sending the HTTP POST in your case)\nHere is an example of the docker-compose file\nconnect:\n    build: kafka-connect\n    links:\n      - kafka\n    ports:\n      - \"8083:8083\"\n    environment:\n        CONNECT_BOOTSTRAP_SERVERS: kafka:9092\n        CONNECT_GROUP_ID: connect-cluster-A\n\nconnect-init:\n    build:\n        context: .\n    depends_on:\n        - connect\n    links:\n        - connect\n    command: [\"/wait-for-it.sh\", \"-t\", \"300\", \"connect:8083\", \"--\", \"/init.sh\"]\nHere is the Docker file for the init container. You can use this wait-for-it.sh file\nFROM bash\nRUN apk add --no-cache curl\nADD init.sh wait-for-it.sh /\nAnd the init.sh file contains your HTTP POST (You can add other commands to this file if you need)\ncurl -X POST \\\n    http://connect:8083/connectors \\\n    -d '{\"name\":\"my-project\",\"config\":{\"name\":\"my-project\",\"topics\":\"my-topic\",\"tasks.max\":2,\"connector.class\":\"the.package.to.my.connector.class\"}}'",
    "How to give Dockerfile input parameters from docker run command": "This has behaved this way because:\nThe RUN commands in the Dockerfile are executed when the Docker image is built (like almost all Dockerfile instructions) - ie. when you run docker build\nThe docker run command runs when the container is run from the image.\nSo when you run docker run and set the value to \"hifi\", the image already exists which has a directory called \"dx\" in it. The directory creation task has already been performed - updating the environment variable to \"hifi\" won't change it.\nYou cannot set a Dockerfile build variable at run time. The build has already happened.\nIncidentally, you're over-writing the value of the zk variable right before you create the directory. If you did successfully pass \"hifi\" into the docker build, it would be over-written and the folder would always be called \"dx\".",
    "Docker-compose volumes doesn't copy any files": "A volume is mounted in a container. The Dockerfile is used to create the image, and that image is used to make the container. What that means is a RUN ls inside your Dockerfile will show the filesystem before the volume is mounted. If you need these files to be part of the image for your build to complete, they shouldn't be in the volume and you'll need to copy them with the COPY command as you've described. If you simply want evidence that these files are mounted inside your running container, run a\ndocker exec $container_name ls -l /\nWhere $container_name will be something like ${folder_name}_app_1, which you'll see in a docker ps.",
    "Nightmare.js with Docker": "What you can do is to put all your files in a subdirectory, say app/ and in your Dockerfile do:\nADD app/ /usr/src/app/\nof course in your app folder there will be env.sh, package.json, tux.js and the lib directory\nThat way if you need to add more files, you wont have to add them manually in your dockerfile.\nPS: It works with COPY too",
    "Creating a testing infrastructure with Pytest , Selenium Grid and Docker": "I think you're almost there.\nI would add a pytest service to the docker-compose.yml, and remove the volumes_from and external_links from the hub image (so that it looks more like the example from the blog post you linked).\nThe pytest service will run the python code, but it should connect to hub to run selenium. I believe somewhere in the org_QA_folder there is configuration that tries to start up selenium locally. It needs to be reconfigured to use the hub server instead.",
    "How to sync the time of a java application running on docker container?": "Mapping localtime and timezone works perfectly.\nExample:\ndocker run -d -v /var/lib/elasticsearch:/var/lib/elasticsearch -v /etc/localtime:/etc/localtime:ro -v /usr/share/zoneinfo/America/Buenos_Aires:/etc/timezone:ro -p 80:80/tcp -p 9200:9200/tcp -p 514:514/udp petergrace/elk",
    "Error using mount command within Dockerfile": "Is there a way to be able to use mount with overlay while building the container?\nShort answer: No, and there won't be any time soon.\nIt seems the consensus is that any kind of privileged operation during build breaks the image portability contract, as it could potentially modify the host system. If a different system were to then pull and run such an image instead of building it from source, the host would be in an invalid state, from the perspective of the resultant container.\nRemember, docker build works by completing each step/layer in the Dockerfile using (loosely) these actions:\nRun the image of the last step/layer as a new container\nComplete the operations for the current step/layer within the container\nCommit the container (incl. new state) to a new image\nRepeat for any further steps\nTherefore, it's clearly possible for a privileged build operation to break out of the temporary container and touch the host in this scenario. No bueno.\nSo, what do?\nSolution 1 (baked-in mount)\nUPDATE \u2014 2015-10-24: Fail. See Solution 2 below for a working implementation.\nNote: YMMV depending upon Docker version, storage/graph driver, etc. Here's my docker info, for comparison:\nContainers: 12\nImages: 283\nStorage Driver: overlay\n Backing Filesystem: extfs\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 4.1.10-040110-generic\nOperating System: Ubuntu 15.04\nCPUs: 4\nTotal Memory: 7.598 GiB\nName: agthinkpad\nID: F6WH:LNV4:HH66:AHYY:OGNI:OTKN:UALY:RD52:R5L5:ZTGA:FYBT:SWA4\nWARNING: No swap limit support\nWell, I'm surprisingly finding it nigh impossible to bake the mount into an image via docker commit. The /proc filesystem, where a file representation of mount metadata is written by the kernel (more specifically /proc/self/mounts, from within a container), doesn't appear to be persisted by Docker at all. From what I can tell, anyway \u2014 /var/lib/docker/overlay/<container-root-lower>/root/proc is empty, and /var/lib/docker/overlay/<container-root-upper>/upper/proc doesn't exist.\nI thought /proc could be manipulated via a volume, and did indeed find some 2+ year old references to bind mounting /proc:/proc to achieve things that probably nobody should even be attempting (like this, possibly? \ud83d\ude09), but it appears this doesn't work at all anymore. Attemping to bind mount /proc to a host directory or even just make a volume of it is now a fatal error, even with docker run --privileged:\nCode: System error\n\nMessage: \"/var/lib/docker/overlay/c091a331f26bed12f22f19d73b139ab0c5b9971ea24aabbfad9c6482805984c9/merged/proc\"\n cannot be mounted because it is located inside \"/proc\"\n\nFrames:\n---\n0: setupRootfs\nPackage: github.com/opencontainers/runc/libcontainer\nFile: rootfs_linux.go@37\n---\n1: Init\nPackage: github.com/opencontainers/runc/libcontainer.(*linuxStandardInit)\nFile: standard_init_linux.go@52\n---\n2: StartInitialization\nPackage: Error response from daemon: Cannot start container c091a331f26bed12f22f19d73b139ab0c5b9971ea24aabbfad9c6482805984c9: [8] System error: \"/var/lib/docker/overlay/c091a331f26bed12f22f19d73b139ab0c5b9971ea24aabbfad9c6482805984c9/merged/proc\" cannot be mounted because it is located inside \"/proc\"\nIn terms of a method that doesn't require running mount \u2026 inside containers spawned from the image via a startup/entrypoint script, I'm really not sure where to go from this point. Because /proc/self/mounts is managed by the kernel, let alone not writable, this may never be possible. Hopefully I'm overlooking something and someone can point me in the right direction.\nIf you *must* do this during image compilation you can roll your own builder script by doing something like the following: \u2014 Optionally, first create a `Dockerfile` and use the stock builder: FROM mybaseimg|scratch COPY ./a /tmp/a RUN foo \u2026 `docker build -t mynewimg .` \u2014 Write a shell script using a combination of: `CID=$(docker create mynewimg)`, [`docker cp \u2026`](https://docs.docker.com/reference/commandline/cp/), `docker start $CID`, `docker exec|run \u2026 $CID \u2026`, `docker stop $CID`, `docker commit $CID mynewimg`, etc. *(Edit: Preferably, use the [API](https://docs.docker.com/reference/api/remote_api_client_libraries/)!)* When you need to apply a privileged operation, you can escalate the `docker run` command, but `--privileged` is total overkill here. If you *think* you shouldn't need `--privileged` for something you almost certainly *don't*. For an OverlayFS/AuFS mount you will need to initialize `docker run` with `--cap-add=[SYS_ADMIN]`, and for Ubuntu (and possibly other) hosts with `AppArmor`, you also need `--security-opt=[apparmor:unconfined]` (or preferably an AppArmor profile which relaxes the necessary restrictions, instead of `unconfined`). I'm not sure about `SELinux`.\nSolution 2 (runtime mounting)\nhost$uname -a\nLinux agthinkpad 4.1.10-040110-generic #201510030837 SMP Sat Oct 3 12:38:41 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nhost$mkdir /tmp/overlay-test && cd /tmp/overlay-test\n./Dockerfile:\nFROM debian:jessie\n\nRUN apt-get update && apt-get install -y curl jq\n\nWORKDIR /usr/local/sbin\n\n# Locate and fetch the latest version of gosu\nRUN [\"/bin/bash\", \"-c\", \"curl -o ./gosu -sSL \\\"$( \\\n      curl -s https://api.github.com/repos/tianon/gosu/releases/latest \\\n      | jq --raw-output \\\n        '.assets[] | select(.name==\\\"gosu-'$(dpkg --print-architecture)'\\\") | .browser_download_url' \\\n    )\\\" && chmod +x ./gosu\"]\n\nCOPY ./entrypoint.sh ./entrypoint\nRUN chmod +x ./entrypoint\n\n# UPPERDIR and WORKDIR **MUST BE ON THE SAME FILESYSTEM**, so\n#  instead of creating a VOLUME for UPPERDIR we have to create a \n#  parent directory for both UPPERDIR and WORKDIR, and then make\n#  it the VOLUME. \nRUN [\"/bin/bash\", \"-c\", \"mkdir -p /var/overlay-test/{lower,upper/{data,work}} /mnt/overlay-test\"]\nVOLUME /var/overlay-test/upper\n\n# Create a file named FOO in the lower/root branch\nRUN touch /var/overlay-test/lower/FOO\n\nENTRYPOINT [\"entrypoint\"]\n./entrypoint.sh:\n#!/bin/bash\nset -e\n\ncd /var/overlay-test\nmount -t overlay -o lowerdir=lower,upperdir=upper/data,workdir=upper/work overlay /mnt/overlay-test\nchown -R \"$DUID\":\"$DGID\" ./\nchown root: ./upper/work\nchmod 0750 ./upper/work\n\ncd /mnt/overlay-test\nexec gosu \"$DUID\":\"$DGID\" $@\nhost$docker build -t overlay-test ./\nSuccessfully built 582352b90f53\nOK, let's test it!\nNote: Under an Ubuntu 15.04 host I had trouble deleting (overlay whiteout) any files that exist in the lowerdir via the mounted directory inside the container. This bug appears to be the culprit: https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1480411 \u2014 Edit: rm works all of a sudden and I can see the char file in upper/data, so I can only assume this is fixed and I received an updated package.\nhost$docker run -it --name=overlay-test --env=\"DUID=$(id -u)\" --env=\"DGID=$(id -g)\" --cap-add=SYS_ADMIN --security-opt=apparmor:unconfined overlay-test /bin/bash\noverlay-test$id\nuid=1000 gid=1000 groups=1000\noverlay-test$mount | grep '/mnt/overlay-test'\noverlay on /mnt/overlay-test type overlay (rw,relatime,lowerdir=lower,upperdir=upper/data,workdir=upper/work)\noverlay-test$pwd\n/mnt/overlay-test\noverlay-test$ls -Al | sed '/^t/d'\n-rw-r--r-- 1 1000 1000    0 Oct 24 03:54 FOO\noverlay-test$touch BAR\noverlay-test$ls -Al | sed '/^t/d'\n-rw-r--r-- 1 1000 1000    0 Oct 24 04:21 BAR\n-rw-r--r-- 1 1000 1000    0 Oct 24 03:54 FOO\noverlay-test$ls -Al /var/overlay-test/{lower/,upper/*} | sed '/^t/d'\nls: cannot open directory /var/overlay-test/upper/work: Permission denied\n\n/var/overlay-test/lower:\n-rw-r--r-- 1 1000 1000 0 Oct 24 03:54 FOO\n\n/var/overlay-test/upper/data:\n-rw-r--r-- 1 1000 1000 0 Oct 24 04:21 BAR\nSo far so good\u2026let's try importing the volume from another container:\noverlay-test$exit\nhost$docker run --rm --user=\"$(id -u):$(id -g)\" --volumes-from=overlay-test debian:jessie /bin/bash -c \"ls -Al /var/overlay-test/upper/* | sed '/^t/d'\"\nls: cannot open directory /var/overlay-test/upper/work: Permission denied\n\n/var/overlay-test/upper/data:\n-rw-r--r-- 1 1000 1000 0 Oct 24 05:32 BAR\nSuccess! Note that you could also RUN echo the mount spec >> /etc/fstab in your Dockerfile and then mount -a inside the entrypoint script, but this method was quirky, in my experience. I'm not sure why, but as there isn't a functional difference between the two methods, I didn't bother to investigate any further.\nClean-up: host$docker rm -v overlay-test && docker rmi overlay-test\nDocumentation for container runtime security in Docker:\nhttps://docs.docker.com/reference/run/#security-configuration https://docs.docker.com/reference/run/#runtime-privilege-linux-capabilities-and-lxc-configuration",
    "Docker automated build shows empty Dockerfile": "The Dockerfile on the hub gets updated after the build runs. Also, the pending for the build means it just hasn't run yet. Sometimes it takes a while. Do you have your github account Linked in the Settings section of the Docker repository? You can reach the status page to see if anything is broken here: https://status.docker.com/.\nBy the way, I have several projects on Docker hub, I went to one of them just now and clicked 'Build' (this isn't necessary if your github is linked, the auto build does that), and it is stuck in Pending. Sometimes it takes a LONG time.\nThere has been at least one outage at Docker registry in the last 3 months while I was actively using it. This might not be an outage, but, it could be :-( It could also be a big load on Docker hub.",
    "Docker: SvelteKit app cannot be accessed from browser": "Are you sure that sveltekit listens on port 5050?\nBecause when I start it up with npm run dev (vite dev) it usually takes port 5173 and if it is already used it counts 1 up until it reaches a free port.\nAdd in the package.json at the dev command a --port=5050.\nFull string:\n\"dev\": \"vite dev --port=5050\",",
    "\u201cUnable to load DLL 'SQLite.Interop.dll' or one of its dependencies\u201d in Docker, but locally it works": "We have experienced the issue updating from System.Data.SQLite.Core 1.0.113.6 to 1.0.117.\nLocally and in build pipeline everything works fine, but as soon as our app deployed in a docker container running mcr.microsoft.com/dotnet/runtime:6.0-nanoserver-1809 image - application throws the named exception Unable to load DLL 'SQLite.Interop.dll': The specified module could not be found.\nI found this thread at sqlite forum which clarified the situation a bit: https://sqlite.org/forum/info/0c471ca86ae7a836\nQuote from the forum:\nThis actually seems to be caused by some extra dependencies in the latest version of SQLite.Interop.dll.\nUsing dumpbin I compared the dependencies between version 1.0.113.7 and 1.0.114.4 and these new dependencies were added:\nmscoree.dll\n         1801473F0 Import Address Table\n         180186090 Import Name Table\n                 0 time date stamp\n                 0 Index of first forwarder reference\n\n                      6F StrongNameSignatureVerificationEx\n                      71 StrongNameTokenFromAssembly\n                      62 StrongNameFreeBuffer\n                       F CorBindToRuntimeEx\n                      61 StrongNameErrorInfo\n\nWINTRUST.dll\n         1801473E0 Import Address Table\n         180186080 Import Name Table\n                 0 time date stamp\n                 0 Index of first forwarder reference\n\n                      82 WinVerifyTrust \nmscoree.dll is not available in in the mcr.microsoft.com/dotnet/core/runtime image I was using. If I copy it in from a different image, SQLite.Interop.dll is loaded successfully\nWe ended up reverting back to version 1.0.113.6 as we didn't want to introduce extra complexity to docker image. I hope the issue with the latest System.Data.SQLite.Core will be resolved in one of the coming versions.",
    "Why does Docker gets stuck after running npm install?": "I solved it by updating docker and waiting :) . Yes, docker hangs up there but give it time it surely will move on and finish the process.",
    "Docker-compose with podman?": "The upcoming Podman 3.0 supports the Docker REST API well enough to be used as back-end for docker-compose. It is planned to be released in a few weeks (see Podman releases).\nCaveats:\nRunning Podman as root is supported, but not yet running as a normal user, i.e. running \"rootless\". (See feature request)\nFunctionality relating to Swarm is not supported\nTo enable Podman as the back-end for docker-compose, run\n sudo systemctl enable --now podman.socket\nPodman will then listen on the UNIX domain socket /var/run/docker.sock\nSee also: https://www.redhat.com/sysadmin/podman-docker-compose",
    "How to ensure that my docker image / container will still work 100 years from now?": "Well, yes. When you build a docker image it stores locally on your host machine. As your image already includes everything you need. You can store it in private docker registry and it will work even after 100 years",
    "Impact of yum install on Docker layer size": "Each RUN instruction creates new docker layer.\nDocker itself is not that smart enough to detect that instruction actually did nothing.\nIt faithfully stores new docker layer in resulting image.\nThat's why you need to try to minimize amount of docker instructions if possible.\nIn your case you can use just one RUN instructon:\nRUN yum -y install nano which && yum -y clean all && rm -fr /var/cache \nUPDATE\nLet's make an experiment:\nFROM centos\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\nRUN yum -y install which\n10 RUN instructions, 9 of them \"doing nothing\".\nLet's build and look for intermediate images\n$ docker build .\n...\n$ docker images -a\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n<none>              <none>              fbd86aedc782        5 seconds ago       263MB\n<none>              <none>              ca70a4bbe722        7 seconds ago       261MB\n<none>              <none>              bd11e0ab02fb        9 seconds ago       259MB\n<none>              <none>              68c20ddfcaad        11 seconds ago      257MB\n<none>              <none>              314a6501ad23        13 seconds ago      255MB\n<none>              <none>              42a62294a5e7        16 seconds ago      253MB\n<none>              <none>              16fad39b9c27        18 seconds ago      251MB\n<none>              <none>              6769fe69c9e1        19 seconds ago      249MB\n<none>              <none>              49cef483e732        21 seconds ago      248MB\n<none>              <none>              c4c92c39f2a4        23 seconds ago      246MB\ncentos              latest              0d120b6ccaa8        3 weeks ago         215MB\nI see that each next docker image layer for \"doing nothing\" adds ~2Mb. (I don't know about ~24 Mb that was in OP question)\nUPDATE 2\nBy advice from emix: Using dive I immediately found files that was changed with every layer in /var/rpm and /var/log",
    "Docker input file and save in output": "Generally, the docker container cannot break out into the host machine.\nHowever, you can mount a local directory from the host machine into the container. The files created in the mount point, inside the container, will also be visible on the host machine.\nIn the example below I am mounting the working directory from the host machine inside the container. My current directory contains an input-file.\nThe container cats the content of the input-file and appends it to the output-file\n// The initial wiorking directory content\n.\n\u2514\u2500\u2500 input-file\n\n// Run my dummy container and ask it to cat the content of the input file into the output file\ndocker run -v $(pwd):/root/some-path ubuntu /bin/bash -c \"cat /root/some-path/input-file >> /root/some-path/output-file\"\n\n// The outcome\n.\n\u251c\u2500\u2500 input-file\n\u2514\u2500\u2500 output-file",
    "Passing a ENV variable for LABEL in Dockerfile": "LABEL sets image-level metadata, so it will be the same for all containers created from this image.\nIf you want to override a label for a container, you have to specify it using a special --label key:\ndocker run --label GROUP=mygroup nginx:test_label\nCurrently you're passing the environment variable to the container during running. The Dockerfile is processed on build stage. If you want to have this variable substituted in the image, you have to pass it on build:\ndocker build --build-arg GROUP=mygroup -t nginx:test_label .",
    "Hyperledger Indy data is not being mounted in Kubernetes volume directory": "Mounting in docker is consistent with standard behaviour of mounting on Linux. Linux mount command docs say\nThe previous contents (if any) and owner and mode of dir become invisible, and as long as this filesystem remains mounted\nThis is as well the way things work in Docker. If you mount a local directory, or an existing named docker volume, the content of filesystem in the container on the location of the mount will be shadowed (or we can call it \"overriden\").\nSimplified example of what is going on\nHaving dockerfile\nFROM alpine:3.9.6\n\nWORKDIR /home/root/greetings\nRUN echo \"hello world\" > /home/root/greetings/english.txt\nCMD sleep 60000\nAnd build it docker build -t greetings:1.0 .\nNow create following docker-compose.yml:\nversion: '3.7'\n\nservices:\n  greetings:\n    container_name: greetings\n    image: greetings:1.0\n    volumes:\n      - ./empty:/home/root/greetings\nand create empty directory empty next to it.\nStart it docker-compose up -d. While the container is running, let's get into container and see what the filestructure inside looks like. docker exec -ti greetings sh. Now when we are inside, if you run ls /home/root/greetings you'll see that the directory is empty - even though in the Dockerfile we have baked file /home/root/greetings/english.txt into the image's filesystem.\nNamed docker containers behave more desirably, if the named docker container is new and doesn't contain any data. If you mount such container on location in container where there already is some data, the named volume will get this data copied on it.\nYou can try this by adjusting the docker-compose.yml to this\nversion: '3.7'\n\nservices:\n  greetings:\n    container_name: greetings\n    image: greetings:1.0\n    volumes:\n      - greetingsvol:/home/root/greetings\n\nvolumes:\n  greetingsvol:\n    driver: local\nand if you repeat the procedure and exec yourself into the container, you'll see that file /home/root/greetings/english.txt is still there.\nThat's because when you cd yourself into /home/root/greetings, you are not looking at actual container's filesystem, but at mounted device - the name docker volume - which has been initialized by copy of container's original files on that given location. (Assuming docker volume greetingsvol did not previously exist.)\nSolution to your problem\nYou are mounting directory /var/kubeshare on your host to container's /var/lib/indy/sandbox. Let's see what the container stores on that location on startup (indypool is how I named built indy sandbox image on my localhost)\ndocker run --rm indypool ls /var/lib/indy/sandbox\ndomain_transactions_genesis\nkeys\npool_transactions_genesis\nSo if you mount your local directory onto /var/lib/indy/sandbox, it will shadow these files and the pool will fail to start up (and therefore consequently won't create files such as node1_additional_info.json etc).\nSo I think you have 2 options:\nUnless you have a good reason not to, use named docker volumes.\nCopy the original image data from container's /var/lib/indy/sandbox into your /var/kubeshare. Then you keep everything else as was. That way, the directory will be shadowed by new filesystem containing exactly the same data as the container expects to find there.",
    "Git not working in NanoServer (in Docker)": "First, you don't have to run the GIt for Windows setup.\nYou could also try and uncompress the Portable extractable archive PortableGit-2.26.0-64-bit.7z.exe anywhere you want in the image, and then add to the path:\nset GH=C:\\path\\to\\git\nset PATH=%GH%\\bin;%GH%\\usr\\bin;%GH%\\mingw64\\bin;%PATH%",
    "How to pass command line argument from oc start-build to dockerfile to set environment variable inside dockerfile": "You may try automate editing of the YAML manifest file with the BuildConfig (e.g. with the yaml python package) to add entries to the buildArgs array, which is located in the dockerStrategy definition of the BuildConfig. For example:\ndockerStrategy:\n...\n  buildArgs:\n    - name: \"foo\"\n      value: \"bar\"\nRefer to the relevant Openshift docs for more details.",
    "docker build --build-arg SSH_PRIVATE_KEY=\"$(cat ~/.ssh/id_rsa)\" returning empty": "I went ahead and used ONVAULT toool to handle the ssh keys. https://github.com/dockito/vault.\nAlso, I had misconfigured my .ssh/config file. The new file looks like this\nHost *\n  IgnoreUnknown AddKeysToAgent,UseKeychain\n  AddKeysToAgent yes\n  UseKeychain yes\n  IdentityFile ~/.ssh/id_rsa \nI hope it helps someone in future.",
    "Why dotnet command is not found in dockerfile?": "FROM ubuntu:16.04\nFROM microsoft/dotnet:2.2-sdk as build-env\nIn the above lines FROM ubuntu:16.04 will be totally ignored as there should be only one base image, so the last FROM will be considered as a base image which is FROM microsoft/dotnet:2.2-sdk not the ubuntu.\nSo if your base image is FROM microsoft/dotnet:2.2-sdk as build-env then why to bother to run these complex script to install dotnet?\nYou are good to go to check version of dotnet.\nFROM microsoft/dotnet:2.2-sdk as build-env\nRUN dotnet --version\noutput\nStep 1/6 : FROM microsoft/dotnet:2.2-sdk as build-env\n ---> f13ac9d68148\nStep 2/6 : RUN dotnet --version\n ---> Running in f1d34507c7f2\n\n> 2.2.402\n\nRemoving intermediate container f1d34507c7f2\n ---> 7fde8596c331",
    "Extending CouchDB Docker image": "Most of the standard Docker database images include a VOLUME line that prevents creating a derived image with prepopulated data. For the official couchdb image you can see the relevant line in its Dockerfile. Unlike the relational-database images, this image doesn\u2019t have any support for scripts that run at first startup.\nThat means you need to do the initialization from the host or from another container. If you can directly interact with it using its HTTP API, then this could look like:\n# Start the container\ndocker run -d -p 5984:5984 -v ... couchdb\n\n# Wait for it to be up\nfor i in $(seq 20); do\n  if curl -s http://localhost:5984 >/dev/null 2>&1; then\n    break\n  fi\n  sleep 1\ndone\n\n# Create the database\ncurl -XPUT http://localhost:5984/db",
    "Does exposing ports in a Dockerfile base image, expose them in a final image?": "The EXPOSE directive seems to be for documentation purposes mostly (only?).\nFrom the documentation:\nThe EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published\nSo I do not see any reason to declare this anywhere other than the final step / Dockerfile.\nThat said, any EXPOSE directive in a parent image - either multi-stage or not - will be reflected in the metadata of any subsequent child image.\nExample:\n# Dockerfile\nFROM scratch AS base\nEXPOSE 80\n\nFROM base\nENV HELLO world\nThen run:\n$ docker build -t temp .\n$ docker image inspect temp\nWhich will output (among other things):\n\"ExposedPorts\": {\n  \"80/tcp\": {}\n},",
    "WordPress image with pre-installed plugins using Dockerfile": "So this is what's happening:\nWhen you are building your custom image, you add the plugin folder /var/www/html/wp-content/plugins/preferred-languages/ and that works just fine.\nYou can test that by simply running docker run -it --rm arslanliaqat/wordpresswithplugin sh and cd /var/www/html/wp-content/plugins and you should see the folder.\nThe reason the folder is missing when you are using your docker-compose.yml file is because you are mounting the volume \"over\" the folder that's already there. Try removing the volumes declaration from wp service in the docker-compose.yml file and then you should be able to see your plugin folder.\nI would suggest you use the wordpress:php7.1-apache for your wp service and mount your plugin folder the same way you are mounting wordpress\nExample:\nversion: '3.3'\nservices:\n  wp:\n    image: \"wordpress:php7.1-apache\"\n    volumes:\n      - './wordpress:/var/www/html'\n      - './preferred-languages:/var/www/html/wp-content/plugins/preferred-languages'\n    ports:\n      - \"8000:80\"\n    environment:\n      WORDPRESS_DB_PASSWORD: qwerty\n  mysql:\n    image: \"mysql:5.7\"\n    environment:\n      MYSQL_ROOT_PASSWORD: qwerty\n    volumes:  \n      - \"my-datavolume:/var/lib/mysql\"\nvolumes: \n  my-datavolume:\nIs there a specific reason you need the plugin to be in the image already?\nUPDATED\nI created a simple gist which should accomplish what you want to do. The entrypoint lacks checks for already existing theme/plugin directories etc, but this should serve as POC\nhttps://gist.github.com/karlisabe/16c0ccc52bdf34bee5f201ac7a0c45f7",
    "Docker: Pulling an image by digest, that internally uses a tag": "You're correct that an image pulled by digest is effectively (!) unchangeable.\nThe image digest is a SHA-256 hash computed from the layers that constitute the image. As a result it's highly improbable that a different image would share the same digest.\nOnce created an image's layers don't change. So even if the FROM image were changed, your existing images would not be changed by it.\nHowever, if you rebuilt your images using the new (same-tagged) FROM image, your image's digest would change and this would be a signal to you that's something has changed.\nIt is possible (and a good practice) to use digests in FROM statements too (for the reasons you cite) but few developers do this. You may wish to ensure your Dockerfiles use digests in FROM statements to ensure you're always using the same image sources.\nHowever, it's turtles all the way down (or up) though and so you are recursively delegating trust to images from which yours are derived all the way up to SCRATCH.\nThis is one reason why image vulnerability tools are recommended.\nI explored this for my own education recently:\nhttps://medium.com/google-cloud/adventures-w-docker-manifests-78f255d662ff",
    "Alternative to using --squash when building docker images on Windows using local files": "We ended up setting up nginx to provide files when building. On our build server, the machine building our docker images and the server that has the installer files have a very good connection between them, so downloading huge files is not a real problem.\nWhen it comes to --squash, it is bugged for Docker on Windows. Here is the relevant issue for it:\nhttps://github.com/moby/moby/issues/31468\nThere is an issue to move --squash out of experimental, but it doesn't seem to have a lot of support:\nhttps://github.com/moby/moby/issues/38657\nThe alternative that some people propose instead of --squash is multi stage build, discussion here:\nhttps://github.com/moby/moby/issues/34565\nThere is an alternative to --squash, if you have local installer files, you don't want to set up a web server, and you would like your docker image to be small, and you are running Windows: Use mapped drives.\nIn Windows, you can share folders with other users on your network. Docker containers are like another computer that is running on your physical machine, and it can access these network drives.\nFirst set up a new user, for example username share and password password1. Create a folder somewhere on your computer. Then right click it, click properties, and then go to the Sharing tab and click \"Share\". Find the user that you have just created, using the little dropdown menu and Find people ..., and share the folder with this user.\nCreate a folder somewhere for your test project. Create a batch file setupshare.bat that looks like this:\n@echo off\nfor /f \"tokens=2 delims=:\" %%i in ('ipconfig ^| findstr \"Default Gateway\"') do (\n    set hostip=%%i\n    goto :end\n)\n:end\nset hostip=%hostip: =%\nnet use O: \\\\%hostip%\\vms /USER:share password1\nThe first part of this file is only to find the ip address that the docker container can use to access its host computer. It is not the most pretty thing I've ever put together, so let me know if there's a better way!\nIt uses a for-loop, as that is the way to save the output of a command to a variable in batch files. The command is ipconfig, and we pipe it to findstr and searches for Default Gateway. We need to use ^| instead of just | because it is in a for-loop. The first part of the for-loop divides each line from the command on the delimiter, which is : in this case, and we only take the second token. The for-loop only handles the first line, if there are multiple entries with a Default Gateway. This script doesn't work if there are multiple entries and the first one is not the correct one.\nThe line set hostip=%hostip: =% is to remove a space at the start of the string.\nWe then have the IP address that we want to use stored in hostip. We use this in the net use command, which will map O:\\ to shared folder vms on the machine with IP hostip. We use the username share and the password password1. Note that this is a very bad way of handling passwords, as they kind of should be secret!\nWith a batch file like this, we can set up a Dockerfile in this way:\n# escape=`\nFROM mcr.microsoft.com/dotnet/core/sdk:3.0\n\nCOPY setupshare.bat .\n\nRUN setupshare.bat && `\n    copy O:\\file.txt file.txt\nThe RUN command will first call setupshare.bat that sets up the network share properly. We can then use any file that we shared, for example a huge installer, and install the things that we want. In this case I have only shared a test file file.txt to see that it works, so just change that line.\nI would still advice everyone to just set up a little web server, for example nginx, and use the standard way of writing Dockerfiles, with downloading files and running it in the same RUN command. That's what people expect when they see a Dockerfile, and it should be a more robust solution.\nWe can also hope that the Docker people either makes a COPY command that can copy, run, and delete installers in the same layer, or that --squash is implemented properly.",
    "'vc_redist.x64 does not install in microsoft/nanoserver image": "Visual C++ Redistributable Packages cannot be installed in NanoServer. However, you can use the binary dlls manually. The redistributable files are installed with Visual Studio.\nSteps:\nOpen Visual Studio Installer, make sure you check Desktop Development with C++. At right details panel, check all versions you want to install:\nMSVC v143 - VS 2022\nMSVC v142 - VS 2019\nMSVC v141 - VS 2017\nMSVC v140 - VS 2015\nFind your VS installation folder, for example, VS2022 should be like C:\\Program Files\\Microsoft Visual Studio\\2022\\Community and VS2019 should be C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community. Then go to the CRT folder. I have installed VS2022 and want to use MSVC v142, so the full path should be: C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Redist\\MSVC\\14.29.30133\\onecore\\x64\\Microsoft.VC142.CRT\nCopy everything under this folder to your application's local folder, or C:\\Windows\\System32 in your NanoServer image.",
    "Can't recreate Conda environment in docker": "I had a similar problem and I find multiple ways to solve it. The main problem with your approach is conda is not platform independent, so will force the environments to use pip.\n1. Conda Like Solution\nChange your my_env.yml so that all the dependencies apart from pip goes under the pip dependency. Notice that the syntax is different when you move under the pip.\nFor instance:\nname: myenv\nchannels:\n  - defaults\ndependencies:\n   - pip=18.1\n   - pip:\n     - wheel==0.32.3\nThen go to your Dockerfile and add the following line:\nRUN conda env update -n base --file myenv.yml\n2. Good old Pip way\nExport your conda environment into a pip requirements file as at this answer\nconda install pip\npip freeze > requirements.txt\nThen go to your Docker file and add the following line:\nRUN python -m pip install -r requirements.txt",
    "Specify \"privileged\" container mode in dockerfile?": "You can't give privileged mode in Dockerfile. You can only run by --privileged when start docker by command line. There is one other way, that you can try start you docker container via Docker API\nAnd set request param for auto run with privileged mode.\nAs I know, normal case you need to run docker in privileged mode is you wanna run docker in docker. What BIRD container is ?",
    "Docker-compose Error : cannot restrict inter-container communication": "Per WARNING message the bridge-nf-call-iptables is disabled, run code below to address that warning:\nsudo sysctl net.bridge.bridge-nf-call-iptables=1\nsudo sysctl net.bridge.bridge-nf-call-ip6tables=1\nAlso, make sure the br_netfilter module is enabled, to do that run command below and make sure br_netfilter is listed in the linux.kernel_modules:\n lxc profile show docker\nIf it's not listed, copy all values listed in the linux.kernel_modules and add ,br_netfilter to the end of the copied value, than put all together in the command below instead of <[COPIED_LIST]>:\nlxc profile set docker linux.kernel_modules <[COPIED_LIST]>",
    "Is there a way to use If condition in Dockefile?": "Docker 17.05 and later supports a kind of conditionals using multi-stage build and build args. Have a look at https://medium.com/@tonistiigi/advanced-multi-stage-build-patterns-6f741b852fae\nFrom the blog post:\nARG BUILD_VERSION=1\nFROM alpine AS base\nRUN ...\nFROM base AS branch-version-1\nRUN touch version1\nFROM base AS branch-version-2\nRUN touch version2\nFROM branch-version-${BUILD_VERSION} AS after-condition\nFROM after-condition \nRUN ...\nAnd then use docker build --build-arg BUILD_VERSION=value ...",
    "Docker Returned a non-zero code 100": "I encountered the same problem. It solved by adding\nUSER root\nbefore running any Linux commands.\nP.S: Seems to be an old question but I thought it may help somebody if put it as an answer.",
    "How to track file changes within a Docker Container": "Check\ninotify docker image https://github.com/pstauffer/docker-inotify\nor\nhttps://hub.docker.com/r/coppit/inotify-command/\nor\nhttps://hub.docker.com/r/coppit/inotify-command/~/dockerfile/",
    "Unable to create node_modules folder at host and mount host folder to container": "starting fresh..\nThe following is, I believe, what you are asking for, but I'm not sure it's what you want.\nWhat is the actual use case you are trying to address?\nIf it is to be able to develop your app, modifying files on your local system and have them reflected in the docker container this is not the way to do it..\nI have used the npm module serve as an easy way to keep the container running.\nI have used a named volume to make things easier. The volume data is defined in docker-compose and mounted on /var/www/app.\nfyi, you could also try moving the VOLUME instruction in your original Dockerfile to the bottom as at https://docs.docker.com/engine/reference/builder/#volume:\nChanging the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\nBut this still only defines the volume mount point in the container, you still need to mount it on the host at run time.\nApp files:\n$ tree\n.\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 package.json\n\u2514\u2500\u2500 docker-compose.yml\n\n1 directory, 3 files\n\n$ cat app/Dockerfile\nFROM node:6.3\nRUN mkdir -p /var/www/app\nWORKDIR /var/www/app\nCOPY . /var/www/app\nRUN npm install\n\n$ cat app/package.json\n{\n  \"name\": \"app\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"start\": \"serve\"\n  },\n  \"dependencies\": {\n    \"serve\": \"*\"\n  }\n}\n\n$ cat docker-compose.yml\nversion: \"2\"\n\nservices:\n  app:\n    build: ./app\n    ports:\n      - \"3000\"\n    command: npm start\n    volumes:\n      - data:/var/www/app\nvolumes:\n  data:\nBuild the app:\n$ docker-compose build app\nBuilding app\nStep 1/5 : FROM node:6.3\n---> 0d9089853221\nStep 2/5 : RUN mkdir -p /var/www/app\n---> Using cache\n---> 18cc43628367\nStep 3/5 : WORKDIR /var/www/app\n---> Using cache\n---> b8fa2b1a2624\nStep 4/5 : COPY . /var/www/app\n---> de38a6c3f784\nStep 5/5 : RUN npm install\n---> Running in 5a035f687de1\nnpm info it worked if it ends with ok\nnpm info using npm@3.10.3\nnpm info using node@v6.3.1\nnpm info attempt registry request try #1 at 1:19:21 PM\nnpm http request GET https://registry.npmjs.org/serve\n...\nnpm info ok\n---> 76b99c707ac1\nRemoving intermediate container 5a035f687de1\nSuccessfully built 76b99c707ac1\nSuccessfully tagged 47173020_app:latest\nBring up the app\n$ docker-compose up -d app\nRecreating 47173020_app_1 ...\nRecreating 47173020_app_1 ... done\nCheck the app logs\n$ docker-compose logs app\nAttaching to 47173020_app_1\napp_1  | npm info it worked if it ends with ok\napp_1  | npm info using npm@3.10.3\napp_1  | npm info using node@v6.3.1\napp_1  | npm info lifecycle app@0.0.0~prestart: app@0.0.0\napp_1  | npm info lifecycle app@0.0.0~start: app@0.0.0\napp_1  |\napp_1  | > app@0.0.0 start /var/www/app\napp_1  | > serve\napp_1  |\napp_1  | serve: Running on port 5000\nNow the volume you are looking for on the host is on the docker host running as a vm on your local machine and can only be accessed from a running container.\nI have taken the following from Inspecting Docker Volumes on a Mac/Windows the Easy Way which you can refer to for more detail.\nList the docker volumes\n$ docker volume ls\nlocal               47173020_data\nInspect the volume to get it's mount point\n$ docker volume inspect 47173020_data\n[\n    {\n        \"CreatedAt\": \"2017-11-13T13:15:59Z\",\n        \"Driver\": \"local\",\n        \"Labels\": null,\n        \"Mountpoint\": \"/var/lib/docker/volumes/47173020_data/_data\",\n        \"Name\": \"47173020_data\",\n        \"Options\": {},\n        \"Scope\": \"local\"\n    }\n]\nStart up a simple container, mount \"/docker\" on the host root, and list the files in the volume which are the ones copied and created by the Dockerfile\n$ docker run --rm -it -v /:/docker alpine:edge ls -l /docker/var/lib/docker/volumes/47173020_data/_data\ntotal 12\n-rw-r--r--    1 root     root           119 Nov 13 12:30 Dockerfile\ndrwxr-xr-x  153 root     root          4096 Nov 13 13:14 node_modules\n-rw-r--r--    1 root     root           144 Nov 13 13:03 package.json",
    "Why httpd container exits immediately without any error in Docker": "So your Webserver is not startet as a foreground process, that is why the container is stopped immediately.\nI think you should change\nENTRYPOINT [\"apache2ctl\", \"-D\", \"FOREGROUND\"]\nto\nCMD [\"apache2ctl\", \"-D\", \"FOREGROUND\"]\nBecause you want that command to run when the container is mounted.\nThe ENTRYPOINT directive declares the executable which is used to execute the CMD.\nENTRYPOINT [\"apache2ctl\", \"-D\", \"FOREGROUND\"]\nresults in:\n$  apache2ctl -D FOREGROUND <command either from the run command line or the CMD directive>\nThe default command from Ubuntu image I could find is /bin/bash. So when you run the container using:\ndocker run -it mg:httpd\nthe resulting command that is executed will be:\n$  apache2ctl -D FOREGROUND /bin/bash\nwhat obviously does not make much sense. I would recommend to go over that post, it explains the details very well.\nAfter the change described above it will look like that:\n(default) ENTRYPOINT [\"/bin/sh\", \"-c\"]\nCMD [\"apache2ctl\", \"-D\", \"FOREGROUND\"]\nwhat results in:\n$  /bin/sh -c \"apache2ctl -D FOREGROUND\"",
    "Docker secret not working for MYSQL_ROOT_PASSWORD_FILE": "I had the same issue running linux containers on a windows machine when I created the secret inline.\necho mypassword | docker secret create mysql_db_root -\nThe actual password gets saved with a space, carriage return, and new line at the end. This can be seen if you look at the bytes in the file. In the running mysql container you can use the od command like the following example below to view the bytes. Or outside the container you can dump the /run/secrets/mysql_db_root content to a file and view with your favorite hex editor on windows to see the added bytes.\nod -xa /run/secrets/mysql_db_root\nTo resolve the issue create the secret by putting it in a file and then using the file to generate the docker secret. See the mysql github issue for more details.\ndocker secret create mysql_db_root mypasswordfile.txt",
    "Docker: how to use SQL file in Directory": "Just add a volume mapping to map a local folder to the /docker-entrypoint-initdb.d container folder, for example : ./init-db:/docker-entrypoint-initdb.d. This file will be loaded on the first container startup.\nConsidering the docker-compose.yml bellow :\ndrop your sql files into /path-to-sql-files-on-your-host host folder)\nrun docker-compose down -v to destroy containers and volumes\nrun docker-compose up to recreate them.\n-\nversion: '3'\n\nservices:\n   db:\n     image: mysql:5.7\n     volumes:\n       - db_data:/var/lib/mysql\n       - /path-to-sql-files-on-your-host:/docker-entrypoint-initdb.d\n     restart: always\n     environment:\n       MYSQL_ROOT_PASSWORD: somewordpress\n       MYSQL_DATABASE: wordpress\n       MYSQL_USER: wordpress\n       MYSQL_PASSWORD: wordpress\n\n   wordpress:\n     depends_on:\n       - db\n     image: wordpress:latest\n     ports:\n       - \"8000:80\"\n     restart: always\n     environment:\n       WORDPRESS_DB_HOST: db:3306\n       WORDPRESS_DB_USER: wordpress\n       WORDPRESS_DB_PASSWORD: wordpress\nvolumes:\n    db_data:",
    "Docker: Unable to correct problems, you have held broken packages": "Debian is setup to only allow one mail transport agent, and your install command is trying to include two, ssmtp and sendmail/sendmail-bin. Since they conflict with each other, you'll need to remove one of these from your install command.",
    "Copying a directory into a docker image while skipping given sub directories": "I considered the method explained by @nwinkler and added few steps to make the build consistent.\nNow my context directory structure is as follows\n-Dockerfile\n-MyApp\n  -libs\n  -classes\n  -resources\n-.dockerignore\n-libs\nI copied the libs directory to the outer of the MyApp directory. Added a .dockerignore file which contains following line\nMyApp/libs/*\nUpdated the Dockerfile as this\nADD libs /opt/MyApp/libs\n## do other operations\nADD MyApp /opt/MyApp\nBecause dockerignore file ignores MyApp/lib directory, there is no risk in over-writing libs directory I have copied earlier.",
    "Docker-compose failing to run a jar file but works with Dockerfile": "You have two syntax errors in your docker-compose.yml file:\nDocker-Compose does not support the add command. If you want to add a file to your container, you either have to do that in a Dockerfile (and use that from the compose file), or map in the file through a volume.\n  volumes:\n    - \"./farr-api-0.1.0.jar:/app.jar\"\nThe environment section expects an array - you need to prefix the JAVA_OPTS line with a dash:\n  environment:\n    - JAVA_OPTS=\"\"\nYou can find more details in the Docker-Compose documentation",
    "Dockerfile build /bin/sh -c returned a non-zero code: 1": "I solved my problem ! Thanks for all, who tried to help me ! After each command (RUN, CMD and else) Docker makes container, save changes on docker image and delete container before next command.Docker also compress directories and files each command iteration. You should know it before do anything, if you don't want to get an exception or error..\nThis is working code :\nFROM ubuntu:16.04\nMAINTAINER S.K.\nRUN apt-get update\nRUN apt-get install curl -y\nRUN curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.32.1/install.sh | bash \\\n&& export NVM_DIR=\"$HOME/.nvm\" && [ -s \"$NVM_DIR/nvm.sh\" ] && . \"$NVM_DIR/nvm.sh\" \\\n&& nvm install 6.9.1 \\\n&& npm i express -g \\\n&& npm i nunjucks -g \\\n&& npm i nodemon -g \\\n&& npm i gulp -g \\\nRUN mkdir -p ./PROJECT\nEXPOSE 1520",
    "Unable to delete some untagged docker images due to conflict": "Make sure the image is actually dangling (meaning it is not referenced by any other image, or is not parent of an image)\ndocker images --filter \"dangling=true\" -q --no-trunc\nIf it is dangling (and should be removed), then there is a couple of pending bug reporting the impossibility to delete such images: issue 13625, issue 12487.",
    "Using docker-compose Mysql + App": "This is a known issue and is yet to be addresses. Check out this link Docker services need to wait\nYou are absolutely right that the MySql service is not yet up while your App container comes up very quickly and hence when it tries to connect to MySql, the connection is refused. On the application side what you can do is that you could write some retry logic. Something on these lines\nwhile(numberOfRetries > 0) {\n\ntry {\n\n  // Try to connect to MySql\n} catch(Exception e) {\n\n     numberOfRetries--;\n\n     if(numberOfRetries == 0)\n       // log the exception \n}\nHope this helps.",
    "Docker CMD weirdness when ENTRYPOINT is a shell script": "Note that the default entry point/cmd for an official centos 6 image is:\nno entrypoint\nonly CMD [\"/bin/bash\"]\nIf you are using the -c command, you need to pass one argument (which is the full command): \"echo foo\".\nNot a series of arguments (CMD [\"echo\", \"foo\"]).\nAs stated in dockerfile CMD section:\nIf you use the shell form of the CMD, then the <command> will execute in /bin/sh -c:\nFROM ubuntu\nCMD echo \"This is a test.\" | wc -\nIf you want to run your <command> without a shell then you must express the command as a JSON array and give the full path to the executable\nSince echo is a built-in command in the bash and C shells, the shell form here is preferable.",
    "How do I add big HTTP files in a Dockerfile and exclude them from image layers?": "I accepted Mykola Gurovs answer because in one of his comments he pointed out an idea that helped me to solve this issue.\nHere is what I did to have proper caching and cache invalidation as well as having the big installer file excluded:\nFROM debian:jessie\n...\nRUN apt-get install -y curl\nENV PROJECT_VERSION x.y.z-SNAPSHOT\n...\nADD http://nexus:8081/service/local/artifact/maven/resolve?r=public&g=my.group.id&a=installer&v=${PROJECT_VERSION}&e=sh&c=linux64 ${INSTALL_DIR}/installer.xml\nRUN curl --silent \"http://nexus:8081/service/local/artifact/maven/content?r=public&g=my.group.id&a=installer&v=${PROJECT_VERSION}&e=sh&c=linux64\" \\\n        --output ${INSTALL_DIR}/installer.sh \\\n    && sh ${INSTALL_DIR}/installer.sh <someArgs> \\\n    && rm ${INSTALL_DIR}/installer.sh\n...\nThe first ADD downloads the Maven metadata for the requested artifact. That XML file is quite small. It uses proper caching so whenever the metadata on the Nexus has been modified the cache gets invalidated.\nThe ADD and all its following instructions are executed without re-using any cached versions in that case.\nIf the metadata on the server did not change since the last download the ADD and the following RUN instruction which executes curl are taken from the image layer cache. And in the RUN it is possible to download, execute and remove the temporary big installer file in one step without having it stored in any image layers.",
    "Unable to run docker image created from ISO": "You're building an image from ubuntu's iso but what you really want is building an image from an ubuntu install.\nWhat you have to do is :\ninstall ubuntu from the iso inside a VM\nmake an archive of the VM's / (this can be done with cd / && tar -cvpzf backup.tar.gz --exclude=/backup.tar.gz --one-file-system /)\nimport with docker (this can be done with docker import /path/to/your/backup.tar.gz)\nI hope that helps",
    "Docker will not attach to an image": "Run docker ps -a to show all containers, not just running containers. You'll see docker_sa_1 listed as a stopped container. This is because it crashed immediately on start-up. Unfortunately, fig doesn't show the logs for you (or shut down the stack automatically) when this happens.\nRun docker logs docker_sa_1 to see the output. Hopefully, there will be a nice Nginx error message for you. If you can't find anything, then remove the sa entry from your fig.yml, do a fig up to get everything else started, then run\ndocker run -it --link=docker_php_1:php-fpm -v $PWD/svn:(?) -v $PWD/cert:(?) -p 8080:80 nginx\n(You'll need to fill in the ?s with the path bits you left out) This is equivalent to what Fig's doing, except we're starting the container interactive with an attached tty instead of attaching later. If you still can't get any error messages, run\ndocker run -it --link=docker_php_1:php-fpm -v $PWD/svn:(?) -v $PWD/cert:(?) -p 8080:80 nginx /bin/bash\nto get a live shell on the container. Then try starting Nginx yourself and dig for log files after it crashes.",
    "Docker Expose ports dynamically": "With Weave network for Docker any port your application might open would be accessible from inside the network with no external intervention, unlike the aforementioned ambassador patter. However, those would be only accessible from the subnet the application is on. You the ports you statically expose will remain NATed by Docker too and would be externally accessible, but ephemeral once would be internal-only.",
    "Curl error (6) on amazonlinux docker container during `yum install`, but no issues with other image": "I'm betting that you're running on a machine where Docker runs with \"old\" seccomp settings. See this thread for https://github.com/amazonlinux/amazon-linux-2023/issues/80#issuecomment-1017798237 discussion and options.",
    "Docker compose fails with \"failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount[...]/Dockerfile: no such file or directory\"": "I can definitely reproduce this having a an empty php folder, so missing the Dockerfile, with the following minimal example.\nFile hierarchy:\n.\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 php\n##  ^-- mind this is an empty folder, not a file\nAnd the minimal docker-compose.yml:\nversion: \"3.9\"\nservices:\n  php-apache-environment:\n    container_name: php-apache\n    build: ./php\nRunning docker compose up yields the same error as yours:\nfailed to solve: rpc error: code = Unknown desc = failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount2757070869/Dockerfile: no such file or directory\nSo, if you create a Dockerfile in the php folder, e.g.:\n.\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 php\n    \u2514\u2500\u2500 Dockerfile\nWith a content like\nFROM php:fpm\nThen the service starts working:\n$ docker compose up    \n[+] Running 1/0\n \u283f Container php-apache  Created                               0.1s\nAttaching to php-apache\nphp-apache  | [14-Apr-2023 08:42:10] NOTICE: fpm is running, pid 1\nphp-apache  | [14-Apr-2023 08:42:10] NOTICE: ready to handle connections\nAnd if your file describing the image inside the folder php has a different name than the standard one, which is Dockerfile, then you have to adapt your docker-compose.yml, using the object form of the build parameter:\nversion: \"3.9\"\nservices:\n  php-apache-environment:\n    container_name: php-apache\n    build: \n      context: ./php\n      dockerfile: Dockefile.dev # for example\nRelated documentation: https://docs.docker.com/compose/compose-file/build/#build-definition",
    "error: failed to solve: openjdk:13: failed to do request: Head \"https://registry-1.docker.io/v2/library/openjdk/manifests/13\": x509": "This is coming months late and I assume that you have figured it out already but for the sake of someone else that has the same issue, I'll share what worked for me.\nFirst thing I did was to check the name of the container for the buildkit:build-stable-1 image, as this was automatically generated for me:\ndocker ps\nNext, stop the container:\ndocker stop buildx_buildkit_trusting_moore0\nFinally, remove the container:\ndocker rm buildx_buildkit_trusting_moore0\nAfterwards, I started the build process. I did encounter this issue a few times but stopping and removing the container always worked. I am relatively new to docker so I am not sure why this happens, but I hope this helps someone.",
    "Build a image with dotnet core 6 using dockerfile get an error NETSDK1064": "Removing --no-restore from the dotnet publish should do the trick.\nFrom the docs https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-publish:\nYou don't have to run dotnet restore because it's run implicitly by all commands that require a restore to occur, such as dotnet new, dotnet build, dotnet run, dotnet test, dotnet publish, and dotnet pack. To disable implicit restore, use the --no-restore option",
    "Docker build fails due to kafka undefined error": "So somehow this seems to depend on cgo when building. I managed to work around this by compiling with cgo, but linking the dependencies statically.\nFROM golang AS builder\n\nWORKDIR /app\nCOPY . .\nRUN go mod download && \\\n    go build -o myapp -ldflags '-linkmode external -w -extldflags \"-static\"'\n\n\n# works also with alpine\nFROM busybox \n\nCOPY --from=builder /app/myapp myapp\nCMD [\"./myapp\"]\nI have used the code below, from their repo, to test. Which gave me initially the kind of error you have shown.\npackage main\n\nimport (\n    \"fmt\"\n\n    \"github.com/confluentinc/confluent-kafka-go/kafka\"\n)\n\nfunc main() {\n\n    p, err := kafka.NewProducer(&kafka.ConfigMap{\"bootstrap.servers\": \"localhost\"})\n    if err != nil {\n        panic(err)\n    }\n\n    defer p.Close()\n\n    // Delivery report handler for produced messages\n    go func() {\n        for e := range p.Events() {\n            switch ev := e.(type) {\n            case *kafka.Message:\n                if ev.TopicPartition.Error != nil {\n                    fmt.Printf(\"Delivery failed: %v\\n\", ev.TopicPartition)\n                } else {\n                    fmt.Printf(\"Delivered message to %v\\n\", ev.TopicPartition)\n                }\n            }\n        }\n    }()\n\n    // Produce messages to topic (asynchronously)\n    topic := \"myTopic\"\n    for _, word := range []string{\"Welcome\", \"to\", \"the\", \"Confluent\", \"Kafka\", \"Golang\", \"client\"} {\n        p.Produce(&kafka.Message{\n            TopicPartition: kafka.TopicPartition{Topic: &topic, Partition: kafka.PartitionAny},\n            Value:          []byte(word),\n        }, nil)\n    }\n\n    // Wait for message deliveries before shutting down\n    p.Flush(15 * 1000)\n}",
    "Golang docker file for debug": "You need to compile dlv itself with the static linking flags. Without that, dlv will have dynamic links to libc which doesn't exist within an alpine image. Other options include switching your production image to be debian based (FROM debian) or change to golang image to be alpine based (FROM golang:1.14.7-alpine). To compile dlv without dynamic links, the following Dockerfile works:\nFROM golang:1.14.7 AS builder\nRUN CGO_ENABLED=0 go get -ldflags '-s -w -extldflags -static' github.com/go-delve/delve/cmd/dlv\nRUN mkdir /app\n\nADD . /app\nWORKDIR /app\nRUN CGO_ENABLED=0 GOOS=linux go build -gcflags=\"all=-N -l\" -o main ./...\n\nFROM alpine:3.12.0 AS production\nCOPY --from=builder /app .\nCOPY --from=builder /go/bin/dlv /\nEXPOSE 8000 40000\nENV PORT=8000\nCMD [\"/dlv\", \"--listen=:40000\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"./main\"]    \nTo see the dynamic links, build your builder image and run ldd against the output binaries:\n$ docker build --target builder -t test-63403272 .\n[+] Building 4.6s (11/11) FINISHED                                                                                                                                                                                 \n => [internal] load build definition from Dockerfile                                                                                                                                                          0.0s\n => => transferring dockerfile: 570B                                                                                                                                                                          0.0s\n => [internal] load .dockerignore                                                                                                                                                                             0.0s\n => => transferring context: 2B                                                                                                                                                                               0.0s\n => [internal] load metadata for docker.io/library/golang:1.14.7                                                                                                                                              0.2s\n => [builder 1/6] FROM docker.io/library/golang:1.14.7@sha256:1364cfbbcd1a5f38bdf8c814f02ebbd2170c93933415480480104834341f283e                                                                                0.0s\n => [internal] load build context                                                                                                                                                                             0.0s\n => => transferring context: 591B                                                                                                                                                                             0.0s\n => CACHED [builder 2/6] RUN go get github.com/go-delve/delve/cmd/dlv                                                                                                                                         0.0s\n => CACHED [builder 3/6] RUN mkdir /app                                                                                                                                                                       0.0s\n => [builder 4/6] ADD . /app                                                                                                                                                                                  0.1s\n => [builder 5/6] WORKDIR /app                                                                                                                                                                                0.0s\n => [builder 6/6] RUN CGO_ENABLED=0 GOOS=linux go build -gcflags=\"all=-N -l\" -o main ./...                                                                                                                    4.0s\n => exporting to image                                                                                                                                                                                        0.2s\n => => exporting layers                                                                                                                                                                                       0.2s\n => => writing image sha256:d2ca7bbc0bb6659d0623e1b8a3e1e87819d02d0c7f0a0762cffa02601799c35e                                                                                                                  0.0s\n => => naming to docker.io/library/test-63403272                                                                                                                                                              0.0s\n\n$ docker run -it --rm test-63403272 ldd /go/bin/dlv\n        linux-vdso.so.1 (0x00007ffda66ee000)\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007faa4824d000)\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007faa4808c000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007faa48274000)\nLibc is a common missing library when switching to alpine since it uses musl by default.",
    "docker : how to share ssh-keys between containers?": "For doing ssh without password you to need to create passwordless user along with configuring SSH keys in the container, plus you will also need to add ssh keys in the sources container plus public key should be added in the authorized of the destination container.\nHere is the working Dockerfile\nFROM openjdk:7\n\nRUN apt-get update && \\\n    apt-get install -y openssh-server vim \n\nEXPOSE 22\n\n\nRUN useradd -rm -d /home/nf2/ -s /bin/bash -g root -G sudo -u 1001 ubuntu\nUSER ubuntu\nWORKDIR /home/ubuntu\n\nRUN mkdir -p /home/nf2/.ssh/ && \\\n    chmod 0700 /home/nf2/.ssh  && \\\n    touch /home/nf2/.ssh/authorized_keys && \\\n    chmod 600 /home/nf2/.ssh/authorized_keys\n\nCOPY ssh-keys/ /keys/\nRUN cat /keys/ssh_test.pub >> /home/nf2/.ssh/authorized_keys\n\nUSER root\nENTRYPOINT service ssh start && bash\ndocker-compose will remain same, here is the testing script that you can try.\n#!/bin/bash\nset -e\necho \"start docker-compose\"\ndocker-compose up -d\necho \"list of containers\"\ndocker-compose ps\necho \"starting ssh test from f-db to f-app\"\ndocker exec -it f-db sh -c \"ssh -i /keys/ssh_test ubuntu@f-app\"\nFor further detail, you can try the above working example docker-container-ssh\ngit clone git@github.com:Adiii717/docker-container-ssh.git\ncd docker-container-ssh; \n./test.sh\nYou can replace the keys as these were used for testing purpose only.",
    "Windows Docker Container has no NAT IP Address. Cannot access container locally": "That's a known bug under docker windows. It is fixed in 19.03. So try updating your docker engine.",
    "How to build a new app from scratch inside a docker container?": "You can follow \"How to use Docker for Node.js development\" by Cody Craven:\nIt does use Docker itself to develop, not just deploy/run a NodeJS application.\nExample:\n# This will use the node:8.11.4-alpine image to run `npm init`\n# with the current directory mounted into the container.\n#\n# Follow the prompts to create your package.json\ndocker run --init --rm -it -v \"${PWD}:/src\" -w /src node:8.11.4-alpine npm init\nThen you can setup an execution environment with:\nFROM node:8.11.4-alpine AS dev\nWORKDIR /usr/src/app\nENV NODE_ENV development\nCOPY . .\n# You could use `yarn install` if you prefer.\nRUN npm install\nAnd build your app:\n# Replace YOUR-NAMESPACE/YOUR-IMAGE with the name you would like to use.\ndocker build -t YOUR-NAMESPACE/YOUR-IMAGE:dev --target dev .\nAnd run it:\n# The `YOUR COMMAND` portion can be replaced with whatever command you\n# would like to use in your container.\ndocker run --rm -it --init -v \"${PWD}:/usr/src/app\" YOUR-NAMESPACE/YOUR-IMAGE:dev YOUR COMMAND\nAll without node installed on your workstation!",
    "Running Java Swing GUI application on Docker": "Did some more research and by hit and trial following code seems to launch the GUI ,there are some errors after that but that must be due to some other issues in the GUI itself:\nFROM openjdk:8\n\n# Set environment\n\nENV JAVA_HOME /opt/jdk\n\nENV PATH ${PATH}:${JAVA_HOME}/bin   \n\n# COPY myJarFolder from local repository to the image\n\nCOPY ./myJarFolder /usr/local/myJarFolder\n\n# Start the image with the jar file as the entrypoint\n\nENTRYPOINT [\"java\", \"-jar\", \"/usr/local/myJarFolder/myJarFile.jar\"]\n\n# EOF",
    "docker container exits on entry points": "Here's what I see happening in this sequence:\nENTRYPOINT [\"bash\", \"/docker-entrypoint.sh\"]\nCMD [\"bash\", \"/home/test/app/start.sh\"]\nWhen you start the container, Docker runs bash /docker-entrypoint.sh bash /home/test/app/start.sh. However, the entrypoint script never looks at its command-line arguments at all, so any CMD you specify here or any command you give at the end of the docker run command line gets completely ignored.\nWhen that entrypoint script runs:\nexec gunicorn ... &\nexec service nginx start\n# end of file\nit starts gunicorn as a background process and continues to the next line; then it replaces itself with a service command. That service command becomes the main container process and has process ID 1. It starts nginx, and immediately returns. Now that the main container process has returned, the container exits.\nFor this code as you've written it, you should delete the exec lines at the end of your entrypoint script and replace them with just\nexec \"$@\"\nwhich will cause the shell to run its command-line parameters (that is, the Dockerfile CMD).\nHowever, there's a readily available nginx Docker image. Generally if you need multiple processes, it's easier and better to run them as two separate containers on the same Docker network. This avoids the complexities around trying to get multiple things running in the same container.",
    "How to run docker:dind to start with a shell": "Just like Andreas Wederbrand said. you can just\ndocker run -it docker:dind sh\nand if you want to use Dockerfile. Just write like this.\nFROM docker:dind\nCMD [\"sh\"]\nIt shouldn't overwrite ENTRYPOINT. You can try to inspect docker:dind image.\ndocker inspect docker:dind\nyou can see entrypoint is a shell script file.\n\"Entrypoint\": [\n  \"dockerd-entrypoint.sh\"\n   ],\nof course, we can find this file in container. get inside the docker\ndocker run -it docker:dind sh\nand then\ncat /usr/local/bin/dockerd-entrypoint.sh\nmore about entrypoint you can see\nhttps://medium.freecodecamp.org/docker-entrypoint-cmd-dockerfile-best-practices-abc591c30e21",
    "How to determine the specific version of a Docker image built from a latest/stable tag?": "As suggested by this answer\ndocker inspect --format='{{index .RepoDigests 0}}' $IMAGE\nThis will give you the sha256 hash of the image.\nThen you can use a service like MicroBadger to get more info about that specific build.\nIf you want to recreate the Dockerfile you can use docker history to examine the layer history:\n$ docker history docker\n\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n3e23a5875458        8 days ago          /bin/sh -c #(nop) ENV LC_ALL=C.UTF-8            0 B\n8578938dd170        8 days ago          /bin/sh -c dpkg-reconfigure locales &&    loc   1.245 MB\nbe51b77efb42        8 days ago          /bin/sh -c apt-get update && apt-get install    338.3 MB\n4b137612be55        6 weeks ago         /bin/sh -c #(nop) ADD jessie.tar.xz in /        121 MB\n750d58736b4b        6 weeks ago         /bin/sh -c #(nop) MAINTAINER Tianon Gravi <ad   0 B\n511136ea3c5a        9 months ago                                                        0 B    \nKeep in mind that if the image has been manually tampered with, I don't know how reliable this output would be.\nFinally if you want to go full hacker mode, this old thread on the Docker community forums has some info.\nI'm not sure how you can get the tag, because I don't believe this is stored in the image itself, but in the repository. So you'd have to query the repository itself, or get a full list of image history and go detective on it.",
    "Running interactive container with power shell": "you need to run your container with the -it switch. this will make you container interactive, so you can poke around\n docker run -it test",
    "Connecting couchdb container from another container": "You forgot to include the port for the connection:\nDB_URL: http://admin:password@couchdb:5984",
    "Docker how to use env_file variable in Dockerfile during build": "You can map the environment variable into the container by adding an environment option in your docker compose yaml file:\nversion: '2'\n\nservices:\n  web:\n    build: .\n    env_file:\n      - /etc/web.env\n    command: python3 manage.py runserver 0.0.0.0:8000\n    volumes:\n      - ./backend:/code\n    ports:\n      - \"8000:8000\"\n    environment:\n      - STAGE: ${STAGE}",
    "How to specify source and target for a VOLUME in Dockerfile?": "You can't mount a VOLUME in a Dockerfile specifying both source (host) and destination (container) paths.\nThis is because this idea does not fit the basic concept that Docker image is portable and independent of host.",
    "Unable to access docker container from the port mapped by docker": "I think your problem is that grunt is binding to localhost:9000 - which is internal to the container so the port you're publishing won't have any effect.\nIt needs to be listening on 0.0.0.0:9000 - I couldn't tell you off hand what your Gruntfile.js should say for that to happen, but off-hand it looks like, out of the box, grunt serve will only serve from localhost.",
    "Cannot call chown inside Docker container (Docker for Windows)": "You have similar issues illustrating the same error message in mongo issues 68 or issue 74\nThe host machine volume directory cannot be under /Users (or ~). Try:\ndocker run --name mongo -p 27017:27017 -v /var/lib/boot2docker/my-mongodb-data/:/data/db -d mongo --storageEngine wiredTiger\nThe PR 470 adds:\nWARNING: because MongoDB uses memory mapped files it is not possible to use it through vboxsf to your host (vbox bug).\nVirtualBox shared folders are not supported by MongoDB (see docs.mongodb.org and related jira.mongodb.org bug).\nThis means that it is not possible with the default setup using Docker Toolbox to run a MongoDB container with the data directory mapped to the host.",
    "how can I add git submodule into git repo as normal directory?": "Seems you cannot do that. The name .git is hard-coded in the source code: https://github.com/git/git/blob/fe9122a35213827348c521a16ffd0cf2652c4ac5/dir.c#L1260\nProbably one way is to make a script which renames .git to something else and back before and after adding it into repo like\nIn working directory under scripts\nmv .git hidden-git\nIn Dockerfile\nRUN mv $JENKINS_HOME/scriptler/scripts/hidden-git     \n$JENKINS_HOME/scriptler/scripts/.git\nAlternatively, probably it's possible to pass GIT_DIR environment variable into the plugin, so it could use another name.",
    "Compatability of Dockerfile RUN Commands Cross-OS (apt-get)": "As I commented in your question, you can add FROM statement to specify which relaying OS you want. for example:\nFROM docker.io/centos:latest\nRUN yum update -y\nRUN yum install -y java\n...\nnow you have to build/create the image with:\ndocker build -t <image-name> .\nThe idea is that you'll use the OS you are familiar with (for example, CentOS) and build an image of it. Now, you can take this image and run it above Ubuntu/CentOS/RHEL/whatever... with\ndocker run -it <image-name> bash\n(You just need to install docker in the desired OS.",
    "Dockerfile production/build/debug/test environment": "The answer might be straightforward: just create 4 Dockerfiles one depending on another.\nYou can add a volume to share build from sources part. The question is whether you want result assets to be included in the image or build it from sources each time.\nCreate 4 folders to have Dockerfile in each.\nProduction\nproduction/Dockefile:\nFROM  # put server here\nCOPY  # put config here\n# some other option\n# volume sharing?\nBuild\nbuild/Dockerfile:\n# install dependencies\nADD # add sources here\nRUN # some building script\nDebug\ndebug/Dockefile:\n# ideally, configure production or build image\nTest\ntest/Dockefile:\nFROM # import production\n# install test dependencies\nRUN # test runner\nThere are also several options. 1. Use .gitignore with negative pattern (or ADD?)\n*\n!directory-i-want-to-add\n!another-directory-i-want-to-add\nPlus use docker command specifying dockerfiles and context:\ndocker build -t my/debug-image -f docker-debug .\ndocker build -t my/serve-image -f docker-serve .\ndocker build -t my/build-image -f docker-build .\ndocker build -t my/test-image -f docker-test .\nYou could also use different gitignore files.\nMount volumes Skip sending context at all, just use mounting volumes during run time (using -v host-dir:/docker-dir).\nSo you'd have to:\ndocker build -t my/build-image -f docker-build . # build `build` image (devtools like gulp, grunt, bundle, npm, etc)\ndocker run -v output:/output my/build-image build-command # copies files to output dir\ndocker build -t my/serve-image -f docker-serve . # build production from output dir\ndocker run my/serve-image # production-like serving from included or mounted dir\ndocker build -t my/serve-image -f docker-debug . # build debug from output dir\ndocker run my/serve-image # debug-like serving (uses build-image with some watch magic)",
    "how to do docker-compose pip install with a proxy?": "Does your change work when doing a docker build manually (not inside composer)?\nOnce you know you have it working with a normal docker build, you should be able to force docker-compose to rebuild using docker-compose build.",
    "Why do some docker images have binary blob layers and some have layer.tar files?": "The first one with layer.tar is docker archive image. The second with blobs/sha256 is OCI image. https://snyk.io/blog/container-image-formats/",
    "Switch USER for single RUN command in Dockerfile": "Or can I at least store some reference to the original USER during the build process somehow?\nYes, with an ENV variable from the parent image, as discussed here: Store and Restore Inherited Dockerfile USER setting\nFrom the Dockerfile reference documentation:\nA stage inherits any environment variables that were set using ENV by its parent stage or any ancestor. Refer here for more on multi-staged builds.\nParent Image:\nENV unprivilegeduser=safeuser\nRUN groupadd -r unprivileged && useradd --no-log-init -r -g unprivileged $unprivilegeduser\nUSER $unprivilegeduser\nDependent image:\n#Switch to root for a single RUN command\nUSER root \nRUN doRootStuff\nUSER $unprivilegeduser\nIt needs three lines instead of a single, elegant \"RUNAS\" command that you wanted, but it is far cleaner than other alternatives like sudo and docker inspect ContainerConfig.User etc.\nAs per the Dockerfile guidance:\nAvoid installing or using sudo as it has unpredictable TTY and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to sudo, such as initializing the daemon as root but running it as non-root, consider using \u201cgosu\u201d.",
    "Rancher desktop, docker file shell commands are not working on mac m1 chip": "Downgrade to Rancher Desktop 1.4.1.\nThis works but if downgrading is not an option there are workarounds on the Rancher Desktop Issues site for similar problems like this: qemu workaround\nAs a temporary workaround, as root in the VM:\nCreate /etc/conf.d/qemu-binfmt, with contents binfmt_flags=\"POCF\"\nRun rc-update --update\nRun rc-service qemu-binfmt restart\nEasy way to connect to the VM and run those commands is (source):\ndocker run -it --rm --privileged --pid=host justincormack/nsenter1",
    "How to interrupt the building of a Docker container in VSCode?": "When you click the container button on the left bottom corner during the build\ncontainer build button\nit gives this option to close the connection, which kills the build.\nclose the connection",
    "Git Actions Job fails because of Username and Password required": "Please make sure that you have configured secret keys in correct place.\nLink should be:\nhttps://github.com/{github_account}/{project_name}/settings/secrets/actions\nOn this page you should have both DOCKERHUB_TOKEN and DOCKERHUB_USER.\nIf you don't have them, create new token here.",
    "How to solve /bin/sh: 1: source: not found during making docker image in MacOS(Golang)?": "It seems like .env file is not contained in your image.\nTry to execute source .env after copying .env file into the image.",
    "Windows 10 Docker Build Error: \"error from sender: open LogFiles\\WMI\\RtBackup: Access is denied.\"": "I had this issue just now. Oddly, it was fixed when I changed directory one level up.",
    "NestJS: Copying Assets not working on Linux": "I had a similar problem trying to dockerize my project on a Linux host (yarn build on local worked just fine, but assets were not copied in the docker image).\nOriginal Setup\nDockerfile:\nFROM node:16-bullseye AS builder\nWORKDIR /usr/app\nCOPY package.json ./\nCOPY yarn.lock ./\nCOPY src ./\nCOPY nest-cli.json ./\nCOPY tsconfig.build.json ./\nCOPY tsconfig.json ./\nRUN yarn install\nRUN yarn build\n...\nnest-cli.json:\n{\n  ...,\n  \"compilerOptions\": {\n    \"assets: [ \"modules/mails/templates/**/*.hbs\" ],\n    ...\n  }\n}\nFix\nThe paths specified within 'compilerOptions.assets' are always relative to the 'src' folder within the project. Inspecting my docker image I realized that the content of the 'src' folder was being copied in the root of the build folder inside docker.\nWhen the content of 'src' is moved to the root, NestJS is able to build the project successfully, but since assets are looked for within the 'src' folder, no files are copied. Changing this in my Dockerfile fixed the problem:\nCOPY src ./src # Instead of 'COPY src ./'",
    "PermissionError: [Errno 13] Permission denied: '/.cache' - Error occurred while building a docker image": "in your terminal\nmkdir ./.cache\nadd your code\nos.environ['SENTENCE_TRANSFORMERS_HOME'] = './.cache'",
    "Docker - Run Oracle DB image and execute init script": "I have the same problem and as far as I know, files in \"docker-entrypoint-initdb.d\" are executed automatically, is that right? But I'm not sure that is this particular image \"store/oracle/database-enterprise:12.2.0.1-slim\" the scripts placed in this path are executed automatically.",
    "Mount docker volume to host machine path": "Looks like a permission issue.\n-v /codecoveragereports:/app/***/codecoveragereports is mounting a directory under the root / which is dangerous and you may not have the permission.\nIt's better to mount locally, like -v $PWD/codecoveragereports:/app/***/codecoveragereports, where $PWD is an environment variable equal to the current working directory.",
    "Docker With a .Net Core API": "So I was able to get the above working...it partly had to do with the --no-restore, and there's a bug associated with it. To get it to work I did the following:\nFROM microsoft/dotnet:2.1-sdk AS build\nWORKDIR /api\n\nCOPY ./*.sln ./\n# COPY ./NuGet.config /root/.nuget/NuGet/\n\nCOPY src/*/*.csproj ./\nRUN for file in $(ls *.csproj); do mkdir -p src/${file%.*}/ && mv $file src/${file%.*}/; done\n\n# COPY test/*/*.csproj ./\n# RUN for file in $(ls *.csproj); do mkdir -p test/${file%.*}/ && mv $file test/${file%.*}/; done\n\nRUN dotnet restore\n\nCOPY . .\n\nRUN dotnet publish -c Release -o out\n\nFROM microsoft/dotnet:2.1-aspnetcore-runtime AS runtime\nWORKDIR /api\nCOPY --from=build /api/src/api/out .\nENTRYPOINT [\"dotnet\", \"api.dll\"]\nI left the comments in the above code, in case you need an example for tests.",
    "GenerateRuntimeConfigurationFiles task failed unexpectedly": "I think some of the commands in the Dockerfile need to be changed.\nFROM microsoft/dotnet:2.1-sdk AS build-env\nWORKDIR /app\n\nCOPY *.csproj ./\nRUN dotnet restore\n\n# Copy everything else and build\nCOPY . ./\nRUN dotnet publish -c Release -o out\n\n# Build runtime image\nFROM microsoft/dotnet:2.1.1-aspnetcore-runtime\nWORKDIR /app\nCOPY --from=build-env /app/out .\nENTRYPOINT [\"dotnet\", \"MyProject.Api.dll\"]\nReferences:\nhttps://docs.docker.com/engine/examples/dotnetcore/#prerequisites\nhttps://raw.githubusercontent.com/dotnet/dotnet-docker/master/samples/dotnetapp/Dockerfile",
    "How to execute a sql file in Docker Oracle 12?": "Follow this link to install an Oracle 12c Docker image. I installed it on my Mac.\nhttps://dzone.com/articles/creating-an-oracle-database-docker-image\nDownload the Oracle Instant Client which includes SQL* Plus.\nOR\nDownload the latest command line tool SQLcl\nNow, to run a script you may Connect to your database using the easy connect with sqlplus or sql commandline.\nsqlplus user/pwd@//localhost:1521/ORCLPDB1 yourpath/yourscript.sql",
    "run java code on docker": "Try following\nMy env: Mac-Os Sierra-10.12.6\nDocker version:\nDocker version 18.03.0-ce, build 0520e24\nTo Check Java version in container:\nexecute $docker run -it docker-hello-world:latest\nget ContainerID by executing $docker ps\nget shell of docker image docker exec -i -t <CONTAINER ID>\nExecute following command\n$java -version\nopenjdk version \"1.8.0_151\"\nOpenJDK Runtime Environment (IcedTea 3.6.0) (Alpine 8.151.12-r0)\nOpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)\nCreate a directory (anyname here I gave a), I created at Desktop,\ncd ~/Desktop\nmkdir a\ncreate Ave.java file in ~/Desktop/a directory\nAlso, have Dockerfile in the same directory(/a)\nConsidering there is no package name in Ave.java\n$javac Ave.java\nnow folder /a will have 3 files, Ave.java, Ave.class, Dockerfile\nExecute following command\n$docker build -t docker-hello-world:latest .\nConsole Logs:\nSending build context to Docker daemon  5.632kB\nStep 1/4 : FROM alpine:latest\n ---> 3fd9065eaf02\nStep 2/4 : ADD Ave.class Ave.class\n ---> 8b94ae6de674\nStep 3/4 : RUN apk --update add openjdk8-jre\n ---> Running in f12eb4589a34\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\n(1/39) Installing libffi (3.2.1-r4)\n(2/39) Installing libtasn1 (4.12-r3)\n(3/39) Installing p11-kit (0.23.2-r2)\n(4/39) Installing p11-kit-trust (0.23.2-r2)\n(5/39) Installing ca-certificates (20171114-r0)\n(6/39) Installing java-cacerts (1.0-r0)\n(7/39) Installing libxau (1.0.8-r2)\n(8/39) Installing libbsd (0.8.6-r1)\n(9/39) Installing libxdmcp (1.1.2-r4)\n(10/39) Installing libxcb (1.12-r1)\n(11/39) Installing libx11 (1.6.5-r1)\n(12/39) Installing libxcomposite (0.4.4-r1)\n(13/39) Installing libxext (1.3.3-r2)\n(14/39) Installing libxi (1.7.9-r1)\n(15/39) Installing libxrender (0.9.10-r2)\n(16/39) Installing libxtst (1.2.3-r1)\n(17/39) Installing alsa-lib (1.1.4.1-r2)\n(18/39) Installing libbz2 (1.0.6-r6)\n(19/39) Installing libpng (1.6.34-r1)\n(20/39) Installing freetype (2.8.1-r2)\n(21/39) Installing libgcc (6.4.0-r5)\n(22/39) Installing giflib (5.1.4-r1)\n(23/39) Installing libjpeg-turbo (1.5.2-r0)\n(24/39) Installing libstdc++ (6.4.0-r5)\n(25/39) Installing openjdk8-jre-lib (8.151.12-r0)\n(26/39) Installing java-common (0.1-r0)\n(27/39) Installing krb5-conf (1.0-r1)\n(28/39) Installing libcom_err (1.43.7-r0)\n(29/39) Installing keyutils-libs (1.5.10-r0)\n(30/39) Installing libverto (0.3.0-r0)\n(31/39) Installing krb5-libs (1.15.2-r1)\n(32/39) Installing lcms2 (2.8-r1)\n(33/39) Installing nspr (4.17-r0)\n(34/39) Installing sqlite-libs (3.21.0-r0)\n(35/39) Installing nss (3.34.1-r0)\n(36/39) Installing pcsc-lite-libs (1.8.22-r0)\n(37/39) Installing lksctp-tools (1.0.17-r0)\n(38/39) Installing openjdk8-jre-base (8.151.12-r0)\n(39/39) Installing openjdk8-jre (8.151.12-r0)\nExecuting busybox-1.27.2-r7.trigger\nExecuting ca-certificates-20171114-r0.trigger\nExecuting java-common-0.1-r0.trigger\nOK: 81 MiB in 50 packages\nRemoving intermediate container f12eb4589a34\n ---> 82d9ecfcc95e\nStep 4/4 : ENTRYPOINT [\"java\", \"-Djava.security.egd=file:/dev/./urandom\", \"Ave\"]\n ---> Running in 28f2df6fb544\nRemoving intermediate container 28f2df6fb544\n ---> bbf098575e6a\nSuccessfully built bbf098575e6a\nSuccessfully tagged docker-hello-world:latest\nExecute command as suggested by @Siking\n$docker run -it docker-hello-world:latest\nhere is snapshot of output:",
    "The command '/bin/sh -c bundle install' returned a non-zero code: 10": "I've made these changes in Dockerfile and it works like a charm:\nFROM phusion/passenger-ruby24\n# Set correct environment variables.\nENV HOME /root\n# Use baseimage-docker's init process.\nCMD [\"/sbin/my_init\"]\n# Additional packages: we are adding the netcat package so we can\n# make pings to the database service\nRUN apt-get update && apt-get install -y -o Dpkg::Options::=\"--force-confold\" netcat\n# Enable Nginx and Passenger\nRUN rm -f /etc/service/nginx/down\n# Add virtual host entry for the application. Make sure\n# the file is in the correct path\nRUN rm /etc/nginx/sites-enabled/default\nADD webapp.conf /etc/nginx/sites-enabled/webapp.conf\n# In case we need some environmental variables in Nginx. Make sure\n# the file is in the correct path\nADD rails-env.conf /etc/nginx/main.d/rails-env.conf\n# Install gems: it's better to build an independent layer for the gems\n# so they are cached during builds unless Gemfile changes WORKDIR /tmp\n\nCOPY Gemfile* /tmp/\nWORKDIR /tmp\nRUN bundle install\n\n# Copy application into the container and use right permissions: passenger\n# uses the app user for running the application \n# RUN mkdir /home/me/Desktop/sg/docker-kubernets\nCOPY . /home/me/Desktop/sg/docker-kubernets\nRUN usermod -u 1000 app\nRUN chown -R app:app /home/me/Desktop/sg/docker-kubernets\nWORKDIR /home/me/Desktop/sg/docker-kubernets\n\n# Clean up APT when done.\nRUN apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\nEXPOSE 80\nruby version Ruby 2.4.2p198 (2017-09-14 revision 59899) [x86_64-linux]\nI hope be helpful!\nRegards!",
    "Postgres Shutdown Immediately after docker compose up?": "Add a link between your web and postgres image like so:\nversion: '2'\nservices:\n  postgres:\n    restart: always\n    image: sameersbn/postgresql:9.6-2\n    ports:\n      - \"5432:5432\"\n    environment:\n      - DB_USER=shorturl\n      - DB_PASS=shorturl\n      - DB_NAME=shorturl\n  web:\n    build: .\n    ports:\n     - \"4000:4000\"\n    volumes:\n     - .:/shortlr\n    depends_on:\n     - postgres\n    links:\n     - postgres\n    command: [\"./wait-for-it.sh\",\"postgres:5432\", \"--\", \"npm\", \"start\"]\nAlthough you should use networks instead because links is a legacy option. Read the docs on networks here",
    "Docker 's Error: libselinux conflicts with fakesystemd": "Try running this inside the container you create, before any installation is made:\nyum swap -y fakesystemd systemd && yum clean all\nyum update -y  && yum clean all\nOr inside a Dockerfile at the begining before the first RUN you have tipped:\nRUN yum swap -y fakesystemd systemd && yum clean all \\\n    && yum update -y  && yum clean all\nHope was useful!",
    "Host Volumes Not getting mounted on 'Docker-compose up'": "This shouldn't work for you on OSX, yet alone Debian. Here's why:\nWhen you add ./command.sh to the volume /home/docker/django/django/ the image builds fine, with the file in the correct directory. But when you up the container, you are mounting your local directory \"on top of\" the one you created in the image. So, there is no longer anything there...\nI recommend adding command.sh to a different location, e.g., /opt/django/ or something, and changing your docker command to ./opt/command.sh.\nOr more simply, something like this, here's the full code:\n# Dockerfile\nFROM python:2.7\nRUN mkdir -p /home/docker/django/\nWORKDIR /home/docker/django/\n\n# docker-compose.yml\ndjango:\n    build: django\n    command: ./command.sh\n    volumes:\n        - ./django/:/home/docker/django/",
    "Dockerfile and issue making work bower install": "Are you perhaps mounting a volume from your host into the container through your docker-compose.yml file or the -v parameter in your docker run command?\nI ask because you mentioned you don't have bower installed on your machine, so you presumably don't have the bower_components folder in the host copy of your codebase. If you then run your container and mount the volume containing the source on your host into the container then the bower_components folder would be lost, explaining why it seems to disappear from the container.\nI would first test to see if this is indeed the problem by not mounting any volumes from your host and seeing if the bower_components folder then remains after you build and run the container. You could then install bower on your host and run bower install so the mounted volume doesn't cause issues while you're in development.",
    "Environment variables with double asterisks in Dockerfile": "It's non-official convention to use these variables as template variables. They will be replaced in run-time.\nOr you can replace them using -e switch of docker run.\nFor example:\nENV MYSQL_USER admin\nENV MYSQL_PASS **Random**\n\n# Replication ENV\nENV REPLICATION_MASTER **False**\nENV REPLICATION_SLAVE **False**\nIf you take a look on start script you can see the following:\nif [ \"$MYSQL_PASS\" = \"**Random**\" ]; then\n    unset MYSQL_PASS\nfi\n\nPASS=${MYSQL_PASS:-$(pwgen -s 12 1)}\nIf variable value is **Random** let's replace it with a randomly generated password.",
    "How to detect if the current script is running in a docker build?": "Try this :\n#!/bin/bash\n\n# myscript.sh\n\nisDocker(){\n    local cgroup=/proc/1/cgroup\n    test -f $cgroup && [[ \"$(<$cgroup)\" = *:cpuset:/docker/* ]]\n}\n\nisDockerBuildkit(){\n    local cgroup=/proc/1/cgroup\n    test -f $cgroup && [[ \"$(<$cgroup)\" = *:cpuset:/docker/buildkit/* ]]\n}\n\nisDockerContainer(){\n    [ -e /.dockerenv ]\n}\n\nif isDockerBuildkit || (isDocker && ! isDockerContainer)\nthen\n  echo \"I am in a docker build\"\nelse\n  echo \"I am not in a docker build\"\nfi",
    "How to specify user and group owner of a mounted device in Docker container?": "I found a hacky solution: I installed sudo in the container and allowed the non-root user to execute chgrp without password, then added sudo chgrp myuser /dev/kvm to my entrypoint script.\nHere an example for kvm access for jenkins:\nFROM jenkins/jenkins:2.339-jdk11\nUSER root\n\nRUN apt-get update && apt-get install -y sudo && \\ \n    rm -rf /var/lib/apt/lists/* && \\\n    echo \"jenkins ALL = (root) NOPASSWD: /bin/chgrp\" >> /etc/sudoers\n\nUSER jenkins\n\nCOPY initscript.sh /initscript.sh\n\nENTRYPOINT [ \"/initscript.sh\" ]\nAnd the initscript file\n#!/bin/bash\nsudo chgrp jenkins /dev/kvm\n/sbin/tini -- /usr/local/bin/jenkins.sh\nmaybe this works for your setup, too.",
    "Change hostname from within a Windows container": "The following worked for me -\nrename-computer.ps1:\n$ComputerName = \"New Name\"\n   \nRemove-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\" -name \"Hostname\" \nRemove-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\" -name \"NV Hostname\" \n\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Computername\\Computername\" -name \"Computername\" -value $ComputerName\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Computername\\ActiveComputername\" -name \"Computername\" -value $ComputerName\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\" -name \"Hostname\" -value $ComputerName\nSet-ItemProperty -path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\" -name \"NV Hostname\" -value  $ComputerName\nSet-ItemProperty -path \"HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" -name \"AltDefaultDomainName\" -value $ComputerName\nSet-ItemProperty -path \"HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" -name \"DefaultDomainName\" -value $ComputerName\nDockerfile:\nCOPY \"rename-computer.ps1\" \"C:/\"  \nRUN powershell -command \"Set-ExecutionPolicy RemoteSigned\" \\\n && powershell -command \"C:/rename-computer.ps1 ; hostname\"  \\\n && powershell -command \"Set-ExecutionPolicy Restricted\" \\\n && <your installer runs here> \nSources -\nhttps://serverfault.com/a/731832/845133\nhttps://gist.github.com/timnew/2373475",
    "\"No matching distribution found for fastapi \" using pip in Ubuntu Docker": "In my case I got this error because I forgot to configure my proxy. As soon as I did\nexport http_proxy=[proxy info]\nexport https_proxy=[proxy info]\nthe error was resolved.\nNote that this may happen even if some of your packages are installing fine. For example, my NPM proxy was configured separately, so it wasn't affected.",
    "Why do I still need to do COPY in my Dockerfile if I'm using a Bind Mount?": "This is an old question, but if anyone else runs into this problem in the future . . .\n@camba1's answer is close, but not correct.\nData Persistence through docker compose down:\nBind Mounts: WILL PERSIST in mount location\nCOPY INTO: WILL PERSIST in image\nData Persistence through Image Deletion:\nBind Mounts: WILL PERSIST in mount location\nCOPY INTO: WILL NOT PERSIST\nData was stored directly in image and will be lost when removed\nTheoretically, your code should work as it is, and there should not be a need for a redundant COPY INTO command.\nMy best guess, since you are using the chown flag in your COPY INTO command, is that there is a file permission issue with accessing the data as the node user/group without the COPY INTO explicitly setting the R/W/X permissions/ownership.\nTo quickly test this, you could try to lower the permissions (safely and for debugging only) on the bound directories directly so that the node user may access them.",
    "Docker Alpine Linux python (missing)": "From this issue on the Docker's repo:\nThis was \"broken\" while updating our base from alpine:3.11 to alpine:3.12.\nIn order to fix it you need to specify the version of Python directly, e.g.:\napk add python2\n// or\napk add python3",
    "Installing ruby on rails by using Dockerfile \"Could not locate gemfile\" during build": "When you build it, make sure you are in your Rails project root folder. You should already have run rails new your-project-name in this folder to generate this.\nThe command ONBUILD ADD . /opt/app will copy the current folder recursively into the Docker image, so it must already contain Gemfile and all other parts of the Rails project.",
    "Force step to be cached in Dockerfile": "Docker's layer cache essentially says \"if I start from image 01234567, and RUN some command, then I will get image 2468ace0\". If something has changed and you now have image 13579bdf instead, there's no way to short-circuit this besides running the command again.\nThere's a couple of approaches that can help mitigate this:\nRun heavy-weight commands that don't actually depend on the application code early in your Dockerfile. RUN apt-get install before you COPY ..\nMinimize the amount of stuff in one image. Don't try to put two separate applications with lengthy builds in the same image.\nUse multi-stage builds, where each stage can independently take advantage of the cache, and you'll have a relatively cheap COPY into the final image.\nPre-compile some part of the sequence on the host, and COPY the result into the image rather than RUN lengthy-install. (This was an extremely useful technique before there were multi-stage builds, and is still relevant.)",
    "Docker socket permissions within container": "Simple change UID of the user,\nRUN useradd -ms /bin/bash -d /usr/local/myuser -u ${UID} myuser\nwhere UID can be taken via\nid \nin terminal",
    "docker build inside docker build": "I don't think this is possible. Even if it was, it sounds like it might be overcomplicated.\nIf you'd like to do something a little better than a DinD (Docker-in-Docker) - you can try building your images with buildah which does not require a Docker daemon (hence - is safer) to build an image and performs better (at least for me) than what I saw with DinD scenarios.",
    "Trying to secure Nginx with Let's Encrypt but can't access .pem files in /etc/letsencrypt/live using a non-root user": "I do not think that is a best practice. You should not access your droplet with a root user either\nYou should setup the droplet with a new user with sudo grants, and remove root access completely, as explained in this guide: https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-16-04\nThen you should verify the droplet and store the SSL certificates using certbot nginx plugin, like explained here:\nhttps://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-16-04\nStep 5 of the last guide explains the renew certificate part.",
    "How to connect a Docker container of Grafana to a Docker conatiner of MySql?": "As stated here:\ngo-mysql: authentication plugin not supported while connecting from go app container to mysql container\nThe problem was compatibility between the Grafana versions and the MySQL version. Once moving to docker image mysql:5.7 (had to migrate the data too) - the issue was resolved (need to also change the COLLATION and CHAR_SET in the DB to downgrade from version 8.0.12 to 5.7).",
    "Check if Docker image exists in cloud repo": "This script will build only if image not exist.\nupdate for V2\nfunction docker_tag_exists() {\n    curl --silent -f -lSL https://hub.docker.com/v2/repositories/$1/tags/$2 > /dev/null \n}\nUse above function for v2\n#!/bin/bash\nfunction docker_tag_exists() {\n    curl --silent -f -lSL https://index.docker.io/v1/repositories/$1/tags/$2 > /dev/null \n\n}\n\nif docker_tag_exists library/node 9.11.2-jessie; then\n    echo \"Docker image exist,....\"\n    echo \"pulling existing docker...\"\n    #so docker image exist pull the docker image\n    docker pull node:9.11.2-jessie\nelse \n    echo \"Docker image not exist remotly....\"\n    echo \"Building docker image...\"\n    #build docker image here with absoult or retlative path\n    docker build -t nodejs .\n\nfi\nWith little modification from the link below. If the registry is private u check this link With username and password",
    "how to install anaconda / miniconda on Linux silently": "can be achieved by bash miniconda.sh -b (thanks @darthbith)\nThe command line usage for this can only be seen with -h flag but not --help, so I missed it.\nTo install the anaconda to another place, use the -p option:\nbash anaconda.sh -b -p /some/path",
    "Static code analysis of Dockerfiles?": "Although this question is 2 years old, however there are two ways to do static analysis of the Dockerfile.\nusing FromLatest\nusing Hadolint\nOption#2 is mostly preferable since this can be used as an automated process inside CICD pipelines.\nHadolint also provide ways to exclude messages/errors using \".hadolint.yml\"",
    "Jekyll-Assets: Missing assets when building site": "TL;DR: Deleting _site before build caused the problem. Also deleting .jekyll-cache before build solved the problem for me.\nI had a very similar experience with Jekyll-assets. During development it occurs that some images were missing in the _site/assets folder. After a bit trying out I found a way to reproduce it.\nCheckout a clean version of the code\nBuild with bundle exec Jekyll build\nRun bundle exec jekyll clean (which deletes, among others, the _site folder)\nBuild with bundle exec Jekyll build again\nAfter these steps I had missing files in the _site/assets folder. What solved the problem for me was additional to using the jekyll clean to delete the folder .jekyll-cache. I quiet not deep enough into jekyll-assets to understand how the caching in this folder works, but at least it solves the problem. If the folder is excluded from version control, and in case it's a generated cache folder it should be, you can also use git clean -xdf but be careful with this, it deletes any file, that is not under version control! I would be grateful if you could verify my solution according to your concrete case.\nI'm sorry that I could not provide you a real explanation for why this happens, but I hope that my answer could help you anyway.",
    "spring boot application in docker terminates immediately": "I don't find any reason to use anapsix/alpine-java:latest\nI recommand you to use the openjdk:8-jre-alpine, light weight and less in size comparitively.\ncreate a war files instead of jar.\nuse the following\nDockerfile\nFROM openjdk:8-jre-alpine\n\nENV SPRING_OUTPUT_ANSI_ENABLED=ALWAYS \\\n    SLEEP_TIME=0 \\\n    JAVA_OPTS=\"\"\n\n# add directly the war\nADD *.war /app.war\n\nEXPOSE 8080\nCMD echo \"The application will start in ${SLEEP_TIME}s...\" && \\\n    sleep ${SLEEP_TIME} && \\\n    java ${JAVA_OPTS} -Djava.security.egd=file:/dev/./urandom -jar /app.war",
    "Starting a service during Docker build": "These *ctl programs usually come in with a few utilities to start / stop / monitor the status of the service.\nIf your case I think the best idea is to have a simple bash script you can run at build time that does this:\nstart ejabberd\nmonitor the status at intervals\nif the status of the process is up, run your command\nLook at this:\nroot@158479dec020:/# ejabberdctl status\nFailed RPC connection to the node ejabberd@158479dec020: nodedown\nroot@158479dec020:/# echo $?\n3\nroot@158479dec020:/# ejabberdctl start \nroot@158479dec020:/# echo $?\n0\nroot@158479dec020:/# ejabberdctl status\nThe node ejabberd@158479dec020 is started with status: started\nejabberd 16.01 is running in that node\nroot@158479dec020:/# echo $?\n0\nroot@158479dec020:/# ejabberdctl stop  \nroot@158479dec020:/# echo $?\n0\nroot@158479dec020:/# ejabberdctl status\nFailed RPC connection to the node ejabberd@158479dec020: nodedown\nroot@158479dec020:/# echo $?\n3\nSo this tells us that if you run a ejabberd status and the daemon is not running you receive exit code 3, 0 if it's up and running instead.\nThere you go with your bash script:\nfunction run() {\n  ejabberdctl start # Repeating just in case...\n  ejabberdctl status &>/dev/null\n\n  if [ $? -eq 0 ]; then\n    echo \"Do some magic here, ejabberd is running...\"\n    exit 0\n  fi \n\n  echo \"Ejabberd still down...\"\n}\n\nwhile true; do run; sleep 1; done\nAnd this is what you'd get at the CLI:\nroot@158479dec020:/# ./check.sh \nEjabberd still down...\nDo some magic here, ejabberd is running...\nroot@158479dec020:/# ejabberdctl stop\nroot@158479dec020:/# ./check.sh \nEjabberd still down...\nEjabberd still down...\nDo some magic here, ejabberd is running...",
    "Using ENTRYPOINT in a Dockerfile makes the base image's CMD be ignored": "See https://github.com/moby/moby/issues/5147 - it explains when and why this behaviour was implemented.",
    "Install Docker in Alpine Docker": "I managed to do that the easy way\ndocker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker --privileged docker:dind sh\nI am using this command on my test env!",
    "Docker mount an directory of container to another container": "If files already exist in the directory of the container you're mounting as a volume, the contents of the directory will be copied to the volume. See Populate a volume using a container .\nThere's no way to mount a container's directory as a volume without copying. An alternative is to not store the data in a container and instead create a volume on the host and populate it with the data. This way, the data will only exist on once.",
    "docker-compose up giving error": "I had the same unhelpful error when I tried to run docker-compose on Windows from a project I created in Linux. If you clone a Linux project on Windows, by default Git adds Windows style line endings which mess up any Bash scripts. If this is the case run git config --global core.autocrlf false to configure Git to never change the line endings. Then delete repository and re-clone it.\nAlternatively if you created the project initially on Windows, many times the IDE adds Windows style line endings. If this is the case then in Notepad++, Click edit menu > EOL Conversion > Unix.",
    "How to handle make missing from alpine docker image?": "If you added RUN apk add --no-cache make and still there is an issue, add this as well to your DockerFile:\nRUN apk add g++\nAlpine images are lightweight and do not have utilities in them, adding g++ solves that. You can check the output by using the which make command, for me it returns /usr/bin/make\nRef: https://mroldan.medium.com/alpine-sh-make-not-found-1e87ab87c56",
    "Docker build does not proceed after finishing RUN statement": "I just ran into this same problem myself. Trying to install a Yocto SDK in a docker image. The script appears to be stuck after it completes. It turns out you need to be patient. I let an installation run sit while I ran an errand and I came back to a successfully installed SDK. Unfortunately, running with -D to enable debugging did not reveal why the installation takes as long as it does.",
    "Dockerfile build from parent directory": "There is no problem. In Dockerfile you cant get out (access parent folder) of build context, not Dockerfile's folder.\nLeave you structure as it was, and explicitly specify path to Dockerfile:\ndocker build -t app -f application-build/Dockerfile . \ndocker build -t celery -f celery-build/Dockerfile . \nAnd inside your Dockerfiles remember that path is /application. So you can easily copy:\nDockerfile\n...\nCOPY code /code\n...",
    "docker multistage build fails with multiple --build-arg": "The reason is because IMAGE_TWO is not in the same scope check this https://docs.docker.com/engine/reference/builder/#scope\nBasically the ARG IMAGE_TWO is still part of the first stage and goes out of scope when that stage ends and will not be part of the second stage.\nDeclaring the arguments at the beginning allow the IMAGE_TWO to be in the second stage.\nARG IMAGE_ONE\nARG IMAGE_TWO\nFROM ${IMAGE_ONE}\nRUN cat /etc/debian_version\n\nFROM ${IMAGE_TWO}\nRUN cat /etc/debian_version\ndocker build --build-arg=IMAGE_ONE=debian:7 --build-arg=IMAGE_TWO=debian:8 .",
    "What happens to files in docker container when host is shut down ungracefully?": "The files are deleted. Containers are ephemeral in nature. When the container starts it creates a new writable layer on top of the image, and whatever changes you make are on that layer. If you are not storing/mapping that layer data to some persistent location, then once the container or Docker daemon or host machine restarts all the data is lost.",
    "Docker container keeps stopping after 'docker start'": "Your Docker image doesn\u2019t actually do anything, container stop when finish its job. Since here no foreground process running it will start and then immediately stop. To confirm your container have no issues, try to put below code into a docker-compose.yml(in same folder as the Dockerfile) and run docker-compose up, now you will see your container is running without exiting.\nversion: '3'\nservices:\n  my-service:\n    build: .\n    tty: true    \nPlease have a look here Docker official tutorial it will guide you to how to work with docker.",
    "How to fix \"Illegal option\" error when compiling a dockerfile?": "After a lot of research, I found the solution.\nThe problem is that Windows uses \\r\\n as an end of line, whereas unix only uses \\n. So, inside my download.sh file, there was a ^M character that led to the error /bin/sh: illegal option\nThe solution was to copy the code from the downlod.sh file and convert it to LF:\nUse the text editor such as Notepad++ at the Windows machine to convert\nGo to menu Edit -> EOL Conversion -> Unix (LF)\nOnce that is done, the docker build works correctly.\nThank you all for the support.",
    "Docker: no such option: --use-wheel": "Your issue isn't due to missing dependencies ( wheel is installed in the build.sh script you referenced: https://github.com/ryansb/sklearn-build-lambda/blob/master/build.sh#L18 )\nuse-wheel was deprecated and no longer exists for pip.\nYou can achieve the same by omitting the --use-wheel entries from the script. Take a look at the Python 3.6 PR on the linked repository: https://github.com/ryansb/sklearn-build-lambda/pull/16/files#diff-0b83f9dedf40d7356e5ca147a077acb4",
    "How to set mysql system variables such as max_allowed_packet max_allowed_packet, character_set_server and collation-server through Dockerfile?": "There are several approaches, this is one:\nCMD [\"--character-set-server=utf8mb4\", \"--collation-server=utf8mb4_unicode_ci\", \"--max-allowed-packet=1073741824\"]\nAs an alternative you can put a custom config file in /etc/mysql/conf.d\ncustom-mysql.cnf:\n[mysqld]\nmax_allowed_packet = 1073741824\ncharacter-set-server = utf8mb4\ncollation-server = utf8mb4_unicode_ci\nDockerfile:\nFROM mysql:latest\nCOPY ./custom-mysql.cnf /etc/mysql/conf.d/\nMore here",
    "Dockerfile for tomcat": "This is what i did to solve this:\nDockerfile\nFROM tomcat\n\nMAINTAINER richard\n\nRUN apt-get update && apt-get -y upgrade\n\nWORKDIR /usr/local/tomcat\n\nCOPY tomcat-users.xml /usr/local/tomcat/conf/tomcat-users.xml\nCOPY context.xml /usr/local/tomcat/webapps/manager/META-INF/context.xml\n\nEXPOSE 8080\nI'm copying those two files in order to access the manager app from outside. If you want it too, add the following to your context and tomcat-users files\nContext.xml\n<Context antiResourceLocking=\"false\" privileged=\"true\" >\n    <!-- <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n        allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" /> -->\n    <Manager sessionAttributeValueClassNameFilter=\"java\\.lang\\.(?:Boolean|Integer|Long|Number|String)|org\\.apache\\.catalina\\.filters\\.CsrfPreventionFilter\\$LruCache(?:\\$1)?|java\\.util\\.(?:Linked)?HashMap\"/>\n</Context>\ntomcat-users.xml\n<tomcat-users xmlns=\"http://tomcat.apache.org/xml\"\n              xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n              xsi:schemaLocation=\"http://tomcat.apache.org/xml tomcat-users.xsd\"\n              version=\"1.0\">\n  <user username=\"admin\" password=\"secret\" roles=\"manager-gui\"/>\n</tomcat-users>\nThen you can build it and run it:\ndocker build -t name/tomcat .\ndocker run -d -p 8080:8080 --name some_name name/tomcat\nDeploy your application as follows:\ndocker cp some/app.war some_name:/usr/local/tomcat/webapps/app.war",
    "Trying to start the kubernetes in Docker-Desktop but it's stuck": "Just follow these steps\nstop docker for desktop\nremove the folder ~/Library/Group\\ Containers/group.com.docker/pki\n   rm -rf ~/Library/Group\\ Containers/group.com.docker/pki\nstart docker for destkop\nFound the solution here\nAnd given that every time I try to start \"Docker for Desktop\" Kubernetes get stuck, to better investigate for me was useful remove the kube autostart from the configuration.\nJust edit the file:\n~/Library/Group\\ Containers/group.com.docker/settings.json\nAnd change kubernetesEnabled value to false",
    "How to edit docker mysql image and increase connection limit": "you can run the container like this:\ndocker run -d -e \"MYSQL_ROOT_PASSWORD=test\" mysql --max_connections=10000\nor make your own Dockerfile:\nFROM mysql\nCMD [ \"--max_connections=10000\" ]\nthen build it:\ndocker build -t my_mysql .\nand run it:\ndocker run -d -e \"MYSQL_ROOT_PASSWORD=test\" my_mysql",
    "how can i pass arguments or bypass it in docker build process? [duplicate]": "You could set the environment variables DEBIAN_FRONTEND=noninteractive and DEBCONF_NONINTERACTIVE_SEEN=true in your Dockerfile, before RUN sudo apt-get install php libapache2-mod-php -y.\nYour Dockerfile should look like this:\nFROM ubuntu:18.04\n\n\nRUN apt-get update && \\\n       apt-get install -y --no-install-recommends apt-utils && \\\n       apt-get -y install sudo\n\nRUN sudo apt-get install apache2 -y\nRUN sudo apt-get install mysql-server -y\n\n\n## for apt to be noninteractive\nENV DEBIAN_FRONTEND noninteractive\nENV DEBCONF_NONINTERACTIVE_SEEN true\n\n## preesed tzdata, update package index, upgrade packages and install needed software\nRUN echo \"tzdata tzdata/Areas select Europe\" > /tmp/preseed.txt; \\\n    echo \"tzdata tzdata/Zones/Europe select Berlin\" >> /tmp/preseed.txt; \\\n    debconf-set-selections /tmp/preseed.txt && \\\n    apt-get update && \\\n    apt-get install -y tzdata\n\n\n\nRUN sudo apt-get install php libapache2-mod-php -y\nRUN rm -rf /var/www/html/\nCOPY . /var/www/html/\nWORKDIR /var/www/html/\nEXPOSE 80\nRUN chmod -R 777 /var/www/html/app/tmp/\n\nCMD systemctl restart apache2\nYou should change Europe and Berlin with wath you want.",
    "How to keep an infinite loop running in order to not close a container in docker": "You can do this by putting the commands you want to execute into a script, and setting the script to be the command Docker runs when it starts a container:\nFROM sixeyed/ubuntu-with-utils\n\nRUN echo 'ping localhost &' > /bootstrap.sh\nRUN echo 'sleep infinity' >> /bootstrap.sh\nRUN chmod +x /bootstrap.sh\n\nCMD /bootstrap.sh\nWhen you build an image from this Dockerfile and run a container from the image, it will start ping in the background and sleep in the foreground, so you can daemonize the container with docker run -d and it will keep running.\nThis is not ideal though - Docker only monitors the last process it started when it ran the container, so it will be checking on sleep rather than ping. If the ping command errors the container will keep running. Typically, you want the real application to be the only thing you start in the CMD.",
    "openssh-server doesn't start in Docker container": "When building a Dockerfile you would create an image. But you can't create an image with an already running ssh daemon or any running service else. First if you create a running container out of the image you can start services inside. E.g. by appending the start instruction to the docker run command:\nsudo docker run -d mysshserver service ssh start\nYou can define a default command for your docker image with CMD. Here is an example Dockerfile:\nFROM ubuntu:14.04.1\nMAINTAINER Thomas Steinbach\nEXPOSE 22\nRUN apt-get install -y openssh-server\nCMD service ssh start && while true; do sleep 3000; done\nYou can build an run this image with the following two commands:\nsudo docker build -t sshtest .\nsudo docker run -d -P --name ssht sshtest\nNow you can connect to this container via ssh. Note that in the example Dockerfile no user and no login was created. This image is just for example and you can start an ssh connection to it, but not login.",
    "How to set port to run static website as nginx docker container?": "The problem is that are assuming just using EXPOSE 3499 changes the port on your nginx config. The nginx is still running on port 80 within the container.\nEXPOSE is to say your intentions, that the image does intend to expose 3499. But the server inside needs to be configured to make sure the port it listens on is the same.\nSince you are using the official docker nginx image, you can read it's documentation below\nhttps://hub.docker.com/_/nginx\nUsing environment variables in nginx configuration\nOut-of-the-box, nginx doesn't support environment variables inside most configuration blocks. But envsubst may be used as a workaround if you need to generate your nginx configuration dynamically before nginx starts.\nHere is an example using docker-compose.yml:\nweb:\n  image: nginx\n  volumes:\n   - ./mysite.template:/etc/nginx/conf.d/mysite.template\n  ports:\n   - \"8080:80\"\n  environment:\n   - NGINX_HOST=foobar.com\n   - NGINX_PORT=80\n  command: /bin/bash -c \"envsubst < /etc/nginx/conf.d/mysite.template > /etc/nginx/conf.d/default.conf && exec nginx -g 'daemon off;'\"\nThe mysite.template file may then contain variable references like this:\nlisten ${NGINX_PORT};\nSo as you see, you need to do some work in the image to make sure your nginx actually listens on 3499\nUpdate: 23rd July 2019\nIn case you don't want to do it using environment variable. Then you can overwrite the file using docker-compose\nSo you will keep a local file default.conf where you will change the contents\nserver {\n    listen       3499;\n    server_name  localhost;\n\n    #charset koi8-r;\n    #access_log  /var/log/nginx/host.access.log  main;\n\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n}\nAnd mount this file in your docker-compose.yml\nweb:\n  image: nginx\n  volumes:\n   - ./default.conf:/etc/nginx/conf.d/default.conf\n  ports:\n   - \"8080:3499\"\nAnd that will make sure that the file is correct. You can even copy that in the Dockerfile, if you don't want to override at run-time",
    "Error: Starting container process caused \"exec: \\\"/docker-entrypoint.sh\\\": permission denied\"": "That is most likely a Linux file permission issue on config/docker-entrypoint.sh. If your host is Linux/Mac, you can run:\nchmod 755 config/docker-entrypoint.sh\nFor more on linux permissions, here's a helpful article: https://www.linux.com/learn/understanding-linux-file-permissions",
    "How can I set an alias in the .bashrc of my Docker image": "simply use RUN to add an alias to bash_profile.\nFROM ubuntu\nMAINTAINER Mojtaba Yeganeh\n\nRUN echo \"alias python=/home/user/python3.6\" >> ~/.bash_profile",
    "Docker - MySQL commands within Dockerfile using RUN (ERROR 2002)": "I think the issue might be that in the service hasn't started within the container used to build your Dockerfile.\nTry starting and configuring MySQL server within a single step. As a reference please check this file: https://github.com/dockerfile/mysql/blob/master/Dockerfile",
    "How can I automatically run a docker container on/after image build?": "What a Dockerfile does, is create an image not a container.\nWhen you do docker build -t <tagname> . and will see the result (tagged) if you run docker images.\nYou can then run that image to create a container.\nYou can also specify a CMD to run (by default) in the Dockerfile. That would be (I think)\nCMD [ \"catalina.sh\", \"run\" ] \nSo:\ndocker build -t my_tomcat_app .\ndocker run -d -p 8888:8080 --name my_tomcat_container my_tomcat_app\nThis will create a container from your image, and run whatever the CMD said. You can poke around inside it with docker exec\ndocker exec -it my_tomcat_container bash\nBut it'll stay running (because of the -d flag) and show up in docker ps.",
    "Docker Compose: exposing ports only to other containers": "Just don't publish ports: for the thing you don't want accessible from outside Docker space.\nversion: '3.8'\nservices:\n  backend:\n    build: .\n    # no ports:\n  proxy:\n    image: nginx\n    volumes: ['./nginx.conf:/etc/nginx/nginx.conf']\n    ports:\n      - '80:80'\nYour Nginx configuration can still proxy_pass http://backend:8080, but the Flask application won't be directly reachable from outside of Docker.\nports: specifically allows a connection from outside Docker into the container. It's not a required option. Connections between containers don't require ports:, and if ports: remap one of the application ports, that remapping is ignored.\nTechnically this setup only allows connections from within the same Docker network, so if you had other services in this setup they could also reach the Flask container. On a native-Linux host there are tricks to directly contact the container, but they don't work on other host OSes or from another system. IME usually these limitations are acceptable.",
    "Next JS app builds in the local but failed inside Docker": "There's an issue importing images in Next 12.1.6 with node 18.\nhttps://github.com/vercel/next.js/issues/38020\nAs noted in the comments, npm i sharp will likely resolve the issue.",
    "Cannot find module '/dist/main' when using docker multi-stage builds": "the main.js file in dist is located on /dist/src/main.js",
    "Docker build image failed? Failed to compute cache key: \"/app\" not found: not found": "The APP folder does not exist in your project\nYou are trying to add an APP folder that does not exist into your docker container and it is complaining because there is no APP folder to add into the container\nIn the line\nCOPY ./APP /APP\nThe ./APP must reflect a folder that is in your project",
    "Way to force docker to accept and proceed with building the image with a non zero response/code": "yum check-update is expected to exit with status 100 if updates are available, as described in its documentation:\ncheck-update\nImplemented so you could know if your machine had any updates that needed to be applied without running it interactively. Returns exit value of 100 if there are packages available for an update. Also returns a list of the packages to be updated in list format. Returns 0 if no packages are available for update. Returns 1 if an error occurred. Running in verbose mode also shows obsoletes.\nSimilarly, the docker RUN command is expected to terminate on any nonzero exit status. If you want to force the command to ignore an exit status of 100 (but still treat other failures as erroneous), you can do so as follows:\nRUN yum -y check-update || { rc=$?; [ \"$rc\" -eq 100 ] && exit 0; exit \"$rc\"; }\nThat the Docker RUN command treats any nonzero exit status as a failure is standard UNIX convention (the only successful exit status is 0), and is explicitly implemented in dockerfile/containerbackend.go:\nif status := <-waitC; status.ExitCode() != 0 {\n    close(finished)\n    logCancellationError(cancelErrCh,\n        fmt.Sprintf(\"a non-zero code from ContainerWait: %d\", status.ExitCode()))\n    return &statusCodeError{code: status.ExitCode(), err: status.Err()}\n}",
    "what is docker run -w flag?": "Just run docker run --help in your shell.\n-w, --workdir string                 Working directory inside the container\nhttps://docs.docker.com/engine/reference/run/#workdir",
    "Error installing nodejs version 12 on alpine linux": "You should not install nodejs-current, as this package is helpful to install *current version of nodejs from edge repository where nodejs version does not exist.\nIn your case, nodejs 12.x package already exists so You should install nodejs if you want to install an older version instead of nodejs-current.\nFROM alpine:3.9\nENV ALPINE_MIRROR \"http://dl-cdn.alpinelinux.org/alpine\"\nRUN echo \"${ALPINE_MIRROR}/v3.11/main/\" >> /etc/apk/repositories\nRUN apk add nodejs --repository=\"http://dl-cdn.alpinelinux.org/alpine/v3.11/main/\"\nRUN node --version\noutput\nRemoving intermediate container a201832610e0\n ---> b0919df78aef\nStep 5/5 : RUN node --version\n ---> Running in cd7950f9303b\nv12.15.0\nRemoving intermediate container cd7950f9303b\n ---> ce54af976f81\nSuccessfully built ce54af976f81",
    "Web container cannot call backend container from docker compose": "The important thing for this setup is that your actual front-end code is not running in Docker, it's running in your browser. That means it has no idea about Docker networking, containers, or anything else; the URL you give it has to be one that reaches a published port on your host. That's why localhost works here (if the browser and containers are running on the same host) but backend doesn't.\nA typical approach to this is to set up some sort of reverse proxy that can both host the front-end application code and proxy to the back-end application. (For example, set up Nginx where its /api route proxy_pass http://backend:8098, and its / route either try_files a prebuilt Javascript application or proxy_pass http://frontend:8080.)\nIf you do this, then e.g. http://localhost:8900 is the front-end and http://localhost:8900/api is the back-end, from the browser's point of view. This avoids the CORS issues @coedycode hints at in their answer; but it also means that the front-end code can use a relative URL /api (with no host name) and dodge this whole problem.\n+-------------+                  | Docker >           /     +----------+\n+-------------+                  |                 /------> | frontend |\n|             |  localhost:8900  |    +-------+    |        +----------+\n|   Browser   | ---------------> | -> | nginx | -> +\n|             |                  |    +-------+    | /api   +----------+\n|             |                  |                 \\------> | backend  |\n+-------------+                  |                          +----------+",
    "In a dockerfile, can we have multiple RUN commands into one RUN command?": "Yes you can and its a good practice\nInstead of doing this\nRUN python -m pip install --upgrade pip\nRUN python -m pip install --upgrade setuptools\nRUN pip install -r requirements.txt \nTry this\nRUN python -m pip install --upgrade pip &&\\\n    python -m pip install --upgrade setuptools &&\\\n    pip install -r requirements.txt \nAdvantages with that approach\nEach instruction in the Dockerfile adds an extra layer to the docker image The number of instructions and layers should be kept to a minimum as it ultimately affects the build performance and time",
    "Do Dockerfile RUN commands run in a login shell environment?": "to get the same environment as what they would run in?\nI understand \"As they would run by default without specfifying a command\"\nThat will simply depend on how your container is actually configured to run:\nif the ENTRYPOINT or CMD is configured to run a login shell, you should use a login shell\nif the ENTRYPOINT or CMD is configured to run a non-login shell, you should use a non-login shell\nYou can identify this by running docker inspect on your container or docker image inspect which will give you ENTRYPOINT and CMD\nSame principle if you first run the container then create a shell using docker exec -it bash [--login]\nFor example, using this Dockerfile:\nFROM alpine\n\nRUN apk add bash\nRUN echo \"export MYVAR=frombashrc\" > /root/.bashrc\nRUN echo \"export MYVAR=fromprofile\" > /root/.bash_profile\n\nENTRYPOINT [\"/bin/sh\", \"-c\"]\nAnd running:\n$ docker build . -t mybashimage\n$ docker run -it --name bashcontainer mybashimage \"bash --login -c 'env && sleep 60'\"\nHOSTNAME=4aeb776a8c56\nMYVAR=fromprofile\n...\nIn another shell while container is running:\n# Running a non-login shell does not have same effect\n$ docker exec -it bashcontainer bash\nbash-4.4# env\nHOSTNAME=5f44398152bf\nMYVAR=frombashrc\n...\n\n# Running login shell doe\n$ docker exec -it bashcontainer bash --login -c 'env'\nHOSTNAME=5f44398152bf\nMYVAR=fromprofile\n...",
    "docker build: Returned a non-zero code: 5": "It is most likely failing on the wget command. That RUN line has 6 separate commands. The other 5 would all likely produce some error message if they failed. The wget has the --quiet flag passed which is suppressing output.\nTo debug further, try removing the --quiet flag from the wget.\nFrom man wget, EXIT STATUS of 5 = SSL verification error. Perhaps you are behind a corporate proxy and do not have the SSL certs installed in the container that is running the wget.",
    "Why does docker-compose create an empty host folder when mounting named volume?": "As an implementation detail, Docker actually uses the Linux kernel filesystem mount facility whenever it mounts a volume. To mount a volume it has to be mounted on to a directory, so if the mount target doesn't already exist, it creates a new empty directory to be the mount point. If the mount point is itself inside a mounted volume, you'll see the empty directory get created, but the mount won't get echoed out.\n(If you're on a Linux host, try running mount in a shell while the container is running.)\nThat is:\n/container_root/app is a bind mount to /host_path/app; they are they same underlying files.\nmkdir /container_root/app/node_modules creates /host_path/app/node_modules too.\nMounting something else on /container_root/app/node_modules doesn't cause anything to be mounted on /host_path/app/node_modules.\n...which leaves an empty /host_path/app/node_modules directory.\nThe first time you start a container, and only then, if you mount an empty volume into a container, the contents from the image get copied into the volume. You're telling Docker this directory contains critical data that needs to be persisted for longer than the lifespan of the container. It is not a magic \"don't use the host directory volume\" knob, and if you do things like change your package.json file, Docker will not update the contents of this volume.",
    "'RUN pip install -r requirements.txt' not working": "Several issues I'm seeing:\nThe ADD command you use creates a file called ToDoApp/ToDoApp, it doesn't even create a sub directory.\nADD is unneeded (you're not extracting a tar or downloading from a URL) so that can switch to a COPY.\nYou need to copy your code.\nThe RUN commands can be reordered for better cache efficiency.\nUse relative paths and the WORKDIR correctly.\nArgs need to be separated when you use the json syntax\nThe resulting Dockerfile looks like:\nFROM python:3\n\n#set envionment variables\nENV PYTHONUNBUFFERED 1\n\n# run this before copying requirements for cache efficiency\nRUN pip install --upgrade pip\n\n#set work directory early so remaining paths can be relative\nWORKDIR /ToDoApp\n\n# Adding requirements file to current directory\n# just this file first to cache the pip install step when code changes\nCOPY requirements.txt .\n\n#install dependencies\nRUN pip install -r requirements.txt\n\n# copy code itself from context to image\nCOPY . .\n\n# run from working directory, and separate args in the json syntax\nCMD [\"python\", \"./manage.py\", \"runserver\", \"0.0.0.0:8000\"]",
    "Docker Build Error : The command '/bin/sh -c apt-get install \u2013y apache2' returned a non-zero code: 100": "I'm not sure how, but I think you're using an en dash instead of a hyphen in front of your y.\nYou want -y rather than \u2013y\nIf you look closely there's a subtle difference.",
    "What is the purpose of Dockerfile command \"Volume\"?": "Defining a volume in a Dockerfile has the advantage of specifying the volume location inside the image definition as documentation from the image creator to the user of the image. That's just about the only upside.\nIt was added to docker very early on, quite possibly when data containers were the only way to persist data. We now have a solution for named volumes that has obsoleted data containers. We have also added the compose file to define how containers are run in an easy to understand and reuse syntax.\nWhile there is the one upside of self documented images, there are quite a few downsides, to the point that I strongly recommend against defining a volume inside the image to my clients and anyone publishing images for general reuse:\nThe volume is forced on the end user, there's no way to undefine a volume in the image.\nIf the volume is not defined at runtime (with a -v or compose file), the user will see anonymous volumes in their docker volume ls that have no association to what created them. These are almost always useless wastes of disk space.\nThey break the ability to extend the image since any changes to a volume in an image after the VOLUME line are typically ignored by docker. This means a user can never add their own initial volume data, which is very confusing because docker gives no warning that it is ignoring the user changes during the image build.\nIf you need to have a volume as a user a runtime, you can always define it with a -v or compose file, even if that volume is not defined in the Dockerfile. Many users have the misconception that you must define it in the image to be able to make it a named volume at runtime.\nThe ability to use --volumes-from is unaffected by defining the volume in the image, but I'd encourage you to avoid this capability. It does not exist in swarm mode, and you can get all the same capabilities along with more granularity by using a named volume that you mount in two containers.",
    "How do you kill a docker containers default command without killing the entire container?": "If you need to restart the process that's running the container, then simply run a:\ndocker restart $container_name_or_id\nExec'ing into a container shouldn't be needed for normal operations, consider that a debugging tool.\nRather than changing the script that gets run to automatically restart, I'd move that out to the docker engine so it's visible if your container is crashing:\ndocker run --restart=unless-stopped ...\nWhen a container is run with the above option, docker will restart it for you, unless you intentionally run a docker stop on the container.\nAs for why killing pid 1 in the container shuts it down, it's the same as killing pid 1 on a linux server. If you kill init/systemd, the box will go down. Inside the namespace of the container, similar rules apply and cannot be changed.",
    "How to Dockerize a tomcat app": "Looking at your code this is what I could gleam:\nYou have some java files stored in current directory (.)\nWhen you call COPY you copy all these contents to /app_name\nYou create a .war on the file\nThere are some things to note, first is that the app_name.war is not on the host disk, it is currently inside of the docker file system. What this means is that you cannot COPY the .war.\nWhat you are really after is this: RUN cp app_name.war  $CATALINA_BASE/webapps/app_name.war\nThis would look like the following: Dockerfile\nFROM tomcat:6\nENV APP_ROOT /app_name\nRUN apt-get update && apt-get install -y default-jdk\nCOPY . $APP_ROOT/\nWORKDIR $APP_ROOT\nRUN jar -cvf app_name.war *\nRUN cp app_name.war $CATALINA_BASE/webapps/app_name.war\nAdding the docker COPY reference here as it explains the command in detail. It might also be helpful for you to make a script called provision.sh, then do something like:\nCOPY provision.sh /tmp/provision.sh\nRUN sh /tmp/provision.sh\nThat way you can put all your building, configuring and other in a single script that you can test locally (again if it helps)\nEDIT: Adding mention about building locally and copying into dockerfile\nYou can build the .war on your machine, use COPY to put is on the machine. Dockerfile\nFROM tomcat:6\nENV APP_ROOT /app_name\nRUN apt-get update && apt-get install -y default-jdk\n\nCOPY app_name.war $CATALINA_BASE/webapps/app_name.war\nWORKDIR $APP_ROOT\nThe above copies the file app_name.war then add it to the filesystem of the container at the path $CATALINA_BASE/webapps/app_name.war. So for that you do this:\nBuild the .war on your machine with java\nPut .war in directory with Dockerfile\nCOPY app_name.war into the container's filesystem",
    "If docker uses the underlying linux os, why specify the OS in the FROM line of a Dockerfile": "I'm confused, though, as to why most Dockerfiles specify the OS in the FROM line of the Dockerfile. I thought that as it was using the underlying OS, then the OS wouldn't have to be defined.\nI think your terminology may be a little confused.\nDocker indeed uses the host kernel, because Docker is nothing but a way of isolating processes running on the host (that is, it's not any sort of virtualization, and it can't run a different operating system).\nHowever, the filesystem visible inside the container has nothing to do with the host. A Docker container can run programs from any Linux distribution. So if I am on a Fedora 24 Host, I can build a container that uses an Ubuntu 14.04 userspace by starting my Dockerfile with:\nFROM ubuntu:14.04\nProcesses running in this container are still running on the host kernel, but there entire userspace comes from the Ubuntu distribution. This isn't another \"operating system\" -- it's still the same Linux kernel -- but it is a completely separate filesystem.\nThe fact that my host is running a different kernel version than maybe you would find in an actual Ubuntu 14.04 host is almost irrelevant. There are going to be a few utilities that expect a particular kernel version, but most applications just don't care as long as the kernel is \"recent enough\".\nSo no, there is no virtualization in Docker. Just various (processes, filesystem, networking, etc) sorts of isolation.",
    "Docker Compose port issue. Cannot launch docker project on localhost": "The service you're running is only listening to the containerlocalhost interface, so nothing outside the container can access it. It needs to listen on 0.0.0.0.",
    "ASPNET Core 3.1 - Dockerfile changes don\u00b4t work when run in Visual Studio": "Nothing like reading the official documentation...\nhttps://learn.microsoft.com/en-us/visualstudio/containers/container-build?view=vs-2019#debugging\nDebugging\nWhen building in Debug configuration, there are several optimizations that Visual Studio does that help with the performance of the build process for containerized projects. The build process for containerized apps is not as straightforward as simply following the steps outlined in the Dockerfile. Building in a container is much slower than building on the local machine. So, when you build in the Debug configuration, Visual Studio actually builds your projects on the local machine, and then shares the output folder to the container using volume mounting. A build with this optimization enabled is called a Fast mode build.\nIn Fast mode, Visual Studio calls docker build with an argument that tells Docker to build only the base stage. Visual Studio handles the rest of the process without regard to the contents of the Dockerfile. So, when you modify your Dockerfile, such as to customize the container environment or install additional dependencies, you should put your modifications in the first stage. Any custom steps placed in the Dockerfile's build, publish, or final stages will not be executed.\nThis performance optimization only occurs when you build in the Debug configuration. In the Release configuration, the build occurs in the container as specified in the Dockerfile.\nI put RUN touch test.txt and ENV my_variable value in the first lines (where Dockerfile build base stage) and it works.\nA tip refers to RUN touch test.txt, this needs to be executed (WORKDIR) in another folder different from the folder mapped with the source code in the hostlocal (/app).\nExample:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\nWORKDIR /tmp\nRUN touch teste.txt\nWORKDIR /app\nENV my_variable value\nEXPOSE 80\nEXPOSE 443\nMy final Dockerfile is:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\nWORKDIR /tmp\nRUN touch teste.txt\nRUN echo \"teste1\" > teste1.txt\nWORKDIR /app\nENV my_variable value\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1-buster AS build\nWORKDIR /src\nCOPY [\"MyTestProject/TestProject.csproj\", \"MyTestProject/\"]\nRUN dotnet restore \"MyTestProject/TestProject.csproj\"\nCOPY . .\nWORKDIR \"/src/MyTestProject\"\nRUN dotnet build \"TestProject.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"TestProject.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENV my_variable value\nRUN touch teste.txt\nENTRYPOINT [\"dotnet\", \"TestProject.dll\"]",
    "Docker Swarm : docker stack deploy results on untagged images <none>": "This is normal. When using Swarm Stacks, it converts your tag into the SHA256 of that image and downloads that specific ID. This guarantees each node has the same image. If it only depended on the tag, you could potentially re-use the tag in a docker push, and it's possible for different nodes to download different images.\nYou can see the tag used in a docker service ls or docker stack ps <stackname>",
    "File Upload by docker container": "Problem\nYou're expecting your code inside the docker container to locate a file in your host's filesystem. A docker container is a fully contained(as the name suggests) runtime environment with its private filesystem, network, etc. You can not directly access the host's filesystem from your docker container unless you have some requisite configurations (mounting) in your docker run .. command or the compose file.\nSolution\nYou'll have to mount the volume /home/storage inside the docker container to read/write from/to that location from within the container. To do so, after changing the filesystem permissions in your host (which you have already), use the following run command to start the container:\ndocker run -d --name file-upload-service -v /home/storage:/home/storage -p 9104:9104 file-upload-service:latest\nThe -v flag in the above command tells docker to mount the /home/storage directory of the host machine (the left side of :) to the /home/storage directory of the container (the right side of :). You can change these directories according to your use case.",
    "run neo4j with docker-compose - neo4j not accessible from localhost:7474": "Adding network_mode: \"bridge\" to the docker-compose.yml file and accessing to the docker-machine ip the image works correctly\ndocker-compose.yml\nversion: '3'\n\nservices:\n  neo4j:\n    image: neo4j:latest\n    network_mode: \"bridge\"\n    ports:\n      - \"7474:7474\"\n      - \"7687:7687\"\n    environment:\n      - NEO4J_dbms_security_procedures_unrestricted=apoc.*\n      - NEO4J_apoc_import_file_enabled=true\n      - NEO4J_dbms_shell_enabled=true\n    volumes:\n      - ./plugins:/plugins\n      - ./data:/data\n      - ./import:/import",
    "\"From ubuntu\" in docker container": "The statment FROM ubuntu:14.04 means use the ubuntu image as a base image. The ubuntu image is not an OS. This image \"mimics\" an Ubuntu OS, in the sense that it has a very similar filesystem structure to an Ubuntu os and has many tools available that are typically found on Ubuntu.\nThe main and fundamental difference is that the Docker Ubuntu image does not have it own linux kernel. It uses the kernel of the host machine where the container is running.\nMoreover, the size difference between the Docker image (73MB) and an Ubuntu iso(around 1Gb) is very significant.",
    "How to add a custom CA-Certificate on an extended (node.js) docker image": "Why not just switch user to root to run the command to add the cert then switch back?\nFROM nodered/node-red-docker\n\nADD DigiCertCA.crt /usr/local/share/ca-certificates/\nUSER root\nRUN update-ca-certificates\nUSER node-red\n\n\nADD settings.js /data/settings.js\n\nRUN npm install node-red-contrib-ttn\nRUN npm install node-red-contrib-influxdb\nRUN npm install node-red-admin\nRUN npm install node-red-node-geohash \n\nCMD [\"npm\", \"start\", \"--\", \"--userDir\", \"/data\"]",
    "docker: failed to register layer: re-exec error: exit status 1: output: ProcessBaseLayer . The system cannot find the path specified": "You have selected Windows Containers at the time of setup rather than Linux Containers so only Windows-based images will run. To resolve this, right-click the tray icon and select 'Switch to Linux Containers' and it will start working.",
    "Neo4j + Docker - unable to create JVM": "I ran into this same issue. Turned out to be a the line endings on the neo4j.conf file. I used the VS code to switch the line endings to 'LF' and ran docker-compose up and everything worked out. Hope this helps.\nVisual Studio Code: How to show line endings",
    "Prevent volume creation on docker run": "Not once it's be added to the Dockerfile. I'd personally avoid doing volumes inside of the Dockerfile since it ends up creating anonymous volumes as you've seen, but also because it breaks attempts to modify that directory in child images or even later steps inside of the same Dockerfile.",
    "\"ACCES: permission denied\" while trying to upload a file on Strapi": "I cannot really replicate this problem, but as a Strapi and Docker enthusiast I can think of some of the causes.\nIn Strapi you have to generate a token which you'll use to run requests towards the endpoints. You can have either a public token (only GET requests, meant for the public) and a private token (meant to upload, modify or delete stuff). So, probably, you are using a bad token.\nOr, the FTP system where you are trying to upload the file has some permission problems on the output folder. Maybe the FTP is inside the Docker Container itself? In this case, it could be easy to fix. Entering the container with the docker exec -it <container_id> /bin/bash command, you can give -R 777 permissions on the output folder (which is /opt/app/public/uploads as it's printed in logs).\nProbably the problem is this line of code in your Dockerfile:\nRUN chown -R node:node /opt/app\nMaybe this node is not the user who is uploading files to /opt/app? Or you have to launch this command with sudo? (I know it's nearly impossible inside a Dockerfile)\nAlso, as I saw in this post, it could be a problem of how you installed npm dependencies. Try running sudo npm install -g --unsafe-perm=true --allow-root in the Dockerfile.\nHope it was useful enough!",
    "How to build docker image for multiple platforms with cross-compile?": "Both of your images were built for your build platform rather than the target platform with this line:\nFROM --platform=$BUILDPLATFORM ...\nYou don't want --platform=$BUILDPLATFORM on the target stage. This is used on the intermediate stages that either cross compile or output non-platform specific results that you can copy into the target stage.\nMore details on these variables defined by buildkit is available at: https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope\nI'd also change your RUN step to a COPY step, which eliminates the need for any emulation on the build server. This means running:\necho \"console.log('hello world!');\" > main.js\nin the same directory with your Dockerfile, to create the main.js file in your build context. Then change the Dockerfile to have:\nFROM node:18-alpine as node\nWORKDIR /usr/src/app\nCOPY main.js .\nEXPOSE 4000\nCMD [ \"node\", \"main.js\" ]\nWith that you can build multi-platform images using your original docker buildx build command.",
    "Docker COPY recursive --chmod --chown": "Short answer: You have to actually give exec permissions to your folder and files by replacing your chmod with --chmod=700. And you can look up more details below or just google `Linux file permissions.\nLong answer: So i recreated your example and as you can see bellow i have some scripts that i'm running in various stages before my server actually starts. I'm following good production docker practices as you do i can see and i also use your method to change permissions on the scripts file.\n    \n# App files\nCOPY --chown=node:node --from=builder /usr/src/app/build ./build\n# Migration files\nCOPY --chown=node:node ./migrations ./migrations\n# Scripts\nCOPY --chown=node:node --chmod=600 ./scripts ./scripts\n# Client\nCOPY --chown=node:node ./client ./build/client\nEXPOSE 80\nAnd no surprise, i get a similar error.\n    sh: ./scripts/wait-for.sh: Permission denied\n    npm ERR! code ELIFECYCLE\n    npm ERR! errno 126\n    npm ERR! webpage-server@0.0.1 preprod: `./scripts/wait-for.sh page-db:5432 -- npm run migrate -- --config build/src/config/database.json -e production `\n    npm ERR! Exit status 126\nThe problem here is the code you are giving to your chmod. Your goal is to make them executable and for security reasons make them executable for only the node(default) docker user. Now these decimal numbers we give to our chmod are just sugar to our eyes, in reality, these will be converted to binary and assigned to every one of the permission variables for the directory and its child files/folders(in this case). You have 4 values in the Linux file system permissions, one bit to indicate file or directory, 3 bit to define user access to file/dir, another 3 for the group of users and 3 more for everyone else, you want to give execute permissions to the current user so you will need 111=7 so the final chmod will look more like this:\n    # App files\n    COPY --chown=node:node --from=builder /usr/src/app/build ./build\n    # Migration files\n    COPY --chown=node:node ./migrations ./migrations\n    # Scripts\n    COPY --chown=node:node --chmod=700 ./scripts ./scripts\n    # Client\n    COPY --chown=node:node ./client ./build/client\n    EXPOSE 80\nAnd as you can see the problem no longer persists\n    > webpage-server@0.0.1 preprod /usr/src/app\n    > ./scripts/wait-for.sh page-db:5432 -- npm run migrate -- --config build/src/config/database.json -e production \n    \n    \n    > webpage-server@0.0.1 migrate /usr/src/app\n    > db-migrate up \"--config\" \"build/src/config/database.json\" \"-e\" \"production\"\n    \n    [INFO] No migrations to run\n    [INFO] Done\n    \n    > webpage-server@0.0.1 prod /usr/src/app\n    > npm run start\n    \n    \n    > webpage-server@0.0.1 start /usr/src/app\n    > node build/src/server.js\n    \n    18:28:17 info: Initializing thanos webpage Server version: 0.0.1. Environment: development\n    18:28:17 info: /usr/src/app/build/src\n    18:28:17 info: Thanos web page server is listening on port -> 80",
    "Rust in Docker image: exec no such file or directory": "My problem was, that I need to make sure that Rust compiles into a static binary. It looks like MUSL is one way to do that.\nThis is now my updated Dockerfile:\n# Build: docker build --platform linux/arm64/v8 -t dasralph/ping:arm64_0.1.0 --push .\n# Run: docker run -p 8080:8080 ping\n# Test: curl http://localhost:8080/\n\n# STAGE 1 is to build the binary\n# Use rust-based image for container\nFROM rust:1.61.0-alpine AS builder\n\n# Adding necessary packages\nRUN apk update\nRUN apk add pkgconfig openssl openssl-dev musl-dev\n\nRUN rustup target add aarch64-unknown-linux-musl\nRUN rustup toolchain install stable-aarch64-unknown-linux-musl\n\n# Set working directory in container; make directory if not exists\nRUN mkdir -p /usr/src/ping\nWORKDIR /usr/src/ping\n\n# Copy all files from local computer to container\nCOPY Cargo.toml .\nCOPY Cargo.lock .\nCOPY .env.docker .env\nCOPY src src\n\n# Build release application\nRUN cargo build --target aarch64-unknown-linux-musl --release\n\n\n# STAGE 2 is to have smallest image possible by including only necessary binary\n# Use smallest base image\nFROM shinsenter/scratch\n\n# Copy application binary from STAGE 1 image to STAGE 2 image\nCOPY --from=builder /usr/src/ping/target/aarch64-unknown-linux-musl/release/ping /\n\nEXPOSE 8080\n\nENTRYPOINT [\"/ping\"]",
    "How can I ignore dockerfile non-zero return codes?": "You could use || to bypass the exit code. Something like next:\nOld Dockerfile:\nFROM ubuntu:16.04\nRUN lll\nRUN pwd\nOld execution:\n$ docker build -t abc:1 .\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM ubuntu:16.04\n ---> 065cf14a189c\nStep 2/3 : RUN lll\n ---> Running in b7f9a2fd7f6d\n/bin/sh: 1: lll: not found\nThe command '/bin/sh -c lll' returned a non-zero code: 127\nNew Dockerfile:\nFROM ubuntu:16.04\nRUN lll || :\nRUN pwd\nNew execution:\n$ docker build -t abc:1 .\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM ubuntu:16.04\n ---> 065cf14a189c\nStep 2/3 : RUN lll || :\n ---> Running in 9a0b3ebea003\n/bin/sh: 1: lll: not found\nRemoving intermediate container 9a0b3ebea003\n ---> af76014cf9aa\nStep 3/3 : RUN pwd\n ---> Running in a4766055c81e\n/\nRemoving intermediate container a4766055c81e\n ---> c82c077ed8ea\nSuccessfully built c82c077ed8ea\nSuccessfully tagged abc:1\nExplain for cmd1 || cmd2:\nIf cmd1 exit code is zero, the cmd2 will not execute, if cmd1 exit code is non-zero, the cmd2 will run. Here, : means a empty command which will result in zero exit code, then it will cheat docker build to let it not exit.\nSo, for you it could be:\nRUN mega-login ${email} ${password} || :",
    "docker-compose python: can't open file './main.py': [Errno 2] No such file or directory": "The problem is in the lines.\nCOPY . /proxy-scraper-checker-master\nCMD [ \"python\", \"main.py\" ]\nYou are copying the entire directory into /proxy-scraper-checker-master, so then your main.py file would be /proxy-scraper-checker-master/proxy-scraper-checker-master/main.py.\nTo debug this, you can enter a bash terminal within the container and look around the directory structure to find main.py. docker-compose will have built the image, so you can find the image name with docker images, or you can rebuild it.\njakub@dash:/tmp/so$ docker build --tag my_python .\njakub@dash:/tmp/so$ docker run --rm -it my_python bash\n# At this point, we are inside the Docker container.\nroot@924a7f854119:/proxy-scraper-checker-master# pwd\n/proxy-scraper-checker-master\nroot@924a7f854119:/proxy-scraper-checker-master# ls\nDockerfile  docker-compose.yml  proxy-scraper-checker-master  requirements.txt\nroot@924a7f854119:/proxy-scraper-checker-master# realpath proxy-scraper-checker-master/main.py \n/proxy-scraper-checker-master/proxy-scraper-checker-master/main.py\nAt this point, we have found the path to main.py. To fix the original issue, we can change the CMD in the Dockerfile to the following:\nCMD [ \"python\", \"/proxy-scraper-checker-master/proxy-scraper-checker-master/main.py\" ]\nWe can improve the Dockerfile to remove redundant COPY instructions. Using the below Dockerfile, the absolute path to main.py is /app/proxy-scraper-checker-master/main.py. We can reference it with the relative path proxy-scraper-checker-master/main.py because we are currently in /app (thanks to the WORKDIR instruction). We could also reference it with the absolute path.\nFROM python:3.8-slim\n\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends libc-dev\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install --no-cache-dir -r requirements.txt\n\nCMD [ \"python\", \"proxy-scraper-checker-master/main.py\" ]\nI recommend reading the COPY documentation to understand its behavior.",
    "Docker CMD - when should shell form be used?": "Shell form will invoke a command shell and do the usual command processing that the shell typically handles (like substitution of environment variables such as $HOME). The exec form doesn't do that.\nThat is closely related to the SHELL directive.\nYou can have multiple SHELL commands in the Dockerfile, but only one CMD. CMD is used to specify what the container should run when it starts. The SHELL directive will overwrite the default shell that is used by the shell-form of various commands (RUN, CMD, ENTRYPOINT).\nUsing this Dockerfile illustrates this better than I could explain it:\nFROM python:3.6\nRUN echo $PATH\nSHELL [\"/bin/bash\" ,\"-c\"]\nRUN echo $PATH\nRUN [\"echo\", \"$PATH\"]\nCOPY run.sh /run.sh\n\nENTRYPOINT [\"/run.sh\"]\nWill result in this when running docker build:\n$ docker build .\n\nSending build context to Docker daemon   5.12kB\nStep 1/7 : FROM python:3.6\n ---> 5bf410ee7bb2\nStep 2/7 : RUN echo $PATH\n ---> Running in 3a08d7c4450c\n/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nRemoving intermediate container 3a08d7c4450c\n ---> 85b4da5d8e5d\nStep 3/7 : SHELL [\"/bin/bash\" ,\"-c\"]\n ---> Running in da1b90ac14f2\nRemoving intermediate container da1b90ac14f2\n ---> ed747f0862a6\nStep 4/7 : RUN echo $PATH\n ---> Running in 5c6a86e133ff\n/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nRemoving intermediate container 5c6a86e133ff\n ---> 8ec42f23d390\nStep 5/7 : RUN [\"echo\", \"$PATH\"]\n ---> Running in cc0650a6d8e8\n$PATH\nRemoving intermediate container cc0650a6d8e8\n ---> 8b11432adb3a\nStep 6/7 : COPY run.sh /run.sh\n ---> a168c58738e7\nStep 7/7 : ENTRYPOINT [\"/run.sh\"]\n ---> Running in f9e28048d139\nRemoving intermediate container f9e28048d139\n ---> d20920ea562c\nSuccessfully built d20920ea562c\nNotice that when it ran the shell mode commands (using both the default shell and bash), $PATH was expanded, but not when run using exec mode.",
    "Can't modify files created in docker container": "By default, Docker containers runs as root. This has two issues:\nIn development as you can see, the files are owned by root, which is often not what you want.\nIn production this is a security risk (https://pythonspeed.com/articles/root-capabilities-docker-security/).\nFor development purposes, docker run --user $(id -u) yourimage or the Compose example given in the other answer will match the user to your host user.\nFor production, you'll want to create a user inside the image; see the page linked above for details.",
    "Can't install Python math library in Docker image": "math is a built in library for python. You don't need to install it. Just remove it from requirements.txt.\nIf you are having a similar problem importing other Python modules in a Docker image using the requirements.txt file, make sure it is not one of Python's many, many other built-in functions. The complete list is here: https://docs.python.org/3.8/py-modindex.html\n(Select the appropriate version from the dropdown menu at the top of the page.)",
    "Docker images lacking /tmp?": "I had the same error as you and as BMitch suggested, adding this layer in my Dockerfile fixed it for me.\nRUN chmod 1777 /tmp",
    "Running shell script while building docker image": "You must copy your install.sh into docker image with this command in your dockerfile:\nCOPY install.sh /tmp\nThen use your RUN command to run it:\nRUN /bin/sh -c \"/tmp/install.sh\"\nor\nRUN sh /tmp/install.sh\nDon't forget to make install.sh executable before run it:\nchmod +x /tmp/install.sh",
    "Trying to install dependencies using dep in docker": "Here is an example of Dockerfile with dep:\nFROM golang:latest \n\nLABEL version=\"1.0\"\n\nRUN mkdir /go/src/app\n\nRUN go get -u github.com/golang/dep/cmd/dep\n\nADD ./main.go /go/src/app\nCOPY ./Gopkg.toml /go/src/app\n\nWORKDIR /go/src/app \n\nRUN dep ensure \nRUN go test -v \nRUN go build\n\nCMD [\"./app\"]",
    "How to dynamically change content in node project run through docker": "You are correct, you should use volumes for stuff like this. During development, give it the same volumes as the COPY directories. It'll override it with whatever is on your machine, no need to rebuild the image, or even restart the container. Perfect for development.\nWhen actually baking your images for production, you remove the volumes, leave the COPY in, and you'll get a deterministic container. I would recommend you read through this article here: https://docs.docker.com/storage/volumes/.\nIn general, there are 3 ways to do volumes.\nDefine them in your dockerfile using VOLUME.\nPersonally, I've never done this. I don't really see the benefits of this against the other two methods. I believe it would be more common to do this when your volume is meant to act as a permanent data-store. Not so much when you're just trying to use your live dev environment.\nDefine them when calling docker run.\ndocker run ... -v $(pwd)/src:/usr/src/app ...\nThis is great, cause if your COPY in your dockerfile is ./src /usr/src/app then it temporarily overrides the directory while running the image, but it's still there for deployment when you don't use -v.\nUse docker-compose.\nMy personal recommendation. Docker compose massively simplifies running containers. For sake of simplicity just calls docker run ... but automates the arguments based on a given docker-compose.yml config.\nCreate a dev service specifying the volumes you want to mount, other containers you want it linked to, etc. Then bring it up using docker-compose up ... or docker-compose run ... depending on what you need.\nSmart use of volumes will DRAMATICALLY reduce your development cycle. Would really recommend looking into them.",
    "Why does my Docker container not return control to the terminal after starting?": "You are not running the docker container as a daemon.\ndocker run -d bot\nIn my experience, print messages don't make it to the logs without input buffering disabled in python.\npython -u jbot.py",
    "Php7 Redis Client on Alpine OS": "For versions of Alpine prior to 3.6, such as the current official PHP Alpine image (Alpine 3.4), you need to build the extension from source. There are a few dependencies you also need to do that: autoconf, git, gcc/g++, and make. As an example, this is a complete Dockerfile for the latest stable release of PHP built on Alpine with the redis extension for php7 installed and enabled:\nFROM php:alpine\n\nRUN apk add --no-cache autoconf git g++ make\n\nRUN \\\n  git clone https://github.com/phpredis/phpredis.git && \\\n  cd phpredis && \\\n  git checkout php7 && \\\n  phpize && \\\n  ./configure && \\\n  make && make install && \\\n  docker-php-ext-enable redis\nIf you want a smaller image you can remove the phpredis directory and the deps that were needed to clone and build it afterward. If you're not using an official PHP image then you will need to replace docker-php-ext-enable redis with a couple of commands to move the redis.so where you need it and add the extension=redis.so line to your PHP config.",
    "How to provide and use command line arguments in docker build?": "If this is purely a build-time variable, you can use the --build-arg option of docker build.\nThis flag allows you to pass the build-time variables that are accessed like regular environment variables in the RUN instruction of the Dockerfile. Also, these values don\u2019t persist in the intermediate or final images like ENV values do.\ndocker build --build-arg hibernate_path=/a/path/to/hibernate -t tag .\nIn 1.7, only the static ENV Dockerfile directive is available.\nSo one solution is to generate the Dockerfile you need from a template Dockerfile.tpl.\nDockerfile.tpl:\n\n...\nENV hibernate_path=xxx\nADD xxx/hibernate.cfg.xml /usr/share/tomcat7/webapps/roc_client/WEB-INF/classes/\n...\nWhenever you want to build the image, you generate first the Dockerfile:\nsed \"s,xxx,${hibernate_path},g\" Dockerfile.tpl > Dockerfile\nThen you build normally: docker build -t myimage .\nYou then benefit from (in docker 1.7):\nbuild-time environment substitution\nrun-time environment variable.",
    "Sending signals to Golang application in Docker": "You are running your server inside a shell, and the shell is the process receiving the signals. Your server doesn't exit until you force the shell to quit.\nWhen you use the \"shell\" form of CMD, it starts your server as an argument to /bin/sh -c. In order to exec the server binary directly, you need to provide an array of arguments to either CMD or ENTRYPOINT, starting with the full path of the executable.\nCMD [\"/go/bin/simple_server\"]\nA note from ENTRYPOINT in the Dockerfile docs:\nThe shell form prevents any CMD or run command line arguments from being used, but has the disadvantage that your ENTRYPOINT will be started as a subcommand of /bin/sh -c, which does not pass signals.",
    "The difference between RUN in a Dockerfile and command in a docker-compose.yml file": "The Dockerfile is a receipt on how to build a new image with e.g. docker build, while docker-compose is used to orchestrate starting (multiple) containers.\nThe RUN directive in an Dockerfile is executed during the build phase of an image and its result get committed to the image. The command attribute in a docker-compose.yml corresponds to the CMD directive in a Dockerfile and to the optional command parameter of docker run and specifies the command to be executed when starting a new container based on the given image.\nSee also: Difference between RUN and CMD in a Dockerfile",
    "docker cant find file in parent folder?": "Your path is wrong. By writing COPY ./myapi.sln . you assume that myapi.sln is in the same folder than the Dockerfile which is not the case.\nBest practice is to put the Dockerfile at the root of your project so you can easily access all your files.\nIn anyway, you can't access a parent directory with absolute or relative path.\nFrom the documentation:\nThe path must be inside the context of the build; you cannot COPY ../something/something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.",
    "How to forward a UDP port from a devcontainer?": "The documentation is kinda fuzzy regarding protocols used for exposed ports with forwardPorts tag:\nforwardPorts\narray\nAn array of ports that should be forwarded from inside the container to the local machine.\nBecause of that I would recommend trying to use the older appPort tag because when I looked here it said they're using the docker-compose syntax which means you can specify the ports and protocol like so:\ndocker-compose:\nports:\n- \"3000\"\n- \"8921:5000\"\ndevcontainers.json:\n\"appPort\": [ 3000, \"8921:5000\" ]\nAnd in your case:\n\"appPort\": [ \"1117:1117/udp\" ]\nAccording to the docs, using the appPort tag is also useful when you want to access the application (or expose the ports) to more than just localhost",
    "java.lang.IllegalStateException: Previous attempts to find a Docker environment failed. Will not retry. Please see logs and check configuration": "For anyone finding this question because of the irritating behaviour of \"Will not retry\", there is a simple workaround.\nTestcontainers caches the failure to find a Docker environment, and if you are using gradle or another build tool which keeps long-running daemons around, this cached failure will stick around basically forever.\nRunning gradle --stop (see https://docs.gradle.org/current/userguide/gradle_daemon.html#sec:stopping_an_existing_daemon) will stop any long-running gradle daemons and will effectively clear the cached failure. You may be using a gradle wrapper, in which case you're looking for ./gradlew --stop.\nYou can also use gradle --status to view running daemons.\nOther build tools like Takari or mvnd (Maven Daemon) will have their own ways of stopping the long-running daemons. For mvnd, the command is mvnd --stop.\nReferences and other reading:\nThe original PR where this behaviour was introduced: https://github.com/testcontainers/testcontainers-java/pull/456\nSome discussion about this issue on Github: https://github.com/testcontainers/testcontainers-java/issues/6441",
    "Dockerfile: Python.h: No such file or directory": "You should add python3-devel package that contains absent headers.\nFROM amazonlinux:latest\n\n\nRUN yum -y install which unzip aws-cli \\\n    && yum install -y python3-pip python3 python3-setuptools \\\n    && yum install -y python3-devel.x86_64 \\\n    && yum install -y tar.x86_64 \\\n    && DEBIAN_FRONTEND=noninteractive yum install -y ksh\n\n\nRUN pip3 install boto3 \\\n    && yum install -y gcc \\\n    && pip3 install s3fs",
    "How to get an interactive bash shell in a Docker container": "You are in fact running an interactive bash with commands like:\ndocker container run -it ubuntu /bin/bash\nWhen you run bash in a docker container, that shell is in a container. So it won't have the command history from outside of the container, that history is maintained on the host filesystem. It also won't have your prompt, the PS1 variable is not automatically exported into the container's environment. And it won't have your .bashrc configuration from your host, since that isn't inside the container. Instead you get a bash shell that is out of the box from a minimal ubuntu host.",
    "How to Add user when creating docker image from alpine base image": "I'd start with first checking if the www-data user even exits in the image. Execute in the running container something like:\nsudo cat /etc/passwd | grep www-data\nIf the user does exist then add the USER www-data directive to the Dockerfile after all commands that do installs, create directories, change permissions, etc. It would be required to also add USER 0 at the beginning to switch to the root user for those commands if the base image doesn't run as root. Looking at the Dockerfile I'd suggest to add USER www-data before the CMD directive.\nIf the www-data user doesn't exist then it has to be added first. The commands for Alpine Linux are addgroup and adduser. Something like these if the user id for www-data is to be 33 and the group it belongs to is also named www-data and has id of 33:\nRUN addgroup -S -g 33 www-data \\\n && adduser -S -D -u 33 -s /sbin/nologin -h /var/www -G www-data www-data\nAdd the above just before RUN chown -R www-data:www-data /var/www/, or make it a single RUN directive:\nRUN addgroup -S -g 33 www-data \\\n && adduser -S -D -u 33 -s /sbin/nologin -h /var/www -G www-data www-data \\\n && chown -R www-data:www-data /var/www/",
    "Dockerfile with entrypoint only from base image": "Several of the Dockerfile directives (notably ENTRYPOINT and CMD, but also EXPOSE, LABEL, and MAINTAINER) just set metadata in the image; they don't really do anything themselves. Within a single Dockerfile this will work just fine:\nFROM ubuntu:18.04\nWORKDIR /app\n# Just remembers this in the image metadata; doesn't actually run it\nCMD [\"/app/main.sh\"]\n# ...we should actually copy the file in too\nCOPY main.sh /app\nWhen you have one Dockerfile built FROM another image it acts almost entirely like you ran all of the commands in the first Dockerfile, then all of the commands in the second Dockerfile. Since CMD and ENTRYPOINT just set metadata, the second image inherits this metadata.\nBuilding and running an image are two separate steps. In the example you show, the COPY directive happens during the docker build step, and the base image's command doesn't take effect until the later docker run step. (This is also true in Docker Compose; a common question is about why Dockerfile steps can't connect to other containers declared in the Compose YAML file.)\nThere is one exception, and it's around ENTRYPOINT. If you have a base image that declares an ENTRYPOINT and a CMD both, and you redeclare the ENTRYPOINT in a derived image, it resets the CMD as well (very last paragraph in this section). This usually isn't a practical problem.",
    "node:latest for alpine, apk not found because sbin is not on path": "node:latest is based on buildpack-deps, which is based on Debian. Debian does not use apk; it uses apt. You either want to use Debian's apt to install packages (apt-get install tzdata) or switch to node:alpine, which uses apk for package management.",
    "Passing args to python script inside Dockerfile": "Docker will not treat them as an Environment variable the way you passed in the CMD because CMD run as docker command and will not involve shell, so no shell means no variable interpolation, so it will not work until you specify to execute as a shell.\nARG INSTANCE_NUM\nENV INSTANCE_NUM=$INSTANCE_NUM\nRUN echo $INSTANCE_NUM\nARG TOTAL_INSTANCES\nENV TOTAL_INSTANCES=$TOTAL_INSTANCES\nRUN echo $TOTAL_INSTANCES\n\nCMD [\"sh\", \"-c\", \"python test.py $INSTANCE_NUM $TOTAL_INSTANCES\"]\nSecond thing you can not use build args in CMD as CMD is supposed to run at boot time.\nSo it will not work and it will just print as it is.\nARG INSTANCE_NUM\nRUN echo $TOTAL_INSTANCES\nCMD [\"python\",\"$INSTANCE_NUM\"]\nTo expend variable, need to specify shell in CMD.\nARG INSTANCE_NUM\nENV INSTANCE_NUM=$INSTANCE_NUM\nCMD [\"sh\", \"-c\", \"echo $INSTANCE_NUM; python $INSTANCE_NUM\"]\nBuild with ARGs or you can overide at run time.\n docker build --build-arg INSTANCE_NUM=testargs -t mypy .\nRUN\ndocker run -it mypy\nOr you now can override at run time if you missed at build time.\n docker run  -e INSTANCE_NUM=overide_value -it mypy \nThe same thing can be done with docker-compose\nversion: \"3\"\nservices:\n mongo:\n   image: mypy\n   environment:\n     - INSTANCE_NUM=overide_from_compose",
    "How to run different python scripts in parallel containers from same image": "You can do this easily using Docker Compose. Here is a simple docker-compose.yml file just to show the idea:\nversion: '3'\n\nservices:\n  app1:\n    image: alpine\n    command: >\n      /bin/sh -c 'while true; do echo \"pre-processing\"; sleep 1; done'\n\n  app2:\n    image: alpine\n    command: >\n      /bin/sh -c 'while true; do echo \"post-processing\"; sleep 1; done'\nAs you see both services use the same image, alpine in this example, but differ in commands they run. Execute docker-compose up and see the output:\napp1_1  | pre-processing\napp2_1  | post-processing\napp2_1  | post-processing\napp2_1  | post-processing\napp1_1  | pre-processing\napp2_1  | post-processing\napp1_1  | pre-processing\napp2_1  | post-processing\n...\nIn your case, you just change the image to your image, say myimage:v1, and change the service commands so that the first service definition will have command: python script1.py line and the second one command: python script2.py.",
    "\"docker load\" on custom images creates a <none> image name": "This is the default behavior when you save an image using its image id. An image id can have multiple tags.\nFor instance in your case executing a docker tag $image_id chacha2 would create a new image chacha2 with the same image id as your initial image.\nSo when saving the image based on its image id docker daemon do not know which image's details you want to export and finally saves the image with no repository/tag details.\nWhat you possibly need is docker save chacha > myimage.tar. When you load this tar, the imported image will contain the repository/tag details of your initial image.\nMore:\ndocker save\nrelated github issue\nrelated stackoverflow question",
    "Where does the docker images stored in local machine": "1.You can export your docker image after building it.\ndocker build -f Dockerfile -t myimage .\ndocker save myimage > myimage.tar You will see this in your directory where you execute docker build command.\nThen you can load it anywhere else as\ndocker load < myimage.tar\nOther than that if there is a docker repo , you can simply push the docker image.\nReference:- https://docs.docker.com/engine/reference/commandline/export/#options\n2.If the other machine can be reachable via a network you can setup the other machine as a docker hub",
    "Cant change content of a file using environment variable in docker image at run time": "Java will read static property files literally and doesn't do any interpolation of these files before running. There are a few options available to you.\nOne is to add to the Dockerfile a step to search and replace the value in the file.\nFROM java:alpine\nARG SERVICE=test\nENV SERVICE $SERVICE\nCOPY runtime.properties /tmp/\nRUN sed -i -e 's/${SERVICE}/asd/g' /tmp/runtime.properties \nRUN chmod 700 /tmp/runtime.properties\nAnother option is to change the properties file to a java class and read the environment variable directly. This gives the advantage of having the default value in the code for standalone running.\npublic enum LocalConfig {\n    INSTANCE;\n\n    private String service = System.getenv(\"SERVICE\") ==null ? \"test\" : System.getenv(\"SERVICE\");\n}\nYet another option if you have lots of environment variables is to use envsubst, this will replace all of the environment variables in the file. But this depends on what your base image is. https://www.gnu.org/software/gettext/manual/html_node/envsubst-Invocation.html\nFROM java\nARG SERVICE=test\nENV SERVICE $SERVICE\nCOPY runtime.properties /tmp/\nRUN envsubst < /tmp/runtime.properties > /tmp/runtime.properties \nRUN chmod 700 /tmp/runtime.properties\nThe last option I can think of is interpreting the environment variables after you inport the file. There is a good thread on that here: Regarding application.properties file and environment variable",
    "Dockerfile - npm: not found": "I confirm that node comes with npm:\n$ docker run -it --rm node /bin/bash\nroot@b35e1a6d68f8:/# npm --version\n5.6.0\nBut the line\nENV PATH /usr/src/app/node_modules/.bin:PATH\noverwrites the initial PATH, so you should try replacing it with\nENV PATH /usr/src/app/node_modules/.bin:${PATH}\nAlso, note that your ADD ./code ... line is clumsy, because it would add all the files of your application (including ./code/package.json!) and this step comes too early (w.r.t. Docker's cache mechanism), so I'd suggest to simply remove that line ADD ./code /usr/src/app and add a line COPY ./code ./ after the RUN npm install ...\nFinally you may also want to take a look at the official documentation for \"dockerizing\" a Node.js app: https://nodejs.org/en/docs/guides/nodejs-docker-webapp/",
    "Docker change .gitconfig with token for private repo access": "I solved it by running the following RUN command. Make sure, to have the correct rights when generating the token on github.\nFROM golang:1.9\nRUN git config --global url.\"https://USERNAME:TOKEN@github.com/\".insteadOf \"https://github.com/\"\n....",
    "Docker+Django, server running but welcome page not showing": "First of all you need to change\nCMD ./manage.py runserver 127.0.0.1:8002\nto\nCMD ./manage.py runserver 0.0.0.0:8002\nThis is because you want to listen on all interfaces inside the container. Listening to 127.0.0.1 inside the container will allow you to access the app only from inside the container.\nNext when you run this file, you need to make sure to map the port to the host os\ndocker run -it -p 8002:8002 <imageid>\nNow your app would be accessible on localhost:8082 on the docker host. If your docker host is inside a VM and then you need to use <VMIP>:8082 to access the app",
    "Can't access 8000 port of a Django project in docker": "Change the building/running command of docker to:\ndocker run -i -t -p 8000:8000 e2 python3 manage.py runserver 0.0.0.0:8000\nThe problem is that you haven't specified the listening host, so it's set directly to the loopback interface 127.0.0.1, that's why it's just accessible just from the container itself.\nSo when you set the host to the broadcast address 0.0.0.0 it will be accessible for everyone.",
    "How to run bower install inside a Dockerfile?": "you should haver a look at https://github.com/marmelab/docker-bower/blob/master/Dockerfile\nI see, among other things\nRUN apt-get install -y -qq npm\n\nRUN ln -s /usr/bin/nodejs /usr/bin/node\n\n# install bower\n\nRUN npm install --global bower",
    "Override FROM image's ENV in Dockerfile": "That won't work. The ACTIVEMQ_VERSION has already been used by the cloudesire/activemq:latest image build to populate its image layers. All the ActiveMQ installation files based on version 5.11.1 are already extracted in their corresponding directories.\nIn your Dockerfile you only can build on top of what has already been build there and add your files. Your own Dockerfile build will not re-run the build instructions described in their Dockerfile.\nIf you need to have your own cloudesire/activemq image based on version 5.9.1 you need to clone their Dockerfile, adjust the version there and build it locally. So you could base your other Dockerfile on it.",
    "How to setting Core file size in Docker container?": "You can set the limits on the container on docker run with the --ulimit flag.\ndocker run --ulimit core=<size> ...\nNote that \"unlimited\" is not a supported value since it is not actually a real value, but -1 is equivalent to it:\n$ docker run -it --ulimit core=-1 ubuntu:18.04 sh -c 'ulimit -a | grep core'\ncoredump(blocks)     unlimited\n~ $",
    "How to run Puppeteer and Node.js inside a Docker Container?": "This solution worked for me.\nTo run Puppeteer inside a Docker container you should install Google Chrome manually because, in contrast to the Chromium package offered by Debian, Chrome only offers the latest stable version.\nInstall browser on Dockerfile :\nFROM node:18\n\n# We don't need the standalone Chromium\nENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD true\n\n# Install Google Chrome Stable and fonts\n# Note: this installs the necessary libs to make the browser work with Puppeteer.\nRUN apt-get update && apt-get install curl gnupg -y \\\n  && curl --location --silent https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \\\n  && sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list' \\\n  && apt-get update \\\n  && apt-get install google-chrome-stable -y --no-install-recommends \\\n  && rm -rf /var/lib/apt/lists/*\n\n# Install your app here...\nAdditionally, If you are in an ARM-based CPU (Apple M1) like me, you should use the --platform linux/amd64 argument when you build the Docker image.\nBuild Command : docker build --platform linux/amd64 -t <image-name> .\nNote : After updating your Dockerfile, make sure to update the puppeteer script, while launching the puppeteer browser add executable path with the path to chrome we recently installed on the machine.\nconst browser = await launch({\n   headless: true,\n   defaultViewport: null,\n   executablePath: '/usr/bin/google-chrome',\n   args: ['--no-sandbox'],\n});",
    "docker could not find a file from wwwroot": "It appears that you are using Linux containers and using Windows file path conventions. Path.Combine does support this, but it needs some help.\n/app/wwwroot\\Db\\product.json\nTry wwwroot/Db/product.json\nIf this is your issue, you may wish to examine the overloads for Path.Combine as the Combine method has additional overloads that accept additional parameters\npublic static string Combine(string path1, string path2, string path3)\npublic static string Combine(string path1, string path2, string path3, string path4)\npublic static string Combine(params string[] paths)\nYour code: Path.Combine(_env.ContentRootPath, @\"wwwroot\\Db\\product.json\");\nWould then become something like: Path.Combine(basePath, \"wwwroot\", \"Db\", \"product.json\");",
    "Override parent Docker arguments": "No, there's no way to do this.\nThe result of docker build is an opaque, immutable image. A downstream Dockerfile's FROM line doesn't rebuild the image, it simply imports that opaque image as-is. You can't change what's in the image or its build options after it's been built, or cause it to be rebuilt in the later Dockerfile.\nIf you control both images, it's possible the thing you're trying to change shouldn't be a build-time ARG at all. Things like upstream package version numbers can be good ARGs (a Java 8 vs. Java 11 version of something is likely \"different\" enough to merit two builds but still be quite similar on the whole; updating from version 3.4.1 to 3.4.2 of some library probably only involves changing a version number but not any logic). Things where you need to control the behavior later on (numeric user IDs, database locations, ...) should be configured some other way and not built in to an image.",
    "Reference Dockerfile from docker-compose.yml?": "You can create an image from the dockerfile:\ndocker build - < Dockerfile\nThen you should tag you image with a proper name.\nAfter creating the image reference it in the docker-compose.yml file:\nmy-service: \n   image: ${image_name}\nAnother option is to simply write:\nmy-service: \n   build: .",
    "Files mysteriously missing in docker image": "If you're mounting a volume over an existing directory you will lose the original content of your image.\nIn your case it might be happening because a folder is mounted in one of the levels of the path /home/node/app.\nUse docker inspect <container> to monitor the list of mounted volumes inside of your container (\"Mounts\" fields). You can also extract to output only Mounts using:\ndocker inspect -f '{{ .Mounts }}' <container>",
    "dial tcp 127.0.0.1:8000: connect: connection refused golang docker containers": "Expanding on my comment. Your client code's network address will not work:\nresp, err := http.Post(\"http://localhost:8000/orders\", \"application/json\", bytes.NewBuffer(requestBody)) // broken\nas it is literally talking to itself (localhost refer to the client docker container - not the host OS).\nThe quickest way - for testing purposes - to expose the two containers to your host OS would be like so:\ndocker run -it --rm -p 8000:8000 dockweb  # server container\ndocker run -it --rm --net host dockcli    # client container\nBeyond trivial testing, this can quickly get unwieldy. So I recommend using things like docker-compose which allows you to trivially link containers and their networks.",
    "Can't run docker image": "There are a couple of things which you should correct, First one is the CMD format which should be\nCMD instruction has three forms:\n\nCMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form)\nCMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\nCMD command param1 param2 (shell form)\nCMD [ \"/bin/bash\" , \"./serve.sh\" ] \nAnother thing, When you do docker run, the instructions are\nUsage:  docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\nwhich means all the options has to be before IMAGE and in your case it is appearing after IMAGE.\nThe correct command should be\ndocker run -it b24b37614e1a\nBTW, small question, why you want to run an interactive container of this application. Ideally, it should be something like\ndocker run -p $HOST_PORT:$APP_PORT b24b37614e1a\n-p => Publish a container's port(s) to the host\nand then you can access your application localhost:$HOST_PORT or machine_IP:$HOST_PORT",
    "Are docker containers totally isolated from the host machine OS?": "A docker container uses the host operating system (kernel) and you can see inside the containers from the host perspective. Usually containers can not see the host processes nor other containers.\nThink of container like an isolated application process running, not like a virtual machine.",
    "Spring Boot live reload inside a docker container not working": "The Spring Dev Tools hot reload mechanism only reloads/restarts the project from already built class files; it won't compile/build the source files. So inside a docker container, even if the source files are changed, the Java class files won't. Thus the change won't reflect and newly added GET API won't be published.\nWhen the container is restarted, it again calls gradle bootRun as specified in Dockerfile. This will build the changed Java files into class files and the change will be reflected.\nWhen we use this in an IDE, the IDE (by default) builds the source file whenever it is changed. So Spring hot reload always loads the updated class files, but this won't happen outside an IDE, unless we have some mechanism to watch source changes and build them, like gradle build --continuous\nThese are my settings now.\napplication.properties\n# scan for trigger file from root\n# trigger file should not be in classpath, or it will go to infinite build loop\nspring.devtools.restart.additional-paths=.\nspring.devtools.restart.trigger-file=reload-trigger.txt\nDockerfile\nFROM gradle:6.7-jdk11\nWORKDIR /app\nCOPY . /app\nEXPOSE 8000\n\n# fetch dependencies\nRUN chmod +x start.sh && gradle getDeps\n\n# script which watches source file changes in background and executes bootRun\nCMD [\"sh\", \"start.sh\"]\nstart.sh\nThis file should be in root as per the Dockerfile should not be in classpath\n# buildAndReload task, running in background, watches for source changes\n(sleep 60; gradle buildAndReload --continuous -PskipDownload=true -x Test)&\ngradle bootRun -PskipDownload=true\nbuild.gradle\n# -----other codes above----- #\ntask buildAndReload {\n    dependsOn build\n    mustRunAfter build    // buildAndReload must run after the source files are built into class files using build task\n    doLast {\n        new File(\".\", \"reload-trigger.txt\").text = \"${System.currentTimeMillis()}\" // update trigger file in root folder for hot reload\n    }\n}\nWith these settings, whenever some source files are changed, the buildAndReload task is being executed. As this task depends on build task, the updated source are built into class file before that. This custom task, then updates the trigger file and Spring loads the updated class files and restarts the application.",
    "apt-get error: Version '5:19.03.4~3-0~ubuntu-bionic' for 'docker-ce' was not found": "You've selected Docker versions based on what's available on your build host, not what's available inside the container image you're building. The jenkins:lts image is based on Debian Stretch, not Ubuntu Bionic.\nDockerfiles are actually just running fairly ordinary Docker operations. So, for example, you can run docker run -ti -u root jenkins/jenkins:lts /bin/bash, run your RUN scripts by hand, and check the apt-cache output inside the container:\n# apt-cache madison docker-ce\n docker-ce | 5:19.03.4~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages\n docker-ce | 5:19.03.3~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages\n docker-ce | 5:19.03.2~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages\n docker-ce | 5:19.03.1~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages\n docker-ce | 5:19.03.0~3-0~debian-stretch | https://download.docker.com/linux/debian stretch/stable amd64 Packages\nAlso, a failed docker build should leave the partially-complete image around; so you can use that directly to investigate a failure. As an example with a trivially failing step RUN false:\n\u22ee\nRemoving intermediate container baaeab34bb8c\n ---> 6d34bab07796\nStep 3/3 : RUN false\n ---> Running in 8347f442dfaa\nThe command '/bin/sh -c false' returned a non-zero code: 1\nThe 6d34bab07796 image is left around. You can pass that to docker run and investigate why the command failed. The 8347f442dfaa container is also left around, though exited; you can use the various docker container subcommands to investigate it as well.",
    "How to solve problem with empty docker-entrypoint-initdb.d? (PostgresQL + Docker)": "In your docker-compose.yml file, you say in part:\npostgres-passport:\n  image: postgres:latest\n  volumes:\n    - \"./postgres-passport:/docker-entrypoint-initdb.d\"\n    - \"./data/postgres_passport_data:/var/lib/postgresql/data\"\nSo you're running the stock postgres image (the Dockerfile you show never gets called); and whatever's in your local postgres-passport directory, starting from the same directory as the docker-compose.yml file, appears as the /docker-entrypoint-initdb.d directory inside the container.\nIn the directory tree you show, if you\ncd deploy/latest\ndocker-compose up\nThe ./postgres-passport is expected to be in the deploy/latest tree. Since it's not actually there, Docker doesn't complain, but just creates it as an empty directory.\nIf you're just trying to inject this configuration file, using a volume is a reasonable way to do it; you don't need the Dockerfile. However, you need to give the correct path to the directory you're trying to mount into the container.\npostgres-passport:\n  image: postgres:latest\n  volumes:\n    #  vvv Change this path vvv\n    - \"../../postgres-passport/docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d\"\n    - \"./data/postgres_passport_data:/var/lib/postgresql/data\"\nIf you want to use that Dockerfile instead, you need to tell Docker Compose to build the custom image instead of using the standard one. Since you're building the init file into the image, you don't also need a bind-mount of the same file.\npostgres-passport:\n  build: ../../postgres-passport\n  volumes:\n    # Only this one\n    - \"./data/postgres_passport_data:/var/lib/postgresql/data\"\n(You will also need to adjust the COPY statement to match the path layout; just copying the entire local docker-entrypoint-initdb.d directory into the image is probably the most straightforward thing.)",
    "Docker image fails to build on Live but fine on Dev": "You are using image python with tag 2.7, however this tag is a \"shared\" tag as per Python readme on Docker Hub which is changing other time: right now python:2.7 is shared with Python python:2.7.16 and python:2 but previously it was probably shared with python:2.7.15, python:2.7.14 etc. (in other words, python:2.7 is following python:2.7.x as it upgrades)\nYour machine and live server probably pulled the image at a different time and now have a different image tagged 2.7. The \"shared\" tags seems to be like latest tags and may point to newer images as they are released.\nWhat you can do:\nEnforce image pull when building even if image is already present (using docker build with --pull option\nUse a documented Simple tag instead, these should be more consistent (such as python:2.7.16-alpine3.9)\nDo not re-build images during your release process, only build once and use the same image in your local and live environment (see below)\nEDIT: this can be put into evidence with:\ndocker images --filter \"reference=python\" --digests --format \"{{.Digest}} {{.Repository}}:{{.Tag}}\"\nsha256:7a61a96567a2b2ba5db636c83ffa18db584da4024fa5839665e330934cb6b2b2 python:2\nsha256:7a61a96567a2b2ba5db636c83ffa18db584da4024fa5839665e330934cb6b2b2 python:2.7\nsha256:7a61a96567a2b2ba5db636c83ffa18db584da4024fa5839665e330934cb6b2b2 python:2.7.16\nsha256:39224960015b9c0fce12e08037692e8a4be2e940e73a36ed0c86332ce5ce325b python:2.7.15\nTo precise on:\nI thought the point of Docker is that everything runs using the same image and that you shouldn't run into issues like this.\nWhy is this the case and what can I do to fix it from here so that it runs the same on both dev and live?\nYes, and the recommended pattern is build image once and use that same image trough all your release process - this ensure you have the exact same context (packages, code, etc.) from development to production. You should not re-build your image from scratch on your live server, but ideally build it during your development phase and use that same image for testing and deploying.",
    "Docker: Install chrome on Windows container using dockerfile": "Here's a dockerfile I use to install headless chrome into an aspnet 4.5 image. Enjoy.\n# extending the `microsoft/aspnet` image.\nFROM microsoft/aspnet\n\nRUN echo 'pull down choco'\nRUN powershell -Command Install-PackageProvider -name chocolatey -Force\nRUN powershell -Command Set-PackageSource -Name chocolatey -Trusted\n\nRUN powershell -Command Get-PackageSource\n\nRUN echo 'install chrome via choco'\nRUN powershell -Command Install-Package GoogleChrome -MinimumVersion 74\nAnother possible route. Copy all your installers into an 'installer directory' next to your dockerfile. Then copy them in and run them manually.\nFROM mcr.microsoft.com/dotnet/framework/runtime:4.8-windowsservercore-ltsc2019\nRUN mkdir installers \nCOPY ./installers/ /installers\nRUN [\"c:/installers/ChromeStandaloneSetup64.exe\", \"/silent\", \"/install\"]\nRUN [\"c:/installers/Firefox Setup 66.0.3.exe\", \"-ms\"]\nHope this helps... Hope this helps",
    "Netstat not showing ports exposed by docker": "Problem was in netstat command, after adding -anp flag, ports are listed.\n$ sudo netstat -anp | grep 8080\ntcp6       0      0 :::8080                 :::*                    LISTEN      16341/docker-proxy",
    "Docker compose missing yarn dependencies on build": "This part of Dockerfile installs yarn packages:\nRUN mkdir /myapp\nWORKDIR /myapp\nADD ./package.json /myapp/\nRUN yarn install\nFolder /myapp is created, package.json is copied to it and yarn packages are installed. Build is successful and, of course, node_modules folder is inside built image.\nBut after that you start built image with:\nvolumes:\n  - .:/myapp\nwhich means that content of folder where docker-compose.yaml is is mounted to /myapp folder inside container, so it covers content of container's /myapp folder.\nYou don't need to mount current folder to container's folder to achieve what you want. Just delete it from your docker-compose.yaml:\nversion: '3'\nservices:\n  web:\n    build: .\nNow you can:\n$ docker-compose build\n$ docker-compose run web bash\nroot@558d5b0c2ccb:/myapp# ls -la\ntotal 268\ndrwxr-xr-x   3 root root   4096 Feb 23 22:25 .\ndrwxr-xr-x  65 root root   4096 Feb 23 22:36 ..\ndrwxr-xr-x 818 root root  36864 Feb 23 22:25 node_modules\n-rw-rw-r--   1 root root    333 Feb 23 22:07 package.json\n-rw-r--r--   1 root root 219075 Feb 23 22:25 yarn.lock\nEDIT:\nBut what I want is when building the image, get these dependencies not when spinning up the containers. Otherwise I have another container which mounts de source code and needs this node_modules folder when running the \"docker-compose up\" and I'd like to avoid some kind of ugly sleep until the node_modules is finished. So I need present this folder on my root host before up the containers somehow\nIf you want to achieve the above goal, you can use the following workaround:\n1. You modify Dockerfile a little:\nFROM ruby:2.5\n\nRUN curl -sL https://deb.nodesource.com/setup_8.x | bash - && \\\n    curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - && \\\n    echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | tee /etc/apt/sources.list.d/yarn.list && \\\n    apt-get update && \\\n    apt-get install -qq -y build-essential libpq-dev nodejs yarn\n\nRUN mkdir /build && mkdir /myapp\nWORKDIR /build\n\nADD ./package.json /build/\n\nRUN yarn install\n\nWORKDIR /myapp\n\nCMD cp -a /build/node_modules/ /myapp/\nThat's means that yarn packages will be built in /build folder inside image and copied to /myapp folder once container is started.\n2. You use the original docker-compose.yaml file:\nversion: '3'\nservices:\n  web:\n    build: .\n    volumes:\n      - .:/myapp\nwhen you start web container:\ndocker-compose up web\nfolder node_modules is copied to mounted folder i.e. to . folder on your host machine.\n3. Now you can start any container and it will be contain node_modules folder inside /myapp:\ndocker-compose run web bash\nSo, you will be able to achieve your goal the following way:\n$ docker-compose build && docker-compose up web\n$ docker-compose run web bash\nroot@4b38e60adfa3:/myapp# ls -la\ntotal 64\ndrwxrwxr-x   3 1000 1000  4096 Feb 24 10:59 .\ndrwxr-xr-x  66 root root  4096 Feb 24 11:13 ..\n-rw-rw-r--   1 1000 1000   497 Feb 24 10:55 Dockerfile\n-rw-rw-r--   1 1000 1000    73 Feb 24 09:02 docker-compose.yaml\ndrwxr-xr-x 818 root root 40960 Feb 24 10:57 node_modules\n-rw-rw-r--   1 root root   333 Feb 23 22:07 package.json",
    "Unable to build a mariaDB base in a Dockerfile": "Using mariadb image from dockerhub it's simpler than what you tried.\nAs explained in \"Initializing a fresh instance section\":\nWhen a container is started for the first time, a new database with the specified name will be created and initialized with the provided configuration variables. Furthermore, it will execute files with extensions .sh, .sql and .sql.gz that are found in /docker-entrypoint-initdb.d. Files will be executed in alphabetical order.\nSo it's enough you add just test.sql script to /docker-entrypoint-initdb.d\nThen your Dockerfile must run mysqld as command.\nSo, I would leave only test.sql in local sql subdirectory and I would use this Dockerfile:\nFROM mariadb:latest\n\nADD sql/ /docker-entrypoint-initdb.d\n\nENV MYSQL_ROOT_PASSWORD test123\nENV MYSQL_DATABASE testDB\nENV MYSQL_USER toto\nENV MYSQL_PASSWORD test123\n\nRUN apt-get update && apt-get -y install vim\n\nEXPOSE 3306\n\nCMD [\"mysqld\"]\nThen build your image\ndocker build -t maria .\nand run the container (may be useful to map port)\ndocker run --name mariadb -ti -d -p 3306:3306 maria\nIt creates testDB and user toto, as you correctly defined using the environment variables in Dockerfiles (remind that you can override them as -e parameters in the docker run).",
    "How to prevent the `#!/bin/bash: not found` error when calling a script from Dockerfile": "Bash is often in different places on different systems. Often at /bin/bash, but on this container it's located here:\n % docker run -it debian:stretch-backports\nroot@bb01a3db779e:/# type bash\nbash is /usr/bin/bash\nThe env command is more predictable, and can be used to locate other programs in the shebang line. So try this in your script:\n#!/usr/bin/env bash",
    "New package.json packages are not showing in Docker container": "I solved this by moving my src code inside an src directory. This means my docker-compose.yml file now looks like this:\napp:\n  build: .\n  command: yarn start:dev\n  environment:\n    NODE_ENV: development\n  ports:\n    - '8080:8080'\n  volumes:\n    - ./src:/home/app/src\nSince I am not mounting the whole dir with the node_modules, new ones seem to be installed correctly.",
    "Dockerfile can not find entrypoint script": "It looks like the first line in your /start.sh script is pointing to something that doesn't exist. If it's a #!/bin/bash you would need to have bash installed in the image. And if your shell is installed in the container, make sure your first line doesn't have a windows linefeed at the end.",
    "Docker containers communication": "Try to use ZMQ in container so, containers would publish work to zmq_container and from localhost you will be able to subscribe channel of zmq_contaniner as well (using one port)",
    "Dockerfile: skip passphrase for private key": "The passphrase is required to decrypt the key. You can't \"skip\" it. You could remove the passphrase on the key using ssh-keygen -p (see the man page for details).\nYou may want to investigate the use of a GitHub Deploy Key, which is a per-repository ssh key that grants read-only access to the repository. These are meant to solve exactly the situation you find yourself in: needing to automaticaly deploy software from a GitHub repository that requires authentication.",
    "Launch two nodejs app inside Docker container using pm2": "Use a process file to manage these two applications: http://pm2.keymetrics.io/docs/usage/application-declaration/\nFor example - process.yml:\napps:\n  - script : 'npm'\n    args   : 'run dev'\n    cwd    : './backend'\n    name   : 'backend'\n  - script : 'npm'\n    args   : 'run dev'\n    cwd    : './frontend'\n    name   : 'frontend'\nThen in the Dockerfile:\nCMD ['pm2-docker', 'process.yml']\nDocumentation about PM2/Docker integration: http://pm2.keymetrics.io/docs/usage/docker-pm2-nodejs/",
    "Docker : Why the Commands from Dockerfile were not being executed?": "Docker images are built layer by layer using (you guessed it!) docker containers. To get Hello World to print when you're trying to run a container you'll need to specify an ENTRYPOINT or a CMD within your Dockerfile.\nTaken from Dockerfile reference:\nNote: don\u2019t confuse RUN with CMD. RUN actually runs a command and commits the result; CMD does not execute anything at build time, but specifies the intended command for the image.\nMoving on swiftly to your queries:\nQuery 1: Why does the below execution has not resulted in any result ? I am expecting to print \"hello World\". What wrong has happened ?\nIn your Dockerfile, when you say RUN echo \"hello world\" what docker actually does is it creates an intermediate container, executes the command and saves the state of the container as a layer on which it'll run the next command. In your example you can see that \"hello world\" was actually printed in Step 3.\n Step 3 : RUN echo \"hello World\"\n ---> Running in 2b513872628e\n hello World\nAnd for your second query:\nQuery 2: I am able to run the execution steps in the following way. Why it has executed the command in the below container and why not when I run the container a8eede1874a2 ?\nSo when you've created container with the hash of a8eede1874a2 what you're actually doing is well... nothing. You haven't specified CMD or ENTRYPOINT or a command through the command line. Your container started and stopped since there was nothing to execute.\nNow in your second example you're executing an interactive shell in which you're running the echo \"hello world\".\nExample Dockerfile:\nFROM ubuntu:14.04\nMAINTAINER RAGHU\nCMD \"echo hello World\"\nUseful references:\nDockerfile Reference\nDocker RUN\nDocker ENTRYPOINT\nDocker CMD\nDocker run cmd (look for --entrypoint)",
    "Docker container command not found or does not exist": "It's not the entrypoint script it can't find, but the shell it's referencing -- alpine:3.3 doesn't by default have bash inside it. Change your myinit.sh to:\n#!/bin/sh\nset -e\n\necho 123\ni.e. referencing /bin/sh instead of /bin/bash",
    "How to use chrome for testing in a dockerFile for selenium use": "This is the way I've been doing it:\nFROM openjdk:17-slim\n\n# please review all the latest versions here:\n# https://googlechromelabs.github.io/chrome-for-testing/\nENV CHROMEDRIVER_VERSION=120.0.6099.71\n\n### install chrome\nRUN apt-get update && apt-get install -y wget && apt-get install -y zip\nRUN wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nRUN apt-get install -y ./google-chrome-stable_current_amd64.deb\n\n### install chromedriver\nRUN wget https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/$CHROMEDRIVER_VERSION/linux64/chromedriver-linux64.zip \\\n  && unzip chromedriver-linux64.zip && rm -dfr chromedriver_linux64.zip \\\n  && mv /chromedriver-linux64/chromedriver /usr/bin/chromedriver \\\n  && chmod +x /usr/bin/chromedriver\n\n###\n # REST OF YOUR DOKCERFILE HERE\n###\nBasically, downloads and installs latest stable chromedriver version. Chromedriver version to be manually set using ENV (see https://googlechromelabs.github.io/chrome-for-testing/)",
    "How to create a working Docker Container With .NET 6.0?": "mcr.microsoft.com/dotnet/runtime:6.0 is a Linux container based on Debian 11 - check out info at docker hub:\n6.0.16-bullseye-slim-amd64, 6.0-bullseye-slim-amd64, 6.0.16-bullseye-slim, 6.0-bullseye-slim, 6.0.16, 6.0 Dockerfile Debian 11 05/03/2023\nSo it can't run .exe files which are build for Windows.\nYou need to either switch to Windows container or change command to a valid one for Debian, something like:\nENTRYPOINT [\"dotnet\", \"DockerTest.dll\"]\nAlso you can try using VS tools to add Docker support. This should create a multistage build Docker file which will handle both building and producing the resulting image.\nP.S.\nAlso I would recommend creating an ASP.NET Core project (using Minimal APIs for simplicity for example) instead of HttpListener directly.\nlocalhost in \"http://localhost:8000/\" will mean that it will accept only requests from localhost which in case of dockerized application will be the container itself - see this answer for details and solutions.",
    "How to mount secret file in docker image build & use variable from secret file in Dockerfile to authenticate a command?": "Your secret would be mounted as /run/secrets/mysecret which can be accessed using the cat command. The RUN command might look something like below:\nRUN --mount=type=secret,id=mysecret \\\n    cat /run/secrets/mysecret\nA more complete example below:\nDockerfile:\nFROM node:16\n\nWORKDIR /app\n\nRUN --mount=type=secret,id=USERNAME \\\n    cat /run/secrets/USERNAME > /app/username.txt\nA docker image can be built from this file, with --secret flag using below command:\nDOCKER_BUILDKIT=1 docker build --secret id=USERNAME,src=username.txt -t node:16-secret .\nNow the built docker image contains the contents of username.txt secret, which was passed at build time, as the file /app/username.txt. That can be verified using below command:\ndocker run --rm -it node:16-secret cat username.txt\nYou can refer this answer for an example of using the mounted secret in a curl command",
    "How to acces env variable with dockerfile": "Environment variable in docker-compose are intended for the application. I.e. runtime environment of your application.\nDockerfile does not have environment variables. Instead - it has build arguments.\nThus, you need to use build arguments i.o. env. variables for customization of build process.\nNamely:\nDockerfile\nARG ENV=prod    # default value\nRUN npm run build-${ENV}\nBuild script\ndocker build --build-arg ENV=dev\nor in docker-compose:\nbuild:\n    context: .\n    args:\n        ENV: dev",
    "ssh key in Dockerfile returning Permission denied (publickey)": "Make sure you have an SSH agent running and that you added your private key to it.\nDepending on your platform, the commands may vary but since it's tagged gitlab I will assume that Linux is your platform.\nVerify that you have an SSH agent running with echo $SSH_AUTH_SOCK or echo $SSH_AGENT_SOCK if both echo an empty string, you most likely do not have an agent running.\nTo start an agent you can usually type:\neval `ssh-agent`\nNext, you can verify what key are added (if any) with:\nssh-add -l\nIf the key you need is not listed, you can add it with:\nssh-add /path/to/your/private-key\nThen you should be good to go.\nMore info here: https://www.ssh.com/academy/ssh/agent\nCheers",
    "dockerfile COPY does not copy all the files": "That repository contains a .dockerignore file which excludes everything except a set of things it selects.\nThat repository's docker directory also contains several build scripts for official images and you may find it easier to start your custom image FROM openzipkin/zipkin rather than trying to reinvent it.",
    "Git clone inside of newly created container - problem with known_hosts": "You can do it in below way\nssh-keyscan github.com >> /root/.ssh/known_hosts",
    "Change base image based on condition in dockerfile": "Make use of combination of ARG and FROM in Dockerfile.\nYou can use variables declared in ARG inside FROM statement.\nARG  APP_IMAGE=alpine:latest\nFROM ${APP_IMAGE}\nCMD  /path/to/mycode\nAnd can also override this value using --build-arg option of docker build command.\ndocker build -t myapp:v1 --build-arg APP_IMAGE=busybox:latest .",
    "Optimize docker image build size with curl": "In fact, you are close to the solution. The one you missed is to delete the curl source package.\nSo next should make the image reduce:\nFROM debian:10.7\n\nRUN apt-get update && \\\n    apt-get install --yes --no-install-recommends wget build-essential ca-certificates libcurl4 && \\\n    wget https://curl.se/download/curl-7.73.0.tar.gz && \\\n    tar -xvf curl-7.73.0.tar.gz && cd curl-7.73.0 && \\\n    ./configure && make && make install && \\\n    apt-get purge -y --auto-remove build-essential && \\\n    cd .. && rm -fr curl-7.73.0.tar.gz curl-7.73.0\n    \nWithout Curl:\n$ docker images abc:1\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nabc                 1                   d742bfdf5fa6        25 seconds ago      148MB\nWith curl & source package delete:\n$ docker images abc:2\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nabc                 2                   afe3d404852a        27 minutes ago      151MB\nAdditional, if you delete apt cache with rm -rf /var/lib/apt/lists/* in Dockerfile, if will be smaller:\n$ docker images abc:3\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nabc                 3                   5530b0e9b44f        2 minutes ago       134MB\nAnother solution maybe use multistage-build, you could use ./configure --prefix=xxx to set a default install location, then stage1 just used to build curl, while stage2 copy the xxx folder from stage1 to final image.",
    "Check docker alpine musl version": "libc itself is an executable. You can run it and the usage message will print out the version:\n/ # /lib/libc.musl-x86_64.so.1\nmusl libc (x86_64)\nVersion 1.1.24\nDynamic Program Loader\nUsage: /lib/libc.musl-x86_64.so.1 [options] [--] pathname [args]",
    "\"dockerfile parse error line 1 from requires either one or three arguments\" but I have only 1 argument": "Solution found thanks to the comment of David Maze.\nTurns out I had Mac line endings (CR), which is ridiculous, because I don't even own a Mac.\nNeither dos2unix nor unix2dos wanted to fix the line ending problem. So in the end I opened the file in nano, and saved it with DOS (CRLF) line endings using Ctrl+O, Alt+D. With this, my Dockerfile worked.\nThen I used dos2unix to convert the line endings from CRLF to LF, and that version also works.\nTo summarize:\nCR line terminators (Mac): doesn't work\nLF line terminators (Linux): works\nCRLF line terminators (DOS): works",
    "Using vendored gems in vendor/cache with Docker": "I managed to solve this. needed to edit the Dockerfile to copy gems at vendor/cache. Here is the modified gemfile which works and utilized docker caching\nFROM ruby:2.6.6\nWORKDIR /home/app/webapp\nCOPY Gemfile Gemfile.lock /home/app/webapp/\nCOPY vendor/cache /home/app/webapp/vendor/cache/\nRUN bundle install\nCOPY . /home/app/webapp/\n# Start the main process.\nCMD ['/home/app/webapp/entrypoint.sh'] \nAdding the answer here, incase someone else get's the same error. Thank you for looking.",
    "Why do some Docker images have no VOLUME defined?": "A preferred pattern for many applications you can run in a container is to store no state at all locally. If I have a Web application, and perhaps it takes a configuration file as input, but it stores all of its data in an external database, I need no local storage. In turn, that means that I can run several copies of it for redundancy or to support larger scales, and I don't need to worry about preserving data if I need to replace a container. If my containers are in fact stateless, then I have no need for a VOLUME (or to attach volumes to my container at runtime).\nEven if your container does have local state, you probably don't want VOLUME. What VOLUME actually means is, when a container runs, if nothing else is mounted on the named filesystem path, create an anonymous volume and mount it there (letting Docker populate it from image content). In practice, this means:\nYou can use docker run -v or Docker Compose volumes: to mount a directory on any container filesystem path, regardless of whether or not it's declared as a VOLUME.\nIf something is declared as a VOLUME, later RUN instructions can't change that directory tree.\nIf something is declared as a VOLUME, you can never create a derived image that has different contents at that location (for example, you can't create an image derived FROM postgresql with preloaded data).\nThere's no particular point to declaring VOLUME /tmp (as seems to be common in Java Dockerfiles) or for declaring VOLUME over parts of your application code.\nVOLUME declarations are unrelated to an image's WORKDIR.",
    "Run a locally made container on kubernetes": "Except for some very limited developer-oriented setups (like Minikube or Kind), a Docker registry of some form is basically required to run applications on Kubernetes.\nThe only two ways to get a Docker image on to a system are by running a registry, or to docker load it. The docker load path involves transferring a (possibly large) image tar file to every node in the cluster, and running the load command there. When you update the application image, you'll have to repeat this step; and you generally want to avoid the latest image tag and similar fixed strings so that the Kubernetes deployment mechanism can correctly update things, so you'll have to repeat this for each version anything in the cluster might still be using. The mechanics of setting up some place to store the image, copying it to every node in the cluster, running docker load everywhere, etc. aren't actually any easier than just running a Docker registry server yourself.",
    "Should I be using Nginx to serve React in production?": "Serve is fine. Nginx might use a few bytes less RAM for serving, but that will be cancelled out by carrying around all the extra features you aren't using. We use a similar Serve setup for a lot of our K8s SPAs and it uses between 60 and 100MB of RAM per pod at full load. For a few other apps we have a cut down version of Caddy and it maxes out around 70MB instead so slightly less but there are probably better ways to worry about 30MB of RAM :)",
    "Docker build how to add libraries to golang build": "Because the library you use has C++ lib dependency, to properly build a Pulsar Golang client docker image, you need use Docker stage build. We have the exact use case. You need download and install Pulsar C++ lib for build and run-time image in Dockerfile.\nThis is our docker file https://github.com/kafkaesque-io/pulsar-beam/blob/master/Dockerfile . I also suggest to use Go Module to build and manage Go dependencies as part of Docker stage build. This is how we do it. I hope it helps.\nRUN wget --user-agent=Mozilla -O apache-pulsar-client.deb \"https://archive.apache.org/dist/pulsar/pulsar-2.4.1/DEB/apache-pulsar-client.deb\"\nRUN wget --user-agent=Mozilla -O apache-pulsar-client-dev.deb \"https://archive.apache.org/dist/pulsar/pulsar-2.4.1/DEB/apache-pulsar-client-dev.deb\"\n\nRUN apt install -y ./apache-pulsar-client.deb\nRUN apt install -y ./apache-pulsar-client-dev.deb\n\n# Copy go mod and sum files\nCOPY go.mod go.sum ./\n\n# Download all dependencies. Dependencies will be cached if the go.mod and go.sum files are not changed\nRUN go mod download\nI use the standard ubuntu image for build as well as run-time image. I understand you are looking for the smallest possible image size. ubuntu:18.04 has a smaller footprint. You can also try alpine that I have not tested yet.\nBy the way, Apache offers a separate native Pulsar Go client library with no C++ dependency. At the time of writing in January 2020, there are still missing features such as customized routing mode for partitioned topics. If these features are not required in your project, I suggest to try the native Go client library to avoid C++ dependency. We have plan to switch to the new native library soon for the same reason.",
    "How do I specify the dockerfile stage in Visual Studio Code Remote?": "runArgs specifies the arguments to use with the \"docker run\" command. In this case, we need to pass arguments to the \"docker build\" command.\nFor specifying build arguments, you need to use the \"build\" property. In your example, you would need your devcontainer.json file to contain:\n\"build\": { \n    \"target\": \"dev\" \n},",
    "How to add many SSL certificates for Java application inside docker?": "In dockerfile call bash file:\nRUN apk update && apk add bash openssl wget && rm -rf /var/cache/apk/*\nCOPY getcerts.sh getcerts.sh\nRUN chmod +x getcerts.sh && ./getcerts.sh\nBash script:\nfor cert in ${tempdir}/*.crt; do\nkeytool -importcert -noprompt -trustcacerts -alias artifactory-${cert2} -file /${destdir}/${cert2} -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit\ndone",
    "Docker Standard_init_linux.go:207: exec user process caused \u201cno such file or directory\u201d": "In case you entrypoint is bash script check whether it contains correct shebang, something like that:\n#!/usr/bin/env bash\nmake -f /app/makefile $@\nEither specify it in your entrypoint command, something like:\nENTRYPOINT [\"sh\", \"/bin/app\"]",
    "Docker pushing to Heroku \"Unexpected HTTP status: 500 internal server error\"": "Solved:\nI forgot Heroku uses Linux containers! I was trying to build and deploy using windows containers still.\nBuilding after switching has resolved this issue :)",
    "Error passing build arguments to dockerfile when building an image - '$MyVersion' is not a valid version string": "Finally I found the issues,\nFor windows based docker file, we have to reference the variable with %MyVar%,\nRUN dotnet build MyApp.csproj /p:Version=%MyVersion% -c Release -o /app\nand regarding to ARG, they must be placed right below the FROM statement.\nFROM microsoft/dotnet:2.1-aspnetcore-runtime-nanoserver-1709 AS base ARG MyVersion=\"0.0.1.0\"\nHere is the working docker file\nFROM microsoft/dotnet:2.1-aspnetcore-runtime-nanoserver-1709 AS base\nARG MyVersion=\"0.0.1.0\"\nWORKDIR /app\nEXPOSE 8081 \n\nFROM microsoft/dotnet:2.1-sdk-nanoserver-1709 AS build\nARG MyVersion\nWORKDIR /src\nCOPY source/MyApp/MyApp.csproj source/MyApp/  \nRUN dotnet restore source/MyApp/MyApp.csproj  \nCOPY . .\nWORKDIR /src/source/MyApp\nRUN dotnet build MyApp.csproj /p:Version=%MyVersion% -c Release -o /app\n\nFROM build AS publish\nRUN dotnet publish MyApp.csproj -c Release -o /app\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app .\nENTRYPOINT [\"dotnet\", \"MyApp.dll\"]\nFor linux, it is the same, except that the ARG is reference using $MyVar\nRUN dotnet build MyApp.csproj /p:Version=$MyVersion -c Release -o /app",
    "How to stop building When I use Dockerfile build image?": "Any shell command that fails (returns an exit status other than 0) will stop the build. Some examples:\n# Probably the most idiomatic shell scripting: if the test\n# returns false, then the \"&&\" won't run mkdir, and the whole\n# command returns false\nRUN test -d /app/bin -a ... \\\n && mkdir /app/control/bin\n\n# \"test\" and \"[\" are the same thing\nRUN [ -d /app/bin -a ... ] \\\n && mkdir /app/control/bin\n\n# This first step will just blow up the build if it fails\nRUN [ -d /app/bin -a ... ]\n# Then do the on-success command\nRUN mkdir /app/control/bin\n\n# Your original suggestion\n# /bin/false does nothing but returns 1\nRUN if [ -d /app/bin -a ... ]; \\\n    then mkdir /app/control/bin; \\\n    else false; \\\n    fi",
    "Docker CentOS systemctl not permitted": "You should assume systemd and systemctl just don't work in Docker, and find another approach to whatever your higher-level goals are. Best practices are to run one service and one service only in a Docker container, and to use multiple containers if you need multiple coordinating services; if you really must run multiple things in the same container then supervisord is a common process manager.\nThe biggest problem with systemd in Docker is that it by default wants to control a lot of things. Look at the graphic on the systemd home page: it wants to do a bunch of kernel-level setup, manage filesystems, and launch several services, all of which have already been done on the host and are unnecessary in a Docker container. The \"easy\" way to run systemd in Docker involves giving it permission to reconfigure your host; the link you provide has a \"hard\" way that involves deleting most of its control files.\nIn a Dockerfile context there's also a problem that each RUN line starts from a clean slate with no processes running at all. So your systemctl start ... command doesn't work because the systemd init isn't running; and even if it did, when that RUN command finished, the process would go away and the service wouldn't be running on the next line.\nYou might be able to find a prebuilt syslog-ng image by typing \"syslog\" into the search box on https://hub.docker.com, which would dodge this issue. It also might work to install syslog-ng on a CentOS base as you do, but skip systemd entirely and just run the service as the primary command the image runs\nCMD [\"syslog-ng\", \"-F\"]",
    "nsenter error:_nl_intern_locale_data: Assertion failed": "Setting LC_ALL=\"C.UTF-8\" will probably fix your problem.",
    "How to run docker commands using a docker file": "Your example is mixing two steps, image creation and running an image, that can't be mixed that way (with a Dockerfile).\nImage creation\nA `Dockerfile`is used to create an image. Let's take this [alpine3.8 docker file][1] as a minimal example\nFROM scratch\nADD rootfs.tar.xz /\nCMD [\"/bin/sh\"]\nIt's a base image, it's not based on another image, it starts FROM scratch. Then a tar file is copied and unpacked, see ADD and the shell is set as starting command, see CMD. You can build this with\ndocker build -t test_image .\nIssued from the same folder, where the Dockerfile is. You will also need the rootfs.tar.xz in that folder, copy it from the alpine link above.\nRunning a container\nFrom that test_image you can now spawn a container with\ndocker run -it test_image\nIt will start up and give you the shell inside the container.\nDocker Compose\nUsually there is no need to build your images over and over again before spawning a new container. But if you really need to, you can do it with docker-compose. Docker Compose is intended to define and run a service stack consisting of several containers. The stack is defined in a docker-compose.yml file.\nversion: '3'\nservices:\n  alpine_test:\n    build: .\nbuild: . takes care of building the image again before starting up, but usually it is sufficient to have just image: <image_name> and use an already existing image.",
    "The command '/bin/sh -c apt-get install erlang' returned a non-zero code: 1": "apt-get asked you for confirmation (Do you want to continue? [Y/n] Abort.) which docker was apparently unwilling to give. So use apt-get -y install erlang instead.",
    "Speed up build by caching packages in Docker container": "This is out-of-the-box Docker functionality. If your Dockerfile says\nFROM node:10\nWORKDIR /app\nCOPY package.json .\nRUN npm install\n\nCOPY ...\nthen, if the package.json hasn't changed, Docker will skip over the RUN npm install step and use the filesystem image that results from doing it.",
    "Dockerfile assign bash command to var": "You can't do that, since RUN command spawns its own shell.\nAlternatively, you can save the information to some file, and use an ENTRYPOINT to set the env variable using some script once the container is running.",
    "how to enable virtualization capabilities when running docker in macbook pro": "The official documentation is here: What to know before you install.\nThis thread mentions:\nThe Getting Started document describes the following prerequisites:\nMac must be a 2010 or newer model, with Intel\u2019s hardware support for memory management unit (MMU) virtualization; i.e., Extended Page Tables (EPT)\nI've done some research on the MMU and EPT part. From https://en.wikipedia.org/wiki/X86_virtualization31 I found out that EPT is part of VT-x. MMU is part of VT-d.\nIn short, that means the CPU should support both VT-x and VT-d.\nSince the requirement described \"Mac must be a 2010 or newer model...\" I made the assumption I was safe. Either way, I was able to look up more information about the CPU in my iMac. Using the sysctl command gives you information about the model number of the CPU, in my case an Intel Core i5-760 Processor.\n$ sysctl -n machdep.cpu.brand_string\nIntel(R) Core(TM) i5 CPU         760  @ 2.80GHz\nThis model information can be used on the Intel Ark website to look up specific details of the processor. For example: http://ark.intel.com/products/48496/Intel-Core-i5-760-Processor-8M-Cache-2_80-GHz199. If you've got another CPU, go to ark.intel.com and use the search box to enter your model.\nOn the details page, I noticed under the \"Advanced Technologies\" part my CPU does support VT-x but doesn't support VT-d.\nSo that would be explaining why Docker for Mac won't run on my iMac.\nNote: if EPT, VT-d and VT-x are supported, check your BIOS to enable them.\nXHyve needs the Hypervisor framework:\nOn OS X, the way of knowing if your CPU complies with all the Hypervisor.framework requirements is by checking the value of the sysctl kern.hv_support key.\n$ sysctl kern.hv_support\nkern.hv_support: 1\nIf it is 1, then your CPU is supported.\nIf it is 0, it means the Hypervisor.framework cannot be used with your CPU, for a reason or another.",
    "How do I pass a local file as an argument with docker run?": "for docker volumes, I think you need to use the absolute address. The usage is:\n$ docker run -tid --name <name_you_want> -v <host_absolute_path>:<path_inside_docker> <image_id> <cmd_such_as_bash>",
    "Strange Git Error on Docker NPM Install": "One reason for this could be that you are using the slim version of node in your Dockerfile:\nFROM node:8-slim\nI presume this does not include git, because when I changed to the full version the error went away:\nFROM node:8.11.2",
    "Unable to execute binary file in Docker container (\"Operation not permitted\")": "The SYS_RAWIO capability needs the --privileged option to access the devices. See capabilities(7).\nhttp://man7.org/linux/man-pages/man7/capabilities.7.html\n   CAP_SYS_RAWIO\n          * Perform I/O port operations (iopl(2) and ioperm(2));\n          * access /proc/kcore;\n          * employ the FIBMAP ioctl(2) operation;\n          * open devices for accessing x86 model-specific registers (MSRs, see msr(4))\n          * update /proc/sys/vm/mmap_min_addr;\n          * create memory mappings at addresses below the value specified by /proc/sys/vm/mmap_min_addr;\n          * map files in /proc/bus/pci;\n          * open /dev/mem and /dev/kmem;\n          * perform various SCSI device commands;\n          * perform certain operations on hpsa(4) and cciss(4) devices;\n          * perform a range of device-specific operations on other devices.",
    "Add supervisor in php:7-fpm image": "I see that php-fpm is located at /usr/local/sbin/php-fpm. So update your supervisor command accordingly:\n[program:php-fpm]\ncommand=/usr/local/sbin/php-fpm\nnumprocs=1\nautostart=true\nautorestart=true\nstderr_logfile=/var/log/php-fpm_consumer.err.log\nstdout_logfile=/var/log/php-fpm_consumer.out.log\npriority=100\n\n[program:blast_consumer]\ncommand=/var/www/html/current/bin/console rabbitmq:consumer blast\nnumprocs=1\nautostart=true\nautorestart=true\nstderr_logfile=/var/log/blast_consumer.err.log\nstdout_logfile=/var/log/blast_consumer.out.log\npriority=200\nTo have further information of your supervisor, do this inside container:\nsupervisorctl status\nsupervisorctl tail php-fpm",
    "Apache Tomcat 8 not starting within a docker container": "Looking at the docker run command documentation, the doc states that any command passed to the run will override the original CMD in your Dockerfile:\nAs the operator (the person running a container from the image), you can override that CMD instruction just by specifying a new COMMAND\n1/ Then when you run:\ndocker run -d -p 8082:8080 imageid tail -f /dev/null\nThe container is run with COMMAND tail -f /dev/null, the original command starting tomcat is overridden.\nTo resolve your problem, try to run:\ndocker run -d -p 8082:8080 imageid\nand\ndocker log -f containerId\nTo see if tomcat is correctly started.\n2/ You should not use the start argument with catalina.sh. Have a look at this official tomcat Dokerfile, the team uses :\nCMD [\"catalina.sh\", \"run\"]\nto start tomcat (when you use start, docker ends container at the end of the shell script and tomcat will start but not maintain a running process).\n3/ Finally, why don't you use tomcat official image to build your container? You could just use the :\nFROM tomcat:latest\ndirective at the beginning of your Dockerfile, and add you required elements (new files, webapps war, settings) to the docker image.",
    "Can't communicate with simple Docker Node.js web app [duplicate]": "Omit hostname or use '0.0.0.0' on listen function. Make it server.listen(port, '0.0.0.0', () => { console.log(Server running..); });",
    "sed inline replacement not working from Dockerfile": "The problem is in the:\nVOLUME /etc/cups/\nAfter that line, any changes to /etc/cups may not be seen in the final container (some single file copy commands do apply, so it's not perfect).\nEither move your volume line to the end, or preferably remove it from your image entirely. These entries block the ability to extend your image with changes to this folder later. And you can always make your own volume with the docker run -v ... where you want it (or in your docker-compose.yml). Creating volumes in the image means you'll get anonymous volumes listed in docker volume ls after running your image.",
    "Dockerfile VOLUME not working": "The VOLUME instruction must be placed after the RUN.\nAs stated in https://docs.docker.com/engine/reference/builder/#volume :\nNote: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\nIf you want to know the source of the volume created by the docker run command:\ndocker inspect --format='{{json .Mounts}}' yourcontainer\nwill give output like this:\n[{\n  \"Name\": \"4c6588293d9ced49d60366845fdbf44fac20721373a50a1b10299910056b2628\",\n  \"Source\": \"/var/lib/docker/volumes/4c6588293d9ced49d60366845fdbf44fac20721373a50a1b10299910056b2628/_data\",\n  \"Destination\": \"/foo/bar\",\n  \"Driver\": \"local\",\n  \"Mode\": \"\",\n  \"RW\": true,\n  \"Propagation\": \"\"\n}]\nSource contains the path you are looking for.",
    "Configure docker images as executable": "Don't forget that any parameter added to docker run override automatically the CMD defined in the image.\ndocker run myimage run_script_b.sh\nThat will run run_script_b.sh even though CMD defined run_script_a.sh.\n(provided the ENTRYPOINT was [ \"/bin/sh\", \"-c\" ].)\nThat being said, you can try build 2 different images from the same Dockerfile, setting build-time variables with --build-arg:\ndocker build --build-arg RUN=run_script_a.sh ... --name run1 .\ndocker build --build-arg RUN=run_script_b.sh ... --name run2 .\nWith Dockerfile including:\nCMD[ $RUN ]\nIf the CMD is at then end of the Dockerfile, the overhead in term of disk space will be minimum for those 2 images (instead of one image).\nThe only advantage is that you don't have to pass anything when running run1 or run2 images: they will launch their respective script.",
    "Docker nginx container exists instantly": "So I didn't know about the docker ps -a; docker logs <last container id> command. I executed this and it seemed I had a duplicated daemon off; command.\nThanks for the help guys ;)!",
    "How to use puppeteer in Docker node:20-alpine": "I had the same problem, there was no way I could get Puppeteer to work in a docker container on an AWS EC2 server, but I succeeded nonetheless. The problem was not only the Dockerfile with the cofiguration, but also the configuration of Puppeteer. In my case I needed to use the \"node-html-to-image\": \"^4.0.0\" library with the Nest.js framework, which also uses Puppeteer under the hood.\nExample of my Dockerfile:\n### Base\nFROM node:20-alpine as base\nENV NODE_ENV=some_env\n\n# Install necessary packages for Puppeteer\n# Installs latest Chromium (100) package.\nRUN apk add --no-cache \\\n    udev \\\n    ttf-freefont \\\n    chromium\n\nENV PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser\n\nUSER username\nWORKDIR /your_work_dir\n\n# Copy base dependencies describing\n# COPY ...\n\nRUN npm install\n\n\n### Builder\nFROM base as builder\n\nRUN npm install \nRUN npm run build\n\n\n### Runtime\nFROM node:20-alpine as runtime\nENV NODE_ENV=some_env\nWORKDIR /your_work_dir\n\n# Install necessary packages for Puppeteer in runtime image\nRUN apk add --no-cache \\\n    udev \\\n    ttf-freefont \\\n    chromium\n\nENV PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser\n\n\n# Copy runtime dependencies\n# COPY ...\n\nCMD [\"npm\", \"run\", \"start:prod\"]\nRun code snippetExpand snippet\nThe exact configuration of the library:\nimport nodeHtmlToImage from 'node-html-to-image';\n\n//CONFIGURATION FOR SERVER USE ONLY\nconst buffer = nodeHtmlToImage({\n            puppeteerArgs: {\n              headless: 'new',\n              executablePath: '/usr/bin/chromium-browser',\n              ignoreHTTPSErrors: true,\n              args: ['--no-sandbox', '--disable-gpu', '--disable-setuid-sandbox'],\n            },\n            html: base64Image, //html string in utf-8 format\n            type: 'png',\n          })) as Buffer\n\n//CONFIGURATION FOR LOCAL USE ONLY\nconst buffer = nodeHtmlToImage({\n        puppeteerArgs: {\n          headless: 'new',\n          defaultViewport: {\n            width: 1200,\n            height: 1080,\n          },\n        },\n        html: base64Image, //html string in utf-8 format\n        type: 'png',\n      })) as Buffer\n\n//Do anything with the buffer\nRun code snippetExpand snippet\nI have a ubuntu 22.04 distribution installed on my local machine.\nAs a result, this approach works fine in a docker container",
    "Docker build hangs when adding user with a large value of user ID / UID": "You can use the useradd's --no-log-init option (or shorter: -l):\nuseradd --no-log-init -m -s /bin/bash -N -u 1000123456 jovyan\nThe huge files created in the Docker image (/var/log/faillog and /var/log/lastlog) are sparse files. Using --no-log-init avoids creating these files (huge in the case you add a user with a large uid). Why are these files huge?\nuseradd by default reserves space for all users, so when using (say), uid 10000, it reserves space for 10000 users, even if only user 10000 is used; for this, a sparse-file is created\nSource: https://github.com/docker/hub-feedback/issues/2263#issuecomment-1205423533\nReferences (it seems to be a known issue for a long time):\nhttps://github.com/moby/moby/issues/5419\nhttps://forums.docker.com/t/run-adduser-seems-to-hang-with-large-uid",
    "Can't redirect traffic to localhost with nginx and docker": "This isn't working because your nginx proxying request to localhost which is container itself but your app is running on host's port 3000 -- outside of container. check this article\nchange\nproxy_pass http://localhost:3000/;\n  to\nproxy_pass http://host.docker.internal.\nadd 127.0.0.1  example.com my.example.com in etc/hosts",
    "Use Dockerfile ARG to determine FROM platform": "Docker provides several Automatic platform ARGs in the global scope, which include a BUILDPLATFORM and a TARGETPLATFORM. In principle you could use one of these as a default value\nARG DOCKER_PLATFORM=$TARGETPLATFORM\nFROM --platform=$DOCKER_PLATFORM python:3.9\nHowever, the default FROM --platform is the target platform, and the docker build --platform option sets this. So rather than using a custom ARG here, it may be enough for you to use the default platform\n# without a --platform option\nFROM python:3.9\nand if you do need an alternate platform, specify it when you build\ndocker build --platform=linux/amd64 ...\nI am not using BuildKit.\nBuildKit has been a non-experimental option since Docker 18.09; if it's not on by default, try setting an environment variable DOCKER_BUILDKIT=1 when you docker build.",
    "Do something using Dockerfile RUN but ignore errors": "Here is the solution:\nRUN dpkg -i wkhtmltox_0.12.6-1.focal_arm64.deb || true\nRUN apt-get update || true\nRUN apt fix-broken install || true",
    "Docker gives 'no such file or directory: unknown' on a docker run command": "In your Dockerfile you have specified the CMD as\nCMD [ \"/home/benchmarking-programming-languages/benchmark.sh -v\" ]\nThis uses the JSON syntax of the CMD instruction, i.e. is an array of strings where the first string is the executable and each following string is a parameter to that executable.\nSince you only have a single string specified docker tries to invoke the executable /home/benchmarking-programming-languages/benchmark.sh -v - i.e. a file named \"benchmark.sh -v\", containing a space in its name and ending with -v. But what you actually intended to do was to invoke the benchmark.sh script with the -v parameter.\nYou can do this by correctly specifying the parameter(s) as separate strings:\nCMD [\"/home/benchmarking-programming-languages/benchmark.sh\", \"-v\"]\nor by using the shell syntax:\nCMD /home/benchmarking-programming-languages/benchmark.sh -v",
    "Can I remove `RUN apk add --no-cache python2 g++ make` from my Dockerfile?": "Reading the PR which added that line, it seems like it was added to fix an issue with Apple M1 support for the node-gyp package. A later PR took the line back out, but that change does not seem to be reflected on the docker website.\nThat does beg the question of why it breaks on M1, but I don't have an M1 laptop, so I can't answer that.",
    "Permission denied while executing script entrypoint.sh from dockerfile in Kubernetes": "Use bash (or your preferred shell if not bash) in the entrypoint:\nENTRYPOINT [ \"bash\", \"-c\", \"./entrypoint.sh\" ]\nThis will run the entrypoint script even if you haven't set the script as executable (which I see you have)\nYou an also use this similarly with other scripts, for example with Python:\nENTRYPOINT [ \"python\", \"./entrypoint.py\" ]\nYou could also try calling the script with full executable path:\nENTRYPOINT [ \"/opt/app/entrypoint.sh\" ]",
    "Dockerfile for Go and chromedp": "Error exec: \"google-chrome\": executable file not found in $PATH running chromedp\nThis is because you did not run your go program in chromedp/headless-shell. You define multi-stage builds, but with this, only the last stage will be act as the base image of final image.\nThis means your go program in fact runs in gcr.io/distroless/base-debian11, not headless-shell.\nTo learn how to run your own program in headless-shell, you could refers to its official document:\nWhen using chromedp/headless-shell as a base image to build an image that runs your own program, You could experience zombie process problem. To reap zombie processeses, use dumb-init or tini on your Dockerfile's ENTRYPOINT\nFROM chromedp/headless-shell:latest\n...\n# Install dumb-init or tini\nRUN apt install dumb-init\n# or RUN apt install tini\n...\nENTRYPOINT [\"dumb-init\", \"--\"]\n# or ENTRYPOINT [\"tini\", \"--\"]\nCMD [\"/path/to/your/program\"]\nA minimal workable example as next.\nmain.go:\npackage main\n\nimport (\n        \"context\"\n        \"log\"\n        \"fmt\"\n        \"time\"\n\n        \"github.com/chromedp/chromedp\"\n)\n\nfunc main() {\n        ctx, cancel := chromedp.NewContext(\n                context.Background(),\n                chromedp.WithLogf(log.Printf),\n        )\n        defer cancel()\n\n        ctx, cancel = context.WithTimeout(ctx, 15*time.Second)\n        defer cancel()\n\n        err := chromedp.Run(ctx,\n                chromedp.Navigate(`https://golang.org/pkg/time/`),\n        )\n        if err != nil {\n            fmt.Println(err)\n        }\n        fmt.Println(\"done\")\n}\nDockerfile:\nFROM golang:latest as build\n\nWORKDIR /go/src/app\nCOPY ./main.go .\nRUN go mod init docker-scraper; go mod tidy\nRUN go build\n\nFROM chromedp/headless-shell:latest\nRUN apt-get update; apt install dumb-init -y\nENTRYPOINT [\"dumb-init\", \"--\"]\nCOPY --from=build /go/src/app/docker-scraper /tmp\nCMD [\"/tmp/docker-scraper\"]\ndocker-compose.yaml:\nversion: '3'\nservices:\n  goservice:\n    build: .\nExecution:\n$ docker-compose up\nRecreating chromedp-docker_goservice_1 ... done\nAttaching to chromedp-docker_goservice_1\ngoservice_1  | done\nchromedp-docker_goservice_1 exited with code 0\nYou could see no error about google-chrome now.",
    "Issue running chrome driver in docker image": "You appear to have copied a chromedriver binary built for MacOS into a Debian machine. Based on how you've tagged this question with macos and the image python:3.9.5-slim-buster you are using is amd64 based on Debian.\nI suggest you continue persisting with trying to curl the Linux 64 bit chromedriver into your machine via your Dockerfile.\nTo test it in your host OS you can simply change your current chromedriver binary to chromedriver.bakup. Download the linux version to chromedriver and rebuild your docker machine and it will copy the linux version into the new image.",
    "wget: unable to resolve host address 'github.com'": "I would check whether you can resolve github.com on your host where you're doing this build, and I would cat /etc/resolv.conf to see the resolvers of your host. If github.com resolves on your host (which you can see via nslookup github.com), then I would try to use the resolvers explicitly by either configuring the Docker daemon to use it as seen here and here or I would try to do it at a per command level as suggested in an answer here, which is kind of creative.\nRUN echo \"nameserver XX.XX.XX.XX\" > /etc/resolv.conf && \\    \n    command_depending_on_dns_resolution",
    "Do docker images share identical layers? [duplicate]": "Short answer: 1.6GB\nThis is an interesting experiment you can perform:\nPull dummy image:\ndocker pull alpine\nPrepare a Dockerfile for a child image alpine(here I created a 10MB file in the image using dd)\nFROM alpine\nRUN dd if=/dev/zero of=file.txt count=10000 bs=1024\nBuild the child image\ndocker build -t alpine-plus-ten-mb .\nThen inspect the two images and have a look at the layers.\nThe lower directory can be read-only or could be an overlay itself.\nThe upper directory is normally writable.\nThe merged directory is the unified view between upper and lower\nThe work directory is used to prepare files as they are switched between the layers.\ndocker image inspect --format='{{json .GraphDriver.Data}}' alpine\n{\n    \"MergedDir\": \"/var/lib/docker/overlay2/0654e44ddf13ebd2a0feb2ac6261e62f6c83a8be1937a71c544f69eb6208d93b/merged\",\n    \"UpperDir\": \"/var/lib/docker/overlay2/0654e44ddf13ebd2a0feb2ac6261e62f6c83a8be1937a71c544f69eb6208d93b/diff\",\n    \"WorkDir\": \"/var/lib/docker/overlay2/0654e44ddf13ebd2a0feb2ac6261e62f6c83a8be1937a71c544f69eb6208d93b/work\"\n}\n\ndocker image inspect --format='{{json .GraphDriver.Data}}' alpine-plus-ten-mb\n{\n    \"LowerDir\": \"/var/lib/docker/overlay2/0654e44ddf13ebd2a0feb2ac6261e62f6c83a8be1937a71c544f69eb6208d93b/diff\",\n    \"MergedDir\": \"/var/lib/docker/overlay2/5ca936630339967105c28d4d8c9669d99f0f449a307c43c09d60f6341cf56271/merged\",\n    \"UpperDir\": \"/var/lib/docker/overlay2/5ca936630339967105c28d4d8c9669d99f0f449a307c43c09d60f6341cf56271/diff\",\n    \"WorkDir\": \"/var/lib/docker/overlay2/5ca936630339967105c28d4d8c9669d99f0f449a307c43c09d60f6341cf56271/work\"\n}\nNote that the UpperDir of the base alpine image (...d93b/diff) appears to be LowerDir for the derived image alpine-plus-ten-mb.\nOne important aspect: the layer ...d93b/diff is read-only for the child image alpine-plus-ten-mb. In other words, that layer is guaranteed to be immutable and this allows other derived images to reuse it and build their own deltas on top of it, without duplicating(creating a copy) it.\nThese can be explored on the host system as well. Here is the ~10MB delta that I artificially added with dd when I built the child image.\nsudo du -sh \"/var/lib/docker/overlay2/5ca936630339967105c28d4d8c9669d99f0f449a307c43c09d60f6341cf56271/diff\"\n9.8M    /var/lib/docker/overlay2/5ca936630339967105c28d4d8c9669d99f0f449a307c43c09d60f6341cf56271/diff",
    "Run python file in subdirectory Docker": "Your WORKDIR is set to /project.\nYour CMD is run as './app', so that translates to '/project/app'.\nYou're copying app to '/app', so use this:\nCMD [ \"python\", \"/app/main.py\" ]",
    "Docker returning exit code 3221225781 installing vc_redist.x64.exe": "I think I figured this out. It seems like the image 'mcr.microsoft.com/dotnet/core/runtime:3.1.1' is just a \"layer\" which only contains the recipe needed to install the runtime, but doesn't contain the underlying OS specification (please correct me if that is wrong). Therefore, I first need to provide the OS, and install to that, then apply the .NET Core runtime. This seems to work:\nFROM mcr.microsoft.com/windows/servercore:ltsc2019\nWORKDIR /app\nADD https://aka.ms/vs/16/release/vc_redist.x64.exe vc_redist.x64.exe\nRUN VC_redist.x64.exe /install /quiet /norestart /log vc_redist.log\nFROM mcr.microsoft.com/dotnet/core/runtime:3.1.1",
    "Pass argument to Dockerfile from a file with docker-compose (SSH private key)": "The docs on args states that :\nYou can omit the value when specifying a build argument, in which case its value at build time is the value in the environment where Compose is running.\nIn your case, you probably want to build worker service with the following command :\nSSH_PRIVATE_KEY=\"$(cat ~/.ssh/id_rsa)\" docker-compose build\nBy the way, your docker-compose.yml is wrong (missing context) and should be :\nversion: '3.7'\nservices:\n  worker:\n    build:\n      context: .\n      args:\n        - SSH_PRIVATE_KEY",
    "VS2019 Docker support and Dockerfile failing": "I had the same problem. This links solved my problem: https://improveandrepeat.com/2019/09/how-to-fix-network-errors-with-docker-and-windows-containers/\nMy default Ethernet Adapter didn't have the lowest metric Check with:\nGet-NetIPInterface -AddressFamily IPv4 | Sort-Object -Property InterfaceMetric -Descending\nSet with:\nSet-NetIPInterface -InterfaceAlias 'Ethernet' -InterfaceMetric 4",
    "How to install Python 3.7 packages to Docker?": "You will face a similar issue with \"mimetypes\" module too. Even this is part of python base and you don't need to install it manually. Also, you can have all the required modules in a requirements.txt file and install them at once. For that, you need to copy the requirements file into the docker image before running install.\nrequirements.txt\nnibabel\npydicom\nmatplotlib\npillow\nmed2image\npandas\nxlsxwriter\nnumpy\nboto\nboto3\nbotocore\noauth2client\nurllib3\nhttplib2\napiclient\nDockerfile\nFROM alpine\nMAINTAINER <abc@gmail.com>\nFROM python:3.7\n\nCOPY requirements.txt /tmp\nWORKDIR /tmp\nRUN pip install --upgrade pip && \\\n    pip install -r requirements.txt",
    "Error in Nifi: Could not write to StandardFlowfileRecord": "In my case I was running out of open file handles, and unfortunately the error gives no indication of this. How I figured it was to:\nQuery the NiFi diagnostics. ex: curl http://localhost:8080/nifi-api/system-diagnostics\nCheck all the 'free' fields (ex. \"provenanceRepositoryStorageUsage.freeSpace\") and ensure you have enough resources.\nFor me this returned values like:\nflowFileRepositoryStorageUsage.totalSpaceBytes=-1\n-1 is reported when NiFi fails to determine these values. A proper error message will get logged as part of the diagnostics. You may need to check the logs quickly. In my case I had rolling logs setup so the log message quickly disappeared (so many processors were logging failures and filling up the logs). The error looked like:\n2019-06-03 18:48:59,619 ERROR [pool-10-thread-1] org.apache.nifi.controller.repository.WriteAheadFlowFileRepository Unable to checkpoint FlowFile Repository due to java.io.FileNotFoundException: ./flowfile_repository/journals/8243.journal (Too many open files)\njava.io.FileNotFoundException: ./flowfile_repository/journals/8243.journal (Too many open files)\n    at java.io.FileOutputStream.open0(Native Method)\n    at java.io.FileOutputStream.open(FileOutputStream.java:270)\n    at java.io.FileOutputStream.\\u003cinit\\u003e(FileOutputStream.java:213)\n    at java.io.FileOutputStream.\\u003cinit\\u003e(FileOtutputStream.java:162)\\n\\tat org.apache.nifi.wali.LengthDelimitedJournal.getOutputStream(LengthDelimitedJournal.java:136)\n    at org.apache.nifi.wali.LengthDelimitedJournal.writeHeader(LengthDelimitedJournal.java:151)\n    at org.apache.nifi.wali.SequentialAccessWriteAheadLog.checkpoint(SequentialAccessWriteAheadLog.java:306)\n    at org.apache.nifi.wali.SequentialAccessWriteAheadLog.checkpoint(SequentialAccessWriteAheadLog.java:251)\n    at org.apache.nifi.controller.repository.WriteAheadFlowFileRepository.checkpoint(WriteAheadFlowFileRepository.java:735)\n    at org.apache.nifi.controller.repository.WriteAheadFlowFileRepository$1.run(WriteAheadFlowFileRepository.java:693)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\nThe important part here is 'Too many open files'.\nTo get the current limits using linux: ulimit -x -n\nTo get the number of open files using linux: sudo ls /proc/$NIFI_PID/fd | wc -l\nI'm using docker so I increased the limit with the docker run command: docker run --ulimit nofile=1048576:1048576 ...\nOther solutions would be: increase the limits on the host machine, determine which processors are taking up the handles and re-work them (not so easy).",
    "No such file or directory : Docker-compose up": "I also had this error, it turned out to be an issue with my version of docker-compose. I'm running WSL on windows 10 and the version of docker-compose installed inside WSL did not handle volume binding correctly. I fixed this by removing /usr/local/bin/docker-compose and then adding an alias to the windows docker-compose executable alias docker-compose=\"/mnt/c/Program\\ Files/Docker/Docker/resources/bin/docker-compose.exe\"\nIf The above does not apply to you then try to update your version of docker-compose",
    "How to use multiple Dockerfiles within a monorepo?": "COPY ../ . is invalid because .. is outside the build context (by default this is the directory from where the docker build command is issued).\nTo fix that, you could build your application from the top directory ($PWD = project/), thus making all files/directories in that directory available from the build context, and specify the Dockerfile you want to use for the build :\ncd project/ && docker build -t <your_image_name> -f node-app/Dockerfile .\nor :\ndocker build -t <your_image_name> -f /path/to/project/node-app/Dockerfile /path/to/project/\nAnd of course replace COPY ../ . by COPY . ., since you are now building from the top directory.",
    "Java - Docker fails to install dependencies - Html2Pdf library": "I managed to do a workaround to this problem by using the native wkhtmltopdf library together with the wkhtmltopdf java wrapper.",
    "Docker set ENV based on if-else": "The literal conditional execution can be achieved with multistage build and ONBUILD.\nARG mode=prod\n\nFROM alpine as img_prod\nONBUILD ENV ELASTICSEARH_URL=whatever/for/prod\n\nFROM alpine as img_qa\nONBUILD ENV ELASTICSEARH_URL=whatever/for/qa\n\nFROM img_${mode}\n...\nThen you build with docker build --build-arg mode=qa .",
    ".NET Core for Linux with dockers - create docker image": "You should run the restore and publish command outside of docker file e.g. PowerShell first and then just copy the output to docker, in docker file.\n1- First run in cmd or powershell:\ndotnet restore\ndotnet publish -o ./publish\n2- In your docker file:\nFROM microsoft/aspnetcore:2.0\nWORKDIR /app\nCOPY ./publish .\nENTRYPOINT [\"dotnet\", \"Receive.dll\"]\n3- Build docker image\n4- Run docker container",
    "Golang Build in docker not finding local import": "What is missing is to put your project under $GOPATH and if you have local namespace it should also be under the same namespace $GOPATH/domain.com/.\nA proper way also to do that is to have a multi-stage builds. The basic principle involved with Multi-stage involves invoking a temporary container which can facilitate the application build, then copying the built assets out of that space into a container image that has the least amount of components required to run the app.\n# builder\nFROM golang:1-alpine AS builder\nRUN apk add git ca-certificates --update\n\nENV SERVICE_NAME my_project\nENV NAMESPACE mydomain #if you don't use namespace space just ignore it and remove it from the following lines\nENV APP /src/${NAMESPACE}/${SERVICE_NAME}/\nENV WORKDIR ${GOPATH}${APP}\n\nWORKDIR $WORKDIR\nADD . $WORKDIR\n\nRUN go build\n\n###############################################################\n\n#image\nFROM alpine\nRUN apk add ca-certificates --update\n\nENV SERVICE_NAME webservice_refArch\nENV NAMESPACE my_domain #if you don't use namespace space just ignore it and remove it from the following lines\nENV APP /src/${NAMESPACE}/${SERVICE_NAME}/\nENV GOPATH /go\nENV WORKDIR ${GOPATH}${APP}\n\nCOPY --from=builder ${WORKDIR}${SERVICE_NAME} $WORKDIR\n\nCMD ${WORKDIR}${SERVICE_NAME}",
    "How to pass dynamic values to Docker container?": "You could use an Entrypoint script:\n$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\nand\nIf the image also specifies an ENTRYPOINT then the CMD or COMMAND get appended as arguments to the ENTRYPOINT.\nSo depending on your Dockerfile you'd have something like this (python sample app):\nFROM jfloff/alpine-python:3.6\n\n# add entrypoint script\nUSER root\nCOPY start.sh /\nRUN chmod a+x /start.sh\n\nENTRYPOINT [\"/start.sh\"]\nCMD [\"arg1\"]\nand start.sh:\n#!/bin/bash\n\necho $1\n\n# don't exit\n/usr/bin/tail -f /dev/null\nNow you can do something like:\n15:19 $ docker run f49b567f05f1 Hello\nHello\n15:21 $ docker run f49b567f05f1\narg1\nNow if your script is set up to take those arguments, you should be able to run it as you want. Reference from Docker is attached, search for \"Overriding Dockerfile image defaults\" in this and then look in the CMD section.\nOr check this Post.",
    "How to run psql commands in a postgres docker container handled by docker-compose?": "you can run some scripts in posgres when you mount the folder with your script to docker container at /docker-entrypoint-initdb.d/ se the docker-compose.yml that mounts the folder sql. The postgres images is awesome and will run the scripts each time the postgres starts...\nversion: '3.6'\n\nservices:  \n  my_db:\n    image: postgres:alpine\n    volumes: [\"/somepath/sql/:/docker-entrypoint-initdb.d/\"]\nthe file structure of /somepath/\n- docker-compose.yml\n- sql\n  - new_extension.sql\nand cat sql/new_extension.sql\n/* create extension for cureent DB */\nCREATE EXTENSION IF NOT EXISTS citext;\n/* create extension over the template1 so all created databases after that also have the extension */\n\\c template1\nCREATE EXTENSION IF NOT EXISTS citext;",
    "Docker copy from one container to another": "If copy --from=publish [\"C:\\\\Program Files\\\\nodejs\", \"C:\\\\nodejs\"] gives:\nERROR: Service 'myapp.api' failed to build: COPY failed: \n CreateFile \\\\?\\Volume{acdcd1b2-fe0d-11e7-8a8f-10f00533bf2a}\\C:Program Filesnodejs:\nTry double-escape:\ncopy --from=publish [\"C:\\\\\\\\Program Files\\\\\\\\nodejs\", \"C:\\\\\\\\nodejs\"]\nOr consider as in this Dockerfile the alternative syntax:\ncopy --from=publish C:\\Program Files\\nodejs C:\\nodejs\nHowever, that aforementioned dockerfile does use both syntax without any issue. For example:\nCOPY --from=SetupPhase [\"C:\\\\Program Files (x86)\\\\Microsoft SDKs\", \"C:\\\\Program Files (x86)\\\\Microsoft SDKs\"]\nBut: it does, before the copy --from:\nRUN icacls 'C:\\\\Program Files (x86)\\\\WindowsPowerShell\\\\Modules' /reset /t /c /q \nRUN attrib -h -r -s 'C:\\\\Program Files (x86)\\\\WindowsPowerShell\\\\Modules' /s\nRUN attrib -h -r -s \"C:/Windows\" /s\n(Replace those paths by the one you want to access)\nThat might explain why it can copy from another Windows image: no access problem because the ACLs were reset.\nThe OP Luka confirms:\nAdd these lines to build\nRUN icacls \"C:\\\\Program Files\\\\nodejs\" /reset /t /c /q \n  RUN attrib -h -r -s \"C:\\\\Program Files\\\\nodejs\" /d /s\nEdited the copy line in final to this:\nCOPY --from=build [\"C:\\\\\\\\Program Files\\\\\\\\nodejs\", \"/nodejs\"]",
    "Is copying /node_modules inside a Docker image not a good idea?": "Public Dockerfiles out there are trying to provide generalized solution. Having dependencies coded in package.json makes it possible to share only one Dockerfile and not depend on anything not public available.\nBut at runtime Docker does not care how files got to container. So this is up to you, how you push all needed files to your container.\nP.S. Consider layering. If you copy stuff under node_modules/, do it in one step, by that only one layer is used.",
    "How to know which command or compose file has been used to start Docker containers?": "i think no option to know which docker-compose file is use. but you can check manual every you project folder.\nthe docker-compose mechanism is by matching the docker-compose.yml file. so if you run command sudo docker-compose ps in every your project folder. docker-compose will match between the docker-compose file used by container and docker-compose file in your project, if the same than the results will be displayed, if not the results is not displayed",
    "Dockerfile contains a python script that writes to an output file, but the output file is not being created on the container": "Change your text_file to /log/output.txt\nBuild image doesn't run CMD.\nAfter your build the image, you run your container, do\ndocker run -it --rm -v $(pwd)/log:/log imagename\nYour script write inside the container and exit right the way. So you won't see it in your current directory. The way to solve this is mount a host directory into the container, and let the script write inside it, so that the data can be persistent.",
    "Keep an Nginx alive after a bash script": "You need to use: nginx -g \"daemon off;\", with the option quoted.",
    "Deploying .net core app to docker gives \"Microsoft.DotNet.Props\" not found error": "As I see Web.xproj in your error, look like you have the same problem as described in this github issue. The root of such problem is that Microsoft updated their docker to the newest SDK which moves back to .csproj from project.json. The solution is to use another, 1.1-sdk-projectjson tag:\nFROM microsoft/dotnet:1.1-sdk-projectjson\nNote on microsoft/dotnet/ docker page:\nThe latest tag no longer uses the project.json project format, but has now been updated to be csproj/MSBuild-based. If you do not wish to migrate your existing projects to MSBuild simply change your Dockerfile to use the 1.1.0-sdk-projectjson or 1.1.0-sdk-projectjson-nanoserver tag. Going forward, new .NET Core sdk images will be MSBuild-based.",
    "What is | bash -": "The | bash means to pipe the output from the curl command, i.e. the downloaded bash script, as input to the bash command. The - makes bash read the script from stdin instead of from a file.\nIn other words, the command downloads a script and executes it with bash.",
    "How to execute script during image build in Docker": "The correct directive for this is RUN. This would run the script on image build. in your case -\nRUN /usr/local/bin/setup_php_settings\nCMD bash",
    "Docker run command ignoring part of Dockerfile CMD when ENTRYPOINT present": "You probably want\nCMD [\"/virtualenv/bin/python /mycode/myscript.py --param1\"]\ninstead of\nCMD [\"/virtualenv/bin/python\", \"/mycode/myscript.py\", \"--param1\"]\nWhen CMD and ENTRYPOINT are both present in Dockerfile, CMD works as default parameters to ENTRYPOINT. So you basically doing\nbash -c \"/virtualenv/bin/python\" \"/mycode/myscript.py\" \"--param1\"\nwhen you want\nbash -c \"/virtualenv/bin/python /mycode/myscript.py --param1\"\nhttps://docs.docker.com/engine/reference/builder/#cmd https://docs.docker.com/engine/reference/builder/#entrypoint https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact",
    "Make Dockerfile VOLUME behave like docker-compose volumes": "The VOLUME command in Dockerfile has a slightly different meaning:\nyou cannot define mounted volumes within a Dockerfile\nit's only used for specifying inter-container volumes\nSo in your Dockerfile, VOLUME ./:/app/ must be changed to VOLUME /app/. Your docker-compose definition will tell docker to mount local dir . to /app in the container.\nThe -v option and the volumes tag in docker-compose work the same way. The host directory is mounted to a container directory. Any file which was present in the image in the container directory will be replaced by the files from the host directory.",
    "Best way to pass config file during Docker build": "What you want is an image that when built doesn't include the license file. It needs to be added each time the container starts.\nI can think of several ways of doing this, the most obvious is to look for a mounted volume of a fixed name containing the license file at startup, and if not found exit with a message.\nThis Dockerfile illustrates the idea\nFROM ubuntu:14.04\n\nRUN echo \"#!/bin/bash\" >> /bin/startup.sh\nRUN echo \"if [ ! -e /license/license.txt ] \" >> /bin/startup.sh\nRUN echo \"then \" >> /bin/startup.sh\nRUN echo \"   echo 'missing license'\" >> /bin/startup.sh\nRUN echo \"exit 1 \" >> /bin/startup.sh\nRUN echo \"fi \" >> /bin/startup.sh\nRUN echo \"top \" >> /bin/startup.sh\n\nRUN chmod +x /bin/startup.sh\n\nENTRYPOINT [\"/bin/startup.sh\"]\nRunning this without a license director will cause it to not start. Running with a license will run top forever.\n> docker run -it test\nmissing license\n> docker run -v `pwd`/license:/license -it test\n[works]",
    "change PATH for building docker image": "For the new version of Docker, you have to change daemon.json.\n sudo nano /etc/docker/daemon.json\nthen add the path to the deamon JSON (In my case path is '/media/newhd/containers/')\n{\"data-root\":\"/media/newhd/containers/\"}\nAfter this restart the docker...",
    "Docker Help : Creating Dockerfile and Image for Node.js App": "It would have been best to ask 4 separate questions rather than put this all into one question. But:\n1) Yes, use the Node image.\n2) The \"regular\" image includes various development libraries that aren't in the slim image. Use the regular image if you need these libraries, otherwise use slim. More information on the libraries is here https://registry.hub.docker.com/_/buildpack-deps/\n3) You would probably be better off by putting the code into a data-container that you add to the container with --volumes-from. You can find more information on this technique here: https://docs.docker.com/userguide/dockervolumes/\n4) I don't understand this question. Note that amazon now have a container offering: https://aws.amazon.com/ecs/",
    "addAfterColumn is not allowed on postgresql": "remove \"afterColumn\" so it works in fresh ENV. to avoid Validation failure in an ENV where the change set is already executed, adding the original checksum works:\n  <changeSet>\n    <validCheckSum>9:6a4c63a04890c57dd610abc39389b4ce</validCheckSum>\n        or\n    <validCheckSum>ANY</validCheckSum>",
    "What is the best way to use existing external MySQL to Dockerized app?": "You have MySQL running in a Docker Container on a Host.\nDifferent scenario's to connect to that MySQL Database:\nFrom an App in another Container started in the same docker-compose.yml\nIf you started an App in the same docker-compose.yml and want to connect to the MySQL Database:\nUse the Service as the Hostname because docker-compose has its own network and will translate the servicename to to right container\nversion: '3.9'\n\nservices:\n\n  mydb:\n    image: mysql:latest\n    volumes:\n      - \"./.mysql-data/db:/var/lib/mysql\"\n    restart: always\n    environment:\n      MYSQL_DATABASE: mysqldb\n      MYSQL_USER: root\n      MYSQL_PASSWORD: root\n      \n  myapp:\n    ....\nIn myapp you connect to MySQL with mysql://mydb:3306/....\nNo Ports section!! This is because MySQL itself publishes port 3306, so no Ports section needed in docker-compose.\nFrom an App in another Container started in another Docker command\nSuppose your App is started on the same Host but in another Docker command (or in another docker-compose.yml file)\nThen you need to access the database via the Host published port. But there's a problem since you cannot access the host from within a Docker Container. (At least not in a stable way, so I leave out the hacky solutions you see here on StackOverflow)\nJust create a (sub)domain and point it to your Host.\nAnd make small change in your docker-compose.yml file: Add a Ports section:\n    ports:\n      - 3333:3306\nThe database can be found on mysql://subdomain:3333/.....\nFrom an App running somewhere else on another Host.\nThis is the same as the previous solution:\nCreate subdomain\nPublish the port\nConnect to that subdomain\nFrom Docker Container on Host to MySQL on another Server (not in Docker)\nAgain, same as before\nCreate subdomain\nPublish the port on the Host\nConnect to that subdomain\nSolution with the subdomain need a ProxyServer?\nIn case your host also answers other requests from outside, you might need a ProxyServer (Nginx?) to route the 3333 traffic to port 3306 in the container.\nAnd if you do have that ProxyServer, you don't need the Ports section anymore.\n(Link to article I wrote about this ProxyServer solution)\nAnswer to the question in the Problem Description\nYou want to connect to a MySQL database running on some AWS host.\nThat database is already running. It has a host, username, password.\nWhy are you starting a new Docker Container with MySQL Image?\nOnly thing you should start is that 'sampleapp' container with software connecting to the external MySQL.\nI don't understand why you start a MySQL Container again if you already have an external one running on AWS.",
    "resolv.conf seems to always be wrong when building Docker container": "I was able to at least get my docker to build the containers by taking the same naming conventions used in the Docker Daemon CLI options and applying them to a /etc/docker/daemon.json manually, then restarting the Docker Daemon.\nRead the host /etc/resolv.config (Yours will likely be different)\n$ cat /etc/resolv.config\n# Generated by expressvpn\nsearch expressvpn\nnameserver 10.53.0.1\nMake a new, or use the /etc/docker/daemon.json (I had to use Super User to write the file)\n$ sudo touch /etc/docker/daemon.json\nUse the Daemon file to manually set the Virtual Network to the host /etc/resolv.conf output as described in #1 (Again yours is likely to be different). You can find the different options here just use the CLI options as keys and arrays with strings as values.\n{\n    \"dns\": [\n        \"10.53.0.1\"\n    ],\n    \"dns-search\": [\n        \"expressvpn\"\n    ]\n}\nHard stop all docker processing\n$ sudo ps axf | grep docker | grep -v grep | awk '{print \"kill -9 \" $1}' | sudo sh \nRestart Docker Daemon\n$ sudo dockerd\nThis is not the most elegant solution, but I was able to at least get my Docker to build the Container and continue on with my work.",
    "How to restore postgreSQL in docker-compose": "Based on the docker image documentation you can include initialization scripts if you mount under /docker-entrypoint-initdb.d. It should work with both *.sql, *.sql.gz, or *.sh. So in your example, the compose file should look something like this:\npostgres:\n    container_name: postgres\n    image: postgres\n    environment:\n      POSTGRES_PASSWORD: \"postgres\"\n    ports:\n      - \"15432:5432\"\n    volumes:\n      - /root/database:/var/lib/postgresql/data\n      - /root/database-init/init-user-db.sql:/docker-entrypoint-initdb.d/init-user-db.sql:ro\n    networks:\n      - production-network\n    restart: unless-stopped\n    depends_on:\n      - rest-proxy\n      - broker\nAssuming that init-user-db.sql file contains your initialization scripts.",
    "Numpy Multi-stage Container Build -- Alpine": "From looking at your Dockerfile, I would suggest against using alpine and multi-stage builds and instead install pre-compiled wheels in a debian-based python image. The Dockerfile below installs your requirements without having to compile anything. The build time is fast, and the size is relatively small. The -slim image does not include build tools, so the image is smaller.\nFROM python:3.8-slim\nENV VIRTUAL_ENV=\"/opt/venv\"\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\nRUN python3 -m venv $VIRTUAL_ENV \\\n    && pip3 install --no-cache-dir \\\n        numpy==1.20.3 \\\n        scipy==1.6.3\nIf you do insist on using alpine, please continue reading, I address building a multi-stage alpine image in the following paragraph. Be advised that the size advantage is not great... The debian-based image above is 284 MB and the alpine-based image below is 211 MB.\nThe problem is that you install numpy in a debian-based image, and then you copy that into an alpine image. Alpine uses musl C whereas debian and other linux distributions use glibc. They are not compatible. Numpy does not ship pre-compiled wheels for musl C, so if you want to use alpine, you will have to compile numpy. I have included a minimal dockerfile that shows how. It can take over 20 minutes for the image to build, because numpy and scipy must be compiled from source.\n# Define these variables once and use throughout the dockerfile.\n# This reduces chance of bugs...\nARG BASE_IMAGE=\"python:3.8-alpine\"\nARG VIRTUAL_ENV=\"/opt/venv\"\n\nFROM $BASE_IMAGE AS builder\nARG VIRTUAL_ENV\nENV VIRTUAL_ENV=$VIRTUAL_ENV \\\n    PATH=\"$VIRTUAL_ENV/bin:$PATH\"\nRUN apk add --no-cache \\\n        build-base \\\n        gcc \\\n        gfortran \\\n        openblas-dev \\\n    && python3 -m venv $VIRTUAL_ENV \\\n    && pip3 install --no-cache-dir \\\n        numpy==1.20.3 \\\n        scipy==1.6.3\n\nFROM $BASE_IMAGE AS production\nARG VIRTUAL_ENV\nCOPY --from=builder $VIRTUAL_ENV $VIRTUAL_ENV\nENV VIRTUAL_ENV=$VIRTUAL_ENV \\\n    PATH=\"$VIRTUAL_ENV/bin:$PATH\"\nRUN apk add --no-cache openblas",
    "No module named conda in Docker": "miniconda3 version specified in installation line inside the Dockerfile is the not the latest one\nThat Dockerfile that you used to build local image will install miniconda3-4.5.11 not the latest version. You can find it here:\n...\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ~/miniconda.sh && \\\n    /bin/bash ~/miniconda.sh -b -p /opt/conda &&\n...\nAnd also in this way with docker:\n$ docker build --tag miniconda3:test .\n$ docker docker run -i -t miniconda3:test /bin/bash\n$ docker history --no-trunc miniconda3:test | grep Miniconda3\n\n/bin/sh -c wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ~/miniconda.sh && ...\nOk, now let's look at official continuumio/miniconda3:\n$ docker run -i -t continuumio/miniconda3 /bin/bash\nand then:\n$ docker history --no-trunc continuumio/miniconda3 | grep Miniconda3\n\n/bin/sh -c wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh && ...\nAs you can see continuumio/miniconda3 image from DockerHub installs latest miniconda3 4.8.2 and not the 4.5.11 version. Thus, your local image built from that Dockerfile will produce container with miniconda3:4.5.11.\nChange in python version breaks conda\nNow, let's figure out why conda fails. First build and run:\n$ docker build --tag miniconda3:test .\n$ docker docker run -i -t miniconda3:test /bin/bash\nGet some info:\n(base) root@61cafd17d954:/# conda info\n\n     active environment : base\n    active env location : /opt/conda\n            shell level : 1\n       user config file : /root/.condarc\n populated config files : \n          conda version : 4.5.11\n    conda-build version : not installed\n         python version : 3.7.0.final.0\n       base environment : /opt/conda  (writable)\n           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n                          https://repo.anaconda.com/pkgs/main/noarch\n                          https://repo.anaconda.com/pkgs/free/linux-64\n                          https://repo.anaconda.com/pkgs/free/noarch\n                          https://repo.anaconda.com/pkgs/r/linux-64\n                          https://repo.anaconda.com/pkgs/r/noarch\n                          https://repo.anaconda.com/pkgs/pro/linux-64\n                          https://repo.anaconda.com/pkgs/pro/noarch\n          package cache : /opt/conda/pkgs\n                          /root/.conda/pkgs\n       envs directories : /opt/conda/envs\n                          /root/.conda/envs\n               platform : linux-64\n             user-agent : conda/4.5.11 requests/2.19.1 CPython/3.7.0 Linux/5.4.0-48-generic debian/10 glibc/2.28\n                UID:GID : 0:0\n             netrc file : None\n           offline mode : False\nWell, we have conda:4.5.11 with python:3.7.0.\nNow, we are going to install jupyter, for example:\n(base) root@61cafd17d954:/# conda install jupyter\nYou may notice, that this installation will update python:\nThe following packages will be UPDATED:\n\n...\n    python:             3.7.0-hc3d631a_0        --> 3.8.5-h7579374_1       \n...\nIf you proceed, this will update python and will break conda:\n...\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nTraceback (most recent call last):\n  File \"/opt/conda/bin/conda\", line 7, in <module>\n    from conda.cli import main\nModuleNotFoundError: No module named 'conda'\nThis is quite known issue and you can find more info on this issue in How does using conda to install a package change my python version and remove conda? answer, and in official conda repo: No module named conda.cli.main #2463.\nUpdate conda or use Dockerfile for the miniconda3:latest\nThere are 3 possible solutions to this issue:\nEdit your Dockerfile by replacing this line:\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ~/miniconda.sh && \\\nwith\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh && \\\nUse the latest official Dockerfile from ContinuumIO, you can find it here.\nUpdate conda inside container before usage:\n(base) root@61cafd17d954:/# conda update conda",
    "Auto answering multiple prompts of a bash command in (Bash/Docker RUN) non interactively": "Technically, you can auto-answer prompts with a tool like expect. However, that's usually much more difficult than getting the program to stop asking you questions.\nI'm not sure why apt is asking for your timezone, but I suspect that you're pulling in the tzdata package somehow, which wants to configure your timezone. To avoid these questions, you should set the frontend to non-interactive:\nTo run dpkg (behind other tools like Apt) without interactive dialogue, you can set one environment variable as\nDEBIAN_FRONTEND=noninteractive\n(Source.)\nIn a Dockerfile, you can set an environment variable like this:\nENV DEBIAN_FRONTEND=noninteractive",
    "Dockerfile COPY ${source:-obj/Docker/publish} not finding the correct source after upgrade to 3.1": "Some debugging suggestions in no certain order.\nDefault Value\n${source:-obj/Docker/publish} is bash syntax that evaluates to the value of the variable source if it was defined, or obj/Docker/publish if it was not.\nI'm trying to figure out where the source argument is defined, and how I should change this.\nIf you can't find anything that defines it to another value, it just means that the default value obj/Docker/publish was being used.\nPublish directory\n3.1 still uses the publish directory, but it will be at bin/Release/netcoreapp3.1/publish. Use a RUN find -type d -iname publish to find the publish directories in your container.\nDont use the full path, it will change every build. See the tmp/docker-builder356954794 in the path?\nThe COPY command uses relative files, based on the path in your machine (outside the docker container). So use a path that's relative to where the context directory (or Dockerfile) is. If the dockerfile is at /var/lib/jenkins/jobs/fusion-core/branches/master/workspace/src/CustomerAPI/Dockerfile and the publish directory on your machine is at /var/lib/jenkins/jobs/fusion-core/branches/master/workspace/src/CustomerAPI/bin/Release/netcoreapp3.1/publish, then use a COPY command like this:\nCOPY bin/Release/netcoreapp3.1/publish .\nFunny paths\nI'm not sure where the \"/var/lib/docker/tmp/docker-builder356954794\" is coming from\nContainers need to share files from one directory (in your host) to another (in your container). Depending on the container technology, it uses one or more file systems or file directories.\nOne of those is /var/lib/docker/tmp/...",
    "Getting Docker ID in Flask Application": "The hostname in the docker container is a short hash of its id. In turn you can use this to get the hostname in Python/Flask:\nimport socket\ndocker_short_id = socket.gethostname()",
    "How can I execute a .sh file after cd command in a dockerfile?": "To combine several command calls use operator &&. Additionally you can use one make call with two targets\ncd /portable && \\\n       ./autogen.sh && \\\n       ./configure --prefix=/opt/libressl --enable-nc && \\\n       make check install",
    "Running an exe file in windows container using docker": "Found out that the exe was running fine and file was created. The way I logged into the container was wrong.\nI should have used:\ndocker exec -it containername powershell\nThis runs a new instance of a non-existing image\ndocker run -it --entrypoint powershell imagename",
    "Have dockerfile overwrite a file in a volume": "When you add a volume to your Docker services, the data in the volume will overwrite any existent data from the Docker image. If you want to have Docker image files that can be used as default files, you need to do the following\nStore the files in the Docker image predefined file ie. (/path/to/default)\nAdd an entry point to your Docker file, this entry point should take care of copying the default file from /path/to/default to the volume path /path/to/data\nDockerfile\nFrom ruby:2.4.5\nCOPY config.xml /path/to/default/config.xml\nENTRYPOINT  [ \"/docker-entrypoint.sh\" ]\ndocker-entrypoint.sh\n#!/bin/sh -e\n\ncp -r /path/to/default/config.xml /path/to/data\nexec \"$@\" # or replace this by the command need to run the container",
    "Custom Docker Image with Azure CLI base for authentication": "Posting an update with an answer here just in case anyone else has a similar problem. I haven't found any other solutions to this so I had to make my own. Below is my Dockerfile. Right now the image is sitting at 1GB so I will definitely need to go through and optimize, but I'll explain what I did:\n#1 Install .NET Core SDK Build Environment\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build-env\nWORKDIR /app\n\n#2 Build YummyApp\nCOPY . ./\nRUN dotnet publish YummyAppAPI -c Release -o out\n\n#3 Install Ubuntu Base Image\nFROM ubuntu:latest\nMAINTAINER yummylumpkins <yummy@lumpkins.com>\nWORKDIR /app\nENV ASPNETCORE_URLS=http://+:80\nEXPOSE 80\n\n#4 Install package dependencies & .NET Core SDK\nRUN apt-get update \\\n    && apt-get install apt-transport-https \\\n    && apt-get update \\\n    && apt-get install -y curl bash dos2unix wget dpkg \\\n    && wget -q https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb \\\n    && dpkg -i packages-microsoft-prod.deb \\\n    && apt-get install -y software-properties-common \\\n    && apt-get update \\\n    && add-apt-repository universe \\\n    && apt-get update \\\n    && apt-get install apt-transport-https \\\n    && apt-get update \\\n    && apt-get install -y dotnet-sdk-3.1 \\\n    && apt-get update \\\n    && rm packages-microsoft-prod.deb\n\n#5 Copy project files from earlier SDK build\nCOPY --from=build-env /app/out .\n\n#6 Install Azure CLI for AppAuthorization\nRUN curl -sL https://aka.ms/InstallAzureCLIDeb | bash\n\n#7 Login to Azure Services and run application\nCOPY entrypoint.sh ./\nRUN dos2unix entrypoint.sh && chmod +x entrypoint.sh\nCMD [\"/app/entrypoint.sh\"]\nStep 1 - Install .NET Core SDK Build Environment: We start with using .NET Core SDK as a base image to build my application. It should be noted that I have a big app with one solution and multiple project files. The API project is dependent on the other projects.\nStep 2 - Build YummyApp: We copy the entire project structure from our local directory to our working directory inside the docker image (/app). Just in case anyone is curious, my project is a basic API app. It looks like this:\n[YummyApp]\n  |-YummyAppDataAccess\n     |YummyAppDataAccess.csproj\n  |-YummyAppInfrastructure\n     |YummyAppInfrastructure.csproj\n  |-YummyAppAPI\n    |-YummyAppAPI.csproj\n  |-YummyAppServices\n    |-YummyAppServices.csproj\n  |-YummyApp.sln\nAfter we copy everything over, we build/publish a Release configuration of the app.\nStep 3 - Install Ubuntu Base Image: We start a new layer using Ubuntu. I initially tried to go with Alpine Linux but found it almost impossible to install Azure CLI on it without having to do some really hacky workarounds so I went w/ Ubuntu for the ease of installation.\nStep 4 - Install package dependencies & .NET Core SDK: Inside the Ubuntu layer we set our work directory and install/update a bunch of libraries including our .NET Core SDK. It should be noted that I needed to install dos2unix for a shell script file I had to run later on. . .I will explain later.\nNote: I initially tried to install .NET Core Runtime only as it is more lightweight and would bring this image down to about 700MB (from 1GB) but for some reason when I tried to run my application at the end of the file (Step 7) I was getting an error saying that no runtime was found. So I went back to the SDK.\nStep 5 - Copy project files from earlier SDK Build: To save space, I copied the built project files from the first 'build image' over to this Ubuntu layer to save some space (about 1GB worth).\nStep 6 - Install Azure CLI: In order to authorize my application to fetch a token from Azure Services, normally I use Microsoft.Azure.Services.AppAuthentication. This package provides a method called AzureServiceTokenProvider() which (via my IDE) authorizes my application to connect to Azure Services to get a token that is then used to access the Azure Key Vault. This whole issues started because my application is unable to do this from within a docker container, because Azure doesn't recognize the request coming from the container itself.\nSo in order to work around this, we need to login via az login in the Azure CLI, inside the container, before we start the app.\nStep 7 - Login to Azure Services and run application: Now it's showtime. I had two different problems to solve here. I had to figure out how to execute az login and dotnet YummyAppAPI.dll when this container would be fired up. But Dockerfiles only allow one ENTRYPOINT or CMD to be executed at runtime, so I found a workaround. By making a shell script file (entrypoint.sh) I was able to put both commands into this file and then execute that one file.\nAfter setting this up, I was getting an error with the entrypoint.sh that read something like this: entrypoint.sh: executable file not found in $PATH. I found out that I had to change the permissions of this file using chmod because otherwise, my docker container was unable to access it. That made the file visible, but the file was still unable to execute. I was receiving another error: Standard_init_linux.go:211: exec user process caused \u201cno such file or directory\u201d\nAfter some more digging, it turns out that this problem happens when you try to use a .sh file created in Windows on a Linux-based system. So I had to install dos2unix to convert this file to something Linux compatible. I also had to make sure the file was formatted correctly. For anyone curious, this is what my entrypoint.sh looks like:\n#!/bin/sh\nset -e\n\naz login -u yummy@lumpkins.com -p ItsAlwaysYummy\ndotnet /app/YummyAppAPI.dll\n\nexec \"$@\"\nNote: The login and password is hard-coded. . .I know this is bad practice (in fact, it's terrible) however, this is only for my local machine and will never see production. The next step would be to introduce environment variables with a service principle login. Since this deployment will eventually happen in the Azure Devops pipeline, I can inject those ENV vars straight into the devops pipeline YAML so that all of this happens without me ever punching in credentials; they will come straight from the Key Vault where they are stored.\nLastly, the size of this container is huge (1GB) and it does need to be optimized if it will be updated/built regularly. I will continue working on that but I am open to suggestions on how best to do that moving forward.\nThanks again all.",
    "Calling different commands in Dockerfiles depending on environment": "The Dockerfile is running in a 'build' context, so any variables available are related to the build environment (when you run docker build), not the execution environment. The build process is running only the first time when you build the image.\nIf you want to use environment variables defined at execution time, you could use a CMD pointing to a container script. Inside this script, all environment variables are available from the initial execution (container start).\nDockerfile\n...\nCOPY ./scripts /script/path\nCMD /script/path/test.sh\n./scripts/test.sh\ncd /your/app/path\necho ENV = $ENV\nnpm run start:$ENV\nAlso you could review the best practices for Dockerfiles with good examples and use cases\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/",
    "Pushing images to Docker Hub for multiple architectures (e.g. amd64, arm64) and pulling the correct one automatically": "I'd recommend using BuildKit with buildx which is available in 19.03. First you probably want some setup on a Linux host using qemu and binfmt_misc for cross compiling. Without it, you would need a build node for each platform you want to build. With binfmt_misc, you need two important details to work inside of a container, first is you need the static user binaries, and second is the --fix-binary flag needs to be used when injecting them into the kernel. For the first, that comes down to the package name you install, e.g. on Debian the package name is qemu-user-static. And for the second, this may require a version of the package from from an unstable release. E.g. here are a few bug reports to get the change included:\nhttps://bugs.debian.org/cgi-bin/bugreport.cgi?bug=868030\nhttps://bugs.launchpad.net/ubuntu/+source/qemu/+bug/1815100\nOnce you've done this, you can verify the --fix-binary result by looking for the F flag in /proc/sys/fs/binfmt_misc/*.\nNext, you need to setup a buildx worker. That can be done with:\ndocker buildx create --driver docker-container --name local --use \\\n  unix:///var/run/docker.sock\ndocker buildx inspect --bootstrap local\nYou should see something like the following from the inspect, note the multiple platforms:\n$ docker buildx inspect --bootstrap local\n[+] Building 54.1s (1/1) FINISHED\n => [internal] booting buildkit                                                                                                  54.1s\n => => pulling image moby/buildkit:buildx-stable-1                                                                               45.4s\n => => creating container buildx_buildkit_local0                                                                                  8.7s\nName:   local\nDriver: docker-container\n\nNodes:\nName:      local0\nEndpoint:  unix:///var/run/docker.sock\nStatus:    running\nPlatforms: linux/amd64, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/arm/v7, linux/arm/v6\nNow you can perform a build for multiple architectures. The $image_and_tag must be an external registry where buildx can push the image. You cannot have a multi-arch image locally because docker images locally must be a single platform, but the registry like Docker Hub does support multi-arch manifests:\ndocker buildx build --platform linux/amd64,linux/arm64 \\\n  --output type=registry -t $image_and_tag .\nAnd you can even test those other images using the qemu cross platform support:\ndocker container run --platform linux/arm64 $image_and_tag\nNote that you may need to enable experimental CLI options in docker, I forget which features have not made it to GA yet. In ~/.docker/config.json, add:\n{\n  \"auths\": {\n    ...\n  },\n  \"experimental\": \"enabled\"\n}\nOr you can export a variable (adding to your .bashrc to make it persistent):\nexport DOCKER_CLI_EXPERIMENTAL=enabled\nNote: docker desktop has included settings for qemu/binfmt_misc for a while, so you can skip straight to the buildx steps in that environment. Buildx can also be run as a standalone tool. See the repo for more details: https://github.com/docker/buildx",
    "Associate Dockerfile language with all Dockerfiles in VS Code": "The language identifier for Dockerfile is a lower-case dockerfile as stated on https://code.visualstudio.com/docs/languages/identifiers. Hence, you need to adjust your snippet as follows:\n\"files.associations\": {\n    \"*Dockerfile*\": \"dockerfile\"\n}",
    "Could not create SSL/TLD secure channel in Docker build": "A bit late, but the RUN [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12; is lost when the RUN command is finished. You need to include that in the same RUN for example:\nRUN [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 ; `\nInvoke-WebRequest \"https://dist.nuget.org/win-x86-commandline/v$env:NUGET_VERSION/nuget.exe\" -UseBasicParsing -OutFile \"$env:ProgramFiles\\NuGet\\nuget.exe\"",
    "How to specify build target in kaniko for multi-stage Dockerfile?": "It appears to be --target, which is in the README but is better described in the feature request.",
    "docker-compose connecting to other container fails": "You need to:\ndefine a common network for your containers\nexpose the port 5001 of your flaskapp container\nIndeed, according to the documentation, the EXPOSE instruction \"exposes ports without publishing them to the host machine - they\u2019ll only be accessible to linked services\". So it allows communication between the container which \"expose\" the port, and other containers in the same network.\nTry something like that:\nversion: '3'\nservices:\n    flaskapp:\n        build: ./rest_server\n        expose:\n            - 5001\n        networks:\n            - docker_network\n\n    web:\n        build: ./web\n        restart: always\n        ports:\n            - 3002:3000\n        networks:\n            - docker_network\n        depends_on:\n            - flaskapp\n\nnetworks:\n  docker_network:\n    driver: bridge",
    "Starting a Windows service in a Docker container": "Instead of using CMD, I used RUN to commit the command to the image, and used multiple RUN commands:\n# Enable Session State Server\nRUN powershell -Command Set-Service aspnet_state -startuptype automatic \nRUN powershell -Command Stop-Service aspnet_state\nRUN powershell -Command Start-Service aspnet_state \nRUN powershell -Command Set-ItemProperty Registry::HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\aspnet_state\\Parameters -Name  AllowRemoteConnection -Value 1 ",
    "Docker push fails after multiple retries on a specific layer": "I had the same issue today, then I found a typo in my ECR location, once it is corrected, the push succeeded immediately, so I believe Docker just doesn't tell you that it can't find the ECR repo and stuck in retrying.",
    "\"coverage run app.py\" inside a docker container not creating .coverage file?": "For anyone stumbling here from google like I did...\nTo extend Wolfium's answer, this is caused by docker killing the python process before coverage has written data to it's datafile (or more correctly, python not being able to interpret docker's default SIGTERM signal gracefully and just falling in a heap when it gets a SIGKILL some seconds later). The quickest way to solve this is to:\nensure the python process is able to receive a relevant signal. If you're running this inside a shell entrypoint script, you'll probably want to prepend with exec, e.g. exec coverage -m swagger_server.\ndefault the stop signal for your image to SIGINT, which most python handlers implement: add the line STOPSIGNAL SIGINT to your Dockerfile.\nThe slightly better solution is to make sure your python handler can respond to SIGTERM appropriately.",
    "Pass environment variable to dockerfile build": "The environment file is not part of the build process, it is used when running the container.\nYou need to use build-args. In docker-compose you can specify build args in the file:\nbuild:\n  context: .\n  dockerfile: Dockerfile-db\n  args:\n    DB_NAME: some_name\n    DB_USER: postgress\n    ...\nThis might not be a good idea if you want to publish the composefile, as you are storing credentials in it. You can explicitly build and pass --build-arg\ndocker-compose build --build-arg DB_NAME= some_name ...\nAnd when running specify no build in docker-compose run --no-build\nUpdate:\nAs suggested by @gonczor, a shorter and cleaner syntax to use pass the env file as build args is:\ndocker-compose build --build-args $(cat envfile)",
    "Dockerfile build error: Unable to locate": "The pip install lines are new commands to use RUN keyword, not part of apt-get, so you need to remove the previous backlash and add RUN before the lines. Try this:\nFROM radare/radare2\n\nUSER root\n\nRUN apt-get update && \\\n apt-get install -y \\\n build-essential \\\n nasm \\ \n gdb \\ \n python \\\n python-pip \\\n python-dev \\\n vim \\\n git \\\n libffi-dev \\\n libssl-dev \\\n libc6-i386 \\\n libc6-dev-i386 \\\n lsb-core\n\nRUN pip install --upgrade pip\nRUN pip install --upgrade pwntools\n\nUSER r2\n\nRUN git clone https://github.com/longld/peda.git ~/peda && \\\n echo \"source ~/peda/peda.py\" >> ~/.gdbinit\n\nRUN \"/bin/bash\"",
    "Unable to gracefully shutdown a docker process": "go run is getting the signal, not the program that is being run.\nCompile the program ahead of time and run the binary directly in docker, and it will work as expected.",
    "Why docker build flag --add-host is not working on linux ubuntu ?": "When you pass --add-host to docker build, the host is only added during the build. If you pass --add-host to docker run, the host will be present at runtime.\nReference: https://github.com/moby/moby/pull/30383#issuecomment-314797629",
    "I run the docker images which start tomcat8 server but it don't start": "A similar docker official tomcat image (8.0.40) runs:\nCMD [\"catalina.sh\", \"run\"]\nWith catalina.sh made to start tomcat in the foreground: the process won't exit immediately.\nIf you tomcat installation does include that script, you should use it instead or startup.sh.\nOr run directly a tomcat image for testing:\n$ docker run -it --rm -p 8080:8080 tomcat:8.0\nYou can test it by visiting http://container-ip:8080 in a browser",
    "The command '/bin/sh -c apk add .... returned a non-zero code: 6": "In the fetch process, dns name server is not able to resolve the address. The reason might be there is a firewall\nUbuntu has google dns servers in /etc/default/docker file\nDOCKER_OPTS=\"--dns 8.8.8.8 --dns 8.8.4.4\"\nThere's more details to resolve the dns problem here Docker - Network calls fail during image build on corporate network",
    "pg_dump issue in docker": "Using run command is not the best way since you need postgres server up and running. Try with exec, have in mind that using -f as argument in pg_dump will create a file inside the running instance so you might want to write file using > to redirect output and create the dump outside the container. In my case I do something like:\ndocker-compose -f $PROJECT_PATH/docker-compose-$ENV.yml exec postgres pg_dump -U $DBUSER -Z 9 $DOCKERSERVICE > backups/db/postgres-`date +\"%m%d%y%H%M%S\".sql.gz`\nWhere $DOCKERSERVICE is usually postgres\nFor you case it might be something like:\ndocker-compose exec web rake db:migrate\nAnd a better reference for docker-compose exec",
    "Executable file not found in $PATH": "I believe you're running into trouble because the default shell run by Docker is not a login shell according to this answer, which means scripts in /etc/profile.d/ don't get processed.\nIf you need profile processing, try changing your last line to CMD [\"/bin/sh\", \"-l\", \"-c\", \"php71-php-fpm\"] to invoke a login shell.",
    "Extend docker postgres image to create extra database": "It was a problem of docker volume.\nI have to docker volume rm postgresvolume and do again a docker-compose up --build\nIt wasn't a problem of environment variables, but juste the fact that the init.sql was in cash and not run at the begining. It's due to some test I make a the start of the project",
    "Dockerfile: Permission denied during build when running ssh-agent on /tmp": "I did it. What I did is, I got rid of ssh-agent. I simply copied the ~/.ssh- directory of my docker-host into the /root/.ssh of the image and it worked.\nDo not use the ~ though, copy the ~/.ssh-directory inside the projectfolder first and then with the dockerfile inside the container.\nFinal dockerfile looked as follows:\nFROM node:4.2.4\nMAINTAINER me\n\nCMD[\"/bin/bash\"]\n\nENV GIT_SSL_NO_VERIFY=1\nENV https_proxy=\"httpsproxy\"\nENV http_proxy=\"httpproxy\"\nENV no_proxy=\"exceptions\"\n\nADD projectfolder/.ssh /root/.ssh\n\nWORKDIR /usr/src/app\n\nRUN git clone git@gitlab.private.address:something/target.git\n\nRUN rm -r /root/.ssh\n\nWORKDIR /urs/src/app/target\n\nRUN npm set registry http://local-npm-registry\nRUN npm install\n\nEXPOSE 3001\nThe dockerfile still has to be improved on efficiency and stuff, but it works! Eureka!\nThe image now has to be squashed and it should be safe to use, though we only use it in our local registry.",
    "Generate all locales in a docker image": "In /etc/locale.gen I've found the hint, that /usr/share/i18n/SUPPORTED lists all supported locale codes. As of https://people.debian.org/~schultmc/locales.html and https://wiki.debian.org/Locale it should be enough to add all wanted codes to /etc/locale.gen and run locale-gen. So this is my solution:\nRUN cp /usr/share/i18n/SUPPORTED /etc/locale.gen\nRUN locale-gen",
    "How can I access internet in docker build?": "Set DEFAULT_FORWARD_POLICY=\"ACCEPT\" @ /etc/default/ufw\nAdd the following lines JUST BEFORE \u201c*filter\u201d @ /etc/ufw/before.rules\n*nat\n:POSTROUTING ACCEPT [0:0]\n-A POSTROUTING ! -o docker0 -s 172.17.0.0/16 -j MASQUERADE\nCOMMIT\nI, as many people, had to add --iptables=trueto make sure my UFW rules were respected by Docker. But all the articles seem leave out out an important thing that is... the time you reboot your machine, a series of configurations done when you enabled that setting on-the-fly and rebooted the Docker Engine will be lost. The problem? Your containers will no longer connect to the internetz.\nEventually I found out an article with a \"full solution\". You can find it here and it basically tells you what I wrote at the beginning of the answer.\nHope it helps",
    "Dockerfile custom commands/directives": "Assuming you are referring to download some files using http (HTTP GET) as one of the example in the question. You can try this.\nRUN wget https://wordpress.org/plugins/about/readme.txt\nor\nRUN curl https://wordpress.org/plugins/about/readme.txt\nThe example Dockerfile with download shell script\nPROJ-DIR\n    - files\n        - test.sh\n    - Dockerfile\nfiles/test.sh\n#!/bin/sh\n\nwget https://wordpress.org/plugins/about/readme.txt\nDockerfile\nFROM centos:latest\n\nCOPY files/test.sh /opt/\nRUN chmod u+x /opt/test.sh\n\nRUN rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY*\nRUN yes | yum install wget\n\nRUN /opt/test.sh\nRUN rm /opt/test.sh\nBuild the image\ndocker build -t test_img .",
    "Spring Cloud Config Server cannot connect to git over ssh in Docker": "The problem come from the ssh config file replace the RUN command with this one\nRUN bash -c 'echo -e \"Host [MASKED DOMAIN]\\n\\tStrictHostKeyChecking no\\n\" >> /root/.ssh/config >> /root/.ssh/config'",
    "Could not install mysql-server inside docker container": "What's probably happening is the apt-get install is asking for input, and failing. What happens if you just run those commands interactively in the base container? e.g. launch a shell with docker run -ti --rm ubuntu:latest /bin/bash\nOne thing you can try is to use the DEBIAN_FRONTEND=noninteractive environment variable, e.g.\nRUN apt-get update && \\\n  DEBIAN_FRONTEND=noninteractive apt-get -yq install mysql-server",
    "Docker + rails 4 :, weird error during : RUN bundle exec rake assets:precompile": "Adding RUN yarn install prior to precompiling assets did the trick for me here on a Rails 6 app. Makes sense, node_modules didn't exist.",
    "How to run mongorestore after mongod in docker": "Start your mongod container\ndocker run -d --name mymongod ... mongo ...\nStart a 2nd container for mongorestore, linking it to the first:\ndocker run --link mymongod:db ... mongo mongorestore -h db ...\nmongorestore will connect to the mymongod container via the alias db that docker creates based on the specified --link",
    "Issue with docker compose": "So I figured I would try updating this evening to the latest docker-composer (1.2.0) and everything just kind of started working. I am still not sure what the problem was. However if someone lands on this page and is still running 1.1.0, I would suggest updating.",
    "docker run with ENTRYPOINT containing a variable": "ENTRYPOINT doesn't do ENV replacement, but WORKDIR does:\nWORKDIR ${INSTALL_DIR}\nENTRYPOINT ./hello_world\nBut I wonder why you want to have the INSTALL_DIR as a variable? If you weren't using containers, you'd do this so that /opt/foo/bar and /opt/foo/baz and /opt/qux/bar can all coexist. But if each of these is in a separate container, then they can all live at /opt/app.",
    "Issue Adding User in Docker with Eclipse Temurin 17-jre Image": "Since yesterday, the 17-jre, 21-jre, 17-jdk, 21-jdk... images point to the noble variant instead of the jammy variant. It seems the adduser and the addgroup commands don't exist in Ubuntu noble (replaced by useradd and groupadd). If you don't want to change your scripts in your Dockerfile, you can change your FROM to use the more specific 17-jre-jammy instead of 17-jre.",
    "Running Ollama in Docker File for Python Langchain Application": "Create a start_service.sh file:\n#!/bin/sh\n\n# Start Ollama in the background\nollama serve &\n\n# Wait for Ollama to start\nsleep 5\n\n# Pull and run <YOUR_MODEL_NAME>\nollama run <YOUR_MODEL_NAME>\nIn your Dockerfile:\nEXPOSE 11434\nRUN chmod +x /app/start_services.sh\n# Run .sh file\nCMD [\"/app/start_services.sh\"]\nHope this may help you. This is how it worked for me,",
    "How to Use Pulled Image as a Cache with `docker buildx`": "Use the --cache-from option when building with docker buildx\ndocker pull \"ghcr.io/foo/bar/baz\"\ndocker buildx build . --tag \"ghcr.io/foo/bar/baz\" --cache-from=\"ghcr.io/foo/bar/baz\"\ndocker push \"ghcr.io/foo/bar/baz\"\nBy specifying this option, Docker will re-use layers from the pulled image where the initial steps have not changed in the Dockerfile and the layers are the same.",
    "How to deploy to docker a flutter mobile app?": "Just as an example I'm providing my config file below, here is just a base setup, for more information you can read this article: https://blog.codemagic.io/how-to-dockerize-flutter-apps. Please feel free to ask additional questions.\nFROM cirrusci/flutter\n\nRUN sdkmanager --install \"platform-tools\" \"platforms;android-33\"\n\nCOPY ./android ./app/android\nCOPY ./assets ./app/assets\nCOPY ./ios ./app/ios\nCOPY ./lib ./app/lib\nCOPY ./analysis_options.yaml ./app\nCOPY ./pubspec.yaml ./app\n\nRUN cd /app && flutter pub get\n\nWORKDIR /app",
    "Docker denied for /bin/sh": "Yeah, your problem is that SELinux is blocking the access. (Which is good! resist the urge to disable SELinux.)\ndocker-compose can relabel the files for you.\nFor files being denied, when you define a volume just add the z or Z flag as appropriate.\nz is for files to be shared between containers, Z is for files to be used by only one container.\nhttps://docs.docker.com/storage/bind-mounts/#configure-the-selinux-label\nDo note this important warning:\n\"This affects the file or directory on the host machine itself and can have consequences outside of the scope of Docker.\"\nIt looks like all of your files are in a home folder and labeled correctly that way. You may just have to use restorecon on them if anything breaks, but be aware.\nSo where you have:\n    volumes:\n      - ./volumes/postgres:/var/lib/postgresql/data\ntry:\n    volumes:\n      - ./volumes/postgres:/var/lib/postgresql/data:Z",
    "How to install local pip package from mounted volume in docker-compose": "For simpler packages, it may be enough to just mount it directly into the site-packages.\nIn your example, you might want to try:\nputting relaton-py (just the package in its source form, e.g. the Git repository) one directory above your Compose file, and\nadding this line to docker-compose.yml under volumes:\n- ../relaton-py/relaton:/usr/local/lib/python3.10/site-packages/relaton:ro\nAssuming you use Python 3.10, this should override whatever gets installed from PyPI during Docker image build with the directory on your host system, mounted in read-only mode to prevent accidental changes from within the container (but any source code changes you make on your main system, of course, will be propagated immediately).\nIf your package includes native modules and requires compilation for particular OS/architecture, there may be extra steps involved in this.",
    "dockerfile --platform option of the FROM instruction not work": "The --platform parameter was introduced in buildkit, and I tend to recommend that for most builds now:\n$ DOCKER_BUILDKIT=1 docker build -t test-platform -f df.platform --progress plain --no-cache .\n#1 [internal] load build definition from df.platform\n#1 sha256:5c840b4d7475cccb1fc86fce5ee78796e600289df0bb6de6c73430d268e9389d\n#1 transferring dockerfile: 38B done\n#1 DONE 0.0s\n\n#2 [internal] load .dockerignore\n#2 sha256:1140f41a9b3ce804e3b52ff100b4cad659a81a19c059e58d6dc857c0e367c821\n#2 transferring context: 34B done\n#2 DONE 0.0s\n\n#3 [internal] load metadata for docker.io/library/golang:1.16-alpine\n#3 sha256:066c23f588b92c8811e28ac05785cd295f354b1e7f60b3e42c4008ec173536c2\n#3 DONE 0.2s\n\n#4 [1/2] FROM docker.io/library/golang:1.16-alpine@sha256:5616dca835fa90ef13a843824ba58394dad356b7d56198fb7c93cbe76d7d67fe\n#4 sha256:d20c37de2e493c7729ae105da84b8907178eed8cc5d1a935db9a50e2370830c2\n#4 CACHED\n\n#5 [2/2] RUN go version\n#5 sha256:158e1ccd4f04dd9d9e1d7cb1008671d8b25cf42ff017d0f2fce6cc08899a77f4\n#5 0.529 go version go1.16.15 linux/arm64\n#5 DONE 0.5s\n\n#6 exporting to image\n#6 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n#6 exporting layers 0.0s done\n#6 writing image sha256:3901f37e2cfca681676cd6c6043d3b88594664c44b1f4e873c183e0a200852d5 done\n#6 naming to docker.io/library/test-platform done\n#6 DONE 0.0s\nWith the classic builder, it will default to the already existing image on the host, and only pull a new one when the image doesn't exist or when you specify --pull:\n$ DOCKER_BUILDKIT=0 docker build -t test-platform -f df.platform .\nSending build context to Docker daemon  23.04kB\nStep 1/2 : FROM --platform=linux/arm64 golang:1.16-alpine\n ---> df1795ddbf41\nStep 2/2 : RUN go version\n ---> Running in f53586180318\ngo version go1.16.8 linux/amd64\nRemoving intermediate container f53586180318\n ---> a250bd04bb4b\nSuccessfully built a250bd04bb4b\nSuccessfully tagged test-platform:latest\n\n$ DOCKER_BUILDKIT=0 docker build -t test-platform --pull -f df.platform .\nSending build context to Docker daemon  23.04kB\nStep 1/2 : FROM --platform=linux/arm64 golang:1.16-alpine\n1.16-alpine: Pulling from library/golang\n9b3977197b4f: Already exists \n1a89e8eeedd5: Already exists \n94645a83ff95: Already exists \n7ed97893b138: Already exists \n57a2943bcc95: Already exists \nDigest: sha256:5616dca835fa90ef13a843824ba58394dad356b7d56198fb7c93cbe76d7d67fe\nStatus: Downloaded newer image for golang:1.16-alpine\n ---> 4a5e4084930e\nStep 2/2 : RUN go version\n ---> [Warning] The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64) and no specific platform was requested\n ---> Running in 5a0893533b89\ngo version go1.16.15 linux/arm64\nRemoving intermediate container 5a0893533b89\n ---> 2dd93e25714a\nSuccessfully built 2dd93e25714a\nSuccessfully tagged test-platform:latest",
    "How to create a reusable dockerfile template?": "We can achieve this with the ARG directive (docs.docker.com). Here is an example Dockerfile:\n# syntax=docker/dockerfile:1\nARG TAG\n\nFROM ubuntu:$TAG\nARG FILE_TO_COPY\nARG EXPOSE_PORT\nCOPY $FILE_TO_COPY file.txt\nCMD cat file.txt\n\nEXPOSE $PORT_TO_EXPOSE\nThere is a semantic difference between any ARG before the first FROM and any arg after the first FROM: the former can be used within the FROM directive, e.g. to define the tag or digest. Its value is not available in a build stage. The latter belongs to a build stage, and thus in only accessible within the build stage. This behaviour is specified here (docs.docker.com).\nWe can build the image, specifying the build arguments:\n$ docker build \\\n  --build-arg TAG=18.04 \\\n  --build-arg FILE_TO_COPY=foo.txt \\\n  --build-arg PORT_TO_EXPOSE=8080 \\\n  --tag test \\\n  .\nand then run it:\n$ docker run -it --rm test\nbar",
    "Can't docker build a Golang project with internal packages": "The issue is in your Dockerfile; after the operation COPY ./src/* ./ the directory structure in your image is as follows:\nZZ> docker run -it d1fae37bbfb1 /bin/sh\n# cd /app\n# ls -al\ntotal 2048\ndrwxr-xr-x 1 root root    4096 Feb 16 19:58 .\ndrwxr-xr-x 1 root root    4096 Feb 16 19:59 ..\ndrwxr-xr-x 3 root root    4096 Feb 16 19:52 foo\n-rwxr-xr-x 1 root root      97 Feb 16 19:57 go.mod\n-rwxr-xr-x 1 root root     352 Feb 16 19:52 go.sum\n-rwxr-xr-x 1 root root     295 Feb 16 19:52 main.go\n# ls foo\nbar  foo.go\nSo there is no internal folder which is why the build is failing.\nThe build will complete successfully if you change this line to:\nCOPY src ./",
    "Error: /bin/sh: java: not found with @openapitools": "Yes you need to install java, or use another openapi generator.\nFor a angular project I use ng-openapi-gen. This does not need it, and there are probably others.",
    "Cannot launch playwright chromium": "Adding EXPOSE 80 to my Dockerfile fixed my problem :)",
    "Running local ansible to create local docker Image": "Fixed by doing following changes\npostgres/Dockerfile\nFROM postgres:10.17-buster\nCOPY deploy-scripts/db-schema/global /docker-entrypoint-initdb.d/\nAnsible Playbook\n- hosts: localhost\n  vars:\n    registry_host: \"localhost\"\n    registry_port: 5000\n    i_postgres_image: \"order-postgres\"\n  tasks:\n    - name: \"Pull & Initialize Postgres Image\"\n      docker_image:\n        name: \"{{ i_postgres_image  }}\"\n        build:\n          path: ../../\n          dockerfile: build/postgres/Dockerfile\n        source: build\n        state: present\n\n    - name: \"Tag & Push to registry\"\n      register: \"out\"\n      docker_image:\n        name: \"{{ i_postgres_image }}\"\n        repository: \"{{ registry_host }}:{{ registry_port }}/{{ i_postgres_image }}\"\n        push: yes\n        source: local\n\n    - name: Show test output\n      debug:\n        msg: \"{{ out }}\"",
    "Docker build command for Dockerfile not in current directory": "docker build -t test-app Test/Backend/App%20Server/Dev/Test/Application",
    "ModuleNotFoundError: No module named 'versioneer'": "Command \"/usr/local/bin/python /usr/local/lib/python3.7/site-packages/pip/_vendor/pep517/in_process.py get_requires_for_build_wheel /tmp/tmp8gvrvde\" failed with error code 1 in /tmp/pip-install-s551_g8p/numpy\nThe error happens when build numpy, and this indicates you need to upgrade pip to make it work:\npip install --upgrade pip\nAdditional, you need install compiler for build with next:\napk add build-base\nWith above, a sample workable Dockerfile as next:\nFROM python:alpine3.7\nRUN pip install --upgrade pip; apk add build-base; pip install numpy\nRUN python -c \"import numpy; print(numpy.__version__)\"\nThe output\uff1a\n$ docker build -t abc:1 .\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM python:alpine3.7\n ---> 00be2573e9f7\nStep 2/3 : RUN pip install --upgrade pip; apk add build-base; pip install numpy\n ---> Running in bba4eed0d626\nCollecting pip\n  Downloading https://files.pythonhosted.org/packages/8a/d7/f505e91e2cdea53cfcf51f4ac478a8cd64fb0bc1042629cedde20d9a6a9b/pip-21.2.2-py3-none-any.whl (1.6MB)\nInstalling collected packages: pip\n  Found existing installation: pip 19.0.1\n    Uninstalling pip-19.0.1:\n      Successfully uninstalled pip-19.0.1\nSuccessfully installed pip-21.2.2\n...\nCollecting numpy\n  Downloading numpy-1.21.1.zip (10.3 MB)\n...\nSuccessfully built numpy\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.21.1\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nRemoving intermediate container bba4eed0d626\n ---> 6e11968fe036\nStep 3/3 : RUN python -c \"import numpy; print(numpy.__version__)\"\n ---> Running in 0f4c47db07cd\n1.21.1\nRemoving intermediate container 0f4c47db07cd\n ---> f189962ad246\nSuccessfully built f189962ad246\nSuccessfully tagged abc:1",
    "Disadvantages of docker --squash": "I think they've provided the pros and the cons in its own document clearly.\nPro:\nSquashing layers can be beneficial if your Dockerfile produces multiple layers modifying the same files, for example, files that are created in one step, and removed in another step.\nCon:\nThe --squash option is an experimental feature, and should not be considered stable.\nSquashing images may actually have a negative impact on performance; when pulling an image consisting of multiple layers, layers can be pulled in parallel, and allows sharing layers between images (saving space).\nAlternatively\nFor most use cases, multi-stage builds are a better alternative, as they give more fine-grained control over your build, and can take advantage of future optimizations in the builder. Refer to the use multi-stage builds section in the userguide for more information.\nSee the documentation",
    "Tag all stages of docker multi-stage build": "This is a feature of docker buildx bake which is at last check still considered experimental and I haven't seen a lot of traction for the syntax from the community.\nThere's certainly been some discussion on adding this functionality in the buildkit issues and I'd recommend following those issues and adding your use case if you'd like to see it added.",
    "JIB - Is it possible to see the dockerfile jib creates behind the scenes?": "Answer recommended by Google Cloud Collective",
    "Docker for local development with multiple environment": "TL;DR;\nYou can configure docker-compose using external networks to communicate with services from other projects or (depending on your project) use the -f command-line option / COMPOSE_FILE environment variable to specify the path of the compose file(s) and bring all of the services up inside the same network.\nUsing external networks\nGiven the below tree with project a and b:\n.\n\u251c\u2500\u2500 a\n\u2502   \u2514\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 b\n    \u2514\u2500\u2500 docker-compose.yml\nProject a's docker-compose sets a name for the default network:\nversion: '3.7'\nservices:\n  nginx:\n    image: 'nginx'\n    container_name: 'nginx_a'\n    expose:\n    - '80'\nnetworks:\n  default:\n    name: 'net_a'\nAnd project b is configured to using the named network net_b and the pre-existing net_a external network:\nversion: '3.7'\nservices:\n  nginx:\n    image: 'nginx'\n    container_name: 'nginx_b'\n    expose:\n    - '80'\n    networks:\n    - 'net_a'\n    - 'default'\nnetworks:\n  default:\n    name: 'net_b'\n  net_a:\n    external: true\n... exec'ing into the nginx_b container we can reach the nginx_a container:\nNote: this is a minimalist example. The external network must exist before trying to bring up an environment that is configured with the pre-existing network. Rather than modifying the existing projects docker-compose.yml I'd suggest using overrides.\nThe configuration gives the nginx_b container a foot inside both networks:\nUsing the -f command-line option\nUsing the -f command-line option acts as an override. It will not work with the above compose files as both specify an nginx service (docker-compose will override / merge the first nginx service with the second).\nUsing the modified docker-compose.yml for project a:\nversion: '3.7'\nservices:\n  nginx_a:\n    container_name: 'nginx_a'\n    image: 'nginx'\n    expose:\n    - '80'\n... and for project b:\nversion: '3.7'\nservices:\n  nginx_b:\n    image: 'nginx'\n    container_name: 'nginx_b'\n    expose:\n    - '80'\n... we can bring both of the environments up inside the same network: docker-compose -f a/docker-compose.yml:b/docker-compose.yml up -d:",
    "Best practice for pulling source code and using it in a docker image": "Great question! Even though there are several ways to solve this, there are quite some differences and drawbacks with some of these approaches. Back in the days the pattern was basically to build stuff outside (on the host) and then copy the relevant things into the image if you wanted to avoid having all the SDKs and sources in your production image.\nLuckily there are better ways to solve this today: multistage docker builds.\nA multistage Dockerfile is like a regular Dockerfile but it contains several stages (aka more than one FROM statement). Each stage is a fresh start of an image build. Not all images might end up in your container registry as some of them are just used to trigger intermediate build steps.\nPseudo code\nFROM node:version AS frontend-build\nWORKDIR /src\nCOPY src/frontend . # or better package.json/package-lock.json first, then install, then the rest\nRUN npm ci # or yarn build\n\nFROM jdk-plus-buildtool:version AS backend-build\nWORKDIR /app\nCOPY src/backend .\nRUN mvn package # or similar\n\nFROM trimmed-down-runtime:version\nWORKDIR /app\nCOPY --from=backend-build target/myapp/ .\nCOPY --from=frontend-build dist/ ./static-files-folder\nCMD your-run-command # or entrypoint\nUsing this approach has several advantages:\nYour final image will contain only the minimal dependencies needed to run your application (e.g. JRE, java application, static javascript files)\nNothing is build outside a container which limits the effects of the environment on the build. Every tool required must be available in the build container, which makes the builds pretty reliable and reproducible\nThe build can easily be run on a developer machine producing the same results even though the developer might have different versions of npm/java locally on their machine\nNo build tools, sdks, source files or intermediate artifacts end up in your final image\nEven the backend part itself can become smaller because you no longer ship the SDK (e.g. JDK for a java app) when moving those into a production container\nYou can leverage the docker build cache even more because whole parts can be skipped if nothing changed (e.g. reuse the java build if only javascript files changed)\nYou have more fine-grained control over the dependencies used in each build step and the build itself has less inter-dependencies as the steps for the different technologies are running in different containers.\nIf you are talking about a static javascript application and an HTTP API backend server, you could also use two separate images (frontend and backend) and then set up network and proxying accordingly so that you only expose the frontend container to the world and all requests are routed through the frontend to the backend application.\nOne more comment: You are talking about different repositories for client and server. Usually the CI environment cares about checking out the desired versions of your code before the real build starts. If this server is basically used from this one client only, I would use the bundled approach and also move the client sources into a subfolder of the main server repository. This makes it easier to do bugfixes for the whole system with a single bugfix branch. If you really cannot move source code between repositories, I would go with some git submodule/subtree approach to avoid dealing with commit references on my own during the build.",
    "How to debug docker pull layer by layer?": "It will be difficult to specify why the layers are slow other than just their size. If there are other reasons upstream, it's a lot of moving pieces from Cloudflare to Docker's hosting (probably in something like an AWS S3 bucket) and then there's your local network. There are also rate limits that kick in for very heavy usage (those limits will become more visible in another few months for other users). Assuming this is just a large layer, you can pull the blob directly from the registry to inspect it. Here's a couple scripts to hit the registry API with curl:\n$ cat manifest-v2.sh \n#!/bin/sh\n\nref=\"${1:-library/ubuntu:latest}\"\nrepo=\"${ref%:*}\"\ntag=\"${ref##*:}\"\napi=\"application/vnd.docker.distribution.manifest.v2+json\"\ntoken=$(curl -s \"https://auth.docker.io/token?service=registry.docker.io&scope=repository:${repo}:pull\" \\\n        | jq -r '.token')\ncurl -H \"Accept: ${api}\" \\\n     -H \"Authorization: Bearer $token\" \\\n     -s \"https://registry-1.docker.io/v2/${repo}/manifests/${tag}\" | jq .\n\n\n$ cat get-blob.sh \n#!/bin/sh\n\nref=\"${1:-library/ubuntu:latest}\"\nrepo=\"${ref%:*}\"\ntag=\"${ref##*:}\"\ndigest=\"$2\"\ntoken=$(curl -s \"https://auth.docker.io/token?service=registry.docker.io&scope=repository:${repo}:pull\" \\\n        | jq -r '.token')\ncurl -H \"Authorization: Bearer $token\" \\\n     -s -L -o - \"https://registry-1.docker.io/v2/${repo}/blobs/${digest}\"\nAnd running those, you can then see the digests you were looking at in the pull and download that specific layer, run it through tar, and see what's in the layer:\n$ ./manifest-v2.sh weberstephanhd/iacbox2:v380\n{\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n  \"config\": {\n    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n    \"size\": 36642,\n    \"digest\": \"sha256:4588fe154684dfe09b266e2a122b0789dc7ee89ff284fb140f14962fa2d5c754\"\n  },\n  \"layers\": [\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 26697127,\n      \"digest\": \"sha256:7595c8c21622ea8a8b9778972e26dbbe063f7a1c4b0a28a80a34ebb3d343b586\"\n    },\n... lots of other layers ...\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 19899,\n      \"digest\": \"sha256:5350cb37a04f91a25c1e2f288310f58769cf77761c00e68059931ec3dcc67301\"\n    },\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 1364512,\n      \"digest\": \"sha256:476cd410766a5218a41bec1d3c772653ced6d5ad0cc14e52c303901da0255327\"\n    },\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 22698271,\n      \"digest\": \"sha256:1437ee729887a466d9d51b97321ce22f7f15c9340a622ce876f90cd7c0bc5952\"\n    },\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 17743524,\n      \"digest\": \"sha256:c7ed8287f08ff5f00a8c6a29afb9ceaa0bcf0e2fa0191f7bbb41aa8213ed55f6\"\n    },\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 5661386,\n      \"digest\": \"sha256:2e8965a34b4adf2d08f2cf12715ee40d1872c82ebda0fd1fb3711510b10ad07d\"\n    },\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 129,\n      \"digest\": \"sha256:6e71916e2b6e0112ea835e239c5770c1954f53c33d8dc80e0285af4b562a07e7\"\n    },\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 115,\n      \"digest\": \"sha256:b9677e754d0339eca6ffe9889d7d920f69efeb9987eb75308ee0479d11fa30cc\"\n    }\n  ]\n}\n\n$ ./get-blob.sh weberstephanhd/iacbox2:v380 sha256:5350cb37a04f91a25c1e2f288310f58769cf77761c00e68059931ec3dcc67301 | tar -tzvf -\ndrwxr-xr-x root/root         0 2020-09-04 07:49 etc/\ndrwxr-xr-x root/root         0 2020-09-04 07:49 etc/profile.d/\n-rw-r--r-- root/root      1186 2020-09-04 07:49 etc/profile.d/iacbox-env.sh\ndrwxr-xr-x root/root         0 2020-09-04 07:49 etc/skel/\ndrwxr-xr-x root/root         0 2020-09-04 07:49 etc/skel/.azure/\n-rwxr-xr-x root/root         0 1969-12-31 19:00 etc/skel/.azure/.wh..wh..opq\n-rw-r--r-- root/root        25 2020-09-04 07:49 etc/skel/.azure/telemetry.json\n-rw-r--r-- root/root       167 2020-09-04 07:49 etc/skel/.bash_aliases\n-rw-r--r-- root/root       169 2020-09-04 07:49 etc/skel/.rvmrc\ndrwx------ root/root         0 2020-09-04 07:49 root/\ndrwxr-xr-x root/root         0 2020-09-04 07:49 root/.azure/\n-rwxr-xr-x root/root         0 1969-12-31 19:00 root/.azure/.wh..wh..opq\n-rw-r--r-- root/root        25 2020-09-04 07:49 root/.azure/telemetry.json\n-rw-r--r-- root/root       167 2020-09-04 07:49 root/.bash_aliases\n-rw-r--r-- root/root       169 2020-09-04 07:49 root/.rvmrc\ndrwxr-xr-x root/root         0 2020-09-04 07:49 root/.ssh/\n-rwxr-xr-x root/root         0 1969-12-31 19:00 root/.ssh/.wh..wh..opq\n-rw-r--r-- root/root        51 2020-09-04 07:49 root/.ssh/config\n-rwxr-xr-x root/root      1249 2020-09-04 07:49 root/profile.d_iacbox-env.sh\ndrwxrwxrwt root/root         0 2020-09-04 07:49 tmp/\n-rwxr-xr-x root/root      2288 2020-09-04 07:49 tmp/acceptance-test.sh\n-rw------- root/root         0 2020-09-04 07:49 tmp/.wh.config_files\ndrwxr-xr-x root/root         0 2020-07-13 10:48 usr/\ndrwxr-xr-x root/root         0 2020-09-04 07:47 usr/local/\ndrwxr-xr-x 503/staff         0 2020-09-04 07:49 usr/local/bin/\nlrwxrwxrwx root/root         0 2020-09-04 07:49 usr/local/bin/bosh2 -> /usr/local/bin/bosh\n-rwxr-xr-x root/root     15672 2020-09-04 07:49 usr/local/bin/execute-iac-command.sh\n-rwxr-xr-x root/root     16675 2020-09-04 07:49 usr/local/bin/iac-cmd.sh\n-rwxr-xr-x root/root      1334 2020-09-04 07:49 usr/local/bin/iac-validate.sh\n-rwxr-xr-x root/root      3850 2020-09-04 07:49 usr/local/bin/prepare_git_credentials.sh\n-rwxr-xr-x root/root        47 2020-09-04 07:49 usr/local/bin/print-iaas-cli-environment.sh\n-rwxr-xr-x root/root       111 2020-09-04 07:49 usr/local/bin/sshtunnel-to-landscape.sh\n-rwxr-xr-x root/root     15700 2020-09-04 07:49 usr/local/bin/tunnel-to-landscape.sh\n-rwxr-xr-x root/root       824 2020-09-04 07:49 usr/local/bin/whitelist-check-external-ips.sh\n-rwxr-xr-x root/root       536 2020-09-04 07:49 usr/local/bin/whitelist-create-external-ips.sh\n-rwxr-xr-x root/root       535 2020-09-04 07:49 usr/local/bin/whitelist-import-external-ips.sh\nAs for why the digests are different, that comes down to compression. The media type for these blobs is a compressed tar on the registry, but once downloaded to the local docker engine, it uncompresses the layer and the local digest is on the uncompressed tar. That also explains why the sizes don't match.\nTo align the layers with the history, you can compare the history with the manifest, just be careful of the order of the lines (docker history shows lines in reverse as does the v1 registry API, while the v2 API calls will show the lines from oldest to newest) and not every line in the history results in a new blob. That becomes a bit more apparent if you look at the config json associated with an image:\n$ more get-config-v2.sh\n#!/bin/sh\n\nref=\"${1:-library/ubuntu:latest}\"\nrepo=\"${ref%:*}\"\ntag=\"${ref##*:}\"\ntoken=$(curl -s \"https://auth.docker.io/token?service=registry.docker.io&scope=repository:${repo}:pull\" \\\n        | jq -r '.token')\ndigest=$(curl -H \"Accept: application/vnd.docker.distribution.manifest.v2+json\" \\\n              -H \"Authorization: Bearer $token\" \\\n              -s \"https://registry-1.docker.io/v2/${repo}/manifests/${tag}\" \\\n         | jq -r .config.digest)\ncurl -H \"Accept: application/vnd.docker.container.image.v1+json\" \\\n     -H \"Authorization: Bearer $token\" \\\n     -s -L \"https://registry-1.docker.io/v2/${repo}/blobs/${digest}\" | jq .\n\n$ ./get-config-v2.sh weberstephanhd/iacbox2:v380\n{\n  \"architecture\": \"amd64\",\n  \"config\": {\n    \"Hostname\": \"\",\n    \"Domainname\": \"\",\n    \"User\": \"root\",\n    \"AttachStdin\": false,\n    \"AttachStdout\": false,\n    \"AttachStderr\": false,\n    \"Tty\": false,\n    \"OpenStdin\": false,\n    \"StdinOnce\": false,\n    \"Env\": [\n      \"PATH=/opt/ostoolset/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin\",\n      \"CF_HOME=/root\",\n      \"CF_PLUGIN_HOME=/usr/local/lib/cf_plugins\",\n      \"TF_PLUGIN_DIR=/usr/local/lib/terraform_plugins\",\n      \"GOROOT=/usr/local/go\",\n      \"PYTHONWARNINGS=ignore:Certificate for :::\",\n      \"OS_VOLUME_API_VERSION=2\",\n      \"LANG=en_US.UTF-8\",\n      \"LANGUAGE=en_US.UTF-8\",\n      \"LC_ALL=en_US.UTF-8\"\n    ],\n    \"Cmd\": [\n      \"/bin/bash\"\n    ],\n    \"ArgsEscaped\": true,\n    \"Image\": \"sha256:622904aeb073d26c0a17d87bf4df9cb026ffa99f1f5e9f3ff0fb6135e6aeb54e\",\n    \"Volumes\": null,\n    \"WorkingDir\": \"\",\n    \"Entrypoint\": null,\n    \"OnBuild\": null,\n    \"Labels\": null\n  },\n  \"container\": \"af499c54916b436d2fee37da88a6e48ee1e1b218dece9747773396567cfcab23\",\n  \"container_config\": {\n    \"Hostname\": \"af499c54916b\",\n    \"Domainname\": \"\",\n    \"User\": \"root\",\n    \"AttachStdin\": false,\n    \"AttachStdout\": false,\n    \"AttachStderr\": false,\n    \"Tty\": false,\n    \"OpenStdin\": false,\n    \"StdinOnce\": false,\n    \"Env\": [\n      \"PATH=/opt/ostoolset/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin\",\n      \"CF_HOME=/root\",\n      \"CF_PLUGIN_HOME=/usr/local/lib/cf_plugins\",\n      \"TF_PLUGIN_DIR=/usr/local/lib/terraform_plugins\",\n      \"GOROOT=/usr/local/go\",\n      \"PYTHONWARNINGS=ignore:Certificate for :::\",\n      \"OS_VOLUME_API_VERSION=2\",\n      \"LANG=en_US.UTF-8\",\n      \"LANGUAGE=en_US.UTF-8\",\n      \"LC_ALL=en_US.UTF-8\"\n    ],\n    \"Cmd\": [\n      \"/bin/sh\",\n      \"-c\",\n      \"#(nop) \",\n      \"USER root\"\n    ],\n    \"ArgsEscaped\": true,\n    \"Image\": \"sha256:622904aeb073d26c0a17d87bf4df9cb026ffa99f1f5e9f3ff0fb6135e6aeb54e\",\n    \"Volumes\": null,\n    \"WorkingDir\": \"\",\n    \"Entrypoint\": null,\n    \"OnBuild\": null,\n    \"Labels\": {}\n  },\n  \"created\": \"2020-09-04T11:49:53.021161021Z\",\n  \"docker_version\": \"19.03.6\",\n  \"history\": [\n    {\n      \"created\": \"2020-07-24T14:38:19.482143079Z\",\n      \"created_by\": \"/bin/sh -c #(nop) ADD file:7d9bbf45a5b2510d44d3206a028cf6502757884d49e46d3d2e6356c3a92c4309 in / \"\n    },\n    {\n      \"created\": \"2020-07-24T14:38:20.335965442Z\",\n      \"created_by\": \"/bin/sh -c [ -z \\\"$(apt-get indextargets)\\\" ]\"\n    },\n    {\n      \"created\": \"2020-07-24T14:38:21.071294363Z\",\n      \"created_by\": \"/bin/sh -c set -xe \\t\\t&& echo '#!/bin/sh' > /usr/sbin/policy-rc.d \\t&& echo 'exit 101' >> /usr/sbin/policy-rc.d \\t&& chmod +x /usr/sbin/policy-rc.d \\t\\t&& dpkg-divert --local --rename --add /sbin/initctl \\t&& cp -a /usr/sbin/policy-rc.d /sbin/initctl \\t&& sed -i 's/^exit.*/exit 0/' /sbin/initctl \\t\\t&& echo 'force-unsafe-io' > /etc/dpkg/dpkg.cfg.d/docker-apt-speedup \\t\\t&& echo 'DPkg::Post-Invoke { \\\"rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true\\\"; };' > /etc/apt/apt.conf.d/docker-clean \\t&& echo 'APT::Update::Post-Invoke { \\\"rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true\\\"; };' >> /etc/apt/apt.conf.d/docker-clean \\t&& echo 'Dir::Cache::pkgcache \\\"\\\"; Dir::Cache::srcpkgcache \\\"\\\";' >> /etc/apt/apt.conf.d/docker-clean \\t\\t&& echo 'Acquire::Languages \\\"none\\\";' > /etc/apt/apt.conf.d/docker-no-languages \\t\\t&& echo 'Acquire::GzipIndexes \\\"true\\\"; Acquire::CompressionTypes::Order:: \\\"gz\\\";' > /etc/apt/apt.conf.d/docker-gzip-indexes \\t\\t&& echo 'Apt::AutoRemove::SuggestsImportant \\\"false\\\";' > /etc/apt/apt.conf.d/docker-autoremove-suggests\"\n    },\n    {\n      \"created\": \"2020-07-24T14:38:21.85928744Z\",\n      \"created_by\": \"/bin/sh -c mkdir -p /run/systemd && echo 'docker' > /run/systemd/container\"\n    },\n    {\n      \"created\": \"2020-07-24T14:38:22.027273323Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  CMD [\\\"/bin/bash\\\"]\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:28.397707805Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV CF_HOME=/root\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:28.50996911Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV CF_PLUGIN_HOME=/usr/local/lib/cf_plugins\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:28.613680523Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV TF_PLUGIN_DIR=/usr/local/lib/terraform_plugins\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:28.716721682Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV GOROOT=/usr/local/go\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:28.819392564Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV PATH=/opt/ostoolset/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:28.92341478Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV PYTHONWARNINGS=ignore:Certificate for :::\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:29.024894545Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV OS_VOLUME_API_VERSION=2\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:29.618098106Z\",\n      \"created_by\": \"/bin/sh -c rm /bin/sh && ln -sf /bin/bash /bin/sh\"\n    },\n    {\n      \"created\": \"2020-08-27T04:23:40.330900434Z\",\n      \"created_by\": \"/bin/sh -c apt-get update &&     DEBIAN_FRONTEND=noninteractive     apt-get install -y     apt-utils     apt-transport-https     gnupg2     curl     wget     locales &&     rm -rf /var/lib/apt/lists/*\"\n    },\n    {\n      \"created\": \"2020-08-27T04:23:41.633234144Z\",\n      \"created_by\": \"/bin/sh -c locale-gen \\\"en_US.UTF-8\\\"\"\n    },\n    {\n      \"created\": \"2020-08-27T04:23:41.742325545Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV LANG=en_US.UTF-8\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:41.845327099Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV LANGUAGE=en_US.UTF-8\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:41.954629412Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ENV LC_ALL=en_US.UTF-8\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-08-27T04:23:42.08703115Z\",\n      \"created_by\": \"/bin/sh -c #(nop) COPY file:298c66e94d89d7e189c54b0e79c24ac300dd4f3524469d0d8adcf16d5f3f5776 in /etc/apt/sources.list.d/extended.list \"\n    },\n    {\n      \"created\": \"2020-08-27T04:23:45.007887008Z\",\n      \"created_by\": \"/bin/sh -c curl -sSL https://deb.nodesource.com/gpgkey/nodesource.gpg.key | apt-key add - &&     curl -sSL https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - &&     curl -L https://packages.microsoft.com/keys/microsoft.asc | apt-key add - &&     curl -sSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     curl -sSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &&     mkdir -p \\\"/usr/local/share/ca-certificates/sap.com\\\" \\\"/usr/local/share/ca-certificates/verizon\\\" &&     curl -sSL http://aia.pki.co.sap.com/aia/SAPNetCA_G2.crt -o \\\"/usr/local/share/ca-certificates/sap.com/SAPNetCA_G2.crt\\\" &&     curl -sSL http://aia.pki.co.sap.com/aia/SAP%20Global%20Root%20CA.crt -o \\\"/usr/local/share/ca-certificates/sap.com/SAP_Global_Root_CA.crt\\\" &&     curl -sSL https://de.ssl-tools.net/certificates/f326e9f894088fb560a001aa2c0ea8b1c20e6c35.pem -o \\\"/usr/local/share/ca-certificates/verizon/Verizon_Public_SureServer_CA_G14-SHA2.crt\\\" &&     update-ca-certificates\"\n    },\n    {\n      \"created\": \"2020-08-27T04:23:45.871796489Z\",\n      \"created_by\": \"/bin/sh -c wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add -\"\n    },\n    {\n      \"created\": \"2020-08-27T04:23:46.464366636Z\",\n      \"created_by\": \"/bin/sh -c echo \\\"deb http://apt.postgresql.org/pub/repos/apt/ bionic\\\"-pgdg main | tee  /etc/apt/sources.list.d/pgdg.list\"\n    },\n... bunch of history trimmed ...\n    {\n      \"created\": \"2020-09-04T11:49:51.582086565Z\",\n      \"created_by\": \"|17 TF_BIN_RELEASE_URL_PREFIX=https://releases.hashicorp.com/terraform TF_PROVIDER_VERSION_ALI=1.70.0 TF_PROVIDER_VERSION_ARCHIVE=1.2.2 TF_PROVIDER_VERSION_AWS=2.70.0 TF_PROVIDER_VERSION_AZURE=2.17.0 TF_PROVIDER_VERSION_AZUREAD=0.10.0 TF_PROVIDER_VERSION_AZURE_LEGACY=1.44.0 TF_PROVIDER_VERSION_DNS=2.1.1 TF_PROVIDER_VERSION_GOOGLE=3.15.0 TF_PROVIDER_VERSION_LOCAL=1.4.0 TF_PROVIDER_VERSION_NULL=2.1.2 TF_PROVIDER_VERSION_OPEN_STACK=1.20.0 TF_PROVIDER_VERSION_POSTGRESQL=1.1.0 TF_PROVIDER_VERSION_RANDOM=2.3.0 TF_PROVIDER_VERSION_TEMPLATE=2.1.2 TF_VERSION=0.12.28 VAULT_VERSION=1.4.2 /bin/sh -c cp /root/go/bin/ossutil /usr/local/bin\"\n    },\n    {\n      \"created\": \"2020-09-04T11:49:51.726671638Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  ARG VERSION=latest\",\n      \"empty_layer\": true\n    },\n    {\n      \"created\": \"2020-09-04T11:49:52.311382499Z\",\n      \"created_by\": \"|18 TF_BIN_RELEASE_URL_PREFIX=https://releases.hashicorp.com/terraform TF_PROVIDER_VERSION_ALI=1.70.0 TF_PROVIDER_VERSION_ARCHIVE=1.2.2 TF_PROVIDER_VERSION_AWS=2.70.0 TF_PROVIDER_VERSION_AZURE=2.17.0 TF_PROVIDER_VERSION_AZUREAD=0.10.0 TF_PROVIDER_VERSION_AZURE_LEGACY=1.44.0 TF_PROVIDER_VERSION_DNS=2.1.1 TF_PROVIDER_VERSION_GOOGLE=3.15.0 TF_PROVIDER_VERSION_LOCAL=1.4.0 TF_PROVIDER_VERSION_NULL=2.1.2 TF_PROVIDER_VERSION_OPEN_STACK=1.20.0 TF_PROVIDER_VERSION_POSTGRESQL=1.1.0 TF_PROVIDER_VERSION_RANDOM=2.3.0 TF_PROVIDER_VERSION_TEMPLATE=2.1.2 TF_VERSION=0.12.28 VAULT_VERSION=1.4.2 VERSION=v380 /bin/sh -c echo ${VERSION} > /docker_image_version\"\n    },\n    {\n      \"created\": \"2020-09-04T11:49:52.91091882Z\",\n      \"created_by\": \"|18 TF_BIN_RELEASE_URL_PREFIX=https://releases.hashicorp.com/terraform TF_PROVIDER_VERSION_ALI=1.70.0 TF_PROVIDER_VERSION_ARCHIVE=1.2.2 TF_PROVIDER_VERSION_AWS=2.70.0 TF_PROVIDER_VERSION_AZURE=2.17.0 TF_PROVIDER_VERSION_AZUREAD=0.10.0 TF_PROVIDER_VERSION_AZURE_LEGACY=1.44.0 TF_PROVIDER_VERSION_DNS=2.1.1 TF_PROVIDER_VERSION_GOOGLE=3.15.0 TF_PROVIDER_VERSION_LOCAL=1.4.0 TF_PROVIDER_VERSION_NULL=2.1.2 TF_PROVIDER_VERSION_OPEN_STACK=1.20.0 TF_PROVIDER_VERSION_POSTGRESQL=1.1.0 TF_PROVIDER_VERSION_RANDOM=2.3.0 TF_PROVIDER_VERSION_TEMPLATE=2.1.2 TF_VERSION=0.12.28 VAULT_VERSION=1.4.2 VERSION=v380 /bin/sh -c echo migrated > /migrated\"\n    },\n    {\n      \"created\": \"2020-09-04T11:49:53.021161021Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  USER root\",\n      \"empty_layer\": true\n    }\n  ],\n  \"os\": \"linux\",\n  \"rootfs\": {\n    \"type\": \"layers\",\n    \"diff_ids\": [\n      \"sha256:7ef3687765828a9cb2645925f27febbac21a5adece69e8437c26184a897b6ec7\",\n      \"sha256:83f4287e1f0496940f8c222ca09cbaf2c7f564a10c57b4609800babe8d1b5b32\",\n      \"sha256:d3a6da143c913c5e605737a9d974638f75451b5c593e58eb7e132fcf0e23c6db\",\n      \"sha256:8682f9a74649fb9fc5d14f827a35259aae8b58c57df8d369f6aa2e92865930c2\",\n      \"sha256:6fd2b828f15530b3e91fdc47694d1165a90a2e33edce9c30961a1fb0c92f2d51\",\n      \"sha256:c41b334c1d5f7c94942e7b6372d5c18f43e8840eb12d7683f6b45856d2776d1a\",\n      \"sha256:a180f973b01e39cd5eb3fc04222d06be573eb308e5a096157c3a9c5c523e3e0c\",\n      \"sha256:424841a53760a9261eb7b4d3dd7c70b62bc74525a13412d0dbbca8deea4255d3\",\n      \"sha256:198605a838ddf053e6120869caebef0272b725c354a42a1d20beeaf8e02674c0\",\n      \"sha256:487242c09dbb7034f6f38585f74f28255d0a574c3db706c830e8408d2436b2fe\",\n      \"sha256:68b89d709fcb495c984b811f195c86dc36831f2cd4f95f38a26fed3baa4e2c5e\",\n      \"sha256:877656568034f2328abd9d73036463d160b0da912ace2535e5c13a75c893413f\",\n      \"sha256:70f7e4f7c12ea90fda9f0442bbb4e6ea7bae448b56d8b6c83c365adb09254367\",\n      \"sha256:0576dc0190175293ca7cbba2a6ea93957db46458246da26f1434b432d66173db\",\n      \"sha256:218cf40a20bbf60704152566772bc0bf803374c594e47261f05ab2b207f497fb\",\n      \"sha256:f5845a9ea4efd834b747a109209d46b6dcb5e05e88a27df67de4aba03dd92f18\",\n      \"sha256:000cddfa5fea51b517db796168add34b9648955dfa6421c8c158709d9687cb32\",\n      \"sha256:4bd3750abf95ceb7d4e6c8f47acdc1c9db754325e050776186ae0a681e032fcd\",\n      \"sha256:810c4b2cb213baf64895dec8566410e7b401436bf524c33d55893a45748c1bba\",\n      \"sha256:5dcb620bac21a7449919d730e5e7ed23f9a0d791f72d3709b5cf3e81853adcd1\",\n      \"sha256:09af1a244f314fe3558afa8d42e66ac376a18bf010544ba4872f3fe7461d1382\",\n      \"sha256:c16ddbd687d181a095ca3ec8b108cf0d27a7a5408cc6e5073c03c903fbd344d3\",\n      \"sha256:8b77549540c08a2571a7d7e8d11cc59519f36b613c0e1ef3c5d713d39f4e3507\",\n      \"sha256:01079aef52a4ee46973ddd207c40f7b1ff4688216a5a05bfb37feea2041fc214\",\n      \"sha256:2e43dfce6411894ef682d3ba313021264700033b0789f826b1304c8874695bc9\",\n      \"sha256:616a5dfd09e5305cc271f77727a84d0c28628c758a994df65df75c33444ea42a\",\n      \"sha256:8c9c8ab8442e9a7e3094090ac8d6659acb050d477d306841cea37eef4a379eed\",\n      \"sha256:047be9dbcd6162c17cdfaee5380611072e27fb7b3de1c462d01d63f436b04f44\",\n      \"sha256:8d25d3692cf4d95dca0327da7d8632c27cbdee71110a9c78628eba94fdc04e6d\",\n      \"sha256:b21e2e4a55c8861e36503e6bb7385968e7db49247e07772927c3baf5c06336de\",\n      \"sha256:ab0696412395709d6a71e21bba15cc474a52a34798149cee761d3accbb896296\",\n      \"sha256:c81651668c205be10467a540129bd91eca20711f22b929a94839f780b1ff0d3f\",\n      \"sha256:57cdc9e21eff8e78820f5222b3e21a06808564aa19884fa3003b12f58b325c7f\",\n      \"sha256:18d72968405bb4ea92c826a718b080cc8b2f0d310f9c791af0bcf6cfd4e9dfc7\",\n      \"sha256:1efa41d22392b374475e2472d0f6c710498052330f5c218296fbf327303d4890\",\n      \"sha256:dd2dd93244de62000c96955f424a55699c6458d64932e4b170e0c42d095b3bc0\",\n      \"sha256:432d521b81b9bcaef07786922cc16581cc6f008ea2bea79fd39d01a496814599\"\n    ]\n  }\n}\nIn there, you'll see \"empty_layer\": true which indicates that step of the Dockerfile only produced a change to the config.json and didn't create a new filesystem layer. So if you're careful you can count the layers in the history and in the manifest to identify which history command resulted in which layer/blob.\nP.S. looking at the command history in those layers, I expect some of the large layers are because of terraform, the binaries and plugins are not small (I've fought with this in the past trying to make a quick provisioner container only to find my image is often larger than what I wanted to provision).",
    "ORA-00604: error occurred at recursive SQL level 1 ORA-01882: timezone region not found": "We had the same issue, when we tried to deploy our application developed in Windows to a Linux/NGNIX machine. While the code worked fine in Windows machine, it threw the error \"ORA-01882: timezone region not found\". After trying several solutions, we found the following solution. In the service file for your .Net application located at /etc/systemd/system/ folder, set the Environment variable for the timezone expected by Oracle. Sample service file is posted below\n[Unit]\nDescription=******\n\n[Service]\nWorkingDirectory=/home/****/*****\nExecStart=/home/*****/*****/*****\nRestart=always\nRestartSec=10 # Restart service after 10 seconds if dotnet service crashes\nSyslogIdentifier=offershare-web-app\nEnvironment=ASPNETCORE_ENVIRONMENT=Development\nEnvironment=TZ=Asia/Calcutta\n\n[Install]\nWantedBy=multi-user.target\nPS: You can get the Oracle timezone by running the following query\nSELECT SESSIONTIMEZONE FROM DUAL",
    "In a dockerfile, how to delete a directory and its contents from docker image? (windows)": "Solution:\nRUN Remove-Item -Path C:\\app\\keys -Force -Recurse\nThere are 2 aspects to resolve this.\nBased on your base image, what is your default shell? Bash or powershell? In my case it was powershell, so there is no need to mention powershell.exe at the beginning of the command.\nThe original command was incorrect RUN Remove-Item -Path C:\\app\\keys -Force because the folder had sub-folders and files. So you have to mention -Recurse",
    "Dockerfile build from source in one stage and then copy and install in second stage": "If you want to build your lib in the first stage and use it in the later stage without all the libs and tools needed to compile it, you can use a multistage build as you say.\nBut, when you copy the builded lib, you need to install the shared library that was linked to it (here musl and unixodbc).\nYou can find them by running ldd:\n/build/mariadb-connector-odbc-3.1.4 # ldd /usr/lib/libmaodbc.so\n    /lib/ld-musl-x86_64.so.1 (0x7fde6847b000)\n    libodbcinst.so.2 => /usr/lib/libodbcinst.so.2 (0x7fde683c5000)\n    libc.musl-x86_64.so.1 => /lib/ld-musl-x86_64.so.1 (0x7fde6847b000)\nAs musl should be already present, you only need to install back the unixodbc lib used for building the lib.\nThis is an example of Dockerfile for that:\nFROM alpine AS build\n# Add build dependencies\nRUN apk add --no-cache alpine-sdk cmake unixodbc-dev mariadb-connector-c mariadb-connector-c-dev mariadb-static unixodbc\n# Download the source code from github\nADD https://github.com/MariaDB/mariadb-connector-odbc/archive/3.1.4.tar.gz /build/mariadb-connector-odbc.tgz\n\n# Build it\nWORKDIR /build\nRUN tar xzf mariadb-connector-odbc.tgz \\\n    && cd mariadb-connector-odbc-3.1.4 \\\n    && CFLAGS=\"$CFLAGS -I/usr/include/mysql\" \\\n       cmake \\\n       -DCMAKE_INSTALL_PREFIX=/usr \\\n       -DCMAKE_INSTALL_LIBDIR=lib \\\n       -DBUILD_SHARED_LIBS=True \\\n       -DCMAKE_BUILD_TYPE=None \\\n       . \\\n    && make install\n\n# Final stage\nFROM alpine\n# Add the dependencies for the lib\nRUN apk add --no-cache unixodbc\n# Copy it from the build image\nCOPY --from=build  /usr/lib/libmaodbc.so  /usr/lib/libmaodbc.so",
    "Docker VS code remove devcontainer": "I think the problem might be that your api container is terminating after running the git config commands. Did you try to add\ncommand: sleep infinity\nto the api service in your docker-compose.yml?",
    "Python Multistage Docker build with setup.py": "You need to change the working directory, as you are copying all your code to the root dir. When you then run pip install . from /, pip copies the entire filesystem to a temporary directory in order to install from there. I wonder what happens if you wait for it to finish?\nAlso, I think that one of the earlier lines might be wrong. I usually do apt update && apt -y upgrade and not the other way around.\nTry this:\nFROM python:3.8-slim AS compile\n\nRUN apt-get update && apt-get -y upgrade\nRUN apt-get install -y --no-install-recommends build-essential gcc\n\nRUN python -m venv /opt/venv\nENV PATH='/opt/venv/bin:$PATH'\n\nWORKDIR /usr/src/app\nCOPY requirements.txt .\n\nRUN pip install --upgrade pip \\\n    && pip install -r requirements.txt \\\n    && pip install gunicorn\n\n# To match the directory structure at host:\nADD src/ ./src\nADD setup.py .\nRUN pip install .\nFor more information on how to write dockerfiles for the Python images, see here.",
    "docker-compose shared volumes between service": "the dockerfile is executed first. The compose-file then overrides the shared folder. You can't access volumes in dockerfiles.\nWhen you first built the docker-container, your app.apk gets copied into the docker-image.\nThen compose starts the container and mounts the new (and probably empty) folder to /var/lib/shared.\nThen the old content of /var/lib/shared is \"overshadowed\" by the new content and you can't access the old content anymore.\nsolution: copy the app into the volume after starting the container.\nusing docker exec -it <container_name> bash you can start a shell in the container after it has started. You can find the name with docker ps.",
    "How to add AdoptOpenJDK to Docker image?": "If you build on basic alpine, you can add in the open-jdk and maven. It might look something like this:\nfrom alpine:3.10\n\nrun apk --no-cache add openjdk11 --repository=http://dl-cdn.alpinelinux.org/alpine/edge/community\nrun apk add bash vim curl wget jq docker git tar unzip bash-completion ca-certificates\nrun cd /opt && curl -sSl http://mirror.vorboss.net/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz | tar -xz\n\nenv PATH \"$PATH:/opt/apache-maven-3.6.3/bin\"\nentrypoint [\"/bin/bash\"]\nThis will set up an alpine image with java 11 and the latest maven (as of writing). Typically we'd do this with a single run command to minimise the number of layers in our image, but I'll leave that optimisation to your imagination.\nI've added a line showing how and where to install other tools, like Docker, bash, vim, curl, etc. Some of these are used installing maven (tar unzip), and some are just for decoration. You only really need curl, tar, and unzip, to answer this question, but as you're building a devbox I thought I'd add some more for demonstration.\nWhen you are deciding which base image to use for a project like this, it's often best to start with the base OS, like apline:3, rather than some random image that happens to have one thing you need installed in it (git in your example). Treat it like OO -- you are inheriting the git image, so you're also a git image. In your case, you're not really a git image, you're a swiss-army knife and you've no idea what the underlying os really is with another choice (and if you use :latest as the tag you've no idea what it will become in the future).",
    "Docker build failed for InvokeWebrequest": "Configure the Docker daemon by typing a json Docker daemon configuration file. { .... \"dns\": [\"1.1.1.1\",\"8.8.8.8\", \"8.8.4.4\"], ... }",
    "How to copy multiple files in different source and destination directories using a single COPY layer in Dockerfile": "You're trying to convert this to a many-to-many copy. This isn't supported by the Dockerfile syntax. You need to have a single destination directory on the right side. And if your source is one or more directories, you need to be aware that docker will copy the contents of those directories, not the directory name itself. The result is that you want:\nCOPY json-files/ ./\nAnd then you need to organize your build context (in docker build . the . or current directory is your build context that is sent to the docker server to perform the build) with a directory called json-files (could be any name) that contains only the files in the directory structure you want to copy:\n| json-files/\n|-  aaa/package.json\n|-  bbb/package.json\n\\-  ccc/package.json\nOption 2:\nYou could structure your build as a multi-stage build to get this down to a single layer without modifying your build context itself:\nFROM scratch as json-files\nCOPY ./aaa/package.json /json-files/aaa/package.json\nCOPY ./bbb/package.json /json-files/bbb/package.json\nCOPY ./ccc/package.json /json-files/ccc/package.json\n\nFROM your_base\nCOPY --from=json-files /json-files .\nWORKDIR aaa\nRUN npm install\nCOPY ./aaa ./aaa\nThis second option is the same as the first from the the view of your COPY command, it just has an image as it's context instead of the build context sent over with the build command.\nAll this said, changing from 3 copy commands to 1, for small individual files that do not overwrite each other, is unlikely to have any noticeable impact on your performance and this looks like premature optimization.",
    "Run coverage test in a docker container": "Your permission issue is most likely due to the fact you have a volume (./backend:/backend) and that you are using a user in your container which does not have the right permission to write on this volume (USER user)\nSince you probably cannot change the permissions on the Travis CI directory ./backend, you could try to change the user which is used to write files to that location. This is easy to do with docker-compose:\nbackend:\n container_name: backend_dev_blog\n build: ./backend\n command: \"sh -c /start_dev.sh\"\n user: root\n volumes:\n   - ./backend:/backend\n ports:\n   - \"8000:8000\"\n networks:\n   - main\n environment:\n   - DB_HOST=db\n   - DB_NAME=blog\n   - DB_USER=postgres\n   - DB_PASS=supersecretpassword\n depends_on:\n  - db",
    "java.awt.AWTError: Can't connect to X11 window server using ':0' as the value of the DISPLAY variable": "You are using java:8 base image which most likely doesn't provide a graphical environment.\nYou can use ubuntu:18.04 base image with manually installed openjdk-11-jdk and xvfb packages. The xvfb-run command will take care of setting up the virtual X Server environment:\nxvfb-run sets up an X authority file (or uses an existing user-specified one), writes a cookie to it (see xauth(1x)) and then starts the Xvfb X server as a background process. The process ID of Xvfb is stored for later use. The specified command is then run using the X display corresponding to the Xvfb server just started and the X authority file created earlier.\nDockerfile\nFROM ubuntu:18.04\nRUN apt-get update -y && apt-get upgrade -y && apt-get install -y openjdk-11-jdk xvfb \nADD JFrameDocker.java MANIFEST.mf ./\nRUN javac JFrameDocker.java\nRUN jar cfm JFrameDocker.jar MANIFEST.mf JFrameDocker.class \nRUN xvfb-run java -jar JFrameDocker.jar\nJFrameDocker.java\nimport java.awt.FlowLayout;\nimport javax.swing.JFrame;\nimport javax.swing.JLabel;\nimport javax.swing.JPanel;\n\npublic class JFrameDocker {\n\n    public static void main(String[] args) {\n        JFrame frame = new JFrame();\n        JPanel panel = new JPanel();\n        JLabel lable = new JLabel(\"Hello World\");\n        panel.setLayout(new FlowLayout());  \n        frame.add(panel);\n        panel.add(lable);\n        frame.setSize(800, 600);\n        frame.setVisible(true);\n        System.out.println(\"Up and running\");\n    }\n\n}\nMANIFEST.mf\nManifest-Version: 1.0\nMain-Class: JFrameDocker",
    "Passing multiple classFile as argument to Dockerfile": "You have to differ between docker run, cmd and entrypoint.\nFor your example you can use an entrypoint and set the parameter via an environment variable.\nOne simple and easy Dockerfile example could be:\nFROM java:8\nENV NAME=\"John Dow\"\nENTRYPOINT [\"/bin/bash\", \"-c\", \"echo Hello, $NAME\"]\nwith docker build . -t test and docker run -e NAME=\"test123\" test\nAlso have a look at some further docu: docker-run-vs-cmd-vs-entrypoint.",
    "Dockerfile > RUN mongoimport => Failed: error connecting to db server: no reachable servers": "RUN mkdir -p /opt/acrc/config\nCOPY *.json /opt/acrc/config/\nRUN mongod --fork --logpath=/tmp/mongodb.log && sleep 20 && \\\n  mongoimport -c=users -d=acrc --mode=upsert --file=/opt/acrc/config/users.json && \\\n  mongoimport -c=tasks -d=acrc --mode=upsert --file=/opt/acrc/config/tasks.json && \\\n  mongoimport -c=configs -d=acrc --mode=upsert --file=/opt/acrc/config/configs.json && \\\n  mongoimport -c=agentGroupConfigs -d=acrc --mode=upsert --file=/opt/acrc/config/agentGroupConfigs.json  ",
    "Creating a file on a mounted volume while building a docker container": "Volume mounts apply at run time, not during a build. The output of a build is a portable image, not an image and some persistent data you would need to ship separately. That file should exist inside your image, depending on the rest of your Dockerfile.\nWhen you mount a host volume at run time, anything inside the built image is hidden by the host mount. You only see files (along with permissions, uid, and gid) as they exist in the host.\nHowever, if you switch to named volumes, docker will initialize that volume when it's new/empty with the contents of the image at that location.",
    "Docker custom user id permissions in a volume (mounted and not mounted)": "A very typical use of an ENTRYPOINT is to have it set to a wrapper script that receives the \"real\" command as command-line arguments. It does whatever setup it needs to do, then runs the actual command.\n#!/bin/sh\nchown -R ...\nexec \"$@\"\nThen in the same way you can run the base images with just a command line, you can run your own images, without a random -c argument\ndocker run ubuntu:18.04 ls -l /\ndocker run test:1 ls -l /\nWhatever it is your container usually does, you'd set it as a CMD.\nCOPY entrypoint.sh /\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD [\"my-server\", \"--foreground\"]\nIf you needed an interactive shell, your entrypoint script gets to do its setup before the actual shell gets launched.\ndocker run --rm -it test:1 sh\nI believe the bind-mounted volume's ownership changing is specifically a feature of Docker for Mac, and it's a consequence of Docker running in a hidden Linux VM that can't directly use the OSX host filesystem. There is some extended discussion in the Docker documentation, more specifically including what happens when a container chowns a file.",
    "exit code 127 and ': No such file or directory`": "start.sh clearly contains DOS newlines. Add a new command:\nRUN dos2unix /opt/start.sh\n...after the COPY.\nTo make this easier to diagnose, you can make your shell use xtrace logging. To quote a comment on the question:\nComment out the ENTRYPOINT and change the CMD to CMD [\"/bin/bash\", \"-x\", \"/opt/start.sh\", \"run\"]. Logs will be on stderr.",
    "How to map user permissions defined in a Docker container that already exist on the host": "You are right about /var/lib/postgresql/data. When you run the container it changes, in the container, the owner of the files in that directory to user postgres (with user id 999). If the files are already present in the mounted volume, changing the ownership may fail if the user you run docker with does not have the right permissions. There is an excellent explanation about file ownership in docker here Understanding user file ownership in docker.\nMy question is, if a container defines permissions that already exist on the host, how do you map permission from the host to the container without having to rebuild the container itself with the configuration from your environment?\nI think what you might be looking for is docker user namespaces. Introduction to User Namespaces in Docker Engine. It allows you to fix permissions in docker volumes.\nFor your specific case if don't want the files in the mounted volume to have uid 999 you could just override the entrypoint of the container and change the uid of the user postgres.\ndocker run --entrypoint=\"bash\" postgres -c  'usermod -u 2006 postgres;exec /docker-entrypoint.sh postgres'",
    "curl (56) Recv failure: Connection reset by peer - when hitting docker container": "Do a small check by running:\ndocker run --network host -d <image>\nif curl works well with this setting, please make sure that:\nYou're mapping the host port to the container's port correctly:\ndocker run -p host_port:container_port <image>\nYour service application (running in the container) is running on localhost or 0.0.0.0 and not something like 127.0.0.1",
    "Docker Error: standard_init_linux.go:195: exec user process caused \"exec format error\"": "Check what is the ENTRYPOINT by inspecting your image.\nIf it is a standard sh -c, a CMD ./run.sh should be enough.\nBut if it is not, ./run.sh might not be compatible with ENTRYPOINT command.\nADD ./SciGraph /usr/share/SciGraph\nThat will copy an executable SciGraph to your Ubuntu image.\nIf that executable was not compiled for that Ubuntu 64bits platform, that would generate an \"exec format error\" at runtime.\nThat is why Go crosscompilation is important.",
    "Dockerfile: ENTRYPOINT and CMD": "You can create you own my-entrypoint.sh\n$ cat my-entrypoint.sh\n#!/bin/bash\n\n#do what you need here e.g. <my-command>\n\n#next command will run postgres entrypoint will all params\ndocker-entrypoint.sh \"$@\"\nAnd your docker file will look as follows\nFROM postgres\n\n# your stuff\n\nENTRYPOINT [\"my-entrypoint.sh\"]\nEXPOSE 5432\nCMD [\"postgres\"]",
    "Dockerfile - parameterized FROM": "Docker 1.9 has added support for build time arguments.\nReason for the error",
    "Docker, Dockerfile and using wait-for-it before I start my next service": "You can use a container called dadarek/wait-for-dependencies as a mechanism to wait for services to be up. Handling this type of thing at runtime should be easier than trying to handle at build time.\nYou didn't post your docker-compose.yml file, but here is how you can implement it.\n1). Add a new service to your docker-compose.yml\n  waitforsso:\n    image: dadarek/wait-for-dependencies\n    depends_on:\n      - sso \n    command: sso:8080\nYour docker-compose.yml should look now look like this:\nversion: '3'\nservices:\n  waitforsso:\n    image: dadarek/wait-for-dependencies\n  depends_on:\n    - sso \n  command: sso:8080\n\n  # MySQL database for Keycloak\n  db:\n    image: mysql:5.7\n    env_file: ./env/.envmysql\n    volumes:\n      - db_accounts:/var/lib/mysql\n\n  # Keycloak server\n  sso:\n    image: dina/keycloak:v0.1\n    env_file: \n      - ./env/.envmysql\n      - ./env/.envaccounts\n    environment:\n      - TZ=Europe/Stockholm\n      - MYSQL_PORT_3306_TCP_ADDR=mysql\n      - MYSQL_PORT_3306_TCP_PORT=3306\n      - PROXY_ADDRESS_FORWARDING=true \n    links:\n      - db:mysql\n\n  # Java JSON-API\n  api:\n    image: dina/accounts-api:v0.1\n    env_file:  \n      - ./env/.envaccounts\n    environment:\n      - VIRTUAL_HOST=alpha-api.dina-web.net\n    volumes:\n      - ./env/.envapi:/usr/src/myapp/project-initdata.yml\n    ports:\n      - \"8181:8181\"\n\n  # Keycloak API proxy\n  ws:\n    image: nginx\n    container_name: alpha-sso.dina-web.net\n    environment:\n      - VIRTUAL_HOST=alpha-sso.dina-web.net\n  #  links:\n  #    - api \n    volumes:\n      - ./nginx-conf/app.conf:/etc/nginx/conf.d/app.conf\n      - ./nginx-certs:/etc/nginx/ssl\n    depends_on:\n      - waitforsso\n      - db\n\n  # Ember frontend\n  ui:\n    image: dina/accounts-ui:v0.1\n    volumes:\n      - ./nginx.conf:/etc/nginx/conf.d/default.template\n    environment:\n      - VIRTUAL_HOST=alpha-accounts.dina-web.net\n      - VIRTUAL_PROTO=http\n      - NGINX_HOST=alpha-accounts.dina-web.net\n      - NGINX_PORT=80\n    command: /bin/ash -c \"envsubst '$$NGINX_HOST $$NGINX_PORT $$NGINX_ROOT $$NGINX_INDEX' < /etc/nginx/conf.d/default.template > /etc/nginx/conf.d/default.conf && nginx -g 'daemon off;'\"\n\n  # Generic proxy\n  proxy:\n    image: jwilder/nginx-proxy:alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - /var/run/docker.sock:/tmp/docker.sock:ro\n      - ./certs:/etc/nginx/certs\n\nvolumes:\n  db_accounts:\n3). You must remove anything related to wait-for-it.sh from your relevant Dockerfiles, then rebuild those images.\n4). Startup compose\ndocker-compose run --rm waitforsso\ndocker-compose up -d sso db api ws proxy ui\nThe result is that your ws service should now wait for port 8080 to be up(i.e. your sso service to be up)",
    "How to Build & Push Container from docker-compose.ci.build.yml File": "I ran into a number of issues getting this to build initially. Fixes for me included updating Visual Studio 2017 to Update 4, updating the Visual Studio Tools for Docker extension, and updating/consolidating NuGet package references. In addition, in docker-compose.ci.build.yml I had to change the build image to microsoft/aspnetcore-build:1.0-2.0-stretch (see this GitHub issue for more details).\nAssuming the docker-compose configurations are the same/similar to the default generated ones, try the following script:\n$ docker-compose -f docker-compose.ci.build.yml up\n$ docker-compose -f docker-compose.yml build\nBringing \"up\" docker-compose.ci.build.yml simply builds the project and then exits. The artifacts are published to the obj/Docker/publish folder. The \"build\" docker-compose.yml actually builds and tags the images using the artifacts from the obj/Docker/publish folder. I clean up at the end of my build script with the following:\n$ docker-compose -f docker-compose.ci.build.yml down\nFor reference/inspiration-- This is working on a custom Linux build server and producing four Docker images from a single solution-- two ASP.NET Core apps and two .NET Core apps.",
    "Is there a recommended max size limit for docker images as good practice?": "After searching blogs and codes, I am nearly sure that Default Base Size is 100GB. So, you can use within this limit. I have collected a code snippet from moby/moby.\ndefaultDataLoopbackSize      int64  = 100 * 1024 * 1024 * 1024\ndefaultMetaDataLoopbackSize  int64  = 2 * 1024 * 1024 * 1024\ndefaultBaseFsSize            uint64 = 10 * 1024 * 1024 * 1024\ndefaultThinpBlockSize        uint32 = 128 // 64K = 128 512b sectors \nPreviously, the size was 10GB. See the Pull Request.\nAlso, You can increase the size. You may review the Device Mapper Driver docs for details.",
    "Docker refused to connect": "Bind your container port to 127.0.0.1:5000.\nBy default, if you don't specify an interface on port mapping, Docker bind that port to all available interfaces (0.0.0.0). If you want to bind a port only for localhost interface (127.0.0.1), you have to specify this interface on port binding.\nDocker\ndocker run ... -p 127.0.0.1:5000:5000 ...\nDocker Compose\nports:\n - \"127.0.0.1:5000:5000\"\nFor further information, check Docker docs: https://docs.docker.com/engine/userguide/networking/default_network/binding/",
    "Add alias to Docker during build": "When docker executes each RUN, it calls to the SHELL passing the rest of the line as an argument. The default shell is /bin/sh. Documented here\nThe problem here is that you need for each layer execution, to set the aliases, because a new shell is launched by each RUN. I didn't find a non-interactive way to get bash read the .bashrc file each time.\nSo, just for fun I did this, and it's working:\naliasshell.sh\n#!/bin/bash\n\nmy_ls(){\n  ls $@\n}\n\n$@\nDockerfile\nFROM ubuntu\n\nCOPY aliasshell.sh /bin/aliasshell.sh\n\nSHELL [\"/bin/aliasshell.sh\"]\n\nRUN ls -l /etc/issue\nRUN my_ls -l /etc/issue\nOutput:\ndocker build .\nSending build context to Docker daemon 4.096 kB\nStep 1/5 : FROM ubuntu\n ---> f7b3f317ec73\nStep 2/5 : COPY aliasshell.sh /bin/aliasshell.sh\n ---> Using cache\n ---> ccdfc54dd0ce\nStep 3/5 : SHELL /bin/aliasshell.sh\n ---> Using cache\n ---> bb17a8bf1c3c\nStep 4/5 : RUN ls -l /etc/issue\n ---> Running in 15ae8f0bb93b\n-rw-r--r-- 1 root root 26 Feb  7 23:55 /etc/issue\n ---> 0337da801651\nRemoving intermediate container 15ae8f0bb93b\nStep 5/5 : RUN my_ls -l /etc/issue                   <-------\n ---> Running in 5f58e0aa4e95\n-rw-r--r-- 1 root root 26 Feb  7 23:55 /etc/issue\n ---> b5060d9c5e48\nRemoving intermediate container 5f58e0aa4e95\nSuccessfully built b5060d9c5e48",
    "How to disable the root access of a docker container?": "You can modify your container creating a user (foo for example) and assigning to him the right permissions. Then you can run the docker container on docker run command using the arguments -u foo. If you run for example: docker run --rm -ti -u foo myCustomImage sh. This will open the sh shell with the $ instead of #. Of course on your Dockerfile you must create foo user before.\nIf you want more restrictions like for example to disable some kernel features, you have available since docker 1.10 the seccomp security feature. Check it out:\nhttps://docs.docker.com/engine/security/seccomp/\nUsing this you can disable and restrict a lot of system features... and easy example to deny the mkdir command. Create a json file like this (name it as sec.json for example):\n{\n    \"defaultAction\": \"SCMP_ACT_ALLOW\",\n        \"syscalls\": [\n                {\n                    \"name\": \"mkdir\",\n                    \"action\": \"SCMP_ACT_ERRNO\"\n                }\n            ]\n}\nThen run your container doing: docker run --rm -ti --security-opt seccomp=/path/on/host/to/sec.json ubuntu:xenial sh. You can check inside the container you are not able to run mkdir command.\nHope this helps.",
    "Cannot start rabbitmq-server in docker container, how to write this Dokcerfile?": "So as we discussed in comments, I'd suggest you to use official docker image for rabbitmq. Therefore you will end-up with 2 containers. In is this case: app and rabbit. Here is an example of Dockerfile and docker-compose.yml:\nDockerfile:\n# use base python image with python 2.7\nFROM python:2.7\n\n# add requirements.txt to the image\nADD requirements.txt /app/requirements.txt\n\n# set working directory to /app/\nWORKDIR /app/\n\n# install python dependencies\nRUN apt-get update\nRUN apt-get -y install libpq-dev python-dev\nRUN pip install -r requirements.txt\nAnd example of docker-compose.yml:\nversion: '2'\n\nservices:\n  # RabbitMQ\n  rabbit:\n    hostname: rabbit\n    image: rabbitmq:3.6.1-management\n    ports:\n      - \"5672:5672\"  # we forward this port because it's useful for debugging\n      - \"15672:15672\"  # here, we can access rabbitmq management plugin\n\n  # App\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    hostname: app\n    volumes:\n      - .:/app  # mount current directory inside container\n    ports:\n      - \"8000:8000\"\n    # set up links so that web knows about db and redis\n    links:\n      - rabbit\nTo start new containers use docker-compose build && docker-compose up -d",
    "How can I dockerize Apache ZooKeeper with seed data inserted?": "You can leverage docker-compose for this.\ndocker-compose.yml\nversion: '2'\n\nservices:\n   zookeeper:\n     image: zookeeper:latest\n     restart: always\n     ports:\n       - \"2181:2181\"\n   zookeeper_setup:\n     image: zookeeper:latest\n     depends_on:\n       - zookeeper\n     links:\n       - zookeeper\n     command: sh /setup/startup.sh\n     volumes:\n       - ./:/setup\nMake sure you remove ./bin/zkServer.sh start-foreground from startup.sh and replace localhost with zookeeper in startup.sh.\nThen just run docker-compose up so it will first start zookeeper service and then it will run zookeeper_setup service with modified command. (make sure you mount directory with startup.sh properly in zookeerper_setup service)\nIf zookeeper needs more time to bootup, you can add sleep 5 (or more) in startup.sh.\nNext time, when you don't need to configure it again with startup.sh, you can run docker-compose up zookeeper to start pre-configured zookeeper service.",
    "Need some advice dockerizing MongoDB": "Do I need VOLUME /data/db /data/configdb at the Dockerfile or would be enough to have this line ~/data/mongo:/data/configdb at docker-compose.yml?\nVOLUME is not required when you are mounting a host directory but it is helpful as metadata. VOLUME does provide some special \"copy data on volume creation\" semantics when mounting a Docker volume (non host dir) which will impact your data initialisation method choice.\nam assuming (and I took it from here) that as soon as I build the Mongo image I will be creating a database and giving full permissions to the user with password as it's on the environment variables? I am right? (I couldn't find anything helpful here)\nMONGO_USER, MONGO_DATABASE and MONGO_PASS do not do anything in the official mongo Docker image or to mongod itself.\nThe mongo image has added support for similar environment variables:\nMONGO_INITDB_ROOT_USERNAME\nMONGO_INITDB_ROOT_PASSWORD\nMONGO_INITDB_DATABASE\nHow do I import a current mongo backup (several JSON files) into the database that should be created on the mongo container? I believe I need to run mongorestore command but how? do I need to create an script and run it each time the container start? or should I run during image build? What's the best approach?\nWhether you initialise data at build or runtime is up to your usage. As mentioned previously, Docker can copy data from a specified VOLUME into a volume it creates. If you are mounting a host directory you probably need to do the initialisation at run time.\nmongorestore requires a running server to restore to. During a build you would need to launch the server and restore in the same RUN step. At runtime you might need to include a startup script that checks for existence of your database.\nMongo is able to initialise any empty directory into a blank mongo instance so you don't need to be worried about mongo not starting.",
    "How do you do cancel a dockerfile image building on the first error it encounters?": "Are you sure the command is really returning an error? The following Dockerfile doesn't get to the echo foo:\nFROM alpine\nRUN false\nRUN echo foo\nIt just gets:\n# docker build .\nSending build context to Docker daemon 3.072 kB\nStep 0 : FROM alpine\n ---> 0a3b5ba3277d\nStep 1 : RUN false\n ---> Running in 22485c5e763c\nThe command '/bin/sh -c false' returned a non-zero code: 1\nTo check whether your command is really failing, you could try something like this:\nFROM alpine\nRUN false || echo failed\nRUN echo foo\nwhich then gets me:\n# docker build .\nSending build context to Docker daemon 3.072 kB\nStep 0 : FROM alpine\n ---> 0a3b5ba3277d\nStep 1 : RUN false || echo failed\n ---> Running in 674f09ae7530\nfailed\n ---> 232fd66c5729\nRemoving intermediate container 674f09ae7530\nStep 2 : RUN echo foo\n ---> Running in c7b541fdb15c\nfoo\n ---> dd1bece67e71\nRemoving intermediate container c7b541fdb15c\nSuccessfully built dd1bece67e71",
    "How do I combine several images into one?": "You can chain them. You can find more here\nhttps://github.com/docker/docker/issues/3378#issuecomment-31314906\nTaken from the above link\n## Dockerfile.genericwebapp might have FROM ubuntu\ncat Dockerfile.genericwebapp | docker build -t genericwebapp -\n## Dockerfile.genericpython-web would have FROM genericwebapp\ncat Dockerfile.genericpython-web | docker build -t genericpython-web -\n## and then this specific app i'm testing might have a docker file that containers FROM genericpython-web\ndocker build -t thisapp .\nauthor of the above SvenDowideit\nIn general though, it's a bad practice to have more than one running processes on the same container.",
    "Docker: Mysql crashes after few seconds when it has been started": "I'm guessing that you've got permissions issues, and that /var/run/mysqld is owned by the UID/GID of the mysql user created by the MySQL install process in the original Docker image. Try this in your Dockerfile:\nFROM mysql:latest\nRUN deluser mysql\nRUN useradd mysql\n\nRUN chown mysql:mysql /var/run/mysqld\n\nRUN mkdir -p /Users/me/docker/mysql/data\nRUN chmod -R 777 /Users/me/docker/mysql/data\nFurther guessing that the case is the same for the 'keyring_file', wherever that's located. Try running a docker run -it manolitomysql /bin/bash to get a shell prompt into your container, and see if you can draw a bead on it.",
    "How to populate a Mysql docker container during build of another container?": "When executing the docker-compose build command, docker will just build the images of each service defined in the docker-compose.yml file (in your case mysql and myapplication), so that means that no container will be run.\nOnce you execute the docker-compose run command, docker will run a container per image built previously, one for the mysql and another for the myapplication. Now that both are up, they can interact with each other.\nSo, what should I do?\nI will just execute the install_all.sh script (guessing that it contains the database setup) when running the container and before the startup.sh script is executed. Note that the mysql container will only be accessible when it is up, that mean only after the docker-compose up execution.\ndocker-compose.yml\n...\nmyapplication:\n  command: bash -c \"/home/myapplication/Tools/install_all.sh && /home/myapplication/startup.sh\"\n...\nAlso, note that your myapplication container has to have installed a mysql-client to be able to communicate with the mysql server. Just adding the below line to your Dockerfile will install it:\nRUN apt-get update && apt-get install mysql-client -y\nAnd in your install_all.sh script, you can have some calls to mysql like below (that create a database):\n#!/bin/bash\nmysql -h $DOCKER_MYSQL_PORT_3306_TCP_ADDR -P $DOCKER_MYSQL_PORT_3306_TCP_PORT -u $DOCKER_MYSQL_ENV_MYSQL_USER -p $DOCKER_MYSQL_ENV_MYSQL_ROOT_PASSWORD -e \"CREATE DATABASE IF NOT EXISTS new_database;\"",
    "Docker VOLUME for different users": "The file paths on the host shouldn't matter. Why do you need absolute paths?\nYou can use paths that are relative to the docker-compose.yml so they should be the same for both developers.\nThe VOLUME instructions in the Dockerfile are always relative to the build context, so if you want, you can use something like this:\napp:\n    container_name: sup-dev\n    build: ..\n    dockerfile: build/Dockerfile\nThat way the build context for the Dockerfile will be the project root.",
    "Home symbol `~` not recognized in Dockerfile": "In Docker, it is not possible to copy files from anywhere on the system into the image, since this would be considered a security risk. COPY paths are always considered relative to the build context, which is current directory where you run the docker build command.\nThis is described in the documentation: https://docs.docker.com/reference/builder/#copy\nAs a result, the ~ has no useful meaning, since it would try and direct you to a location which is not part of the context.\nIf you want to put your local id_rsa file into the docker, you should put it into the context first, e.g. copy it along side the Dockerfile, and refer to it that way.",
    "Docker Compose Dev and Production Environments Best Workflow": "According to the official docs:\n... you\u2019ll probably want to define a separate Compose file, say production.yml, which specifies production-appropriate configuration.\nNote: The extends keyword is useful for maintaining multiple Compose files which re-use common services without having to manually copy and paste.",
    "Privileges in an Ubuntu Docker container after USER statement in Dockerfile": "If you don't want to use sudo, you could have a Dockerfile without USER (so it runs the command as root) and CMD pointing to a script that does the user switching, that way a docker exec would run as root.\nOther way is to set the root password and use su. An example of doing that is in the tutum images\nhttps://github.com/tutumcloud/tutum-centos/blob/master/set_root_pw.sh",
    "Docker Server Error : request returned Internal Server Error for API route and version": "I ran into the same issue after I upgraded docker desktop to 4.28.0 in windows 10 machine. My fix is to uninstall docker desktop then install 4.27.2.",
    "npm install exited with status 232": "Docker sets the \"number of open files\" limit when running commands inside the container. In your case, this limit is too low, causing the error you see when running npm.\nTo fix, run the same docker command, but pass a new flag: --ulimit nofile=65535:65535 along with the other flags to the docker command.\nExample:\ndocker build --ulimit nofile=65535:65535 -t my_container_name .",
    "A connection was successfully established... (provider: SSL Provider, error: 31 - Encryption( ssl /tls) handshake failed)": "I just solved my concern by combining the following.\nUsed Microsoft.Data.SqlClient instead of Systems.Data.SqlClient.\nAdded commands to Dockerfile of the API\nRUN sed -i 's/DEFAULT@SECLEVEL=2/DEFAULT@SECLEVEL=1/g' /etc/ssl/openssl.cnf\nRUN sed -i 's/DEFAULT@SECLEVEL=2/DEFAULT@SECLEVEL=1/g' /usr/lib/ssl/openssl.cnf\nAdded TrustServerCertificate=True to connection string of the database.\nThis lowers the TLS version of docker image or container to TLSv1. However, this is not the best solution. It would be better to upgrade the SQL server to allow the connection to accept TLSv1.2. Which in my case is not applicable.",
    "How can I fix \"unable to update registry, network failure seems to have happened\" error when building with cargo in Docker container?": "I added CARGO_NET_GIT_FETCH_WITH_CLI=true before cargo build and now it works.",
    "Add SSL certificate to store in docker": "In order to execute admin tasks you should use ContainerAdministrator user\nFROM mcr.microsoft.com/dotnet/core/runtime:3.1-nanoserver-1809 \nARG source\nARG BUILD_ENV=development\nUSER ContainerAdministrator\n...",
    "Docker: mount: permission denied (are you root?) on DIND": "You can try to update the following file /etc/gitlab-runner/config.toml by putting the privileged variable to true just like this :\nconcurrent = 1\ncheck_interval = 0\n\n[[runners]]\n  name = \"kms\"\n  url = \"http://example.org/ci\"\n  token = \"3234234234234\"\n  executor = \"docker\"\n  [runners.docker]\n    tls_verify = false\n    image = \"alpine:3.4\"\n    privileged = true\n    disable_cache = false\n    volumes = [\"/cache\"]\n  [runners.cache]\n    Insecure = false\nI didn't try it myself but it could be a solution though!",
    "Docker image does not run with specified Python version": "The error is in the way you have defined the Dockerfile\nWhile creating a docker multi stage build, your final container will always be based off the last docker container you have referenced\nSo in your case it will be aaftio/face_recognition which uses Python 3.4.9 and not python:3.7.12-slim-buster which uses Python 3.7.12\nReference for docker multi stage build - here\nYou can try something like this\nFROM aaftio/face_recognition as intermediate\nRUN pip install face_recognition\n\n\nFROM python:3.7.12-slim-buster\n\n#Copy the python installed libraries from intermediate container\n#COPY --from=intermediate face_recognition\n\nRUN pip install redis\nRUN pip3 install glob2\nCOPY ./worker.py /worker.py\nCOPY ./rediswq.py /rediswq.py\n\nCMD  python3 worker.py",
    "Module Not found after attaching volume in docker": "Your problem was you build your image somewhere and then try to map another folder to it.\n|_MyFolder/\n  |_ all-required-files\n  |_ all-required-folders\n  |_ Dockerfile\ndocker build -t node-app-image .\ndocker run -p 3000:3000 -d --name node-app node-app-image\nSimplified Dockerfile\nFROM node:15\n# sets the folder structure to /app directory\nWORKDIR /app\n\n# Copy all files from current directory to current directory in docker(app)\nCOPY . ./\n\nRUN npm install\n\nEXPOSE 3000\nCMD [\"node\",\"index.js\"]",
    "Optimizing dockerfile image size. What more & how can i reduce size of this image?": "There are several ways to make your Docker Image smaller. Looking at your example 2 thing come to mind:\nTry to create an Image in more than one Stage. Take the tools you need to creating the image in one Stage and create the last version of the container by only copying files from the previous Stages. See Docker documentation on MultiStage Containers\nYou are taking a Ubuntu image, which if very large. Better to take Alpine in the last Stage\nIn that Ubuntu container you are setting up the whole application (Python, Maven, Java). This is not the philosophy of Docker. Better to create an Image for every service. Python-container, Java-container, etc. And with this setup try to stick to standard images. The moment you need to do apt-get in a container you need to think where you went wrong and how you can split it up in different containers.\nFor the different containers talking to each other, use docker-compose.",
    "Why do we need to have an Alpine or Ubuntu Base Image in the Dockerfile?": "All processes in the Docker Container are isolated.\nSo By design your Host programs are not available inside the Docker image.\nTechnically, you do not need Alpine or Ubuntu Linux base image. For example you can use the Scratch image: https://hub.docker.com/_/scratch\nIdea of the docker is to provide fully isolated exactly the same environment for application, they do not force to use any base image.\nHowever it's good to use official base image because:\nYou have a lot of tutorials about common distributions\nYou have a lot of preinstalled tools provided with base image.\nOfficial images are maintained by community so they are fixing issues for you.\nDocker uses layer design, to minimize size of images. More info here: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/",
    "Docker build step name cannot start with number": "You could try and see if LolHens's idea of changing the hostname in the container namespace (during the docker build) works for you.\ndocker build . | tee >((grep --line-buffered -Po '(?<=^change-hostname ).*' || true) | \\\n                       while IFS= read -r id; do \\\n                         nsenter --target \"$(docker inspect -f '{{ .State.Pid }}' \"$id\")\"\\\n                                 --uts hostname 'new-hostname'; \\\n                       done)\nThe docker build output is parsed to:\ndetect a \"change-hostname\" directive\ndo a nsenter, which runs a program in the UTS (UNIX Time Sharing) namespace, with a different hostname (different than the SHA-generated random one)\nThat means your RUN step should be:\nRUN echo \"change-hostname $(hostname)\"; \\\n    sleep 1; \\\n    printf '%s\\n' \"$(hostname)\" > /etc/hostname; \\\n    printf '%s\\t%s\\t%s\\n' \"$(perl -C -0pe 's/([\\s\\S]*)\\t.*$/$1/m' /etc/hosts)\" \"$(hostname)\" > /etc/hosts; \\\n    init_db.sh\nThat way, init_db.sh should run in an intermediate container with a different hostname (one you do have control over, and which would not start with a number).",
    "Docker container exited with exit code 2 \"sh can't open 'start_script.sh': No such file or directory\"": "As per the comments on my question I used docker-compose run backend sh to inspect my container. It turned out that I indeed didn't copy any source code or the start_script.sh to my container. I made changes to my Dockerfile to make it work:\nFROM python:3.7.4-alpine3.10\nLABEL maintainer = ******\n\nENV PYTHONUNBUFFERED 1S\nENV RUNNING_IN_DOCKER True\n\nRUN apk add --update --no-cache build-base postgresql-client exiftool jpeg-dev zlib-dev gettext git openssl\nRUN apk add --update --no-cache gcc libc-dev linux-headers postgresql-dev file-dev py-magic libffi-dev libxml2-dev\n\nRUN mkdir /app\nCOPY ./start_script.sh /app/start_script.sh.      --------> Copy the start_script.sh\nCOPY ./requirements.txt /app/requirements.txt\nRUN pip install -r /app/requirements.txt\n\nCOPY . /app                                       --------> Copy the source code\nWORKDIR /app\n\nCMD sh start_script.sh",
    "Set Assembly version of net core app using Azure DevOps with a DockerFile": "From pipeline:\n- task: Docker@1\n  displayName: 'Build the image'\n  inputs:\n    dockerfile: 'Dockerfile'\n    imageName: '$(imageRepoName):$(GitVersion.SemVer)'\n    arguments: '--build-arg version=$(GitVersion.SemVer)'\n  continueOnError: true\nOn DockerFile:\nFROM mcr.microsoft.com/dotnet/sdk:5.0-alpine AS build\nWORKDIR /src\nCOPY . .\nARG version  #define the argument\n#use this argument while running build or publish, for example\nRUN dotnet publish \"./yourProject.csproj\" -c Release /p:Version=${version} -o /app/publish --no-restore",
    "System hangs after pip install in docker build": "install pip first (in this order in your dockerfile)\nRUN pip install -U pip\nthen run pip install (ex. for python3)\nRUN python3 -m pip install --no-cache-dir -r requirements.txt",
    "docker-compose not reading environment variables": "It looks like Docker is interpreting the value after the equal sign as a literal.\nCheck out https://docs.docker.com/compose/compose-file/#variable-substitution. It mentions using an env file to set defaults or doing them inline. It also uses a dollar sign and braces for the variable.\nFor example: - MY_VAR=${MY_ENV_VAR:my_var_default_value}\nIn all of my cases I either prefix the substitution with '$'. In some, I also surround it with braces.\nFERRO_LOG_LEVEL=${FERRO_LOG_LEVEL:1}\nFERRO_SECRET=${FERRO_SECRET}",
    "passing $SOURCE_COMMIT to Dockerfile commands on docker hub": "This is how I've always done it too. I don't think there's another way.\nIn fact, I think it was intended to be this way.\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg = flag. If a user specifies a build argument that was not defined in the Dockerfile, the build outputs a warning.\nDocs",
    "Bash Heredoc doesn't seem to work with Docker RUN statement": "The Docker build process is completely non-interactive, and if you are looking for some input then you need to pass build-args, and the reference these build-args in your sub-sequent RUN command.\nBut as mentioned in the commend you need to run them as a CMD as it will stuck the build process.\nHere is Dockerfile with some example entrypoint that may help you,\n# start steamcmd to force it to update itself\nRUN ./steamcmd/steamcmd.sh +quit\n\n# start the server main script\nENTRYPOINT [\"bash\", \"/home/steam/server_scripts/server.sh\"]\nYou can change it to\n/home/steam/steamcmd/steamcmd.sh \\\n    +login anonymous \\\n    +exit\nand one example from above list is cs_go.sh which you need to rename to server.sh is\n#!/bin/bash\n\n# update server's data\n/home/steam/steamcmd/steamcmd.sh \\\n    +login anonymous \\\n    +force_install_dir /home/steam/server_data \\\n    +app_update 740 \\\n    +exit\n\n# start the server\n/home/steam/server_data/srcds_run \\\n    -game csgo -console -usercon \\\n    -secure -autoupdate -tickrate 64 +hostport 27015 \\\n    +game_type 0 +game_mode 1 +mapgroup mg_active +map de_dust2 \\\n    -port 27015 -console -secure -nohltv +sv_pure 0 +ip 0.0.0.0\n\nexit 0\nupdated:\nAutomating SteamCMD\nThere are two ways to automate SteamCMD. (Replace steamcmd with ./steamcmd.sh on Linux/OS X.)\nCommand line\nNote: When using the -beta option on the command line, it must be quoted in a special way, such as +app_update \"90 -beta beta\".\nNote: If this does not work, try putting it like \"+app_update 90 -beta beta\" instead.\nAppend the commands to the command line prefixed with plus characters, e.g.:\nsteamcmd +login anonymous +force_install_dir ../csgo_ds +app_update 740 +quit\nAutomating SteamCMD",
    "docker-compose external build context with relative dockerfile": "Is there anyway I can have docker-compose load the ctx.dockerfile from same directory as docker-compose.yml AND set the context the way that I am?\nAFAIK: No, there isn't.\nEverything that the Dockerfile interacts with on build time must be in the defined context. So, you need .aws and the current folder where the docker-compose.yml etc. lives to be in the same context, i.e. the context would need to be the highest level of your relevant directory structure and then you would have to define relative paths to the files you need (the Dockerfiles and .aws).\nMaybe you could set /home/$USER as your build context (or even higher level, depending on where your Dockerfiles etc. live), but then you would also have to create a .dockerignore file and ignore everything in the context besides .aws and the current folder... As you can see, this would be a mess and not very reproducible.\nI would suggest to use a volume instead of COPYing the ~/.aws folder inside your container.\nExample:\nnico@lapap12:~$ ls -l ~/.aws\ntotal 0\n-rw-r--r-- 1 nico nico 0 May 22 17:45 foo.bar\ndocker-compose.yml:\nversion: \"3.7\"\n\nservices:\n  allinone:\n    image: alpine:latest\n    volumes:\n      - ~/.aws:/tmp/aws:ro\n    command: ls -l /tmp/aws\nnico@lapap12:~/local/so$ docker-compose up\nCreating so_allinone_1 ... done\nAttaching to so_allinone_1\nallinone_1  | total 0\nallinone_1  | -rw-r--r--    1 1000     1000             0 May 22 15:45 foo.bar\nso_allinone_1 exited with code 0\nYou could go from there and copy the content of /tmp/aws to /root/.aws if you want to change this folder's content in the container, but don't want to touch it on the actual host.",
    "permission denied in containers after moving docker home": "When doing copy, do use \"-p\" option to preserve attributes. This fixed the issue on my side.\nMake sure your destination partition has not set \"nosuid\" option. Check /etc/fstab. Otherwise, you'll meet other permission issues. https://github.com/wodby/docker4drupal/issues/388\nAll permission issues are gone now.",
    "Docker compose mapping local directory to dockerfile volume": "Docker for Windows uses a CIFS/Samba network file share to bind-mount host files into the Linux VM running docker. That is always done as root:root so all bind-mount files/dirs will always show that when seen from inside container. This is a known limitation of the way docker shares these files between the OS's.\nWorkarounds:\nIn many cases, this isn't an issue. The host files are shared into the container world-readable, so local app development while running in the container is fine. For cache files, user uploads, etc. just be sure they are written into a container path that isn't to the host-bind mount, so they stay in Linux where you can control the perms.\nIf needed, for development only, run the app in the container as root if it needs write permissions to host OS files. You can override this at runtime: e.g. docker run -u root or user:root in docker-compose.yml\nFor working with database files, don't bind-mount them, but use named volumes to keep the files in the Linux VM. You can always use docker cp to copy files in and out of volumes for a quick backup.",
    "Docker Compose File cant get .env Variables": "    env_file: ./env.env\nThe file env.env isn't loaded to parse the compose file, it is loaded to add environment variables within the container being run. At the point docker processes the above instruction, the yaml file has already been loaded and variables have been expanded.\nIf you are using docker-compose to deploy containers on a single node, you can rename the file .env and docker-compose will load variables from that file before parsing the compose file.\nIf you are deploying with docker stack deploy, then you need to import the environment variables into your shell yourself. An example of doing that in bash looks like:\nset -a && . ./env.env && set +a && docker stack deploy ...",
    "Docker multistage build doesn't recognise installed application": "Two issues going on here. The \"php:7.2.5-apache\" image won't have /root/local/bin in the path, and you did not add it to the path during your build. The npm commands will work when you login interactively likely because of some changes to the shell login scripts that setup the environment. You'll need to run these environment setup scripts before running any npm commands, and that must be done within the same RUN command. To verify for yourself, you can check your .bashrc for variables or commands run to setup the npm environment. And you can verify the environment is different by comparing the PATH value with an env command in the interactive shell and in your build, you should see two different outputs if this is your issue. When I ran part of your run image, I saw the following in the .bashrc:\nexport PATH=$HOME/local/bin:$PATH\nSo you'll want to update the line in your Dockerfile for the run image:\nENV PATH /root/local/bin:${PATH}:/home/site/wwwroot\nPer your edit 3, that's an entirely different issue. You created a file in one new image, and then went back to the base image where the file doesn't exist. If you wanted to see the file in a multi-stage build, then you either need to copy it between the stages, or use the previous image as your \"from\".\nFROM alpine as img1\nRUN echo \"$HOME\" > $HOME/test.txt\n\nFROM alpine as img2\nCOPY --from=img1 /root/test.txt /root/test.txt\nRUN cat $HOME/test.txt\nor\nFROM alpine as img1\nRUN echo \"$HOME\" > $HOME/test.txt\n\nFROM img1 as img2\nRUN cat $HOME/test.txt",
    "Create custom Neo4j Docker image with intial data from cypher file": "I was using Bierbarbar's approach. I got it working after getting over the following two pitfalls:\nFirstly, $NEO4J_HOME/data was symlinked to /data, which seem to have permission issues. Changing the default data folder: by adding dbms.directories.data=mydata line to $NEO4J_HOME/conf/neo4j.conf fixed this.\nSecondly, make sure data.cypher file contains correct format for cypher-shell: 1) Semicolon is needed at the end of each cypher statemnt; 2) there are :begin and :commit commands in some versions (or all versions?) of cypher-shell",
    "ERROR: type \"...\" does not exist in Postgresql": "Are you sure that the type is created before the function?\nIs the type created in the same schema as the function? (Is not SET search_path somewhere in the sql file used?)\n$$ and $BODY$ starts \"dolar quoted string\", which are terminated by the same ($$ or $BODY$). Instead of BODY you can have any characters (or any), it just allowes to write $$, \", etc. inside the string without problems. Why postgres use $BODY$ and not $ILOVEBEER$ remains unknown.",
    "Docker Unable to load the service index for source https://api.nuget.org/v3/index.json": "Docker is running in it's own (separated) build environment, but per default it is not aware of your network proxy configuration.\nTo solve the problem, this answer gave me a hint. I ended up issuing:\ndocker build --build-arg HTTP_PROXY=<proxy URL> --build-arg HTTPS_PROXY=<proxy URL> -t <application name>\nThe proxy URL follows this format: http://USERNAME:PASSWORD@PROXYIP:PROXYPORT",
    "How to define docker commit message in Dockerfile": "It is well explained in the official docs here, here is how you do it:\nFirst, commit a container to a image:\n$ docker commit --message \"Foo bar\" 94bde3da7ffa dockertestcommess\nThen, tag the image to fit the registry address:\n$ docker tag dockertestcommess spekulant/dockertestcommess\nAnd finally push the commited image:\n$ docker push spekulant/dockertestcommess\nAnd my docker history shows the message I commited:\n$ docker history spekulant/dockertestcommess\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\nd3c3f4e85723        7 minutes ago       /bin/sh -c cat helloworld.txt                   0B                  Foo bar\nfec5f399e907        3 days ago          /bin/sh -c #(nop)  CMD [\"/bin/sh\" \"-c\" \"cat \u2026   0B\n0f0405202b75        3 days ago          /bin/sh -c #(nop) COPY file:17e1650f32b894fc\u2026   8B\n3fd9065eaf02        3 months ago        /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B\n<missing>           3 months ago        /bin/sh -c #(nop) ADD file:093f0723fa46f6cdb\u2026   4.15MB",
    "Dockerfile run python command not working": "Try following steps and let me know output:\nFROM python:3\nENV PYTHONUNBUFFERED 1\nRUN mkdir /code\nWORKDIR /code\nADD requirements.txt /code/\nADD reports /code/\nRUN pip install -r requirements.txt\nADD . /code/\nRUN ls -l /code/reports/report/manage.py  # gives expected result\nRUN ls -l /code/reports/build_static/  # gives expected result\nRUN python /code/reports/report/manage.py build full_report.views.RenderView  \nRUN ls -l /code/reports/build_static/  # should give you expected list of files\nGive me output for the last step. I'll help you out based on output.",
    "spotify/dockerfile-maven plugin: Could not build image: javax.net.ssl.SSLException: Unrecognized SSL message, plaintext connection": "You need to set pullNewerImage to false, as it defaults to true. For more detail, please refer below link. https://github.com/spotify/dockerfile-maven/issues/3",
    "Docker volumes with Visual Studio": "You need to use docker volumes. You can set the volume on host like below\ndocker run -v ~/Desktop/images:/my/images/inside/docker\nMake sure your code writes to /my/images/inside/docker inside the container. This will move it to the host as well.\nYou can also use named volumes\ndocker volume create images\n\ndocker run -v images:/my/images ...\nThis will store images on a volume which will remain even container dies. But this will not store it on your host\nEdit-1\nSo docker-compose.override.yml is what you need to use. Change the content of the file as\nversion: '3'  \n\nservices:  \n  ci-build:  \n    volumes:  \n      - .:/src\n      - ~/Desktop/images:/images/inside\nThis will combine both the files and then run it.",
    "Dockerfile naming: .dockerfile vs Dockerfile": "This is the docker build command, see the official docs for details.\ndocker build [OPTIONS] PATH | URL | -\nThe docker build command expects the file name to exactly be Dockerfile. So, in that case, you can simply do\ndocker build .\nIn other cases, you have to specify the full name as\ndocker build -f Dockerfile.build .\nSo the file name has nothing to do with the container itself. Those different names are for your own convenience.",
    "Using ccache in automated builds on Docker cloud": "You should try saving and restoring your cache data from a third party service: - an online object storage like Amazon S3 - a simple FTP server - an Internet available machine with ssh to make a scp\nI'm assuming that your cache data is stored inside the \u00b4~/.ccache\u00b4 directory\nUsing Docker multistage build\nFrom some time, Docker supports Multi-stage builds and you can try using it to implement the solution with a single Dockerfile:\nWarning: I've not tested it\n# STAGE 1 - YOUR ORIGINAL DOCKER FILE CUSTOMIZED\n# CACHE_TAG is provided by Docker cloud\n# see https://docs.docker.com/docker-cloud/builds/advanced/\n# using ARG in FROM requires min v17.05.0-ce\nARG  CACHE_TAG=latest\n\nFROM  qgis/qgis3-build-deps:${CACHE_TAG} as builder\nMAINTAINER Denis Rouzaud <denis.rouzaud@gmail.com>\n\nENV CC=/usr/lib/ccache/clang\nENV CXX=/usr/lib/ccache/clang++\nENV QT_SELECT=5\n\nCOPY  . /usr/src/QGIS\n\nWORKDIR /usr/src/QGIS/build\n\n# restore cache\nRUN curl -o ccache.tar.bz2 http://my-object-storage/ccache.tar.bz2\nRUN tar -xjvf ccache.tar.bz2\nCOPY --from=downloader /.ccache ~/.ccache\n\nRUN cmake \\\n -GNinja \\\n -DCMAKE_INSTALL_PREFIX=/usr \\\n -DBINDINGS_GLOBAL_INSTALL=ON \\\n -DWITH_STAGED_PLUGINS=ON \\\n -DWITH_GRASS=ON \\\n -DSUPPRESS_QT_WARNINGS=ON \\\n -DENABLE_TESTS=OFF \\\n -DWITH_QSPATIALITE=ON \\\n -DWITH_QWTPOLAR=OFF \\\n -DWITH_APIDOC=OFF \\\n -DWITH_ASTYLE=OFF \\\n -DWITH_DESKTOP=ON \\\n -DWITH_BINDINGS=ON \\\n -DDISABLE_DEPRECATED=ON \\\n .. \\\n && ninja install\n\n# save the current cache online\nWORKDIR ~/\nRUN tar -cvjSf ccache.tar.bz2 .ccache\nRUN curl -T ccache.tar.bz2 -X PUT http://my-object-storage/ccache.tar.bz2\n\n\n# STAGE 2\nFROM alpine:latest\n# YOUR CUSTOM LOGIC TO CREATE THE FINAL IMAGE WITH ONLY REQUIRED BINARIES\n# USE THE FROM IMAGE YOU NEED, this is only an example\n# E.g.:\n# COPY --from=builder /usr/src/QGIS/build/YOUR_EXECUTABLE /usr/bin\n# ...\nIn the stage 2 you will build the final image that will be pushed to your repository.\n Using Docker cloud hooks\nAnother, but less clear, approach could be using a Docker Cloud pre_build hook file to download cache data:\n#!/bin/bash\necho \"=> Downloading build cache data\"\ncurl -o ccache.tar.bz2 http://my-object-storage/ccache.tar.bz2 # e.g. Amazon S3 like service\ncd /\ntar -xjvf ccache.tar.bz2\nObviously you can use dedicate docker images to run curl or tar mounting the local directory as a volume in this script.\nThen, copy the .ccache extracted folder inside your container during the build, using a COPY command before your cmake call:\nWORKDIR /usr/src/QGIS/build\n\nCOPY /.ccache ~/.ccache    \n\nRUN cmake ...\nIn order to make this you should find a way to upload your cache data after the build and you could make this easily using a post_build hook file:\n#!/bin/bash\necho \"=> Uploading build cache data\"\ntar -cvjSf ccache.tar.bz2 ~/.ccache\ncurl -T ccache.tar.bz2 -X PUT http://my-object-storage/ccache.tar.bz2\nBut your compilation data aren't available from the outside, because they live inside the container. So you should upload the cache after the cmake command inside your main Dockerfile:\nRUN cmake...\n  && tar ...\n  && curl ...\n  && ninja ...\n  && rm ...\nIf curl or tar aren't available, just add them to your container using the package manager (qgis/qgis3-build-deps is based on Ubuntu 16.04, so they should be available).",
    "Docker Run-time Configuration File": "You can mount the directory that contains your json file into the container when running the container using the volume option:\ndocker run -v /host/config:/config myImage\nIf the directory on the host is specified using an environment variable, you can replace /host/config with $CONFIG_LOCATION where CONFIG_LOCATION is a env variable defined on the host.\nIf the env variable does not map directly to the location of the config on the host machine, create a script that resolves the host config location from the env variable and at the end call the above command",
    "docker add \"requires at least one argument\" error": "You need a source and destination for the ADD command. The source here is the app folder path. The destination should be where you want the dockerfile is run.\nTry this I think it might work",
    "Mongodb Docker - Creating initial users and setting up initial structures": "You can use a slight different approach. You can feed mongo with data whenever it starts at first time. You can accomplish that using an auxiliar container mongo-seed.\nSee this approach:\nversion: \"2\"\n\nmongodb:\n  image: mongo\n  ports:\n    - \"27017:27017\"\nmongo-seed:\n  image: mongo\n  command: mongoimport --host mongodb --db example-db --collection MyDummyCollection --type json --file /init.json --jsonArray\n  volumes:\n    - ./init.json:/init.json\ninit.json\n[\n  {\n    \"name\": \"Joe Smith\",\n    \"email\": \"jsmith@gmail.com\",\n    \"age\": 40,\n    \"admin\": false\n  },\n  {\n    \"name\": \"Jen Ford\",\n    \"email\": \"jford@gmail.com\",\n    \"age\": 45,\n    \"admin\": true\n  }\n]\nRun as:\ndocker-compose up\ndocker-compose exec mongodb mongo\n> use example-db\n> db.MyDummyCollection.find()\n{ \"_id\" : ObjectId(\"592f5496fcdcafc0a8e0e0c8\"), \"name\" : \"Joe Smith\", \"email\" : \"jsmith@gmail.com\", \"age\" : 40, \"admin\" : false }\n{ \"_id\" : ObjectId(\"592f5496fcdcafc0a8e0e0c9\"), \"name\" : \"Jen Ford\", \"email\" : \"jford@gmail.com\", \"age\" : 45, \"admin\" : true }\nI took the code from here, but I've improved it without the need of a Dockerfile for the seed.",
    "Docker container for SQL Server Linux keeps exiting": "A Solution That Works\nBased on the feedback I received, it was clear that CMD from Dockerfile was returning. Here are the complete steps that solve the issue.\n[Please let me know if you have found a more reliable solution which does not depend on a certain timeout value I have coded in this solution to allow SQL Server to bootstrap itself as the container starts]\nPREPARE FILES\n(1) Create a local Windows folder, in my case,\nC:\\temp\\docker\\\n(2) Add \"Dockerfile\" (note no file extension) with the following content.\nFROM microsoft/mssql-server-linux:latest\nCOPY ./custom /tmp\nRUN chmod +x /tmp/entrypoint.sh\nCMD /bin/bash /tmp/entrypoint.sh\n(3) Add a subfolder \"custom\"\nC:\\temp\\docker\\custom\\\n(4) Add 2 files to \"custom\" subfolder as follows.\nentrypoint.sh\n#!/bin/bash\n\nset -e\nrun_cmd=\"/opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P HdkPKD57%@6876^ -l 30 -d master -i /tmp/createdb.sql\"\n\n>&2 echo \"Allowing 30 seconds for SQL Server to bootstrap, then creating database..\"\nuntil $run_cmd & /opt/mssql/bin/sqlservr; do\n>&2 echo \"This should not be executing!\"\ndone\ncreatedb.sql\nCREATE DATABASE DemoData;\nGO\nUSE DemoData;\nGO\nCREATE TABLE Products (ID int, ProductName nvarchar(max));\nGO\nRUN DOCKER COMMANDS\n(A) Open PowerShell and cd to folder in step (1) above, in my case\ncd C:\\temp\\docker\n(B) Run docker build command\ndocker build .\n(C) After image is built, get first 3 characters of the image, in my case 086\ndocker images\n(D) Create a container using the correct image id and correct password\ndocker run -d -e 'SA_PASSWORD=HdkPKD57%@6876^' -e 'ACCEPT_EULA=Y' -p 1433:1433 086\n(E) Check your container is running\ndocker ps -a\nThis container does not exit! Intended database \"DemoData\" is created. Problem solved!\ndocker logs 2aa command (2aa is my container id, yours will be different) shows clean build, no errors or warnings. The logs begin as follows\nAllowing 30 seconds for SQL Server to bootstrap, then creating database..\nThis is an evaluation version.  There are [173] days left in the evaluation period.\n2017-05-21 17:39:50.69 Server      Setup step is copying system data file 'C:\\templatedata\\master.mdf' to '/var/opt/mssql/data/master.mdf'.\n....\nand end as follows.\n2017-05-21 17:39:54.20 spid51      Starting up database 'DemoData'.\n2017-05-21 17:39:54.43 spid51      Parallel redo is started for database 'DemoData' with worker pool size [1].\n2017-05-21 17:39:54.44 spid51      Parallel redo is shutdown for database 'DemoData' with worker pool size [1].\n2017-05-21 17:39:54.51 spid7s      Recovery is complete. This is an informational message only. No user action is required.\nChanged database context to 'DemoData'.\nConnect with SSMS\nI was able to successfully connect to this database using SSMS as shown below (IP address 10.0.75.1 is that of the docker host that contains the container)\nImportant Notes\nsqlcmd SA Password\nsqlcmd is the utility being used to run dbcreate.SQL and create the database DemoData. The utility uses ODBC driver on Linux and is sensitive to how you can specify values for switches, especially the password -P.\nTo avoid login related issues found here and explained here, choose your strong password carefully and specify it in entrypoint.sh under -P without wrapping with double quotes or single quotes or []. See (4) above.\nFurther, this password has to match the docker run environment variable you are passing to the container. See (D) above to avoid password mismatch.\nDetailed documentation on sqlcmd is here.\nsqlcmd Login Timeout\nNote how I used the -l switch to specify login timeout of 30 seconds for sqlcmd in entrypoint.sh file. This is the crux of my solution to avoid the CMD from returning (which makes the container to exit). This timeout is sufficiently long enough for SQL server to start.",
    "Express does not run on docker, but on localhost": "After you run docker build, when you run docker run you need to implement port forwarding by using the -p tag.\nIn your case, you would run\ndocker run -p 5000:9080 <image id>\nNow when you go to http://localhost:5000 you should see your app running.",
    "Docker-Compose: how to mapping volume from host into container?": "Your compose config sets up a named data volume called dbvolume\nA named volume will be stored in /var/lib/docker by default:\n\u2192 docker volume inspect dbvolume\n[\n    {\n        \"Driver\": \"local\",\n        \"Labels\": null,\n        \"Mountpoint\": \"/var/lib/docker/volumes/dbvolume/_data\",\n        \"Name\": \"dbvolume\",\n        \"Options\": {},\n        \"Scope\": \"local\"\n    }\n]\nMounting a host directory as a volume is slightly different in a compose file.\n    volumes:\n      - /var/lib/mongodb:/var/lib/mongodb\nThe source is still on the left, but you specify the full path instead of a name.",
    "Installing Java on docker in docker image": "There's a Dockerfile reference on how official Java image is baking on top of alpine image: https://github.com/docker-library/openjdk/blob/9a0822673dffd3e5ba66f18a8547aa60faed6d08/8-jdk/alpine/Dockerfile\nOr you could do it another way around,\n# build ontop of official Java image\nFROM java:openjdk-8-jdk-alpine\n\nRUN apk update && \\\n    apk add docker\n...",
    "How to handle differences in Dockerfile for dev/prod": "You may be surprised but three Dockerfiles looks better!\nFirst Dockerfile contains common things for dev and production environment, This may named a \"my-app-base\";\nSecond - dev Dockerfile based on \"First\" image and contains only specific things for dev env;\nThird - production Dockerfile based on \"First\" image and contains only specific things for production env.\nLet me show you on the example of php project :\nBase dockerfile\nWe create base image for our project. Dockerfile.base will look like\n# Set the base image\nFROM php:5.6-apache\n\n# install php dependencies\nRUN apt-get update && apt-get install -y \\\n        php5-pgsql \\\n        postgresql-client \\\n        php5-mongo \\\n        libxml2-dev \\\n        php-soap \\\n        libpq-dev \\\n        libmcrypt-dev \\\n        php-pear \\\n    && docker-php-ext-install pdo \\\n    && docker-php-ext-install pgsql \\\n    && docker-php-ext-install pdo_pgsql \\\n    && docker-php-ext-install soap \\\n    && docker-php-ext-install pcntl \\\n    && docker-php-ext-install mcrypt\n\n# preconfiguring apache\nRUN a2enmod rewrite && a2enmod headers && a2enmod proxy && a2enmod proxy_http\nRUN echo \"export DISPLAY=':0'\" >> /etc/apache2/envvars\nVOLUME /var/log/apache2\nEXPOSE 80\n\n# configure envarionments\nRUN echo \"date.timezone=Europe/Moscow\" > $PHP_INI_DIR/conf.d/date_timezone.ini\n\n...settings env, configuring apache, etc...\nand image will named my_company/php5.6:base\nProduction Dockerfile\nIn production i want to have container with source code inside, then my Dockerfile.prod will look like:\n# Set the base image\nFROM my_company/php5.6:base\n\n# begin instalation\n# copy src\nADD . /src\nDev Dockerfile\nIn dev env i want to have ability to edit source code outside of container, then my Dockerfile.dev will look like:\n# Set the base image\nFROM my_company/php5.6:base\n\nRUN docker-php-ext-install xdebug\n\nVOLUME /src",
    "Django docker container failed to connect to mysql container with error \"Can't connect to MySQL server on 'db' (111)\")": "If Django tries to connect to your database and MySql is not ready, then you will probably see this error:\ndjango.db.utils.OperationalError: (2003, \"Can't connect to MySQL server on 'db' (111)\")\nThe solution for this case is wait for the db server to be ready before run the web server.\nHow we can do that?\nInstead of running the web server directly we create and run a script that it will wait for the db server to be ready. After that the script will run the web server.\ndocker-compose.yml file (The relevant part is when we overwrite the web:command directive and we run the script instead of run the web server immediately).\nversion: '3'\nservices:    \n  db:\n    restart: always\n    image: mysql:5.7\n    container_name: my_db\n    env_file: env_db\n    ports:\n      - \"3306:3306\"\n\n  web:\n    restart: always\n    build: ./web\n    env_file: env_web\n    command: ./wait_for_db_and_start_server.sh\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - db\nNow env_web file (The relevant part is START_CMD variable).\n# Add Environment Variables\nDB_NAME=docker\nDB_USER=docker_user\nDB_PASS=docker_password\nDB_SERVICE=db\nDB_HOST=db\nDB_PORT=3306\nSTART_CMD=python manage.py runserver 0.0.0.0:8000\nNow, our web Dockerfile (The relevant part is when we add wait_for_db_and_start_server.sh).\nFROM python:2.7\n\nADD wait_for_db_and_start_server.sh .\n\nADD requirements.txt .\nRUN pip install -r requirements.txt\n\nADD . .\nNow wait_for_db_and_start_server.sh file. (Basically, while we cannot connect to our db server on the specified $DB_HOST:$DB_PORT defined in our env_web file, we wait 3 seconds and try again for M_LOOPS times. If we are able to connect to the server, then we run $START_CMD that was defined in our env_web file.)\n#!/bin/sh\n# Wait for database to get available\n\nM_LOOPS=\"10\"\n\n#wait for mysql\ni=0\n# http://stackoverflow.com/a/19956266/4848859\nwhile ! curl $DB_HOST:$DB_PORT >/dev/null 2>&1 < /dev/null; do\n  i=`expr $i + 1`\n\n  if [ $i -ge $M_LOOPS ]; then\n    echo \"$(date) - ${DB_HOST}:${DB_PORT} still not reachable, giving up\"\n    exit 1\n  fi\n\n  echo \"$(date) - waiting for ${DB_HOST}:${DB_PORT}...\"\n  sleep 3\ndone\n\necho \"$(date) - ${DB_HOST}:${DB_PORT} Reachable ! - Starting Daemon\"\n#start the daemon\nexec $START_CMD",
    "docker: freeze library dependencies": "Based on How to tweak \"dpkg -l\" output - Ask Ubuntu:\ndpkg-query --show -f='${binary:Package}=${Version} \\\\\\n' build-essential \\\n     ca-certificates \\\n     git \\\n     fonts-liberation \\\n     libfreeimage3 \\\n     imagemagick \\\n     python \\\n     python-numpy \\\n     python-pip \\\n     curl \\\n     xsltproc \\\n     xz-utils\nRun this command in a docker container based on this Dockerfile to get the text to copy into your Dockerfile.",
    "Dockerfile COPY takes a stale copy of the file?": "RUN executes command within the container, so sed is applied to the file ./dir/file.yaml that is in it. You probably have a/the same file in your WORKDIR/dir/file.yaml (WORKDIR), that explains why the second option works.\nThe COPY in your first version overrides the /usr/src/app/node_modules/serve-swagger-editor/node_modules/swagger-editor/spec-files/ file with the ./dir/file.yaml that is in your host build directory. This one has not been affected by the sed command before, as it is outside the container.\nSo what you can do is COPY the file first inside the container, and then use your RUN command to modify it. Or modify it before running docker build.",
    "docker-compose image export and import": "My question is if i commit the newly created image to another image and export the container and import to another environment, during execution of docker run command,will it start both the nginx and redis service?\nAll you need is change the container names and port mapping declared in the docker-compose.yml, and you could launch as many instances of those images as you want (no need to export/import)",
    "dockerfile is not creating directory and copying files?": "I tried something similar and got\nStep 4 : RUN mkdir ~/.m2\n ---> Running in 9216915b2463\nmkdir: cannot create directory '/home/jenkins/.m2': No such file or directory\nyour useradd is not enough to create /home/jenkins\nI do for my user gg\nRUN useradd -d /home/gg -m -s /bin/bash gg\nRUN echo gg:gg | chpasswd\nRUN echo 'gg ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers.d/gg\nRUN chmod 0440 /etc/sudoers.d/gg\nUSER gg\nENV HOME /home/gg\nWORKDIR /home/gg\nThis creates the directory of the user gg `",
    "Docker plugin : java.io.FileNotFoundException: /src/main/docker/Dockerfile gradle-docker fails": "Remove the '/' before src in the path. The path is relative from the current directory (project root) and not the volume root.",
    "Docker mount namespace to share between containers": "This is called a data colume container: see \"Creating and mounting a data volume container\"\n$ docker create -v /dbdata --name dbdata training/postgres /bin/true\n$ docker run -d --volumes-from dbdata --name db1 training/postgres\n$ docker run -d --volumes-from dbdata --name db2 training/postgres\nHere, even if the container db1 exits, db2 has still access to the shared voume dbdata.\ndbdata is a data volume container that you don't \"run\" (there is no process running in it, only a shared volume of data); you \"create\" it only. (and you don't \"exit\" it either, since you never ran it)",
    "how do I manage specific versions of builds with Dockerfile": "Although it is cumbersome, I would go with option #2 (multiple directories) or with an extended version of this using git branches instead of directories.\nThe guys at docker library (creating/maintaining base image like debian, wordpress, gcc, ...) use the approach with separate directories (see for example GCC dockerfile on GitHub). This way you can integrate nicely with Docker Hub (e.g. automated builds). Another option is to use different branches, if you are using git. So you would have your repository myimage and then branches for 1.0, 1.2, 1.4 and so on. Now you would also be able to label your patches. Again Automated builds from Docker Hub can be used with this approach.\nI would say that from a software engineering / release management point of view the branch-based option is the cleanest solution. You can build the different versions of your application from the dedicated branch and then ship it via the proper version of the Dockerfile. This also means you can merge changes from one branch to the other and have it properly documented in the history.\nBy the way, it seems like best practice to have your Dockerfile in the root folder of your application's source code.",
    "How to containerize a single node app from a mono-repo that utilizes npm workspaces": "Your approach is correct of my knowledge. You can improve the way of excluding the unwanted files can be optimized like below. This will ensure that only required dependencies are being added to the final docker file with npm and docker's multi-stage build mechanism.\nRefer to Docker file in the code below :\nFROM node:20.5.1-alpine3.18 AS builder\nRUN apk update && apk --no-cache add build-base python3\nCOPY . /mono-repo\nWORKDIR /mono-repo\nARG NODE_APP_DIR\nRUN npm ci --cache .npm --prefer-offline -w \"$NODE_APP_DIR\"\nRUN mkdir /app && cp -R \"$NODE_APP_DIR\" /app && cd /app && npm prune --production\nFROM node:20.5.1-alpine3.18\nCOPY --from=builder /app /app\nWORKDIR /app\nARG NODE_APP_DIR\nCMD npm start --workspace \"$NODE_APP_DIR\"\nAbove will copy the entire mono repo, and use npm ci to install dependencies after that copy files to the new directory /app from there onwards copy relevant app code to the created directory. Finally running npm prune --production inside the /app directory will remove unnecessary node_modules.\nThen copy only the pruned /app directory from the builder to stage to the final image.\nSet the working directory to /app and make sure to set the NODE_APP_DIR in your case it probably is apps/app2.\nThen you can build your docker image for app2 by using the following command:\ndocker build --build-arg NODE_APP_DIR=\"apps/app2\" -t app2:test .\nHope this helps. Don't hesitate to ask if there is any question.",
    "Shared code library between Docker containers": "The important detail here is that a Dockerfile can never COPY anything into an image from a parent or sibling directory. This means you need to build an image starting from the nearest ancestor directory that contains everything you need to include; in your case, the root of the source tree.\n# foo/Dockerfile\nFROM something\nCOPY common/ /app/common/\nCOPY foo/ /app/foo/\nWORKDIR /app/foo\n...\n# in a shell, at the root of the source tree\ndocker build -f foo/Dockerfile .\nNote here that the left-hand side of COPY is relative to the path argument to docker build, the . at the very end, and not to the location of the Dockerfile.\nOnce you've done this, inside the image, from the /app/foo directory, there will be a ../common directory reachable as a relative path, and if you COPY a symlink into the image or RUN ln -s it will work the way you expect.\nFrom your description you make it sound like the common library code isn't that large or that complex. This approach will give each image a separate copy of it, but for a small and simple library that shouldn't be a practical problem. If the applications themselves are small you could just build one image containing everything together and run it with different commands; this may or may not be practical.\nAlso note that nothing here involves Git or any other source-control system; it is only based on the files that actually exist on the host system. You can manage these in whatever way you want to.",
    "Build .deb package in one Docker build stage, install in another stage, but without the .deb package itself taking up space in the final image?": "You can mount the build layer just for the dpkg command:\nFROM debian:buster AS build_layer\nCOPY src/ /src/\nWORKDIR /src\nRUN ./build_deb.sh\n\nFROM debian:buster AS app_layer\nRUN --mount=type=bind,from=build_layer,target=/mnt dpkg -i /mnt/src/myapp.deb\nENTRYPOINT [\"/usr/bin/myapp\"]",
    "Dockerize my gradle java project but encounter jdk version mismatch between compile time and runtime": "From oracle documentation\nJava SE | Released | Major | Supported majors\n1.0.2 | May 1996 | 45 | 45\n1.1 | February 1997 | 45 | 45\n1.2 | December 1998 | 46 | 45 .. 46\n1.3 | May 2000 | 47 | 45 .. 47\n1.4 | February 2002 | 48 | 45 .. 48\n5.0 | September 2004 | 49 | 45 .. 49\n6 | December 2006 | 50 | 45 .. 50\n7 | July 2011 | 51 | 45 .. 51\n8 | March 2014 | 52 | 45 .. 52\n9 | September 2017 | 53 | 45 .. 53\n10 | March 2018 | 54 | 45 .. 54\n11 | September 2018 | 55 | 45 .. 55\n12 | March 2019 | 56 | 45 .. 56\n13 | September 2019 | 57 | 45 .. 57\n14 | March 2020 | 58 | 45 .. 58\n15 | September 2020 | 59 | 45 .. 59\n16 | March 2021 | 60 | 45 .. 60\n17 | September 2021 | 61 | 45 .. 61\n18 | March 2022 | 62 | 45 .. 62\nYour error says\nhas been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 52.0\nYou have compiled with JAVA 17 and try to run it with JAVA 8. Switch either the compilation or the environment that runs the compiled application according to the above table.\nYour JRE 8 that exists in docker supports any code that is compiled with JDK 8 or earlier versions (1 .. 5, 6, 7) .\nIf you wish to compile the project with JDK 17 you need to provide a running environment in docker >= JDK 17.\nEdit:\nAnswering the comment\nI also wonder what is the image I should use to run the code that is compiled by JDK 17\ntry with\nFROM openjdk:17-alpine as my-app ...",
    "Dockerize flutter web project": "So here is the thing. I think there are some problems with his code. As I tried to build with his code yesterday but things weren't working as they should be. I spend 2hours with his code of which 50% was spent on building the image but it was never completed. No problem I tried to modify some things and I am sharing some code with you on GitHub https://github.com/ash-0001/fludino.git. Just keep learning and sharing. Good Is Image.\nThe final image looks like this:\nYou have to use two commands to initialize this: Before that move into the directory with your cmd\ndocker build -t flut .\ndocker run -i -p 808:4040 -td flut",
    "Why are Python project files copied after installing requirements in dockerfile and not before? [duplicate]": "Docker checks every ADD and COPY statement to see if any files have changed and invalidate the cache for it and every later step if it has.\nSo, in the later, after changes in your code, all requirements will be reinstalled.",
    "Run Ubuntu Docker container as User with sudo access": "I don't understand why this doesn't work:\nFROM continuumio/miniconda3\n# FROM --platform=linux/amd64 continuumio/miniconda3\n\nMAINTAINER Brando Miranda \"brandojazz@gmail.com\"\n\nRUN apt-get update \\\n  && apt-get install -y --no-install-recommends \\\n    ssh \\\n    git \\\n    m4 \\\n    libgmp-dev \\\n    opam \\\n    wget \\\n    ca-certificates \\\n    rsync \\\n    strace \\\n    gcc \\\n    rlwrap \\\n    sudo\n\n# https://github.com/giampaolo/psutil/pull/2103\n\nRUN useradd -m bot\n# format for chpasswd user_name:password\nRUN echo \"bot:bot\" | chpasswd\nRUN adduser bot sudo\n\nWORKDIR /home/bot\nUSER bot\n\n# CMD /bin/bash",
    "Asp.net 6 Multi Application Hosting in Docker": "I guess you need to change your compose file.\nchanges like.\nversion: '3.4'\n\nnetwork:\n  account_network:\n  external: true\n  driver: bridge\n\nservices:\n  wcaccountapi:\n    container_name:wcaccountapi\n    hostname:wcaccountapi\n    image: ${DOCKER_REGISTRY-}wcaccountapi\n    build:\n      context: .\n      dockerfile: wc.account.api/Dockerfile\n\n  wcaccount:\n    container_name:wcaccount\n    hostname:wcaccount\n    image: ${DOCKER_REGISTRY-}wcaccount\n    build:\n      context: .\n      dockerfile: wc.account/Dockerfile\nplease check your docker file if it is wrong please update it.\nnext, you need to check your appsettings.json file\n{\n  \"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Information\",\n      \"Microsoft.AspNetCore\": \"Warning\"\n    }\n  },\n  \"grpcUrl\": {\n    \"accountUrl\": \"https://wcaccount/\"\n  },\n  \"Authentication\": {\n    \"Key\": \"jaN8yp&bsffsL5Qnn6L7qyp&bsff8v3uyp&bsffsL5Qnn6L7\",\n    \"Issuer\": \"https://localhost:46641/\",\n    \"Audiance\": \"https://localhost:46641/\",\n    \"AccessExpireMinutes\": \"40320\"\n  }\n}\nThanks in Advance :)",
    "How to set proxy inside docker container using powershell": "The error is because in your start script of docker container, your syntax cannot be executed by plain sh, you should use bash instead.\nI have re-produced with a simple example.\ncat sh_bash.sh\nwinner=bash_or_sh\nif [[  ( $winner == \"bash_or_sh\" ) ]]\n   then\n      echo \" bash is winner\"\n    else\n      echo \"sh is looser\"\nfi\n$ sh sh_bash.sh\nsh_bash.sh: 2: Syntax error: word unexpected (expecting \")\")\n$ bash sh_bash.sh\n bash is winner\nSo, try docker container exec -it microsofttranslator /bin/bash \nShould you need to pass proxy env variables , please read this",
    "Git repo build URL with Docker compose": "Build Docker Image from git repository URL with Docker compose file\ndocker-compose.yml\nversion: \"3.9\"\nservices:\n  ordersvc:\n    build:\n      context: https://github.com/Omkeshs/e-com.git #git repo link\n      dockerfile: ./order/Dockerfile #Dockerfile path in repo\n    ports:\n      - 8080:8080\n\n  productsvc:\n    build:\n      context: https://github.com/Omkeshs/e-com.git\n      dockerfile: ./product/Dockerfile\n    ports:\n      - 8000:8000\nIn my case, I have Created micro-service based architecture(Order-Product services), In the same git repo. both docker images create from a same git repo.\nref - https://github.com/Omkeshs/e-com",
    "Maria DB docker Access denied for user 'root'@'localhost'": "Update\nBased on the update to your question, you're trying to run the mysqladmin ping command inside the container. mysqladmin is attempting to connect as the root user, but authenticating to your database server requires a password.\nYou can provide a password to mysqladmin by:\nUsing the -p command line option\nUsing the MYSQL_PWD environment variable\nCreating a credentials file\nIf we move the root password out of your image, and instead set it at runtime, we can write your docker-compose.yml file like this:\nversion: \"3\"\n\nservices:\n  mariadb:\n    restart: always\n    image: mariadb_image\n    container_name: mariadb_container\n    build: topcat_mariadb/.\n    environment:\n      - \"MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD\"\n      - \"MYSQL_PWD=$MYSQL_ROOT_PASSWORD\"\n    healthcheck:\n      test: [\"CMD-SHELL\", 'mysqladmin ping']\n      interval: 10s\n      timeout: 2s\n      retries: 10\nAnd then in our .env file we can set:\nMYSQL_ROOT_PASSWORD=pw1\nNow after the container starts up we see that the container is healthy:\n$ docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS                    PORTS      NAMES\nc1c9c9f787e6   mariadb_image    \"docker-entrypoint.s\u2026\"   28 seconds ago   Up 27 seconds (healthy)   3306/tcp   mariadb_container\nAs a side note, it's not clear from this example why you're bothering to build a custom image: it's better to set the environment variables at runtime, rather than creating an image with \"baked-in\" credentials.\nPrevious answer\nI can't reproduce your problem when using the mysql client or Python code. Given the following docker-compose.yml:\nversion: \"3\"\n\nservices:\n  mariadb:\n    restart: always\n    image: mariadb_image\n    container_name: mariadb_container\n    build: topcat_mariadb/.\n\n  shell:\n    image: mariadb:10.6.4\n    command: sleep inf\n(The directory topcat_mariadb contains the Dockerfile from your question.)\nIf I exec into the shell container:\ndocker-compose exec shell bash\nAnd run mysql like this:\nmysql -h mariadb_container -u user1 -p db1\nIt works just fine:\nroot@4fad8e8435df:/# mysql -h mariadb_container -u user1 -p db1\nEnter password:\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 6\nServer version: 10.6.4-MariaDB-1:10.6.4+maria~focal mariadb.org binary distribution\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [db1]>\nIt looks like you may be using sqlalchemy. If I add a Python container to the mix:\nversion: \"3\"\n\nservices:\n  mariadb:\n    restart: always\n    image: mariadb_image\n    container_name: mariadb_container\n    build: topcat_mariadb/.\n\n  shell:\n    image: mariadb:10.6.4\n    command: sleep inf\n\n  python:\n    image: python:3.9\n    command: sleep inf\nAnd then run the following Python code in the python container:\n>>> import sqlalchemy\n>>> e = sqlalchemy.engine.create_engine('mysql+pymysql://user1:user1pw@mariadb_container:3306/db1')\n>>> res = e.execute('select 1')\n>>> res.fetchall()\n[(1,)]\nIt also seems to work without a problem.",
    "Visual Studio Code not displaying docker files after attaching to a container": "It was a stupid wsl issue.\nRunning wsl --shutdown and wsl afterwards fixed the issue for me",
    "Docker build: returned a non-zero code: 1, when test is failed": "It may be that your tests are failing on an assertion and that failed assertion may be throwing the non 0 error code.\nthis link outlines the expected exit codes for each scenario\nExit code 0\nAll tests were collected and passed successfully\n\nExit code 1\nTests were collected and run but some of the tests failed\n\nExit code 2\nTest execution was interrupted by the user\n\nExit code 3\nInternal error happened while executing tests\n\nExit code 4\npytest command line usage error\n\nExit code 5\nNo tests were collected",
    "Install apt packages in docker via build step and copy": "I was also after this answer, as this helps complex multistage builds better leverage inline cache (which we use in our CI). We want to always install apt packages before any step that can have its cache affected by a previous stage in the build.\nOne solution is brute force. In you Dockerfile you could do:\nFROM debian:buster as my-image \n<build the image>\n\nFROM debian:buster as my-other-image\nCOPY --from=my-image / /\nas, from the COPY docs\nThe COPY instruction copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest>.\nemphasis on new\nanother solution is using dpkg-query --listfiles e.g. after building my image, I ran the following inside the image:\ndpkg --get-selections | awk '{print $1}' | xargs dpkg --listfiles | cut -d \"/\" -f2 | sort | uniq\nwhich gave something like:\nbin\nboot\ndev\n...\nfor each of these you could have a statement under your latter build stages like:\nCOPY --from=my-image /bin /bin\nCOPY --from=my-image /boot /boot\n...\nBoth solutions seem to work fine in my testing, but for some reason the\nCOPY --from=my-image / /\nsolution is really slow on my local machine >10 minutes but fast on our CI <30s.",
    "Dask Cluster: AttributeError: 'DataFrame' object has no attribute '_data'": "My guess is that you have a version mismatch somewhere. What does client.get_versions(check=True) say?",
    "docker-entrypoint exec nginx not found": "There is already an nginx image with rtmp module installed:\ndocker pull tiangolo/nginx-rtmp\nHow to use:\nhttps://hub.docker.com/r/tiangolo/nginx-rtmp/\nNow for your problem, if you go to the official nginx docker image repository in https://github.com/nginxinc/docker-nginx, line 38 is:\nexec \"$@\"\nSo what does that do?\nMore information here:\nhttps://unix.stackexchange.com/questions/466999/what-does-exec-do",
    "How to network between Docker images at build time?": "In the build-time, there is no network configuration comes up. Because Docker container is like VM. you can't connect to VM in its boot time via it's IP address. docker container also the same. The only differents is container fast booting process than the VM; therefore, you need to wait until it comes up. After booting up a container, it's life cycle is the runtime.",
    "How to password protect a docker container?": "There's no way to do this. Docker containers generally don't have \"users\"; to the extent that they do, they almost never have passwords set; and you don't \"log in\" to them, you just run a command.\nFor example, a typical command to get a debugging shell\ndocker run --rm -u root --entrypoint /bin/sh -it imagename\ndirectly runs the interactive shell, as root, with no checks. Docker jumps straight into the sh process; there is nothing before it that would have the possibility of checking a password.\nIf you're trying to use this as a way to hide the image's contents, you can't. Anyone who can run any docker command at all can access any file in any container, and can pretty much trivially root the entire host system.",
    "Command not found in container": "When you install software in your image, you try to set an alternate path in .bashrc, but most common paths of running a Docker image never read those dotfiles. The JSON-array form of ENTRYPOINT doesn't involve a shell at all, so nothing will ever read what you write out to the .bashrc or .bash_profile.\nThe source directives also have no effect: you're running it from a script so the environment variables that get set \"expire\" at the end of the bcp-build.sh script, and in any case a Dockerfile RUN directive doesn't notice or preserve changes in the environment.\nIf you need to modify environment variables in your image, you need to use the Dockerfile ENV directive. (The only other thing that could change the container's environment is a script that runs at container startup time.)\nENV PATH $PATH:/opt/mssql-tools/bin\nIn general Debian packages are required to install their software in the normal \"system\" directories, so you might file a bug that this support tool isn't being installed into /usr/bin like a normal binary would.",
    "Docker image not running with https and tomcat": "You need to expose port 8445 in Dockerfile using EXPOSE 8445 and run the container using -p 8445:8445 to map local port 8445 to port 8445 running inside the container.\nYour logs don't show that tomcat is aware that it needs to prepare a connector to run on port 8445. It must be using default server.xml configuration. You need to provide your server.xml to the container using something like following.\nCOPY server.xml /usr/local/tomcat/conf/",
    "Docker-compose stop removes the data from tables in the database": "The docker-compose.yml looks good to me:\nYou set up a persistent volume which is stored by default at /var/lib/docker/volumes/<project>_postgres_data\nby default <project> is the name of the folder the docker-compose.yml resides\nThe volume will survive (I verified) a docker-compose down or docker-compose restart. See https://docs.docker.com/compose/reference/restart/\nNot commiting database transaction\nMaybe the python program opens a database transaction, insert data to the table(s), but does not commit the transaction therafter. See more about transaction management, auto-commit mode, rollback etc.\nWould you mind to provide more information how you verified that your data really was in the tables?\nNamed volume interferences (MacOS)\nMaybe the named volume interferes with the anonymous volume created by the postgres image you are using. Please modify your docker-compose.yaml named volume and environment:\nversion: '3'\n\nservices:\n    app:\n        build: .\n        command: python app/manage.py runserver 0.0.0.0:8000\n        volumes:\n            - ./:/app/\n        ports:\n            - 8000:8000\n        env_file:\n            - ./.env.dev\n        depends_on:\n            - db\n    db:\n        image: postgres\n        volumes:\n            - postgres_data:/var/postgres_data  #<----- modify\n        environment:\n            - PGDATA: /var/postgres_data        #<----- add\n            - POSTGRES_USER=cmpt_user\n            - POSTGRES_PASSWORD=password\n            - POSTGRES_DB=cmpt_project \nvolumes:\n    postgres_data:\nSource: https://github.com/docker/compose/issues/5012",
    "Kong - custom plugins returning: custom-plugin plugin is enabled but not installed;": "Check these resources out:\nhttps://github.com/luarocks/luarocks/wiki/installation-instructions-for-unix\nhttps://luarocks.org/\nhttps://luarocks.org/modules/seifchen/kong-path-whitelist\nhttps://github.com/Kong/kong/issues/4696\nBug Report from Kong: https://github.com/Kong/kong/issues/4696\nI don't think it's the luarocks' problem\nIndeed, I installed kong in docker whose image is built by a dockerfile.\nIn my docker file, I went into the folder which store the custom plugins,and then traverse and luarocks make them by shell.It looks like:\n\n#install private plugins\ncd APIGPlugins\n\nfor dir in `ls`; do\n    if [ -d $dir ]; then\n      cd $dir;\n      luarocks make;\n      cd ../;\n    fi\ndone\nand then, I run the docker images for a container by the directive:\n\nsudo docker run -d --name kong \\\n    -e \"KONG_DATABASE=off\" \\\n    -e \"KONG_DECLARATIVE_CONFIG=/etc/kong/kong.yml\" \\\n    -e \"KONG_PLUGINS=apig-response-transform\" \\\n    -e \"KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl\" \\\n    -p 8000:8000 \\\n    -p 8443:8443 \\\n    -p 8001:8001 \\\n    -p 8444:8444 \\\n    kong-plugin:v4\nAs u see, I set the kong plugin by the docker run env variable parameter for enable the plugin instead of setting in the kong.conf.\nThe logs were generated by directive of docker logs \"container ID\"\nIt works when I tried to install another custom plugin in this way,but not work when install the custom plugin I described before\nUpdate\nyou need to install the lua-package manager luarocks\nDocumentaton Link\nDownload Link",
    "docker run not working: no container created despite making container image": "Place the 'shebang' -- #!/bin/sh -- on the first line of the boot.sh shell script.\nHow I found this answer: This blog post which refers to this Stackoverflow post.\nThe problem: the original script has a comment on the first line and the shebang on the second line.\nNote: The title of the 'Question' is misleading: a docker container was built. The container, however, was short-lived and given I used the -rm option in the docker run command, the container was deleted after it terminated within 2 seconds; this is why it didn't appear in the docker images -a command.",
    "Build of Dockerfile hangs or taking long?": "The reason my docker build was taking much more time than executing the commands on the host directly is because after the RUN that executes the bitbake command, I am not cleaning up my layer!\nAs you may know, each RUN command is a layer, and each layer is auto-cached by docker to make use of later if no changes occurred. That means docker has to tar, remove and cache my layer including all the stuff that was cloned and all the outputs/artifacts of running bitbake. So rather than have docker struggle with all this unwanted stuff, I removed it manually:\nRUN . ./oe-init-build-env rpi-build && \\\n        cd conf && perl -0777 -pi -e 's/(BBLAYERS \\?= \"[^\"]*)\"/\\1\\/home\\/rpi-user\\/poky\\/meta-raspberrypi \\\\\\n  \"/g' bblayers.conf && \\\n        sed -i 's/\\(MACHINE ??= \\).*$/\\1\"raspberrypi3\"/g' local.conf && \\\n        cat /tmp/local.conf.append >> local.conf && \\\n        cd ../ && bitbake -DDDDvvv rpi-basic-image && mv tmp/deploy /home/rpi-user/ && rm -rf *\nNote the last two bash commands in this RUN (the mv and the rm).\nOne could also remove the clones.",
    "How to fix 'Permission denied' in Docker sh entrypoint": "The file isn't owned by steam in the container, so the chmod +x was insufficient. Either add --chown=steam to the ADD, or change your chmod from +x to a+rx.\nAlso, you didn't specify CWD or a path to put those files in. It's likely that the root version of that image has a CWD that steam can't access. You should use /home/steam/ for that instead.",
    "The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1": "Don't put each shell command in a separate RUN. It causes them to run in separate shells and environment variables from earlier shells don't affect later ones since every process has its own environment.\nRUN export PYTHONPATH=/usr/bin/python \\\n && pip install -r requirements.txt\nAlternatively you could use temporary assignment syntax to set the variable and call pip in one command:\nRUN PYTHONPATH=/usr/bin/python pip install -r requirements.txt\nSplit RUN statements create unnecessary extra layers which leads to overly large images. Standard practice in Dockerfiles is to merge related RUNs into one large command with backslashes. With apt-get, it's a good idea to also erase apt's cache files at the end so they don't become part of the layer.\nRUN apt-get update \\\n && apt-get install -y \\\n        swig \\\n        python-dev \\\n        libssl-dev \\\n && rm -rf /var/lib/apt/lists/*\nBy the way, FROM lines don't combine. When you write two of them the second one wins and the first is ignored.\nFROM ubuntu:16.04\nFROM python:2\nYou may think you need to pick an OS and add python on top, but that's not how Docker images work. The python:2 image already has an OS layer underneath. Remember, Docker images are self-contained bundles. When you pull in python:2 you get not just the Python interpreter but also all of its dependencies.\npython:2 has FROM buildpack-deps:stretch.\nbuildpack-deps:stretch has FROM buildpack-deps:stretch-scm.\nbuildpack-deps:stretch-scm has FROM buildpack-deps:stretch-curl.\nbuildpack-deps:stretch-curl has FROM debian:stretch.\nAh, there it is. Debian is the ultimate base image.",
    "Docker: provide custom context in python-sdk": "You just need to provide the dockerfile argument:\nclients.images.build(\n    path=\"../../../.\",\n    dockerfile=\"path_to_my_dockerfile/Dockerfile\",\n    tag=\"mytag\"\n)\nNote that the dockerfile should be relative to path, not your current working directory.\nIf you have dockerfile and context relative to your current working directory, you may be able to compute the relative path with something like this:\ndockerfile = pathlib.Path(os.path.normpath(dockerfile)).absolute()\ncontext = pathlib.Path(os.path.normpath(context)).absolute()\n\npath = dockerfile.relative_to(context)\nAlternatively, you can also provide a custom build context, by creating a tarfile yourself with custom_context. This is the most flexible way, as you don't even need a filesystem that way.",
    "Buiding docker containers for golang applications without calling go build": "You are correct that you can compile your application locally and simply copy the executable into a suitable docker image.\nHowever, there are benefits to compiling the application inside the docker build, particularly for larger projects with multiple collaborators. Specifically the following reasons come to mind:\nThere are no local dependencies (aside from docker) required to build the application source. Someone wouldn't even need to have go installed. This is especially valuable for projects in which multiple languages are in use. Consider someone who might want to edit an HTML template inside of a go project and see what that looked like in the container runtime..\nThe build environment (version of go, dependency managment, file paths...) is constant. Any external dependencies can be safely managed and maintained via the Dockerfile.",
    "Troubleshoot directory path error in COPY command in docker file": "a Dockerfile COPY command can only refer to files under the context - the current location of the Dockerfile, aka . so you have a few options now:\nif it is possible to copy the /home/ubuntu/authentication/workspace/ directory content to somewhere inside your project before the build (so now it will be included in your Dockerfile context and you can access it via COPY ./path/to/content /home/ubuntu) it can be great. but sometimes you dont want it.\ninstead of copying the directory, bind it to your container via a volume:\nwhen you run the container, add a -v option:\ndocker run [....] -v /home/ubuntu/authentication/workspace:/home/ubuntu [...]\nmind that a volume is designed so any change you made inside the container dir(/home/ubuntu) will affect the bound directory on your host side (/home/ubuntu/authentication/workspace) and vice versa.\ni found a something over here: this guy is forcing the Dockerfile to accept his context- he is sitting inside the /home/ubuntu/authentication/workspace/ directory, and running there\ndocker build . -f /path/to/Dockerfile\nso now inside his Dockerfile he can refer to /home/ubuntu/authentication/workspace as his context (.)",
    "Start the docker container under a different user": "Finally made it work. Added a powershell script that run \"net use\" first and the dotnet command after.\nUse cmd in the docker file to start the ps script.\nThanks for the answers",
    "Why is container exiting immediately after executing docker run command?": "I am running ubuntu os and it works (Let me know your setup if it still cannot work). The only problem with the code is that you cannot use the address 127.0.0.1 and must use 0.0.0.0 in your http-server.js. For more explanation refer to the link below\nhttps://forums.docker.com/t/network-param-for-connecting-to-127-0-0-1-in-container/2333",
    "Is possible to use local image into pods yaml in kubernetes?": "By local you mean it doesn't pull from dockerhub or any of the public registry. Yes it's possible if you run a single node kubernetes. You will utlize the docker cache where your kubernetes/kubelet is running.\nFirst thing is, you need to set your imagePullPolicy: IfNotPresent. Then, when you build your image, you need to point to the docker instance your kubernetes is using.\nI do this mostly with minikube, so the dev iteration is faster without pushing to my registry.",
    "Docker - error when extracting in Centos (invalid tar header)": "The error message indicates that the image you are attempting to download has been corrupted. There are a few places I can think of where that would happen:\nOn the remote registry server\nIn transit\nIn memory\nOn disk\nBy the application\nGiven the popularity of the image, I would rule out the registry server having issues. Potentially you have an unstable server with memory or disk issues that were triggered when downloading a large image. On Linux, you'd likely see kernel errors from this in dmesg.\nThe version of docker is recent enough that any past issues on this have long since been fixed. There's only a single issue on the tar file processing related to very large layers (over 8GB) which doesn't apply to the image you are pulling. The tar processing is embedded directly into docker, so changing or upgrading your tar binary won't affect docker.\nPotentially you could have an issue with the storage driver and the backend storage device. Changing from devicemapper to overlay2 if you haven't already would be a good first step if docker hasn't already defaulted to this (you can see your current storage driver in docker info and change it with an entry in /etc/docker/daemon.json).\nMy first guess on that list is the \"in transit\" part. Since the request will be over https, this won't be from a bad packet. But a proxy on the network that intercepts all web traffic could be the culprit. If you have a proxy, make sure docker is configured to login and use your proxy. For more details on that, see https://docs.docker.com/config/daemon/systemd/#httphttps-proxy",
    "Linux Hosting & .NET Core Docker : PlatformNotSupportedException: System.DirectoryServices.AccountManagement is not supported on this platform": "The namespace (System.DirectoryServices.AccountManagement) is not supported when we use docker container to host our application. But we can do the similar setup to validate user credentials on Active Directory with this nuget package https://github.com/dsbenghe/Novell.Directory.Ldap.NETStandard and its working with docker container hosting.\n// using Novell.Directory.Ldap;\n\ntry\n{\n    using (var connection = new LdapConnection())\n    {\n        connection.Connect(\"DomainUrl\", 389);\n        connection.Bind(\"username@domain\", \"password\");\n        if (connection.Bound)\n        {\n            return true;\n        }\n        else\n        {\n            return false;\n        }\n    }\n}\ncatch (LdapException ex)\n{\n    return false;\n}",
    "How to use multiline EOF in Dockerfile": "You could instead wrap your R command in a script (in which you can have multiple lines and EOF)\nYou would COPY your script first (from your local context to your image), then RUN it in your Dockerfile.",
    "Docker run command is not applied to the image": "This has to do with how Docker handles VOLUMEs for images.\nFrom docker inspect my/temp:\n\"Volumes\": {\n  \"/var/jenkins_home\": {}\n},\nThere's a helpful ticket about this from the moby project:\nhttps://github.com/moby/moby/issues/12779\nBasically you'll need to do your chmod at run time.\nSetting your HOME envvar to a non-volume path like /tmp shows the expected behavior:\n$ docker run -it --rm my/temp ls -la /tmp/.ssh\ntotal 8\ndrwx------ 2 jenkins jenkins 4096 May  3 17:31 .\ndrwxrwxrwt 6 root    root    4096 May  3 17:31 ..\n-rw------- 1 jenkins jenkins    0 May  3 17:24 dummy",
    "EOF error while pulling images from docker": "Here is my setting:\netc/systemd/system/docker.service.d\nmore http-proxy.conf\n[Service]\nEnvironment=\"HTTP_PROXY=http://user:password@proxy.mycompany.com:8080/\" \"HTTPS_PROXY=http://user:password@proxy.mycompany.com:8080/\"  \"NO_PROXY=localhost,127.0.0.1,.mycompany.com\"\nNote that HTTP_PROXY like HTTPS_PROXY both are using an http URL for the proxy. And make sure (with NO_PROXY) that any internal URL does not use the proxy.",
    "Do we need to clean-up in a multi-stage build?": "Cleaning resources from your \"helper\" is not mandatory, you will not have artifacts from previous stages in your final image.\nYou can see history and size of each step with docker history [OPTIONS] IMAGE\nHowever for saving space on your machine it's recommended, each stages are saved as a docker image. You can see images whith docker images command.",
    "Angular app show different UI between local & production environment": "had the same issue and solved it by adding --extract-css=false to my build command",
    "Using docker swarm to execute singular containers rather than \"services\"": "You can apply the ideas presented in \"One-shot containers on Docker Swarm\" from alex ellis.\nYou still neeeds to create a service, but with the right restart policy.\nFor instance, for a quick web server:\ndocker service create --restart-condition=none --name crawler1 -e url=http://blog.alexellis.io -d crawl_site alexellis2/href-counter\n(--restart-condition, not --restart-policy, as commented by ethergeist)\nSo by setting a restart condition of 0, the container will be scheduled somewhere in the swarm as a (task). The container will execute and then when ready - it will exit.\nIf the container fails to start for a valid reason then the restart policy will mean the application code never executes. It would also be ideal if we could immediately return the exit code (if non-zero) and the accompanying log output, too.\nFor the last part, use his tool: alexellis/jaas.\nRun your first one-shot container:\n# jaas -rm -image alexellis2/cows:latest\nThe -rm flag removes the Swarm service that was used to run your container.\nThe exit code from your container will also be available, you can check it with echo $?.",
    "Connect from docker container to host elasticsearch runing instance": "You need to remove the EXPOSE 9200 directive in the Dockerfile because port number 9200 is already taken by the elastic search service.\nYou should curl with the ip address of your host machine. Docker attaches containers to the bridge network to start with so you need a way to get the ip address of the host.\nYou may need to set an alias depending on whether or not your host is connected to a wider network. I set this alias for my bridge0 interface.\nsudo ifconfig bridge0 alias <ip_address>\nIf your host connected to a wider network, use the inet address of assigned to your ethernet device. You can get the inet address by running:\nifconfig en0 | grep \"inet \" | cut -d \" \" -f2\nYou can either way pass the inet address of your network interface as an environment variable to docker:\ndocker run -e MY_HOST_IP=$(ip_address) -it my-simple-image /bin/bash\n# or\ndocker run -e MY_HOST_IP=$(ifconfig en0 | grep \"inet \" | cut -d \" \" -f2) -it my-simple-image /bin/bash\n\ncurl $MY_HOST_IP:8000\nSee this thread for more information about your question",
    "How to run Arangodb on Openshift?": "With ArangoDB 3.4 the docker image has been migrated to an alpine based Image, and its core now shouldn't invoke CHOWN/CHRGRP anymore when invoked in the right way.\nThis should be one of the requirements to get it working on Openshift.\nIf you still have problems running ArangoDB on openshift, use the github issue tracker with the specific problems you see. You may also want to try to add changes to the dockerfile, so it can be improved.",
    "Docker build apt-get update failed to fetch with a 403": "I hit something similar: apt-get update would fail during 'docker build' though /etc/apt/apt.conf\nis set up in the host, and 'sudo apt-get update' works in the host.\nI fixed it as follows:\nStep 1: Copy /etc/apt/apt.conf to the local directory where Dockerfile resides (build context directory for docker).\nStep 2: Add the following lines to Dockerfile:\nRUN mkdir -p /etc/apt  \nCOPY apt.conf /etc/apt`\nRUN apt-get update && \\ \n  apt-get -y install`",
    "Passenger+Nginx+Docker app setup": "Hmmm... Try adding:\nports:\n  - 80:80\nto the app service.",
    "how to run any exe application on docker": "I think the error is in the ENTRYPOINT line. You use the path \"name.exe/bin\" instead of \"bin/name.exe\" which is where your COPY put the file.\nYou actually don't need the entrypoint if you use CMD as @helmbert said. I think the difference between ENTRYPOINT and CMD is that you can override a CMD command at the run if you want. So you could use \"docker run -i myimage powershell\". You can try without the entrypoint line and see.",
    "How to add $(\u2026):$(...) in Docker Compose": "The following doesn't work\nDOCKER_PATH=$(which docker) docker-compose up\n\nvolumes:\n   - '/var/run/docker.sock:/var/run/docker.sock'\n   - ${DOCKER_PATH}:/usr/bin/docker:ro\nBecause variable substitution needs to be inside double quotes (single quotes wont work), like this\n- \"${DOCKER_PATH}:/usr/bin/docker:ro\"\nYou can read more about it here: https://docs.docker.com/compose/compose-file/#variable-substitution",
    "Docker compose-build with rake assets:precompile": "Your question has shifted a bit, so I'm going to address your last sentence:\nSo I got it to work with just typing in the keys...but obviously that isn't a good solution long term to have the keys programmed in.\nIf you're sure you want to handle this during your build, as opposed to adding your rake task to your command entry, you can set build args in your docker-compose.yml config file.\n# compose.yml\nversion: '2'\nservices:\n  app:\n    # ...\n    build:\n      context: .\n      args:\n        # This will make your variables available during the\n        # \"build\" phase.\n        # You can hardcode these values here, or better, \n        # add them to a .env file, whose contents  \n        # Docker/Compose will make available during the build.\n        - AWS_ACCESS_KEY_ID\n        - AWS_SECRET_ACCESS_KEY\n        - DATABASE_URL\n        - SECRET_TOKEN\n    environment:\n      # You should also add these values to your application's \n      # environment.\n      # You can hardcode these values here, or better, \n      # add them to a .env file, whose contents  \n      # Docker/Compose will make available to your running container.\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n      - DATABASE_URL\n      - SECRET_TOKEN\nYou can then declare and use the build args in your Dockerfile:\n# Dockerfile\n# ...\nARG AWS_ACCESS_KEY_ID\nARG AWS_SECRET_ACCESS_KEY\nARG DATABASE_URL\nARG SECRET_TOKEN\n# these values will now be available to your rake task \n# in ENV['AWS_ACCESS_KEY_ID'], etc.\nRUN bundle exec rake RAILS_ENV=production assets:precompile",
    "Make docker or Dockerfile sense git branch": "You probably shouldn't be doing a git pull in the Dockerfile.\nFollowing Infrastructure as Code practices, it makes sense to include your Dockerfile in the same repository as your code, so changes in one that require changes in the other are bundled together in the same commit. If this is the case, your application's source code is available any time you're building the Docker image, and you can copy it from disk instead of pulling it from GitHub.\nUnfortunately, Docker's COPY directive doesn't allow navigating up the directory tree, even though symlinks or other tricks, so you'll need to manually create a copy of the repo inside the repo every time before you build the Docker image. That may look something like this:\n[~/projects/my-app $]> rm -r docker/repo\n[~/projects/my-app $]> cd ..\n[~/projects $]> cp -r my-app repo\n[~/projects $]> mv repo my-app/docker/\n[~/projects $]> cd my-app\n[~/projects $]> docker build docker\nYour Dockerfile then will contain a COPY repo/ /myrepo instruction to copy it into the container.\nI would recommend tagging your image with the git sha that you've copied into it.\nWhen doing development, to save yourself from having to rebuild all the time, you can simply use docker run's -v option to mount your local repo on top of the version baked into the image.",
    "How to escape CMD in Dockerfile": "I've just run into the same issue with starting a Java application inside the docker container when running it.\nFrom the docker reference you have three opportunities:\nCMD [\"executable\",\"param1\",\"param2\"]\nCMD [\"param1\",\"param2\"]\nCMD command param1 param2\nHave a look here: Docker CMD\nI'm not familiar with JavaScript, but assuming that the application you want to start is a Java application:\nCMD [\"/some path/jre64/bin/java\", \"server.jar\", \"start\", \"forever\", ...]\nAnd as the others in your comments say, you could also add the script via docker ADD or COPY in your Dockerfile and start it with docker RUN.\nYet another solution would be to run the docker container and mount a directory with the desired script via docker run .. -v HOSTDIR:CONTAINERDIR inside the container and trigger that script with docker exec.\nHave a read here: Docker Filemounting + Docker Exec",
    "postgres table does not exist from using docker-compose": "Are you sure your database init script is getting called? The postgresql docker uses files in /docker-entrypoint-initdb.d/* as you can see in the Dockerfile here: https://github.com/docker-library/postgres/blob/443c7947d548b1c607e06f7a75ca475de7ff3284/9.4/docker-entrypoint.sh#L77\nYou can test this by bringing up ONLY the database, by running something like\ndocker-compose up db\nwhich will ONLY bring up the db container, and then you can docker exec -it in and psql into the postgres processes, or use your exposed port on 5432\nAlso, you may want to pass in the initialization of the database, username, and password the way the Dockerfile expects them, as env variables as you can see in the same file\nEnvironment Variables The PostgreSQL image uses several environment variables which are easy to miss. While none of the variables are required, they may significantly aid you in using the image.\nPOSTGRES_PASSWORD This environment variable is recommended for you to use the PostgreSQL image. This environment variable sets the superuser password for PostgreSQL. The default superuser is defined by the POSTGRES_USER environment variable. In the above example, it is being set to \"mysecretpassword\".\nPOSTGRES_USER This optional environment variable is used in conjunction with POSTGRES_PASSWORD to set a user and its password. This variable will create the specified user with superuser power and a database with the same name. If it is not specified, then the default user of postgres will be used.\nPOSTGRES_DB This optional environment variable can be used to define a different name for the default database that is created when the image is first started. If it is not specified, then the value of POSTGRES_USER will be used.\nhttps://hub.docker.com/_/postgres/",
    "How to interpret this docker registry creation command?": "As mentioned in the official registry image\nThe tags >= 2 refer to the new registry.\nOlder tags refer to the deprecated registry.\nThe Deploying a registry server page uses the new registry server (with a simpler mapping):\ndocker run -d -p 5000:5000 --restart=always --name registry registry:2\nThis is for localhost access only, since accessing it from other hosts would necessitate ssl certificates.\ndocker run -d -p 5000:5000 --restart=always --name registry \\\n  -v `pwd`/certs:/certs \\\n  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\\n  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\\n  registry:2\nThe OP Zack adds in the comments:\nIt seems that it does not work for tags = 3\nThat is because the official page only list the following tags:\n2, 2.2, 2.2.0 (Dockerfile)\nThere is no tag 3.",
    "Keycloak is failing to start in docker container": "Check your KC_DB properties, if there is none in your keycloak env add and try\nKC_DB should be database vendor name and not your database name For Postgres db\n  KC_DB: postgres\nFor mariadb\n  KC_DB: mariadb",
    "Docker errors when trying to build in ARM64 Apple M1: \"Failed to resolve full path of the current executable [/proc/self/exe]\"": "This is not an answer but a followup to the problem, which might shed some more light on the issue.\nI'm using aspnet:6 and it passes the build.\nThe problem later on is that the runnable .dll throws\ncannot execute binary file: Exec format error\nwhich seems to be a platform issue, using Mac with an M1 Silicon chipset.\nI have no issue with other docker images that are made for linux/amd64\nThe Docker daemon seems to deal with this fine (having installed Rosetta2 and the Latest Docker Desktop for M1) https://docs.docker.com/desktop/mac/apple-silicon/\nHave you managed to resolve the issue and execute the dll?",
    "How do I install tesseract-ocr v4.1.1 in a docker image": "From @phil-o's comment:\nRUN export PATH=/user/local/bin:$PATH \n\nRUN apt-get update && \\ \n    apt-get install libleptonica-dev automake make pkg-config libsdl-pango-dev libicu-dev libcairo2-dev bc ffmpeg libsm6 libxext6 -y \n\nRUN wget github.com/tesseract-ocr/tesseract/archive/4.1.1.zip && \\\n    unzip 4.1.1.zip && \\\n    cd tesseract-4.1.1 && \\\n     ./autogen.sh && \\\n     ./configure && \\\n     make && \\\n     make install && \\\n     ldconfig && \\\n     make training && \\\n     make training-install && \\\n     tesseract --version\nthis works for me.",
    "tmpfs mount for dockerfile": "No, it's not possible to do this from a Dockerfile.\nPossible alternatives depend on what your final goal is:\nIf you need to create some temporary files during one step, and exclude them from the resulting image, then delete them within the same step.\nIf you need to create some temporary files and access them across several steps, but exclude them from the final image, then use multi-stage builds.\nIf you just want to use a tmpfs to speed up the build, try enabling Buildkit.\nIf you want to configure a path inside the image where the user can specify a specific filesystem (including a tmpfs), use the VOLUME dockerfile instruction.\nIf you really need very specifically a tmpfs while the image is running, it might be possible to do this from a running container (i.e. a script launched with docker run ...) if the container is privileged.",
    "Docker image error \"Service chromedriver unexpectedly exited. Status code was: 127\"": "Had the same problem. Fixed it by installing chrome-browser.\nPlease check this answer: How to install Google chrome in a docker container",
    "Why is Docker producing different images when building my Rust program on macOS versus Ubuntu? Is this a bug in Docker?": "Is this a bug in Docker or do I fundamentally misunderstand how Docker works?\nI think this is more of a Docker limitation than a bug.\nas far as I know, Docker should not produce different images when building the same thing on two different operating systems?\nThis is not entirely true. Docker does provide an \"OS environment\" to each container, but the kernel is always the host's kernel - Docker is not a full VM.\nIt is possible for a program to run differently on different OSes. Therefore it is possible to get different build results as well.\nI am not convinced that's what is happening in this specific case, however. I think you might be right that there is some slight difference on the host or Docker side. I would suggest checking the modified and creation times on both the outside (host) side and inner (Docker) side of the containers of the src directory and main.rs",
    "Docker container - How to set GID of socket file to groupID 130?": "It seems like this would be difficult with docker compose. A workaround that might apply for users of docker run is to get the docker group ID from the host:\ndocker_group_id=$(getent group docker | cut -d: -f3)\nThen add arguments to mount /etc/passwd and /etc/group:\n  -u \"$(id -u):$docker_group_id\" \\\n  -v /etc/passwd:/etc/passwd \\\n  -v /etc/group:/etc/group \\",
    "Docker - cannot build simple dotnet webapi container: COPY failed: stat /var/lib/docker/overlay2 error": "I have faced the same error in Node. I was resolve using\nCOPY . .",
    "What is the proper shebang to use in a python script to be used in a Dockerfile against the official python:3 docker image?": "you need to use this shebang simply:\n#!/usr/bin/env python",
    "docker-compose build with Dockerfile containing experimental feature": "Compose support for connecting to the Docker Engine using the ssh protocol was added in version 1.25.0-rc3.",
    "How to password protect a Docker image or a container?": "How about you make one user id and put the line about login as that user?\ntry\nRUN useradd --system --create-home --shell /bin/bash --password password username\nRUN login username\nSo you can put the password as you want.",
    "How can I check the local image is up to date with registry image?": "You can execute the pull command to make sure your local docker repo is up-to-date. If you try pull the image, docker will compare the remote image digest with local image digest. If the digests are equal, docker won't update the image and will say it is up-to-date.\nExample below:\n    docker pull myrepo/app:v0.1\n    car0.1: Pulling from myrepo\n    Digest: sha256:cb7b50c26124ffba06fd559fa86751e5fe9b4a49990fc8dbf4dfaf0fb9d58206\n    Status: Image is up to date for myrepo/app:v0.1",
    "Name a stage per build-arguments using a Dockerfile with multi-stage-builds": "You can do this with BuildKit, which requires docker 18.09+. (https://docs.docker.com/develop/develop-images/build_enhancements/)\nAll you have to do is set an env variable before building:\nDOCKER_BUILDKIT=1 docker build -t whatever .\nI don't think it's possible without BuildKit.",
    "Docker : run service on specific nodes using --placement-pref": "It is not possible to do this with the only placement-pref because:\nNodes which are missing the label used to spread still receive task assignments. As a group, these nodes receive tasks in equal proportion to any of the other groups identified by a specific label value. In a sense, a missing label is the same as having the label with a null value attached to it. If the service should only run on nodes with the label being used for the spread preference, the preference should be combined with a constraint.",
    "Communication between containers in docker swarm": "This is your problem:\nWeb Socket server started on address ws://localhost:3000\nHttp server started on address http://localhost:8080\nDon't bind to localhost. Bind to any IP 0.0.0.0. Or for more security bind to hostname -I IP For all outside your container is not localhost it's another host despite you run on the same machine.",
    "Share source code between php container and nginx container": "Unfortunately, there is no trivial solution at the time.\nIf you mount a volume with existing data to a directory it will be overwritten with the volume's content. This behavior is desired by docker developers since volumes are often used as storage for valuable data, and it protects you from losing the data. There is no option to change this behavior nor any other functionality of docker-compose or docker for this problem.\nBut you can implement your own solution. One simple way to do that is to run a script which copies your files into a volume which is shared between the both services.\nFor example, assuming your files are in the ./public directory and you start the app with the command app:\ndocker-compose.yml\nservices:\n  nginx-proxy:\n    image: nginx\n    volumes:\n      - assets:/www/data:ro\n\n  web:\n    image: my_web_app\n    volumes:\n      - assets:/assets:rw\n\nvolumes:\n  assets:\nDockerfile\n...\nCOPY public ./public\nCOPY entrypoint.sh ./\nCMD [\"sh\", \"./entrypoint.sh\"]\nentrypoint.sh\n#!/bin/sh\ncp -r ./public/* /assets\napp\nThis will copy your files into the /www/data directory of the nginx-proxy when the application container starts. If you want to copy files even when you run a custom command on the service, change CMD to ENTRYPOINT.",
    "Dockerfile not using cache in RUN composer install command": "As a work-around you could create two Dockerfiles. One that creates an image at the point where you would like to cache. The second Dockerfile can then use the first image as its base and make modifications as required.\nFROM quay.io/my_company/phpjenkins\n\nWORKDIR /usr/src/my_project\nADD composer.json composer.json\nADD composer.lock composer.lock\n\nRUN composer install -o\n\nCMD [\"tail\", \"-f\", \"/dev/null\"]\nBuild this file to mycomposerimage using\ndocker build -t mycomposerimage .\nThen second dockerfile picks up from there\nFROM mycomposerimage\nWORKDIR /usr/src/my_project\nADD . .\n\nRUN mkdir -p temp/unittest/cache log\n\nRUN cp app/config/config.unittest.template.neon app/config/config.unittest.neon\n\nCMD [\"tail\", \"-f\", \"/dev/null\"]",
    "curl not downloading file inside container while building": "I have run your dockerfile, and it just works.\nI guess the image you used doesn't had quite recent ssl certificates bundle.\nThe image you are using (resin/rpi-raspbian:wheezy) had plenty of release since you used it last time !\nJust pull the new image and rebuild your container and you should be good !",
    "How to set password to a docker container": "There is already answer for similar question which can cover yours. Take a look Docker: What is the simplest way to secure a private registry?.\nIn short:\n# Run the registry on the server, allow only localhost connection\ndocker run -p 127.0.0.1:5000:5000 registry\n\n# On the client, setup ssh tunneling\nssh -N -L 5000:localhost:5000 user@server",
    "Is there any way to make a COPY command on Dockerfile silent or less verbose?": "You can tar/zip the files on the host, so there's only one file to copy. Then untar/unzip after it's been copied and direct the output of untar/unzip to /dev/null.",
    "Docker-compose starting containers twice, running command twice and have same content": "When you do docker-compose up it will start streaming all logs from all containers that were brought up.\nThis simply looks like server_1 output some stuff, and then view_1 output some stuff, and then server_1 output some more stuff, etc...\nIf you want to see the logs for an individual service, you can run docker-compose logs server or docker-compose logs view, and get the logs for a specific service only.",
    "VS2022 Debug gets stuck on healthcheck using a Docker Compose": "I had the exact same issue. I went and reviewed the LaunchSettings.json and there was some service I missed. Weird enough, when I disabled the HealthChecks in docker-compose.yml file, it also worked. But the correct fix was the LaunchSettings.json. HealthChecks are back and they are working.\nThis is a Visual Studio debug requirement as the solution working with docker compose up command and running docker compose in Powershell running a release version and not with a debugger attached to the container.",
    "Docker | failed to solve with frontend dockerfile.v0: failed to create LLB definition: rpc error: code = Unknown desc": "I searched for a while on this one. Docker gets in a muddle with your credentials. This following worked for me. Make sure you are logged in on docker desktop then:\nrm ~/.docker/config.json\nThen do docker login to refresh the credentials",
    "Cannot build dockerfile with sdkman": "Actually the script /root/.sdkman/bin/sdkman-init.sh sources the sdk\nsource is a built-in to bash rather than a binary somewhere on the filesystem.\nsource command executes the file in the current shell.\nEach RUN instruction will execute any commands in a new layer on top of the current image and commit the results.\nThe resulting committed image will be used for the next step in the Dockerfile.\nTry this:\nFROM ubuntu:latest\n\nMAINTAINER kesarling\n\nRUN apt update && apt upgrade -y\nRUN apt install nginx curl zip unzip -y\nRUN apt install openjdk-14-jdk python3 python3-doc clang golang-go gcc g++ -y\nRUN curl -s \"https://get.sdkman.io\" | bash\nRUN /bin/bash -c \"source /root/.sdkman/bin/sdkman-init.sh; sdk version; sdk install kotlin\"\n\nCMD [ \"echo\",\"The development environment has now been fully setup with C, C++, JAVA, Python3, Go and Kotlin\" ]",
    "How to redirect FastAPI Documentation while running on Docker": "To avoid showing the redirect in the docs page\n@app.get(\"/\", include_in_schema=False)\nasync def docs_redirect():\n    return RedirectResponse(url='/docs')\nDocumentation here: https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#exclude-from-openapi",
    "Run multiple scripts in docker image": "You can use CMD in your Dockerfile and use command & to run two command in parallel:\nCMD server_command & client_command\n(where server_command is the command used to start the server and client_command is the command used to start the client)",
    "Dockerfile wget fails": "As Ntokozo stated, each RUN command is a separate \"session\" in the build process. As such, Docker is really designed to pack as many commands into a single RUN as possible allowing for smaller overall image size and fewer layers. So the command could be written like so:\nRUN apt-get update && \\\n    apt-get install -y wget && \\\n    mkdir -p example && \\\n    cd example/ && \\\n    wget -r https://host/file.tar && \\\n    tar -xvf *.tar && \\\n    rm -r example/*.tar && \\\n    mv example/foo example/bar",
    "Docker-compose ERROR [internal] booting buildkit, http: invalid Host header while deploy Django": "I have the same problem. I guess you are using a snap docker. The (snap) docker version (2904) on my server has the same problem.\nOn my laptop the docker version (2893) is working as expected. I changed the snap channel on my laptop to latest/candidate (2904) and the same problem occurs. Switched back to latest/stable and everything is fine again.\nSo:\nsnap info docker\nto get some version info.\nUse:\nsnap refresh --stable docker\nto switch to the stable channel.\nnow your project build on my laptop as to be expected. I did have to change:\nFROM python 3.10\nto\nFROM python:3.10\nThe switching works on my laptop (Ubuntu 23.10), but not on my server (Ubuntu 18.04.6).\nOn the server i had to use:\nsudo snap refresh --revision=2893 docker",
    "Centos 7 docker yum installation gets stuck": "You are hitting this bug (also see this pull request with the fix). Older versions of rpm (such as in CentOS 7) include code that attempts to set the CLOEXEC flag on all file descriptors. Because Docker sets a high limit on the number of files (1073741816), this process takes a ridiculously long time.\nThe simplest workaround -- if you're forced to use an older distribution like CentOS 7 -- is to reduce the limit before running yum:\nFROM centos:7\n\nRUN echo 'hosts: files mdns4_minimal [NOTFOUND=return] dns mdns4' > /etc/nsswitch.conf\n\nRUN yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nRUN ulimit -n 1024 && yum -q install -y docker-ce docker-ce-cli containerd.io\n\n# -- Install OS packages:\nRUN ulimit -n 1024 && yum -q update -y && yum -q install -y \\\n    bash \\\n    bzip2-devel \\\n    ca-certificates \\\n    curl \\\n    epel-release \\\n    gcc \\\n    gcc-c++ \\\n    git \\\n    gnutls \\\n    gnutls-devel \\\n    libffi-devel \\\n    make \\\n    ncurses-devel \\\n    openssh-clients \\\n    openssh-server \\\n    openssl \\\n    openssl-devel \\\n    rsync \\\n    readline-devel \\\n    tar \\\n    unzip \\\n    wget \\\n    zip \\\n    zlib-devel \\\n    temurin-11-jdk\nUsing the above Dockerfile:\n$ time docker build -t yumtest .\n[...]\nSuccessfully built e5ae777ff3ff\nSuccessfully tagged yumtest:latest\n\nreal    1m2.668s\nuser    0m0.008s\nsys     0m0.015s",
    "Pull access denied, repository does not exist or may require authorization: server message: insufficient_scope:authorization failed": "It looks like you've got a multi-stage Dockerfile but haven't named any stages. Specifically, the COPY --from=build /app/out . line references a stage named build that is not defined. This should resolve the issue (note the first line defines the build stage):\nFROM mcr.microsoft.com/dotnet/sdk:3.1 AS build\n\nWORKDIR /app\n\nCOPY *.csproj . \n\nRUN dotnet restore \n\nCOPY . . \n\nRUN dotnet publish -c release -o out\n\nFROM mcr.microsoft.com/dotnet/aspnet:3.1\n\nWORKDIR /app\n\nEXPOSE 80\n\nCOPY --from=build /app/out .\n\nENTRYPOINT [\"dotnet\" \"weather.dll\"]",
    "asp.net core docker image \"aspnet:2.2-stretch-slim\"": "stretch-slim refers to the base debian image used by the aspnet image. Stretch is the development codename for Debian 9 and Slim is the minimal prerequisites required by debian and excludes additional tool and packages from the image.\nThe .NET Core 2.2 Docker images are currently all available in four flavors, depending on the OS image they're based on: debian:stretch-slim, ubuntu:bionic, alpine:3.8, and alpine:3.9. There are also ARM32 versions of the debian and ubuntu images. Debian is the default OS used by the .NET Core docker images.\nAsp.net core chooses the debian:stretch-slim as the base image and this provides the asp.net images with the smallest OS foot print possible.\nThe following tags all refer to the same image 2.2.6-stretch-slim, 2.2-stretch-slim, 2.2.6, 2.2\nReferences:\nhttps://andrewlock.net/exploring-the-net-core-mcr-docker-files-runtime-vs-aspnet-vs-sdk/#3-mcr-microsoft-com-dotnet-core-aspnet-2-2-3\nhttps://wiki.debian.org/DebianStretch\nhttps://github.com/dotnet/core/blob/master/Documentation/prereqs.md\nhttps://github.com/dotnet/dotnet-docker/blob/7e4359dfe6e1bc649fbb58de273da3d35bf864f1/2.2/aspnet/stretch-slim/amd64/Dockerfile",
    "How to send a password to `sudo` from a Dockerfile": "I'd recommend skipping the sudo completely since you can change users with your Dockerfile:\n....\n# allow writes to the home directory\nUSER root\nRUN chmod 777 /home\nUSER user\n....\nAdding sudo to your image means it's there for an attacker to use. You can change the user of a docker run or docker exec command with the -u root option any time you need to get back into the container as root.",
    "using shared dockerfile for multiple dockerfiles": "IMPORT directive will never be implemented\nA long time ago there was proposed IMPORT directive for Docker\nUnfortunately, issues are closed while PR's are still open:\ndocker - using shared dockerfile for multiple dockerfiles - Stack Overflow\nProposal: Dockerfile add INCLUDE \u00b7 Issue #735 \u00b7 moby/moby\nimplement an INCLUDE verb/instruction for docker build files \u00b7 Issue #974 \u00b7 moby/moby\nAdd INCLUDE feature \u00b7 Issue #40165 \u00b7 moby/moby\ndocker build: initial work on the include command by flavio \u00b7 Pull Request #2108 \u00b7 moby/moby\nDockerfile templating to automate image creation\nTemplating your Dockerfile like a boss! | by Ahmed ElGamil | Dockbit\nSolution for your case\nBut for your case, all you need - is just a bit of sed\nE.g.:\n# Case1: inplace templating\nEXPOSED_PORT=8081 sed -i \"s/EXPOSE 8080/EXPOSE $EXPOSED_PORT/\" Dockerfile\n\n# Case2: generating Dockerfile from template\nsed \"s/EXPOSE 8080/EXPOSE $EXPOSED_PORT/\" Dockerfile.template > Dockerfile\nExplanation:\nEXPOSED_PORT=8081 declares local bash variable\nsed is a tool for text manipulation\nsed -i \"s/EXPOSE 8080/EXPOSE $EXPOSED_PORT/\" Dockerfile replaces EXPOSE 8080 to EXPOSE 8081\nsed \"s/EXPOSE 8080/EXPOSE $EXPOSED_PORT/\" Dockerfile.template > Dockerfile generates the new Dockerfile from Dockerfile.template",
    "nodejs app doesn't connect to localhost when running within a docker container": "Ok, as you say in your question, your process is listening to connection on localhost:4200.\nInside a docker container, localhost is the address of the loopback device of the container itself, which is separated and not reachable from your host network.\nYou need to edit your node process in order to make it listen to all addresses by editing the entrypoint in your Dockerfile as follows:\nENTRYPOINT [\"ng\", \"serve\", \"-H\", \"0.0.0.0\"]",
    "Running docker container with user": "There are two possibilities to run docker containers with a user different from root.\nFirst possibility: Create user in Dockerfile\nIn your example Dockerfile, you create user newuser with command useradd. You can write instruction\nUSER newuser\nin the Dockerfile. All following commands will be executed as user newuser. This goes for all following RUN instructions as well as for docker run commands.\nSecond possibility: option --user (tops possible USER instruction in image)\nYou can use docker run option --user. It can be used to specify either an UID without a name:\ndocker run --user 1000\nOr specify UID and GID without a name:\ndocker run --user 1000:100\nor specify a name only without knowing which UID the user will get:\ndocker run --user newuser\nYou can combine both ways. Create a user in Dockerfile with specified (!) UID and GID and add him to all desired groups. Use matching docker run --user UID:GID, and your container user will have all attributes you gave him in the Dockerfile.\n(I do not understand your approach with --security-opt label=user:newuser, either it is wrong or it is something I know nothing about.)",
    "Dockerfile parse error line 18: ARG names can not be blank": "Arguments should not have any space while assigning the value.\nJust remove the space.\nARG DEV = false // Incorrect Approach  \nARG DEV=false // Correct Approach ",
    "Building Dockerfile that has \"RUN apt-get update\" gives me \"jailing process inside rootfs caused 'permission denied'\"": "There are several issues in your question:\nDo not run docker with sudo. If your own user is not allowed to run docker, you should add yourself to the docker group: sudo usermod -aG docker $(whoami)\nSome of your RUN commands have no meaning, or at least not the meaning you intend - for example: RUN cd anything will just change to the directory inside that specific RUN step. It does not propagate to the next step. Use && to chain several commands in one RUN or use WORKDIR to set the working directory for the next steps.\nIn addition, you were missing the wget package\nHere is a working version of your Dockerfile:\nFROM ubuntu:18.04\n\nRUN apt-get update && apt-get -y install \\\n    build-essential libpcre3 libpcre3-dev zlib1g zlib1g-dev libssl-dev wget\n\nRUN wget http://nginx.org/download/nginx-1.15.12.tar.gz\n\nRUN tar -xzvf nginx-1.15.12.tar.gz\n\nWORKDIR nginx-1.15.12\n\nRUN ./configure \\\n    --sbin-path=/usr/bin/nginx \\\n    --conf-path=/etc/nginx/nginx.conf \\\n    --error-log-path=/var/log/nginx/error.log \\\n    --http-log-path=/var/log/nginx/access.log \\\n    --with-pcre \\\n    --pid-path=/var/run/nginx.pid \\\n    --with-http_ssl_module\n\nRUN make && make install",
    "Unable to run './mvnw clean install' when building docker image based on \"openjdk:8-jdk-alpine\" for Spring Boot app": "We were experimenting this issue only on Windows 10 and were able to solve this issue on just applying dos2unix\ndos2unix mvnw",
    "Bandwidth and Disk space for Docker container": "Memory and CPU are controlled using cgroups by docker. If you do not configure these, they are unrestricted and can use all of the memory and CPU on the docker host. If you run in a VM, which includes all Docker for Desktop installs, then you will be limited to that VM's resources.\nDisk space is usually limited to the disk space available in /var/lib/docker. For that reason, many make this a different mount. If you use devicemapper for docker's graph driver (this has been largely deprecated), created preallocated blocks of disk space, and you can control that block size. You can restrict containers by running them with read-only root filesystems, and mounting volumes into the container that have a limited disk space. I've seen this done with loopback device mounts, but it requires some configuration outside of docker to setup the loopback device. With a VM, you will again be limited by the disk space allocated to that VM.\nNetwork bandwidth is by default unlimited. I have seen an interesting project called docker-tc which monitors containers for their labels and updates bandwidth settings for a container using tc (traffic control).",
    "How to copy a csproj file using docker without visual studio and without docker compose?": "This is caused by the wrong root folder for the file path in dockerfile.\nFor launching from Docker, its root folder is C:\\Users\\...\\repos\\TryNewDocker2, but while running from command, its root fodler is C:\\Users\\...\\repos\\TryNewDocker2\\TryNewDocker2, so the path for TryNewDocker2.csproj has changed from TryNewDocker2/TryNewDocker2.csproj to TryNewDocker2.csproj\nTry dockerfile below:\nFROM microsoft/dotnet:2.1-aspnetcore-runtime AS base\nWORKDIR /app\nEXPOSE 59162\nEXPOSE 44342\n\nFROM microsoft/dotnet:2.1-sdk AS build\nWORKDIR /src\nCOPY [\"TryNewDocker2.csproj\", \"TryNewDocker2/\"]\nRUN dotnet restore \"TryNewDocker2/TryNewDocker2.csproj\"\nCOPY . ./TryNewDocker2/\nWORKDIR \"/src/TryNewDocker2\"\nRUN dotnet build \"TryNewDocker2.csproj\" -c Release -o /app\n\nFROM build AS publish\nRUN dotnet publish \"TryNewDocker2.csproj\" -c Release -o /app\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app .\nENTRYPOINT [\"dotnet\", \"TryNewDocker2.dll\"]\nUpdate\nFor working in both Docker and command, do not change your dockerfile, and from path below to run your command with specifying the dockerfile path.\nC:\\Users\\...\\repos\\TryNewDocker2>docker build -t gogo -f TryNewDocker2/Dockerfile .",
    "Exposing docker container ports not working": "Run below command\ndocker run -d -p 9065:8080 testportmapping:latest\nImage name should be the last parameters and all other parameters should come before it.\nP.S. As mentioned by @David Maze in the comments, whatever comes after the image name is passed on to the container",
    "How can docker-compose be used to build Dockerfiles?": "docker-compose can be used as part of your workflow to build Dockerfiles.\nA nice way to arrange this is to have a docker-compose.yml in the root, and then have a sub folder for each service.\ncompose.yml\nservices:\n  php7.4:\n    image: kaiza/image-php74:latest\n    build:\n      context: php74\n      args:\n        PHP_VERSION: ${VERSION-7.3}\nAssuming the relative path from the docker-compose to the php74 Dockerfile is php74/Dockerfile then:\ndocker-compose build php7.4 will build, and tag the image as kaiza/image-php74, and docker-compose up or docker-compose run php7.4 will build the image if requred, or use the locally built image if available.\nThe full reference for the build section is available on github",
    "In a Docker Build with the build-arg flag,values are not getting passed correctly": "Every FROM instruction in a Dockerfile instructs docker to start a new Stage and by default there is no context propagated from one stage to the next. You could think of each stage as it's an independent image build. More on multi-staged docker image builds and why they were introduced can be found in the docs here.\nIn your case this means that you are declaring the build arg BASE_IMAGE and TEMP_DIR in the first stage (in this case stage 0) of your docker build. However your echo command is in the second stage of your docker build and therefore doesn't know anything about these build args (they are scoped to stage 0).\nTo solve this issue you can just declare your build args in every stage they are needed. This should work fine:\n# start of stage 0\nARG BASE_IMAGE\n\nFROM $BASE_IMAGE\n\n# start of stage 1\nARG BASE_IMAGE\nARG TEMP_DIR\n\nRUN echo $TEMP_DIR && \\\n    echo $BASE_IMAGE",
    "Multiple Dockerfiles": "You can have more than one Dockerfile in the same directory if desired. To specify the Dockerfile to use, use the -f argument, e.g\ndocker build -f wildfly.Dockerfile ./wildfly\ndocker build -f mysql.Dockerfile ./mysql\ndocker build -f other.Dockerfile ./other\nIn Compose, these arguments correspond to the dockerfile and context properties.\nIt is not always the case that you need to write a docker file, for example for the database service you can simply pull the image from the docker hub and use it directly. something like below\ndb:\n    image: mysql\nYou can, of course, have them share the same context, e.g.\ndocker build -f wildfly.Dockerfile .\ndocker build -f mysql.Dockerfile .\ndocker build -f other.Dockerfile .\nJust be aware that the context is sent in full to the daemon (respecting .dockerignore) so this might lead to longer build times if there is a lot of redundant data.\nIf there is a lot of reuse between the Dockerfiles, you can even have all of them in one file, e.g.\nFROM ubuntu:20.04 as base\n...\nFROM base AS wildfly\n(install wildfly)\n\nFROM base AS mysql\n(install mysql)\n...\nThen you can build the specific image with e.g.\ndocker build --target wildfly  .\nIn Compose, these arguments correspond to the target and context properties.\nThis is called multi-stage builds, and is not always a good idea but is sometimes helpful to mitigate Docker's lack of support for #include.",
    "Docker PHP access-control-allow-origin CORS": "You need to add the relevant header in apache configuration in the image. You will also need to enable mod_headers as it is not by default in your image. I used a similar technique as the one described on php docker image documentation (search for \"Changing DocumentRoot (or other Apache configuration)\" on the page).\nHere is a possible Dockerfile. I added as well some good practice to limit the size of layer after running apt.\nFROM php:7.4-apache\n\nRUN apt-get update && apt-get upgrade -y \\\n    && apt-get clean && rm -rf /var/lib/apt/lists/*\n\nRUN docker-php-ext-install mysqli && docker-php-ext-enable mysqli \\\n    && a2enmod headers\n    && sed -ri -e 's/^([ \\t]*)(<\\/VirtualHost>)/\\1\\tHeader set Access-Control-Allow-Origin \"*\"\\n\\1\\2/g' /etc/apache2/sites-available/*.conf\n\nEXPOSE 80\nReferences:\nhttps://hub.docker.com/_/php/\nhttps://enable-cors.org/server_apache.html",
    "How to pack and ship a simple c application in docker without the gcc compiler?": "You can use Alpine which is less then 5MB, In the case of multi-stage build, you can have the same bonus of 5MB\nStage one: Compiling the source code to generate executable binary and\nStage two: Running the result.\n# use alpine as base image\nFROM alpine as build-env\n# install build-base meta package inside build-env container\nRUN apk add --no-cache build-base\n# change directory to /app\nWORKDIR /app\n# copy all files from current directory inside the build-env container\nCOPY . .\n# Compile the source code and generate hello binary executable file\nRUN gcc -o hello helloworld.c\n# use another container to run the program\nFROM alpine\n# copy binary executable to new container\nCOPY --from=build-env /app/hello /app/hello\nWORKDIR /app\n# at last run the program\nCMD [\"/app/hello\"] \nhelloworld.c or replace with your own one\n#include <stdio.h>\n\nint main(){\n   printf(\"Hello World!\\n\");\n   return 0;\n}\nAnother way to copy compiled code to your image which is also in just 5MB,\nFROM alpine:latest\nRUN mkdir -p /app\nCOPY hello /app\nWORKDIR /app\nCMD [\"/app/hello\"] ",
    "Docker /bin/sh: COPY: command not found": "You can't do, inside a\nRUN\na Dockerfile\nCOPY\nYou need to find another way, you may have a script that creates different Dockerfile based on your test.",
    "Docker Swarm Deploy a local Dockerfile": "On Docker Swarm you can't build an image specified in a Docker Compose file:\nNote: This option is ignored when deploying a stack in swarm mode with a (version 3) Compose file. The docker stack command accepts only pre-built images. - from docker docs\nYou need to create the image with docker build (on the folder where the Dockerfile is located):\ndocker build -t imagename --no-cache .\nAfter this command the image (named imagename) is now available on the local registry.\nYou can use this image on your Docker Compose file like the following:\nversion: '3'\n\nservices:\n  example-service:\n    image: imagename:latest",
    "Docker-Compose WordPress MySql Can't Connect": "WARNING: This was the only container I had on my server so I did not mind deleting everything. Running these commands blindly can wipe out all your docker containers, volumes, etc.\nI was able to solve my problem by deleting the docker volume.\nEven when you run this command:\n$ docker-compose stop && docker-compose rm -v\ndoes not remove volumes.\nEvery time I tried to create a new docker container it used my old volume that stored my old mysql database with my old mysql users table thus my old password which I would change and was wrong.\nto get rid of old docker volumes use:\ndocker volume rm $(docker volume ls -q )\nwill remove all docker volumes. then rebuild which will create a new volume with an updated table from your env variables.",
    "Linking Docker container : Node.js + MongoDB": "By default, mongoose will try to connect to a local mongodb. To be able to use your mongodb container, you should use its hostname (db_1) while you make the connection.",
    "docker root crontab job not executing": "Why use cron? Just write a shell script like this:\n#!/bin/bash\nwhile true; do\n  python /opt/com.org.project/main.py >> /opt/com.org.project/var/log/cron.log\n  sleep 60\ndone\nThen just set it as entrypoint.\nENTRYPOINT [\"/bin/bash\", \"/loop_main.sh\" ]",
    "Docker bash: syntax error: unexpected \"(\" (expecting \"then\")": "I would have loved to ask that question as a comment, but I'm not allowed to, yet. Anyway, after having had a quick test, I assume your docker image is Alpine Linux-based. The standard Alpine Linux docker images do not come with a bash as you might be expecting but with an ash and ash does not have a [[-built in command, i.e. you should be sticking to the standard [ aka test command or get goind completely without it. And, sadly enough, test does not have the ability to handle regular expressions.\nThat said and again assuming that you do not want to bloat the Alpine image with a complete bash, grep comes to the rescue. A solution for your case would be (lines 13, ff.)\nif ! echo $service | grep -qE \"^(file|user)$\" ; then\n    printRed \"Incorrect service: $service\"\n    exit 1\nfi\nexplanation\nMake the if directly test the return code from grep using your pattern. grep returns non-zero if there was no match, 0 otherwise. -q suppresses output of the match, -E switches to extended regular expression, as needed for the |.",
    "Docker's \"executable file not found in $PATH: unknown\" trying to run \"cd\"": "cd is a special built-in utility, in the language of the POSIX shell specification. It's something that changes the behavior of the running shell and not a standalone program. The error message means what it says: there is no /bin/cd or similar executable you can run.\nRemember that a Docker container runs a single process, then exits, losing whatever state it has. It might not make sense for that single command to just change the container's working directory.\nIf you want to run a process inside a container but in a different working directory, you can use the docker run -w option\ndocker run -it \\\n  -w /workspace \\\n  cloudbuildtoolset:latest \\\n  the command you want to run\nor, equivalently, add a WORKDIR directive to your Dockerfile.\nYou can also launch a shell wrapper as the main container process. This would be able to use built-in commands like cd, but it's more complex to use and can introduce quoting issues.\ndocker run -it cloudbuildtoolset:latest \\\n  /bin/sh -c 'cd /workspace && the command you want to run'",
    "With multi-stage Dockerfile, the pip wheel still needs the dependencies from the base builder": "OK, after spending hours, I found the problem and fixed it. So I'll answer my own question.\nFirst of all, I was using --no-deps parameter in the pip wheel command. This parameter causes that the pip wheel only downloads the main packages, without their dependencies. So in the second build image (stage), pip was trying to download all the sub dependencies for these main dependencies. So these dependencies were in need for some system packages to build.\nRemoving this --no-deps parameter would normally fix the problem in such situation, but it caused another error for my situation. There were some version conflicts between the dependencies, which were not exist with regular pip install -r ...\nInstead of the pip wheel technique, I just used pip install --user -r option to install my requirements in the folder /root/.local. Then I moved the /root/.local folder to the second stage's /root:\n..\nCOPY --from=builder /root/.local /home/.local\n..\nThat's all. All my requirements were installed in the second build image too.\nDo not forget to add the /root/.local/bin path to your $PATH:\nENV PATH=/root/.local/bin:$PATH",
    "sh: curl: not found even install curl inside k8s pod": "Your Dockerfile consists of multiple stages, which is also called multi-stage build.\nEach FROM statement is a new stage and new image. In your case you have 2 stages:\nbuilder where you build you app and install curl\nsecond stage which copies /usr/src/app from builder stage\nIn this case second FROM node:12-alpine statement will contain only basic alpine packages, node tools and /usr/src/app which you have copied from the first stage.\nIf you want to have curl in your final image you need to install curl in second stage (after second FROM node:12-alpine):\nFROM node:12-alpine AS builder\n\n# Variables from outside\nARG NODE_ENVIRONMENT=development\nENV NODE_ENV=$NODE_ENVIRONMENT\n\n# Create app directory\nWORKDIR /usr/src/app\n\n# Do not install \n\n# Install app dependencies\nCOPY package*.json ./\nRUN npm install\n\n# Bundle app source\nCOPY . .\n\n# Build Stage 2\n# Take the build from the previous stage\nFROM node:12-alpine\n\n#Install curl\nRUN apk update && apk add curl\n\nWORKDIR /usr/src/app\nCOPY --from=builder /usr/src/app /usr/src/app\n\n# run the application\nEXPOSE 50005 9183\n\nCMD [ \"npm\", \"run\", \"start:docker\" ]\nAs it was mentioned in comments you can test it by running docker container directly - no need to run pod in k8s cluster:\ndocker build -t image . && docker run -it image sh -c 'which curl'\nIt is common to use multi-stage build for applications implemented in compiled programming languages.\nIn the first stage you install all necessary dev tools and compilers and then compile sources into a binary file. Since you don't need and probably don't want sources and developer's tools in a production image you should create a new stage.\nIn the second stage you copy compiled binary file and run it as CMD or ENTRYPOINT. This way your image contains only executable code, which makes them smaller.",
    "Fail to resolve docker compose service name from front end": "It will not work, because your browser(internet client) is not part of docker stack network, you have to configure you frontend service to connect to http://localhost:6002 instead of http://backend:4001",
    "Docker image erorr bin not found after npm installation": "this will work:\nFROM openjdk:8-jdk-slim\n\n\nARG ND=v12.13.0\n\nRUN apt-get update && \\\n    apt-get install --yes --no-install-recommends curl \\\n    && NODE_HOME=/opt/nodejs; mkdir -p ${NODE_HOME} \\\n    && curl --fail --silent --output - \"http://nodejs.org/dist/${ND}/node-${ND}-linux-x64.tar.gz\" \\\n     | tar -xzv -f - -C \"${NODE_HOME}\" \\\n    &&  ln -s \"${NODE_HOME}/node-${ND}-linux-x64/bin/node\" /usr/local/bin/node \\\n    && ln -s \"${NODE_HOME}/node-${ND}-linux-x64/bin/npm\" /usr/local/bin/npm \\\n    && ln -s \"${NODE_HOME}/node-${ND}-linux-x64/bin/npx\" /usr/local/bin/ \\\n    && npm install --prefix /usr/local/ -g grunt-cli\n\nENV PATH=\"${PATH}:/usr/local/bin\"\nRUN ls /usr/local/bin\nRUN grunt -v\nusing --prefix will tell npm to install grunt in /usr/local/bin\nls output:\nStep 5/6 : RUN ls /usr/local/bin\n ---> Running in 96493743512d\ndocker-java-home\ngrunt\nnode\nnpm\nnpx\ngrunt -v output:\nStep 6/6 : RUN grunt -v\n ---> Running in c6248c4fce6c\ngrunt-cli: The grunt command line interface (v1.3.2)",
    "How to change permission on file in Dockerfile?": "chmod +r /opt/sonarqube/extensions/plugins/ throws an error because the sonarqube container is run as user sonarqube(see the USER sonarqube command in the Dockerfile: https://github.com/SonarSource/docker-sonarqube/blob/master/7.7-community/Dockerfile)\nsudo is not installed in the image, so you won't be able to run commands with sudo. Instead, as M. Alekseev mentiones, change the user to root and run your custom commands.\nUSER root\nRUN ...\nRUN ...\n\n# switch back to user sonarqube for security\nUSER sonarqube\nI'd recommend to switch back to user sonarqube after installing your custom packages etc.\nNote that you might need to set permissions on files created by the root user then.",
    "Docker and laravel not clearing the cache": "Any commands that are being executed using RUN in the Dockerfile will be executed only during the build phase. If you wish to run them while starting a container so you don't have to run them manually then you need to use a script e.g. bash script that can be used as an ENTRYPOINT for your Dockerfile and then make this script execute the command that you should run to start the application. So your entrypoint.sh can look like this:\n# entrypoint.sh\n#!/usr/bin/env sh\n...\nphp artisan optimize\nphp artisan config:cache\nphp artisan view:cache\nphp artisan view:clear\n...\nphp artisan serve # or use exec \"$@\" and pass php artisan serve through CMD in Dockerfile\nThis will make every container that you start execute all of these commands before start serving your application.\nOther links that you might want to check:\nWhat is the difference between CMD and ENTRYPOINT in a Dockerfile?\nWhat does set -e and exec \u201c$@\u201d do for docker entrypoint scripts?",
    "Is there a reason why people write everything to the DockerFile instead of a separate shell script?": "Update:\nIn additional to the point made by David about the complexity, I believe writing everything to the Dockerfile makes it easier for share (thus creating a survivorship bias for you). Namely on the DockerHub, most of the time, you have a \"Dockerfile\" tab to quickly get the idea on how the image is built. If the author uses COPY and RUN xyz.sh, he/she would have to host the script elsewhere or the Dockerfile alone becomes meaningless.\nCMD is executed at runtime, that is when the container is created from the image. RUN is a build time instruction. So the question is actually why people run things with RUN instead of CMD at runtime. (You can of course COPY script.sh /script.sh then RUN bash /script.sh)\nIf you do things like installing dependencies, it could take a lot of time, in case of scaling up your service, this would make auto-scaling useless because it can't be fast enough to absorb the peak.\nAt build time, RUN can be cached, so next time the build will be a lot faster.\nBecause the way docker file system works, creating 10 containers from the same image takes only a few more space than creating 1 container. So you can save disk space by installing packages in the image, while if you install them at runtime, they will all occupy a part of disk space.",
    "Deploy existing Prestashop to server using Docker": "Ok, so I deeped into problem and solution for ma quesstion is as below. What I did is pull original image from prestashop and copy there my files.\nNext step was use mariadb image. I had backup.sql file exported from previous store phpmyadmin\nversion: '2'\n\nservices: \n  prestashop:\n    image: prestashop\n    ports:\n      - 80:80\n    links:\n      - mariadb:mariadb\n    depends_on:\n      - mariadb\n    volumes:\n      - ./src:/var/www/html\n      - ./src/modules:/var/www/html/modules\n      - ./src/themes:/var/www/html/themes\n      - ./src/override:/var/www/html/override\n    environment:\n      - PS_DEV_MODE=1\n      - DB_SERVER=mariadb\n      - DB_USER=root\n      - DB_PASSWD=root\n      - DB_NAME=prestashop\n      - PS_INSTALL_AUTO=0\n\n  mariadb:\n    image: mariadb\n    volumes:\n     - backup.sql:/docker-entrypoint-initdb.d\n    environment:\n      - MYSQL_ROOT_PASSWORD=root\n      - MYSQL_DATABASE=prestashop\n\n  phpmyadmin:\n    image: phpmyadmin/phpmyadmin\n    links:\n      - mariadb\n    ports: \n      - 81:80 \n    environment:\n      - PMA_HOST=mariadb\n      - PMA_USER=root\n      - PMA_PASSWORD=root\nThe biggest issue is IP in docker-machine. Keep in mind that if you are using docker toolbox you have IP 192.168.99.100 but in Docker for Windows your IP depends on localhost (or just type localhost).",
    "How can I make PyCharm recognize Dockerfiles that are not called exactly Dockerfile?": "By following the directions here, do the following:\nNavigate to File --> Settings (Preferences on Mac?)\nClick on Editor\nClick on File Types\nScroll until you find Dockerfile in the Recognized File Types list.\nIn the Registered Patterns area, click the plus symbol on the upper right.\nEnter a pattern like Dockerfile*",
    "Getting docker-compose to build container that uses a parent container": "As of January 2021, there is another way to do this by using the profiles key. Support for this key was added in Docker Compose 1.28.0.\nversion: '3.9'\nservices:\n  parent-notservice:\n    build:\n      dockerfile: containers/parent/Dockerfile\n      context: .\n    profiles:\n      - donotstart\n\n  my-service:\n    image: parent-notservice\n\n  another-service:\n    build:\n      context: .\n      dockerfile: ./containers/service/Dockerfile\nmy-service will be based directly on parent-notservice. Then you can add whatever ports and volumes are needed for that container in the docker-compose.yml file.\nanother-service will be built from the Dockerfile which could also be based on parent-notservice using the FROM command. Other packages can then be installed on that container that are not part of the parent.\nFROM parent-notservice\nRUN...\nAnd the best part is that when you use docker-compose up the parent-notservice will not start.\nFurther documentation on the profiles key can be found here: https://docs.docker.com/compose/profiles/",
    "Allow Docker strategy in Openshift 3": "If you are using OpenShift Online, it is not possible to enable the docker build type. For OpenShift Online your options are to build your image locally and then push it up to an external image registry such as Docker Hub, or login to the internal OpenShift registry and push your image directly in to it. The image can then be used in a deployment.\nIf you have set up your own OpenShift cluster, my understanding is that docker build type should be enabled by default. You can find more details at:\nhttps://docs.openshift.com/container-platform/latest/admin_guide/securing_builds.html\nIf you are after a way to deploy a site using a httpd web server, there is a S2I builder image available that can do that. See:\nhttps://github.com/sclorg/httpd-container",
    "Dockerfile with tomcat + mysql + filesystem app": "You can do either, but your first option is going to be more maintainable and more scalable in the long run.\nComposing a service from multiple containers means that you can take advantage of the work other people have already done to create standard images (like the mysql image). It also means that you can independently scale different parts of your application. Need more web front-ends? No problem! Need to use an existing database server or cluster instead of a container? No problem!\nA tool like docker-compose will help you maintain an application built from multiple containers.\nThere are a number of quickstarts (e.g., this one) that might help you get started.",
    "Do the USER and WORKDIR instructions hold in downstream docker file?": "From this Dockerfile:\nFROM debian:8\n\nENV HOME /home/user\nRUN useradd --create-home --home-dir $HOME user \\\n    && mkdir -p $HOME \\\n    && chown -R user:user $HOME\n\nWORKDIR $HOME\nUSER user\nA build and a run later:\n$docker build -t deb .\n$docker run --rm deb bash -c \"pwd && whoami\"\n/home/user\nuser\nNow from this Dockerfile, based from the previous image:\nFROM deb\nENTRYPOINT [ \"sh\" ]\nBuild and run:\n$docker build -t debb .\n$docker run --rm -it debb\n[container]$ pwd && whoami\n/home/user\nuser\nSo, yes the USER and the WORKDIR are inherited.\nClient:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   20f81dd\n Built:        Thu Mar 10 15:38:58 2016\n OS/Arch:      linux/amd64\n\nServer:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   20f81dd\n Built:        Thu Mar 10 15:38:58 2016\n OS/Arch:      linux/amd64",
    "Heroku docker: Error on Mac apple silicon based processors": "A workaround that I found useful was to build the image using the Docker CLI under linux/amd64 architecture, tag and push to the appropriate Heroku URL, then release using the Heroku CLI.\ndocker build -t app-name --platform linux/amd64 .\ndocker tag app-name registry.heroku.com/app-name/web\ndocker push registry.heroku.com/app-name/web\nheroku container:release web -a app-name",
    "How to specify the port to map to on the host machine when you build the image wiith Dockerfile": "You cant specify ports in Dockerfile but you can use docker-compose to achieve that.\nDocker Compose is a tool for running multi-container applications on Docker.\nexample for docker-compose.yml with ports:\nversion: \"3.8\"\nservices :\n  rabbit1:\n    image : mongo\n    container_name : rabbitmq:3-management\n    ports: \n      - 8000:5672\n      - 8001:15672",
    "Dockerfile fail to verify Postgres apt signature": "The apt has a set of trusted keys and sometimes we only need to add the ones that are missing.\nYou can add the missing key just by running the following command (before the 'apt-get update'):\napt-key adv --keyserver keyserver.ubuntu.com --recv-keys 7FCC7D46ACCC4CF8\nGreetings.",
    "Docker Compose service volume not reflecting my host machine changes to the running container": "I had similar issues on my Mac - and this turned out to be related to the option in Docker Desktop which was enabled to \"Use gRPC FUSE for file sharing\". Once I disabled this option I was able to add and edit files on the host and see the updates reflected in the container without having to stop and start the containers.",
    "Dockerfile Multistage Python Poetry - install": "Here is an example build that you can reference for multi-stage with Poetry:\nhttps://stackoverflow.com/a/64642121/14305096\nFROM python:3.9-slim as base\n\nENV PYTHONFAULTHANDLER=1 \\\n    PYTHONHASHSEED=random \\\n    PYTHONUNBUFFERED=1\n\nRUN apt-get update && apt-get install -y gcc libffi-dev g++\nWORKDIR /app\n\nFROM base as builder\n\nENV PIP_DEFAULT_TIMEOUT=100 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    POETRY_VERSION=1.1.3\n\nRUN pip install \"poetry==$POETRY_VERSION\"\nRUN python -m venv /venv\n\nCOPY pyproject.toml poetry.lock ./\nRUN . /venv/bin/activate && poetry install --no-dev --no-root\n\nCOPY . .\nRUN . /venv/bin/activate && poetry build\n\nFROM base as final\n\nCOPY --from=builder /venv /venv\nCOPY --from=builder /app/dist .\nCOPY docker-entrypoint.sh ./\n\nRUN . /venv/bin/activate && pip install *.whl\nCMD [\"./docker-entrypoint.sh\"]",
    "Use variables in Dockerfile": "You need to know that ARG is not available in the container. While ENV is available in the container. Please see this good diagram from here:\nSo in your case you need to use the ENV name and not the ARG name like this:\n# escape=`\nFROM mcr.microsoft.com/windows/servercore:1909\nARG PYTHON_VERSION=python-3.8.1.amd64\nENV PYTHON=$PYTHON_VERSION\n\nRUN echo $PYTHON",
    "cant install pip in ubuntu 18.04 docker /bin/sh: 1: pip: not found": "To run pip for python3 use pip3, not pip.",
    "How do I add MongoDB to a Docker container?": "I think having multiple purpose container (ie. business logic + db) is not really docker like, so it probably why you don't find anything about this. What you should propably do in this situation is use multiple containers, expose the correct port then call it from your application.\nI suggest you to look into Docker-Compose to do such infrastructure, which will probably have two services : one for your node server and one for your mongoDB.\nIt will even be easier to maintain and configure than having all your service inside one containers, since you can split your logic easily.\nEDIT: I did'nt test the following docker-compose.yml, so it'll probably need some fix, but it should help you enough if you read the documentation along side.\nStarting from here, you could have a docker-compose.yml file looking like this :\nversion: \"3\"\nservices:\n  app:\n    container_name: app\n    restart: always\n    build: ./path/to/your/dockerfile/root\n    ports:\n      - \"3000:3000\"\n    links:\n      - mongo\n  mongo:\n    container_name: mongo\n    image: mongo\n    volumes:\n      - ./data:/data/db\n    ports:\n      - \"27017:27017\"\nThen, inside your apps, you can access you db from this kind of url : mongodb://mongo:27017/db",
    "Fedora Docker image doesn't have xargs": "If you need xargs, then install xargs. Docker images are intentionally minimal; you are meant to customize them with the tools you need by using them as the base for building your own image.\nFROM fedora:27\n\nRUN yum -y install findutils\nRUN xargs\nIf you're not sure which package provides a command you can also run yum -y install /usr/bin/xargs.",
    "docker could not mount the linux destination /": "Docker won't permit you to bind a host directory or volume to root (/) inside a container. You'll need to bind it at a subdirectory, e.g:\ndocker run -v /Users/xx/var/temp:/var/temp -it alpine bash\nThis would make the directory available at /var/temp inside the container.",
    "Dockerfile unable to execute CMD command (docker for windows)": "I think you may be misunderstanding the nature of the CMD versus ENTRYPOINT vs RUN instruction. Both CMD and ENTRYPOINT take effect at container startup/runtime, whereas RUN is a build-time command. Somewhat confusingly, you use RUN to prepare your container image during the docker build process, while the CMD is what gets run in the container.\nvia the documentation,\nThe main purpose of a CMD is to provide defaults for an executing container...\nAny given container image will only ever have a single CMD instruction.\nThere can only be one CMD instruction in a Dockerfile. If you list more than one CMD then only the last CMD will take effect.\nIf you want to make sure that an executable runs at container start, then you might consider using the ENTRYPOINT instruction; think of ENTRYPOINT as being a command that supplies defaults to the CMD instruction. From the docs:\nDockerfile should specify at least one of CMD or ENTRYPOINT commands. ENTRYPOINT should be defined when using the container as an executable. CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container. CMD will be overridden when running the container with alternative arguments.\nThe RUN command is a build-time instruction that you use to prepare your container image with necessary files, configuration, etc.\nSee this SO question for more on ENTRYPOINT vs CMD.\nED: The case you're looking at, running a SQL script in the container, is a tricky one because the script must not run before the database engine is ready to accept commands. You could run the upgrade script in the ENTRYPOINT instruction as a first step. You may also consider running the SQL upgrade script post-container provisioning, via a docker exec command.\nED2: The example provided:\nENTRYPOINT [\"powershell\", \"-NoProfile\", \"-Command\", \"sqlcmd\"]\nCMD [\"sqlcmd -S database -i C:\\db\\db_scripts\\upgradescript.sql -U sa -P mypassword\"]\ndoes not work because it results in what is probably a nonsense command-line instruction. Remember that the CMD supplies parameters to the ENTRYPOINT instruction, so you'll only specify the powershell sqlcmd in the ENTRYPOINT, leaving CMD to supply the parameters (this is untested, OTOMH):\nENTRYPOINT powershell sqlcmd -S database -U sa -P mypassword -i\nCMD C:\\db\\db_scripts\\upgradescript.sql\nED 3: By combining the two statements and adding a statement terminator (;) to the ENTRYPOINT, you could then allow the standard SQL container .\\Start.ps1 script to run, which enters an infinite loop that prevents the container from stopping immediately (containers run only as long as the executed process in them is running). This guarantees that your startup script is executed, even if the user overrides the CMD instruction at runtime:\nENTRYPOINT powershell \"sqlcmd -S database -U sa -P mypassword -i 'C:\\db\\db_scripts\\upgradescript.sql';\"\nCMD .\\Start.ps1",
    "Is it a bad practice to use version managers like RVM inside docker containers?": "This would be considered a bad practice or anti-pattern in docker. RVM is trying to solve a similar problem that docker is solving, but with a very different approach. RVM is designed for a host or VM with all the tools installed in one place. Docker creates an isolated environment where only the tools you need to run your single application are included.\nContainers are ideally minimalistic, only containing the prerequisites needed for your application, making them more portable. Docker also uses layers and a union filesystem to reuse common base images for each image, so any copy of something like Ruby version X is only downloaded and written to disk once, ever (ignoring updates to that image).",
    "Docker build failing (rpc error: code = 2 desc = \"oci runtime error: exec format error\")": "It's not an issue.\nDocker Hub simply does not support image builds for other architectures than x86.\nMine was ARM.\nBelow response from Docker support:\nThe image that your build is based upon, resin/rpi-raspbian:wheezy-20160518, is an ARM-based image. Cross architecture image builds are not supported on Docker Hub. Only x86-based images can be built in Docker Hub/Cloud.",
    "PHP and Composer with Docker build: failed to clone git": "The accepted solution could work but it could also be insecure because there is a github token hardcoded into the config.json file. See here for a detailed explanation and more secure solution: https://www.previousnext.com.au/blog/managing-composer-github-access-personal-access-tokens\nThe root cause is that github limits the number of api calls made by a client like composer. Composer uses github api to download the files to your vendor directory and when it reaches de limit (https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting) you will see that error message.\nIn certain circumstances (like mine which is not exactly the @DMCoding 's situation), if you have no way to generate the token because, for example, the repository is not under your control, another alternative is to reduce the number of api requests composer does by setting the parameter no-api to true in composer.json, for example:\n\"repositories\": [\n  {\n    \"type\": \"vcs\",\n    \"url\": \"https://github.com/user/repo\",\n    \"no-api\": true\n  }\n]\nComposer will reduce the calls per repo from around 20 to just one: the call to download the zipped repo from github. There is a way to set the option for all packages from github: that way composer will download the zipped repo only once and try to run git pull in every update: https://getcomposer.org/doc/06-config.md#use-github-api",
    "docker + rails + redis - rescue workers are not running": "I was able to solve this by including a new image to be started based on the web image. My new docker-compose.yml file is as follows (added new worker image):\nweb:  \n  build: .\n  command: bundle exec unicorn -p 3000 -c config/unicorn.rb\n  volumes:\n    - .:/fitmo\n    - ../fitmo-core:/fitmo-core\n  ports:\n    - \"3000:3000\"\n  links:\n    - db\n    - redis\n  environment:\n    - REDIS_URL=redis://redis:6379\n\nworker:\n  build: .\n  command: bundle exec rake environment resque:work QUEUE=*\n  volumes:\n    - .:/fitmo\n    - ../fitmo-core:/fitmo-core\n  links:\n    - db\n    - redis\n  environment:\n    - REDIS_URL=redis://redis:6379\n\ndb:\n  build: ./db/docker-files\n  ports:\n    - \"5432\"\n\nredis:  \n  image: redis:latest\n  ports:\n    - \"6379\"\nThere is a subsequent issue if you have AppSignal gem installed and you are extending your jobs with \"extend Appsignal::Integrations::ResquePlugin\" The issue is that the ResquePlugin tries to write a socket to a tmp folder and Docker doesn't allow it.\nI'm sure there is a Docker configuration to allow the socket to be written but I have instead created a development section in my appsignal.yml file and set the active flag to false. The latest version of the Appsignal gem still attempts to write to the socket even when active = false. Version 0.13.0 of the gem (to be released tomorrow) should have a fix for this in place\nappsignal.yml\nproduction:\n  api_key: \"xxxxxxxxxxxxxxxxxxxxxxxx\"\n  active: true\n  slow_request_threshold: 200\nstaging:\n  api_key: \"xxxxxxxxxxxxxxxxxxxxxxxx\"\n  active: true\n  slow_request_threshold: 200\ndevelopment:\n  api_key: \"xxxxxxxxxxxxxxxxxxxxxxxx\"\n  active: false\n  slow_request_threshold: 200",
    "Composer plugins disabled in Docker: Set COMPOSER_ALLOW_SUPERUSER=1 to enable plugins": "If you want to use superuser for composer, you have to include\nENV COMPOSER_ALLOW_SUPERUSER=1\nBefore the composer install command in your Dockerfile.\nHowever, a safer option is to use a non-root user such as USER www-data in your Dockerfile instead of superuser.",
    "Dockerfile with HEREDOC running in bash": "Please put set -eu at the beginning of the heredoc.\nElse builds can succeed when they must fail.\nFROM debian\nRUN <<EOT bash\n  set -eux\n  to_install=(\n    vim\n  )\n  apt-get update\n  failing_command # set -e exit with non-zero status \n  apt-get install -y \"\\${to_install[@]}\"\nEOT\nset -e or set --errexit = If any command fail the script will exit with non-zero value\nset -u or set --nounset = If a variable is unset it will exit with a non zero status\nset -x or set --xtrace = Useful for debugging, print + and the executed command.",
    "How to use a multiarch base docker image when specifying SHA": "Update: Docker Hub now shows the digest for the multi-platform index, so you can use that, or one of the tools listed below to get that digest:\nThe manifest list for a multi-platform image has its own digest, and that is what you want to provide to tools. There are a variety of tools that can get this. My own tool is regclient with the regctl CLI, go-containerregistry from Google has crane, and Docker has been including an imagetools CLI under buildx:\n$ regctl image digest bitnami/minideb\nsha256:713d1fbd2edbc7adf0959721ad360400cb39d6b680057f0b50599cba3a4db09f\n\n$ crane digest bitnami/minideb\nsha256:713d1fbd2edbc7adf0959721ad360400cb39d6b680057f0b50599cba3a4db09f\n\n$ docker buildx imagetools inspect bitnami/minideb\nName:      docker.io/bitnami/minideb:latest\nMediaType: application/vnd.docker.distribution.manifest.list.v2+json\nDigest:    sha256:713d1fbd2edbc7adf0959721ad360400cb39d6b680057f0b50599cba3a4db09f\n           \nManifests: \n  Name:      docker.io/bitnami/minideb:latest@sha256:2abaa4a8ba2c3ec9ec3cb16a55820db8d968919f41439e1e8c86faca81c8674a\n  MediaType: application/vnd.docker.distribution.manifest.v2+json\n  Platform:  linux/amd64\n             \n  Name:      docker.io/bitnami/minideb:latest@sha256:3c44390903734b2657728fcad8fb33dcdf311bdeaafcc3b9f179d78bdf4da669\n  MediaType: application/vnd.docker.distribution.manifest.v2+json\n  Platform:  linux/arm64",
    "Docker Airflow - ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv": "Dockerfile [CORRECT]\nFROM apache/airflow:2.4.1-python3.8\n\n# Compulsory to switch parameter\nENV PIP_USER=false\n\n#python venv setup\nRUN python3 -m venv /opt/airflow/venv1\n\n# Install dependencies:\nCOPY requirements.txt .\n\n# --user   <--- WRONG, this is what ENV PIP_USER=false turns off\n#RUN /opt/airflow/venv1/bin/pip install --user -r requirements.txt  <---this is all wrong\nRUN /opt/airflow/venv1/bin/pip install -r requirements.txt\nENV PIP_USER=true",
    "Could not find a production build in the '/app/.next' directory. Try building your app with 'next build' before starting the production server": "The below command solved my error\nnpm run build",
    "Why is umask setting in dockerfile not working?": "The umask is a property of a process, not a directory. Like other process-related characteristics, it will get reset at the end of each RUN command.\nIf you're trying to make a directory writeable by a non-root user, the best option is to chown it to that user. (How to set umask for a specific folder on Ask Ubuntu has some further alternatives.) None of this will matter if the directory is eventually a bind mount or volume mount point; all of the characteristics of the mounted directory will replace anything that happens in the Dockerfile.\nIf you did need to change the umask the only place you can really do it is in an entrypoint wrapper script. The main container process can also set it itself.\n#!/bin/sh\n# entrypoint.sh\numask 000\n# ... other first-time setup ...\nexec \"$@\"",
    "Selenium inside Docker image for Java application": "your dockerfile for Chromium (not chrome) is correct. you have to add in java code:\nchromeOptions.setBinary(\"/usr/bin/chromium-browser\");\nI have also working version: jdk 11 (ubuntu) + chrome (for selenium):\n# on ubuntu\nFROM azul/zulu-openjdk:11\n\nRUN apt-get update && apt-get install -y \\\ngnupg2 \\\nwget \\\nless \\\n&& rm -rf /var/lib/apt/lists/*\n\n#######################\n# Google Chrome\n\n# Adding trusting keys to apt for repositories\nRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | \napt-key add -\n# Adding Google Chrome to the repositories\nRUN sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ \nstable main\" >> /etc/apt/sources.list.d/google-chrome.list'\n# Updating apt to see and install Google Chrome\nRUN apt-get -y update\n# Magic happens\nRUN apt-get install -y google-chrome-stable\n# Installing Unzip\nRUN apt-get install -yqq unzip\n\n\n#######################\n# Google Chrome Driver - now i have driver in my java app (i need both \njava app version on docker and non-docker)\n\n# Download the Chrome Driver\n#RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | \napt-key add -\n#RUN sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ \nstable main\" >> /etc/apt/sources.list.d/google-chrome.list'\n#RUN apt-get -y update\n#RUN apt-get install -y google-chrome-stable\n# install chromedriver\n#RUN apt-get install -yqq unzip\n#RUN wget -O /tmp/chromedriver.zip \nhttp://chromedriver.storage.googleapis.com/`curl -sS \n chromedriver.storage.googleapis.com/LATEST_RELEASE`/chromedriver_linux64.zip\n#RUN unzip /tmp/chromedriver.zip chromedriver -d /usr/local/bin/\n\n#######################\n\nRUN apt-get update\n\n\n# Set display port as an environment variable\nENV DISPLAY=:99\n\n\nARG JAR_FILE=target/*.jar\nCOPY ${JAR_FILE} bot-app.jar\nENTRYPOINT [\"java\",\"-jar\",\"/bot-app.jar\"]\nIt works fine.",
    "how to connect to standalone selenium-firefox container from another container": "You need to create docker-compose.yml file with all of the containers what you're going to create:\nversion: '3.8'\nservices:\n  chrome:\n    image: selenium/standalone-chrome:85.0\n    hostname: chrome\n    ports:\n      - \"4444:4444\"\n  e2e-tests:\n    build: .\n    depends_on:\n      - chrome\nand use host name 'chrome' inside container what is going to use it like:\ncls.driver = webdriver.Remote(command_executor='http://chrome:4444/wd/hub',desired_capabilities=DesiredCapabilities.CHROME)",
    "WORKDIR as VOLUME": "While you can mount a volume over the WORKDIR that you were using when building your image, the volume isn't available at build time. Volumes are only available for a container, not while building an image.\nYou can COPY files into the image to represent the content that will exist in the volume once a container is running, and use those temporary files to complete the building of the image. However, those exact files would be inaccessible once a volume is mounted in that location.\nTo have a directory from the host machine mounted inside a container, you would pass a -v parameter (you can do multiple -v params for different directories or for individual files) to the docker run command that starts the container:\ndocker run -v /var/lib/docker/volumes:/full/path/inside/container your_image_name",
    "Docker-compose can't import Django (ModuleNotFoundError)": "Hi I just solved the problem. After you run\ndocker build .\nrun the docker-compose build instead of docker-compose up.\nAnd then finally run docker-compose up",
    "docker run python script in CMD with environment variables [duplicate]": "You need to run with sh -c to evaluate the environment variables:\nCMD [\"sh\", \"-c\", \"python script.py --pool ${POOL}\"]",
    "have different base image for an application in docker": "You can use multiple FROM in the same Dockerfile, provided you are doing a multi-stage build\nOne part of the Dockerfile would build an intermediate image used by another.\nBut that is generally use to cleanly separate the parents used for building your final program, from the parents needed to execute your final program.",
    "Docker MongoNetworkError: connection timed out when build Node JS App by docker": "mongo were installed on container so please make sure:\nExpose 27017 port when you run to build container,\ndocker run -p 27017:27017 ....\nIn node code, connect to mongo container using : IP address gateway of container network is 172.17.0.1:27017\nSo:\nlet dbRoute = \"mongodb://172.17.0.1:27017/kgp_news\"",
    "How to I expand arguments in a Docker RUN command using Powershell?": "Arguments passed to a Powershell command run via RUN are not substituted by Docker, but by Powershell itself. They are treated like normal environment variables by Powershell, so the correct syntax is:\nFROM microsoft/nanoserver\nSHELL [\"powershell\"]\nARG mydir=tmp\nRUN mkdir $env:mydir\nSo in the same way you can also expand normal environment variables:\nRUN mkdir $env:LOCALAPPDATA\\$env:mydir\nNote that this is only valid within the context of a RUN command. With other Docker commands variables still expand using the normal notation:\nCOPY ./* $mydir\nIt's confusing on Windows/Powershell as on Linux containers using a bash shell the notation is the same in both cases.",
    "Start particular service from docker-compose": "It's very easy:\n$ docker compose up <service-name>\nIn your case:\n$ docker compose -f docker-compose.yml up setup -d\nTo stop the service, then you don't need to specify the service name:\n$ docker compose down\nwill do.\nLittle side note: if you are in the directory where the docker-compose.yml file is located, then docker-compose will use it implicitly, there's no need to add it as a parameter. You need to provide it in the following situations:\nthe file is not in your current directory\nthe file name is different from the default one, eg. myconfig.yml",
    "Duplication in Dockerfiles": "I will suggest to use one Dockerfile and just update your CMD during runtime. Litle bit modification will work for both local and Heroku as well.\nAs far Heroku is concern they provide environment variable to start container with the environment variable. heroku set-up-your-local-environment-variables\n    FROM alpine:3.7\n\nENV PYTHONUNBUFFERED 1\nENV APPLICATION_TO_RUN=default_application\n\nRUN mkdir /code\nWORKDIR /code\nCOPY Pipfile Pipfile.lock ./\n\nRUN apk update && \\\n apk add python3 postgresql-libs jpeg-dev git && \\\n apk add --virtual .build-deps gcc python3-dev musl-dev postgresql-dev zlib-dev && \\\n pip3 install --no-cache-dir pipenv && \\\n pipenv install --system && \\\n apk --purge del .build-deps\n\nCOPY . ./\n\n# Run the image as a non-root user\nRUN adduser -D noroot\nUSER noroot\n\nEXPOSE $PORT\n\nCMD $APPLICATION_TO_RUN\nSo When run you container pass your application name to run command.\ndocker run -it --name test -e APPLICATION_TO_RUN=\"celery beat\" --rm test",
    "Get Host IP in Dockerfile": "You need to use build-time variables (\u2013build-arg).\nThis flag allows you to pass the build-time variables that are accessed like regular environment variables in the RUN instruction of the Dockerfile.\nSo, Dockerfile is modified to:\nARG IP_ADDRESS\nRUN ... && echo \"xdebug.remote_host=$IP_ADDRESS\" >> /usr/local/etc/php/conf.d/xdebug.ini`\nAnd you just need to define build-time variable IP_ADDRESS during image building:\n$ docker build --build-arg IP_ADDRESS=<IP_ADDRESS> .\nIf you use docker-compose:\n1. Create file .env with the following content:\nIP_ADDRESS=\"<IP_ADDRESS>\"\nYou can make it every time like (example is for a linux machine):\nIP_ADDRESS=$(ip a | grep <interface> | grep inet | awk '{print $2}' | awk -F'/' '{print $1}')\necho \"IP_ADDRESS=$IP_ADDRESS\" > .env\n2. Use the following docker-compose.yaml to build your image:\nversion: '3'\nservices:\n  myservice:\n    build:\n      context: .\n      args:\n        IP_ADDRESS: ${IP_ADDRESS} \n3. Build the above image:\ndocker-compose build",
    "Docker: created files disappear between layers": "The upstream maven image defines this directory as a volume. Once an image does this, you cannot reliably make changes to that directory in the image.\nFrom their Dockerfile:\nARG USER_HOME_DIR=\"/root\"\n...\nVOLUME \"$USER_HOME_DIR/.m2\"\nThe Dockerfile documentation describes this behavior:\nChanging the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\nYour options are to:\nUse another directory for your build\nRequest that the upstream image removes this VOLUME definition\nBuild your own image without this definition (it's fairly easy to fork their repo and do your own build)\nFor more details, you can see an old blog post by me about this behavior and the problems it creates.",
    "Pull ssh-client container via docker-compose": "Write a Dockerfile (https://docs.docker.com/engine/reference/builder/):\nFROM alpine:latest\n# install ssh-client and bash\nRUN apk --no-cache add openssh-client bash\n# example ssh usage to print version\nENTRYPOINT [\"ssh\", \"-V\"]\nBuild and run it with: docker build -t ssh . && docker run -t ssh ssh Or use Docker-Compose.\nAfter building with docker build you can reuse the ssh Docker image in your other projects in Dockerfiles.\nFROM ssh\n...\nUsing Docker-Compose docker-compose.yml:\nversion: '3'\nservices:\n  ssh:\n    build: .\n    image: ssh\nYou can add multiple services in the compose file as you like. The docker compose file from above uses the Dockerfile from above to build your ssh docker image. See the compose file reference: https://docs.docker.com/compose/compose-file/",
    "Building Dockerfile - Retaining files between intermediate containers": "The keymetrics/pm2 image contains the following line in their Dockerfile:\nVOLUME [\"/app\"]\nThat breaks any later steps and child images from modifying the /app directory. From docker's documentation:\nChanging the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\nYou'll need to run your build in a different directory, make a different base image without that line in it, or convince the upstream author to remove that line.\nI've blogged about the issues with volumes defined in Dockerfiles before, and this is one of those issues. There's really no upside to having this line in their Dockerfile.",
    "start Mongodb container as non root user": "Using docker-compose:\nversion: '3.5'\nservices:\n    yourmongo:\n        image: mongo:xenial\n        container_name: yourmongo\n        restart: always\n        user: 1000:1000\n        volumes:\n            - ./yourmongo-folder:/data/db\n1000:1000 are the chosen values for uid:gid (user-id:group-id)\nEDIT: don't forget to add the folder with the correct permissions: mkdir yourmongo-folder && chown 1000:1000 yourmongo-folder (This must be done on the Docker host machine BEFORE bringing up the Docker service).",
    "How can I provide application config to my .NET Core Web API services running in docker containers?": "It's possible to create data volumes in the docker image/container. And also mount a host directory into a container. The host directory will then by accessible inside the container.\nAdding a data volume\nYou can add a data volume to a container using the -v flag with the docker create and docker run command.\n$ docker run -d -P --name web -v /webapp training/webapp python app.py\nThis will create a new volume inside a container at /webapp.\nMount a host directory as a data volume\nIn addition to creating a volume using the -v flag you can also mount a directory from your Docker engine\u2019s host into a container.\n$ docker run -d -P --name web -v /src/webapp:/webapp training/webapp python app.py\nThis command mounts the host directory, /src/webapp, into the container at /webapp.\nRefer to the Docker Data Volumes",
    "DockerFile : how to get bash command line after start?": "bash won't run at all if stdin is closed. If you don't provide the -i flag, bash will simply exit immediately. So when you...\ndocker run environment:full \n...bash exits immediately, and so your container exits. You would see it if you ran docker ps -a, which shows container that have stopped.\nbash won't give you an interactive prompt if it's not attached to a tty. So if you were to run...\ncoerk run -i environment:full\n...you would get a bash shell, but with no prompt, or job control, or other features. You need to provide -t for Docker to allocate a tty device.\nYou can't get what you want without providing both the -i and -t options on the command line.\nAn alternative would be to set up an image that runs an ssh daemon, and have people ssh into the container. Instead of behaving \"like if I was in a SSH connection\", it would actually be an ssh session.\nAlso, note that this:\ndocker run environment:full -ti\nIs not the same as this:\ndocker run -it environment:full\nThe former will run bash -ti inside a container, while the latter passes the -i and -t options to docker run.",
    "Docker with a new nameserver": "So, one of the ways you can add new DNS information to your container's build process is by adding some startup options to your Docker daemon. The documentation for that process reveals that the option you'll use is --dns. The location of your configuration file depends on your specific distro. On my Linux Mint machine, the file is in /etc/default/docker. On Linux Mint, look for the DOCKER_OPTS= line, and add the appropriate --dns=x.x.x.x entries to that line.\nFor example, if you want to use Google's DNS, you should change that line to look like this:\nDOCKER_OPTS=\"--dns=8.8.4.4 --dns=8.8.8.8\"\nAdditionally, in the absense of --dns or --dns-search startup options, Docker will use the /etc/resolv.conf of the host it's running on instead.",
    "Docker : using pv (pipe viwer) output nothing on build process": "Use pv -f or pv --force and it will work the way you expect it to. Quoting the man page for pv:\n   -f, --force\n          Force output.  Normally, pv will not output any visual display if standard error is no  a  terminal.\n          This option forces it to do so.\nI don't know if this was an option three+ years ago when this question was posed, but it works now.\nWith this in my Dockerfile like so:\nRUN echo \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\" | pv -f -L 5 -s57 > /dev/null\nI get the following as an example output along the way:\nStep 3/3 : RUN echo \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\" | pv -f -L 5 -s57 > /dev/null\n  ---> Running in 57c657f93610\n25.0 B 0:00:05 [5.21 B/s] [==============>                     ] 44% ETA 0:00:06\n... also, to get the \"100%\" right, you either need to run pv -s57 or echo -n to suppress the newline.",
    "Will apt update run every time in my docker build?": "The layer generated by the RUN apt update -y command is cached and the command will not run again unless the cache is invalidated, which can happen in a number of different ways:\nYou explicitly disable the cache by using the --no-cache option.\nYou modify parts of your Dockerfile that precede the apt update line.\nYou update the base image (e.g. via docker pull or docker build --pull).\nYour Dockerfile copies a file into the image that has changed since the last time the image was built.",
    "Running docker containers with .net6": "I was also facing the same issue, however, it got it fixed after replacing below line in the docker file.\nOld:\nENTRYPOINT [\"dotnet\", \"DotNet.Docker.dll\"] \nNew:\nENTRYPOINT [\"dotnet\", \"SampleWebApplication.dll\"]\nUse your_application_name.dll in place of SampleWebApplication.dll.",
    "Docker: non-root user does not have writing permissions when using volumes": "You can run the container as the user ID matching the host user ID owning the directory. Often this is the current user:\n docker run -u $(id -u) -v /host/path:/container/path ...\nFor this to work, your image needs to do a couple of things:\nThe data needs to be kept somewhere completely separate from the application code. A top-level /data directory as you show is a good choice.\nThe application proper should be owned by root, and world-readable but not world-writeable; do not RUN chown ... the application, just COPY it in and run its build sequence as root.\nThe image should create a non-root user, but it does not need to match any particular host user.\nThe image needs to create the data directory, but it should be completely empty.\nThe image startup (often an entrypoint wrapper script) needs to be able to populate the data directory if it is totally empty at startup time.\nFROM some-base-image\n\n# Do all of the initial setup and build as root\nWORKDIR /app\nCOPY . .\nRUN ...\n\n# Create some non-root user that owns the data directory by default\nRUN useradd -r myuser          # no specific user ID\nRUN mkdir /data && chown myuser /data\n# VOLUME [\"/data\"]             # optional, the only place VOLUME makes sense at all\n\n# Specify how to run the container\nUSER myuser                    # not earlier than this\nEXPOSE ...                     # optional but good practice\nENTRYPOINT [\"/entrypoint.sh\"]  # knows how to seed /data, then `exec \"$@\"`\nCMD my_app ...                 # a complete command line",
    "How to enable a native module in nginx alpine docker image": "nginx is built with --with-http_realip_module by default.\nyou can verify whether the module is built by examine nginx:alpine docker images\n$ docker run --rm nginx:alpine nginx -V\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\n10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf\n10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh: Configuration complete; ready for start up\nnginx version: nginx/1.19.6\nbuilt by gcc 9.3.0 (Alpine 9.3.0)\nbuilt with OpenSSL 1.1.1g  21 Apr 2020 (running with OpenSSL 1.1.1i  8 Dec 2020)\nTLS SNI support enabled\nconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --with-perl_modules_path=/usr/lib/perl5/vendor_perl --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-Os -fomit-frame-pointer' --with-ld-opt=-Wl,--as-needed\nas you can see --with-http_realip_module is there.\nall you need to do, is to configure nginx appropriately (in nginx.conf)\nfor instance:\nset_real_ip_from  192.168.1.0/24;\nset_real_ip_from  192.168.2.1;\nset_real_ip_from  2001:0db8::/32;\nreal_ip_header    X-Forwarded-For;\nreal_ip_recursive on;",
    "Docker run returning exit code 100 or 51": "Apparently I didn't have enough storage left on my server which caused the error.",
    "Cannot start container: OCI runtime create failed: container_linux.go:349": "I had a similar issue:\nOCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:722: waiting for init preliminary setup caused: EOF: unknown\nand the problem turned out to be the wrong version of my WSL distro, which was 1 instead of 2:\nPS C:\\Users\\myself> wsl -l -v\n  NAME      STATE           VERSION\n* Ubuntu    Running         1\nSo I used the wsl --set-version command to upgrade it:\nPS C:\\Users\\myself> wsl --set-version Ubuntu 2\nPS C:\\Users\\myself> wsl -l -v\n  NAME      STATE           VERSION\n* Ubuntu    Running         2\nThen I was able to successfully build my Docker image.\nHope can help someone.",
    "Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-xtrlkujj/pyaudio/": "Try run first:\npip3 install --upgrade setuptools\nto make sure you have the recent version from the Setuptools package. That sometimes fixes that problem.\nAnd if pip needs setup tools to work. So if you don't have it you need to install it with\napt-get install python3-setuptools",
    "Docker: The command returned a non-zero code: 137": "As described, the command RUN appmanage.py appconfig appAdd.json run successfully as expected and reported that System check identified no issues (0 silenced)..\nMoreover, the command \"insisted\" on killing itself and return exit code of 137. The minimum changes for this to work is to update your Dockerfile to be like\n...\n#Run a script\n#Note: Here appmanage.py is a file inside the pip installed location(site-packages), but it will be accessible directly without cd to the folder\nRUN appmanage.py appconfig appAdd.json || true\n...\nThis will just forcefully ignore the return exit code from the previous command and carry on the build.",
    "SSH Tunnel within docker container": "The solution was actually quite simple and works without using -net=host. I needed to bind to 0.0.0.0 and use the Gateway Forwarding Option to allow remote hosts (the database client) to connect to the forwarded ports.\nssh -g -L *:<hostport>:localhost:<mqtt-port/remote port> <user>@<remote-ip>\nOther containers within the same Docker bridge network can then simply use the connection string <name-of-ssh-container>:<hostport>.",
    "How to multi-line echo json from a Dockerfile": "You can use \\n\\ this will keep the file in structure as well, as sometime new line is required in some config file while with \\ this it will print in a single line.\nFROM node:8\nRUN echo '{ \\n\\\n    \"a\" : \"b\", \\n\\\n    \"c\" : \"d\", \\n\\\n    \"e\" : \"f\" \\n\\\n}'",
    "Dockerfile : How to mount host directory in container path?": "You can not mount a volumn in Dockerfile\nBecause:\nDockerfile will build an image, image is independent on each machine host.\nImage should be run everywhere on the same platform for example on linux platform it can be running on fedora, centos, ubuntu, redhat...etc\nSo you just mount volumn in to the container only. because container will be run on specify machine host.\nHope you understand it. Sorry for my bad English.",
    "Running Angular app as docker image using Node js": "your Dockerfile does not run/serve your application, in order to do that you have to:\ninstall angular/cli\ncopy the app\nrun/serve the app\nFROM node:10.15.3\n\nRUN npm config set strict-ssl false \\\n    && npm config set proxy http://proxy.xxxxxx.com:8080\n\n# get the app\nWORKDIR /src\nCOPY . .\n\n# install packages\nRUN npm ci\nRUN npm install -g @angular/cli\n\n# start app\nCMD ng serve --host 0.0.0.0\nhope this helps.",
    "Docker How to save and restore WORKDIR in Dockerfile?": "You can create an environment variable.\nENV WORKDIR=/my/path/\nWORDIR $WORKDIR",
    "Code 14 when Dockerizing Web App Using node.js and MongoDB": "I am assuming you are running Docker on Windows, and if so, I experienced the same issue and found the answer in the below post:\nWindows Docker mongo container doesn't work with volume mount\nThe problem is that the volume mount is specified as a host volume. I resolved my issue by changing my volume mount to be a named volume. If you need to use host, you might be able to use the rsync tool specified in the answer to the question I linked.\nMy docker-compose.yml file\nversion: '3'\nservices:\n\n  mongodb1:\n    image: mongo:latest\n    restart: always\n    volumes:\n      - data1:/data/db\n      - config1:/data/configdb\n    ports:\n      - 30001:27017\n    entrypoint: [ \"/usr/bin/mongod\", \"--bind_ip_all\", \"--replSet\", \"rs0\" ]\nvolumes:\n  data1:\n  config1:",
    "install .net framework 4.7.2 in docker": "Instead of Installing the NET-Framework yourself, you could use\nFROM microsoft/aspnet\nor\nFROM microsoft/dotnet-framework:4.7.2\nto use an image with dotnet framework already installed.\nor whatever version you need.\nSee https://hub.docker.com/u/microsoft/ for all the images on docker hub",
    "invalid reference format when building a Docker Image": "Since you always base on the same repo, it's cleaner to do like this.\nARG VERSION\nFROM some/repository:${VERSION} as builder\nRUN mkdir test\nFROM scratch\nCOPY --from=builder /test /\nSource:\nhttps://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact",
    "Cannot access server running in container from host": "From the information provided in the question if you see logs of the application (docker logs <container_id>) than the docker application starts successfully and it looks like port exposure is done correctly.\nIn any case in order to see ports mappings when the container is up and running you can use:\ndocker ps \nand check the \"PORTS\" section\nIf you see there something like 0.0.0.0:3000->3000/tcp\nThen I can think about some firewall rules that prevent the application from being accessed...\nAnother possible reason (although probably you've checked this already) is that the application starts and finishes before you actually try to access it in the browser.\nIn this case, docker ps won't show the exited container, but docker ps -a will.\nThe last thing I can think of is that in the docker container itself the application doesn't really answer the port 3000 (I mean, maybe the startup script starts the web server on some other port, so exposing port 3000 doesn't really do anything useful).\nIn order to check this you can enter the docker container itself with something like docker exec -it <container_id> bash\nand check for the opened ports with lsof -i or just wget localhost:3000 from within the container itelf",
    "How do you create Dockerfile to RUN command with \"&\"?": "When trying to run multiple commands with background process you have to group the command and & using ().\nso The run statement should look as follows.\nRUN set -ex \\\n&& apt-get update -yqq \\\n&& apt-get upgrade -yqq \\\n&& apt-get install -yqq --no-install-recommends \\\n    python3-pip \\\n    python3-requests \\\n    software-properties-common \\\n    python-software-properties \\\n    xvfb \\\n&& ( Xvfb :99 & ) \\\n&& export DISPLAY=:99\nReference: Run multiple commands as bg in Linux",
    "Apt-get not working within ubuntu dockerfile": "Execute 'apt-get update' and 'apt-get install' in a single RUN instruction. This is done to make sure that the latest packages will be installed. If 'apt-get install' were in a separate RUN instruction, then it would reuse a layer added by 'apt-get update', which could have been created a long time ago.\nRUN apt-get update && \\\n    apt-get install -y git-core\nNote: RUN instructions build your image by adding layers on top of the initial image.",
    "how to run gulp task from dockerfile": "You have to install gulp globally AND locally:\nRUN npm install -g gulp\nRUN npm install gulp\nSee this post for a discussion about the reason.",
    "docker entrypoint with multiple arguments": "Put the two commands in a shell script, COPY the shell script in the Dockerfile, then use that shell script as the entrypoint.\ndocker-entrypoint.sh:\ngeth --datadir /home/ubuntu/eth-dev init /home/ubuntu/eth-dev/genesis.json \ngeth --networkid 45634 --verbosity 4  --ipcdisable --rpc --port 30301 --rpcport 8545 --rpcaddr 0.0.0.0 console 2>> /home/ubuntu/eth-dev/eth.log\nDockerfile:\nCOPY docker-entrypoint.sh /usr/bin/docker-entrypoint.sh\nENTRYPOINT [\"/usr/bin/docker-entrypoint.sh\"]\nBe sure to chmod +x the script, either before copying or in a RUN command in the Dockerfile.",
    "Docker building fails: returned a non-zero code: 1": "Someone posted an answer and deleted it before I could accept it. But here it is -\nFROM ubuntu:latest\nMAINTAINER ME\n\nRUN apt-get update && apt-get install -y \\\nnet-tools inetutils-traceroute \\\niputils-ping xinetd telnetd\nThis works!!",
    "Docker cannot find file when building the image": "When you do\nRUN cd /app/\nRUN pip install -r requirements.txt\nDocker will enter the /app/project, and then, will come back to the previous folder, then, the second command will fail. You have to use it like this:\nRUN cd /app/ && pip install -r requirements.txt\nYou can visualize how Django works when using Docker on this link that has a container running Django with a similar Dockerfile.\nEDIT: To match a comment, you should replace the export line to an ENV, example:\nENV DJANGO_SETTINGS_MODULE project.settings",
    "Command line templating in Dockerfiles": "You can use a build argument and use docker build --build-arg foo=bar to build the image.",
    "\"Value cannot be null. Parameter name: userName\" error when installing MSSQL 2014 Express in Docker": "So I've finally found what causes the problem. The error occurs when using powershell as default shell (SHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"])\nI made it work by skipping SHELL command and running installations from default command line.\nRUN /SQLEXPR_x64_ENU.exe /qs /x:setup\nRUN /setup/setup.exe /q /ACTION=Install /INSTANCENAME=SQLEXPRESS /FEATURES=SQLEngine /UPDATEENABLED=0 /SQLSVCACCOUNT=\"NT AUTHORITY\\System\" /SQLSYSADMINACCOUNTS=\"BUILTIN\\ADMINISTRATORS\" /TCPENABLED=1 /NPENABLED=0 /IACCEPTSQLSERVERLICENSETERMS\nMSSQL 2008, 2012 and 2016 all work ok with powershell, only 2014 has this issue.",
    "Cannot hit docker container running locally": "The script is listening to 127.0.0.1 in the container, making it inaccessible from the host. It must listen to 0.0.0.0.",
    "How can I add dos2unix to an Alpine based docker container?": "From your own link, dos2unix is (at this time, February 2017) only in testing, not in main or community. From the relevant documentation --\nIf you only have the main repository enabled in your configuration, apk will not include packages from the other repositories. To install a package from the edge/testing repository without changing your repository configuration file, use the command below. This will tell apk to use that particular repository.\napk add cherokee --update-cache --repository http://dl-3.alpinelinux.org/alpine/edge/testing/ --allow-untrusted\nIn this case, you would want to substitute dos2unix for cherokee.",
    "Dockerized Django with webpackDevServer": "Thanks to this SO thread I found the solution.\ndocker-compose.yml\nversion: '2'\nservices:\n  webpack:\n    build:\n      context: .\n      dockerfile: docker/webpack\n    volumes_from:\n      - webapp:rw\n\n  webapp:\n    build:\n      context: .\n      dockerfile: docker/webapp\n    command: python manage.py runserver 0.0.0.0:8000\n    volumes:\n      - .:/code\n    ports:\n      - \"8000:8000\"\ndocker/webapp\nFROM python:latest\nENV PYTHONUNBUFFERED 1\n\n# set working directory to /code/\nRUN mkdir /code\nWORKDIR /code\n\n# add requirements.txt to the image\nADD ./requirements.txt /code/\n\n# install python dependencies\nRUN pip install -r requirements.txt\n\nADD . /code/\n\n# Port to expose\nEXPOSE 8000\ndocker/webpack\nfrom node:latest\n\nRUN npm install webpack -g\n\nADD docker/start-webpack.sh .\nRUN chmod +x /start-webpack.sh\n\nCMD ./start-webpack.sh\ndocker/start-webpack.sh\n#!/usr/bin/env bash\n\nuntil cd /code && npm install\ndo\n  echo \"Retrying npm install\"\ndone\n\nwebpack --watch --watch-polling",
    "How use `echo` in a command in docker-compose.yml to handle a colon (\":\") sign?": "The colon is how YAML introduces a dictionary. If you have it in a value, you just need to quote the value, for example like this:\nimage: \"elasticsearch:2.4\"\nOr by using one of the block scalar operators, like this:\ncommand: >\n  /bin/bash -c \u201cecho 'http.cors.enabled: true' > /usr/share/elasticsearch/config/elasticsearch.yml\"\nFor more information, take a look at the YAML page on Wikipedia. You can always use something like this online YAML parser to test out your YAML syntax.\nProperly formatted, your first document should look something like:\nelasticsearch:\n  ports:\n    - 9200:9200/tcp\n  image: \"elasticsearch:2.4\"\n  volumes:\n    - /data/elasticsearch/usr/share/elasticsearch/data:/usr/share/elasticsearch/data\n  command: >\n    /bin/bash -c \u201cecho 'http.cors.enabled: true' > /usr/share/elasticsearch/config/elasticsearch.yml\"\n(The indentation of the list markers (-) from the key isn't strictly necessary, but I find that it helps make things easier to read)\nA docker container can only run a single command. If you want to run multiple commands, put them in a shell script and copy that into the image.",
    "Updating jar in docker image": "It looks like you missed a point of docker containers/images. New app version ==> new Docker image is true path and sounds to be what you want.\nAnd if only last layer is changed (that is the case with Dockerfile you provided) docker push will only upload the new layer of the image. So in this case something about the size of app.jar...",
    "Does Dockerfile always have to be placed in the build context root?": "Docker 1.5 introduced an option to \"Specify the Dockerfile to use in build\":\nSpecify the Dockerfile to use in build\nContributed by: Doug Davis \u2013 Link to PR\nThis is possibly one of the most requested feature in the past few months: the ability to specify the file to use in a docker build rather than relying on the default Dockerfile. docker build -f allows you to define multiple Dockerfiles in a single project and specify which one to use at build time. This can be helpful if you require separate Dockerfiles for testing and production environments.\nWith this feature you don't need to have separated folders for multiple Dockerfiles, so the need of having different folders for each Dockerfile is not need any more. Simply have in your root folder multiple Dockerfiles and build each one with -f option.",
    "How to install torch without nvidia?": "As (roundaboutly) documented on pytorch.org's getting started page, Torch on PyPI is Nvidia enabled; use the download.pytorch.org index for CPU-only wheels:\nRUN pip install torch --index-url https://download.pytorch.org/whl/cpu\nAlso please remember to specify a somewhat locked version of Torch, e.g.\nRUN pip install torch~=2.4.0 --index-url https://download.pytorch.org/whl/cpu",
    "Running Puppeteer within a docker container": "The solution was to try and install the drivers. This did it:\nFROM node:20\n\nEXPOSE 8181\n\n# Install Google Chrome Stable and fonts\n# Note: this installs the necessary libs to make the browser work with Puppeteer.\nRUN apt-get update && apt-get install gnupg wget -y && \\\n  wget --quiet --output-document=- https://dl-ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor > /etc/apt/trusted.gpg.d/google-archive.gpg && \\\n  sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list' && \\\n  apt-get update && \\\n  apt-get install google-chrome-stable -y --no-install-recommends && \\\n  rm -rf /var/lib/apt/lists/*\n\n# Create app directory\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\n\nRUN npm install\nRUN npm install --save-dev\n\nCOPY . .\n\nCMD npm test",
    "Docker apt-get update fails during building phase": "I belive that's because Ubuntu 22.10 has reached End of Life on July 20 2023.\nBetter to use a LTS version. e.g. Ubuntu 22.04",
    "Removed Docker image is reappearing again upon new build command": "It's building from the cache. Since no inputs appear to have changed to the build engine, and it has the steps from the previous build, they are reused, including the image creation date.\nYou can delete the build cache. But I'd recommend instead to run:\ndocker build --pull --no-cache -f dockerfile -t ***_seis:latest .\nThe --pull option pulls a new base image should you have an old version pulled locally. And the --no-cache option skips the caching for various steps (in particular a RUN step that may fetch the latest external dependency).",
    "docker image ls is not showing the installed base images (\"FROM\" images)": "It looks like you're using Docker buildkit.\nWhile the traditional docker build mechanism would pull referenced images into the local image collection, buildkit has its own caching mechanism and the images it pulls won't show up in the output of docker image ls.",
    "Docker wipes out mongoDB container data": "You can create an external volume and add the data of the mongoDB into it. That way your data doesn't get wiped even when you turn off your docker-compose.\nversion: '3'\nservices:\n  node:\n    restart: always\n    build: ./nodeServer\n    container_name: nodeserver\n    ports:\n      - 5000:5000\n    depends_on:\n      - database\n    networks:\n      twitter_articles:\n        ipv4_address: 172.24.0.2 \n    environment:\n      - TZ=Europe/Athens\n  database:\n    restart: always\n    build: ./mongoDump/database\n    container_name: mongodb\n    ports:\n      - 27017:27017\n    networks:\n      twitter_articles:\n        ipv4_address: 172.24.0.4\n    volumes: \n      - mongo_data:/data/db\n    environment:\n      - TZ=Europe/Athens\n  pythonscript:\n    restart: always\n    build: ./python\n    container_name: pythonscript\n    depends_on:\n      - database\n    networks:\n      twitter_articles:\n        ipv4_address: 172.24.0.3 \n    environment:\n      - TZ=Europe/Athens\nnetworks:\n  twitter_articles:\n    ipam:\n      config:\n        - subnet: 172.24.0.0/24\nvolumes:\n  mongo_data:\n    external: true\nnow you have to create a volume in your docker using\n docker volume create --name=mongo_data\nthen docker-compose down and\n docker-compose up --build -d",
    "`apt-get update` never completes in Docker build": "\"Hello, have you tried turning it off and on again\"?\nI just needed to restart the docker daemon.",
    "permission denied creating directories as non-root in docker": "That happens because your /test directory was created by root, and by default won't allow any other users to create anything in it. To change ownership to the user you want, you can use chown before your USER testuser step:\nRUN chown testuser /test\nIf there are already files inside the directory, you will need to pass the -R flag to change the permission recursively:\nRUN chown -R testuser /test\nAnother option would be giving the directory red+write+execute permissions for all users. However, this is probably NOT what you want, the above should serve you well for almost all cases.\nRUN chmod 777 /test",
    "Docker Can't Find Install.sh": "The cause of the issue was saving the install.sh file using Windows line endings. It needed to be saved using Unix line endings. In Visual Studio Code, change the \"CRLF\" button to \"LF\".",
    "How to run singularity container on HPC cluster? - ERROR : Failed to create user namespace: user namespace disabled": "Short answer:\nbug your HPC admins to install Singularity\nLonger answer:\nThere are two ways to install Singularity, as a privileged installation or an unprivileged / user namespace installation. The first way is the default, but requires sudo/root for certain actions (mostly singularity build). The latter removes the need for root, but has other system requirements. It's possible additional OS configuration is necessary for Singularity to function as expected.\nIn addition to privileged/unprivileged installations, disk storage in clusters is usually on NFS or another networked/distributed filesystem so all nodes have access to the same data. Unfortunately, as is usually the case any time it is involved, NFS is a likely cause for your problem. Singularity relies on SUID for its core functionality, but for (quite good) security reasons SUID is disabled on NFS by default. It is unlikely the cluster admins will enable that option, so your best bet is to ask them install it locally on whichever computer/interactive nodes you need it on.",
    "ERROR: cannot start nginx as networking would not start on alpine docker image": "I managed to get Nginx to work within anapsix/alpine-java:7_jdk image after seeing this amazing answer.\nHere is a working Dockerfile :\nFROM anapsix/alpine-java:7_jdk\n\nCOPY script.bash .\n\nRUN apk --update add nginx openrc\nRUN openrc\nRUN touch /run/openrc/softlevel\n\nCMD bash ./script.bash\nand here is the script.bash used in CMD :\n#!/bin/bash\n\n# Tell openrc loopback and net are already there, since docker handles the networking\necho 'rc_provide=\"loopback net\"' >> /etc/rc.conf\n\n# get inside the container bash\nbash\nafter building the image using docker build . -t nginx_alpine_java run the following commands :\ndocker run -it -p 80:80 nginx_alpine_java\nnow we are inside our container bash\nbash-4.3# rc-service nginx status\n * status: stopped\n\nbash-4.3# rc-service nginx start\n * /run/nginx: creating directory\n * /run/nginx: correcting owner                                [ ok ]\n * Starting nginx ...                                          [ ok ]\nI hope that it will work with you.",
    "From the command line (apk --update add), how to install node.js 'Current latest features' version, instead of 'LTS recommended for most users'?": "See this:\n\"edge\" is the name given to the current development tree of Alpine Linux. It consists of a APK repository called \"edge\" and contains the latest build of all available Alpine Linux packages. Those packages are updated on a regular basis.\nAnd, nodejs-current is the package name for latest feature release, so you could use next:\napk add --no-cache nodejs-current --repository=\"http://dl-cdn.alpinelinux.org/alpine/edge/community\"\nWhole run:\n/ # apk update\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.14/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.14/community/x86_64/APKINDEX.tar.gz\nv3.14.2-38-g27e4ada230 [https://dl-cdn.alpinelinux.org/alpine/v3.14/main]\nv3.14.2-36-g70ff2140e8 [https://dl-cdn.alpinelinux.org/alpine/v3.14/community]\nOK: 14938 distinct packages available\n/ # apk search --no-cache nodejs-current --repository=\"http://dl-cdn.alpinelinux.org/alpine/edge/community\"\nfetch http://dl-cdn.alpinelinux.org/alpine/edge/community/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.14/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.14/community/x86_64/APKINDEX.tar.gz\nnodejs-current-doc-16.9.1-r0\nnodejs-current-16.9.1-r0\nnodejs-current-dev-16.9.1-r0\n/ # apk add --no-cache nodejs-current --repository=\"http://dl-cdn.alpinelinux.org/alpine/edge/community\"\nfetch http://dl-cdn.alpinelinux.org/alpine/edge/community/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.14/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.14/community/x86_64/APKINDEX.tar.gz\n(1/8) Installing ca-certificates (20191127-r5)\n(2/8) Installing brotli-libs (1.0.9-r5)\n(3/8) Installing c-ares (1.17.2-r0)\n(4/8) Installing libgcc (10.3.1_git20210424-r2)\n(5/8) Installing nghttp2-libs (1.43.0-r0)\n(6/8) Installing libstdc++ (10.3.1_git20210424-r2)\n(7/8) Installing libuv (1.41.0-r0)\n(8/8) Installing nodejs-current (16.9.1-r0)\nExecuting busybox-1.33.1-r3.trigger\nExecuting ca-certificates-20191127-r5.trigger\nOK: 74 MiB in 22 packages\n/ # node -v\nv16.9.1",
    "How to find the right libnvinfer version for Cuda": "To check which version of TensorRT your tensorflow installation expects:\n>>> import tensorflow\n>>> tensorflow.__version__\n'2.8.0'\n>>> from tensorflow.python.compiler.tensorrt import trt_convert as trt\n>>> trt.trt_utils._pywrap_py_utils.get_linked_tensorrt_version()\n(7, 2, 2)\n>>> trt.trt_utils._pywrap_py_utils.get_loaded_tensorrt_version()\n2022-03-24 08:59:15.415733: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\nThe last command shows that indeed libnvinfer.so is missing on your system (you can also check this fact using ldconfig -p | grep libnv).\nTo install it (adapted from Tensorflow's gpu.Dockerfile), take the TensorRT version in your output from above, double check it's available for your CUDA version on the nvidia repository, and install:\nexport LIBNVINFER=7.2.2 LIBNVINFER_MAJOR_VERSION=7 CUDA_VERSION=11.0\napt-get install -y libnvinfer${LIBNVINFER_MAJOR_VERSION}=${LIBNVINFER}-1+${CUDA_VERSION} libnvinfer-plugin${LIBNVINFER_MAJOR_VERSION}=${LIBNVINFER}-1+${CUDA_VERSION}\nThe last Python command above should now start working. If you get a similar error again, double check that the so file locations (check with ldconfig -p | grep libnv) are included in LD_LIBRARY_PATH.\nAlso double check your CUDA version. In my case, I was running the docker.io/nvidia/cuda:11.5.1-cudnn8-runtime-ubuntu20.04 image, which already contains the math libraries and specifically libnvrtc.so.11.2 (so for a newer CUDA version than TensorRT supports on the nvidia repository). This becomes evident after running the apt-get command above, which gave this output:\nThe following packages have unmet dependencies:\n libnvinfer7 : Depends: cuda-nvrtc-11-0 but it is not going to be installed",
    "testcontainers - cannot connect to exposed port of generic container": "If you say testcontainers to expose port 9000, that means, to bind the port 9000 inside the container to some free port on the local machine, it does not mean, that the local port 9000 is bound to something.\nYou can ask the container to which local port the exposed container port is bound (e.g. containerinstance.getMappedPort(9000)\nIf your client is not in a container, it has to connect to the local port. If it is inside the container (or any container in the same docker network) use port 9000.",
    "Python import error with cronjob in a docker container": "You need to speicfy the absoulte path for Python installation in the crontab file.\n#run python script every minutes\n* * * * * /usr/local/bin/python /app/main.py > /proc/1/fd/1 2>/proc/1/fd/2\nYou can find Dockerfile here.\nmain.py\nimport numpy as np\na = np.arange(15).reshape(3, 5)\nprint(a)",
    "define a locale environment variable in Dockerfile": "Use ARG instead of ENV\nARG MY_JAR=myJar.jar  # ARG is only available during the build of a Docker image\nCOPY bin/$MY_JAR $ORACLE_HOME/user_projects/domains/$DOMAIN_NAME/lib/\nCOPY bin/$MY_JAR $ORACLE_HOME/wlserver/server/lib/mbeantypes/\nsee also ARG or ENV, which one to use in this case?",
    "How to run Dockerfile at Silicon Mac M1 from Intel Mac": "Apple M1 is arm64v8 instruction set processor, so you can not run amd64 (X86_64) on it without emulation. In general, Docker has the ability to emulate other architectures if the emulation based on bitfmt is set up (and on maxOS with intel CPU it's already set-up), however the emulation for amd64 on M1 is not stable yet. This means that for some period of time you will be restricted to 'arm64' images.\nTo fetch the proper image for you architecture during the build you need to add the following.\nFROM --platform linux/arm64 <image name>\nYou need to check if there are images for db2 development environment for arm by doing\ndocker pull --platform linux/arm64 store/ibmcorp/db2_developer_c",
    "docker-compose file can't build images": "When you build a Docker image, there are two basic things you can control. The context directory is the base directory for COPY commands; the Dockerfile location is a specific file within the context that's the Dockerfile. In Compose you specify these with the build: {context: ..., dockerfile: ...} options; in plain Docker these are the path argument to docker build and the -f option.\nWhen your Dockerfile says:\nbuild:\n  context: .\n  dockerfile: foo/Dockerfile\nThat's the equivalent of the CLI command:\ndocker build -f foo/Dockerfile .\n...and paths in foo/Dockerfile will be interpreted relative to the root (.) directory.\nProbably what you're actually running is\ncd foo\ndocker build .\nor, equivalently,\ndocker build foo\nwhich you can express in Compose syntax\nbuild:\n  context: foo\n  # dockerfile: Dockerfile\nor more succinctly\nbuild: foo\n(Compose on its own will assign reasonable defaults for image: and container_name: and you don't need to explicitly specify these if you're build:ing a local image, unless you're planning to push the built image somewhere.)",
    "Change time zone for docker and Dockerfile": "This is a dockerfile I customized based on Debian, you can refer to it\uff1a\nFROM debian:stable-slim\n\nARG ARG_TIMEZONE=Asia/Shanghai\nENV ENV_TIMEZONE                ${ARG_TIMEZONE}\n\n# install base dependence\nRUN echo 'debconf debconf/frontend select Noninteractive' | debconf-set-selections \\\n    && apt-get update && apt-get install -y -q \\\n        dialog apt-utils \\\n        locales systemd cron \\\n        vim wget curl exuberant-ctags tree \\\n        tzdata ntp ntpstat ntpdate \\\n    && apt-get clean && apt-get autoremove -y && rm -rf /var/lib/apt/lists/* \\\n    && localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8\n\n# sync timezone\nRUN echo '$ENV_TIMEZONE' > /etc/timezone \\\n    && ln -fsn /usr/share/zoneinfo/$ENV_TIMEZONE /etc/localtime \\\n    && dpkg-reconfigure --frontend noninteractive tzdata",
    "Do you need to update the path in the dockerfile from \"/usr/src/app/\"?": "Each container and each image has an isolated filesystem, so it's just fine to have the same path in every image even for different projects. I tend to use just /app. There's no reason to use a very long-winded path like in your last example, and the container paths don't need to match the host paths.\nFor simplicity, I also tend to make the right-hand side of COPY instructions relative paths. These are interpreted relative to the current WORKDIR, but that means, whatever path I pick, I only need to write it once.\nFROM python:3.8.2-alpine\nWORKDIR /app                                # or whatever other path you choose\nENV PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1\nRUN pip install --upgrade pip\nCOPY ./requirements.txt ./requirements.txt  # note, relative path on RHS\nRUN pip install -r requirements.txt\nCOPY . .                                    # note, relative path on RHS\nCMD [\"/app/main.py\"]                        # interpreted at runtime\nYou'll also see occasional SO questions that put install-time paths in environment variables. There's no particular benefit to doing this, since the paths are fixed at image build time.",
    "How to set memory limit for Java 11 inside Docker?": "As java 11 (10+) can automatically detect the container's memory you can set memory limit on your container and it should WAI:\ndocker run -m 512 .... \nAs for the choice of JDK, you can either use oracle JDK which is licensed or open source OpenJDK.\nMore details in this article: https://www.docker.com/blog/improved-docker-container-integration-with-java-10/",
    "Dockerfile - Run a init sql file for setting up tables": "You probably don't even need the Dockerfile unless you are doing something else that isn't listed above. You should be able to do what you want with a simple docker-compose like this:\nversion: '3.3'\nservices:\n  database:\n    image: mysql\n    volumes:\n       - \"./sql-scripts:/docker-entrypoint-initdb.d\"\n    environment:\n      MYSQL_DATABASE: stock_app\n      MYSQL_ROOT_PASSWORD: password\n    ports:\n      - '3306:3306'\nWhenever you update your sql script you would need to recreate the containers with docker-compose up -d --force-recreate. By design the mysql image will run any sql files found in the \"/docker-entrypoint-initdb.d\" folder so you should not have to manually run those every time you recreate the containers.",
    "How do I set timezone for my docker container?": "The SQL Server image is based on Ubunutu 16.04 (according to its DockerHub reference page). According to the answers to this question, there's a bug in Ubuntu 16.04 with setting the time zone.\nTry changing your docker file to:\nENV TZ=America/New_York\nRUN ln -fs /usr/share/zoneinfo/$TZ /etc/localtime && dpkg-reconfigure -f noninteractive tzdata\nYou definitely should be setting America/New_York, not EST or EDT.",
    "Machine Learning Tools Docker Image Size Issue": "Get some method from other's Dockerfile,or documents:\ndelete apt cache\ndo rm -rf /var/lib/apt/lists/* after you run apt-install ,such as\nRUN apt-get update && apt-get install -y \\\n        ca-certificates \\\n        netbase \\\n    && rm -rf /var/lib/apt/lists/*\nNot:\nRUN apt-get update && apt-get install -y \\\n      ca-certificates \\\n      netbase\nRUN rm -rf /var/lib/apt/lists/*\nno-install-recommends\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        ca-certificates \\\n        netbase \\\n    && rm -rf /var/lib/apt/lists/*\nno-install-recommends means : do not install non-essential dependency packages.\nremove middle software\negg:\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        gcc \\\n        g++ \\\n    && pip install cython && apt-get  remove -y gcc g++ \\ \n    && rm -rf /var/lib/apt/lists/*\nSome software ,like gcc,only use when install some software,we can remove it after install finish.\npip use no cache\negg:\nRUN pip install --no-cache-dir -r requirements.txt\ndownload and remove better than copy?\nI am not sure it.From other's Dockerfile, they download file and finally delete it after use in one RUN,not copy file in it.\nNot docker a model data into a image.\nIf you use tensorflow or other AI application,you may have some model data(size is a few G),better way is download it when run in container or by ftp,object storage,or others way \u2014\u2014 not in image,just mount or download.\ntake care about the .git folder\nJust in my experience. If you use git to contorl codes. The .git folder may very very big. The command COPY . /XXX will copy .git to image.Find a way to filter the .git.For my use:\nFROM  apline:3.12 as MID\nCOPY XXX /XXX/\nCOPY ... /XXX/\n\nFROM image:youneed\nCOPY --from=MID /XXX/ /XXX/ \nRUN apt-get update && xxxxx\n\nCMD [\"python\",\"app.py\"]\nor use .dockerignore.\nget above from :\nPython:3.6-slim\nIn your Dockerfile\n# Did wget,cmake and some on  is necessary?\n\nCOPY . /opt/program\n\nWORKDIR /opt/program/\n\n# Install dependencies\nRUN chmod +x train && apt-get update \\\n    && apt-get upgrade -y \\\n    && apt-get autoremove -y \\\n    && apt-get install -y \\\n    gcc \\\n    build-essential \\\n    zlib1g-dev \\\n    wget \\\n    unzip \\\n    cmake \\\n    python3-dev \\\n    gfortran \\\n    libblas-dev \\\n    liblapack-dev \\\n    libatlas-base-dev \\\n    && apt-get clean && pip install --upgrade pip \\\n    && pip install --no-cache-dir \\\n    ipython[all] \\\n    nose \\\n    matplotlib \\\n    pandas \\\n    scipy \\\n    sympy \\\n    && pip install --no-cache-dir --install-option=\"--prefix=/install\" -r requirements.txt\n    && apt-get remove -y gcc unzip cmake \\ # just have a try,to find what software we can remove.\n    && rm -rf /var/lib/apt/lists/*\n    && rm -fr /root/.cache\nOf course, by this way, you may get a just smaller size image,but docker build process, will not use docker's cache .So during you try to find what software can delete, split to two or three commands RUN to use more docker cache.\nHope to help you.",
    "Load shell configuration file when starting container": "DOCKERFILE\nYou need to add the command chsh -s /path/to/shell in order to add the ZSH shell as the default for the user in the container:\nFROM archlinux:latest\n\n# Install things...\n# Install zsh & oh-my-zsh\n# Retrieve custom .zshrc from a repository and place it at ~/.zshrc\n# Clone extensions for oh-my-zsh\n\n# Make ZSH the default shell for the current user in the container\n# To check that the shell was indeed added: `chsh -l` and you should see it in the  list.\nRUN chsh -s ~/.zshrc\n\n# Run zsh on container start\nCMD [ \"zsh\" ]\nOTHER APPROACHES\nDockerfile CMD\nThis does not work, because the execution order:\nCMD [ \"zsh && source ~/.zshrc\" ]\nBut this should work(not tested):\n# using `root` user, adjust as needed for your case\nCMD [ \"source /root/.zshrc\", \"zsh\"]\nDocker Entrypoint\nIf you don't want to add it to the Dockerfile then use it in as the entrypoint:\ndocker run --rm -it --entrypoint \"chsh -s /root/.zshrc\" image-name\nNote that the example assumes the user in the container is root, please adjust accordingly your case.",
    "How do I create a Docker container on a Windows 10 PC to run on a Raspberry Pi 4": "You have to specify the platform. Either in your Dockerfile, or from the command line.\nFROM --platform=linux/arm/v7 python:3.6-stretch \nYou might need to use BuildKit or enable experimental features for your Docker daemon if you want to be able to set the platfrom from the command line:\nDOCKER_BUILDKIT=1 docker build --platform=linux/arm/v7 .\nYou need to have Qemu and docker/binfmt installed to be able to build ARM images on x86_64. The installation process is explained here: https://www.docker.com/blog/getting-started-with-docker-for-arm-on-linux/\nOn Linux, you have to install this yourself. From what I've gathered, it's included with Docker for Windows.",
    "Why should I use docker ONBUILD?": "In general you shouldn't use ONBUILD at all. Having a later Dockerfile FROM line do something other than simply incorporate its contents violates the principle of least surprise.\nIf the thing you're trying to do ONBUILD is something like a RUN or ENV instruction, semantically it makes no difference whether you do it in the base image or the derived image. It will be more efficient if you do it in the base image (once ever, as opposed to once each time a derived image is built).\nIf you're trying to ONBUILD COPY ... then you're trying to force a specific file to be on the host system at the point you run docker build, which is a little strange as a consumer. Docker's Best practices for writing Dockerfiles notes\nBe careful when putting ADD or COPY in ONBUILD. The \u201conbuild\u201d image fails catastrophically if the new build\u2019s context is missing the resource being added. Adding a separate tag, as recommended above, helps mitigate this by allowing the Dockerfile author to make a choice.\nAs that page notes, if you must use ONBUILD, you should call it out in the image tag so it's clear when you build a Dockerfile FROM that image, something strange is going on. Most current Docker Hub images don't have -onbuild variants at all, even for things like tomcat that generally have extremely formulaic uses.",
    "Unable to put message in ibm mq using container": "Localhost inside your spring container, is your spring container. Instead of setting the host to localhost for your MQ Connection you need your container's / host ip.\nHow you do it depends on your host's platform, but it will be something like host.docker.internal\nUsing your docker-compose.yml, you should be able to set\nIBM_MQ_HOST: ibm-mq-mock",
    "How to mount mongodb data from docker container to localhost storage on Windows 10?": "docker can only be accessed through the linux file system.\nTo solve this problem, you have to create volume separately.\nlike below. and please see this link\ndocker volume create --name=mongodata\ndocker run -d -p 27015:27017 -v mongodata:/data/db mongo",
    "Run Multiple Main Methods from the same Dockerfile": "The usual answer to \"how do I run multiple processes from one image\" is to run multiple containers. Given the Dockerfile you show this is fairly straightforward:\n# Build the image (once)\ndocker build -t myapp .\n\n# Then run the five containers as background processes\ndocker run -d --name app1 java .:./App.jar path.to.main.class1\ndocker run -d --name app2 java .:./App.jar path.to.main.class2\ndocker run -d --name app3 java .:./App.jar path.to.main.class3\ndocker run -d --name app4 java .:./App.jar path.to.main.class4\ndocker run -d --name app5 java .:./App.jar path.to.main.class5\nSince all of the commands are pretty similar, you could write a script to run them\n#!/bin/sh\n\n# Use the first command-line argument as the main class\nMAIN_CLASS=\"$1\"\nshift\n\n# Can also set JAVA_OPTS, other environment variables, ...\n\n# Run the application\nexec java -jar App.jar \"path.to.main.$MAIN_CLASS\" \"$@\"\ncopy that into the image\nCOPY run_main.sh /usr/local/bin\nand then when you launch the containers just run that wrapper\ndocker run -d --name app1 run_main.sh class1",
    "How to Have Pycrypto at Docker Properly Working?": "Well, well, well...today was a lucky day for me.\nSo simple: all I had to do was replace\npycrypto==2.6.1\nby\npycryptodome\non my requirements.txt file.\nThis thread says: \"Highly recommend NOT to use pycrypto. It is old and not maintained and contains many vulnerabilities. Use pycryptodome instead - it is compatible and up to date\".\nAnd that's it! Docker builds just fine with pycryptodome.",
    "Env variable in Dockerfile": "The docker run syntax is docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...], anything passed after the IMAGE[:TAG|@DIGEST] is passed as [COMMAND] [ARG...].\nThe environment variable setting should be passed in the run [OPTIONS] i.e.: docker run -p 3000:3000 -e connectionString=myConnString imagename",
    "How to start with Docker, Dockerfile, and Docker Compose? [closed]": "Your Dockerfile is similar to the Maven POM file; its a set of instructions for Docker to create an image with (docker build image-name .). Dockerfile is a must, you cannot use Docker without a one. It's like trying to use a Maven without a POM file.\nName of the image is what you give for the Maven plugin (<repository>spotify/foobar</repository>) or docker build <image-name> . and this can be anything you like.\nDocker Compose is a tool that can be used to manage a service, that can be comprised of multiple micro-services. It allows users to create an orchestration plan that can be run later. This allows users to script complex information of the Docker environment like volumes, networking, restart policies and many more.\nDocker Compose file is an optional one and can be replaced with a different alternative like HashiCorp Nomad But Docker Compose is one of the easiest to use, stick to this if you're new to Docker.\nDocker Compose is able to build and use an image at runtime (useful for development) or run an image that already exists in a repository (production recommendation). Full Docker Compose documentation should explain how to write a one.\nBuild at runtime\nversion: '3.1'\nservices:\n  myflashcards-service-dictionary:\n    build: path/to/folder/of/Dockerfile\n  db:\n    image: postgres\n    restart: always\n    ports:\n      - 5434:5432\nRun a pre-existing image\nversion: '3.1'\nservices:\n  myflashcards-service-dictionary:\n    image: myflashcards-service-dictionary\n  db:\n    image: postgres\n    restart: always\n    ports:\n      - 5434:5432\nDockerfile can be used without a Docker Compose, the only difference is that it's not practical to use in production since it considered as a single service deployment. As far as I'm aware, it cannot be used with Docker Swarm\nAs far as CI/CD goes, you can use a Maven plugin like Dockerfile Maven Plugin. You can find the docs here. This image then can be pushed to a repository like Docker Hub, AWS ECR or even a self-hosted one (I wouldn't recommend this unless you're comfortable with setting up highly secure networks especially if it's not an internal network).",
    "Docker doesn't download recommended packages": "apt-get update should not install anything. The only thing apt-get update should do is update the local description of what packages are available. That does not download those packages though -- it just downloads the updated descriptions. That can take a while.\napt-get install will of course install packages. In order to install those packages, it needs to download them. Using --no-install-recommends tells apt-get to not install \"recommended packages\". For example, if you install vim, there are many plugins that are also recommended and provided as separate packages. With that switch, those vim plugins will not be installed. Of course, installing the packages you selected can also take a while.\nWhat you're doing, using && \\ is to put all of that into a single docker command. So every time you rebuild your image, you will have to do that every time because the list of packages changes every day, sometimes even multiple times per day.\nTry moving pip install -r requirements.txt to its own RUN command after you've run apt-get stuff. If that then does what you want, then I suggest reading and learning more about how Docker works under the hood. In particular, it's important to understand how each single command adds a new layer and how any dynamic information in a single layer can cause long build times because the layer will frequently change with large amounts of changes.\nAdditionally, you might want to move ADD . /abc to after the RUN commands. Any changes you've made to the files being added (source code, I assume) will invalidate the layer which represents the apt-get command that has been executed. Since it's been invalidated, it will need to be rebuilt. If you're actively working on and developing those projects, that can easily cause apt-get to be executed every time you build your image.\nThere are plenty of resources you can search for which discuss how to optimize your time when using Docker. I won't recommend any specific one and will leave it to you for learning.",
    "Docker - install stuff in Dockerfile or external shell script": "There is nearly no difference for your afforded options. If you insist, I can tell you some little difference:\n1. The output of two images have different number of layers & a little size difference:\nA simple example as next:\nOption 1:\nDockerfile:\nFROM centos:7\nRUN yum install -y net-tools\nBuild command:\n$ docker build -t trial:1 . --no-cache\nOption 2:\nDockerfile:\nFROM centos:7\nADD install.sh .\nRUN ./install.sh\ninstall.sh:\nyum install -y net-tools\nBuild command:\n$ docker build -t trial:2 . --no-cache\nCompare:\n$ docker history trial:1\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\nf86f153f9d95        12 minutes ago      |0 /bin/sh -c yum install -y net-tools          105MB\n9f38484d220f        3 months ago        /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n<missing>           3 months ago        /bin/sh -c #(nop)  LABEL org.label-schema.sc\u2026   0B\n<missing>           3 months ago        /bin/sh -c #(nop) ADD file:074f2c974463ab38c\u2026   202MB\n\n$ docker history trial:2\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n775be0061903        10 minutes ago      |0 /bin/sh -c ./install.sh                      105MB\nb7ca1d2a7e8b        10 minutes ago      /bin/sh -c #(nop) ADD file:6f96562be8deac728\u2026   25B\n9f38484d220f        3 months ago        /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n<missing>           3 months ago        /bin/sh -c #(nop)  LABEL org.label-schema.sc\u2026   0B\n<missing>           3 months ago        /bin/sh -c #(nop) ADD file:074f2c974463ab38c\u2026   202MB\nFrom above, you can see if you use ./install.sh, you will have to have a ADD instruction in Dockerfile. Compared to the option 1, it will increase a image layer about 20B for your finally generated image.\n2. Maximum image layer limit:\nSee this discussion, the maximum of docker image layers is 127, so if you in a special situation which base image or your image uses too many layers already, it maybe helpful.\n3. Efficiency of copy files from host to container:\nADD need to copy file from docker host to container. Internally, docker build will create a tar file of the context directory, sending it to the docker daemon and unpack it. So the question will be why we need this if we could do it directly in Dockerfile?\nBut do you really care the 20B, one more layer & a little speed of COPY? I guess unless in some very bare situations, there is no difference for the 2 options.\nHowever, seems put package install directly in Dockerfile is most people's choice, because folks may ask why I should put effort to maintain a install.sh if it did not bring any benefit to me.",
    "Convert Dockerfile to Kubernetes yaml": "As per Define a Command and Arguments for a Container docs:\nThe command and arguments that you define in the configuration file override the default command and arguments provided by the container image.\nThis table summarizes the field names used by Docker and Kubernetes:\n| Docker field name | K8s field name |\n|------------------:|:--------------:|\n|    ENTRYPOINT     |     command    |\n|       CMD         |      args      |\nSo, for your example you should use the following:\nDocker:\nENTRYPOINT [\"/dockerstartup/dockerstartup.sh\"]\nCMD [\"--wait\"]\nKubernetes:\ncommand: [\"/dockerstartup/vnc_startup.sh\"]\nargs: [\"--wait\"]",
    "Reduce Docker image size of Laravel 5.8": "Your docker image's layers are put on top of the compose docker image's layers which sum up to 157MB. Your image being 193MB, the only thing you can reduce is from the 36MB (193 - 157) you are adding on top of the compose image. Let see what can be squeezed.\nCOPY . /src\nCOPY . /src will put all the content of your working directory to the container /src directory. Which might be more than you want. One way to limit what will be copied over to the container is to make use of the .dockerignore file (here is a great article on the subject).\nSuch a file usually have content similar to:\n.dockerignore\n.git\n.gitignore\nREADME.md\nChangelog.md\nDockerfile\ndocker-compose.yml\ndocs\nThe .git directory can be quite large on some projects. If you also have a directory containing database dumps or other huge files not needed in your docker image, make sure to add them to the .dockerignore file.\nRUN composer install\nRUN composer install will also install dev dependencies that should not be needed in your image. Change it to\nRUN composer install --no-dev\nGoing further that path this SO answer sugests to use\nRUN composer install --no-ansi --no-dev --no-interaction --no-progress --no-scripts --optimize-autoloader.\nDive into your image\nUse dive to explore each of your image layer and figure out if unnecessary files where added of left behind at some point.\nLeverage Docker multistage build\nIf composer is only useful for downloading and installing your PHP libraries, then once those libraries downloaded and installed, you might want to get rid of composer. This cannot be done from a classic Dockerfile because every single RUN/COPY or ADD directive will produce a new layer on top of the preceding one.\nSo let say you have one layer with a 100MB file, if you delete that file later on in the same Dockerfile, the deletion will occur in a new layer. The previous layer with that 100MB is still there in your layer sandwich.\nOne way docker brought us to fight that is the multistage build Dockerfile.\nYour Dockerfile could then be structured as follow (not tested):\nFROM composer:latest as build_stage\nCOPY . /src\nADD .env.example /src/.env\nWORKDIR /src\nRUN composer install\nRUN php artisan key:generate\n\nFROM php:7-fpm-alpine\nCOPY --from=build_stage /src /var/www/html\n\nRUN mkdir /var/www/html/storage/ \\\n    && chmod -R 777 /var/www/html/storage/\nwarning: this is just the main structure of the Dockerfile, you might need to tweak it more to add missing php extensions. Refer to the official php docker image guide.",
    "How to create docker image in .NET Core with multiple project?": "To make it efficient you have to copy all the project's .csproj files first and then restore to cache the results. Then copy everything else and publish the project. So your Dockerfile will look similar to\nFROM microsoft/dotnet:sdk AS build-env\nWORKDIR /app\n\n# Copy csproj and restore as distinct layers\nCOPY aspnetapp/aspnetapp.csproj aspnetapp/\nCOPY TestData/TestData.csproj TestData/\nCOPY TestLogic/TestLogic.csproj TestLogic/\nCOPY TestData2/TestData2.csproj TestData2/\nRUN dotnet restore ./aspnetapp/aspnetapp.csproj\n\n# Copy everything else and build\nCOPY . ./\nRUN dotnet publish ./aspnetapp -c Release -o out\n\n# Build runtime image\nFROM microsoft/dotnet:aspnetcore-runtime\nWORKDIR /app\nCOPY --from=build-env /app/out .\nENTRYPOINT [\"dotnet\", \"aspnetapp.dll\"]\nYou may want to explicitly copy the projects after restore if you have more projects in the solution to reduce the container build time.\nIn my project I created a tool that creates COPY directives for main project dependencies from the command line to simplify the process.",
    "How do I break a long ENV declaration in Dockerfile?": "You can use \\ to break it up over multiple lines.\nFROM alpine:3.8\n\nENV SPECIAL_PATHS \\\n/foo/bar:\\\n/yada/yada:\\\n/the/end\nHere's the env in container run from the resulting image.\n$ docker container run --rm env-test env \nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=2fae9abd1eea\nSPECIAL_PATHS=/foo/bar:/yada/yada:/the/end\nHOME=/root",
    "Docker ADD giving error \"No source files were specified\"": "be sure that the path of the file is accessible where the Dockerfile is. When you run the build, the . folder is where the Dockerfile is. So you directory structure has to be something similar to this:\n.\n..\nDockerfile\ntest-web-app (folder)\nTo be sure that the war file is accessible try to list the file (on your host machine) for example.\n$ ls ./test-web-app/build/libs/test-web*.war",
    "`chmod` not working on Dockerfile (macbook)": "A Dockerfile can only have one CMD instruction, which define the command occuring when the container is launched.\nTry to replace these lines:\nCMD chmod 777 -R /src/main/*\nCMD chmod 777 -R /app/main/*\nby:\nRUN chmod 777 -R /src/main/*\nRUN chmod 777 -R /app/main/*\nIn this case, chmod commands will occurs when building the image.",
    "containerized ReactJs application (from nginx image )does not serve all routes": "It turns out that using react-router and nginx needs to have specific configuration because nginx doesn't really recognize the routes that I have specified with react router in the application code.\n1. Add those lines to the Dockerfile to tell docker to not use conf.d default nginx configurations.\nRUN rm -rf /etc/nginx/conf.d\nCOPY conf /etc/nginx\n2. Add a folder conf, conf.d and a file default.conf\n conf\n   |\n    conf.d\n    default.conf\n3. Add those lines in the default.conf file\nserver {\n listen 80;\n  location / {\n    root   /usr/share/nginx/html;\n    index  index.html index.htm;\n    try_files $uri $uri/ /index.html;\n  }\n  error_page   500 502 503 504  /50x.html;\n  location = /50x.html {\n    root   /usr/share/nginx/html;\n  }\n}\nWhich they tell nginx to serve the index.html file",
    "Docker map an external config file": "You want to use a folder-name to map the volume: docker run MyImage -v /home/path/:/folder1/folder2/ So now /home/path folder on the host machine is mounted to /folder1/folder2 inside the container.\nThen just pass the path of the conf file as seen within the container to the cmd. ENTRYPOINT [\"python3\", \"main.py\", \"/folder1/folder2/myconf.conf\"]",
    "Why does CTRL-C no longer stop my nginx container anymore?": "https://forums.docker.com/t/docker-run-cannot-be-killed-with-ctrl-c/13108/2\nSo there are two factors at play here:\nIf you specify a string for an entrypoint, like this:\nENTRYPOINT /go/bin/myapp\nDocker runs the script with /bin/sh -c 'command'. This intermediate script gets the SIGTERM, but doesn\u2019t send it to the running server app.\nTo avoid the intermediate layer, specify your entrypoint as an array of strings.\nENTRYPOINT [\"/go/bin/myapp\"]",
    "Setup ulimit parameter in dockerfile": "You can't set ulimits on docker containers in the dockerfile - needs to be set when running the container from the command line. Try this:\ndocker run --ulimit nofile=262144:262144 IMAGE",
    "Docker container restarting on defining an entrypoint": "I think what you want is docker run and not docker exec command. Docker exec is to launch something on an already deployed up and running container... and what you want is to launch a script once upon deployed.\nModify your Dockerfile. Use this instead of entrypoint on last line:\nCMD [\"bash\", \"-c\", \"/abc.sh\"]\nUse this command to run it and the script will be launched automatically:\ndocker run -it drupaldocker_myapp_1\nIf you put bash at the end of the docker run command you'll get an interactive bash console inside the container but the automatic script launching is avoided:\ndocker run -it drupaldocker_myapp_1 bash\nAnswering to your question \"Does a container gets stopped once the entrypoint script is executed?\" the answer is yes. If the entrypoint script is executed and there is nothing more to be executed, the container stops because it has nothing more to do. If you want a container to be kept up and running you should run inside it something with no immediate end like a script.\nHope it helps.",
    "Minimizing the number of layers in Dockerfile": "The Dockerfile \"design\" mainly depends on your needs and what you want to balance. Minimising the number of layers is considered to be a best practice, but as you already mentioned, caching works by explicitly creating a new layer. The linked issue with a limited number of layers might become a problem with bigger Dockerfiles, but that also depends on the configured storage driver on your system. Your example Dockerfile (even with each script in its own RUN statement) doesn't reach the limit of layers, so you don't need to worry.\nThat said, I suppose you didn't fully understand how the layer caching works (maybe you didn't post the complete Dockerfile?). Docker doesn't know which file system changes your scripts will produce during a RUN statement. Consequently, when you re-run the Docker build with that exact Dockerfile, Docker won't run your scripts again. You mentioned as example that the phpredis version might change, but the Dockerfile doesn't reflect that variable. I suggest to declare some ENV variable before the relevant RUN statement. Example:\n# https://hub.docker.com/_/php/\nFROM php:5.5.23-fpm\n\nRUN /scripts/base.sh \\\n && /scripts/composer.sh \\\n && /scripts/mbstring.sh \\\n && /scripts/bcmath.sh \\\n && /scripts/mcrypt.sh \\\n && /scripts/sockets.sh \\\n && /scripts/zip.sh \\\n && /scripts/cleanup.sh\n\nENV PHPREDIS_VERSION=1.2.3\n\nRUN /scripts/phpredis.sh \\\n && /scripts/cleanup.sh\nThe phpredis.sh should use the environment variable. Every time you change the ENV ... statement, Docker will re-run every statement after that step, including your phpredis.sh script.\nMaybe the recently announced multi stage builds also help to re-design the way you keep your images tiny and reduces the need for the cleanup.sh script.",
    "Bridged network connection in Dockerfile": "By default docker server creates a default bridge but you can create your own bridge network and assign your container to that custom bridge on start. Or you can use docker network command which seems to be an easier path. That way you can specify the IP range to match with your DHCP settings.\nCreate your own bridge\ndocker network create --driver=bridge \\\n--subnet=192.168.127.0/24 --gateway=192.168.127.1 \\\n--ip-range=192.168.127.128/25 yourbridge\nRun the container using your custom bridge;\ndocker run -d --net=yourbridge ..\nSee this blog post for a concrete example: http://www.dasblinkenlichten.com/docker-networking-101-user-defined-networks/\nWithin a user-defined bridge network, linking is not supported. You can expose and publish container ports on containers in this network. This is useful if you want to make a portion of the bridge network available to an outside network.",
    "Local development with docker - do I need 2 Dockerfiles?": "No you may not need two files. You can use same folder in ADD command in volume.\nSee this django tutorial from official docker page:\nhttps://docs.docker.com/compose/django/\nDockerfile\nFROM python:2.7\nENV PYTHONUNBUFFERED 1\nRUN mkdir /code\nWORKDIR /code\nADD requirements.txt /code/\nRUN pip install -r requirements.txt\nADD . /code/\nCompose file\nversion: '2'\nservices:\n  db:\n    image: postgres\n  web:\n    build: .\n    command: python manage.py runserver 0.0.0.0:8000\n    volumes:\n      - .:/code\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - db",
    "Dockerfile entrypoint script arguments in docker run": "RUN instructions happen at build time.\nENTRYPOINT and CMD instructions happen at run time.\nYou probably want something like this in your Dockerfile:\n....\nENTRYPOINT [\"xyz.sh\"]\n\nCMD [\"--IP\", \"127.0.0.1\"]\n....\nThen you can run with:\ndocker run -it some-image --IP 127.0.0.1\nArguments after the image overwrite the CMD instruction so then it runs the ENTRYPOINT instruction followed by your arguments.",
    "Basic Dockerfile not running CMD command": "It turns out that I need double quotes instead of single like this.\nCMD [\"/startup_nginx.sh\"]\nNote This is not explicit in the docs and seems like something basic that should be spelled somewhere as there are no error messages. It fails silently which makes it extremely hard to debug as the startup script is what is needed to create a running process so the container doesnt exit.",
    "How can I use a created volume in my container using docker compose?": "Try this:\nmysql:\n    image: percona\n    ports:\n        - \"3306:3306\"\n    environment:\n        - MYSQL_ROOT_PASSWORD=123456\n    volume_driver: local\n    volumes:\n        - \"testing-volume:/var/lib/mysql\"\nIf you are using any custom driver you can change on volume_driver directive.",
    ": and := inside a bash parameter expansion [duplicate]": ": is simply a command that does nothing. However, the arguments for the command are still evaluated.\n${WORDPRESS_DB_NAME:=wordpress} is a parameter expansion that check if WORDPRESS_DB_NAME is unset or has the empty string for a value. If either is true, the parameter is assigned the value wordpress. The result of the expansion is then the (possibly newly assigned) value of WORDPRESS_DB_NAME.\nTogether, they form a common shell idiom for setting the value of a variable if it does not already have a (non-null) value.\nA (non-DRY) Python equivalent might be\ntry:\n    if not WORDPRESS_DB_NAME:\n        WORDPRESS_DB_NAME = \"wordpress\"\nexcept NameError:\n    WORDPRESS_DB_NAME = \"wordpress\"",
    "RUN command not called in Dockerfile": "You are confusing RUN and CMD. The RUN command is meant to build up the docker container, as you correctly did with the first one. The second command will also executed when building the container, but not when running it. If you want to execute commands when the docker container is started, you ought to use the CMD command.\nMore information can be found in the Dockerfile reference. For instance, you could also use ENTRYPOINT in stead of CMD.",
    "Why does Chroma DB crash with \"illegal instruction\" in this python:3.10-slim container?": "I was able to get past my illegal instruction errors with Chroma by setting the environment variable HNSWLIB_NO_NATIVE=1 before running pip install chromadb. Looking at the source code, this removes use of the -march=native compiler flag. As @CharlesDuffy indicates in a comment above, illegal instruction indicates a difference in CPU features between where you built it and where you're running it. So this workaround, if not ideal, at least makes sense.\nIn my case, I'm building on AWS CodeBuild and running in Lambda. One idea to (maybe?) ensure the CPUs are the same is the new support for using Lambda runtimes in CodeBuild, but you can't use docker commands there (a further suggestion is to use podman instead\u2014maybe I'll try it sometime, but for now \ud83e\udd37\u200d\u2642\ufe0f, things are working and it all seems fine).",
    "Streamlit with Poetry is not found when run my docker container": "With poetry config virtualenvs.in-project true poetry will create a virtual environment in the .venv directory and install all it's dependencies in it.\nThe typical approach is to activate a virtual environment before using it. Typically this is done with the .venv/bin/activate (or with poetry run / poetry shell). E.g. you could do something like:\nCMD source /app/.venv/bin/activate && exec streamlit run ...\nAlternatively you can also activate a virtual environment by manipulating the path environment variable. If you prepend the path to the bin directory of the virtual env, the Docker environment will find all binaries:\nENV VIRTUAL_ENV=/app/.venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\nCMD [\"streamlit\", \"run\", ...]\nYou can find these methods, and extended explanations at https://pythonspeed.com/articles/activate-virtualenv-dockerfile/.",
    "Docker Ports are not available exposing port TCP 0.0.0.0:5432 -> 0.0.0.0:0": "Solved: Listed all ports of postgres (Mac):\nsudo lsof -i -P | grep LISTEN | grep :5432\npostgres   9785              postgres    7u  IPv6 0xd0180dc2abc35a25      0t0    TCP *:5432 (LISTEN)\npostgres   9785              postgres    8u  IPv4 0xd0180dc778b0fe35      0t0    TCP *:5432 (LISTEN)\nKilled all these processes (How do I close an open port from the terminal on the Mac?)",
    "docker and two services on different ports": "According to the docker containers inspect output you provided it seems you are running the following command:\n \"Path\": \"docker-entrypoint.sh\",\n \"Args\": [\n   \"npm\",\n   \"run\",\n   \"dev\"\n ],\n\n \n\n \"Cmd\": [\n   \"npm\",\n   \"run\",\n   \"dev\"\n ]\nin both of them, which is consistent with the command you provided for both containers in your docker-compose.dev.yml file:\ncommand: npm run dev\nAs stated in the docker-compose documentation, please, be aware that by doing so you are overwriting your containers Dockerfile command.\nIt means that both containers are running the dev scripts.\nAccording to your project structure it turns out both containers are sharing the same package.json and code.\nIn package.json you defined the dev script like this:\n\"scripts\": {\n  \"start\": \"nodemon index.js && nodemon server.js\",\n  \"dev\": \"nodemon index.js && nodemon server.js\"\n},\nIt means you are launching in both containers index.js, with different ports, according to your PORT variables declaration in each container, but they are both running index.js after all.\nThat explains why you are getting the same output in every call:\nconnected to port index ...\nAlthough my advice is that you separate the source code for every container, to solve the problem you could try running a different script per every container. I mean, in your package.json file I would include a new script, server, for example:\n\"scripts\": {\n  \"start\": \"nodemon index.js && nodemon server.js\",\n  \"dev\": \"nodemon index.js\",\n  \"server\": \"nodemon server.js\"\n},\nAnd use the new script when launching your container in docker-compose:\n  project-socket-server:\n    build:\n      context: \".\"\n      dockerfile: \"./Dockerfile-socket\"\n      args:\n        NODE_ENV: development\n    volumes:\n      - .:/server\n      - /server/node_modules\n    environment:\n      - NODE_ENV=development\n      - MONGO_USER=project_admin\n      - MONGO_PASSWORD=9293\n    command: npm run server\nAs an alternative, you can remove the lines:\ncommand: npm run dev\nfrom your docker-compose.dev.yml file:\nersion: \"3\"\nservices:\n  project-server:\n    build:\n      context: .\n      args:\n        NODE_ENV: development\n    volumes:\n      - .:/server\n      - /server/node_modules\n    environment:\n      - NODE_ENV=development\n      - MONGO_USER=project_admin\n      - MONGO_PASSWORD=9293\n  project-socket-server:\n    build:\n      context: \".\"\n      dockerfile: \"./Dockerfile-socket\"\n      args:\n        NODE_ENV: development\n    volumes:\n      - .:/server\n      - /server/node_modules\n    environment:\n      - NODE_ENV=development\n      - MONGO_USER=project_admin\n      - MONGO_PASSWORD=9293\n  mongo:\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=project_admin\n      - MONGO_INITDB_ROOT_PASSWORD=9293\nThis will make docker to use the CMD provided in the different Dockerfiles:\nCMD [\"node\", \"index.js\"]\nand:\nCMD [\"node\", \"server.js\"]\nwhich I think it will work as well.\nAs I said, please, consider in any case reorder your project structure to avoid using the same directory for building your two apps, as you can see it could be the cause of several problems.",
    "Docker download and install binary at hugo version": "When docker build executes the line: RUN hugo version, then by default, it does not show the output of the run commands that were not loaded from the cache. And hence you are not seeing its output.\nWhen I ran docker build command with this flag: --progress=plain, I could see the output of \"non-cached\" line for RUN command. More details can be found in this answer. Here is the screenshot for the output I got:\nA few observations:\nI saw in one of the comments that you tried to run docker build using this flag but it still did not work. This is because, if you closely observe, that \"RUN hugo version\" line is \"CACHED\". And this flag --progress=plain shows intermediate steps of the lines that are not cached or freshly executed. So, if you wish to view the output, you need to first clear your docker build cache and all the dangling images using the commands:\n$docker builder prune -a\n$docker image prune -a\nAfter this step, you will be able to freshly execute all your build steps and will be able to see output of RUN hugo version.\nTo keep your hugo container running after you spin it from the image you build, you need to specify either CMD or ENTRYPOINT command. The command specified with these dockerfile instructions are executed only when you spin a container from the image that you have already built, not when the image is being built. For example if my dockerfile is:\nFROM debian:11.3\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n         hugo\nCMD hugo version\nThe output during build would be: The instruction CMD hugo version is not executed.\nAfter I run a container from this built image via command: Then only I would see the output of this instruction coming.\nI hope this helps in building the understanding!",
    "How can I pass ENV value in Dockerfile from .env file?": "You could firstly export these variables as environmetal variables to your bash\nsource .env\nThen use --build-arg flag to pass them to your docker build\ndocker image build --build-arg VERSION=$VERSION --build-arg DATE=$DATE .\nNext in your dockerfile\nARG VERSION\nARG DATE\nENV version=$VERSION\nENV date=$DATE\nAs a result, for example you can access the variables in your build phase as VERSION and in your container as version",
    "Cache Rust dependencies with Docker build and lib.rs": "(I couldn't reproduce this at first. Then I noticed that having at least one dependency seems to be a necessary condition.)\nWith the line\nRUN rm ./target/release/deps/rocket_example_pro*\nyou're forcing rebuild of the rocket_example_pro binary. But the library will remain as built from the first empty file. Try changing to\nRUN rm ./target/release/deps/librocket_example_pro*\nThough personally, I think deleting random files from the target directory is a terribly hacky solution. I'd prefer to trigger the rebuild of the lib by adjusting the timestamp:\nRUN touch src/lib.rs && cargo build --release --locked ## Doesn't fail anymore\nFor a clean solution, have a look at cargo-chef.\n[Edit:] So what's happening here?\nTo decide whether to rebuild, cargo seems to compare the mtime of the target/\u2026/*.d to the mtime of the files listed in the content of the *.d files.\nProbably, src/lib.rs was created first, and then docker build was run. So src/lib.rs was older than target/release/librocket_example_pro.d, leading to target/release/librocket_example_pro.rlib not being rebuilt after copying in src/lib.rs.\nYou can partially verify that that's what's happening.\nWith the original Dockerfile, run cargo build, see it fail\nRun echo >> src/lib.rs outside of docker to update its mtime and hash\nRun cargo build, it succeeds\nNote that for step 2, updating mtime with touch src/lib.rs is not sufficient because docker will set the mtime when COPYing a file, but it will ignore mtime when deciding whether to use a cached step.",
    "Can I use a docker container to keep static files so other docker containers can acces to it?": "I don't think it's a good thing to go for this style of architecture. They are not microservices if they are coupled in this fashion. You would do better to publish your shared library or even do a bit of copying and pasting to duplicate the code.\nHowever, technically speaking, it's possible. You could do something along these lines:\nThe Dockerfile, for the shared service, creates some dummy content in the directory /shared.\nFROM busybox\n\nWORKDIR /shared\nRUN echo \"shared content\" > /shared/data.txt\nThen 2 services are started. Both mounting a shared volume. The shared service mounts the volume first because app-a has a depends_on with condition.\nname: example\n\nservices:\n  shared:\n    build: ./\n    volumes: [ shared:/shared ]\n  app-a:\n    image: busybox\n    command: cat /shared/data.txt\n    volumes: [ shared:/shared ]\n    depends_on:\n      shared:\n        condition: service_completed_successfully\n\nvolumes:\n  shared:\n$ docker compose up\n[+] Running 4/4\n \u283f Network example_default     Created                                                                                                                                                                     0.6s\n \u283f Volume \"example_shared\"     Created                                                                                                                                                                     0.0s\n \u283f Container example-shared-1  Created                                                                                                                                                                     0.1s\n \u283f Container example-app-a-1   Created                                                                                                                                                                     0.1s\nAttaching to example-app-a-1, example-shared-1\nexample-shared-1 exited with code 0\nexample-app-a-1   | shared content\nexample-app-a-1 exited with code 0\nWhen an empty named volume is mounted, the content of the container's file system is first copied into the volume and then its mounted.\nThat way, app-a, sees the shared content.\nSometimes you may also want to let the shared service populate the volume on startup, manually. Because what I am doing here is very specific to docker. It would not work in Kubernetes, for example. There you had to copy the shared content into the volume when the shared service starts.\nshared:\n  image: busybox\n  command: sh -c 'echo \"shared content\" > /shared/data.txt'\n  volumes: [ shared:/shared ]",
    "How to set WORKDIR in dockerfile from environment ARG?": "ARGs are scoped to the stage, or the from lines when defined at the top of the file. So define your ARG after the FROM line, preferably just before you use it (for cache efficiency).\n# define top level args used in FROM lines here\nARG BUILD_IMAGE=maven:3.8.4-eclipse-temurin-11\n\nFROM $BUILD_IMAGE as dependencies\n# define your args scoped within the stage after the FROM for that stage\nARG APP=/opt/app\nWORKDIR ${APP}\nIf you want to set the arg value once and reuse it, that looks like:\n# define top level args used in FROM lines here\nARG BUILD_IMAGE=maven:3.8.4-eclipse-temurin-11\n# the top section can also define the default value of an arg that is defined elsewhere\nARG APP=/opt/app\n\nFROM $BUILD_IMAGE as dependencies\n# define your args scoped within the stage after the FROM for that stage\nARG APP\nWORKDIR ${APP}",
    "How to separate environments of a project in docker and docker-compose": "Should the Dockerfile be the same for every environment?\nYes. Build a single image and reuse it in all environments. Do not use Dockerfile ARG to pass in \"is it production\", or host names, or host-specific user IDs. Especially you should use an identical environment in your pre-production and production environments to avoid deploying an untested image.\nShould docker-compose.yml be the same for every environment?\nThis is the main place you have to control deploy-time options like, for example, where your database is located, or what level of logs should be output. So it makes sense to have a separate Compose file per environment.\nCompose supports multiple Compose files. You can use docker-compose -f ... multiple times to specify specific Compose files to use; or if you do not have that option then Compose will read both a docker-compose.yml and a docker-compose.override.yml. So you might have a base docker-compose.yml file that names the images to use\n# docker-compose.yml\nversion: '3.8'\nservices:\n  app:\n    image: registry.example.com/app:${APP_TAG:-latest}\nIn the question you suggest a docker-compose.prod.yml. That can set $RAILS_ENV and point at your production database:\n# docker-compose.prod.yml\nservices:\n  app:\n    environment:\n      - RAILS_ENV=production\n      - DB_HOST=db.example.com\n      - DB_USERNAME=...\n    # (but don't repeat image:)\nYou could separately have a docker-compose.dev.yml that launched a local database, and had instructions on how to build the image:\n# docker-compose.dev.yaml\nversion: '3.8'\nservices:\n  app:\n    build: .\n    environment:\n      - RAILS_ENV=development\n      - DB_HOST=db\n      - DB_USERNAME=db\n      - DB_PASSWORD=passw0rd\n  db:\n    image: postgres:14\n    environment:\n      - POSTGRES_USER=db\n      - POSTGRES_PASSWORD=passw0rd\n    volumes:\n      - dbdata:/var/lib/postgresql/data\nvolumes:\n  dbdata:\nIf you use the docker-compose -f option, you need to always mention both files you're using\ndocker-compose \\\n  -f docker-compose.yml \\\n  -f docker-compose.dev.yml \\\n  run app \\\n  rake db:migrate\nYou could also symlink docker-compose.override.yml to point at an environment-specific file, and then Compose would be able to find it by default.\nln -sf docker-compose.test.yml docker-compose.override.yml\ndocker-compose run app rspec",
    "Copy directory into docker build no matter if empty or not - fails on \"COPY failed: no source files were specified\"": "The solution is to use\nCOPY csv/. /csv/\nThis question gave me a hint (although the behavior desired by me is unwanted for its OP).",
    "How to run only specific command as root and other commands with default user in docker-compose": "In docker-compose.yml create another service using same image and volumes.\nOverride user with user: root:root, command: your_command_to_run_as_root, for this new service and add dependency to run this new service before starting regular working container.\napi_server:\n    build:\n      context: .\n      target: prod-env\n    image: company/server\n    volumes:\n      - ./shared/model_server/models:/models\n      - ./static/images:/images\n    ports:\n      - 8200:8200\n    command: gunicorn -b 0.0.0.0:8200 --threads \"8\" --log-level info --reload \"server:gunicorn_app(command='start', project='app_server')\"\n    # This make sure that startup order is correct and api_server_decorator service is starting first\n    depends_on:\n      - api_server_decorator\napi_server_decorator:\n    build:\n      context: .\n      target: prod-env\n    image: company/server\n    volumes:\n      - ./shared/model_server/models:/models\n      - ./static/images:/images\n    # No ports needed - it is only decorator\n    # Overriding USER with root:root\n    user: \"root:root\"\n    # Overriding command\n    command: python copy_stuffs.py; chmod -R a+rwx models; chmod -R a+rwx /images\nThere are other possibilities like changing Dockerfile by removing USER restriction and then you can use entrypoint script doing as root what you want as privileged user and running su - nobody or better exec gosu to retain PID=1 and proper signal handling.",
    "Docker cache from command for new pulled images doesnt work": "With buildkit, there's an additional setting to include cache metadata in the created image:\n--build-arg BUILDKIT_INLINE_CACHE=1\nThat only handles the final image layers. If you want to include the cache for intermediate stages of a multi stage build, you likely want to cache to a registry, which I believe needs buildx to access these buildkit options.\n--cache-from type=registry,ref=localhost:5000/myrepo:buildcache\n--cache-to type=registry,ref=localhost:5000/myrepo:buildcache,mode=max\nBuildkit defines several other cache options in their readme.",
    "Docker compose not building while docker build not having any problem": "why under services.backend.build is target set? Your docker file is not a multi stage build. I recommend you remove that line. https://github.com/compose-spec/compose-spec/blob/master/build.md#target",
    "Visual Studio Dockerfile COPY explained": "Docker images are created using layers. One layer for each command in the dockerfile. If no changes have been made, the layer can be reused in a later build. By doing it this way, the restore step can reuse an older layer as long as you haven't changed the project file. If you do it the way you propose, the restore has to be done every time you change anything in your C# code.",
    "docker build logs are different from what I'm used to": "Looks like you're now using BuildKit. You have probably set the environment variable DOCKER_BUILDKIT=1(?).\nTo build without BuildKit (and get back the logs that you're used to):\nDOCKER_BUILDKIT=0 docker build -t sometag .\nNow, note that BuildKit offers many enhancements over the classic builder. So personally I would instead try to get used to the new log output.",
    "unable to create docker image using git bash": "The error mentions failed to fetch oauth token\nHave you tried to run\ndocker login\nin your terminal prior to running the build command?",
    "How to connect to VSCode container locally using ssh?": "There are several issues to address:\nSince your host is using port 22 you have to use another one. You can do this with appPort:\n    \"appPort\": \"2222:22\",\nThis notation maps host's port 2222 to container's 22.\nrunArgs and forwardPorts are redundant.\nYou need to add \"overrideCommand\": false to prevent VSCode overriding CMD declared in the Dockerfile.\nYour sed in Dockerfile is incorrect, default config does not contain a line PermitRootLogin prohibit-password but it contains #PermitRootLogin <some-other-value. Change sed command to this:\nRUN sed -i 's/.*PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config\nAnd here are modified files for convenience:\nDockerfile:\nFROM ubuntu:18.04\n\nRUN apt-get update && apt-get install -y --no-install-recommends net-tools iputils-ping openssh-client openssh-server \n\n\nRUN mkdir /var/run/sshd\nRUN echo 'root:test' | chpasswd\nRUN sed -i 's/.*PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config\n\n# SSH login fix. Otherwise user is kicked off after login\nRUN sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd\n\nENV NOTVISIBLE \"in users profile\"\nRUN echo \"export VISIBLE=now\" >> /etc/profile\n\nEXPOSE 22\nCMD [\"/usr/sbin/sshd\", \"-D\"]\ndevcontainer.json:\n{\n    \"name\": \"Ubuntu\",\n    \"build\": {\n        \"dockerfile\": \"Dockerfile\",\n    },\n    \"settings\": {\n        \"terminal.integrated.shell.linux\": \"/bin/bash\"\n    },\n    \"extensions\": [],\n    \"appPort\": \"2222:22\",\n    \"overrideCommand\": false\n}\nWhen you run the container you can connect to it with ssh root@localhost -p 2222 and password 'test'.\nAlso, I don't know why you decided to go with VSCode specific way to Docker, maybe there is a solid reason to do this, but there is a better way. You can use docker-compose to create a testing environment. It is:\nbetter documented;\nwidely used;\nsupported by many IDE's (including VSCode).\nTake a look at this docker-compose.yml:\n# Check out this reference https://docs.docker.com/compose/compose-file/\n# for list of available versions, their differences, and the file format in general.\nversion: \"3.0\"\n\n# This is where you declare containers you want to run.\nservices:\n\n  # This is the name of the service. One cool thing about it is that is will be a DNS name\n  # in the networks where this service will be present. So when you need to connect this\n  # service from another container you can simply do 'ssh username@ssh-server'.\n  ssh-server:\n\n    # This is the name of the image to use. In this case I intentionally used a nonexistent name.\n    # Because of that when Docker will build the image from the Dockerfile, it will assign this\n    # name to the image. This is not required since I've added 'build' property but giving the\n    # right name could come handy.\n    image: myssh\n\n    # This is equivalent to 'build an image from the Dockerfile in current working directory' or\n    # 'docker build .'\n    build:\n      context: .\n      dockerfile: Dockerfile\n\n    # This maps host's port 2222 to container's 22. This isn't necessary unless you want to connect\n    # to this container from outside (e.g. from host or another machine). Containers do not\n    # require 'exposure' or any other step to reach one another within one network - they have all\n    # ports open. That is why it is called port forwarding or mapping.\n    ports:\n      - \"2222:22\"\n\n  # Same image as the server but with a different command to execute.\n  ssh-client:\n    image: myssh\n    build:\n      context: .\n    # Just a loop to run a command every second. Won't work with password, you need a key or some hacks.\n    command: bash -c 'while sleep 1; do ssh root@ssh-server ls /; done'\nIf you save it to a directory with the Dockerfile above, you can run it with docker-compose up. Or you can integrate it with VSCode: when there is no .devcontainer directory and you click Reopen in container, you can select From 'docker-compose.yml', then select one of the services you want and it will build and start a container. It will also create .devcontainer directory with devcontainer.json in it.",
    "How can I pass docker environment variables to an npm script?": "When you use the JSON form of CMD (or ENTRYPOINT or RUN), there is no interpolation at all; your script should literally see the string -e=$ENVIRONMENT as the argument. Instead you need to use the shell form, which will wrap this in a shell that expands environment variables. You can't do this with this particular split of ENTRYPOINT and CMD, but at the same time, it's not really necessary; just put the whole thing in CMD.\n# No ENTRYPOINT\n# No quoting; Docker wraps this in `sh -c ...`\nCMD npm start -- -e=\"$ENVIRONMENT\" -t=\"$TESTS\"\nYou can also handle these directly in your application. The yargs library for example has a .env() function that allows environment variables to be used directly as options. You could also make process.env.TESTS be the default value for the option if it's not provided directly. This approach gets around the trouble of constructing (and possibly extending) a valid command line with the combination of arguments you need.",
    "Could not connect to server: Connection refused Is the server running on host \"127.0.0.1\" and accepting TCP/IP connections on port 5432?": "When running with Docker-Compose, you should access the DB via the service name.\nDATABASE_URL=pgsql://kapitain_user:kapitain_user_password@db:5432/kapitain\nMaybe you have to allow access via pg_hba file.\nhttps://www.postgresql.org/docs/9.1/auth-pg-hba-conf.html",
    "How to set environment variables dynamically by script in Dockerfile?": "You need to cause the shell to load that file in every RUN command where you use it, and also at container startup time.\nFor startup time, you can use an entrypoint wrapper script:\n#!/bin/sh\n# Load the script of environment variables\n. /opt/intel/openvino/bin/setupvars.sh\n# Run the main container command\nexec \"$@\"\nThen in the Dockerfile, you need to include the environment variable script in RUN commands, and make this script be the image's ENTRYPOINT.\nRUN . /opt/intel/openvino/bin/setupvars.sh && \\\n    /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh && \\\n    /opt/intel/openvino/deployment_tools/demo/demo_squeezenet_download_convert_run.sh\n\nRUN ... && \\\n    . /opt/intel/openvino/bin/setupvars.sh && \\\n    cmake ... && \\\n    make && \\\n    ...\n\n COPY entrypoint.sh .\n ENTRYPOINT [\"./entrypoint.sh\"]\n CMD same as the command you set in the original image\nIf you docker exec debugging shells in the container, they won't see these environment variables and you'll need to manually re-read the environment variable script. If you use docker inspect to look at low-level details of the container, it also won't show the environment variables.\nIt looks like that script just sets a couple of environment variables (especially $LD_LIBRARY_PATH and $PYTHONPATH), if to somewhat long-winded values, and you could just set these with ENV statements in the Dockerfile.\nIf you look at the docker build output, there are lines like ---> 0123456789ab after each build step; those are valid image IDs that you can docker run. You could run\ndocker run --rm 0123456789ab \\\n  env \\\n  | sort > env-a\ndocker run --rm 0123456789ab \\\n  sh -c '. /opt/intel/openvino/bin/setupvars.sh && env' \\\n  | sort > env-b\nThis will give you two local files with the environment variables with and without running this setup script. Find the differences (say, with comm(1)), put ENV before each line, and add that to your Dockerfile.\nYou can't really use .bashrc in Docker. Many common paths don't invoke its startup files: in the language of that documentation, neither a Dockerfile RUN command nor a docker run instruction is an \"interactive shell\" so those don't read dot files, and usually docker run ... command doesn't invoke a shell at all.\nYou also don't need sudo (you are already running as root, and an interactive password prompt will fail); RUN sh -c is redundant (Docker inserts it on its own); and source isn't a standard shell command (prefer the standard ., which will work even on Alpine-based images that don't have shell extensions).",
    "dotnet restore comand into Dockerfile always finish with Exception": "You can solve it by using the below line\nRUN dotnet restore --disable-parallel\nAlso check here",
    "Alpine: unsatisfiable constraints": "I show the version of alpine with cat /etc/alpine-release and see that it is the 3.12.0. Then I search https://pkgs.alpinelinux.org/ for the package jasper-dev in the branch v3.12 and we see that it does not appear.\nSo we look in previous branches and see that it appears in v3.10, so I modify my Dockerfile to add that repo to /etc/apk/repositories with RUN echo 'http: // dl-cdn. alpinelinux.org/alpine/v3.10/main ' >> /etc/apk/repositories. This will install the package by executing the docker build",
    "How do I get this to work in a docker environment - missing ext-zip extension": "As I experienced, sometimes it is so confusing to install a php extension. Fortunately there is an awesome repository(docker-php-extension-installer) for doing it painlessly:\nCOPY --from=mlocati/php-extension-installer /usr/bin/install-php-extensions /usr/bin/\n\nRUN install-php-extensions zip\nAlso you can add any extensions without concerning about the dependencies.",
    "Docker throwing SyntaxError: Unexpected token ':'": "The issue is with the command Im running. Instead of running docker run test -p 8080:3000 it should have been docker run -p 8080:3000 test.",
    "How to install .pfx certificate in windows docker image": "To install a certificate (pfx or otherwise) into a nanoserver container during the docker build process, you need to use certoc.exe. Certoc.exe is part of windows server, you can find it on any server in c:\\windows\\system32\\certoc.exe. It doesn't come with nanoserver, however.\nHere's a section of dockerfile that I use to install CA certs into the trusted root certification authorities (\"root\") store:\nRUN MKDIR \"\\temp\"\nWORKDIR \"/temp\"\nCOPY [\"my-ca.cer\",\"/temp\"]\nCOPY [\"certoc.exe\",\"/temp\"]\nUSER \"ContainerAdministrator\"\nRUN .\\certoc -addstore root \"c:\\temp\\my-ca.cer\"\nRUN del /f /q .\\certoc.exe\nRUN del /f /q .\\my-ca.cer\nUSER \"ContainerUser\"\nNote that I'm doing the certificate install as ContainerAdmin, but everything else runs as ContainerUser (which is the best practice).\nYou can adapt the above to use certoc to import a pfx file, using\nUSER \"ContainerUser\"\nCOPY [\"mycert.pfx\",\"/temp\"]\nRUN .\\certoc -ImportPFX -p your_pfx_password_here \"My\" \"c:\\temp\\mycert.pfx\"\nwhich imports the pfx into the personal (\"My\") store of the container user.",
    "AppSettings for a .NET Core Project not overriding in Docker Container": "The Reason was surely not the ASP.NET Core migration but there was a mistake in using the Configuration builder. One need to specifically provide config.AddEnvironmentVariables(); in Program.cs so that the environment variables could be overwritten by the dockerfile\nIt's a small reason but sometime it could hit hard as it not possible to debug it in a production environment and also debugging is quite hard with docker environment.\nA good source to understand Environment Variable in .NET Core. Please Refer here",
    "Docker multi-stage build: COPY --from=builder failed: no such file or directory": "You cannot use $HOME (or any environment variable) in the COPY operation. You have details in this issue https://github.com/moby/moby/issues/34482\nSimple test:\nThis works:\nFROM ubuntu AS builder\n\nRUN echo \"fooo\" > ${HOME}/test.txt\n\nFROM ubuntu\nCOPY --from=builder /root/test.txt /tmp/test.txt\nRUN cat /tmp/test.txt\nThis doesn't work, same error as yours:\nFROM ubuntu AS builder\n\nRUN echo \"fooo\" > ${HOME}/test.txt\n\nFROM ubuntu\nCOPY --from=builder ${HOME}/test.txt /tmp/test.txt\nRUN cat /tmp/test.txt",
    "Local npm dependency \"does not a contain a package.json file\" in docker build, but runs fine with npm start": "Ok, this works. I changed my Dockerfile.dev to the following:\nFROM node:alpine\nWORKDIR '/app'\nCOPY ./shared /shared\nCOPY ./web /app\nRUN npm install\nCMD [\"npm\", \"run\", \"start\"]\nFrom the base project directory (where /shared and /web reside), I run:\ndocker build -t sockpuppet/client -f ./web/Dockerfile.dev .",
    "Docker multistage build without copying from previous image?": "Or are multi stage builds only useful for preparing some files and then copying those into another base image?\nThis is the main use-case discussed in \"Use multi-stage builds\"\nThe main goal is to reduce the number of layers by copying files from one image to another, without including the build environment needed to produce said files.\nBut, another goal could be not rebuild the entire Dockerfile including every stage.\nThen your suggestion (not copying) could still apply.\nYou can specify a target build stage. The following command assumes you are using the previous Dockerfile but stops at the stage named builder:\n$ docker build --target builder -t alexellis2/href-counter:latest .\nA few scenarios where this might be very powerful are:\nDebugging a specific build stage\nUsing a debug stage with all debugging symbols or tools enabled, and a lean production stage\nUsing a testing stage in which your app gets populated with test data, but building for production using a different stage which uses real data",
    "Docker - multiple volumes inside the dockerfile": "VOLUME X in a Dockerfile doesn't mean mount X from the host system. All this is doing is telling Docker to persist data written to /apps/demo/conf from within the container in a host directory. You might do that if you wanted to persist some data across container restarts for instance. If you want to mount a host filesystem within the container, you must use run -v as you said.",
    "Dockerfile from scratch using dynamic build": "THIS WORKED OUT BEST FOR ME - Dynamic Build\nThe very first thing was to find out all the dependencies for your exe file\n    $ldd dytest_publisher\n\n    linux-vdso.so.1 =>  (0x00007ffec3df9000)\n    libdl.so.2 => /usr/lib64/libdl.so.2 (0x00007ffb0e740000)\n    libnsl.so.1 => /usr/lib64/libnsl.so.1 (0x00007ffb0e526000)\n    libpthread.so.0 => /usr/lib64/libpthread.so.0 (0x00007ffb0e30a000)\n    librt.so.1 => /usr/lib64/librt.so.1 (0x00007ffb0e102000)\n    libstdc++.so.6 => /usr/local/lib64/libstdc++.so.6 (0x00007ffb0dd28000)\n    libm.so.6 => /usr/lib64/libm.so.6 (0x00007ffb0da26000)\n    libc.so.6 => /usr/lib64/libc.so.6 (0x00007ffb0d665000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007ffb0e945000)\n    libgcc_s.so.1 => /usr/local/lib64/libgcc_s.so.1 (0x00007ffb0d44c000)\nCopy the above libraries along with the folder structure in your project folder and create a docker file with name Dockerfile\nDockerfile\nFROM scratch\n\nCOPY rti_license.dat / \nCOPY USER_QOS_PROFILES.xml /     \nCOPY /objs/x64Linux3gcc4.8.2/DynamicTest_publisher /     \nCOPY /objs/x64Linux3gcc4.8.2/DynamicTest_subscriber /     \nCOPY /lib64/* /lib64/     \nCOPY /usr/lib64/* /usr/lib64/      \n\nENV LD_LIBRARY_PATH=/usr/lib64/:/lib64/     \n\nCMD [\"/dytest_publisher\"]   \nNote: project folder must have the directory: /usr/lib64/ and /lib64/ along with all the depended libraries. Depended libraries can be checked by the following command :\n$ldd \"your exe file\" \nBuild the docker image\ndocker build --tag dynamictest .\nRunning the created image\ndocker run --rm -it dynamictest\nTo see the running container\ndocker container ls\nPS : thanks to everyone for their help on this",
    "Docker copy file to host, after entrypoint execution": "Use docker volume.\nRun your container with docker run -v host/path:/temp imageID cp mvnOutputPath /temp\ncp mvnOutputPath /temp is the docker command CMD, executed by exec \"$@\" in your entrypoint",
    "Docker container connecting to Postgres database": "It looks like you're trying to connect to localhost. Your Postgres database is notrunning inside the same container as your django app, so you're not going to be able to find it at localhost. You need to point your app at the address of the Postgres container.\nIf you run your containers in a user defined network (using docker-compose will do this for you automatically), then you can use container name as hostnames.\nThe documentation on container networking is a good place to start.\nUpdate\nThe fact that you're running Postgres on your host doesn't substantially change the answer: you still need to point your webapp at the address of the Postgres server, rather than localhost.\nThe easiest way of doing that depends on whether or not you're running Docker natively on Linux, or you're running Docker-For-X, where X is MacOS or Windows.\nOn Linux, just point your webapp at the ip address of the docker0 interface. This is an address of your host, and since you have Postgres configured to listen on all interfaces, this should work out just fine.\nIf you're on Mac or Windows, there is a special \"magic hostname\" that refers to services on your host. E.g., read this for details under MacOS. In both cases, you can point your webapp at host.docker.internal.",
    "Passing environment variables in dockerfile": "I ran your Dockerfile and myIP is indeed empty when I run env inside of the container.\nTo fix it, try putting the ARG line AFTER the FROM line.\nSo,\nFROM node:11\n\nARG myIP\n\nENV myIP1 $myIP\n\nENV REACT_APP_MOCK_API_URL=http://${myIP1}:8080/API\nENV REACT_APP_MOCK_API_URL_AUTH=http://${myIP1}:8080/API/AUTH\nENV REACT_APP_MOCK_API_URL_PRESENTATION=http://${myIP1}:8080/API/PRESENTATION\nBuilding using this Dockerfile, I was able to set myIP.",
    "how can I communicate between containers?": "You want to use expose instead of ports:\nhttps://docs.docker.com/compose/compose-file/compose-file-v2/#expose\nFor example:\nservices:\n    app1:\n        expose:\n            - \"3000\"\n    app2:\n        ...\nassuming some API on port 3000 in app1, then app2 would be able to access http://app1:3000/api",
    "Is that possible to make common dockerfile for multiple containers which has kind of same configuration?": "Check out multistage builds: https://docs.docker.com/develop/develop-images/multistage-build/\nYour example would become:\nFROM ubuntu:14.04 as image1\nRUN apt-get update\nRUN apt-get install -y php5 php5-mysql\n\nFROM image1 as image2\nRUN apt-get install -y php5-dev php5-gd php5-memcache php5-pspell\nYour build command would need to specify the build target using the --target flag.",
    "How to assign different port to container replicas in docker swarm": "Ideally, tasks in a Swarm service should not be unique or contain unique data from one another. I know there are a few ways around this but it always feels non-ideal or hackish, especially when you're dealing with persistent data volumes that need a 1-to-1 match to a task.\nI'd recommend you split the three replicas out into three separate services. I know it sounds like more work, but in a single stack file, it'll give you the flexibility to deal with individual ports and volumes on them since they are unique containers.",
    "Docker rails elastic search": "You need to set ENV for ELASTICSEARCH_URL to correct value\nELASTICSEARCH_URL=\"http://<ip-of-your-docker-container>:9200\"\nAs you have linked network, you can provide as bellow\nELASTICSEARCH_URL=\"http://elasticsearch:9200\"\nLinks allow you to define extra aliases by which a service is reachable from another service. Any service can reach any other service at that service\u2019s name\nIf no ENV is set, your rails app will use http://localhost:9200",
    "Will the application run as PID 1 and will signals be received if we bootstrap it in a entrypoint.sh?": "Here is my try to follow what also the MySQL 5.7 Dockerfile does... but with an Nginx image.\nDockerfile:\nFROM nginx:latest\nCOPY docker-entrypoint.sh /usr/local/bin/\nRUN chmod 777 /usr/local/bin/docker-entrypoint.sh\n\nENTRYPOINT [\"docker-entrypoint.sh\"] \nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\ndocker-entrypoint.sh:\n#!/bin/bash\nset -e\n\necho \"preparing...\"\n\nexec \"$@\"\nThe idea is to do anything needed as a preparation inside the docker-entrypoint.sh script and with exec \"$@\" you then pass the signals to the CMD. (This means that you will have to use envsubst < /etc/nginx/conf.d/site.template > /etc/nginx/conf.d/default.conf inside docker-entrypoint.sh)\nLink of relevant example to Dockerfile docs: If you need to write a starter script for a single executable, you can ensure that the final executable receives the Unix signals by using exec and gosu commands...",
    "How to run official Tensorflow Docker Image as a non root user?": "Docker containers by default run as root. You can override the user by passing --user <user> to docker run command. Note however this might be problematic in case the container process needs root access inside the container.\nThe security concern you mention is handled in docker using User Namespaces. Usernamespaces basically map users in the container to a different pool of users on the host. Thus you can map the root user inside the container to a normal user on the host and the security concern should be mitigated.",
    "Angular in docker compose does not reload changes": "THe problem is related to how the filesystem works in Docker. To fix this I suggest you to either perform hot reloads (you have add EXPOSE 49153 in Dockerfile and ports - '49153:49153' in docker-compose.yml)\nThere are other solution like inotify or nodemon but they require that you use the --poll option when you start your application. The problem is that they keep polling the fs for changes and if the application is big your machine will be a lot slower than you'd like.\nI think I found the issue. You copy the ./app in /usr/src/app but you're setting .:/app as a volume. So this means that if you get in your docker instance you'll find your application in 2 places: /app and /usr/src/app.\nTo fix this you should have this mapping: .:/usr/src/app\nBtw, you're going to use the node_modules from your host and this might create some issues. To avoid this you can add an empty volume mapping: /usr/src/app/node_modules\nIf you get inside your running container, you'll find that the folder app exists twice. You can try it, by executing:\ndocker exec -it $instanceName /bin/sh\nls /app\nls /usr/src/app\nThe problem is that only the content of /app changes during your coding, while your application is currently executing the content of /usr/src/app which remains always the same.\nYour frontend in the docker-compose should look like this:\nfrontend:\n    build: ./frontend\n    volumes:\n      - .:/usr/src/app\n      - /usr/src/app/node_modules",
    "How to install ruby and bundler with dockerfile?": "There are pre built ruby images (e.g Alpine 3.11 Ruby 2.7) that include bundler. It's easier to start with them as they generally use the current \"best practices\" to build.\nNotice that they set the BUNDLE_SILENCE_ROOT_WARNING environment variable with ENV directive in the image build to remove that root warning.\nYou normally wouldn't run bundler as the CMD for a container either, you might run bundler during a RUN image build step though.\nRunning containers as non-root users is not a bad idea in any case. Use the USER directive to change that.\nFROM ruby:2.7\nWORKDIR /app\nADD . /app/\nRUN set -uex; \\\n    bundle install; \\\n    adduser -D rubyapp; \\\n    mkdir -p /app/data; \\\n    chown rubyapp /app/data\nUSER rubyapp\nCMD [ \"ruby\", \"whatever.rb\" ]",
    "How to order a Kafka startup shell script for a docker container?": "Basically you want to start ZK, then Kafka. Then somehow wait until Kafka is ready (that's the tricky part), do your job with kafka (e.g. topic creation in your case), and then wait until Kafka & ZK have finished (what happens on interrupt).\nstart-zookeeper &\nZK_PID=$!\nstart-kafka &\nKAFKA_PID=$!\n\n# that's the tricky part\nwait_for_kafka\ncreate-topic.sh\n\nwait \"${KAFKA_PID}\"\nwait \"${ZK_PID}\"\nAs mentioned, the Kafka-readiness might be tricky - the following ways might be helpful:\nwaiting until Kafka responds to read requests (e.g. probing with kafka-topic.sh --list periodically)\ncreating a pocket consumer / AdminClient (java kafka 0.11+) and getting metadata (similar to above point)\nchecking existence of JMX beans for logs/controller etc.\nchecking listening port availability",
    "Is it possible to remove unwanted packages from docker image?": "First of all if you want to reduce an OS size, don't start with big one like CentOS, you can start with alpine which is small\nNow if you are still keen on using CentOS, do the following:\ndocker run -d --name centos_minimal centos:7.2.1511 tail -f /dev/null\nThis will start a command in the background. You can then get into the container using\ndocker exec -it centos_minimal bash\nNow start removing packages that you don't need using yum remove or yum purge. Once you are done you can commit the image\ndocker commit centos_minimal centos_minimal:7.2.1511_trial1\nExperimental Squash Image\nAnother option is to use an experimental feature of the build command. In this you can have a dockerfile like below\nFROM centos:7\nRUN yum -y purge package1 package2 package2\nThen build this file using\ndocker build --squash -t centos_minimal:squash .\nFor this you need to add \"experimental\": true to your /etc/docker/daemon.json and then restart the docker server",
    "Can't start ASP.NET Core web API Docker on a specified port": "You have exposed the port, but you haven't published it. You can either use -P to publish all exposed ports or -p and then specify port mapping. For example:\ndocker run -p 58912:58912 -e \"ASPNETCORE_URLS=http://+:58912\" -it --rm sga",
    "passing name of generated jar file from Maven pom.xml to Dockerfile": "Inside pom.xml\n<build>\n    <plugins>\n        <plugin>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-maven-plugin</artifactId>\n        </plugin>\n        <plugin>\n            <artifactId>maven-resources-plugin</artifactId>\n            <executions>\n                <execution>\n                    <id>copy-resources</id>\n                    <phase>validate</phase>\n                    <goals>\n                        <goal>copy-resources</goal>\n                    </goals>\n                    <configuration>\n                        <outputDirectory>${basedir}/target/</outputDirectory>\n                        <resources>\n                            <resource>\n                                <directory>src/main/docker</directory>\n                                <filtering>true</filtering>\n                            </resource>\n                        </resources>\n                    </configuration>\n                </execution>\n            </executions>\n        </plugin>\n    </plugins>\n</build>\nMake sure to place the Dockerfile in the location mentioned in pom, so that it could build and copy the Dockerfile with replacing the references dynamically.\nInside Dockerfile\nADD @project.artifactId@-@project.version@.jar app.jar\ncredits: https://github.com/vmudigal/microservices-sample",
    "Is it possible to skip a FROM command in a multistage dockerfile?": "I don't think you can skip FROM command. Build should start from somewhere, even if it is scratch.\nWhile for trying to create a dynamic dockerfile, you can create the dockerfile using a shell script. I came across one such script at parity-deploy.sh, which dynamically creates a docker-compose.yml file on the basis of configurations provided by user.",
    "Module Not Found When Run on Docker-Compose": "My bad. It was case sensitive errors. Since I was on windows it just went through without problems.\nSo the error happened because the import statement was finding App folder, but the actual folder was app.",
    "Make build docker task more productive": "No need to rebuild your docker image every time, use volumes instead.\nFor example run:\ndocker run -v /path/to/the/code/on/host:/path/to/the/code/on/container your image nodemon your_file.js",
    "ADD command with network path in Windows Containers Dockerfiles": "Whatever you ADD or COPY must be in the docker build context.\nWhen you do this:\ndocker build .\nThat directory param (the . in the example) is the context that is copied and sent to the Docker daemon. Then the docker daemon use those files to COPY or ADD. It won't use any file that is not in that context.\nThat is the issue that you are experiencing. I'm not sure how you can solve it anything than copying the files from \\\\myserver to your build directory.\nADD is capable of download files by providing an URL (should investigate if it supports Windows' shares)",
    "Passing arguments from CMD in docker": "You can do that through a combination of ENTRYPOINT and CMD.\nThe ENTRYPOINT specifies a command that will always be executed when the container starts.\nThe CMD specifies arguments that will be fed to the ENTRYPOINT.\nSo, with Dockerfile:\nFROM node:boron\n...\n\nENTRYPOINT [\"node\", \"src/akamai-client.js\"]\n\nCMD [\"purge\", \"https://www.example.com/main.css\"]\nThe default behavior of a running container:\ndocker run -it akamaiapi\nwould be like command :\nnode src/akamai-client.js purge \"https://www.example.com/main.css\"\nAnd if you do :\ndocker run -it akamaiapi queue\nThe underlying execution in the container would be like:\nnode src/akamai-client.js queue",
    "Docker can't start MariaDB/MySQL during Docker build": "1)It is not stuck. The MariaDB start running. But you need to use CMD command and not RUN command to achieve your purpose.\nThe command CMD, similarly to RUN, can be used for executing a specific command. However, unlike RUN it is not executed during build, but when a container is instantiated using the image being built.\n2) The last part of your dockerfile should look like this\n#Run as user mysql\nUSER mysql\n\nRUN echo \"@TODO: Create DB, User and grant access\"\n\n#Expose Port\nEXPOSE 3306\n\n#Right way to run mysqld\nCMD [\"mysqld\"]\nFor building your image\ndocker build -t testmariadb .\nFor running the built image\ndocker run testmariadb\nOR ( for detached mode )\ndocker run -d testmariadb\n3) Please ask your self why are you using ubuntu base image (FROM ubuntu:14.04). According to me you should use https://hub.docker.com/_/mariadb/ if you just want to run mariaDb in the container.",
    "Dockerfile build using complex powershell script on windows container": "In order to resolve this issue I think you need to add additional double quotes into the section of your command which replaces the database name. To show an example I have pasted a Dockerfile below which produced the results I expected based on what I understand you are trying to achieve in the question.\nFROM microsoft/windowsservercore\n\nCOPY telegraf.conf C:\\\\telegraf\\\\telegraf.conf\n\nRUN powershell -NoProfile -Command \" \\\n$lines = (Get-Content c:\\telegraf\\telegraf.conf).replace('http://localhost:8086', 'http://publichost.com:8086'); \\\nSet-Content c:\\telegraf\\telegraf.conf $lines; \\\n$lines = (Get-Content c:\\telegraf\\telegraf.conf).replace('database = \"\"\"telegraf\"\"\"', 'database = \"\"\"qa\"\"\"'); \\\nSet-Content c:\\telegraf\\telegraf.conf $lines; \\\n\"\nObviously I expect your Dockerfile will look different, but if you take the example I have used above and add the additional double quotes around those that you have inside your string then you should find that it works as expected.\nAny questions, let me know.",
    "Docker: share private key via arguments": "If you can use the latest docker 1.13 (or 17.03 ce), you could then use the docker swarm secret: see \"Managing Secrets In Docker Swarm Clusters\"\nThat allows you to associate a secret to a container you are launching:\ndocker service create --name test \\\n    --secret my_secret \\\n    --restart-condition none \\\n    alpine cat /run/secrets/my_secret\nIf docker swarm is not an option in your case, you can try and setup a docker credential helper.\nSee \"Getting rid of Docker plain text credentials\". But that might not apply to a private ssh key.\nYou can check other relevant options in \"Secrets and LIE-abilities: The State of Modern Secret Management (2017)\", using standalone secret manager like Hashicorp Vault.",
    "Docker apache and nginx port conflict": "https://docs.docker.com/compose/compose-file/#/ports explains that the host port comes first. You're explicity exposing port 80 for nginx and for apache. For nginx you're exposing container 80 as host 80, and in apache container 800 as port 80.\nLikely you just got the numbers reversed.",
    "What are the differences between sharing a dockerfile on git and sharing a docker container?": "Both are effectively the same thing\nA rough analogy would be saying:\nShould I share my source code? or\nShould I share the compiled executable?\nYou can choose anything.. but if the end user isn't interested in modifying the source, you might as well make it convenient for them and just share the compiled executable.\nSame goes for the dockerfile or the image created using the dockerfile.",
    "how to copy dir from remote host to docker image": "I'd say there are at least options for dealing with that:\nOption 1: If you can execute scp before running docker build this may turn out to be the easiest option:\nRun scp -r somewhere:remote_dir ./local_dir\nAdd COPY ./local_dir some_path to your Dockerfile\nRun docker build\nOption 2: If you have to execute scp during the build:\nStart some key-value store such as etcd before the build\nPlace a correct SSH key (it cannot be password-protected) temporarily in the key-value store\nWithin a single RUN command (to avoid leaving secrets inside the image):\nretrieve the SSH key from the key-value store;\nput it in ~/.ssh/id_rsa or start an ssh-agent and add it;\nretrieve the directory with scp\nremove the SSH key\nRemove the key from the key-value store\nThe second option is a bit convoluted, so it may be worth creating a wrapper script that retrieves the required secrets, runs any command, and removes the secrets.",
    "Docker container does not give me a shell": "docker run -t -i moul/phoronix-test-suite /bin/bash will not give you a bash (contrary to docker run -it fedora bash)\nAccording to its Dockerfile, what it will do is execute\nphoronix-test-suite /bin/bash\nMeaning, it will pass /bin/bash as parameter to phoronix-test-suite, which will exit immediately. That leaves you no time to execute a docker exec -it <container> bash in order to open a bash in an active container session.",
    "Use Docker environment variables for credentials": "That looks like a very good approach overall.\nHowever, I don't think you can complete hide the environment variables from someone who has permissions to inspect process envs. It seems to me that if you find out the process id of the application process (inside the container or from the host) you should be able to find its environment in /proc. Won't show up as docker env, but it's still in there somewhere. Also, any such person can probably connect to your Vault directly anyway.\nMeaning, yes, this will reliably not make the environment of child processes show up in the container environment, but it does not really hide it from anyone (who can already access your host machine and control docker).\nStill, congrats on this setup. Much better than having credentials built into images.",
    "Setting a policy for RabbitMQ as a part of Dockerfile process": "You're in a slightly tricky situation with RabbitMQ as it's mnesia data path is based on the host name of the container.\nroot@bf97c82990aa:/# ls -1 /var/lib/rabbitmq/mnesia\nrabbit@bf97c82990aa\nrabbit@bf97c82990aa-plugins-expand\nrabbit@bf97c82990aa.pid\nFor other image builds you could seed the data files, or write a script that RUN calls to launch the application or database and configure it. With RabbitMQ, the container host name will change between image build and runtime so the image's config won't be picked up.\nI think you are stuck with doing the config on container creation or at startup time.\nOptions\nCreating a wrapper CMD script to do the policy after startup is a bit complex as /usr/lib/rabbitmq/bin/rabbitmq-server runs rabbit in the foreground, which means you don't have access to an \"after startup\" point. Docker doesn't really do background processes so rabbitmq-server -detached isn't much help.\nIf you were to use something like Ansible, Chef or Puppet to setup the containers. Configure a fixed hostname for the containers startup. Then start it up and configure the policy as the next step. This only needs to be done once, as long as the hostname is fixed and you are not using the --rm flag.\nAt runtime, systemd could complete the config to a service with ExecStartPost. I'm sure most service managers will have the same feature. I guess you could end up dropping messages, or at least causing errors at every start up if anything came in before configuration was finished?",
    "Why does \"docker run\" error with \"no such file or directory\"?": "Using service isn't going to fly - the Docker base images are minimal and don't support this. If you want to run multiple processes, you can use supervisor or runit etc.\nIn this case, it would be simplest just to start mongo manually in the script e.g. /usr/bin/mongod & or whatever the correct incantation is.\nBTW the lines where you try to clean up don't have much effect:\nRUN rm -f /tmp/apache-maven-3.2.2.tar.gz\n...\n# remove download archive files\nRUN apt-get clean\nThese files have already been committed to a previous image layer, so doing this doesn't save any disk-space. Instead you have to delete the files in the same Dockerfile instruction in which they're added.\nAlso, I would consider changing the base image to a Java one, which would save a lot of work. However, you may have trouble finding one which bundles the official Oracle JDK rather than OpenJDK if that's a problem.",
    "EADDRNOTAVAIL when Dockerizing Node.js app": "Turns out robertklep in the comments to my question was correct: the issue was trying to explicitly pass the IP of the server into app.js. That's the way the app was configured to work just by itself with Node, but that can't be done with Docker.\nThe only code change needed was removing config_json.app_host from app.listen in app.js. Then, running the container by binding the exposed port to any available port on the server causes it to work.",
    "Docker node development environment on windows": "Try the following:\nFROM node\nCOPY ./package.json /src\nRUN cd /src && npm install\nCOPY . src/\nEXPOSE  3000\nCMD [\"node\", \"/src/express.js\"]\nThe way you originally have it will install npm packages everytime you change something within src. If we separate this step, these packages will only be installed if the package.json file changes.",
    "C# Protobuf project fails to build in Docker": "Just ran into the same issue. For those who might encounter this in the future, the error stems from the Alpine Docker image lacking certain tools that grpc.tools needs. You can resolve this by installing the necessary tools directly into the Alpine image. Here's how:\n    FROM mcr.microsoft.com/dotnet/sdk:7.0.203-alpine3.17 AS build\n    RUN apk add --no-cache libc6-compat\n    # ... rest of your Dockerfile ...",
    "Why execute command in docker container get different result with writing the command in Dockerfile?": "You need to remove the single quotes inside the JSON-array-syntax CMD line\nCMD [\"mvn\", \"exec:java\", \"-Dexec.mainClass=com.demo.App\", \"-e\"]\n#                                          ^          ^ no single quotes\nWhen you run this via a shell, the shell parser sees the single-quoted string, ignores the special meaning of any punctuation inside it, and removes the single quotes. In a Docker exec-form command, none of this processing happens; the only escaping is what's necessary to make the command a valid JSON array.\nThis means in your original form the single quotes were passed on to the actual command. You see that in the final exception, where Maven is trying to interpret a string (still including single quotes) as a Java class name.",
    "Docker COPY cannot find the file": "When you build using docker build - there is no build context and you can't use COPY or ADD, unless they copy from a URL.\nSince there is no context, a Dockerfile ADD only works if it refers to a remote URL.\nYou need a context, so you should use\ndocker build -t myimage .\ninstead.\nMore info here https://docs.docker.com/engine/reference/commandline/build/#build-with--",
    "gunicorn: command not found": "You should RUN pip install as root, without the --user option.\nIf you run pip install --user, it installs packages into ~/.local. In Docker the $HOME environment variable isn't usually defined, so this will install them into /.local. If you looked inside the built image, you'd probably find a runnable /.local/bin/gunicorn, but that /.local/bin directory isn't on the $PATH.\nTypical use in Docker is to install Python packages into the \"system\" Python; the Docker image itself is isolation from other Pythons. This will put the *.py files into somewhere like /usr/local/lib/python3.10/site-packages, and the gunicorn executable will be in /usr/local/bin, which is on the standard $PATH.\nFROM python:3.10.5-alpine\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nRUN adduser -D appuser\nWORKDIR /home/appuser/\n# but do not change USER yet\n\nCOPY --chown=appuser:appuser requirements.txt .\n\n# Without --user option\nRUN python -m pip install --no-cache-dir --disable-pip-version-check --requirement requirements.txt\n\n# Change USER _after_ RUN pip install\nUSER appuser\n\nCOPY --chown=appuser:appuser . .\nENTRYPOINT [ \"./entrypoint.sh\" ]",
    "Grafana on Docker": "It looks like there are some problems with the volume configuration in your Grafana container:\nFirst, I think this was simply a typo in your question:\n      - grafana_ini:/etc/grafana/grafana.inianticipated location in container\nI suspect that you were actually intending this:\n      - grafana_ini:/etc/grafana/grafana.ini\nWhich doesn't make any sense: grafana.ini is a file, but a volume is a directory. Docker won't allow you to mount a directory on top of a file, hence the error:\nERROR: .../etc/grafana/grafana.ini is not directory\nYou have the same problem with the grafana_data volume, which you're attempting to mount on top of datasource.yml:\n      - grafana_data:/etc/grafana/provisioning/datasources/datasource.yml\nI think you may be approaching this configuration in the wrong way; you may want to read through these documents:\nhttps://grafana.com/docs/grafana/latest/installation/docker/\nhttps://grafana.com/docs/grafana/latest/administration/configure-docker/\nhttps://grafana.com/docs/grafana/latest/administration/provisioning/\nIt is possible to configure Grafana (and Prometheus!) using only bind mounts and environment variables (this includes installing plugin, data sources, and dashboards), so you don't need to build your own custom images.\nUnrelated to this particular problem, there are some other things in your docker-compose.yml that are worth changing. You should no longer be using the links directive...\n    links:\n      - prometheus\n...because Docker maintains DNS for you automatically; your containers can refer to each other by name with no additional configuration.",
    "Connect SQL Server and nodejs with docker-compose": "You gave wrong sever address. Never forget \"localhost\" in dockers means inside that docker.\nTo fix this config sever address \"db\"",
    "Can we run AVX required software without, having it?": "I've just helped to fix similar problem on M1 Mac to a colleague, we managed to resolve it by defining platform along with the image name. We used:\nimage: arm64v8/mongo:5.0\nplatform: linux/arm64/v8\nso in the command it is:\ndocker run --platform linux/arm64/v8 ...\nand in the docker-compose.yml:\nservices:\n  mongodb:\n    image: arm64v8/mongo:5.0\n    platform: linux/arm64/v8\nIn our case it works at the end also without the --platform, only with changing the image name from mongo to arm64v8/mongo.",
    "Docker File: EACCES: permission denied error - Could not write file : Node": "COPYed files are always owned by root by default. Add --chown= to your COPY statement to have the copied files be owned by 'myuser' like this\nCOPY --chown=myuser:myuser package.json /home/myuser/app/spa-proxy\nCOPY --chown=myuser:myuser package-lock.json /home/myuser/app/spa-proxy\nand\nCOPY --chown=myuser:myuser . /home/myuser/app/spa-proxy",
    "How should I build a docker image for a development database?": "I've found my way to the following approach:\nVolume definitions are stored in image metadata, so it is possible to manually alter this metadata in order to remove VOLUME declarations and save the result to a new image. This process has been encapuslated in a helpful Python package called docker-copyedit.\n$ docker pull mariadb\n$ pip install docker-copyedit\n$ docker-copyedit.py FROM mariadb INTO mariadb:novolumes REMOVE ALL VOLUMES\nNow that I have a local :novolumes image of MariaDB, I can automate the database inside of a Dockerfile build. In order to use the image's built-in functionality for creating a new database and restoring from the /docker-entrypoint-initdb.d/ directory, I created a bash script which sources docker-entrypoint.sh and then executes the relevant portions of _main() to do the restore and then exit without executing _main() directly (which would cause the database to be restarted so that the build never exits). Then I use a second build stage to import just the /var/lib/mysql directory from the restore stage.\nDockerfile\nFROM mariadb:novolumes AS restorer\n\nENV MARIADB_ALLOW_EMPTY_ROOT_PASSWORD=yes\n\nCOPY data/source/database.sql /docker-entrypoint-initdb.d/\n\nCOPY restore_db.sh restore_db.sh\nRUN [\"./restore_db.sh\", \"mysqld\"]\n\nFROM mariadb:novolumes\n\nCOPY --from=restorer /var/lib/mysql /var/lib/mysql\nrestore_db.sh\n#!/bin/bash\n\n# Import helper functions\nsource docker-entrypoint.sh\n\n# Direct copy-paste of lines 356-392 from docker-entrypoint.sh:\n\nmysql_note \"Entrypoint script for MariaDB Server ${MARIADB_VERSION} started.\"\n\nmysql_check_config \"$@\"\n# Load various environment variables\ndocker_setup_env \"$@\"\ndocker_create_db_directories\n\n# If container is started as root user, restart as dedicated mysql user\nif [ \"$(id -u)\" = \"0\" ]; then\n  mysql_note \"Switching to dedicated user 'mysql'\"\n  exec gosu mysql \"$BASH_SOURCE\" \"$@\"\nfi\n\n# there's no database, so it needs to be initialized\nif [ -z \"$DATABASE_ALREADY_EXISTS\" ]; then\n  docker_verify_minimum_env\n\n  # check dir permissions to reduce likelihood of half-initialized database\n  ls /docker-entrypoint-initdb.d/ > /dev/null\n\n  docker_init_database_dir \"$@\"\n\n  mysql_note \"Starting temporary server\"\n  docker_temp_server_start \"$@\"\n  mysql_note \"Temporary server started.\"\n\n  docker_setup_db\n  docker_process_init_files /docker-entrypoint-initdb.d/*\n\n  mysql_note \"Stopping temporary server\"\n  docker_temp_server_stop\n  mysql_note \"Temporary server stopped\"\n\n  echo\n  mysql_note \"MariaDB init process done. Ready for start up.\"\n  echo\nfi\nFinally, I can do the build and run the container to quickly create a local development database, and/or push the image to a repository.\n$ docker build -t NReilingh/devdb .\n$ docker run --rm -d NReilingh/devdb",
    "Redirect STDOUT of a RUN command in a Dockerfile": "you are using RUN with the exec from RUN [\"executable\", \"param1\", \"param2\"] where shell features like expanding variables, sub commands, piping output, chaining commands together won't work, use RUN in shell form enter the command as one string RUN <command>\nCOPY ./a /tmp\nCOPY ./b /tmp\nRUN ls /tmp\nRUN cat /tmp/a /tmp/b > /tmp/c\nRUN ls /tmp",
    "How can we run flyway/migrations script inside Cassandra Dockerfile?": "Your problem is that you don't call the original entrypoint to start Cassandra - you overwrote it with your own code, but it just running the cqlsh, without starting Cassandra.\nYou need to modify your code to start Cassandra using the original entrypoint script (source) that is installed as /usr/local/bin/docker-entrypoint.sh, and then execute your script, and then wait for termination signal (you can't just exit from your script, because it will terminate the image.",
    "Why docker needs to build all the previous stages?": "There are two different backends for docker build. The \"classic\" backend works exactly the way you describe: it runs through the entire Dockerfile until it reaches the final stage, so even if a stage is unused it will still be executed. The newer BuildKit backend can do some dependency analysis and determine that a stage is never used and skip over it as you request.\nVery current versions of Docker use BuildKit as their default backend. Slightly older versions have BuildKit available, but it isn't the default. You can enable it by running\nexport DOCKER_BUILDKIT=1\nin your shell environment where you run docker build.\n(It's often a best practice to run the same Docker image in all environments, and to use separate Dockerfiles for separate components. That avoids any questions around which stages exactly get run.)",
    "Can I specify the color of a service's log prefix in my docker-compose?": "There's an old issue requesting this feature in docker-compose, which was (unfortunately?) rejected.\nIf the main issue is that the color clashes with your background, you could use the --no-color option to get a monochrome output. It's not great, but at least it would stop clashing with your background.",
    "NodePort exposed Port connection refused": "For structural reasons, it's better to specify the nodePort in your service yaml configuration file or kubernetes will allocate it randomly from the k8s port range (30000-32767). In the ports section it's a list of ports no need, in your case, to specify a name check the nodePort_docs for more infos. This should work for you :\napiVersion: v1\nkind: Service\nmetadata:\n  name: posts-srv\nspec:\n  type: NodePort\n  selector:\n    app: posts\n  ports:\n    - port: 4000\n      targetPort: 4000\n      nodePort: 32140\n      protocol: TCP\nTo connect to the nodePort service verify if any firewall service is up then verify that this port is enabled in your VMs : (centos example)\nsudo firewall-cmd  --permanent --add-port=32140/tcp\nFinally connect to this service using any node IP address (not the CLusterIP, this IP is an INTERNAL-IP not accessible outside the cluster) and the nodePort : <node_pubilc_IP>:<nodePort:32140>",
    "Docker - /bin/sh: nodemon: not found": "Try with below,\n#Dockerfile\n\nFROM node:alpine\n\nWORKDIR /backend\n\nCOPY package*.json .\n\nRUN yarn\n\nRUN yarn add global nodemon\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"yarn\", \"dev\"]\nSIDE NOTE: You better not using nodemon in production. Serve build files as in your start script. You have to add the build command before yarn start.\n#Dockerfile\n\nFROM node:alpine\n\nWORKDIR /backend\n\nCOPY package*.json .\n\nRUN yarn\n\nCOPY . .\n\nRUN yarn run build\n\nEXPOSE 5000\n\nCMD [\"yarn\", \"start\"]",
    "Add files to a docker image using a loop": "You can use scripting to create your Dockerfile;\necho \"FROM ubuntu:20.04\" > Dockerfile\necho \"RUN apt-get update && \\\n    rm -rf /var/lib/apt/lists/*\" >> Dockerfile\n\nfor f in part_*; do echo \"ADD $f /data/\" >> Dockerfile; done\n\necho \"COPY entrypoint.sh /usr/local/bin/\" >> Dockerfile\necho \"CMD entrypoint.sh\" >> Dockerfile",
    "Rebuild Docker images when its packages received (security) updates": "I would say this is the biggest weakness that docker has not solved at all.\nThe usual workaround afaik is to rebuild the image from scratch nightly (on a build server?) and then auto promote it to production if the your tests run fine against it.",
    "Docker-compose doesn't copy Test files from local folder into Container , but copies everything else": "Based on the comments, I thought I'd be more formal & a more complete answer. When using Docker, if you want something included inside the container you need to make sure that you don't have a .dockerignore file, in hindsight it's obvious, like any other ignore file, you're choosing to exclude things from whatever it is you're working on. I.E. .gitignore, same idea, different tool I guess? \ud83d\ude42\nThough, as you mentioned *.test.js, I'd wonder why you'd want that in your container & not as a part of your CI process, even if you run your tests via Docker, i.e. if you look at this example, it could save you removing anything from your .dockerignore file. But of course this is a context specific subject & it's entirely up to you & your team at the end of the day, it's nothing more than a suggestion at most.\nHope that helps! \ud83d\ude00",
    "Can we update git version in the Docker Image?": "Next means node:10.13.0 use debian9, aka stretch.\n$ docker run --rm node:10.13.0 cat /etc/issue\nDebian GNU/Linux 9 \\n \\l\nNext means node:10.13.0 default use git 2.11.\n$ docker run --rm node:10.13.0 git --version\ngit version 2.11.0\nIn fact, git in debian 9 apt repo use the version 2.11, if you want to upgrade to a newer version, you could use debian backports, which means:\nBackports are packages taken from the next Debian release\nBy default, backports won't be used when use apt. You could use next sample to enable this.\nDockerfile:\nFROM node:10.13.0\nRUN echo \"deb http://deb.debian.org/debian stretch-backports main contrib non-free\" >> /etc/apt/sources.list; \\\n    apt-get update; \\\n    apt-get -t stretch-backports install git -y\nVerify it:\n$ docker build -t mynodeimage .\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM node:10.13.0\n......\nSuccessfully tagged mynodeimage:latest\n$ docker run --rm mynodeimage git --version\ngit version 2.20.1",
    "docker-compose doesn't see contents of a particular directory": "This must have been a glitch in Docker. On 12/18 release 3.0.2 was available. Installing the update, and bringing up the containers made the issue go away. To confirm I rolled back to 3.0.1, and witnessed the issue again. I also did a full purge of my containers by running docker system prune -a and rebuilding everything from scratch.\nAs of Docker Desktop Community edition for Mac 3.0.2, this is no longer an issue.",
    "Build args are not resolved": "Your issue is explained in this note of the docker-compose documentation:\nScope of build-args\nIn your Dockerfile, if you specify ARG before the FROM instruction, ARG is not available in the build instructions under FROM. If you need an argument to be available in both places, also specify it under the FROM instruction. Refer to the understand how ARGS and FROM interact section in the documentation for usage details.\nSource: https://docs.docker.com/compose/compose-file/#args, emphasis, mine\nSo the fix is as simple as moving the ARG XDEBUGVERSION of your Dockerfile below the FROM.\nARG PHPVERSION\n\nFROM php:${PHPVERSION}-fpm\n\nARG XDEBUGVERSION\n\nRUN apt-get update && apt-get install -y \\\n    && pecl install xdebug-${XDEBUGVERSION} \\\n    && docker-php-ext-enable xdebug \nAlong with your docker-compose.yml and the container is starting as expected.\nThen to convince you that the PHP image is the right one, you should look at the execution step (here is for my trial):\nStep 2/4 : FROM php:${PHPVERSION}-fpm\n ---> 8407023453aa\nThen run the command: docker images:\nREPOSITORY                    TAG                 IMAGE ID            CREATED             SIZE\nphp                           7.4-fpm             8407023453aa        6 days ago          405MB\nAnd if the image ID there does match, then you are sure you have the right image.",
    "Exchangelib on Docker with custom SSL Certificates": "Ok, after more than a week spent on checking my certificates and what was going on with docker, @Erik Cederstrand enlightened me, suggesting to check the requests's certificate path.\nIt turned out that indeed it was set to a different location, not sure why. So, to be sure, I changed both REQUESTS_CA_BUNDLE and SSL_CERT_FILE environment variables to point to my ca certificates file (which is the system one on ubuntu/debian).\nAnyway, my final Dockerfile looks as follows\nFROM python:3.8-slim\n\nWORKDIR .\n\nCOPY . .\n\nENV SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt\nENV REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\n\nCOPY ./ssl/* /usr/local/share/ca-certificates/\n\nRUN update-ca-certificates\n\nRUN pip install -r requirements.txt\n\nEXPOSE 8000\n\nCMD [ \"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\"]\nMany thanks to @Erik Cederstrand",
    "are VOLUME in Dockerfile persistent in kubernetes": "(answer based on what I observed with Rancher 2.4, kubernetes 1.17 using Docker)\nShort answer:\na Docker volume is created in /var/lib/docker/volumes/... and removed when the pods is stopped or redeployed.\nLong answer:\nKubernetes doesn't seems to have any knowledge of the volume the containers / in the Dockerfile. No Kubernetes objects seems to be created.\nWhen Kubernetes tell Docker daemon to start a container, Docker creates a volume (like docker volume create) and attach the created volume (unless kubernete's provides a volume to mount).\nWhere are the file stored ?\nThe file is a regular Docker volume (see docker volume below)\nIs the volume persistent ?\nNo, the docker volume is removed when the pod is removed (like if you ran docker rm $mycontainer --volumes)\n    docker inspect 6ce5f52186d4 | grep '\"Driver\": \"local\"' -A5 -B5\n            {\n                \"Type\": \"volume\",\n                \"Name\": \"679135a23430ceea6adb8d89e04c6baf9da33239a83ecb4c4ec3263e2c925d39\",\n                \"Source\": \"/var/lib/docker/volumes/679135a23430ceea6adb8d89e04c6baf9da33239a83ecb4c4ec3263e2c925d39/_data\",\n                \"Destination\": \"/var/lib/postgresql/data\",\n                \"Driver\": \"local\",\n                \"Mode\": \"\",\n                \"RW\": true,\n                \"Propagation\": \"\"\n            }\n $ docker volume ls\nDRIVER              VOLUME NAME\nlocal               679135a23430ceea6adb8d89e04c6baf9da33239a83ecb4c4ec3263e2c925d39\n $ du -hs /var/lib/docker/volumes/679135a23430ceea6adb8d89e04c6baf9da33239a83ecb4c4ec3263e2c925d39/\n51.4M   /var/lib/docker/volumes/679135a23430ceea6adb8d89e04c6baf9da33239a83ecb4c4ec3263e2c925d39/",
    "ignore tests in librd kafka": "I just had to run it -tags musl\ngo vet -tags musl ./...",
    "How to configure application properties file dynamically in docker": "entrypoint is the secret.\nYou have two solutions:\ndesign the image to receive these parameters thru environment variables, and let the ENTRYPOINT injects them inside App/bin/config/application.properties\ndesign the image to listen to a directory. If this directory contains *.properties files, the ENTRYPOINT will collect these files and combine them into one file and append the content with App/bin/config/application.properties\nBoth solutions have the same Dockerfile\nFrom java:x\n\nCOPY jar ...\nCOPY myentrypoint /\nENTRYPOINT [\"bash\", \"/myentrypoint\"]\nBut not the same ENTRYPOINT (myentrypoint)\nsolution A - entrypoint:\n#!/bin/bash\n# if the env var DB_URL is not empty\nif [ ! -z \"${DB_URL}\" ]; then\n  \n   echo \"url=${DB_URL}\" >> App/bin/config/application.properties\nfi\n# do the same for other props\n#...\nexec call-the-main-entry-point-here $@\nTo create a container from this solution :\n docker run -it -e DB_URL=jdbc:postgresql://localhost:5432/sakila myimage\nsolution B - entrypoint:\n#!/bin/bash\n\n# if /etc/java/props.d/ is a directory\nif [ -d \"/etc/java/props.d/\" ]; then\n   cat /etc/java/props.d/*.properties\n   awk '{print $0}' /etc/java/props.d/*.properties >> App/bin/config/application.properties\nfi\n\n#...\nexec call-the-main-entry-point-here $@\nTo create a container from this solution :\n docker run -it -v ./folder-has-props-files:/etc/java/props.d myimage",
    "VS Code \"Attach Visual Studio Code\" to remote container error": "This may not be a real answer but it's too much for a comment.\nI believe you have a local machine and docker on a remote server.\nThe first thing you have to do is to install docker on your local machine and configure it so that's its looking for the docker host on your remote server.\nThen you can create a .devcontainer.json on your machine. If you have the extension installed, VSCode will offer you do open this as container environment. Since your docker host sits on remote, this will now happen on your server instead of your local machine.\nWhen I did the setup, I followed amongst other things this guide. Especially the SSH-Agent was required to get a remote docker host working. https://code.visualstudio.com/docs/remote/containers-advanced#_a-basic-remote-example\nHere is a example .devcontainer file of mine.\nNow back to your initial question, I don't think you will be able to use the remote container extension on a container that wasn't started as dev container. This is because vscode will install a bunch of stuff in there when its first set up. Similar to the SSH Extension. I may be wrong on this so take it with a grain of salt.\nIt may also be worth noting that once you connect to your server via SSH and have then the regular docker extension (which is not the remote container extension) installed, on remote, you will see your docker images listed there. But that does not mean you will be able to connect like that from local to remote container. For that you need to configure a docker remote host.",
    "Installing SSH on a Docker image": "You can use DEBIAN_FRONTEND to disable interaction with user (DEBIAN_FRONTEND):\n   noninteractive\n          This is the anti-frontend. It never interacts with you  at  all,\n          and  makes  the  default  answers  be used for all questions. It\n          might mail error messages to root, but that's it;  otherwise  it\n          is  completely  silent  and  unobtrusive, a perfect frontend for\n          automatic installs. If you are using this front-end, and require\n          non-default  answers  to questions, you will need to preseed the\n          debconf database; see the section below  on  Unattended  Package\n          Installation for more details.\nLike this:\nFROM ubuntu:latest\nENV DEBIAN_FRONTEND=noninteractive\nENV TZ=Europe/London\nRUN apt update && \\\n    apt -y install ...",
    "docker : do not override base image entrypoint": "As mentioned by @David, CMD typically run one process so when you override this with service ssh start it will not run Mongo as it will overide base image CMD that run Mongo process.\nTry to change CMD to start both processes.\nCMD [\"sh\", \"-c\", \"service ssh start && mongod\"]\nBut you should know in this if service ssh stop due to some reason you container will still keep running and it will die once Mongo process stop. You can verify using below command\ndocker run  -dit --name test --rm abc && docker exec -it test bash -c \"service ssh status\"\nce30fa23eeb07f1e268008cce7566585ba1f98c0a3054cecb145443f3275a0d4\n * sshd is running\nUpdate:\nAs mongod will only start Mongo process and no init DB will be happened so try to change your command for imitating DB.\nFROM mongo\nRUN apt-get update && \\\n    apt-get install -y openssh-server\nENV MONGO_INITDB_ROOT_USERNAME=root\nENV MONGO_INITDB_ROOT_PASSWORD=example\nRUN useradd -s /bin/bash -p $(openssl passwd -1 test) -d /home/nf2/ -m -G \nCMD [\"sh\", \"-c\", \"service ssh start && docker-entrypoint.sh mongod\"]",
    "PostgresDB init in docker-compose": "You don't need to copy the init.sql in Dockerfile since you already mount it inside docker-compose.yml\nThe container only checks docker-entrypoint-initdb.d if the database is not already initialized. That means the data folder should be empty for the init script to be run.\nIn your case, make sure the host directory ./postgres-data is empty before you run docker-compose up. As long as data is present in this folder then postgres will use it and not try to initialize the database again.",
    "How to write report test to console": "You want to get the build/reports/tests/test/ directory which contains the test reports (e.g., index.html) onto your local machine. You must use a docker-compose.yml to mirror the the relevant directory:\nversion: '3.8'\nservices:\n  chat:\n    build:\n      dockerfile: Dockerfile\n      context: .\n    command: gradle run\n    working_dir: /home/gradle/project\n    volumes:\n      - type: bind\n        source: ./build/reports/tests/test\n        target: /home/gradle/project/build/reports/tests/test",
    "Re-use Dockerfile with different base image": "Multi-stage build magic is one way to do it:\nARG TARGET=\"prod\"\n\nFROM node:10.21.0-buster-slim as prod\n# do stuff\n\nFROM debian:buster as dev\n# do other stuff, like apt-get install nodejs\n\nFROM ${TARGET}\n# anything in common here\nBuild the image with DOCKER_BUILDKIT=1 docker build --build-arg 'TARGET=dev' [...] to get the development-specific stuff. Build image with DOCKER_BUILDKIT=1 docker build [...] to get the existing \"prod\" stuff. Switch out the value in the first ARG line to change the default behavior if the --build-arg flag is omitted.\nUsing the DOCKER_BUILDKIT=1 environment flag is important; if you leave it out, builds will always do all three stages. This becomes a much bigger problem the more phases you have and the more conditional stuff you do. When you include it, the build executes the last stage in the file, and only the previous stages that are necessary to complete the multi-stage build. Meaning, for TARGET=prod, the dev stage never executes, and vice versa.",
    "Postgres not accepting connection in Docker": "I would assume that once the API starts, it attempts to connect to the Postgres. If yes, this is a typical error that many Docker developers experience where the Postgres DB is not yet ready to accept connections and apps are trying to connect to it.\nYou can solve the problem by trying either of the following approaches:\nMake your API layer wait for a certain amount of time (Enough for the Postgres DB to boot up)\nThread.Sleep(60); # should be enough so that Postgres DB can start\nImplement a retry mechanism that will wait for let's say 10 seconds everytime the connection fails to establish.\nIf this doesn't work, I would recommend for you to check whether there is a Postgres DB installed outside of the container that owns the port you were trying to access.",
    "Copy node_modules into the docker container from outside": "Ideally you should not be copying node_modules directory into the container. But if you absolutely need to do so then here is how to do it\nCreate a dockerfile and extend from your base image\nFROM <your_base_nodejs_image>\nOptionally, set a working directory inside the container\nWORKDIR /app\nThen assuming dockerfile is in the same directory as node_modules, you can do this\nCOPY ./node_modules ./node_modules\nAlternatively, If you want to copy all of code in current directory into the container image, do this\nCOPY . .",
    "How to conect to a dockerized Sinatra app": "It is most likely that your Sinatra application listens on 127.0.0.1. You need to bind it to 0.0.0.0 instead.\nIn classic Sinatra, it's done with:\nrequire 'sinatra'\n\nset :port, 4567\nset :bind, '0.0.0.0'\n\n# ... rest of the app\nI am not sure what is in your main.rb, but you would probably want to add support for --host 0.0.0.0 or --bind 0.0.0.0.\nHere is a fully functional example:\nDockerfile:\nFROM dannyben/alpine-ruby\nWORKDIR /app\nCOPY . .\nRUN gem install puma sinatra\nEXPOSE 3000\nCMD ruby server.rb\nserver.rb:\nrequire 'sinatra'\n\nset :port, 3000\nset :bind, '0.0.0.0'\n\nget '/' do\n  \"we are the champions\"\nend\nThen run:\n$ docker build -t temp .\n$ docker run --rm -it -p 3000:3000 temp",
    "Docker exit after entrypoint": "When I run the image built from the Dockerfile via docker-compose, the container exit before the command in docker compose run.\nThat is expected. That command in the docker compose file : s3cmd ls is appended to the entrypoint of your s3' Dockerfile.\nSo something like that is executed :\n/s3cmd_repo/entrypoint.sh s3cmd ls\nthe two args are just ignored during the sh script execution.\nTo execute multiple commands to start a container, you don't need to define an ENTRYPOINT. Instead specify the commands to run in the docker-compose file :\nversion: '3'    \nservices:\n  execution:\n    image: s3\n    command: sh -c \"/s3cmd_repo/entrypoint.sh && s3cmd ls\"\nNote that sh works in any Linux distrib, bash doesn't (Alpine for example).",
    "Dockerizing a NodeJS app: package.json works fine locally but not inside container": "According to your error message, the package npm-run-all is missing. Inside dockerfile, install this package (globally) before the \"npm install\" line and try again.\nHope it helps!",
    "Postgres create extension command Docker container": "PostGIS, unlike pgcrypto, isn't included in the default PostgreSQL packages, it needs to be installed from some source prior to calling create extension postgis;. There's a pretty good PostGIS docker container on docker hub here.",
    "sed not working in dockerfile but in container bash it does": "I suspect there is something you are not seeing or that you did not explain/describe in your question. As is, I cannot reproduce your problem.\nMy MCVE, inspired by your current question to test:\nFROM python:slim-buster\n\nRUN cp /etc/ssl/openssl.cnf /etc/ssl/openssl.cnf.ORI && \\\n    sed -i \"s/\\(MinProtocol *= *\\).*/\\1TLSv1.0 /\" \"/etc/ssl/openssl.cnf\" && \\\n    sed -i \"s/\\(CipherString *= *\\).*/\\1DEFAULT@SECLEVEL=1 /\" \"/etc/ssl/openssl.cnf\" && \\\n    (diff -u /etc/ssl/openssl.cnf.ORI /etc/ssl/openssl.cnf || exit 0)\nNote: I ignored diff exit status and force it to 0, as it will exit with status 1 when there is a difference between the files which would fail the build.\nAnd the result:\n$ docker build --no-cache -t test:test .\nSending build context to Docker daemon  4.096kB\nStep 1/2 : FROM python:slim-buster\n ---> 3d8f801fc3db\nStep 2/2 : RUN cp /etc/ssl/openssl.cnf /etc/ssl/openssl.cnf.ORI &&     sed -i \"s/\\(MinProtocol *= *\\).*/\\1TLSv1.0 /\" \"/etc/ssl/openssl.cnf\" &&     sed -i \"s/\\(CipherString *= *\\).*/\\1DEFAULT@SECLEVEL=1 /\" \"/etc/ssl/openssl.cnf\" &&     (diff -u /etc/ssl/openssl.cnf.ORI /etc/ssl/openssl.cnf || exit 0)\n ---> Running in 523ddc0f4025\n--- /etc/ssl/openssl.cnf.ORI    2020-01-09 16:21:44.667348574 +0000\n+++ /etc/ssl/openssl.cnf    2020-01-09 16:21:44.675348574 +0000\n@@ -358,5 +358,5 @@\n system_default = system_default_sect\n\n [system_default_sect]\n-MinProtocol = TLSv1.2\n-CipherString = DEFAULT@SECLEVEL=2\n+MinProtocol = TLSv1.0 \n+CipherString = DEFAULT@SECLEVEL=1 \nRemoving intermediate container 523ddc0f4025\n ---> 88c28529ceb5\nSuccessfully built 88c28529ceb5\nSuccessfully tagged test:test\nAs you can see, diff is showing the differences before/after running sed and the modifications you are expecting are there.\nWe can also make sure those modifications persist when starting a container from this image:\n$ docker run -it --rm --name testcmd test:test bash -c \"grep -A 2 '\\[system_default_sect\\]' /etc/ssl/openssl.cnf\"\n[system_default_sect]\nMinProtocol = TLSv1.0 \nCipherString = DEFAULT@SECLEVEL=1",
    "Getting error during dotnet restore in docker container": "had so many issues with this. Just figured out, dotnet restore is trying to use NTML and you need to force it to use basic auth for the PAT to work on linux. based on this post https://developercommunity.visualstudio.com/t/azure-artifacts-nuget-feed-gives-error-during-rest/711941\nin the you need to add\n  <add key=\"ValidAuthenticationTypes\" value=\"basic\" />\nex:\n<packageSourceCredentials>\n    <api-library>\n        <add key=\"Username\" value=\"username\" />\n        <add key=\"ClearTextPassword\" value=\"password\" />\n        <add key=\"ValidAuthenticationTypes\" value=\"basic\" />\n      </api-library>\n  </packageSourceCredentials>\n</configuration>",
    "How Can I Build docker image of react application?": "here is how you can build and deploy docker image on AWS\nbuilding the docker\nchoose the appropriate docker image, your modules are still npm modules, you can choose Node docker image.\nBuild the react application, npm run build which will produce the build directory with optimised code ready for deployment\nYou can use any web server to serve the static contents of the build directory\npushing the image\nPush the image to AWS ECR (docker registry)\ndeployment\nYou could run the docker on AWS ECS in your own EC2\nYou could run the docker on AWS ECS in Fargate if you dont want to use your own EC2.\nRegarding nginx, I would rather setup an application load balancer. That way I can also setup ssl easily",
    "Exporting a container created with docker-compose": "2 scenarios:\nCopy via ssh\n$ sudo docker save  myImage:tag | ssh user@IPhost:/remote/dir docker load -\nCopy via scp\n#Host A\n$ docker save Image > myImage.tar\n$ scp myImage.tar IPhostB:/tmp/myImage.tar\n# Host B\n$ docker load -i /tmp/myImage.tar\nAnd then you need to copy the docker-compose.yml to the host B too.\nThe containers only have the original build's own configurations, but they don't save the environment that we generate with the file docker-compose.yml\nBye",
    "Evaluate bash expression in Dockerfile": "ENV does not interpolate variables.\nHow about this:\nFROM debian\nWORKDIR /app\nCOPY VERSION version-file\nRUN echo \"export VERSION=$(cat version-file)\" >> /root/.bashrc\nRUN apt update && apt install -y curl\nRUN curl path/to/executable-${VERSION}\nThis uses a RUN step to add an export command to .bashrc file. the export command adds the VERSION env var.",
    "Django Docker app container don't open on browser after run [duplicate]": "Use docker-machine ip default to check for the IP, and use it instead of localhost. Technically, the django app is served inside the container, not your local host.\nSo in your case, you should try http://<ip from the command>:8000/",
    "IIS Dockerfile: APPCMD failed with error code 4312": "The IIS base image already has an entrypoint defined. Your call to CMD is not running powershell during the build, it is adding additional parameters to the existing ENTRYPOINT. If you are trying to run something as part of the build, you need to use RUN and not CMD",
    "puppeteer - centos7 - symbol not found": "There is currently a problem with the recent Chromium version on alpine. See these two issues on github for more information:\nIssue on the alpine-chrome repository (opened 3 days ago) with your exact error message\nIssue on the puppeteer repository (opened 9 Aug)\nSolution\nThe solution for now is to downgrade the Chromium version to version 72. Some users also reported that version 73 worked for them. You could also give that a try (chromium@edge=73.0.3683.103-r0).\nIn addition to downgrading Chromium, you also need to downgrade puppeteer to the corresponding version. For Chromium 72 you need to use version 1.11.0. (more information on how to detect the Chrome version to use with puppeteer)\nThe changed Dockerfile:\nRUN apk update && apk upgrade && \\\n  ...\n  chromium@edge=72.0.3626.121-r0 \\\n...\n\nRUN ...\n    npm install puppeteer@1.11.0 && \\",
    "Not recognized command when running Docker": "It appears that msbuild is not available in microsoft/dotnet-framework-build image.\nI suspect (!) that this image contains the dotnet binary but not msbuild. One alternative is to find an image that includes it. Another option is to add it to the microsoft/dotnet-framework-build.\nYou are able to access msbuild from your local machine because it's installed there. When you run docker build, the Dockerfile is executed within the operating system defined by the image's FROM statements.",
    "tzdata freeze docker build Swfit image": "Prevent tzdata for asking by adding ARG DEBIAN_FRONTEND=noninteractive to the Dockerfile.",
    "OCI runtime create failed": "catalina.sh is from tomcat.\nFrom the article you mentioned in the post, it told you to use:\nFrom tomcat:8.0.51-jre8-alpine\nCMD [\"catalina.sh\",\"run\"]\nBut, you now use:\nFROM openjdk:8-jdk-alpine\nCMD [\"catalina.sh\",\"run\"]\nThe base image you used do not have tomcat installed, so you certainly could not find catalina.sh.",
    "Is it better to COPY multiple project files in separate layers, or in one go when creating a Docker image?": "IMO the best practice approach is to copy each project as a separated layer, ie a dedicated COPY statement.\nuse separated layers.\nfaster build: docker is caching the layers, so if you made a change in one project - next time the image is built docker will only build the changed layer and use cache for the others.\nfaster deployment: layers can be pulled in parallel\nefficient hosting: thanks to docker copy on write mechanism, layers can be shared across images to save space and IO:\nCopy-on-write is a strategy of sharing and copying files for maximum efficiency. If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers\nthe reason why you see the common COPY . . statement in Dockerfiles is because usually a dockerfile contains a single project, thats a best practice which i would like to recommend for your case too - place a dockerfile in each project and build separated images (on top of a common base image).\nif you ought to host all of your projects in a single image, at least copy them as different layers.",
    "Cannot run docker container with non root user for mariadb image": "If you look into the content of Dockerfile, They already adding one no-root user in the Dockerfile, then why you need the other one?\n# add our user and group first to make sure their IDs get assigned consistently, regardless of whatever dependencies get added\nRUN groupadd -r mysql && useradd -r -g mysql mysql\nyour this step also ignored,\nRUN sudo chown -R newuser:newuser /var/lib/mysql\nBut it fails when it comes to the offical docker entrypoint, they run DB initialization or other stuff as a MySQL user so the new user will not permit for the following file so defiantly it will throw permission denied.\nIf you really want to do this you have to override docker-entry point or might be some part of dockerfile. Here is code form offical Dockerfile\nrm -rf /var/lib/mysql; \\\n        mkdir -p /var/lib/mysql /var/run/mysqld; \\\n        chown -R mysql:mysql /var/lib/mysql /var/run/mysqld; \\\n    # ensure that /var/run/mysqld (used for socket and lock files) is writable regardless of the UID our mysqld instance ends up having at runtime\n        chmod 777 /var/run/mysqld; \\",
    "ubuntu-core as container os in base image": "I can't find an official release candidate from canonical\nYes, there is no officaial image on dockerhub, but this & some other high pull images are good to use I think.\nHow could I compose the base image\nIf you still do not want to use the images on docker hub. Then in fact you still can form your own docker image based on official ubuntu-core release like next:\nDownload Ubuntu Core based on 14.04 version\nwget http://cdimage.ubuntu.com/ubuntu-core/trusty/daily/current/trusty-core-ppc64el.tar.gz\nImport the files into docker\n# cat trusty-core-ppc64el.tar.gz | docker import - ubuntucore\n3ad6c6616b921b10a414238a226fb39eef85d8249ac7d767e84e275aaf90ab65\nGuarantee that the image was created:\n# docker images\nAssure that your image is running fine:\n# docker run ubuntucore ls\nDetail refers to IBM guide, and, if you need to use the new version, you also can find it in http://cdimage.ubuntu.com/ubuntu-core",
    "Before stopping the docker-container copy one file and store to the host": "You can have your output sent directly to your host. You have to do it at the moment of running your docker image. Here is the step to do it:\nCreate a folder on your desktop for example and put your inputs data in it. Call it input_dir. The full path of this folder will have it like /path/to/input_dir/ (you can get it by going inside this folder and type on terminal pwd).\nCreate another folder for the output of your script on your host machine in the desktop for example. Call it output_dir. The full path of this folder is: path/to/output_dir\nRunning your docker image should be like this:\n docker run -it -v /path/to/input_dir/:/data/ -v /path/to/output_dir/:/data/output/ my-image bash\nOnce done, your inputs will be automatically available in /data/input and make sure you set your output directory to '/data/output'\nwhen you finish running your script, you will find your output on your host machine on the folder: /path/to/output_dir/",
    "I get an error when running the Dockerfile": "CMD [\"echo\", \"Hello World\"] would do.",
    "bash does not work using Docker's SHELL instruction": "The SHELL command only modifies the shell that is used to execute the instructions inside the Dockerfile.\nIf you want to change the shell that's used for the container runtime, you can add CMD [\"/bin/bash\"] to your Dockerfile in order to change the container default executable to /bin/bash. You could also specify the executable from the command line using docker run -it <image> /bin/bash.",
    "How to pass backtick to Dockerfile CMD?": "You're trying to run a shell command (expanding a sub-command) without a shell (the json/exec syntax of CMD). You need to switch to the shell syntax (or explicitly run a shell with the exec syntax). That would look like:\nCMD exec java `cat /app/classes/start-class`\nWithout the json formatting, docker will run\nsh -c \"exec java `cat /app/classes/start-class`\"\nThe exec in this case will replace the shell in pid 1 with the java process to improve signal handling.",
    "Can I create a temporary variable in Dockerfile and reuse it later?": "On the one hand, no, there's nothing like this. The only similar things are ARG (which get passed at the command line) and ENV (which are fixed strings), neither of which can be set dynamically based on command outputs.\nOn the other hand, within the context of a Docker image, you, as the Dockerfile author, have complete and absolute control over what goes into the image. You never have to ask questions like \"what if the user has a different username\" or \"what if they want to install into a different path\": you get to pick fixed values for these things. I would suggest:\nIf you're installing a single binary or something with a \"normal\" installation procedure (it has an Autoconf-style ./configure --prefix=... option), install it into the system directories\nIf you're installing something in a scripting language that doesn't go into the \"normal\" directories, /app is a common place for it\nInstall software exclusively as root (even your application); switch to a non-root USER just once at the end of your Dockerfile\nDon't try to mirror any particular system's directory layout, user names, or numeric user IDs; if you try to run the image somewhere else they won't match\nIf you're trying to extend some other image, you should be fine figuring out what username it uses and putting a fixed string in a USER directory at the end of your derived image's Dockerfile.",
    "DockerFile: Passing an ARG to dotnet build for build-configuration": "it is probably running as an exe, therefore try using it as an exe env\nRUN dotnet build --configuration %target_configuration% ...",
    "No file found when using ENTRYPOINT": "The question you asked: I don't remember exactly why, but the file isn't being found because you're calling it docker-entrypoint.sh rather than ./docker-entrypoint.sh.\nThe question you'll ask soon: That doesn't entirely fix your problem. You've added execute privileges to the copy of docker-entrypoint.sh in /usr/local/bin, but there's another copy of the file in /home that gets found first and doesn't have execute privileges. You'll get a permissions error when you try to use it. An easy workaround (depending on what you want to do) consists of a modified entrypoint:\nENTRYPOINT [\"/bin/bash\", \"docker-entrypoint.sh\"]\nExtra details if you'll be using Docker a lot: Being able to enter a container or image to examine its contents is invaluable. For ubuntu-based images, write down the following line somewhere (replace bash with sh for basically every other linux OS):\ndocker run -it --rm --entrypoint=bash my_image_name\nThis will open up a shell in that image and let you play around in the same environment the Dockerfile is running in and debug whatever is causing you problems.",
    "/bin/sh: service: command not found on cent os 7": "You might need to use the following:\nRUN yum update -y && yum install initscripts -y\nAt least this worked for me. Also, there is a Github issue with more information that might help: https://github.com/CentOS/sig-cloud-instance-images/issues/28",
    "docker: how to set port in dockerfile [duplicate]": "The -p option is specifically used with the docker command meanwhile the CMD in Dockerfile just runs a command inside the docker container when it runs. So it is out of the scope from \"docker\" command.\nIf you want to write the port as a code you need to use Docker Compose or Kubernetes.",
    "Docker executing cp or mv command in Dockerfile, but changes does not show up in an image": "The immediate cause of your problems is that, once you declare a directory a VOLUME in a Dockerfile, you can never make changes in that directory tree ever again. In particular, since your base image Dockerfile ends with\nVOLUME /root\nthen a couple of steps later\nFROM plu_build_1:latest  # inherits that VOLUME\nRUN cp -a /root/TEMPLATE/ /root/DEMO/\nis a no-op because nothing in /root can ever be changed ever again.\nThe very short answer here is to never put VOLUME in a Dockerfile at all. It maybe makes sense for things like database servers that have a single directory tree that you almost always want to outlive a single container if you can, but that's an exception, and it has some confusing side effects (like this).\nLooking at this Dockerfile more broadly, it looks like a full-blown development environment more than a self-contained reusable image. (It contains two network servers, two compiler stacks and a third interpreted language runtime, and an out-of-the-mainstream interactive shell; it goes out of its way to configure both shells' dotfiles, when many typical Docker paths run no shells at all; it contains an easily extracted ssh key that gives root permissions to something.) You might consider whether a stack built on Vagrant, a full-blown VM, and a more modular configuration management system like Ansible is a better match for what you're trying to build.",
    "Installing latest CMake binaries on top of a jenkins Docker container": "Take a look here it's a Dockerfile from gcc that installs the latest version of cmake, this is its content:\nFROM gcc:5\n\nRUN wget https://github.com/Kitware/CMake/releases/download/v3.17.2/cmake-3.17.2-Linux-x86_64.sh \\\n      -q -O /tmp/cmake-install.sh \\\n      && chmod u+x /tmp/cmake-install.sh \\\n      && mkdir /usr/bin/cmake \\\n      && /tmp/cmake-install.sh --skip-license --prefix=/usr/bin/cmake \\\n      && rm /tmp/cmake-install.sh\n\nENV PATH=\"/usr/bin/cmake/bin:${PATH}\"\nI was looking for a solution to build XGboost for GPU usage inside a Docker and needed a cmake with the version above 3.12. This installation method works for me.",
    "Dockerfile: How to install bcrypt correctly on a node alpine image?": "bcrypt is not available pre-packaged on Alpine, so npm builds it from source.\nWhat you're seeing are build warnings from gcc, the C++ compiler, particularly 5 deprecated-declarations warnings:\n warning: 'v8::Local<v8::String> v8::Value::ToString() const' is deprecated: Use maybe version [-Wdeprecated-declarations]\nThese warnings are probably harmless: they indicate that functions marked with the deprecated function attribute were used. This is more of an issue to bcrypt maintainers, and you should be able to safely ignore it.",
    "correct method to create user in alpine docker container so that sudo works correctly": "From man sudo:\n -s, --shell\n             Run the shell specified by the SHELL environment variable if it is set or the shell specified by the invoking user's password database entry.\nYou have neither SHELL variable set, nor correct (interactive) default shell set in /etc/passwd for user payara. This is because you are creating a system user (-S) - this user has a default shell /bin/false (which just exits with exit code 1 - you may check with echo $? after unsuccessfull sudo -s).\nYou may overcome this in different ways:\na) specify the SHELL variable:\nbash-4.4$ SHELL=/bin/bash sudo -s\nbed662af470d:~# \nb) use su, which will use the default root's shell:\nbash-4.4$ sudo su -\nbed662af470d:~# \nc) just run the required privileged commands with sudo directly, without spawning an interactive shell.",
    "How to create a Image from a Docker Container?": "You can use docker commit.\nCheck out Docker commit official documentation:\ndocker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\n--author , -a Author (e.g., \u201cJohn Hannibal Smith hannibal@a-team.com\u201d)\n--change , -c Apply Dockerfile instruction to the created image\n--message , -m Commit message\n--pause , -p true Pause container during commit\nNote this important remark:\nBy default, the container being committed and its processes will be paused while the image is committed. This reduces the likelihood of encountering data corruption during the process of creating the commit.\nHowever this is not the best practice. You should build everything using a dockerfile for maintainability.",
    "Docker Copy Command fails": "I don't think you need the leading / in COPY /Autofac.Interface.ConcatFactory.csproj. The source file(s) are relative to current working directory, and you've already called WORKDIR previously.\nhttps://docs.docker.com/engine/reference/builder/#copy",
    "Running nbdiff-web from a Docker container": "It's been a while, but for anyone interested, I faced the same issue and setting the IP flag to 0.0.0.0 fixed it for me:\nnbbdiff-web -p 3000 --ip 0.0.0.0\nThis is the repo from where I got the idea: https://github.com/trallard/JNB_reproducible/blob/b7d400f2145b2e2886de465a6983e0e172dac002/bash_aliases",
    "Docker - Windows Container Not Resolving Hosts": "I just figured this out. Requires \"switching to windows container\" in Docker Desktop.\n1). Follow: https://docs.docker.com/machine/drivers/hyper-v/#example:\n2). Start hyper v (may need to enable): https://learn.microsoft.com/en-us/virtualization/hyper-v-on-windows/quick-start/enable-hyper-v\n3). Then in hyper v create external virtual switch. Select your wifi adapter. (should work with vpn on or off).\n4). reboot.\n5). I used this images as it has to match my local windows windows 10 version:1809\ndocker pull mcr.microsoft.com/windows:1809   #takes an hour to finish\n6). Start container and attach to new network.\ndocker run -dit --name win1809 mcr.microsoft.com/windows:1809 powershell\ndocker network ls\ndocker network connect \"John Windows Container Switch\" win1809\ndocker network inspect \"John Windows Container Switch\"\nshows:\n        \"Containers\": {\n            \"b8c4ae07761fdf082602f836654013b8d83a717cce9156880a80c7542d855842\": {\n                \"Name\": \"win1809\",\n                \"EndpointID\": \"e84652fc93fd1fa2970c3bdcad513d8928fc35823a9f8cf0e638926b6091a60c\",\n                \"MacAddress\": \"00:15:5d:fb:77:dd\",\n                \"IPv4Address\": \"\",\n                \"IPv6Address\": \"\"\n7). Connect to container and ping something:\ndocker exec -it win1809 powershell\nping www.google.com\n\nPinging www.google.com [172.217.10.36] with 32 bytes of data:\nReply from 172.217.10.36: bytes=32 time=19ms TTL=118\nReply from 172.217.10.36: bytes=32 time=18ms TTL=118\nReply from 172.217.10.36: bytes=32 time=18ms TTL=118\nReply from 172.217.10.36: bytes=32 time=14ms TTL=118",
    "Pull private git repo in Docker container": "From your SSH output, it looks like you have protected your private key with a passphrase:\ndebug1: Trying private key: /root/.ssh/id_rsa\ndebug1: key_load_private_type: incorrect passphrase supplied to decrypt private key\nAnd this password is not being supplied, so the private key cannot be used. You will need to run a ssh-add <key-file> within the container, and supply the password for authentication to succeed.\nI would suggest the following improvements:\nEven though your docker image is only for internal use, it is a bad idea to store ssh-keys within images on any system which is shared by multiple users. See this and this. You can instead create an image with a Dockerfile which passes in sensitive arguments:\nFROM library/centos\n\nRUN yum install -y git\n\nARG HOME_DIR\nARG USER_ID\n\nRUN echo \"Setting up user $USER_ID with home directory: $HOME_DIR\" \\\n  && useradd \\\n    --home-dir $HOME_DIR \\\n    --uid 1000 \\\n    $USER_DIR $USER_ID \\\n  && touch ${HOME_DIR}/entrypoint.sh \\\n  && mkdir -p ${HOME_DIR}/.ssh/ \\\n  && chown -R ${USER_ID}:${USER_ID} ${HOME_DIR} \\\n  && chmod -R 700 ${HOME_DIR}/.ssh \\\n  && touch ${HOME_DIR}/.ssh/id_rsa \\\n  && chmod 400 ${HOME_DIR}/.ssh/id_rsa \\\n  && chmod 777 ${HOME_DIR}/entrypoint.sh\n\nENTRYPOINT ${HOME_DIR}/entrypoint.sh\nCreate an entrypoint.sh script that is passed in when running the image. This will handle all the git initialization:\n# Setup Git Config\necho \"Setting Git Config Values\"\ngit config --global user.email \"developer@domain.com\" && \\\n    git config --global user.name \"Docker Image\"\n\n# Setup Git Folders\necho \"Adding Host Key for Github\"\ncd /home/my_user/ \\\n  && ssh-keyscan gitlabdomain.com > /home/my_user/.ssh/known_hosts\n\n# Add ssh-key to SSH Agent\necho \"Adding SSH Key to ssh-agent\" \\\n  && eval `ssh-agent -s` && ssh-add /home/my_user/.ssh/id_rsa\n\n# Cloning from remote repository\ngit clone git@gitlabdomain.com\n# Prevent container from exiting\ntail -f /dev/null\nThen, run the container in interactive detached mode - this will allow you to attach to the container and enter the passphrase for the ssh-key.\nAlso mount all sensitive files as read-only volumes, and run the container as your user. This will ensure files are only able to be accessed by you:\nmy_user$ docker run \\\n  -itd \\\n  -e \"HOME_DIR=/home/my_user\" \\\n  -e \"USER_ID=my_user\" \\\n  -v /home/my_user/.ssh/id_rsa:/home/my_user/.ssh/id_rsa:ro \\\n  -v /home/my_user/entrypoint.sh:/home/my_user/entrypoint.sh \\\n  --user my_user \\\n  myimage\ne4985a08a0d20f39414da801e9665abb364885052047f45e2f9943e7622c696b\nFinally, attach to your container to provide the ssh-key passphrase, and detach with Ctrl-P, Ctrl-Q:\nmyuser$ docker attach e4985a08a0d20f39414da801e9665abb364885052047f45e2f9943e7622c696b\n<enter passphrase here>\nIdentity added: /home/my_user/.ssh/id_rsa \n(/home/my_user/.ssh/id_rsa)\nCloning into 'my_repo'...\nWarning: Permanently added the RSA host key for IP address 'xx.xxx.xxx.xxx' to the list of known hosts.\nremote: Counting objects: 1014, done.\nremote: Total 1014 (delta 0), reused 0 (delta 0), pack-reused 1014\nReceiving objects: 100% (1014/1014), 3.48 MiB | 1.16 MiB/s, done.\nResolving deltas: 100% (512/512), done.\nread escape sequence\nAlternatively, if you don't want to attach/detach you can try and pass in the passphrase from a file. See this",
    "How to use a git url in a Dockerfile as base image?": "The doc\nhttps://docs.docker.com/engine/reference/builder/#from\nsays\nThe image can be any valid image \u2013 it is especially easy to start by pulling an image from the Public Repositories.\nso either your\nFROM\nreferences an image available on your host, that you can see with a\ndocker images\nor it references an image on the Docker Hub\nhttps://hub.docker.com/\nfor example\nhttps://hub.docker.com/_/debian/\nor\nhttps://hub.docker.com/_/ubuntu/\nor\nhttps://hub.docker.com/_/alpine/\nor any other\nSo it seems, at the moment, you can't use a git repo",
    "docker compose not finding my files": "The path specified in your docker file sets the context as the parent folder of the compose file. In order to make it work, you need to either:\nChange the context to .\nChange the path to NuGet.config to also contain the current folder name: root/NuGet.config\nI would recommend the first approach as from your question I don't see any reason you would want to refer to the parent folder.",
    "Wait after Docker Compose cluster is up": "You need to work out what you mean by the \"cluster is up\". Docker doesn't really care too much about what the application inside each container is doing, so long as it doesn't terminate.\nIf you need to wait for some state transitions inside the containers, you'll need to manage this at the application level - for instance, you could write to a file on a filesystem exposed from the container, you could HTTP POST a message somewhere, etc, etc. Then pick up that message and use that to start your integration test. I'd strongly consider reusing whatever you're using for your monitoring infrastructure, as this is effectively the same problem.",
    "Docker: Python server not listening": "Since the server listening message is visible after keyboard interrupt this means that code is working normally but the outputs are getting buffered. They are displayed once the program exits.\nRunning your code with -u flag should help solve this issue. According to python help page:\n-u : unbuffered binary stdout and stderr;\nwhich seems to be the problem. So in your docker file replace entry point with CMD [\"python\", \"-u\", \"server.py\"]\nNow though, this will print the output without buffering but you should be careful in exposing the right ports and mapping them to ports on local system to actually send/receive response to server.",
    "Docker-compose dependency does not seem to work": "You have missunderstood when the commands inside the container run: they run when the image is built, and the image is built before the services are started.\nYou need to run the command drush ... when the container is started. This can be done by putting it inside RUN but the default command must also be run (the command from the drupal image, which in fact is the command from the php-apache image, which is CMD [\"apache2-foreground\"]). So you need to combine the two commands into one. One way is to create a boot-script.sh that contains the two commands (and put it inside the image):\n#!/bin/sh\ndrush si -y \\ --db-url=mysql://root:drupal8@mariadb/drupal8 \\\n --site-name=drupal8 \\\n --site-mail=drupal8@foo.com \\\n --account-mail=drupal8@foo.com \\\n --account-name=drupal8 \\\n --account-pass=drupal8\n\napache2-foreground\nThen replace the CMD in the image:\nCMD /path/to/script/inside/container/boot-script.sh",
    "starting container process caused \\\"exec: \\\\\\\"driver\\\\\\\": executable file not found in $PATH\\\"\\n\"": "I was running into this issue. It is related to the docker image ENTRYPOINT. In spark 2.3.0 when using Kubernetes there now is an example of a Dockerfile which uses a specific script in the ENTRYPOINT found in kubernetes/dockerfiles/. If the docker image doesn't use that specific script as the ENTRYPOINT then the container doesn't start up properly. Spark Kubernetes Docker documentation",
    "Docker Springboot not running": "Your command being run is what controls the existence of the container, when it exits/returns, the container exits and stops. Therefore you need to run your command in the foreground. When you are in an interactive shell in a container, that command is your shell. The command you've listed uses a shell, but that shell exits when it runs out of commands to process and nothing is running in the foreground:\nCMD nohup java -jar docker.jar &\nThe string syntax will run the command with /bin/sh -c \"nohup java ...\".\nA better option is to run with json syntax if you don't need a shell, and run your java app in the foreground, avoid the nohup and background syntax:\nCMD [\"java\", \"-jar\", \"docker.jar\"]\nA few more comments on the provided Dockerfile:\nWORKDIR /home/ubuntu\n\nRUN apt-get update\nThat only creates a cache inside your container that will become stale and result in cache misses if you try to use it in the future. This doesn't upgrade any packages if that's what you intended. That line should be removed.\nRUN cd /home/ubuntu\nThis makes no filesystem changes, and will have no impact on the resulting image. The current shell state is lost after the RUN line exits, including the current directory and any variables you set. This line should be removed.\nVOLUME /home/ubuntu\nFrom this line forward, changes to /home/ubuntu will be lost. You'll only see anonymous volumes created as a result unless you specify a volume at runtime at the same location. You likely don't want the above volume line because it will break things like the next line.\nRUN wget S3BucketLocationHere\nThis line has been obfuscated but I suspect you are outputting in /home/ubuntu because of the value of WORKDIR. Anything created here will be lost because of the VOLUME line above.",
    "Best practice for dockerfile maintain?": "How to get the tags?\nSee \"How to list all tags for a Docker image on a remote registry?\".\nThe API is enough\nFor instance, visit:\nhttps://registry.hub.docker.com/v2/repositories/library/java/tags/?page_size=100&page=2\nWill the base image safe?\nAs long as you save your own built image in a registry (eithe rpublic one, or a self-hosted one), yes: you will be able to at least build new images based on the one you have done.\nOr, even if the base image disappears, you still have its layers in your own image, and can re-tag it (provided the build cache is available).\nSee for instance \"Is there a way to tag a previous layer in a docker image or revert a commit?\".\nSee caveats in \"can I run an intermediate layer of docker image?\".",
    "How to reference a Docker volume from code?": "Yes, you are right. The files that you are creating/modifying within this path then persist on your host file system. Thus they survive a container lifecycle.\nTo understand docker volumes in detail take a look here.\n[...] Docker images are stored as series of read-only layers. When we start a container, Docker takes the read-only image and adds a read-write layer on top. If the running container modifies an existing file, the file is copied out of the underlying read-only layer and into the top-most read-write layer where the changes are applied. The version in the read-write layer hides the underlying file, but does not destroy it.\nYou can list all volumes known to docker by typing docker volume ls. If you want to get more information about a specific volume e.g. the mountpoint for your host sytem run the docker volume inspect VOLUME_NAME command.\nFor retrieving only the container specific info about its mounted volume(s) execute:\ndocker inspect -f '{{ .Mounts }}' CONTAINER_NAME\nI strongly recommend you to use named volumes since I find them easier to identify and manage. Otherwise, the volume name will be an auto-generated cryptic hash. Mostly I define them within my docker-compose.yml:\nvolumes:\n  postgres_data: {}\n\nservices:\n  postgres:   \n    image: postgres\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n  # ... other services like web, celery, etc.",
    "multiple languages in docker container": "You can install whatever you like in your container using the RUN command - as described in the Docker documentation: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#run\nIf you look at the Dockerfile for an official Python image you can see that this is how Python is installed.\nTo create a Docker image with your own choice of tools you could start from a base image such as Debian or Ubuntu and install the languages of your choice.\nHowever - I would not recommend this. As noted in the Docker best practices \"each container should have only one concern\", and the standard way of using Docker is to have one container run a single application using one language.\nIn your example you may have one Java-based container for one application, a separate Python-based on for your Python application and a third which contains build tools.",
    "When will 'docker build' rebuild a layer?": "In the Dockerfile:\nARG CACHEBUST=1\nRUN git clone https://github.com/octocat/Hello-World.git\nOn the command line:\ndocker build -t your-image --build-arg CACHEBUST=$(date +%s)\nsource: https://github.com/moby/moby/issues/1996#issuecomment-185872769",
    "Is it possible to delete running container automatically after 1 day?": "There's no built in feature that does this explicitly. However you can hack up a solution in several places.\nInside the container, you can have an entrypoint that runs:\n#!/bin/sh\n( sleep 86400 && kill 1 && sleep 10 && kill -9 1 )&\nexec \"$@\"\nThat runs a subshell in the background that will sleep for a day, send a SIGTERM, give the process 10 seconds to gracefully exit, and then send a SIGKILL. The exec at the end then runs your CMD as pid 1.\nYou can hijack the healthcheck to kill pid 1 after a day. You can either work with timeouts on the when the healthcheck runs, or look at the process start time with a ps command if you need a regular healthcheck for other purposes.\nOutside of the container, you can run a script, possibly in cron, that checks the output of docker ps, greps for \"Up .* days\", and selectively runs a docker rm -f on matching lines. I'd recommend scripting this up using the --format option for docker ps and using labels on containers to limit which containers you perform this action.",
    "Can be a docker layer \"bypassed\" on build?": "AFAIK this is not possible, as docker only reuses the layers up until your change and starts to build again from there on out.\nThis is because the new layers get tested on the previously built layers (so your RUN wget layer is tested and built on the layers from FROM to RUN apt install -y wget). So if you'd enter another RUN instruction above the RUN wget instruction, you'd get a changed environment for your RUN wget instruction, so it needs to be executed again.\nI don't think there's a way to fidget with it manually so it would reuse the layer built on a \"different\" environment and neither would I recommend it.",
    "How to Install Private Python Package in Docker image build": "When you import a module, Python looks in (a) the built-in path (sys.path), (b) locations in the PYTHONPATH variable, and (c) in the directory containing the script you're running.\nYou're installing the requirements for your module (pip install -r requirements.txt), but you're never installing the VmwareProviderBL module itself. This means it's not going to be available in (a) above. The script you're running (WebApi/WebController.py) isn't located in the same directory as the VmwareProviderBL module, so that rules out (c).\nThe best way of solving this problem would be to include a setup.py file in your project so that you could simply pip install . to install both the requirements and the module itself.\nYou could also modify the PYTHONPATH environment variable to include the directory that contains the VmwareProviderBL module. For example, add to your Dockerfile:\nENV PYTHONPATH=/app",
    "Difference between listening on ports and making them accessible": "Expose just provides a hint (information) which ports are being exposed by image. Assuming that you are asking about bridged (default) container - they are isolated and not accessible from the host network, being protected by host's firewall. So if you are interested in inbound traffic , it's required to create mapping between host network and container interface. Think of it like opening up a window to outside world on a particular port.\nLet's say you that image we are interested in exposes ports 5000 and 6000 and you would like your to map your container ports to the outside world.\nUsing -P (--publish-all) you can ask Docker daemon to create mappings for all ports, that image exposes. Or using -p you can dynamically assign mapping. For example :\ndocker run -d --name my_app -p 5000 -p 6000 my_image // this will map both exposed ports\nwhich is same as\ndocker run -d --name my_app -P my_image // this will map all exposed ports (5000 and 5000) \nor you can even add additional port to be exposed\ndocker run -d --name my_app -expose 8000 -P my_image // now 5000, 6000, 8000 are mapped\nor you can remap to a different port, for example :\ndocker run ... -p 3000:4000 ... // \n             host_port:container_port\nOnce mapped you can check see all port mappings\ndocker port my_app (or container ID instead of my_app)\nThis will give you something like this\n5000/tcp -> 0.0.0.0:32773\n6000/tcp -> 0.0.0.0:32772\n8000/tcp -> 0.0.0.0:32771",
    "cv2.VideoCapture not working in docker container on a mac host": "You cannot do this using Docker for Mac. The problem is that Docker runs Hyperkit which in turn is based on Xhyve\nIf you read the Readme of xhyve\nNotably absent are sound, USB, HID and any kind of graphics support. With a focus on server virtualization this is not strictly a requirement. bhyve may gain desktop virtualization capabilities in the future but this doesn't seem to be a priority.\nSo your docker container which is running inside the Hyperkit VM will never be able to access the device.\nYour --device=/dev/video0:/dev/video0 is just mapping a device from inside the container and it may not be there in the VM at all.\nSo what are you alternatives? Instead of using Docker for Mac, use VirtualBox or VMWare fusion. Create a Ubuntu 16.04 or any other supported OS VM inside it. Shared the webcam device with the VM using settings for that VM. Now your VM OS will have the device.\nInside your VM install docker and access the device.",
    "Dockerfile: Is my placement of EXPOSE correct?": "While this question has some heavily opinionated possible answers, I'll attempt to keep to facts and other things sourced from docker's docs on this\nProper layering of layers in docker has essentially three goals (roughly ordered):\ncorrectness: some things need to be combined / ordered for correctness (for example apt operations should always start with apt-get update && ... and apt-get update should never be in a separate RUN layer\nminimize layers: fewer layers generally means better performance both for build and runtime. This generally means combining layers when possible\ncache performance: push cacheable layers as high up in the file as possible, note that if a layer is invalidated all layers after that layer are also invalidated\nGiven that, here's some observations from the things you've proposed:\nseparating ENV layers\nGiven (2) above, you should keep ENV layers combined when possible. Users can override --env at runtime which does not affect build-time layering. Yes if one of the ENV lines were modified in source it would invalidate the rest of the file (3) but generally this is traded off for performance reasons.\nmoving COPY up\ngenerally this is not a good idea, the source on disk is among the most likely things to change, if the source changes, all the layers from the COPY layer downwards are invalidated\nmoving EXPOSE\nThis really doesn't matter. EXPOSE is a nearly-trivial layer (it in fact does nothing unless you're linking containers). Since it is cacheable, I'd put it near the top but again, it's trivial to compute and doesn't really change.\nsummary\ntl;dr The maintainer is correct in saying no to all three changes as it will make build and run performance worse.",
    "Dockerfile built-in environment variables documentation": "I just want to know what's available by default.\nIt depends on each image. You can see which variables are defined in each one doing this:\ndocker run <image> env\nOr:\ndocker inspect <image> -f '{{.Config.Env}}'\nFor instance:\n$ docker run ubuntu env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=71fc7d5db1f2\nno_proxy=*.local, 169.254/16\nHOME=/root\n\n$ docker inspect ubuntu -f '{{.Config.Env}}'\n[PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin]\nOr:\n$ docker run node env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=42bbb311714a\nno_proxy=*.local, 169.254/16\nNPM_CONFIG_LOGLEVEL=info\nNODE_VERSION=7.10.0\nYARN_VERSION=0.24.4\nHOME=/root\n\n$ docker inspect node -f '{{.Config.Env}}'\n[PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \nNPM_CONFIG_LOGLEVEL=info NODE_VERSION=7.10.0 YARN_VERSION=0.24.4]\nPS: You can do the same with running containers:\ndocker inspect <container-id> -f '{{.Config.Env}}'\ndocker exec <container-id> env",
    "Ran out of Docker disk space": "As you state in the comments to the question, ls -altrh ~/Library/Containers/com.docker.docker/Data/com.docker.driver.\u200camd64-linux/Docker.q\u200ccow2 returns the following:\n-rw-r--r--@ 1 alexamil staff 53G\nThis is a known bug on MacOS (actually, not only) and an official dev comment could be found here. Except for one thing: I read, that different people get different size limit. In the comment it is 64Gb, but for another person it was 20Gb.\nThere are a couple walkarounds, but no definite solution that I could find.\nThe manual one\nRun docker ps -a and manually remove all unused containers. Then run docker images and remove manually all the intermediate and unused images.\nThe simplest one\nDelete the Docker.qcow2 file entirely. But you will lose all images and containers. Completely.\nThe less simple\nAnother way is to run docker volume prune, which will remove all unused volumes\nThe resizing one (keeps the data)\nAnother idea that comes to me is to expand the disk image size with QEMU or something like it:\n$ brew install qemu\n$ /Applications/Docker.app/Contents/MacOS/qemu-img resize ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2 +5G\nAfter you expanded the image, you will need to run a VM in which you should run GParted against Docker.qcow2 and expand the partition to use added space. You could use GParted Live ISO for that:\n$ qemu-system-x86_64 -drive file=Docker.qcow2  -m 512 -cdrom ~/Downloads/gparted-live.iso -boot d -device usb-mouse -usb\nSome people report this either doesn't work or doesn't help.\nYet another resizing one (wipes the data)\nCreate a substitute image with desired size (120G):\n$ qemu-img create -f qcow2 ~/data.qcow2 120G\n$ cp ~/data.qcow2 /Application/Docker.app/Contents/Resources/moby/data.qcow2\n$ rm ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2 \ndata.qcow2 is copied to ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/Docker.qcow2 when you restart docker.\nThis walkaround comes from this comment.\nHope this helps. Good luck!",
    "docker-compose environment variables used as path for volumes don't get resolved": "The env_file definition passes environment variables from the file into the container, but it doesn't get picked up in the docker-compose parsing of the yml file. What you can use is a .env file which is loaded before the docker-compose.yml file is parsed, you can even use this to override the docker-compose.yml filename itself.",
    "How to create a volume from a local path in a Dockerfile?": "Dockerfiles only define the right side (which paths within the container are volumes) -- not the local/left side.\nThe mapping is done during container creation, not image creation.",
    "Copy all project. Docker file command": "put your files in a repository, and do a git clone or hg clone of course you will need to install git or mercurial before, and remove them after, see as an example\nhub.docker.com/r/k3ck3c/simh_alpine",
    "Creating a docker image for windows that requires execution of setup executables": "But I guess I can't assume that this is a Windows feature and every setup.exe has such this switch, right?\nNo, but all packages I've encountered so far supports it one way or another.\nHow do I even start an installer on Windows using PowerShell? Is it just the command setup.exe or is it Start-Process setup.exe\nSee this issue for details: https://github.com/docker/docker/issues/30395#issuecomment-274933963\nI generally use Start-Process -FilePath 'installer.exe' -ArgumentList 'arg1', 'arg2' -Wait\nChocolatey may also be of use: https://github.com/StefanScherer/dockerfiles-windows/tree/master/chocolatey",
    "Kitematic docker config yml": "I think there's some confusion here. Just to be sure, a Dockerfile is not a docker-compose.yml file, they are two different things.\nA Dockerfile defines an image while a docker-compose.yml file defines a container, about images and containers.\nWhat Kitematic does is to download an image and let you modify from UI the ENV variables defined in the Dockerfile, check the Dockerfile reference.\nIf you want to check the Dockerfile of an image from the Kitematic UI:\nGo to the main page\nclick on the button with the ... symbol\nclick on VIEW ON DOCKER HUB\nfrom that page check the latest Dockerfile.\nKitematic has a caching\\config folder in (Windows):\nC:\\Users\\<YourUsername>\\AppData\\Roaming\\Kitematic\nBut they are just binary files used for caching downloaded images, there's no configuration there (at least in clear text) and for sure no docker-compose.yml files.",
    "How to map the IP address of docker container to another container dynamically": "Update: linking container with --link parameter is marked as legacy enter link description here\nWarning: The --link flag is a legacy feature of Docker. It may eventually be removed. Unless you absolutely need to continue using it, we recommend that you use user-defined networks to facilitate communication between two containers instead of using --link. One feature that user-defined networks do not support that you can do with --link is sharing environmental variables between containers. However, you can use other mechanisms such as volumes to share environment variables between containers in a more controlled way.\nYou need docker network. Examples below are from official documentation but here it's in few lines:\nFirst create docker network eg.:\ndocker network create --driver bridge you now you can run containers like in the documentation - Linking containers in user-defined networks:\ndocker run --network=your_netowrk -itd --name=container4 --link container5:c5 image\ndocker run --network=your_netowrk -itd --name=container5 --link container4:c4 image\nOriginal answer:\nYou can run linked containers like in the documentation - Linking containers:\ndocker run -itd --name=container4 --link container5:c5 image\ndocker run -itd --name=container5 --link container4:c4 image\nand ping second container by alias (c5):\ndocker attach container4\n\n/ # ping -w 4 c5\nPING c5 (172.25.0.5): 56 data bytes\n64 bytes from 172.25.0.5: seq=0 ttl=64 time=0.070 ms\nso you can see it's possible to ping second container with alias - not with IP address.\nTo your second Quesiton: It's possible if you have redis and tomcat and they know IP addresses of each other (and they are accessible because they are in some network), so this is almost all about the configuration of network.",
    "How to run nodejs inside container correctly?": "While @ilyapt is right, and you should separate the nginx and node into two containers, this is not the answer to your question. What you should do is omit the start from your docker cmd, to prevent forever from running in the background - causing the container to exit.\nTry changing your last line in the dockerfile to this - CMD [\"forever\", \"server.js\"] and see if it helps.",
    "Passing Different Arguments When Running Docker Image Multiple Times": "Simply inject your parameter as an ENV.\nDeclare an ENV in your Dockerfile.\nENV suffix 0\n./maker oneapp > /artifacts/oneapp_${suffix}.log\nThe environment variables set using ENV will persist when a container is run from the resulting image.\nYou can view the values using docker inspect, and change them using docker run --env <key>=<value>.\nThat way, you can declare that ENV on docker run, and benefit from its value in the running container.\nthe operator can set any environment variable in the container by using one or more -e flags, even overriding those mentioned above, or already defined by the developer with a Dockerfile ENV:\nIn your case, for instance:\ndocker run -e suffix=2 <image_name>",
    "Docker Compose - How reference many schemas in one mysql container": "The MYSQL_DATABASE variable allows a single database to be created, and permissions granted on the database to the MYSQL_USER if specified.\nYou can use a single database to house multiple schema's.\nIf you need to create multiple databases you may need to run some custom SQL as flyway can't do database creation for you. The flyway test resources include a mysql example.",
    "Is it possible to set the default command when the `docker exec` command is run on an already running container?": "You can\ndocker exec -it container_id echo hello\nKeep in mind that docker exec just launches commands, like some\ncat file\nor\necho abc >> /etc/abc.def\nor\nsed regex file\nor a shell\nDocker exec has no relationship with the ENTRYPOINT or CMD of the Dockerfile",
    "Why does apache2 does not remove pid file on reboot on docker image": "Is it possible to put a \"pre-CMD\" command in docker file ?\nAs mentioned in docker-library/php issue 53:\nI did it with cleanup old PID is exist in my startup entry.\nSee as an example PR 59 and its new apache2-foreground starting script:\n#!/bin/bash\nset -e\n\n# Apache gets grumpy about PID files pre-existing\nrm -f /var/run/apache2/apache2.pid\n\nexec apache2 -DFOREGROUND\nThe Dockerfile install that script:\nCOPY apache2-foreground /usr/local/bin/\nWhy this pid file does not create any problem on a physical computer ?\nThe aforementioned issue included this comment:\nI've run into this a few times myself, particularly when I've let the containers be stopped by running sudo stop docker (either directly or during a reboot). I just looked through how the init scripts and shutdown work under Ubuntu/Debian and it looks like everything just tries to shut down too fast and that all the containers are shut down in series.\nIf you've got one container that stops slowly (ahem, memcached...), then you can easily run into the situation where your containers are not stopped cleanly. I've generally been fortunate enough in most cases to be able to just delete the container and recreate it from scratch, but that can be a hard pill to swallow if all you've done is try to do a graceful reboot.\nThen:\nIs it possible to tell apache to erase this pid file at startup ?\nmd5 added:\nI also looked at the httpd source code and found that there's no good way to make Apache deal with this situation itself (cf. https://github.com/apache/httpd/blob/2.4.x/server/mpm_unix.c#L768) Is it possible to tell apache to erase this pid file at startup ?",
    "is it possible to run shell script and then node app.js inside docker container": "What I have done in the past is have my entrypoint defined in the Dockerfile be a shell script and then run the node command from within that script.\nSo in my Dockerfile I have this -\nENTRYPOINT [\"./docker-start.sh\"]\nAnd my docker-start.sh script contains this:\n#! /bin/bash\n\n# Initialization logic can go here\n\necho \"Starting node...\"\nnode start.js $* # The $* allows me to pass command line arguments that were passed to the docker run command. \n\n# Cleanup logic can go here",
    "docker: npm install on docker start": "Make your CMD point to a shell script.\nCMD [\"/my/path/to/entrypoint.sh\"]\nwith that script being:\n#!/bin/bash\nnpm install git+ssh://git@mycompany.de/my/project.git#develop\nnpm start\n# whatever else\nI find this easier for a few reasons:\nInevitably these commands increase with more being done\nIt makes it much easier to run containers interactively, as you can run them with docker run mycontainer /bin/bash and then execute your shell script manually. This is helpful in debugging",
    "How to COPY / ADD resources via a Dockerfile for privileged users?": "That looks a little clumsy to me?\nYes, it is clumsy, and it was discussed in issue 7537, even patched in PR 9934.\nBut rejected.\nYou can look into PR 10775 and its COPY --user= option, which was merged.",
    "Files unavailable when mouting the VOLUME with -v": "You are almost there, the command needs a small correction:\ndocker run --name=test -v $(pwd)/hostlogfolder:/log  dockerimage:1\nNote that the brackets are different:\nwrong: ${pwd}\nright: $(pwd)\nOnce running, you can verify the mounted volumes using:\ndocker inspect <container id> -- you can get the container id using docker ps\nCheck the Mounts section of the command output.\n\"Mounts\": [\n            {\n                \"Source\": \"<host path>\",\n                \"Destination\": \"<container path>\",\n                \"Mode\": \"\",\n                \"RW\": true\n           }]",
    "Build Docker image for ARM architecture on Intel machine (Mac)": "This is a little convoluted right now. I am sure it will be made easier by docker in the near future.\nBasically you need to build a contained based on a container that has the qemu-arm-static binary in it already.\nYou can see how it is done by looking at Raspberry Pi base image w/qemu-arm-static which builds the images directly with travis.\nWhat I did was basically on my raspberry pi build my base docker image with this binary added and push that to the docker-hub.\nOnce I have that image as my base I can build and run containers that are derived from it, including building new derived containers, on my OSX machines and then run it on my raspberry pi's unmodified.\nOn my raspberry pi I build an image using this Dockerfile. I am starting with hypriot's base alpine image. You should be able to use any base image you want.\nFROM hypriot/rpi-alpine-scratch\n\nRUN apk update && \\\napk upgrade && \\\napk add bash && \\\nrm -rf /var/cache/apk/*\n\nCOPY qemu-arm-static /usr/bin/qemu-arm-static\n\nCMD [\"/bin/bash\"]\nOnce I push that to Dockerhub I know have a container I can build based on on my intel machines and run and build on my raspberry pi's.\nThe qemu-arm-static binary I got by launching a debian i386 docker container, installing qemu-user-static and copying the binary out.",
    "openshift pod fails and restarts frequently": "You are seeing this because whatever process your image is starting isn't a long running process and finds no TTY and the container just exits and gets restarted repeatedly, which is a \"crash loop\" as far as openshift is concerned.\nYour dockerfile mentions below :\nENTRYPOINT [\"container-entrypoint\"]\nWhat actually this \"container-entrypoint\" doing ?\nyou need to check.\nDid you use the -p or --previous flag to oc logs to see if the logs from the previous attempt to start the pod show anything",
    "Specifying ELasticsearch's node.name in a docker-compose.yml": "-Des.node.name=\"TestNode\" is just an argument to the command, so you would specify it as part of the command:\ncommand: \"elasticsearch -Des.node.name=TestNode\"\nYou can also use the entrypoint as the \"base\" for command, and add args using command, like this:\nentrypoint: elasticsearch\ncommand: \"-Des.node.name=TestNode\"\nIf ENTRYPOINT is already set in the Dockerfile, you can omit it entirely.",
    "Dockerfile: Permission denied when trying to install ruby-build": "You are trying to do install ruby-build as root using the deploy user. \"Installing as a standalone program (advanced)\" as per here.\nYou can try something like this (using sudo):\nFROM centos:6.6\n\nRUN yum update -y\nRUN yum install git openssl-devel openssh-server sudo openssl readline-devel readline zlib-devel zlib libxml2-devel libxml2 libxslt-devel libxslt nginx tar gcc libaio libaio-devel -y\nRUN rpm -Uvh https://opscode-omnibus-packages.s3.amazonaws.com/el/6/x86_64/chef-12.5.1-1.el6.x86_64.rpm\nRUN sed -i -e \"s/Defaults    requiretty.*/ #Defaults    requiretty/g\" /etc/sudoers\n\nRUN mkdir -p /var/run/sshd\n\n# RUN adduser deploy -g wheel -p Password1\nRUN useradd -m -u 1000 -G wheel deploy && echo '%wheel  ALL=(ALL)  NOPASSWD: ALL' >> /etc/sudoers.d/wheel\nUSER deploy\n\nRUN git clone https://github.com/sstephenson/rbenv.git $HOME/.rbenv/\nRUN git clone https://github.com/sstephenson/ruby-build.git $HOME/.rbenv/plugins/ruby-build\nRUN sudo $HOME/.rbenv/plugins/ruby-build/install.sh\n\nENV PATH /home/deploy/.rbenv/bin:$PATH\nRUN echo 'eval \"$(rbenv init -)\"' | sudo tee -a /etc/profile.d/rbenv.sh\nRUN echo 'eval \"$(rbenv init -)\"' >> $HOME/.bashrc\nRUN echo 'eval \"$(rbenv init -)\"' >> $HOME/.bash_profile\nRUN source $HOME/.bash_profile\nENV CONFIGURE_OPTS --disable-install-doc\n\nRUN rbenv install 2.2.3\nRUN rbenv global 2.2.3\nRUN bash -l -c 'gem update --system'\nRUN bash -l -c 'gem update'\nRUN bash -l -c 'gem install nokogiri -- --use-system-libraries'\nRUN bash -l -c 'gem install bundler rails-api --no-rdoc --no-ri'\n\nRUN touch /etc/sysconfig/network\n\nEXPOSE 3306\nEXPOSE 22\nEXPOSE 80\nEXPOSE 3389",
    "Docker app in Exited (0) status": "You can use docker run --rm -it test_simple bash to get a shell into the container, then try to run nodemon /src/index.js manually and find out what may go wrong there, as well as run node directly instead of nodemon while debugging this issue.\nAlso, it may be better to base off the official node image",
    "Unable to create docker virtual machine": "I fixed this by reinstalling the Docker Toolbox, make sure you install all the components of the Oracle VM VirtualBox.",
    "Trying to Dockerize Maven Application but got \"release version 21 not supported\"": "I tried the maven eclipse temurin-21 image for JDK-21 maven:3.9.8-eclipse-temurin-21 for building the service and openjdk:21 for building my docker image. This worked for me for Java version 21.\nUpdated code of your DockerFile\nFROM maven:3.9.8-eclipse-temurin-21 AS build\n\nCOPY src /app/src\n\nCOPY pom.xml /app\n\nWORKDIR /app\nRUN mvn clean install -U\n\nFROM openjdk:21\nCOPY --from=build /app/target/mal-randomizer-0.0.1-SNAPSHOT.jar /app/app.jar\n\nWORKDIR /app\n\nEXPOSE 8080\n\nCMD [\"java\", \"-jar\", \"app.jar\"]\nMake sure the version in pom.xml is Java version 21 if you are using the above mentioned DockerFile.\npom.xml\n...\n<java.version>21</java.version>\n...",
    "Frontend app running inside docker container not accessible from browser [duplicate]": "Looking at the container logs it seems like your app is only listening on the 127.0.0.1 interface. When running inside a container youcan not connect to the 127.0.0.1.\nYou should configure the app to listen on 0.0.0.0 (probably via CMD [\"npm\", \"run\", \"dev\", \"--host\", \"0.0.0.0\"])",
    "WeaviateStartUpError: Weaviate did not start up in x seconds": "2023-06-20 15:21:05 weaviate.exceptions.WeaviateStartUpError: Weaviate did not start up in 25 seconds. Either the Weaviate URL http://localhost:8080 is wrong or Weaviate did not start up in the interval given in 'startup_period'.\nWhy is your code trying to connecto to weaviate at http://localhost:8080?\nif the buscahibrida service needs to communicate with the weaviate service, it sould be http://weaviate:8080",
    "Where are the python packages installed in docker": "Packages are installed in /usr/local/lib/<python version>/site-packages/<package name>/.\nfor example, the requests package for python 3.12 is located in:\n/usr/local/lib/python3.12/site-packages/requests/\nYou can also run pip show requests from within your docker container which should return the package location.",
    "Dockerfile - exec form of ENTRYPOINT and shell form of CMD": "If either ENTRYPOINT or CMD are not JSON arrays, they are interpreted as strings and converted to a length-3 array [\"/bin/sh\", \"-c\", \"...\"].\nThe resulting two lists are concatenated.\nSo in your example, the final command list would be\n[\"java\", \"/bin/sh\", \"-c\", \"$JAVA_OPTS -jar app.jar\"]\nor in Bourne shell syntax\njava /bin/sh -c '$JAVA_OPTS -jar app.jar'\nThis passes the shell interpreter /bin/sh as an argument to java; that almost certainly is not what you intend.\nIf the CMD is anything other than a complete command, it must use the JSON-array syntax, which in turn means it can't use any shell features and it can't expand environment variable references. This would include both the \"container-as-command\" pattern where ENTRYPOINT is the command to run and CMD its arguments, and the antipattern you show here where ENTRYPOINT is the interpreter only (and you have to repeat the -jar app.jar option in a docker run command override).\nI prefer a setup where CMD is always a complete shell command. If you have an ENTRYPOINT at all, it's a script that does some startup-time setup and then runs exec \"$@\" to run the command passed as arguments. This can accept either form of CMD.\n# ENTRYPOINT [\"./docker-entrypoint.sh\"]  # optional\nCMD java $JAVA_OPTS -jar app.jar         # in a single shell-format CMD",
    "NextJS environment variables not accessible in production build": "I ended up solving my own issue after hours of trying to figure it out.\nMy solution was to create a .env.production file and commit it to git.\nI also adjusted my Dockerfile to include: COPY --from=builder /my-site/.env.production ./\nI am not a fan of that solution, as it involves pushing secrets to a repo, but it works.",
    "Using cache for cmake build process in Docker build": "The important detail about Docker layer caching is that, if any of the previous steps have changed, then all of the following steps will be rebuilt. So for your setup, if you change anything in one of the earlier dependencies, it will cause all of the later steps to be rebuilt again.\nThis is a case where Docker multi-stage builds can help. The idea is that you'd build each library in its own image, and therefore each library build can be independently cached. You can then copy all of the build results into a final image.\nThe specific approach I'll describe here assumes (a) all components install into /usr/local, (b) /usr/local is initially empty, and (c) there aren't conflicts between the different library installations. You should be able to adapt it to other filesystem layouts.\nEverything below is in the same Dockerfile.\nI'd make a very first stage selecting a base Linux-distribution image. If you know you'll always need to install something \u2013 TLS CA certificates, mandatory package updates \u2013 you can put it here. Having this helps ensure that everything is being built against a consistent base.\nFROM ubuntu:20.04 AS base\n# empty in this example\nSince you have multiple things you need to build, a next stage will install any build-time dependencies. The C toolchain and its dependencies are large, so having this separate saves time and space since the toolchain can be shared across the later stages.\nFROM base AS build-deps\nRUN apt-get update \\\n && DEBIAN_FRONTEND=noninteractive \\\n    apt-get install --no-install-recommends --assume-yes \\\n      build-essential \\\n      cmake\n    # libfoo-dev\nNow for each individual library, you have a separate build stage that downloads the source, builds it, and installs it into /usr/local.\nFROM build-deps AS sqlite\nWORKDIR /sqlite\nADD https://sqlite.org/2022/sqlite-autoconf-3380200.tar.gz sqlite-autoconf-3380200.tar.gz\n...\nRUN make install\n\nFROM build-deps AS proj\nWORKDIR /proj\nADD https://download.osgeo.org/proj/proj-9.0.0.tar.gz proj-9.0.0.tar.gz\n...\nRUN cmake --build . --target install\nTo actually build your application, you'll need the C toolchain, plus you'll also need these various libraries.\nFROM build-deps AS app\nCOPY --from=sqlite /usr/local/ /usr/local/\nCOPY --from=proj /usr/local/ /usr/local/\nWORKDIR /app\nCOPY ./ ./\nRUN ./configure && make && make install\nOnce you've done all of this, in the app image, the /usr/local tree will have all of the installed libraries (COPYed from the previous image) plus your application. So for the final stage, start from the original OS image (without the C toolchain) and COPY the /usr/local tree in (without the original sources).\nFROM base\nCOPY --from=app /usr/local/ /usr/local/\n\nEXPOSE 12345\nCMD [\"myapp\"] # in `/usr/local/bin`\nLet's say you update to a newer patch version of proj. In the sqlite path, the base and build-deps layers haven't changed and the ADD and RUN commands are the same, so this stage runs entirely from cache. proj is rebuilt. That will cause the COPY --from=proj step to invalidate the cache in the app stage, and you'll rebuild your application against the newer library.",
    "How to use for loop in Dockerfile?": "There are no FOR/LOOP instructions in docker file, so I suggest you to:\nProvide a single COPY instruction which copies all the files arranged into distinct subfolders.\nWrite and COPY a shell script which loops through subfolder compiling them.\nProvide a single RUN instruction for the above script.\nRegards.",
    "Buildx failed with error: \"No such file or directory\" when trying to push docker on Github": "Make sure that you haven't excluded your target folders in your .dockerignore file.\nThat's what fixed it for me after several days of despairing search...",
    "Live Auto Reload of golang apps - Cosmtrek/air": "I hit the same problem on windows with docker desktop.\nTo resolve it, I activated the poll method so I don't have to rely on fsnotify, as it's not fully working on windows.\nAlso, I needed to add the poll frequency. Otherwise the hot reload will only work during the ~10 first second after the container is started.\nHere's a simple example of .air.toml\n[build]\n\ncmd = \"go build -o ./tmp/main ./\"\n\n# required on windows, fsnotify is not reliable on windows\npoll = true\npoll_interval = 500",
    "vue3 and vite.js, docker build production failed \"Error: Could not resolve entry module (index.html).\"": "My bad -_-!\nI made a mistake in the COPY path in the frontend.\nThis solution works:\n# Build backend application\nFROM node:14.19.1-alpine AS builder\nWORKDIR /panda-planner/backend-planner/\nCOPY /backend-planner/package*.json .\nRUN npm install\nCOPY /backend-planner/ .\nRUN npm run build\nEXPOSE 1337\nCMD [\"npm\", \"run\", \"start\" ]\n\n# Build frontend application\nFROM builder AS frontend\nWORKDIR /panda-planner/frontend-planner/\nCOPY /frontend-planner/package*.json .\nRUN npm install --legacy-peer-deps\nCOPY /frontend-planner/ .\nRUN npm run build\n\n# Setup nginx server for frontend\nFROM nginx:stable-alpine AS nginx\nCOPY --from=frontend /panda-planner/frontend-planner/dist/ /usr/share/nginx/html/\n#COPY ./default.conf /etc/nginx/conf.d/default.conf\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\" ]",
    "Can't install fasttext on docker container": "You can add this to Dockerfile to get the C++11 package:\nRUN apt-get update &&\\\n    apt-get install --no-install-recommends --yes build-essential",
    "Trouble with updating running VueJs app in production without downtime": "TL;DR\ndocker-compose down\ndocker volume rm mkstat_frontend_dist\ndocker-compose up --build\n... is not optimal as the services will be down for the duration of the build. The services don't need to be taken down for the build, thus:\ndocker-compose build\ndocker-compose down\ndocker volume rm mststat_frontend_dist\ndocker-compose up -d\n... would be slightly more efficient as the services would only be taken offline to purge the old containers and volume then new containers will be created from the pre-built images.\nThe volume backing the dist files isn't needed as far as I can tell, you could remove it. The dist files served in the deployment would be that which is built in the image without having to drop the volume every re-deployment.\nIf you're pushing your images to Docker Hub or other docker registry then the image(s) have already been built, you don't need to re-build during the re-deployment. Your process might look like:\nbuild the docker image on your PC\npush the docker image to the docker registry\npull the docker image on the server\ndocker-compose down the services to remove the old containers\ndocker-compose up -d to start new containers from the images\nFor a production deployment of nginx, this is the example config that I use as a base:\nworker_processes 1;\n\nerror_log /var/log/nginx/error.log warn;\npid /var/run/nginx.pid;\n\nevents {\n  worker_connections 1024;\n}\n\nhttp {\n  map $http_upgrade $connection_upgrade {\n    default upgrade;\n    '' close;\n  }\n\n  include /etc/nginx/mime.types;\n  default_type application/octet-stream;\n\n  log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                    '$status $body_bytes_sent \"$http_referer\" '\n                    '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n  client_max_body_size 0;\n  access_log /var/log/nginx/access.log main;\n  sendfile on;\n  keepalive_timeout 65;\n\n  upstream web {\n    server web:80 max_fails=3;\n  }\n  server {\n    listen *:80;\n    listen [::]:80;\n    server_name _;\n    return 301 https://$host$request_uri;\n  }\n  add_header X-Frame-Options SAMEORIGIN;\n  add_header X-Content-Type-Options nosniff;\n  add_header Strict-Transport-Security \"max-age=31536000; includeSubdomains; preload\";\n  add_header Content-Security-Policy \"default-src 'self';\";\n  server {\n    listen *:443 ssl http2;\n    listen [::]:443 ssl http2;\n    server_name *.example.com example.com;\n    charset utf-8;\n\n    error_page 404 = @notfound;\n\n    server_tokens off;\n\n    proxy_buffer_size 128k;\n    proxy_buffers 4 256k;\n    proxy_busy_buffers_size 256k;\n\n    ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;\n    ssl_session_cache shared:SSL:50m;\n    ssl_session_timeout 1d;\n    ssl_session_tickets off;\n    ssl_dhparam /etc/nginx/ssl/dhparam.pem;\n\n    ssl_prefer_server_ciphers on;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers 'ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS';\n\n    resolver 127.0.0.11 1.1.1.1 8.8.8.8 8.8.4.4 valid=86400s;\n    resolver_timeout 5s;\n    ssl_stapling on;\n    ssl_stapling_verify on;\n    ssl_trusted_certificate /etc/letsencrypt/live/example.com/chain.pem;\n\n    error_page 404 = @notfound;\n\n    location @notfound {\n      return 301 /;\n    }\n    location /healthz {\n      allow 127.0.0.1;\n      deny all;\n      stub_status;\n    }\n    location / {\n      proxy_http_version 1.1;\n      proxy_set_header HOST $host;\n      proxy_set_header X-Real-IP $remote_addr;\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_set_header Connection $connection_upgrade;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-Server $host;\n      proxy_set_header X-Forwarded-Port $server_port;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_pass http://web/;\n    }\n  }\n}\nThe nginx.conf example is the result of a few third-party pentests, e.g. server_tokens (Default: on) will be flagged, allowing old versions of TLS/SSL will be flagged, not setting the Content-Security-Policy header will be flagged, etc.",
    "Cassandra ERROR: java.io.IOException: failed to connect to /127.0.0.1:7000 for streaming data": "I needed to change the IP 127.0.0.1 to 172.18.0.2 to access the port 7000.\nThe IP Address 172.18.0.2 is a private IP address. Private IP addresses are used inside a local area network (LAN) and are not visible on the internet. Private IP addresses are defined in RFC 1918 (IPv4) and RFC 4193 (IPv6).",
    "Can't launch Chromium as executable doesn't exists if customized image is build from Dockerfile": "I believe your problem lies with using alpine.\nAccording to the playwright developers, there are no plans to support playwright on alpine. This makes your whole undertaking more complex. It's correct that you need to provide your own chromium and cannot use the browsers that come with playwright. Therefore, you should set PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD to prevent any (incompatible) browsers from being loaded.\nThe chromium executable should be in your Docker image under /usr/bin/chromium-browser. You need to use playwright's browserType.launch to set the path to the executable:\nconst { chromium } = require(\"playwright-chromium\");\n// ...\nconst browser = await chromium.launch({\n    executablePath: process.env.PLAYWRIGHT_CHROMIUM_EXECUTABLE_PATH,\n});\nIf you want a simpler solution, I would suggest using the Docker image of Zanika, containing chromium and playwright already. Here the link to the tag on DockerHub. At the very least you can see it as a reference implementation if you still want to use your own image.",
    "Dockerfile not COPY pakcage json": "Try out by editing your docker compose, you have a problem in your volumes that is copying the hole project directory.\nreplace\n    volumes:\n      - .:/src\nwith\n    volumes:\n      - ./backend:/src",
    "How to pass host computer working directory as a build argument to my Dockerfile using docker-compose?": "You need to use string interpolation (\"${ENV_VAR}\") to get the actual value of an environment variable (see documentation).\nversion: \"3.7\"\nservices:\n  runner:\n    build:\n      context: \"./runner\"\n      dockerfile: \"Dockerfile\"\n      args:\n        HOST_WORKING_DIRECTORY: \"${PWD}\"",
    "Installing brew package during Docker build not working": "It's most likely the $PATH problem. The reason why the mentioned answer did not work for you is that you are trying to use arduino-cli when $PATH is not yet changed. This should make it work:\nRUN git clone https://github.com/Homebrew/brew ~/.linuxbrew/Homebrew \\\n&& mkdir ~/.linuxbrew/bin \\\n&& ln -s ../Homebrew/bin/brew ~/.linuxbrew/bin \\\n&& eval $(~/.linuxbrew/bin/brew shellenv) \\\n&& brew --version \\\n&& brew install arduino-cli\n\n# first change PATH\nENV PATH=~/.linuxbrew/bin:~/.linuxbrew/sbin:$PATH\n\n# then run\nRUN arduino-cli version\n\n# not vice-versa",
    "Connecting to a docker localhost port from my local machine": "Running on http://127.0.0.1:8000/\nIf that's the flask server's console output, then it sounds like you're launching it incorrectly within the container. This looks like Flask is running on 127.0.0.1 (localhost) inside the container. It needs to run on the container's external interface!\nAcheive this by launching with:\nflask run -h 0.0.0.0\nOr if using the (outdated) method, app.run within the app, pass it a host argument:\nif __name__ == '__main__':\n    app.run(host='0.0.0.0',\n        # Any other launch args you have\n        )\n0.0.0.0 is special address which means \"all interfaces\".\nI'd also take @jdickel's advice, and omit 127.0.0.1 from the run command.",
    "Passing environment variables in Dockerfile not working when I use 2 baseImages": "You made two mistakes: mixed ENV with ARGS and a syntax error.\nWhen you define an ARG you can pass a value when build the image, if not specified the default value is used; then read the value on Dockerfile without the braces: (Note: I used slightly different docker images)\nDockerfile\nFROM node:12-alpine as build\n\n# Note: I specified a default value\nARG APP_ENV=dev\nWORKDIR /app\n\nENV PATH /app/node_modules/.bin:$PATH\n\nCOPY dist /app/dist\nCOPY define-env.js /app\nCOPY environments.json /app\n\n# Note $APP_ENV and NOT ${APP_ENV} without braces\nRUN node define-env.js $APP_ENV\n\n# NGINX\nFROM nginx:1.19\nCOPY --from=build /app/dist /usr/share/nginx/html\n# Not necessary to specify port and CMD\nNow for build the image you must specify the APP_ENV argument value\nBuild\ndocker build --build-arg APP_ENV=uat -t docker-uat .\nRun\ndocker run -d -p 8888:80 docker-uat\n;TL,DR\nYour request is to specify the configuration at runtime, this problem has two solutions:\nUse ENV var when running container\nUse ARGS when building container\nFirst solution has a drawback of security but the advantage of distributing a single image: you copy the entire file from host to container and when running the container you passing the correct ENV value.\nSecond solution is slightly more complex do not has the security problem but you must build separate images each for each config.\nThe second solution is the already written answer, for the first solution you must simply read the env variable at runtime\nDockerfile\n# NodeJS\nFROM node:12-alpine as build\n\nWORKDIR /app\nENV PATH /app/node_modules/.bin:$PATH\n\nCOPY dist /app/dist\nCOPY environments.json /app/dist\n\n# NGINX\nFROM nginx:1.19\n\n# Define the environment variable\nENV APP_ENV=dev\nCOPY --from=build /app/dist /usr/share/nginx/html\nBuild\ndocker build -t docker-generic .\nRun\ndocker run -d -p 8888:80 -e APP_ENV=uat docker-generic\nCode\n# On your nodejs code you simply read the ENV value\n// ....\nvar env = process.env.APP_ENV\nReferences\nDockerfile ARG\nDocker build ARG\nDockerfile ENV\nDocker run ENV",
    "NodeJS application stops working when containerised [FTP server in a container]": "Listen on 0.0.0.0:5000 in the container, with passive ports defined\nconst FtpSvr = require ( 'ftp-srv' );\n  \nconst hostname = '0.0.0.0';\nconst port = 5000;\n\nconst ftpServer = new FtpSvr ({\n  url: `ftp://${hostname}:${port}`,\n  anonymous: true,\n  pasv_url: `ftp://${hostname}:${port}`,\n  pasv_min: 65500,\n  pasv_max: 65515,\n});\nBuild the container as is and then run with the following ports mapped, which can all be used in an ftp connection:\ndocker run -p 5000:5000 -p 65500-65515:65500-65515 --rm rrakshak/ftp-demo\nGives the response:\n$ curl ftp://localhost:5000\n-rw-r--r-- 1 1 1          141 Oct 21 01:22 Dockerfile\ndrwxr-xr-x 1 1 1         4096 Oct 21 01:21 node_modules\n-rw-r--r-- 1 1 1        21137 Oct 21 01:21 package-lock.json\n-rw-r--r-- 1 1 1           52 Oct 21 01:21 package.json\n-rw-r--r-- 1 1 1          660 Oct 21 01:23 server.js\n-rw-r--r-- 1 1 1        20287 Oct 21 01:21 yarn.lock\nThe ftp client must be set to use passive mode.\nWhen an FTP client is in active mode, the FTP server receives a PORT command from the client and creates a new TCP connection from the container back out to the client for data on that PORT.\nDue to the Docker port mapping into the container, the source address of this data connection often won't match what the FTP client is using as the initial destination for the FTP server. Similar issues occur when setting up FTP servers behind NAT on classic servers.",
    "nginx: [emerg] mkdir() \"/var/lib/nginx/tmp/client_body\" failed (13: Permission denied)": "There are different issues in this image:\nchmod is used incorrectly\nno chown is present\nyou plan to use a privileged port (80) with a non-root user\nIMO this Dockerfile portion should fix part of your troubles:\nRUN mkdir -p /var/lib/nginx/tmp /var/log/nginx \\\n    && chown -R api-gatway:api-gatway /var/lib/nginx /var/log/nginx \\\n    && chmod -R 755 /var/lib/nginx /var/log/nginx\n\nEXPOSE 1080\nUSER api-gatway\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\nRemember to change the port in the nginx configuration as well.\nLast but not least, I think you're trying to map the a folder to /var/lib/nginx/tmp/client_body since I don't see any content being copied in the image. In this case, you have to make sure that the folder on the host can be read by the user in the docker container.\nPersonal opinion: you're better off with the official nginx docker image.",
    "Dockerfile set runtime ENV dinamically by sourcing a script": "you can create an enviroment file and just pass it to your container with the --env-file flag. This will make all the variables in the file available in the container.\nubuntu@vps-f116ed9f:~$ cat my_env_file\nTEST=Hello World\n\nubuntu@vps-f116ed9f:~$ docker container run -it --rm --env-file my_env_file ubuntu bash -c \"echo \\$TEST\"\nHello World\n\nubuntu@vps-f116ed9f:~$ docker container run -it --rm --env-file my_env_file ubuntu bash -c \"echo \\$TEST | wc -c\"\n12\nhere you can see i have used the latest ubuntu image, i pass my_env_file to it and then using the bash shell i print the value of this variable (Note i have to escape the $ other wise the shell will interpolate this before passing it to docker, this could be avoided by using single qoutes as the shell wont interpolate variables in single qoutes.)\nI also dont see any issues using pipe or &&\nubuntu@vps-f116ed9f:~$ docker container run -it --rm --env-file my_env_file ubuntu bash -c 'ls | head -n1 && echo \"$TEST\"'\nbin\nHello World\nThis also will persist in detached containers\nubuntu@vps-f116ed9f:~$ docker container run -itd --rm --name=c1 --env-file my_env_file ubuntu bash\n3d7705f2f91f3f30c45e855778bd80f08a35616bbe822545c20d5a8886139693\n\nubuntu@vps-f116ed9f:~$ docker container exec c1 sh -c \"ls | head -1 && echo \\$TEST\"\nbin\nHello World",
    "How do I replace \"\\r\" line endings when running Docker script on Windows?": "I just ran into the same problem, and found out that it is caused by the file end of line break type. On windows most file will be saved in CRLF instead of LF. Change the break type from CRLF to LF would solve the issue.\nif you're on vscode you can easily change it at the bottom right.",
    "common commands between docker images": "Scratch does not contain anything, better to use alpine as a base image and remove the below command from reusable section\nRUN useradd -s /bin/bash -p $(openssl passwd -1 user) -d /home/user -m -G sudo user\nUSER user\nAs these commands vary from OS to OS(base image) and it will not work in alpine etc\nSo I will suggest something like\nimage1  \n--->Dockerfile\nimage2  \n--->Dockerfile\nbaseimage\n--->ssh-keys  \n------>config  \n------>id_rsa  \n------>id_rsa.pub  \n--->Dockerfile\nDesing base image\nFROM alpine as sshconfig\nWORKDIR /home/user\n\nRUN mkdir -p /home/user/.ssh/ && \\\n    chmod 0700 /home/user/.ssh  && \\\n    touch /home/user/.ssh/authorized_keys && \\\n    chmod 600 /home/user/.ssh/authorized_keys && \\\n    touch /home/user/.ssh/config && \\\n    chmod 600 /home/user/.ssh/config\nCOPY ssh-keys/ /keys/\nRUN cat /keys/id_rsa.pub >> /home/user/.ssh/authorized_keys\nRUN cat /keys/config >> /home/user/.ssh/config\nBuild this image\ndocker build -t sshconfig .\nNow subsequent Docker Image will copy from this sshconfig Docker base image.\nImage1\nFROM ubuntu\n# Add user\n# change user\n\n# copy ssh-config from base image\nCOPY --from=sshconfig /home/user/.ssh /home/user/.ssh\n\n#for testing and verify keys\nCOPY --from=sshconfig /keys/ /keys/\n# list keys copies form base image\nRUN ls -lstrah /home/user/ /home/user/.ssh /keys/",
    "Access a network drive from docker container": "I have this script for mounting Windows shared folders in Ubuntu Docker container. You have to copy the script in your image. Then you should run the container, and after that call these commands:\ndocker exec container_name mkdir -p %shared folder inside docker path%\ndocker exec container_name /bin/bash /~path to your script~/mount_folder.sh %username% %password% %network_path% %docker_path%\nFor instance:\ndocker exec container_name /bin/bash /~path to your script~/mount_folder.sh \"Admin\" somePassword //192.168.7.1/shared_folder /data\nScript is (mount_folder.sh):\n#!/bin/bash\n\nset -e\n\nUSERNAME=${1}\nPASSWORD=${2}\nNETWORK_PATH=${3}\nDOCKER_PATH=${4}\n\nmount -t cifs -o rw,username=\"${USERNAME}\",password=\"${PASSWORD}\",vers=3.0,nolock $NETWORK_PATH $DOCKER_PATH",
    "How to reduce docker image size for CentOS 7?": "You have some unnecessary instructions, Could you try with this Dockerfile?\n#Use CentOS as base\nFROM centos:7.5.1804\n\n# Install custom tools\nRUN yum -y install openssl ccrypt nc epel-release python3 && \\\n    pip3 install j2cli && \\\n    yum clean all && rm -rf /var/cache/yum",
    "Expected directory: node-v83-linux-x64-musl. Found: [node-v72-linux-x64-musl]": "Module version 83 corresponds to Node 14, not 13. Unfortunately, we have not yet published binaries for Node 14, so it's going to be difficult to get that working\nFull answer - https://github.com/grpc/grpc-node/issues/1460#issuecomment-638965479",
    "Build and run your image in docker - network problem": "It's a simple firewalld issue.\nrun\nsudo firewall-cmd --permanent --zone=trusted --add-interface=docker0\nand restart your firewalld,\nsudo systemctl restart firewalld",
    "How to run systemctl enable xxx during docker build?": "Your build is trying to do a privileged operation. The issue is, --privileged flag is unavailable at build time. Here's the issue, which makes an interesting reading : https://github.com/moby/moby/issues/1916\nTL;DR there seems to be two possibilities with docker itself\nEnable experimental mode then RUN --security=insecure ./xxx.run, as documented here https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/experimental.md#run---securityinsecuresandbox\nInstall buildx cli plugin, then docker buildx build --allow security.insecure, as documented here https://github.com/docker/buildx/blob/master/README.md#--allowentitlement\nOther solution outside docker would be to use buildah, but that's a longer shot by far. But an interesting one, especially if you works in the RedHat ecosystem, since they want to get rid of Docker in favor of Buildah/Podman. Here's the get started. Though I'm not clear about how it solve your issue, it's been highlighted as being able to deal with this kind of issues.",
    "How to write docker-compose file for bbb (big blue button)?": "just you need lines below in Docckerfiles becuse bbb need configure and this code passing this step;\n `-RUN apt-get install -y bigbluebutton(remove)\n +RUN apt-get install -y bigbluebutton || :\n +RUN gem install bundler -v 1.16.1\n +RUN apt-get install -y bigbluebutton\n  RUN apt-get install -y bbb-demo`\nthis is not bug just compile error.",
    "Is there a way to see a complete Docker image inheritance?": "Nope. It is not possible unless you have the Dockerfiles or you have built the image on your machine and you still have the build cache.\nFor the second case, you can use docker history to approximately rebuild the Dockerfile of an image. docker history prints the image ids of the layers. Then you can get the repo tags using docker inspect. The base image used will usually be the last layer in the output of docker history.\neg:\n$ docker history t:1\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n10b4cce00fb8        3 days ago          /bin/sh -c #(nop)  CMD [\"flask\" \"run\"]          0B\n824987ef6cab        3 days ago          /bin/sh -c #(nop) COPY dir:1973b65388e92428e\u2026   406B\nd4b6f433a5df        3 days ago          /bin/sh -c pip install -r requirements.txt      4.98MB\n8827b3f01d00        3 days ago          /bin/sh -c #(nop) COPY file:98271fcaff00c6ef\u2026   0B\n65b8c98138e6        2 weeks ago         /bin/sh -c apk add --no-cache gcc musl-dev l\u2026   113MB\n01589531f46d        2 weeks ago         /bin/sh -c #(nop)  ENV FLASK_RUN_HOST=0.0.0.0   0B\n6c4640b8027a        2 weeks ago         /bin/sh -c #(nop)  ENV FLASK_APP=app.py         0B\nb4c8fc7f03d6        2 weeks ago         /bin/sh -c #(nop) WORKDIR /code                 0B\n16a54299f91e        2 weeks ago         /bin/sh -c #(nop)  CMD [\"python3\"]              0B\n$ docker inspect 16a54299f91e\n[\n    {\n        \"Id\": \"sha256:16a54299f91ef62cf19d7329645365fff3b7a3bff4dfcd8d62f46d0c9845b9c6\",\n        \"RepoTags\": [\n            \"python:3.7-alpine\"   ---> Base image used in FROM instruction. \nSee Is there a command to find out the base image of a Docker image?",
    "docker build error: psql: could not connect to server: Connection refused": "I found the solution, follow this\ndockerfile: db0.dockerfile\nFROM postgres:11\n\nENV POSTGRES_PASSWORD postgres\nCOPY db0.sql /docker-entrypoint-initdb.d/\nofficial postgres image will run .sql scripts found inside /docker-entrypoint-initdb.d/\nand then run docker run -d -p 5432:5432 -v /srv/docker_data/db0:/var/lib/postgresql/data/ -e POSTGRES_PASSWORD=postgres --name=db0 db0",
    "Docker - List environment variables defined in dockerfile from within a container": "You should not assign root privileged just for the sake of ENV, better to print all env and exclude the system environment that you are not intrested in.\nThe best option is to grep only that are defined in Dockerfile is to print all env that start from WEB_, as your mentioned Dockerfile ENV all start from WEB_*.\nENV WEB_DOCUMENT_ROOT=/app \\\n    WEB_DOCUMENT_INDEX=index.php \\\n    WEB_ALIAS_DOMAIN=*.vm \\\n    WEB_PHP_TIMEOUT=600 \\\n    WEB_PHP_SOCKET=\"\"\nENV WEB_PHP_SOCKET=127.0.0.1:9000\nENV WEB_NO_CACHE_PATTERN=\"\\.(css|js|gif|png|jpg|svg|json|xml)$\"\nSo this will print Dockerfile ENV only.\ndocker run -it --rm webdevops/php-apache-dev printenv | grep -E \"^WEB\"\nOr to exclude some variables you can try\ndocker run -it --rm webdevops/php-apache-dev printenv | grep -vE \"^PATH|^HOME|^TERM\" \nthis will not show your mentioned ENV that you are not intrested in.",
    "Bash instances are not nesting in Dockerfile `RUN`": "This answer is possibly still incomplete, but fascinated by the problem, I have taken some time to debug it using the following Dockerfile:\nFROM debian:10\nSHELL [\"/bin/sh\", \"-ec\"]\nRUN apt-get update > /dev/null; apt-get -y install psmisc > /dev/null\n\n# First layer is good: expected 1, got 1\nRUN /bin/bash -exc \"pstree; echo SHLVL=\\$SHLVL\"\n\n# The surprising example (getting the escaping right is already tricky)\nRUN /bin/bash -exc \"/bin/bash -exc \\\"pstree; echo SHLVL=\\\\\\$SHLVL\\\"\" # expected 2, got 1\nRUN /bin/bash -exc \"/bin/bash -exc \\\"/bin/bash -exc \\\\\\\"pstree; echo SHLVL=\\\\\\\\\\\\\\$SHLVL\\\\\\\"\\\"\" # expected 3, got 1\n\n# Now what happens if two commands run in the inner bash\nRUN /bin/bash -exc \":; /bin/bash -exc \\\":; pstree; echo SHLVL=\\\\\\$SHLVL\\\"\" # expected 2, got 1\nRUN /bin/bash -exc \":; /bin/bash -exc \\\":; /bin/bash -exc \\\\\\\":; pstree; echo SHLVL=\\\\\\\\\\\\\\$SHLVL\\\\\\\"\\\"\" # expected 3, got 1\nThe interesting thing seems to be: In case a bash invocation is followed directly by another one, it will be \"optimized\" (?) away. As far as I can tell, this is not a Docker-specific thing, because it can be reproduced interactively (on my Debian 10 system, the interactive command sequence from the question produces 1, 2, 2 and not 1, 2, 3 for the nesting!).\nIn any case, the output from building the Dockerfile is as follows:\nSending build context to Docker daemon  30.21kB\nStep 1/8 : FROM debian:10\n ---> 8e9f8546050d\nStep 2/8 : SHELL [\"/bin/sh\", \"-ec\"]\n ---> Running in 3509ef249c45\nRemoving intermediate container 3509ef249c45\n ---> 8956c1fddb7c\nStep 3/8 : RUN apt-get update > /dev/null; apt-get -y install psmisc > /dev/null\n ---> Running in 5cabec19144a\ndebconf: delaying package configuration, since apt-utils is not installed\nRemoving intermediate container 5cabec19144a\n ---> 64cad97f7793\nStep 4/8 : RUN /bin/bash -exc \"pstree; echo SHLVL=\\$SHLVL\"\n ---> Running in 22a0aa663163\n+ pstree\nsh---bash---pstree\n+ echo SHLVL=1\nSHLVL=1\nRemoving intermediate container 22a0aa663163\n ---> 4caa146b24f6\nStep 5/8 : RUN /bin/bash -exc \"/bin/bash -exc \\\"pstree; echo SHLVL=\\\\\\$SHLVL\\\"\" # expected 2, got 1\n ---> Running in 538ff45db230\n+ /bin/bash -exc 'pstree; echo SHLVL=$SHLVL'\n+ pstree\nsh---bash---pstree\nSHLVL=1\n+ echo SHLVL=1\nRemoving intermediate container 538ff45db230\n ---> 1d4c9c2638fa\nStep 6/8 : RUN /bin/bash -exc \"/bin/bash -exc \\\"/bin/bash -exc \\\\\\\"pstree; echo SHLVL=\\\\\\\\\\\\\\$SHLVL\\\\\\\"\\\"\" # expected 3, got 1\n ---> Running in 3f0650d4d21b\n+ /bin/bash -exc '/bin/bash -exc \"pstree; echo SHLVL=\\$SHLVL\"'\n+ /bin/bash -exc 'pstree; echo SHLVL=$SHLVL'\n+ pstree\nsh---bash---pstree\nSHLVL=1\n+ echo SHLVL=1\nRemoving intermediate container 3f0650d4d21b\n ---> 2d977033884d\nStep 7/8 : RUN /bin/bash -exc \":; /bin/bash -exc \\\":; pstree; echo SHLVL=\\\\\\$SHLVL\\\"\" # expected 2, got 1\n ---> Running in 39b79af0f558\n+ :\n+ /bin/bash -exc ':; pstree; echo SHLVL=$SHLVL'\n+ :\n+ pstree\nsh---bash---bash---pstree\n+ echo SHLVL=2\nSHLVL=2\nRemoving intermediate container 39b79af0f558\n ---> 48170e9bcb01\nStep 8/8 : RUN /bin/bash -exc \":; /bin/bash -exc \\\":; /bin/bash -exc \\\\\\\":; pstree; echo SHLVL=\\\\\\\\\\\\\\$SHLVL\\\\\\\"\\\"\" # expected 3, got 1\n ---> Running in 456e6ec421ca\n+ :\n+ /bin/bash -exc ':; /bin/bash -exc \":; pstree; echo SHLVL=\\$SHLVL\"'\n+ :\n+ /bin/bash -exc ':; pstree; echo SHLVL=$SHLVL'\n+ :\n+ pstree\nsh---bash---bash---bash---pstree\n+ echo SHLVL=3\nSHLVL=3\nRemoving intermediate container 456e6ec421ca\n ---> 30a07d3bdc95\nSuccessfully built 30a07d3bdc95\nSuccessfully tagged test:latest\nFinally, the pstree output is interesting because it shows how there is actually no other bash process running at the respective points (i.e. the variable tracks the actual shell nesting correctly in all of the cases, it is just that sometimes there are less shells running than expected).",
    "How to run maven wrapper from docker?": "I find the way how to fix this issue. I had just added chnod +x ./mvnw command and final RUN command looks like this:\nRUN chmod +x ./mvnw && \\\n./mvnw -s .mvn/settings.xml -B -f /app/pom.xml dependency:resolve-plugins dependency:resolve dependency:go-offline",
    "Docker container: pip is not found even though I have set the PATH during building": "I don't see pip in the base image you used in your Dockerfile, you can check the offical Dockerfile, nor in the base image of nvidia/cuda, you can check the base image too 10.0-cudnn7-devel-ubuntu18.04\nInstalled pip and then try\nFROM nvidia/cuda:10.0-cudnn7-devel-ubuntu18.04\nRUN apt update && apt install python3-pip -y\nRUN pip3 --version",
    "Docker django runs only if I specify the command": "I think that you can use only Entrypoint command.\nTry with:\nFROM python:3.6-slim-buster\n\nWORKDIR /app\n\nCOPY . /app\n\nRUN pip install -r Requirements.txt\n\nEXPOSE 8000\n\nENTRYPOINT [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\nOr you can write script file (entrypoint.sh) with the line. And maybe you can run makemigrations and migrations in the same file.",
    "Is it possible to re-use the image from another stage in Docker multi-stage builds?": "Apparently this is perfectly possible as pointed out by @Zeitounator. To do it, just use the stage name as image name in FROM like so:\nFROM ubuntu:latest AS production\nRUN /bin/bash install-my-app.sh\n\nFROM production AS debug\nRUN /bin/bash install-debug-tools.sh",
    "Running cron in a docker container on a windows host": "rferalli's answer on the github issue did the trick for me:\n\"Had the same issue. Fixed it by changing line ending of the crontab file from CRLF to LF. Hope this helps!\"",
    "How to use the same dockerfile for dev and prod": "What about something like this? I didn't test it and didn't think through your example deeply, but maybe is somewhere close to what you need/helps you finding out the final solution?\nFROM golang:alpine AS base\nWORKDIR /go/src/gitlab.com/company/project\nCOPY . .\nRUN go build -o ./release/api .\n\nFROM base AS dev\nCMD [\"./release/api\"]\n\nFROM scratch AS prod\nEXPOSE 9999\nCOPY --from=base /go/src/gitlab.com/company/project/release/api .\nCMD [\"./api\"]\nDepending on the value specified in target docker build --target=prod or docker build --target=dev, a different image will get built.",
    "Docker ENTRYPOINT variable not carried forward into CMD": "I suspect there are 1-2 issues:\nENTRYPOINT and CMD aren't inherited from the FROM image\nYou may be using the exec form of ENTRYPOINT|CMD\nThere are 2 forms of ENTRYPOINT and CMD, the shell and exec forms. The exec form \"does not invoke a command shell\" and so you won't get environment variable processing:\nhttps://docs.docker.com/engine/reference/builder/#cmd\nHere's an example which shows ENV values passing through FROM'd images:\necho '\nFROM bash\nENV DOG=Freddie\nENTRYPOINT echo \"dog=${DOG}\"\n' | docker build --tag=1st --file=- .\necho '\nFROM 1st\nENV CAT=Emmett\nENTRYPOINT echo \"dog=${DOG}\" \"cat=${CAT}\"\n' | docker build --tag=2nd --file=- .\ndocker run 1st\ndocker run 2nd\nResults in:\ndog=Freddie\ndog=Freddie cat=Emmett\nNB ${DOG} (and its value) is inherited from 1st",
    "How to solve Node.js Error: Cannot find Module?": "Have volume folder mapping for node_modules and ensure it has mongo folder copied / created\nversion: '3'\nservices:\n  web:\n    build: .\n    command: npm start\n    volumes:\n      - ./src:/usr/app/\n      - ./src/node_modules:/usr/app/node_modules\n    ports:\n      - 80:8080\nRef:https://morioh.com/p/42531a398049/containerizing-a-node-js-application-for-development-with-docker-compose",
    "Newbie: Create dockerfile for SQL Server 2017": "as a preface: you don't need a dockerfile or docker-compose in order to run a sql server instance for your development environment. If you just want quickly start your latest 2017 container just enter the comand you already found.\nIf you however want to easily store the command, not having to type it everytime, why not add a simple bat script to you folder.\nThough, if you want to extend your docker image (e.g. install additional libraries) then you would need a dockerfile. Please also consult the docker documentation (great documentation) for how to do that. Or if you want to compose multiple docker instances (in order for them to easily communicate with each other) then a docker-compose file is the way to go.\nBut in order to actually answer your question: Create some folder with 2 files\nfolder\n |__> DockerFile\n |__> docker-compose.yml\nThe contents of your DockerFile would then be: (mind that this example here actually does nothing except pointing to the existing microsoft image, but if you use it without the docker-compose you can also define file mappings, password config etc. here and simply start a new container from this new pre-configured image)\nFROM microsoft/mssql-server-windows-developer:2017\n# environment configuration moved into docker-compose file\n# EXPOSE 1433:1433\n# ENV attach_dbs=\"[{'dbName':'YourDBName','dbFiles':['C:\\\\temp\\\\yourDB.mdf','C:\\\\temp\\\\yourDB_Log.ldf']}]\"\n# ENV ACCEPT_EULA=Y\n# ENV sa_password=yourPassword\nThen your docker-compose.yml might look like this:\nversion: '3'\nservices:\n  yourServiceName:\n    container_name: yourContainerName\n    build: .\n    ports:\n    - \"1433:1433\"\n    volumes:\n    - .:C:/temp/\n    environment:\n      sa_password: \"yourPassword\"\n      ACCEPT_EULA: \"Y\"\n      attach_dbs: \"[{'dbName':'YourDBName','dbFiles':['C:\\\\\\\\temp\\\\\\\\yourDB.mdf','C:\\\\\\\\temp\\\\\\\\yourDB_Log.ldf']}]\"\ncalling docker-compose up in cml or PowerShell inside of the folder will start this new SQL-Server container.\nRegardless i would recommend mapping the mdf and ldf files for easy access and e.g. for storing a state even if you shut down the docker image (if that is your desired behaviour).",
    "How to pass variables through docker-compose to Dockerfile": "Regarding the proper way to pass values from docker-compose (or just from CLI) to Dockerfile, I guess you need to add some ARG directive, for example:\nFROM heroku/heroku:18 AS production\nARG MY_UID=\"...default UID...\"\nARG MY_GID=\"...default GID...\"\nRUN useradd -ms /usr/bin/fish -p $(openssl passwd -1 django) --uid \"$MY_UID\" --gid \"$MY_GID\" -r \nThen to test it:\n$ docker build --build-arg=MY_UID=\"1000\" --build-arg=MY_GID=\"1000\" -t test .\nI use a similar approach in the Dockerfile of coqorg/base (which is based on Debian).\nHowever, if you are especially interested in passing variables to ensure that the UID/GID match, note that another approach is possible that has the additional benefit to make your image compatible with several hosts using different UID/GID. It is described in this SO answer by @BMitch which proposes to fix the permissions of a container's directory at startup time, see e.g.:\nhttps://github.com/sudo-bmitch/docker-base/blob/master/examples/nginx/entrypoint.d/10-fix-perms.sh\nhttps://github.com/sudo-bmitch/docker-base/blob/master/bin/fix-perms",
    "Docker container prints \"no such file or directory\"": "Docker run with 'Container command not found or does not exist' means the command in the entrypoint does not exist.\nIn your case the command in the .sh file: sh or java both exist in the image openjdk:8-jre. So you can check the issue in the shell script syntax. If you edit the sh file with a Windows editor, make sure you don't have the CRLF (\\r\\n) at the end of the command. One way to remove \\r is to run sed before add the sh file to the container:\nsed -e 's/\\r//g' run_motus.sh",
    "How do I specify run arguments for a base image in a Dockerfile?": "TL;DR: you could use the CMD directive by doing something like this:\nFROM parent_org/parent:1.0.0\nCMD [\"--special-arg\"]\nhowever note that passing extra flags to docker run as below would overwrite --special-arg (as CMD is intended to specify default arguments):\ndocker build -t child_org/child .\ndocker run child_org/child  # would imply --special-arg\n\ndocker run child_org/child --other-arg  # \"--other-arg\" replaces \"--special-arg\"\nIf this is not what you'd like to obtain, you should redefine the ENTRYPOINT as suggested below.\nThe CMD and ENTRYPOINT directives\nTo have more insight on CMD as well as on ENTRYPOINT, you can take a look at the table involved in this other SO answer: CMD doesn't run after ENTRYPOINT in Dockerfile.\nIn your case, you could redefine the ENTRYPOINT in your child image (and if need be, the default CMD) by adapting child_org/child/Dockerfile w.r.t. what was defined in the parent Dockerfile.\nAssuming the parent_org/parent/Dockerfile looks like this:\nFROM debian:stable  # for example\n\nWORKDIR /usr/src/foo\nCOPY entrypoint.sh .\nRUN chmod a+x entrypoint.sh\n\nENTRYPOINT [\"./entrypoint.sh\"]\nCMD [\"--default-arg\"]\nYou could write a child_org/child/Dockerfile like this:\nFROM parent_org/parent:1.0.0\nRUN [\u2026]\n\n# Redefine the ENTRYPOINT so the --special-arg flag is always passed\nENTRYPOINT [\"./entrypoint.sh\", \"--special-arg\"]\n\n# If need be, redefine the list of default arguments,\n# as setting ENTRYPOINT resets CMD to an empty value:\nCMD [\"--default-arg\"]",
    "Docker compose and external images multi-stage builds": "I think you should be able to do that just fine.\ndocker-compose:\nversion: '3'\nservices:\n    my-shared-build:\n        image: my-shared-build:latest\n        build: my-shared-build\n\n    my-process-one:\n        image: my-process-one:latest\n        build: my-process-one\n        depends_on:\n            - my-shared-build\n\n    my-process-two:\n        image: my-process-two:latest\n        build: my-process-two\n        depends_on:\n            - my-shared-build\n            - my-process-one\nAssuming your Dockerfiles are in subdirectories my-shared-build, my-process-one, my-process-two this should build all 3 images (in order)",
    "use volume defined in Dockerfile from docker-compose": "When configured in the Dockerfile, a volume will result in any container started from that image, including temporary containers later in the build process from the RUN command, to have a volume defined at the specified location, e.g. /stuff. If you do not define a source for that volume at run time, you will get an anonymous volume created by docker for you at that location. However, you can always define a volume with a source at run time (even without the volume being defined) by specifying the location in your compose file:\nversion: \"3\"\nservices:\n  app:\n    image: your_image\n    volumes:\n      - data:/stuff\nvolumes:\n  data:\nNote that there are two volumes sections, one for a specific service that specifies where the volume is mounted inside the container, and another at the top level where you can specify the source of the volume. Without specifying a source, you'll get a local volume driver with a directory under /var/lib/docker bind mounted into the container.\nI do not recommend specifying volumes inside the Dockerfile in general, it breaks the ability to extend the image in later steps for child images, and clutters the filesystem with anonymous volumes that are not easy to track back to their origin. It's best to define them at runtime with something like a compose file.",
    "Dockerizing play framework": "I think your problem is that you use localhost to indicate your database. You should use your link instead. From the point of view of your \"sbt\" container, localhost is itself and not your local machine (with the port forwarding) or your postgresql container.\nGeneraly what I use is environment variables.\nlinks:\n  - postgres\n  - nginx\nenvironment: \n  - POSTGRES_SERVICE_HOST=postgres\nAfter you need to specify in your SBT script how to use your POSTGRES_SERVICE_HOST variable",
    "if else with dockerfile instructions": "If-Else isn't a thing in Dockerfiles. You would be better off using a one-liner bash command in your RUN that provided If-Else:\nRUN if git clone <repo>; then echo \"Exists\"; else echo \"Error\"; fi\nOr if the behavior you're after requires more scripting, put it in a script and run it that way to keep all the clutter out of your Dockerfile:\nCOPY git-script.sh /usr/local/bin\nRUN chmod +x /usr/local/bin/git-script.sh && \\\n    bash git-script.sh\nFound a similar answer here as well.",
    "Deleted Kubernetes Deployment but Node(docker) containers keep recreating themselves": "So if you delete a node on Kubernetes it just deletes it from etcd where Kubernetes keeps its state. However, the kubelet is still running on your node and may hold a cache (not 100% sure about it). I would try:\nsystemctl stop kubelet\nor\npkill kubelet\nverify that is not running:\nps -Af | grep kubelet  # should not return anything.\nThen stop and remove your container like you did initially.",
    "Docker is not running ... failed": "I had the same issue. I found a solution here:\nWhen you start your Jenkins container use -v to hand the host's docker.sock to the container:\ndocker run -v /var/run/docker.sock:/var/run/docker.sock ...\nIt might also be an issue that you run an root, while Jenkins is being run by user jenkins. I'd expect a different error though.\nHere is my Jenkinsfile (I don't install docker-compose)\nFROM jenkins/jenkins\n\nUSER root\n\nRUN apt update && apt install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common\nRUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add -\nRUN apt-key fingerprint 0EBFCD88\nRUN add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable\"\nRUN apt update && apt install -y docker-ce\nRUN usermod -aG docker jenkins\n\nUSER jenkins\nI start the container with\ndocker run --name mycustomjenkins \\\n  -p 8080:8080 -p 50000:50000 \\\n  -v jenkins_home:/var/jenkins_home \\\n  -v /var/run/docker.sock:/var/run/docker.sock <IMAGE-ID>\nThat way I am able to use Docker agents in my Jenkins.",
    "Robot Framework - Docker - Chromedriver": "I ran into this issue recently using a docker container and Amazon Linux running robot tests. I found that even though I added the required arguments within the robot framework test as in the example below chrome was crashing without even starting with the same message you received. I resolved the issue by updating the python settings in the options.py within the container.\nI updated my docker container with the command below to set the options in the python selenium chrome WebDriver options.py file. In my case I'm using python version 3.7 - so you want to make sure that the path you use is correct.\nRUN sed -i \"s/self._arguments\\ =\\ \\[\\]/self._arguments\\ =\\ \\['--no-sandbox',\\ '--disable-dev-shm-usage'\\]/\" /usr/local/lib/python3.7/site-packages/selenium/webdriver/chrome/options.py\nExample Robot - this is what I tried within robot framework that didn't fix the problem.\n${chrome_options} =     Evaluate sys.modules['selenium.webdriver'].ChromeOptions()    sys, selenium.webdriver\nCall Method    ${chrome_options}   add_argument    headless\nCall Method    ${chrome_options}   add_argument    disable-gpu\nCall Method    ${chrome_options}   add_argument    no-sandbox\nCall Method    ${chrome_options}   add_argument    disable-dev-sim-usage    ${options}=     Call Method     ${chrome_options}    to_capabilities\n${options}=  Evaluate  sys.modules['selenium.webdriver'].ChromeOptions()  sys, selenium.webdriver\nopen browser  about:blank  ${BROWSER}  desired_capabilities=${options}\nI'm not sure if this will address your issue. You could try updating your file manually before updating your container to see if it helps. I spent a lot of time troubleshooting this. It would be great if the error was a bit more descriptive. Good luck.",
    "Problem mounting local directory to container in docker (docker volumes).": "In your build file you are copying the contents of src into the image (under /var/www/html). The src directory is part of the build context, and must be below the 'build' root directory. This is probably where you ran the docker build command from, and is usually the place the dockerfile lives.\nIn your compose file you are mapping the src directory over the top of /var/www/html, replacing its contents with src at runtime.\nI'm guessing that you only actually want to do one of these two things. Either is fine, depending on what you're trying to achieve, but based on your report it looks like the place you're running your docker-compose up from has an empty src directory, hence the error message. To fix this, either use an absolute path to the directory you want to serve or remove the volumes definition for that container.\nOne way to check this (assuming you tagged your image HelloWorld):\ndocker run -it -d --name helloworld -p 80:80 HelloWorld\nThen, docker ps just to check that it's running. If everything is ok, try this:\ndocker exec -it -w /var/www/html helloworld ls -gAlFh\nThis should give you a list of what's in that directory. You can also try this:\ndocker exec -it -w /var/www/html helloworld sh\nTo get a shell in that directory so that you can explore more easily.",
    "File permissions problems in Dockerfile for a ruby application": "If you want a pure docker solution, try this:\nFROM ruby:2.5\n\nRUN apt-get update -qq && apt-get install -y build-essential libpq-dev nodejs\n\n# Reduce layers by grouping related commands into single RUN steps\nRUN groupadd -r -g 1000 appuser && \\\nuseradd -r -m -u 1000 -g appuser appuser\n\n# Setting workdir will also create the dir if it doesn't exist, so no need to mkdir\nWORKDIR /home/appuser/myapp\n\n# Copy everything over in one go\nCOPY . ./\n\n# This line should fix your issue \n# (give the user ownership of their home dir and make Gemfile.lock writable)\n# Must still be root for this to work\nRUN chown -R appuser:appuser /home/appuser/ && \\\nchmod +w /home/appuser/myapp/Gemfile.lock\n\nUSER appuser\n\nRUN bundle install\nMight be a better idea to fix the permissions on your host system with something like this:\nsudo chmod g+w Gemfile.lock",
    "The difference between commands under build process and in shell": "When you run docker-compose build web it will try to build the docker image according to the specifications in the Dockerfile. When you run docker-compose run web XXX it will run the container web with the command XXX for the existing image. That means the commands in the Dockerfile will not be executed. Most likely the error comes from a misconfiguration in the Gemfile, but I have no experience with that.",
    "How to hide user and pass in curl command in dockerfile": "You can use multi stage build to achieve a lightweight image, but you have to use one single docker build, instead of two. Like this single Dockerfile:\nFROM maven as build\n(... Your app build....)\nFROM tomcat\nCOPY --from=build artifact.war /dest/dir\nEverything before the second FROM is discarded from the resulting image, so it will contain Tomcat, not Maven, and your copied artifact.",
    "Docker error when containerizing a spring boot app": "Ok it looks like you've moved on a bit now and this has become another question. If you want to communicate with mysql from your Spring boot app, you'll need to put them on a network. You can declare the network in your docker compose file anywhere (I like to do it all the bottom) but then for each service you need to add your service to that network. You then need to set an environment variable for your Spring boot service that = the container name of the mysql container. In your case mysql-demo. You'll need to update your application.properties in the Spring boot project to accept that variable e.g.\nMYSQL_HOST=localhost spring.datasource.url=jdbc:mysql://${MYSQL_HOST}/YOUR-DB-NAME\nThat way your app will default to localhost unless you pass the environment variable in, which you'll do from your docker compose file and this will tell your app to point to the mysql container you've put on the same network as your app.",
    "Dockerfile run scrapy crawl command in a folder": "You should set up WORKDIR, ENTRYPOINT and CMD in your docker file:\nWORKDIR /tutorial-crawler\nENTRYPOINT [\"scrapy\"]\nCMD []\nThen:\n$ docker run -it image_name list\ntutorial\n$ docker run -it image_name crawl tutorial",
    "ERROR: unsatisfiable constraints: nodejs-npm (missing)": "Docker image python:3.6.1-alpine is based on Alpine Linux v3.4.\nAccording to alpine packages portal, npm binary can be found in nodejs package in Alpine Linux version 3.4.\nSo, the final Dockerfile is:\nFROM python:3.6.1-alpine\nRUN apk update && \\\n    apk add --update nodejs  && \\\n    npm install newman --global",
    "Cannot access exposed Dockerized React app on Kubernetes": "The short version is that the Service is listening on the same TCP/IP port on every Node in your cluster (34604) as is shown in the output of describe service:\nNodePort:                 <unset>  34604\nIf you wish to access the application through a \"nice\" URL, you'll want a load balancer that can translate the hostname into the in-cluster IP and port combination. That's what an Ingress controller is designed to do, but it isn't the only way -- changing the Service to be type: LoadBalancer will do that for you, if you're running in a cloud environment where Kubernetes knows how to programmatically create load balancers for you.",
    "Cannot find module error for a node js app running in a docker environment": "Without knowing the contents of C:\\portal\\environments\\development I will assume that there isn't server/index.js in it.\nI couldn't figure out why giving this error even though all folders are in place in the docker container(RUN ls- shows the folder structure in the container). still i couldnt start the container because of this error.\nRUN ls is executed after you copy the build dir, when building the docker image, if you run ls when you start the container you will see that there is no server/index.js\nWhile you're copying the build folder to /usr/src/app/, you're overriding that files when mounting development folder on /usr/src/app.\nThe easiest way, is to mount C:\\portal\\environments\\development on /usr/src/app/development.\ndocker run -d -v C:\\portal\\environments\\development:/usr/src/app/development/ -p 3000:80 f057e4bc8191\nAlso you can mount development on /usr/src/app and then use the build/server that was build on the image:\n-v C:\\portal\\environments\\development:/usr/src/app/ -v /usr/src/app/server\nYou will have to do that for every folder that you need from the built image.\n-v /usr/src/app/server -v /usr/src/app/public -v ...",
    "Docker compose run tests against container": "There's different kinds of tests. I tend to think of unit tests as things that only depend on the code base, and ideally the smallest chunks of code within the system, and so you can and should run these on your development system before you build the container. These are tricky to run inside a container, since usually you wouldn't ship this kind of test code with the application, but there's a very tight code-level connection between the code and the tests.\nOn the other hand, integration tests or system tests tend to answer your final question, \"verify the running API is correct\", with real or at least test databases. For these you'd often have a separate test driver that contacted your service, made requests, and verified the results. Then you can do what you propose:\ndocker build -t myimage .\ndocker run -d --name test -p 12345:8080 myimage\n./integration_tests http://localhost:12345\ndocker stop test\ndocker rm test\ndocker push myimage\nIn this example the service listens on port 8080; we map it to port 12345 on the host; and integration_tests is your test runner that connects to the running service, and we point it at the container. You could wrap many of these steps up in a Docker Compose YAML file, especially if running the container needs a lot of arguments.",
    "Securing/Encrypting the sensitive environment variables": "For variables needed in built-time (image creation):\nARG: --build-arg\nFor env variables needed when container starts:\n--env-file: It lets you nobody can see your variables doing history inspecting your cli command.\nUse docker secrets: possible in swarm, docker enterprise. (docker swarm secrets)",
    "Through docker Image, how to list out only the dependencies specified in dockerfile and not the ones which are automatically installed?": "In your container shell, you should try to look for a log file mentioning the recent installed packages.\nSee \"Is it possible to get a list of most recently installed packages?\"\nFor instance:\ngrep \" install \" /var/log/dpkg.log",
    "Dockerfile: Persist symlink in RUN instruction": "For making files (in fact everything that is an inode) persistent you need to create a volume. In this special case (the /dev directory) it is most probably not possible because /dev is for system files.\nBut you probably know about the CMD command in a Dockerfile. This is the command that is executed to start your image. You could point to a shell script that will first create your link and then hand over execution to your code. This shell script has to be added to the image and needs to have the execute bit being set.\nLike this in your Dockerfile:\nADD start.sh /\nCMD /start.sh\nAnd in start.sh:\n#!/bin/sh\n\nln -s /dev/null /dev/raw1394\nexec /your/binary_or_whatever",
    "executable file not found in $PATH Dockerfile": "You misunderstood volumes in Docker I think. (see What is the purpose of VOLUME in Dockerfile)\nI'm citing @VonC answer:\nA volume is a persistent data stored in /var/lib/docker/volumes/...\nYou can either declare it in a Dockerfile, which means each time a container is stated from the image, the volume is created (empty), even if you don't have any -v option.\nYou can declare it on runtime docker run -v [host-dir:]container-dir. combining the two (VOLUME + docker run -v) means that you can mount the content of a host folder into your volume persisted by the container in /var/lib/docker/volumes/....\ndocker volume create creates a volume without having to define a Dockerfile and build an image and run a container. It is used to quickly allow other containers to mount said volume.\nSo you should use docker run -v /home/user/Documents/folder1/folder2/folder3/Projectname:/workdir1 when starting the container\nAnd your Dockerfile volume declaration should be:\nVOLUME /workdir1\nThat being said, you define both Entrypoint and CMD. What is the CMD being for ? You will never use your image without using runapp.sh ? I prefer using only CMD for development since you can still do docker run -it my_container bash for debugging purpose with this syntax.\nThis time I'm using @Daishi answer from What is the difference between CMD and ENTRYPOINT in a Dockerfile?\nThe ENTRYPOINT specifies a command that will always be executed when the container starts.\nThe CMD specifies arguments that will be fed to the ENTRYPOINT.\nIf you want to make an image dedicated to a specific command you will use ENTRYPOINT [\"/path/dedicated_command\"]\nOtherwise, if you want to make an image for general purpose, you can leave ENTRYPOINT unspecified and use CMD [\"/path/dedicated_command\"] as you will be able to override the setting by supplying arguments to docker run\nMoreover, runapp.sh isn't in your $PATH and you call it without absolute path, so it will not find the file even if the volume is mounted correctly. You could just use:\nCMD /workdir1/runapp.sh \"$NEO4J_CONFIG\" \"$BENCHMARK_NAME\"\nNow be careful, on your host you mention that the shell script is named script.sh and you call runapp.sh in your Dockerfile, I hope it's a typo. By the way your script needs to be executable.",
    "docker-compose running entrypoint script in another directory": "I think I have figured it out. You are using the build option so that the docker image can live inside the docker-compose project. This is a little non-standard, but perfectly OK. Using image is more standard.\nThe entrypoint is executed after the build, regardless of the position in the docker-compose.yml file. When we write the entrypoint, we are writing a reference to a file in the image not the local file system.\nYou should probably edit your .app/the/Dockerfile as follows add \"COPY docker-entrypoint.sh /\" add \"ENTRYPOINT [\"sh\", \"/docker-entrypoint.sh\"]\nThen you can edit your docker-compose.yml by removing the \"entrypoint: ./app/the/docker-entrypoint.sh\"\nOLD ANSWER\nWithout testing it out myself, I would assume that these would work:\nAs @B0rn2C0de suggest set the workdir appropriately.\nAlternatively, use a relative path to where the WORKDIR actually is\nAlternatively, use an absolute path.",
    "standard_init_linux.go:185: exec user process caused \"no such file or directory\" building docker image": "Your docker image starts from alpine linux dstro. Hence you should use a supported cpu architecture\nGOARCH=amd64 instead of GOARCH=386.\nYou can see other supported architectures here. https://hub.docker.com/r/library/alpine/\nLinks to the alpine docker images of other architectures can be found here https://github.com/docker-library/official-images#architectures-other-than-amd64",
    "Error running script inside Dockerfile": "The \\r in the error message suggests that your line endings are badly encoded.\nEnsure that end of line chars of SqlCmdStartup.sh are LF for Unix not CR LF (Windows) as it seem to be.\nMore details here : Difference between CR LF, LF and CR line break types?",
    "Docker - capturing logs when replacing PID 1": "I can't see anything wrong in what you are describing. The stdout/stderr of process ID 1 should be captured, as will any sub process if they inherit stdout/stderr of the parent process (process ID 1).\nWhere you can have problems is if an application is set up to log to a normal file and doesn't use stdout/stderr. In these cases if they will only accept a file, use /proc/1/fd/1 as the log file path. This will result in the log messages getting output through stdout of process ID 1.\nDo note that if your application uses a logging framework that wants to do its own log file rotation on the path you give it, you will need to disable that, you want it to keep using the same file path and not try and rename or truncate it.",
    "How to debug phoenix application which running with docker?": "use docker exec to attach to bash and run your custom iex session. for example:\ndocker exec -it NAME bash",
    "how to get the folder from one container to another container using docker compose": "this is the docker-compose.yml:-\nversion: '3.2'   \nservices:    \n  ant:   \n    build:        \n      context: \".\"   \n      dockerfile: dockerfile_ant  \n    container_name: ant-container  \n    links:  \n     - tomcat   \n    volumes:  \n      - ant:/usr/local/tomcat:rw     \n    ports:  \n      - \"8080:8080\"        \n  tomcat:   \n    build:    \n      context: \".\"  \n      dockerfile: dockerfile_tomcat   \n    container_name: tomcat-container   \n    restart: always   \n    volumes:   \n      - ant:/usr/local/tomcat:rw   \n    expose:   \n      - 80   \nvolumes:     \n  ant: {}  \nand no need to specify volume in ant-dockerfile and tomcat-dockerfile",
    "Run a bash script after the primary service in a docker container is fully loaded": "Checking the process doesn't seem a good approach for checking if elastic-search is ready.\nI suggest a better alternative using the REST api of elastic-search.\nThe api is exposed on port 9200. There is an official healthcheck script for elastic search. The health check uses the command curl -fsSL \"http://$host:9200/_cat/health?h=status which will return green if elastic-search is healthy.\nYou can use the same api and wait for a green status and then execute your init_sg.sh script.",
    "From where/how the files get populated in /var/www/html?": "In this case, the entrypoint is copying the files if they don't already exist. Note in the Dockerfile that the wordpress source is added to /usr/src/wordpress. Then, when the container starts, the entrypoint checks if some files exist and if they don't, it copies the wordpress source into the current directory, which is WORKDIR, which is /var/www/html.\nGeneral Docker Volume Stuff\nWith /var/www/html specified as a VOLUME, the only way to get files into there from the container's perspective is to attach a docker volume with files to that. Think of it as a mountpoint.\nYou can either attach a local filesystem to that volume:\ndocker run -v /path/to/local/webroot:/var/www/html wordpress\nor you can create a docker volume and use it for a persistent, more docker-esque object:\ndocker volume create webroot\nAnd then move the files into it with a transient container:\ndocker run --rm -v /path/to/local/webroot:/var/www/html \\\n           -v webroot:/var/www/html2 \\\n           ubuntu cp -a /var/www/html/ /var/www/html2\nat which point you have webroot as a docker volume you can attach to any container.\ndocker run -v webroot:/var/www/html wordpress",
    "RabbitMQ With With Docker Compose rabbitmq.config file gets replaced on Run": "Changed my docker file to this\nFROM rabbitmq:3.6.11-management\nCOPY rabbitmq.config /etc/rabbitmq/\nRUN chmod 777 /etc/rabbitmq/rabbitmq.config\nWORKDIR /var/lib/rabbitmq/\nRUN rabbitmq-plugins enable rabbitmq_management  --offline\nRUN rabbitmq-plugins list\nand my compose file to this\nversion: '2'\nservices:\n  rabbit:\n    build:\n      context: .\n      dockerfile: rabbit.dockerfile\n    container_name: rabbit\n    hostname: lightrabbit    \n    networks:\n      - rabnet\n    environment:\n      - RABBITMQ_DEFAULT_USER=lighttrading\n      - RABBITMQ_DEFAULT_PASS=P@ssw0rd_rabbit                 \n    ports:\n      - 15672:15672  \n      - 5672:5672    \n\nvolumes:\n  rabbitdata:\n    driver: local\nnetworks:\n    rabnet:\n        driver: bridge\nOne of the tricky parts is its a must to define this section inside our rabbitmq.config other wise running the command rabbitmq-plugins enable rabbitmq_management will not work\n{ rabbitmq_management, [\n        { listener, [\n            { port, 15672 },\n            { ssl, false }\n        ] },\n        { load_definitions, \"/etc/rabbitmq/definitions.json\" }\n    ] }",
    "Accessing links with docker compose": "The communication among containers takes place through their hostname. By default service name is considered the hostname, so you should use http://myexpress:3000 to connect to myexpress.\nAlso, you can specify hostname separately. More details here.",
    "Docker : Pull images from Local git repo / hard drive": "You need this resulting file somewhere. This example is for ubuntu:latest image. Use docker save:\ndocker save ubuntu:latest > /somewhere/ubuntu.latest.tar\nBut you can gzip it to reduce its size:\ndocker save ubuntu:latest | gzip > ubuntu.latest.tar.gz\nThen, having that file, with docker load you can:\n\u25b6 docker load < /somewhere/ubuntu.latest.tar.gz\nLoaded image: ubuntu:latest",
    "Docker error The command non-zero code: 1 python": "Seems issue with the location of the file try to give absolute path of the file that you are referring in dockerfile",
    "How can I reflect the data in one container to another?": "You need to setup an standard replication between mysql containers.\nYou cannot use something like volume to share mysql data at file level because the mysql's will mess the things up and corrupt data for sure.\nYou can make a replication as Master-Slave scheme or Master-Master (according to your needs).\nRefer to the docs to have detailed information.",
    "Insert data after mysql started in a docker container": "According to the docs, the MySQL entrypoint will automatically execute any files with .sh, .gz or .sql scripts found in /docker-entrypoint-initdb.d. So, create a script to execute your Python script for you. If you call this file 01-my-script.sh, your Dockerfile will look like this:\nFROM mysql:5.7\n\nEXPOSE 3306\nENV MYSQL_ROOT_PASSWORD 123456\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    python3 \\\n    python3-pip \n\n# Copy requirements in first, and run them (so cache won't be invalidated)\nCOPY ./requirements.txt ./requirements.txt\nRUN pip3 install --user -r requirements.txt\n\n# Copy SQL Fixture\nCOPY ./01-my-script.sh /docker-entrypoint-initdb.d/01-my-script.sh\nRUN chmod +x /docker-entrypoint-initdb.d/01-my-script.sh\n\n# Copy the rest of your project\nCOPY . .\nAnd your script will only contain:\n#!/bin/sh\n\npython3 /app/init.py\nNow, when you bring up your container, your script will execute. Monitor the execution of the running container with docker logs -f <container_name> to make sure your script is running.",
    "Starting Postgres in Docker Container": "In a Dockerfile you have\na configuration phase, the RUN directive (and some others)\nthe process(es) you start, that you put in either\nCMD\nor\nENTRYPOINT\nsee the docs\nhttps://docs.docker.com/engine/reference/builder/#cmd\nand\nhttps://docs.docker.com/engine/reference/builder/#entrypoint\nwhen a container has completed what it has to do in this start phase, it dies.\nThis is why the reference Dockerfile for PostgreSQL, at\nhttps://github.com/docker-library/postgres/blob/3d4e5e9f64124b72aa80f80e2635aff0545988c6/9.6/Dockerfile\nends with\nCMD [\"postgres\"]\nif you want to start several processes, see supervisord or such tool (s6, daemontools...)\nhttps://docs.docker.com/engine/admin/using_supervisord/",
    "Oh My Zsh install in docker fail": "No idea, but you don't have to use the one step install shorthand which might give you a better idea of where the command is failing.\nRUN set -uex; \\\n    wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh; \\\n    sh ./install.sh; \\\n    rm ./install.sh",
    "Dockers container image update": "Unfortunately if image was updated, you need to create new container based on new image. No another way.\nBut you can set static IP for container how explains in this question",
    "Docker-compose : volumes and user": "do not change anything in docker-compose.yml\n...\nvolumes:\n  - $HOME/web/IV3/code-front:/var/www/\n...\ncreate directory first & change ownership before creating the mount point in nginx-front/Dockerfile\n...\nRUN mkdir -p /var/www ; chown -R safe-user:safe-user /var/www\nVOLUME /var/www\n...",
    "Setting up our own private docker hub": "Adding below line in docker client machine's /etc/sysconfig/docker file resolved the issue:\nINSECURE_REGISTRY='--insecure-registry <ip>:5000'",
    "Docker - Override content of linked volume": "You can achieve what you want by changing the CMD used when starting the container, either in your Dockerfile, or in the docker-compose.yml file.\nInstead of just starting node ./src/app.js, you want to do two things:\nCopy the node_modules over.\nStart Node\nUsing the docker-compose.yml, I would do the following:\napp:\n  build: ./dockerfiles/app\n  volumes:\n    - /Users/home/work/app:/usr/app\ncommand: >\n  bash -c \"\n\n  rm -rf /usr/app/node_modules\n  && cp -R /tmp/node_modules /usr/app/node_modules\n  && node ./src/app.js\n  \"\nThis will delete the existing node modules on the mapped-in volume, then copy in the ones from container, and then finally starts the node app. This is going to happen every time the container is started.",
    "Lift Sails inside Docker container": "By using command: node app you are overriding the command CMD [\"/app/app.js\", \"--no-daemon\"] which as a consequence will have no effect. WORKDIR /app will create an app folder so you don't have to RUN mkdir /app. And most important you have to RUN cd /app; npm i before CMD [\"/app/app.js\", \"--no-daemon\"]. NPM dependencies have to be installed before you start your app.",
    "How to remove data from docker volume?": "If you want to change the files on rebuild, you probably don't want to do it in the volume. The volume is generally for data you want to persist. Remember the volume mounting will occur after the container builds, so what's probably happening is the volume with the old data is mounting over any changes you are making in the image (re)build.\nWhat are you using /myLibs for? If they are read-only files you want to set up in the build, you might be better off not using a volume and make them part of the image. If you want to modify them, it's probably better to manage that after the build - there is no real reason to rebuild the image if you are just changing files in a networked volume.",
    "Golang Dockerfile Failing": "When you build a binary, go build assumes that you are trying to build for your current computer, it chooses values for GOOS and GOARCH (described here) for you.\nIf you are not building on a linux machine then you will need to cross compile the binary for linux, as this is what the OS inside the docker container will be running. Explanation here\nYou need something like:\nGOOS=linux\n\nbuild:\n    $(GOGET); GOOS=$(GOOS) $(GOBUILD) -v -o engine",
    "Generic Docker Image and Dockerfile for SpringBoot Apps using Gradle/Maven": "Backtracking from the Dockerfile, we could just require to add \"app.jar\". So, from\nADD gs-spring-boot-docker-0.1.0.jar app.jar\nto\nADD app.jar app.jar\nThis leads to the need of renaming or copying the generated executable Jar. This example renames the executable jar to \"app.jar\", and so, making it easy for building the docker image. A generic task that can be copied to any SpringBoot app to be built in Gradle can be found below.\nbuild.gradle\n/**\n * Generic support for building docker images for SpringBoot Apps\n */\ntask buildDocker(type: Docker, dependsOn: build) {\n  push = false\n  applicationName = rootProject.name\n  dockerfile = file('src/main/docker/Dockerfile')\n\n  doFirst {\n    // Rename the app jar to \"app.jar\" so that the Dockerfile does not require renames\n    copy {\n      from \"${project.buildDir}/libs\"\n      into stageDir\n      include \"${rootProject.name}-${version}.jar\"\n      rename(\"${rootProject.name}-${version}.jar\", \"app.jar\")\n    }\n  }\n\n  doLast {\n    println \"Run the Docker Container\"\n    println \"docker run -ti -p 8080:8080 $project.group/$applicationName:$version\"\n  }\n} \nThe final resulting Dockerfile is as follows:\nsrc/main/docker/Dockerfile\nFROM frolvlad/alpine-oraclejdk8:slim\nMAINTAINER Marcello_deSales@intuit.com\nVOLUME /tmp\nADD app.jar app.jar\nRUN sh -c 'touch /app.jar'\nENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/app.jar\"]\nThe command \"gradle buildDocker\" will generate docker images and as bonus, will print the complete command for you to execute the app (note that the default port number is hard-coded and must be changed if you change that value).",
    "Docker how to start nodejs app with redis in the Container?": "A docker file can have but one entry point(either CMD or ENTRYPOINT, not both). But, you can run multiple processes in a single docker image using a process manager like systemd. There are countless recipes for doing this all over the internet. You might use this docker image as a base:\nhttps://github.com/million12/docker-centos-supervisor\nHowever, I don't see why you wouldn't use docker compose to spin up a separate redis container, just like you seem to want to do with mysql. BTW where is the mysql definition in the docker-compose file you posted?\nHere's an example of a compose file I use to build a node image in the current directory and spin up redis as well.\nweb:\n  build: .\n  ports:\n    - \"3000:3000\"\n    - \"8001:8001\"\n  environment:\n    NODE_ENV: production\n    REDIS_HOST: redis://db:6379\n  links:\n    - \"db\"\ndb:\n  image: docker.io/redis:2.8\nIt should work with a docker file looking like the one you have minus trying to start up redis.",
    "How can I configure environment variables on a Docker container without hard-coding them in the Dockerfile? [closed]": "Apparently the \"pseudo-code\" I came up to illustrate what I wanted actually worked for docker ...Thanks Collin",
    "Multiple dockerfiles in one docker or multiple images from one dockerfile": "These are two different dockerfiles. One for the database. And the other with the webserver.",
    "Is there any reason to favour concatenated RUN directives over RUNning a script?": "In contrast to the other answer, I generally prefer:\nRUN a && \\\n    b && \\ \n    c\nThe main reason being that it is immediately clear what is happening. If you instead use a script, you've effectively hidden the code. For a new user to understand what's happening, they now need to find the project with the build context before they can look into your script.\nIt is a trade-off and once things get too complex, you should refactor into a script. However, you might prefer to curl the script from a known location rather than COPY it, so that the Dockerfile remains standalone.",
    "Docker log in command does not work": "Update February 2016\nPR 19891 \"Enable cross-platforms login to Registry\" is supposed to fixed the issue\nUse a daemon-defined Registry URL for docker login.\nThis allows a Windows client interacting with a Linux daemon to properly use the default Registry endpoint instead of the Windows specific one.\nIt is in commit 19eaa71 (maybe for docker 1.10?)\ntyagian reports the following solution in issue 18019:\nOpen the config json file: C:\\Users\\Username\\.docker\\config.json\nIt will look like this:\n{\n  \"auths\": {\n    \"https://index.docker.io/v1/\": {\n      \"auth\": \"<hash value>\",\n      \"email\": \"<email-address>\"\n    }\n  }\n}\nChange it to:\n{\n  \"auths\": {\n    \"https://index.docker.io/v1/\": {\n      \"auth\": \"<hash value>\",\n      \"email\": \"<email-d>\"\n    },\n    \"https://registry-win-tp3.docker.io/v1/\": {\n      \"auth\": \"<hash value>\",\n      \"email\": \"<email-address>\"\n    }\n  }\n}\nand then try again:\n$ docker push username/image-name",
    "Dockerfile VOLUME definition to mount project tree": "This doesn't work:\nVOLUME .:/hello\nYou can't specify the host directory for a volume in a Dockerfile. The reason is that it would be non-portable.\nEither copy the files into the image with COPY or just mount the volume at run-time as you have been doing.",
    "Storing different Docker Images in single Docker repository": "Yes. The tags are of the form [url]/[user]/image[:tag]. If you can authenticate as 'user' at 'url' you can store any number of repos there with any number of tags.\nIn the usual case, your 'code' and 'software' tags will be related in some way, say by having a common ancestor, but there's nothing that enforces that rule.",
    "Creating a Dockerfile - docker starts from scratch on each new build": "As demas said, if you're simply appending lines, the previous lines will be cached.\nHowever, if anywhere in your Dockerfile you have a line like\nADD . /some/path\nthen Docker will assume that that line has changed even if it was only the Dockerfile that changed. So that line and anything after it will never be cached, unless nothing in the folder you're adding has changed.\nYou should be able to see whether this is happening by paying close attention to the output of the docker build command.\nAs a side note: a consequence of this is that if you're building a Dockerfile, you generally want to add the files in the directory as late as possible, doing any preparations beforehand. Of course, you will end up having to do things to your files (like some kind of build process) which is unfortunately hard to cache.",
    "Mismatching @next/swc version, detected: 14.0.3 while Next.js is on 13.5.3": "There appears to be a bug with NextJS builds in Docker. During npm install the SWC binaries are supposed to be installed with the same version of next defined by package.json, yet it seems that mismatching versions are sometimes installed. More specifically, the latest version of SWC is pulled with a potentially older version of next, which causes the build to fail.\nWe found two way to fix it:\nUpgrade next to the latest version. This is not ideal because new versions come out relatively quickly, so you must make sure you're always on the latest versions. In our case this was not an option because the latest versions of the past couple of days have introduced other unrelated glitches that are too severe to disregard.\nLock the next version. This will force both next and SWC to be of the same version. So, for example, change:\n \"next\": \"^14.2.5\",\nTo:\n \"next\": \"14.2.5\",\nRemoving the ^ means that your next version won't automatically get the latest patches, but it might be preferable to being unable to containerize your app. Try to re-introduce ^ every once in a while to see if the bug has been fixed.",
    "How to run Nuxt3 with docker (docker compose)": "The issue you're experiencing is related to the way volumes are managed in Docker Compose. When you use multiple Compose files with the -f flag, the order in which you specify them matters. The settings in the later Compose files will override those in the earlier ones. However, volumes defined in the docker-compose.yaml file are not overwritten by default when you use additional Compose files. Instead, volumes defined in multiple Compose files are merged.\nIn your case, you have volumes defined in both docker-compose.yaml and production.yaml. Since you're using -f docker-compose.yaml -f production.yaml, the volumes from both files are being merged. This is why you're seeing the issue where the volume from docker-compose.yaml is not being overridden by the empty volume definition in production.yaml. By removing the volumes section from production.yaml, you ensure that the volume defined in docker-compose.yaml is used for development, and it won't be overridden",
    "How to use prometheus to monitor all containers in a docker-compose [closed]": "It can be solved by using cadvisor+prometheus. I follow the tutorial here and deploy it successfully.\nIn detail, I have prometheus.yml and docker-compose.yml as following:\n#prometheus.yml\nglobal:\n  scrape_interval:     5s \n  evaluation_interval: 5s \n\n  external_labels:\n      monitor: 'codelab-monitor'\n\n\nrule_files:\n  # - \"first.rules\"\n  # - \"second.rules\"\n\nscrape_configs:\n  - job_name: 'prometheus'\n\n    static_configs:\n      - targets: ['host.docker.internal:9092']\n\n  - job_name: 'docker'\n\n    static_configs:\n      - targets: ['host.docker.internal:9323']\n\n  - job_name: 'cadvisor'\n\n    static_configs:\n      - targets: ['host.docker.internal:8082']\n#docker-compose.yml\nversion: '2'\nservices:\n  cadvisor:\n    image: gcr.io/cadvisor/cadvisor\n    container_name: cadvisor\n    volumes:\n          - /:/rootfs:ro\n          - /var/run:/var/run:ro\n          - /sys:/sys:ro\n          - /var/lib/docker/:/var/lib/docker:ro\n          - /dev/disk:/dev/disk/:ro\n    ports:\n        - '8082:8080'\n\n  prometheus:\n    image: prom/prometheus\n    container_name: prometheus\n    user: root\n    volumes:\n          - ./prometheus.yml:/etc/prometheus/prometheus.yml\n    command:\n          - '--config.file=/etc/prometheus/prometheus.yml'\n    ports:\n        - '9092:9090'\nNote that If you are using Docker Desktop, you need to add \"metrics-addr\": \"127.0.0.1:9323\", in Settings->Docker Engine first to enable docker metrics exporting. Then you can visit localhost:9092 to query metrics for each container.",
    "How to execute npm in a Dockerfile for a dotnet project": "The npm command is looking for node and it does not find it (ie. its location is not in PATH).\nWhy don't you start from one of the image (say aspnet) and use apt-get to install the other packages (say node), you'll get a proper setup with the required environment variables.",
    "Golang chromedp dockerfile": "The Chrome browser is installed in the build-stage only. It's not available in the final image that is created by the build-release-stage.\nI tried to install Chrome with this Dockerfile:\n# Deploy the application binary into a lean image\nFROM gcr.io/distroless/base-debian11 AS build-release-stage\n\nRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \\\n    && echo \"deb http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list\nRUN apt-get update && apt-get -y install google-chrome-stable\nRUN chrome &\nBut it failed with this message:\n...\nStep 2/4 : RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -     && echo \"deb http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list\n ---> Running in 7596202a5684\nfailed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/bin/sh\": stat /bin/sh: no such file or directory: unknown\nI think you have to choose another base image on which it is easy to install Chrome. A better alternative is to use chromedp/headless-shell as the base image. This image contains Chrome's headless shell, which is very small. The demo Dockerfile below also shows compiling the test binary first and then run the test in the chromedp/headless-shell image:\nFROM golang:1.20.5-buster AS build-stage\n\nWORKDIR /app\n\nCOPY go.mod go.sum ./\nRUN go mod download\n\nCOPY . .\n\nRUN CGO_ENABLED=0 go build -o dockergo\n# Build the test binary\nRUN CGO_ENABLED=0 go test -c -o dockergo.test\n\n# Run the tests in the container\nFROM chromedp/headless-shell:114.0.5735.199 AS run-test-stage\n\nWORKDIR /app\n# Copy other files that is needed to run the test (testdata?).\nCOPY . .\nCOPY --from=build-stage /app/dockergo.test ./dockergo.test\nRUN /app/dockergo.test -test.v\n\n# Deploy the application binary into a lean image\nFROM chromedp/headless-shell:114.0.5735.199 AS build-release-stage\n\nCOPY --from=build-stage /app/dockergo /dockergo\n\nEXPOSE 8080\n\nENTRYPOINT [\"/dockergo\"]",
    "Code changes for springboot application doesn't reflected on Docker": "Thank you for the help from @DavidMaze. The problem is fixed.\nReason why problem occur:\nThe docker run command runs a command in a new container, pulling the image if needed and starting the container. From Docker doc\nIn the original Dockerfile, RUN ./mvnw package -Dmaven.test.skip=true build the jar file inside the container. COPY target/*.jar springboot-flash-cards-docker.jar will copy the jar file \"target/*.jar\" from the host machine to the container. And what I expected is to move and rename the jar built inside the docker container. Since I didn't package code on my host after code changes, the code changes is not reflected on docker.\nHow to fixed it:\nUse RUN mv target/*.jar springboot-flash-cards-docker.jar instead of COPY target/*.jar springboot-flash-cards-docker.jar. This command will move and rename the \"target/*.jar\" inside the container to \"springboot-flash-cards-docker.jar\".\nThe fixed docker file:\nFROM openjdk:17 as buildstage\nWORKDIR /app\nCOPY mvnw .\nCOPY .mvn .mvn\nCOPY pom.xml .\nRUN ./mvnw dependency:go-offline\nCOPY src src\nRUN ./mvnw package -Dmaven.test.skip=true\nRUN mv target/*.jar springboot-flash-cards-docker.jar\n\nFROM openjdk:17\nCOPY --from=buildstage /app/springboot-flash-cards-docker.jar .\nENTRYPOINT [\"java\", \"-jar\", \"springboot-flash-cards-docker.jar\"]",
    "Docker devcontainer with oh-my-zsh and Powerlevel10k": "So the way I got it to work was to host my specific powerlevel10k file on github and just downloading it during the Dockerfile creation and adding a line to the .zshrc to source the .p10k.zsh file like so:\n# Install oh-my-zsh with customized theme\nRUN sh -c \"$(wget -O- https://github.com/deluan/zsh-in-docker/releases/download/v1.1.5/zsh-in-docker.sh)\" -- \\\n    -p git\nRUN echo '[[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh' >> .zshrc\nRUN wget https://raw.githubusercontent.com/chbroecker/dotfiles/main/zsh/.p10k.zsh -O .p10k.zsh",
    "Unable to load service index for source when build the docker image": "As you confirmed, this issue needs you to pass credentials like PAT token into the dockerfile, otherwise it will be impossible to access the feed.\nBy the way, this type of issue always have secondary error output, you can use fiddler to capture the network trace. In this way, you will get the actual cause of the issue.",
    "docker build fails inside gitlab-runner but works locally : spring boot native compilation with GraalVm": "Is your GitLab runner configured to use a non-root user when executing the Dockerfile?\nAs @jilliss pointed out, it seems that it's the Java binary that needs execute permission, but maybe only root has the permission (which is why it works locally as by default you will be running it as root).\nIf the Ops team have tried to run the Dockerfile as another user, then it could explain why /opt/graalvm-ce-java17-22.3.1/bin/java is no longer executable.\nTry adding a whoami log and see which user is running when it runs in GL.",
    "installing powershell in aspnet:6.0 , sdk:6.0 images": "it helped for me\ndotnet tool install --global PowerShell",
    "When running Docker, it returns an error when installing PHP8.1": "I have the same problem since August 2, looks like something wrong with ondrej/php repository. First, I tried to install systemd as @matiaslauriti suggested, but the following error has occured:\nThe following packages have unmet dependencies:\n[07:33:33]#24 7.480  systemd : Depends: libsystemd0 (= 237-3ubuntu10.53) but 237-3ubuntu10.54 is to be installed\nThat problem has been fixed by adding the following to sources.list:\nRUN echo \"deb http://archive.ubuntu.com/ubuntu/ bionic-proposed main\" >> /etc/apt/sources.list\nYou don't need install systemd explicitly, it will be installed as one of php dependency.",
    "Docker on RHEL 8 creating files and folder with 027 permission": "when docker container starts\nThat means you need to build your own image, based on nginxinc/nginx-unprivileged:stable-alpine, with a new entry point like:\n#!/bin/sh\n# entrypoint.sh\numask 022\n# ... other first-time setup ...\nexec \"$@\"\nSee \"Change umask in docker containers\" for more details, but the idea remains the same.",
    "Containerd port mapping from host to container": "You cannot map from container to outside work using the ctr command. Instead, use nerdctl (https://github.com/containerd/nerdctl) as it has the same docker CLI functionality.",
    "Simple docker-compose.yml to setup an Ubuntu container with ssh access and persistent files": "It would be best if you use persistence. That can be achieved by using volumes - some examples of this can be found:\n(this assumes you are staying on the server/computer in question) https://docs.docker.com/engine/reference/builder/#volume\nIf you are not interested in the official documentation, I feel like this gist is very concise: https://gist.github.com/onlyphantom/0bffc5dcc25a756e247cb526c01072c0\nI would start by checking that the volume information you posted above is written in the way you expect.\nI took your configuration above and ran it on my Mac OS Montery and Docker Desktop 4.16 and made the following changes, resulting in persistence.\nI made a single change here (note that you need to think through each service, \"thing\" that you want to keep around when it comes back online.) This doesn't fix and persist all changes only those that are connected and in the SSH folder. Hopefully that makes sense.\nvolumes:\n  - sshd:/etc/ssh/\nports:\n  - \"22:22\"\nnetwork_mode: bridge \nvolumes:   \nsshd:\nIf you are not wanting to stay on the same system and want to move the container, say to another computer/server/friend whatever you will probably want to actually commit the changes to the container and push them which can be done with the following commands:\nYou need to commit your changes to the container and then run it. (the answer below is not mine, but does a great job of explaining: I lose my data when the container exits)\nTry this:\nsudo docker pull ubuntu\n\nsudo docker run ubuntu apt-get install -y ping\nThen get the container id using this command:\nsudo docker ps -l\nCommit changes to the container:\nsudo docker commit <container_id> iman/ping \nThen run the container:\nsudo docker run iman/ping ping www.google.com\nThis should work.",
    "Specific file in bind-mount directory does not update in Docker container when editing on host?": "You should remove COPY ./app/graphql/src . directive from your Dockerfile because this folder will mounted to container as volume.",
    "How do I remediate a critical vulnerability in my Docker image?": "I see that libcurl is pulled in by apk add git (click \"depends\"): https://pkgs.alpinelinux.org/package/edge/main/x86/git\nBut on alpine 3.12 the libcurl version is 7.79.1 which is not affected by the CVE: https://nvd.nist.gov/vuln/detail/CVE-2021-22945\nMaybe run apk update before apk add and see if it pulls in the right version?",
    "Authentication token manipulation error when trying to change use password in dockerfile": "working for me now:\nuse a single RUN command for less docker images.\nRUN groupadd --system ${UNAME} --gid ${UID}\nRUN useradd --uid ${UID} --system --gid ${UNAME} --home-dir /home/${UNAME} --create-home --comment \"Docker image user\" ${UNAME}\nRUN chown -R ${UNAME}:${UNAME} /home/${UNAME}\nRUN usermod -aG sudo ${UNAME}\nRUN echo \"${UNAME}  ALL=(ALL) ALL\" | sudo tee /etc/sudoers.d/permissions\nRUN echo ${UNAME}:password | chpasswd\nNotice the added line RUN echo \"${UNAME}  ALL=(ALL) ALL\" | sudo tee /etc/sudoers.d/permissions",
    "Airflow set python library versions to improve build speed": "I think the various google packages you're seeing are dependencies of apache-airflow[gcp].\nTo speed up the install, the documentation recommends you use one of the constraint files they provide. They create tags named constraints-<version> that contain files you can pass to pip with --constraint.\nFor example, when trying to install 2.2.0, there is a constraints-2.2.0 tag. In this tag's file tree, you'll see files like constraints-3.8.txt, where 3.8 is the python version I'm using.\npip install apache-airflow[gcp]==2.2.0 --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.2.0/constraints-3.8.txt\"",
    "Dockerfile how to make start command different between dev and prod?": "In kubernetes you can overwrite the default command args:\napiVersion: apps/v1\nkind: Deployment\n[...]\nspec:\n  template:\n    spec:\n      containers:\n      - name: CONTAINER-NAME\n        image: IMAGE-NAME\n        args: [\n          \"npm\",\n          \"start\" ]\nSee the kubernetes documentation.\nThe detailed implementation depend on the deployment system you're using:\nYou can write two different .yaml files, one for the development and one for the production environment.\nIf you're deploying with helm, you can set this configuration in a value file per environment.\nYou can also use Kustomize as described in this example.",
    "Why does RUN in dockerfile not process npx , node commands": "Running the command in array format worked for me,\nTry this:\nRUN [\"npx\",\"- tsc\",\"--skipLibCheck\",\"/usr/src/app/src/gql/generate-typings.ts\"]\nInsetad of:\nRUN npx - tsc --skipLibCheck /usr/src/app/src/gql/generate-typings.ts",
    "Environment variables from .env and/or docker-compose not replaced in Dockerfile CMD": "The .env file and the environment directive in your docker-compose file are Docker Compose concepts applying at runtime, not applying to image build. Your configuration would inject those environment variables to the built, running container, but it won't inform the build environment.\n(Edit: Upon reflection, this may be what you actually want.)\nYou could fully leverage the ARG instruction in your Dockerfile by changing your docker-compose to say:\nversion: \"3\"\nservices:\n  discord-initiative-savageworlds:\n    build:\n      context: .\n      args:\n        - TOKEN=${TOKEN}\n        - DECK=${DECK}\nThis will effectively set your 'environment' at build time, and will enable your Dockerfile to function as pasted above.\nEdit:\nI suspected there might be things in the redacted portion of the Dockerfile that could throw wrenches. The TL;DR from our comment thread is that your Dockerfile would need to change, too:\nFROM ubuntu:latest\n...Run Installers and Stuff...\nCOPY . .\nRUN dotnet publish\nRUN cd DiscordInitiative/bin/Debug/net5.0/publish\nARG TOKEN\nARG DECK\nRUN echo \"${TOKEN}\"\nCMD ./DiscordInitiative --token=\"${TOKEN}\" --deck=\"${DECK}\"\nMoving those ARG instructions as near to the bottom as possible sets them from your docker-compose.yml after a bunch of other RUN instructions, and similar things that have the potential to blow away the intermediate container state.\nEdit 2:\nIn reality, though, what you're asking for is variables that are available in the container at runtime (when CMD is executing). So, your question would lead me to believe that you don't need the ARGs at all, and that your container would function as expected.\nAlso, if you eliminate the ARGs you can get rid of the RUN echo ..., as that would never return anything at that point in the build process.\nInjecting ARGs is totally not what you want to do with sensitive values, as the arguments then become a part of your image layers, and subject to snooping. The pattern of injecting sensitive values to the environment at runtime is much preferred (though I wouldn't say most preferred).",
    "How to run command on container startup, and keep container running after command is done?": "An old Docker (v2) tricks to prevent premature container closing consisted in letting run an \"infinite\" loop command in it, such as:\nCMD tail -f /dev/null",
    "How to run a python script which is on the host machine using docker file": "See docker build --help:\nUsage:  docker build [OPTIONS] PATH | URL | -\\\nOptions:\n  -f, --file string             Name of the Dockerfile (Default is 'PATH/Dockerfile')\nThe PATH here is build context which usually you will specify as ., means current dictory.\nUsually you won't specify -f, means it will use Dockerfile in $PATH(Usually . as current folder)\nWhen docker build, docker will compress all items in build context, then pass it to build daemon, all files outside of build context can't be accessed by Dockerfile.\nSo, for your scenario, you have to change build context to the folder which has py script in it, or the parent folder. Then, specify Dockerfile explicitly.\nA minimal example as next for your reference.\nFolder structure:\ncake@cake:~/20210904$ tree\n.\n\u251c\u2500\u2500 docker\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 run.py\n\n1 directory, 2 files\nrun.py:\nprint(\"hello\")\ndocker/Dockerfile:\nFROM python:3\nCOPY run.py /\nRUN python run.py\nExecution:\ncake@cake:~/20210904/docker$ docker build -t abc:1 -f Dockerfile ..\nSending build context to Docker daemon  3.584kB\nStep 1/3 : FROM python:3\n ---> da24d18bf4bf\nStep 2/3 : COPY run.py /\n ---> 6a4c4a76a7fb\nStep 3/3 : RUN python run.py\n ---> Running in 4b736175e410\nhello\nRemoving intermediate container 4b736175e410\n ---> 951dd3de7299\nSuccessfully built 951dd3de7299\nSuccessfully tagged abc:1\nExpalination:\nAbove change build context PATH to .. to have ability to access run.py\nNow, the Dockerfile no longer the default PATH/Dockerfile, so we should change it to Dockerfile which means it in current folder.\nCOPY run.py / implict means copy the file from PATH/run.py.",
    "How to set locale in postgres using docker-compose?": "Imho: You can only use the tags \"image\" or \"build\" in the compose.yml If both are present, the image is ranked higher. Hence your build section is ignored.\nAlso your compose has a few double entries and a logical error on the data folder.\nTo combine compose.yml and a Dockerfile here is an example:\ncompose.yml\nservices:\n\n  postgres:\n    container_name: 'test_pg_db'\n    build: \n      context: .\n      dockerfile: ./Dockerfile\n    # image: 'postgres:latest'  # as we build the image, this line is actually obsolete\n    restart: always\n    ports:\n      # depending on the access you can skip the port binding\n      - 5432:5432\n\n    # set shared memory limit when using docker-compose (standard is 64MB) - no clue why the docker tutorial recommends this\n    shm_size: '128mb'\n \n    environment:\n      POSTGRES_USER: test_admin # The PostgreSQL user (useful to connect to the database)\n      POSTGRES_PASSWORD: MyVerySecretPassword  # The PostgreSQL password (useful to connect to the database)\n      POSTGRES_DB: test_db   # The PostgreSQL default database (automatically created at first launch)\n              \n    volumes:\n    # Move the data from the container to a save location (otherwise deleting the container will delete all data)\n    \n    # Use this entry to have docker taking care of the path of the volume on the host \n    # uncomment the two lines at the volume section as well!\n    # - pgdata:/var/lib/postgresql/data \n \n    # Use this entry to use a specific folder on the host\n      - /docker_data/postgres_db:/var/lib/postgresql/data\n\n    healthcheck:\n      # had to add the user and database for no FATAL errors in the logs (role root does not exist9\n      test: [\"CMD-SHELL\", \"pg_isready -U test_admin -d test_db\"]\n      interval: 1s\n      timeout: 5s\n      retries: 10\n\n# uncomment if you want docker to handle the volume for the data\n# volumes:\n  # pg-data:\nhere is how the Dockerfile should look like (stored in the same diretory as the compose.yml)\nFROM postgres:latest\nRUN localedef -i de_DE -c -f UTF-8 -A /usr/share/locale/locale.alias de_DE.UTF-8\nENV LANG=de_DE.utf8",
    "Unable to find the template docker-compose.vs.debug.yml": "Your docker-compose.vs.debug.yml should be placed in the same folder where you have your docker-compose.yml file. You can check here\nYou might also want to look at the section here to see how you can structure the file and the entry point to attach the debugger.",
    "MS dotnet core container images failed to pull, Error: CTC1014": "so as @Chris Culter mentioned in a comment above, I just restarted my machine and it works again.\nIt is kind of strange because I already updated my Docker Desktop, restarted, and cleaned/ purged the docker data. None of those helped, just after restarting my windows it works again!",
    "How to install Node.js with yarn package manager in Dockerfile": "Top-Master, thank's a lot for your input. It has helped me a lot to find my solution.\nHere it is...\nI have used the Dockerfile you posted above and made some small changes:\nENV NODE_VERSION=16.5\nENV NVM_DIR=/root/.nvm\nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash\nRUN . \"$NVM_DIR/nvm.sh\" \\\n    && nvm install ${NODE_VERSION}\nRUN . \"$NVM_DIR/nvm.sh\" \\\n    && nvm use v${NODE_VERSION}\nRUN . \"$NVM_DIR/nvm.sh\" \\\n    && nvm alias default v${NODE_VERSION}\nENV PATH=\"/root/.nvm/versions/node/v${NODE_VERSION}/bin/:${PATH}\"\n\nRUN npm install -g yarn\nThe problem was, the RUN npm install -g yarn command run into an error and the build fails. After some googling around I've found this webpage https://docs.npmjs.com/downloading-and-installing-node-js-and-npm.\nThey recommend, that npm should be installed separately because of the directory structure and user rights. They are not the same if you install npm with Node.js together.\nSo I've done this:\nENV NODE_VERSION=16.5\nENV NVM_DIR=/root/.nvm\nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash\nRUN . \"$NVM_DIR/nvm.sh\" \\\n    && nvm install ${NODE_VERSION}\nRUN . \"$NVM_DIR/nvm.sh\" \\\n    && nvm use v${NODE_VERSION}\nRUN . \"$NVM_DIR/nvm.sh\" \\\n    && nvm alias default v${NODE_VERSION}\nENV PATH=\"/root/.nvm/versions/node/v${NODE_VERSION}/bin/:${PATH}\"\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n        npm\nRUN npm install -g yarn\nFor me (and I hope for many other people too) it works now :)\nThank's again!\nHave a nice time!",
    "Can I run scripts from a docker build context without a copy?": "It's a long shot as Windows is a tricky thing with file system, but you could do this way:\nIn your Dockerfile use a COPY command, install then RUN del ... to remove the installation files\nBuild your image docker build -t my-large-image:latest .\nRun your image docker run --name my-large-container my-large-image:latest\nStop the container\nExport your container filesystem docker export my-large-container > my-large-container.tar\nImport the filesystem to a new image cat my-large-container.tar | docker import - my-small-image\nCaveat is you need to run the container once which might not be what you want. And also I haven't tested with windows container, sorry.",
    "How to make docker wait for SQL script inside /docker-entrypoint-initdb.d to be executed": "Solution is to utilise HEALTHCHECK [OPTIONS] CMD command \nThe command after the CMD keyword can be either a shell command (e.g. HEALTHCHECK CMD /bin/check-running) or an exec array (as with other Dockerfile commands; see e.g. ENTRYPOINT for details).\nThe command\u2019s exit status indicates the health status of the container. The possible values are:\n0: success - the container is healthy and ready for use\n1: unhealthy - the container is not working correctly\n2: reserved - do not use this exit code\nProbably you can create some shell script\n#!/bin/sh\n\n# if your last table exists we assume may be \n# we are now ready for migration \n# put whatever logic you have to ensure data import was successful\n\n# if file exits then exit code 0 else exit code 1\n[ -f  \"/path/to/my/last/table\" ] && exit 0 || exit 1\nIn docker-compose.yml you need\ndepends_on:\n      database:\n           condition: service_healthy",
    "Docker build fails with 'secret pip not found: not found' error": "This is using the relatively new --secret option which allows you to mount secrets at build time\nThe general way you utilize it is you have a secret file outside and assign it an id\nin your case, you'd have a pip.conf file somewhere and specify it in your build command:\ndocker build --secret id=pip,src=pip.conf -t robottests .\nthis will make the pip.conf available during the build, but not part of your image (presumably because it contains authentication secrets for accessing your internal pypi)",
    "Docker Build COPY Pipfile Pipfile.lock Cache Key Error": "Your Dockerfile has to be inside your hello dir\nI got the same problem because i have my docker file inside my Desktop so i moved my Dockerfile to my hello Dir(ie, your project dir) and it works!",
    "How to use docker buildx bake to build docker compose containers for both linux/armv7 and linux/amd64": "you can supply platform parameter under key xbake as mentioned below. (reference document: https://docs.docker.com/engine/reference/commandline/buildx_bake/)\n# docker-compose.yml\nservices:\n  addon:\n    image: ct-addon:bar\n    build:\n      context: .\n      dockerfile: ./Dockerfile\n      args:\n        CT_ECR: foo\n        CT_TAG: bar\n      x-bake:\n        tags:\n          - ct-addon:foo\n          - ct-addon:alp\n        platforms:\n          - linux/amd64\n          - linux/arm64\n        cache-from:\n          - user/app:cache\n          - type=local,src=path/to/cache\n        cache-to: type=local,dest=path/to/cache\n        pull: true\n\n  aws:\n    image: ct-fake-aws:bar\n    build:\n      dockerfile: ./aws.Dockerfile\n      args:\n        CT_ECR: foo\n        CT_TAG: bar\n      x-bake:\n        secret:\n          - id=mysecret,src=./secret\n          - id=mysecret2,src=./secret2\n        platforms: linux/arm64\n        output: type=docker\n        no-cache: true",
    "Docker - Module not found: Can't resolve 'react-plotly.js'": "I faced the same issue but below steps helped me solve the issue.\nWhile adding a new package to our React project and running it with docker-compose following these steps:\nStop any docker-composeif running, with docker-compose down -v\nNow add your npm module in your react application, npm install  react-plotly.js (in your case)\ndocker-compose up -d --build\nAfter looking at your docker file it looks fine to me, so I think it's the way you're installing the package is causing the issue.",
    "Gradle cannot build JAR with dependecies for Docker": "You are using Spring Boot in your application and in your build you are trying very hard to not use it. In short don't, use the Spring Boot Gradle plugin to build a proper jar\nplugins {\n    id 'org.springframework.boot' version '2.3.6.RELEASE'\n    id 'io.spring.dependency-management' version '1.0.10.RELEASE'\n    id 'java'\n}\n\ngroup = 'com.example'\nversion = '0.0.1-SNAPSHOT'\n\nsourceCompatibility = '1.8'\n\nrepositories {\n    mavenCentral()\n}\n\ncompileJava.options.encoding = 'UTF-8'\ncompileTestJava.options.encoding = 'UTF-8'\n\ndependencies {\n//    extraLibs group: 'net.java.dev.jna', name: 'jna-platform', version: '4.2.2'\n    // https://mvnrepository.com/artifact/org.apache.commons/commons-lang3\n    implementation group: 'org.apache.commons', name: 'commons-lang3', version: \n'3.11'\n\n    implementation 'org.springframework.boot:spring-boot-starter-web'\n    implementation 'org.springframework.boot:spring-boot-starter-validation'\n    implementation 'org.springframework.boot:spring-boot-starter-thymeleaf'\n    implementation 'org.springframework.boot:spring-boot-starter-data-jpa'\n    implementation 'org.springframework.boot:spring-boot-starter-security'\n\n    testImplementation group: 'org.springframework.boot:spring-boot-starter-test'\n\n    // Swagger UI\n    implementation group: 'io.springfox', name: 'springfox-swagger-ui', version: '2.9.2'\n    // Swagger 2\n    implementation group: 'io.springfox', name: 'springfox-swagger2', version: '2.9.2'\n    // https://mvnrepository.com/artifact/org.postgresql/postgresql\n    implementation group: 'org.postgresql', name: 'postgresql'\n    // https://mvnrepository.com/artifact/org.flywaydb/flyway-core\n    implementation group: 'org.flywaydb', name: 'flyway-core'\n    // MapStruct\n    implementation 'org.mapstruct:mapstruct:1.3.1.Final'\n    annotationProcessor 'org.mapstruct:mapstruct-processor:1.3.1.Final'\n    // https://mvnrepository.com/artifact/org.projectlombok/lombok\n    compileOnly 'org.projectlombok:lombok:1.18.12'\n    annotationProcessor 'org.projectlombok:lombok:1.18.12'\n    // https://mvnrepository.com/artifact/org.hibernate.orm/hibernate-jpamodelgen\n    annotationProcessor('org.hibernate:hibernate-jpamodelgen:6.0.0.Alpha5')\n    // https://mvnrepository.com/artifact/io.jsonwebtoken/jjwt\n    compile group: 'io.jsonwebtoken', name: 'jjwt', version: '0.9.1'\n    // https://mvnrepository.com/artifact/javax.xml.bind/jaxb-api\n    compile group: 'javax.xml.bind', name: 'jaxb-api', version: '2.4.0-b180830.0359'\n}\n\ntest {\n    useJUnitPlatform()\n    testLogging {\n        events \"passed\", \"skipped\", \"failed\"\n    }\n}\nNow when you do ./gradlew build it will generate a proper Spring Boot jar that you can run on the command-line. This jar you can also use in your docker images.\nAs of Spring Boot 2.3 it is possible to let Spring Boot create the image as well using build packs or regular docker.\nWhen using a build pack, the above build.gradle is enough to create an image. Just run ./gradlew bootBuildImage, it will then use build packs to generate an image.",
    "phpMyAdmin error \"No address associated with hostname\" in Docker?": "For that to work, you need to at least create a bridge network and connect phpmyadmin and your db to it.\nSomething like this should work:\nversion: \"3\"\nvolumes:\n  dbdata:\nnetworks:\n  backend:\n    driver: bridge\nservices:\n  database:\n    container_name: mysql_database\n    image: mysql:5.7\n    volumes:\n      - dbdata:/var/lib/mysql\n    environment:\n      - \"MYSQL_DATABASE=dev-db\"\n      - \"MYSQL_USER=phpmyadmin\"\n      - \"MYSQL_PASSWORD=phpmyadmin\"\n      - \"MYSQL_ROOT_PASSWORD=123456\"\n    ports:\n      - 8991:3306\n    networks:\n      - backend\n\n  phpmyadmin:\n    container_name: phpmyadmin\n    image: phpmyadmin/phpmyadmin\n    ports:\n      - \"8992:80\"\n    depends_on:\n      - database\n    environment:\n      - \"PMA_HOST=database\"\n      - \"MYSQL_USER=phpmyadmin\"\n      - \"MYSQL_PASSWORD=phpmyadmin\"\n      - \"MYSQL_ROOT_PASSWORD=123456\"\n      - \"UPLOAD_LIMIT=3000000000\"\n    networks:\n      - backend\n  \nAlso I do not recommend exposing DB port\nEDITED: Fixed docker-compose file",
    "Docker, docker-compose and copying files between projects": "As the author correctly pointed out, volumes are used for persisting data. Here I want to show two solutions, on how to use them for sharing data between container. This solution is far from being perfect!\nSolution 1\nDownsides\nFirst, I want to point out the downsides of this solution.\nYou need to clean up the volumes. Volumes only get populated with a container's content at first creation. See here for an explanation. Because of that, docker-compose down -v must be done, if some files in the project-directories have changed.\nAn alternative way to docker-compose down -v``  is to manually delete the named volumes using docker volume rm ``.\nIf you don't not want this, you can temporally COPY the files to a folder (which is not a mounted volume, e. g. /tmp). Using an entrypoint script you can than copy the files to its intended position (e. g. /home/developer/). See Solution 2 for this.\nMy setup: Folder-Structure\nMy folder structure looks similar to yours:\n\u251c\u2500\u2500 docker-compose.yaml\n\u251c\u2500\u2500 project1\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u251c\u2500\u2500 sub1\n\u2502   \u2502   \u251c\u2500\u2500 testfile_project_1_1.txt\n\u2502   \u2502   \u2514\u2500\u2500 testfile_project_1_2.txt\n\u2502   \u2514\u2500\u2500 sub2\n\u2502       \u2514\u2500\u2500 testfile_project_1_3.txt\n\u251c\u2500\u2500 project2\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u251c\u2500\u2500 sub1\n\u2502   \u2502   \u2514\u2500\u2500 testfile_project_2_1.txt\n\u2502   \u2514\u2500\u2500 sub2\n\u2502       \u251c\u2500\u2500 testfile_project_2_2.txt\n\u2502       \u251c\u2500\u2500 testfile_project_2_3.txt\n\u2502       \u251c\u2500\u2500 testfile_project_2_4.txt\n\u2502       \u2514\u2500\u2500 testfile_project_2_5.txt\n\u2514\u2500\u2500 project-db\n    \u251c\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 entrypoint.sh\nSources\ndocker-compose.yaml\nversion: \"3.8\"\nservices:\n  first-service:\n    build: \n      context: ./project1\n      dockerfile: Dockerfile\n    volumes:       \n      - data-first-service:/home/developer/\n\n  second-service:\n    build: \n      context: ./project2\n      dockerfile: Dockerfile\n    volumes:       \n      - data-second-service:/home/developer/\n\n  databse-service:\n    build: \n      context: ./project-db\n      dockerfile: Dockerfile\n    volumes:       \n      - data-first-service:/home/developer/project1/\n      - data-second-service:/home/developer/project2/\n    depends_on: \n      - first-service\n      - second-service\n \nvolumes: \n  data-first-service:  \n  data-second-service:\nDockerfile(s)\nThey are pretty much the same. The dockerfile for the* db-service* only copies it's entrypoint-script. The part with the sudoers is here, because this is my default testing image. I just included it to make clear which permissions my user has and to make passwordless sudo with regular user possible. It is not mandatory.\nFROM ubuntu:latest\n# We need some tools\nRUN apt-get update && apt-get install -y sudo\n# We want to have another user than `root`\n## USER SETUP \nRUN adduser developer\n# We want to have passwordless sudo access\nRUN \\\n    sed -i /etc/sudoers -re 's/^%sudo.*/%sudo ALL=(ALL:ALL) NOPASSWD: ALL/g' && \\\n    sed -i /etc/sudoers -re 's/^root.*/root ALL=(ALL:ALL) NOPASSWD: ALL/g' && \\\n    sed -i /etc/sudoers -re 's/^#includedir.*/## **Removed the include directive** ##\"/g' && \\\n    echo \"developer ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers;  su - developer -c id\n\n# Run now with user developer\nUSER developer\nCOPY sub1 /home/developer/sub1\nCOPY sub2 /home/developer/sub2\nRUN ls -l\n\nADD ./entrypoint.sh /entrypoint.sh\nRUN sudo chmod +x /entrypoint.sh\nENTRYPOINT [ \"/entrypoint.sh\" ]\nentrypoint.sh\nThe entrypoints for the two projects are not special, they just contain a simple ls -l to /home/developer. The entrypoint for the* db-service* just shows a tree output (see Results):\n#!/bin/bash\ncd ~\necho \"db service - You are here: ${PWD} \"\ntree --du -shaC | grep -Ev '(  *[^ ]* ){5}\\['\nResult\nAs you can see, the project-db container now contains the files from the other two projects.\ndatabse-service_1  | .\ndatabse-service_1  | |-- [ 220]  .bash_logout\ndatabse-service_1  | |-- [3.7K]  .bashrc\ndatabse-service_1  | |-- [ 807]  .profile\ndatabse-service_1  | |-- [ 17K]  project1\ndatabse-service_1  | |   |-- [ 220]  .bash_logout\ndatabse-service_1  | |   |-- [3.7K]  .bashrc\ndatabse-service_1  | |   |-- [ 807]  .profile\ndatabse-service_1  | |   |-- [4.0K]  sub1\ndatabse-service_1  | |   |   |-- [   0]  testfile_project_1_1.txt\ndatabse-service_1  | |   |   `-- [   0]  testfile_project_1_2.txt\ndatabse-service_1  | |   `-- [4.0K]  sub2\ndatabse-service_1  | |       `-- [   0]  testfile_project_1_3.txt\ndatabse-service_1  | `-- [ 17K]  project2\ndatabse-service_1  |     |-- [ 220]  .bash_logout\ndatabse-service_1  |     |-- [3.7K]  .bashrc\ndatabse-service_1  |     |-- [ 807]  .profile\ndatabse-service_1  |     |-- [4.0K]  sub1\ndatabse-service_1  |     |   `-- [   0]  testfile_project_2_1.txt\ndatabse-service_1  |     `-- [4.0K]  sub2\ndatabse-service_1  |         |-- [   0]  testfile_project_2_2.txt\ndatabse-service_1  |         |-- [   0]  testfile_project_2_3.txt\ndatabse-service_1  |         |-- [   0]  testfile_project_2_4.txt\ndatabse-service_1  |         `-- [   0]  testfile_project_2_5.txt\ndatabse-service_1  | \ndatabse-service_1  |   42K used in 6 directories, 17 files\nHow to use\nAs said, this method has some downside. You need to class a docker-compose down in order to make this solution work. So, the workflow looks similar to this: docker-compose build && docker-compose up. If you change a file in one of the project-directories or if you want to update the content ud must call docker-compose down -v, otherwise it will still reuse the pre-populated content from the old volumes.\nSolution 2\nBasically, it\u2019s the same as Solution 1. The difference is, that the \u201cproject-containers\u201d first cop the sources to a temporal location and after the container has been started (and the volume is mounted) to the path where the volume is mounted.\nDockerfile\nJust minor changed for this solution\n[...]\n# Run now with user developer\nUSER developer\nCOPY sub1 /tmp/sub1\nCOPY sub2 /tmp/sub2\nRUN ls -l\n\nADD ./entrypoint.sh /entrypoint.sh\nRUN sudo chmod +x /entrypoint.sh\nENTRYPOINT [ \"/entrypoint.sh\" ]\nand the entrypoint looks loke this\n#!/bin/bash\nmv /tmp/sub1 /home/developer/sub1\nmv /tmp/sub2 /home/developer/sub1\n\n# Then do your stuff",
    "how can I run a web-app with a docker-container?": "You have to enable and start the Docker daemon in your system.\nIf you are on Linux, try it: sudo systemctl enable docker && sudo systemctl start docker\nIf systemctl is not recognized as a command, you should use: service docker start.\nThe systemctl start is required for the first run because enable will only auto-start the daemon after reboot. After enabling it, it will auto start on boot.",
    "Container Build error - failed to shutdown container - container encountered an error during Shutdown": "The issue due to multiple reasons, Did following changes to fix\nIncreased CPU cores (The CPU reaches 100% while performing docker build operation, Due to this container got exit in between).\nWhile performing docker build used the \"--memory=16g\" parameter. Refer to Runtime options with Memory, CPUs, and GPUs for more details.\nApplication EXE expecting reboot configured \"/noreboot\" in the configuration.",
    "How to build an image but not start it as a service using docker-compose": "Run, docker-compose build. It will build the image but not run a service.",
    "Update dependencies in the Dockerfile and create an image without re downloading the previously mentioned dependencies": "1. If there is any way to keep building images mentioning the new dependencies without re-downloading the old dependencies?\nWell, i often optimize Dockerfile using layer caching. Whenever you write down a command in Dockerfile, it creates a new layer. Between 2 times build, docker compares the Dockerfile's commands top down and rebuild from where it detects command's changes. So i often put stable layers (like dependencies, environment setup) at the top of dockerfile. Otherwise layers like EXPOSE Port or CMD which i often change so i put them at bottom of the file. By doing this, it saves a lot of time whenerver i rebuild image.\nYou can also use multistage-build. But i not often use it so you can check it here: https://docs.docker.com/develop/develop-images/multistage-build/\n2. without keeping the old image and import from that into the new one?\nSometime when i want to reinstall everything again, i just rebuild image use option --no-cache.**\ndocker build --no-cache=true .\n3. Without the hassle of writing a new \"FROM\" in the dockerfile\nSometimes i use base image like linux alpine and install everything i need from scratch so my image will have smaller size and does not contain things that i dont need. FROM is just pulling images from Dockerhub which are created by the some way.\nFor example Dockerfile of image nginx-alpine : https://github.com/nginxinc/docker-nginx/blob/2ef3fa66f2a434cd5e44e35a02f4ac502cf50808/mainline/alpine/Dockerfile\nYou can checkout alpine linux for more details: https://alpinelinux.org/",
    "Execute Java Jar using shell script in Docker container": "The plain ubuntu docker container (FROM ubuntu) does not include the java runtime environment.\nYou have the following options:\nInstall java into the ubuntu container. See here: https://dzone.com/articles/creating-a-docker-image-with-ubuntu-and-java\nUse the openjdk as your base container, e.g. FROM openjdk:11\nI would prefer the second option unless you really need ubuntu as your base container. See also the docker hub page for openjkd, which has several other variantes available, e.g. opnejdk",
    "Docker WORKDIR path is added to relative path": "When you give CMD (or RUN or ENTRYPOINT) in the JSON-array form, you're responsible for manually breaking up the command into \"words\". That is, you're running the equivalent of the quoted shell command\n'tail -f /dev/null'\nand the whole thing gets interpreted as one \"word\" -- the spaces and options are taken as part of the command name to look up in $PATH.\nThe most straightforward workaround to this is to remove the quoting and just use a bare string as CMD.\nNote that the container you're building doesn't actually do anything: it doesn't include any application source code and the command you're providing intentionally does nothing forever. Aside from one running container with an idle process, you get the same effect by just not running the container at all. You typically want to copy your application code in and set CMD to actually run it:\nFROM node:12.17.0-alpine\nWORKDIR /src/webui\nCOPY package.json yarn.lock ./\nRUN yarn install\nCOPY . ./\nCMD [\"yarn\", \"start\"]\n# Also works:  CMD yarn start\n# Won't work:  CMD [\"yarn start\"]",
    "Run Asp Core MVC on Docker - the static files / wwwroot files are not loading": "You must run the dotnet publish command so that all the required files are copied into your app folder.\nYour release folder only has the dll files for your application so when you copy it into the docker container the wwwroot files are not copied.\nIf you create a new web app project and tick the Docker support option it will make a dockerfile for you. Create a sample project so you can see the recommended way to containerize your application.\nHere is an example that uses the publish command:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1-buster-slim AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1-buster AS build\nWORKDIR /src\nCOPY [\"MyApp.SGC.Site/MyApp.SGC.Site.csproj\", \"MyApp.SGC.Site/\"]\nRUN dotnet restore \"MyApp.SGC.Site/MyApp.SGC.Site.csproj\"\nCOPY . .\nWORKDIR \"/src/MyApp.SGC.Site\"\nRUN dotnet build \"MyApp.SGC.Site.csproj\" -c Release -o /app/build\n\nFROM build AS publish\n# publish will copy wwwroot files as well\nRUN dotnet publish \"MyApp.SGC.Site.csproj\" -c Release -o /app/publish\nWORKDIR /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"MyApp.SGC.Site.dll\"]",
    "Access environment variable value in docker ENTRYPOINT ( exec ) from second parameter(with customerentrypoint script as first parameter)": "The environment for CMD is interpreted slightly differently depending on how you write the arguments. If you pass the CMD as a string (not inside an array), it gets launched as a shell instead of exec. See https://docs.docker.com/engine/reference/builder/#cmd.\nWhat you can try if you want to use array is\nENTRYPOINT [\"/bin/sh\", \"-c\", \"echo ${VARIABLE}\"]",
    "How to get MYSQL to start on docker run and creating mysql users in Dockerfile": "I wanted to at least update with a partial answer. I haven't yet figured out how to do everything I wanted but I've at least gotten further. Since I was already using supervisor, I added the starting of mysql to it. Below you can see my Dockerfile, .sh files, default and supervisord.conf file.\nDockerfile\n#Download base image ubuntu 16.04\nFROM ubuntu:16.04\n\n# Let the conatiner know that there is no tty\nENV DEBIAN_FRONTEND noninteractive\n\n# Update Software repository\nRUN apt-get update\n\n# Add Language Packs\nRUN apt-get install -y language-pack-en-base\n\n# Set the locale\nRUN sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen && \\\n    locale-gen\nENV LANG en_US.UTF-8  \nENV LANGUAGE en_US:en  \nENV LC_ALL en_US.UTF-8 \n\n# Add Current PHP Repository\nRUN apt-get install software-properties-common -y\nRUN add-apt-repository -y ppa:ondrej/php\nRUN add-apt-repository -y ppa:ondrej/mysql-5.6\nRUN apt-get update\n\n# Basic Requirements\nRUN apt-get -y install pwgen python-setuptools curl git nano sudo unzip openssh-server openssl vim htop\nRUN apt-get -y install php7.4-fpm php7.4-common php7.4-mysql php7.4-xml php7.4-xmlrpc php7.4-curl php7.4-gd php7.4-imagick php7.4-cli php7.4-dev php7.4-imap php7.4-mbstring php7.4-soap php7.4-zip php7.4-bcmath php7.4-memcache php7.4-mysql\nRUN apt-get -y install mysql-server-5.6 mysql-client-5.6 nginx\nRUN apt-get install -y supervisor && \\\n    rm -rf /var/lib/apt/lists/*\n\n# mysql config\nRUN sed -i -e\"s/^bind-address\\s*=\\s*127.0.0.1/explicit_defaults_for_timestamp = true\\nbind-address = 0.0.0.0/\" /etc/mysql/mysql.conf.d/mysqld.cnf    \n\n#Define the ENV variable\nENV nginx_vhost /etc/nginx/sites-available/default\nENV php_conf /etc/php/7.4/fpm/php.ini\nENV nginx_conf /etc/nginx/nginx.conf\nENV supervisor_conf /etc/supervisor/supervisord.conf\n\n# Enable php-fpm on nginx virtualhost configuration\nCOPY default ${nginx_vhost}\nRUN sed -i -e 's/;cgi.fix_pathinfo=1/cgi.fix_pathinfo=0/g' ${php_conf} && \\\n    echo \"\\ndaemon off;\" >> ${nginx_conf}\n\n#Copy supervisor configuration\nCOPY supervisord.conf ${supervisor_conf}    \n\nRUN mkdir -p /run/php && \\\n    chown -R www-data:www-data /var/www/html && \\\n    chown -R www-data:www-data /run/php\n\n# Volume configuration\nVOLUME [\"/etc/nginx/sites-enabled\", \"/etc/nginx/certs\", \"/etc/nginx/conf.d\", \"/var/log/nginx\", \"/var/www/html\", \"/var/lib/mysql\"]\n\n# ROOT PASSWORD\nENV MYSQL_ROOT_PASSWORD=root\n\nENV MYSQL_DATABASE=tsc_sandbox\nENV MYSQL_USER=tsc_user\nENV MYSQL_PASSWORD=tsc_user\n\n# Setup Mysql DB\nCOPY db-init.sh /docker-entrypoint-initdb.d/db-init.sh\nRUN chmod +x /docker-entrypoint-initdb.d/db-init.sh\nRUN /etc/init.d/mysql start\n\n# Configure Services and Port\nCOPY start.sh /start.sh\nCMD [\"./start.sh\"]\n\nCOPY tsc_sandbox.sql.gz /tsc_sandbox.sql.gz\nRUN chmod +x /tsc_sandbox.sql.gz\n\n# Network Ports\nEXPOSE 80 443 3306 22 11211 3000 9000 10137 20080\nstart.sh\n#!/bin/sh\n\n/usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf\ndb-init.sh\n#!/bin/bash\n\nmysql -u root -e \"CREATE USER 'tsc_user'@'%' IDENTIFIED BY 'tsc_user';\"\nmysql -u root -e \"CREATE USER 'memaster'@'%' IDENTIFIED BY 'memaster';\"\nmysql -u root -e \"GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, ALTER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, SHOW VIEW ON *.* TO 'tsc_user'@'%';\"\nmysql -u root -e \"GRANT SELECT ON *.* TO 'memaster'@'%';\"\nmysql -u root -e \"CREATE DATABASE tsc_sandbox;\"\n\ngunzip < /tsc_sandbox.sql.gz | mysql -u root tsc_sandbox;\ndefault\nserver {\n    listen 80;\n    listen [::]:80 ipv6only=on default_server;\n\n    root /var/www/html;\n    index index.html index.htm index.nginx-debian.html;\n\n    server_name _;\n\n    location / {\n        try_files $uri $uri/ =404;\n    }\n\n    location ~ \\.php$ {\n        include snippets/fastcgi-php.conf;\n        fastcgi_pass unix:/run/php/php7.4-fpm.sock;\n    }\n\n    # deny access to .htaccess files, if Apache's document root\n    # concurs with nginx's one\n    #\n    #location ~ /\\.ht {\n    #    deny all;\n    #}\n}\nsupervisord.conf\n[unix_http_server]\nfile=/dev/shm/supervisor.sock   ; (the path to the socket file)\n\n[supervisord]\nlogfile=/var/log/supervisord.log ; (main log file;default $CWD/supervisord.log)\nlogfile_maxbytes=50MB        ; (max main logfile bytes b4 rotation;default 50MB)\nlogfile_backups=10           ; (num of main logfile rotation backups;default 10)\nloglevel=info                ; (log level;default info; others: debug,warn,trace)\npidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\nnodaemon=false               ; (start in foreground if true;default false)\nminfds=1024                  ; (min. avail startup file descriptors;default 1024)\nminprocs=200                 ; (min. avail process descriptors;default 200)\nuser=root             ;\n\n; the below section must remain in the config file for RPC\n; (supervisorctl/web interface) to work, additional interfaces may be\n; added by defining them in separate rpcinterface: sections\n[rpcinterface:supervisor]\nsupervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface\n\n[supervisorctl]\nserverurl=unix:///dev/shm/supervisor.sock ; use a unix:// URL  for a unix socket\n\n; The [include] section can just contain the \"files\" setting.  This\n; setting can list multiple files (separated by whitespace or\n; newlines).  It can also contain wildcards.  The filenames are\n; interpreted as relative to this file.  Included files *cannot*\n; include files themselves.\n\n[include]\nfiles = /etc/supervisor/conf.d/*.conf\n\n\n[program:php-fpm7.4]\ncommand=/usr/sbin/php-fpm7.4 -F\nnumprocs=1\nautostart=true\nautorestart=true\n\n[program:nginx]\ncommand=/usr/sbin/nginx\nnumprocs=1\nautostart=true\nautorestart=true\n\n[program:mysql]\ncommand=/usr/bin/pidproxy /var/run/mysqld/mysqld.pid /usr/sbin/mysqld\nautorestart=true\nIdeally I would be able to have the db-init.sh script run the first time the docker is ran without the need to exec into it and run it. Outside of that, this works pretty well thus far.",
    "Environment variable not being set in Dockerfile": "you don't need to set the env vars in the docker images at all, and you can fix the issue using the changes below\nASPNETCORE_ENVIRONMENT works because the app only check it at runtime and not at build time\ndatabase docker file\nFROM mcr.microsoft.com/mssql/server:2017-latest\n\nENV ACCEPT_EULA=Y\nENV MSSQL_PID=Developer\nENV MSSQL_TCP_PORT=1433\n\nWORKDIR /src\nCOPY ./ /scripts/\n\nEXPOSE 1433 \n\nCOPY ./docker-entrypoint.sh /\nENTRYPOINT [\"/docker-entrypoint.sh\"]\ndocker-entrypoint.sh (chmod a+x)\n#!/bin/bash -e\n\n\n(/opt/mssql/bin/sqlservr --accept-eula & ) | grep -q \"Service Broker manager has started\" && sleep 5s && (for foo in /scripts/*.sql;do /opt/mssql-tools/bin/sqlcmd -S127.0.0.1 -Usa -P$SA_PASSWORD  -i$foo;done)\n\nexec \"$@\"\n\nexit 0\napp docker file\nFROM mcr.microsoft.com/dotnet/core/sdk:3.0 AS build-env\nWORKDIR /app\n\n# Copy csproj and restore as distinct layers\nCOPY *.csproj ./\nRUN dotnet restore\n\n# Copy everything else and build\nCOPY . ./\nRUN dotnet publish -c Debug -o out MyProject.csproj\n\n# Build runtime image\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.0\nWORKDIR /app\nCOPY --from=build-env /app/out .\n\nEXPOSE 80\n\nENTRYPOINT [\"dotnet\", \"MyProject.dll\"]\nyou can move also these to the env file and it is recommended to separate the env files for the containers\nENV ACCEPT_EULA=Y\nENV MSSQL_PID=Developer\nENV MSSQL_TCP_PORT=1433",
    "Pass ARG from docker-compose.yml to a docker image": "From my understanding, you are trying to pass the NPM_TOKEN arguments from the environment variable named MY_NPM_TOKEN.\nHowever, there is an error in the syntax that you should update your docker-compose.yaml file\nfrom - NPM_TOKEN={MY_NPM_TOKEN}\nto - NPM_TOKEN=${MY_NPM_TOKEN}",
    "Unable to install Dask[complete] in alpine 3.9 docker image": "Alpine is a minimal working linux image, therefore quite often required libraries are missing to compile python packages. I ran into similar issues quite often, depending on the pyhton package I wanted to install.\nYour log shows, that you are missing header files for zlib:\nThe headers or library files could not be found for zlib, a required dependency when compiling Pillow from source.\nTherefore, you need to install the according library zlib-dev.\nFor the image I create I usually use the following snippet:\nRUN apk add --update --no-cache --virtual .build-deps \\\n        gcc \\ \n        musl-dev \\ \n        make \\ \n    && apk add --update --no-cache \\\n        python3 \\\n        python3-dev \\ \n        krb5-dev \\ \n        libxml2-dev \\ \n        libxslt-dev \\ \n        libffi-dev \\\n        jpeg-dev \\\n        zlib-dev \\\n        libffi-dev \\ \n        openssl-dev \\\n        git \\\n    && pip3 install --upgrade pip \\\n    && pip3 install --cache-dir=/pipcache -r requirements.txt && rm -rf /pipcache \\\n    && apk del .build-deps\nHope this helps.",
    "Getting error : Unknown MySQL server host 'db' (-2) in django python for docker": "the problem is with your command:\ndocker-compose run app sh -c \"python app/manage.py migrate\" \nthat will start only the app container but not the db.\ntry to start your stack with:\ndocker-compose up -d \nthen run your command so:\ndocker exec -ti MY_APP_CON sh -c \"python app/manage.py migrate\"",
    "Docker NameError: name 'app' is not defined": "The error you have shown in the image and the code does not seem matched. to reproduce your error is to pass app to flask object instead of __name__.\nHere you go with HelloWorld\nFROM python:alpine3.7\nRUN pip install flask==0.10.1\nCOPY . /app\nWORKDIR /app\nEXPOSE 5000\nCMD python app.py\nand app.py\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef index():\n    return \"Welcome to the Data Science Learner!\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=int(\"5000\"), debug=True)\nbuild\ndocker build -t flask-test .\nrun\ndocker run -it --rm flask-test\nYou can use the same with Docker compose,\ndocker-compose rm -f && docker-compose up --build",
    "Auto generated Dockerfile for Web .net core application": "A Dockerfile is a series of steps used by the docker build . command. There are at a minimum three steps required:\nFROM some-base-image\nCOPY some-code-or-local-content\nCMD the-entrypoint-command\nAs our application becomes more and more complex additional steps are added. Like restoring packages and dependencies. Commands like below are used for that:\nRUN dotnet restore \n-or-\nRUN npm install\nOr the likes. As it becomes more difficult the image build time will increase and the image size itself will increase.\nDocker build steps generates multiple docker images and caches them. Notice the below output:\n$ docker build .\nSending build context to Docker daemon  310.7MB\nStep 1/9 : FROM node:alpine\n ---> 4c6406de22fd\nStep 2/9 : WORKDIR /app\n ---> Using cache\n ---> a6d9fba502f3\nStep 3/9 : COPY ./package.json ./\n ---> dc39d95064cf\nStep 4/9 : RUN npm install\n ---> Running in 7ccc864c268c \nnotice how step 2 is saying Using cache because docker realized that everything upto step 2 is the same as from previous build step it is safe to use the cached from the previous build commands.\nOne of the focuses of this template is building efficient images. Efficiency could be achieved in two ways:\nreducing the time taken to build the image\nreducing the size of the final image\nFor #1 using cached images from the previous builds is leveraged. Dividing dockerfile to rely more and more on the previous build makes the build process faster. It is only possible to rely on cache if the Dockerfile is written efficiently.\nBy separating these stages of build and publish the docker build . command will be better able to use more and more cache from previous steps in docker file.\nFor #2 avoid installing packages that are not required, for example.\nrefer docker documentation for more details here.",
    "Getting \"The serve command requires to be run in an Angular project, but a project definition could not be found\" running Docker Compose": "I think that the problem is with the host path mentioned in the \"ct-front\" volume:\nvolumes:\n        - 'contacts-front:/app'",
    "How to make a simple docker service accessible from external hosts": "As I derive from the comments, this is not a docker-compose issue! The problem is that you want to access your \"home\" network from the public internet wich is not possible by default. You need to configure your modem to allow this.",
    "Creating a user for a non existing database not working": "var passwd = '$MONGO_INITDB_USERNAME';\nAre you setting the correct password here? (It's using the $USERNAME variable rather than the password one)",
    "Docker Mysql: What causes SQLSTATE[HY000] [2002] Connection timed out?": "Turned out I had wrong env variable name in config/database.php for DB_HOST (it was like 'host'=> env('DB_HOST_1', 'some ip'))... Did not notice at first. Now it works",
    "Docker - Share local storage between services": "You can share the same local directory with your services.\nJust make sure that your code refers to the directory accordingly (shared path).\nIn this case, /usr/src/app/orchestrator/data\nSample:-\norchestrator:\nimage: orchestrator-mabsed\nbuild: orchestrator/\nenvironment:\n  ES_HOST: 'elasticsearch'\ntty: true\nvolumes:\n  - MABSED/orchestrator/data/:/usr/src/app/orchestrator/data\n\nstreamer:\n image: streamer-mabsed\n build: streamer/\n tty: true\n volumes:\n  - MABSED/orchestrator/data/:/usr/src/app/orchestrator/data",
    "docker-compose volume not synced": "OMG I found my files are correctly synced but gunicorn cached them so added --reload to CMD in Dockerfile, and finally it fixed. Thank you for helping and soooo sorry for my foolishness...!",
    "Error in parse_repo_spec(repo) : Invalid git repo specification: 'rga'": "I managed to solve this by using\nRUN R -e \"devtools::install_github('skardhamar/rga')\"",
    "Is there a way to make docker config editable from inside a container?": "The config will be read only by design. But you can copy this to another file inside your container as part of an entrypoint script defined in your image.\ndocker config create my-config config.txt\ndocker service create \\\n--name redis \\\n--config src=my-config,target=/config.orig,mode=0660 \\\nusername/redis:custom\nThe entrypoint script would include the following:\nif [ ! -f /config.txt -a -f /config.orig ];\n  cp /config.orig /config.txt\nfi\n# skipping the typical exec command here since redis has its own entrypoint\n# exec \"$@\" # run the CMD as pid 1\nexec docker-entrypoint.sh \"$@\"\nYour Dockerfile to build that image would look like:\nFROM redis:alpine\nCOPY /entrypoint.sh /\nENTRYPOINT [ \"/entrypoint.sh\" ]\nAnd you'd build that with:\ndocker build -t username/redis:custom .",
    "Docker error: FileNotFoundError: [Errno 2]": "So I found a temporary \"bad\" solution. I just removed debug=True from app.run(debug=True) and it is running fine. It would nice if somebody could explain why this is happening.\nThanks!",
    "Dockerfile copy timed out issue on Windows": "This is not a solution on how to fix the issue, is a solution on how to avoid the issue:\nFirst compressed the folder and then copy only the zip file.\nDockerfile:\nFROM node:alpine AS builder\n\nRUN apk add zip\n\nWORKDIR /app\n\nCOPY . .\n\nRUN npm cache clean --force\nRUN npm install > npm.log 2>&1\nRUN npm run build\n\nRUN zip -r npm.zip /app/dist\n\nFROM nginx:alpine\n\nWORKDIR /usr/share/nginx/html\n\nRUN apk add unzip\n\nCOPY --from=builder /app/npm.zip /usr/share/nginx/html/\n\nRUN unzip npm.zip",
    "Dockerfile apt-get install - 503 service unavailable": "I found that there is a lot of confusion between APT::Acquire::Retries and Acquire::Retries, but the latter works for me. i.e.\necho 'Acquire::Retries \"20\";' > /etc/apt/apt.conf.d/80-retries;",
    "Testing tools for Docker Images": "Download the latest shaded dist from https://github.com/dgroup/docker-unittests/releases:\nwget https://github.com/dgroup/docker-unittests/releases/download/s1.1.1/docker-unittests-app-1.1.1.jar\nDe fine an *.yml file with tests.\nversion: 1.1\n\nsetup:\n- apt-get update\n- apt-get install -y tree\n\ntests:\n\n- assume: java version is 1.9, Debian build\n cmd:    java -version\n output:\n   contains:\n    - openjdk version \"9.0.1\"\n    - build 9.0.1+11-Debian\n\n- assume: curl version is 7.xxx\n cmd:    curl --version\n output:\n   startsWith: curl 7.\n   matches:\n    - \"^curl\\\\s7.*\\\\n.*\\\\nProtocols.+ftps.+https.+telnet.*\\\\n.*\\\\n$\"\n   contains:\n    - AsynchDNS IDN IPv6 Largefile GSS-API\n\n- assume:  Setup section installed `tree`\n cmd:     tree --version\n output:\n   contains: [\"Steve Baker\", \"Florian Sesser\"]\nRun tests for image\njava -jar docker-unittests.jar -f image-tests.yml -i openjdk:9.0.1-11\nhttps://i.sstatic.net/DSv72.png\nMore you can find in https://github.com/dgroup/docker-unittests.\nFeel free to submit new issues/questions/etc.",
    "Dockerfile appending PATH environment variable not working?": "RUN setx /M PATH \"C:\\Program Files\\dotnet;%PATH%\"",
    "Can't access my Laravel project through Apache in Docker - /var/www/ directory is empty": "It looks like the problem is with your docker-compose.yml file. The files you are copying to /var/www in your php-apache.dockerfile are being hidden by the volume mapping in your docker-compose.yml file. Remove the following lines from the compose file:\n  volumes:\n    - ./:/var/www",
    "Run Java Gui Application in Docker Container": "Browsers can run a very limited set of language runtimes; essentially only Javascript and things that can be recompiled into Javascript. There's not a generic way to take a running desktop application and republish it into a browser.\nDocker here is just an intermediate layer. It's very good for things like HTTP servers that are self-contained except for a network port; okay for command-line applications (if you don't mind running it as root with a command line three times as long, it works fine); and bad for GUI applications (works only on Linux, the command lines are long and arcane, and things are still highly likely to look different). It doesn't provide any magic to cross these layers.\nYou might be able to find some existing software that can fill some of these gaps; perhaps a browser-based X server, or maybe one of the full-blown VM systems can display a VM's desktop in a browser, or maybe you might be able to use the all-but-dead Java applet system, or maybe there's a Java layer that provides a websocket-based client and lets you run your Java-native GUI application with minor modifications.\nI'd encourage you to first make your application work, and solve the \"how do I actually make it appear the way I want\" problem, and only then bring in Docker if it's appropriate. The layer of separation it provides can make many things more difficult especially while you're actively developing or exploring an unknown space.",
    "Files inside Docker container not updating when I edit in host": "If you want a copy of the files to be visible in the container, use a bind mount volume (aka host volume) instead of a named volume.\nAssuming your docker-compose.yml file is in the root directory of the location that you want in /usr/src/app, then you can change your docker-compose.yml as follows:\nversion: \"3.3\"\nservices:\n  nodejs:\n    build: ./nodejs-server\n    ports:\n      - \"8001:8080\"\n    links:\n      - db:db\n    env_file:\n      - ./.env-example\n    volumes:\n      - .:/usr/src/app\n  db:\n    build: ./mysql-server\n    volumes:\n      - ./mysql-server/data:/docker-entrypoint-initdb.d #A folder /mysql-server/data with a .sql file needs to exist\n    env_file:\n      - ./.env-example",
    "Docker Swarm - How to set environment variables for tasks on various nodes": "This is resolved thanks to bmitch, see comments. For anyone else that runs into this. Entrypoint scripts work fine when a task creates cotnainer / child process. So, any variables set in them will be available to the containers / child process.\nThe non-issue was that when I used docker exec to do one-off commands inside a specific container / child process, it creates a new shell which does not call the entrypoint script, therefore does not have access to the variables set in the entrypoint. However, you can set them again in the shell and the child process will have access to them. e.g. database migrations, etc.",
    "Dockerfile says Copy failed: in .NET Core Dockerfile": "Publish creates the destination directory using the following format:\nbin\\$(Configuration)\\netcoreapp<version>\\publish\nSo your out directory will be created under:\nCOPY ${source:-MyWebApp/bin/Release/netcoreapp<version>/publish/out} .\nNote: replace <version> with your .net version\nYou can read more about it here",
    "Can I inform QNAP if my Docker Image is Intel or Arm": "If you're using an old version of Docker you have to use manifest-tool. There is a small tutorial in the README.md of the repository or you can also follow this one.\nIf your Docker version is recent enough, you can directly use the docker manifest command.\nThen you can use it that way:\ndocker manifest create name/app:latest name/app:amd64 name/app:arm64\ndocker manifest push name/app:latest\nHope that helps.",
    "Add C library to docker": "Add the docker commands to download, compile and install the library on your docker image.\nRUN wget https://crypto.stanford.edu/pbc/files/pbc-0.5.14.tar.gz && \\\n    tar -xvf pbc-0.5.14.tar.gz && \\\n    cd pbc-0.5.14 && \\\n    ./configure --prefix=$HOME/.local && \\\n    make && make install\n\nRUN rm pbc-0.5.14.tar.gz && rm -rf pbc-0.5.14\nthis is of course a very simple way, you need to know how you want to compile it, and what custom flags to use.",
    "UNable to install LibreOffice in docker Alpine": "Use below line form dockerfile and also share a entrypoint.sh script.\nFROM alpine:latest",
    "How to share sub folders from a docker named volume?": "Since mounting subdirectories of named volumes is not a possibility, maybe you can try the following.\nAs I see it, multiple containers need access to the same filesystem. According to the docker volume docs:\nRemember that multiple containers can mount the same volume, and it can be mounted read-write for some of them and read-only for others, at the same time.\nSo essentially, you can create a single docker volume and share it between all your containers:\n# Initialize the Docker Volume\ndocker volume create my_project_volume\ndocker run -v my_project_volume:/my_project --name helper alpine true\ndocker cp . helper:/my_project\ndocker rm helper\nIf you want your applications (such as apache) to only access subfolders within your volume, you can always mount the volume to / (or anywhere of your choosing), and create a symlink from the directory your application will access to the volume subdirectory. For example:\n# 1. Run your apache container, mounting the named volume to root dir\ndocker run -v my_project_volume:/my_project --name my_apache httpd:latest\n\n# 2. Create symlink to named volume subdirectory\ndocker exec -it my_apache ln -s /usr/local/apache2/htdocs /my_project/src",
    "Error when running Keycloak in Docker": "typedScopes is changed to scopes from 4.0.0 version, you realm-export.json file is generated from older version of keycloak",
    "Can't access Docker container containing Vue JS front-end app": "If you are hitting the container IP from within other container so you should use port it actually listens to, so use 80 in your nginx.conf instead of 8080.\nPublished ports will work on interface\\s docker interface bridges to.",
    "docker run <image> Error: Can't find python executable to run": "This should be a simple path issue. Notice how you call python on ./turn.py? This means you will assume it is a child of the current directory, which is - depending on your entrypoint - not necessarily the root folder where you copied it to.\nSimply changing your call CMD [\"python\", \"./Turn.py\"] CMD [\"python\", \"/Turn.py\"] should resolve the issue.",
    "How to install Java 9 and Gradle in a Docker image": "The ubuntu:latest tag is currently ubuntu:18.04 (bionic), which only contains the java packages for openjdk-8-jdk-headless and openjdk-11-jdk-headless but not openjdk-9-jdk-headless (which has already reached end-of-life, at least for public updates).\nopenjdk-9-jdk-headless is available in ubuntu:16.04 (xenial), though.\nI got the build working by switching to ubuntu:16.04, and also adding wget and unzip to the list of packages to install as they are subsequently used to download and unpack gradle but are not installed by default.\nFROM ubuntu:16.04\nMAINTAINER Hari Sekhon (https://www.linkedin.com/in/harisekhon)\n\nLABEL Description=\"Java + Ubuntu (OpenJDK)\"\n\nENV DEBIAN_FRONTEND noninteractive\n\nARG JAVA_VERSION=9\nARG JAVA_RELEASE=JDK\n\nENV JAVA_HOME=/usr\n\nRUN bash -c ' \\\n    set -euxo pipefail && \\\n    apt-get update && \\\n    pkg=\"openjdk-$JAVA_VERSION\"; \\\n    if [ \"$JAVA_RELEASE\" = \"JDK\" ]; then \\\n        pkg=\"$pkg-jdk-headless\"; \\\n    else \\\n        pkg=\"$pkg-jre-headless\"; \\\n    fi; \\\n    apt-get install -y --no-install-recommends wget unzip \"$pkg\" && \\\n    apt-get clean'\n\n\nCMD /bin/bash\n\n#install Gradle\nRUN wget -q https://services.gradle.org/distributions/gradle-4.5.1-bin.zip \\\n    && unzip gradle-4.5.1-bin.zip -d /opt \\\n    && rm gradle-4.5.1-bin.zip\n\n# Set Gradle in the environment variables\nENV GRADLE_HOME /opt/gradle-4.5.1\nENV PATH $PATH:/opt/gradle-4.5.1/bin",
    "Docker depends_on Order Not Working": "docker-compose up\nIt starts services in dependency order, about docker-compose --build up don't sure. And maybe your port, network name are defined wrong:\nhub:\nimage: selenium/hub\nnetworks:\n - robottestsnw\nports:\n - 4444:4444",
    "psql: could not translate host name \"postgres\" to address: Temporary failure in name resolution": "There may be a workaround, but the most obvious solution is to use docker-compose. Docker compose will fire up both the postgres container and your server container. Docker compose will also create a network for both of these container, so you will be able to connect to them.",
    "Shared build logic with docker-compose and multi-stage Dockerfiles": "It turns out you can \"compose\" docker-compose: https://docs.docker.com/compose/extends/#adding-and-overriding-configuration which is what I was looking for.",
    "How to Install mysql-server via dockerfile": "For an ubuntu:16.04 base image, mysqld is found in /usr/sbin, not /usr/bin\nIf you can add a step RUN which mysqld before your final RUN command that will show you where the mysqld executable is found. It may vary depending on which base image/distro you're using.\nYou can also use RUN mysqld ... without a full path, if the file is in your $PATH\nYou may also need to update your RUN sed line as below, adding spaces around the quoted string:\nRUN sed -i -e \"s/^bind-address\\s*=\\s*127.0.0.1/bind-address = 0.0.0.0/\" /etc/mysql/my.cnf\nOtherwise, you may see the following error:\nThe command '/bin/sh -c sed -i -e\"s/^bind-address\\s*=\\s*127.0.0.1/bind-address = 0.0.0.0/\"/etc/mysql/my.cnf' returned a non-zero code: 1",
    "Stack GHCJS project initaliziation error when building a docker image": "You need to install ghcjs before making a new project with it (ghcjs-base will be part of the ghcjs installation).\nCheck here and here",
    "Docker: Cannot start service nginx": "This has nothing to do with Nginx.\nIf you use volumes in docker-compose.yml, always make sure that the files on the left side of the colon : exists! If you miss that, sometimes Docker Compose creates folders instead of files \"on the left side\" (= on your host) IIRC. Which leads to subsequent errors on the next run.",
    "Docker pull image save path": "Edit-1: Will work on Windows containers\nFor changing windows containers data location you need to change the data-root of the docker daemon. This can be done through settings window. Click on Daemon option in Settings windows and switch from Basic to Advanced settings. Then change the config like below\n{\n  \"data-root\": \"c:/dockerdata\",\n  \"registry-mirrors\": [],\n  \"insecure-registries\": [],\n  \"debug\": true,\n  \"experimental\": true\n}\nAnd the data will now be stored at this new location.\nOriginal Answer - Will work when you use Linux Containers\nYou can change the VM location from Settings of Docker for windows - \"Images and volumes VHD location\"\nEdit-1\nIf you need to need to see the VM then you need to launch Hyper-V manager and check the VM details",
    "Access volume in docker build": "What you probably want to do, is to ADD or COPY the package.json file to the correct location, RUN npm install, then ADD or COPY the rest of the source into the image. That way, docker build will re-run npm install only when needed.\nIt would probably be better to run frontend and backend in separate containers, but if that's not an option, it's completely feasible to run ADD package.json-RUN npm install-ADD . once for each application.",
    "How to use docker to create a minimal live ISO that runs docker?": "Well, you can use FAI - Fully Automatic Installation to create an ISO that installs and configure Ubuntu and Docker.\nI have created a Docker image (ricardobranco/fai:4.3.3) to create these ISO's. The code at GitHub: https://github.com/ricardobranco777/fai\nThere are scripts to create the mirror, another to validate it, and to create the ISO. I've just committed a simple FAI config that I use to create ISO's to the faiconfig folder in my GitHub repo\nPD: I haven't had success with FAI 5.x and Ubuntu 16 (for some unknown reason at the time). For this reason I use 4.3.3. The maintainer was kind enough to keep the 4.3.3 Debian packages in the FAI repository.",
    "How to prevent a Docker Container to turn on while another Container in the same Service is trying to?": "You can add another app container to act as coordinator.\nIt is much related to this question.\nMy idea is to have a shared Redis in which you have a shared-lock. So each new container that is starting query for that lock that is in the shared Redis, and when the lock is acquired, container finishes its startup, then release the lock.\nhttps://redis.io/topics/distlock\nDistributed locks are a very useful primitive in many environments where different processes must operate with shared resources in a mutually exclusive way.",
    "docker-compose volume not appearing in container": "Volumes are runtime configurations in Docker. Because they are configurable, if you were to reference volumes during the build phase you would essentially be creating a potentially uncheckable broken dependency.\nI'm sure there is a more technical reason - but it really shouldn't be done. Move all that stuff to the runtime setup command and you should be OK.",
    "Pushing Dockerfile or docker-compose to heroku": "You only need to push Dockerfile. Docker-compose the only way to exec multiple container.",
    "Changing my project files doesn't change files inside the Docker machine": "Looking at the docker-compose.yml you've linked to, I don't see any volume entry. Without that, there's no connection possible between the files on your host and the files inside the container. You'll need a docker-compose.yml that includes a volume entry, like:\nversion: '2'\n\nservices:\n\n  angular-seed:\n    build:\n      context: .\n      dockerfile: ./.docker/angular-seed.development.dockerfile\n    command: npm start\n    container_name: angular-seed-start\n    image: angular-seed\n    networks:\n      - dev-network\n    ports:\n      - '5555:5555'\n    volumes:\n      - .:/home/app/angular-seed\n\nnetworks:\n  dev-network:\n    driver: bridge\nDocker-machine runs docker inside of a virtual box VM. By default, I believe c:\\Users is shared into the VM, but you'll need to check the virtual box settings to confirm this. Any host directories you try to map into the container are mapped from the VM, so if your folder is not shared into that VM, your files won't be included.\nWith the IP, localhost works on Linux hosts and newer versions of docker for windows/mac. Older docker-machine based installs need to use the IP of the virtual box VM.",
    "How to setup Docker for a polyglot microservice-based application?": "The usual approach is to put every independent component into a separate container. General Docker idea is 1 container = 1 logical task. 1 task is not exactly 1 process, it's just the smallest independent unit.\nSo you would need to find 4 basic images (probably existing ones from Docker registry should fit):\nPHP7-NGINX\nPYTHON-FLASK-NGINX\nMariaDB\nDgraph\nYou can use https://hub.docker.com/search/ to search for appropriate images.\nThen create custom Docker file for every component (taking either PHP7-NGINX or PYTHON-FLASK-NGINX as a parent image).\nYou probably would not need custom Docker file for databases. Typically database images require just mounting config file into image using --volume option, or passing environment arguments (see description of base image for details).\nAfter that, you can just write docker-compose.yml and define here how your images are linked and other parameters. That would look like https://github.com/wodby/docker4drupal/blob/master/docker-compose.yml . By the way, github is full of good examples of docker-compose.yml\nIf you are going to run services on different servers, then you can create a Swarm cluster, and run your docker-compose.yml against it: https://docs.docker.com/compose/swarm/ . After that, you can scale easily by deploying as many instances of each microservice as you need (that's why it's more useful to have separate images for every microservice).",
    "Docker image just contain software without OS": "Docker is designed to also abstract the OS dependencies - that is what it has been build for. Beside it also encapsulates the runtime, memory and things, it specifically is used as a extreme-better variant of chroot ( lets say chroot on ultra-steroids ).\nIt seems like you neither want the runtime seperation nor the OS layer seperation ( dependencies ) - thus docker makes absolutely no sense for you.\nDeploying with docker the is not \"simple\" or simpler as using other tools. You can use capistrano or, probably something like https://www.habitat.sh/ which actually does not require a software to be bundled in docker containers to be \"deployable\", it also works on barebones and uses its own packaging format. Thus you have a state-of-the-art deployment solution, and with habitat, you can later even upgrade using docker-containers.",
    "Cannot use process substitution during docker build because bash goes into posix mode": "If you are sure you have bash in your image being built, then you can change the shell invokation by using the SHELL command, which I described in another question.\nYou can use SHELL [ \"/bin/bash\", \"-c\" ]. Consider:\n$ docker build --no-cache - < <(echo '\n> FROM fedora\n> RUN cat <(echo hello world)\n> ')\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM fedora\n ---> ef49352c9c21\nStep 2/2 : RUN cat <(echo hello world)\n ---> Running in 573730ced3a3\n/bin/sh: -c: line 0: syntax error near unexpected token `('\n/bin/sh: -c: line 0: `cat <(echo hello world)'\nThe command '/bin/sh -c cat <(echo hello world)' returned a non-zero code: 1\n$ docker build --no-cache - < <(echo '\n> FROM fedora\n> SHELL [\"/bin/bash\", \"-c\"]\n> RUN cat <(echo hello world)\n> ')\nSending build context to Docker daemon  2.048kB\nStep 1/3 : FROM fedora\n ---> ef49352c9c21\nStep 2/3 : SHELL [\"/bin/bash\", \"-c\"]\n ---> Running in e78260e6de42\nRemoving intermediate container e78260e6de42\n ---> ff6ec782a9f6\nStep 3/3 : RUN cat <(echo hello world)\n ---> Running in afbb42bba5b4\nhello world\nRemoving intermediate container afbb42bba5b4\n ---> 25f756dcff9b\nSuccessfully built 25f756dcff9b",
    "Can I create wheels for python packages on macOS usable for a ubuntu docker image?": "If you have no C extensions, then you should be able to do:\npython setup.py bdist_wheel --universal \nto get a non-platform specific some_packagename-X.Y.Z-py2.py3-none-any.whl\nIf you have C extensions, you need to generate platform specific wheels.\nThe best way to do this is by using Docker (which on MacOS runs in a virtual machine) and the manylinux Docker containers to generate wheels for Python 2.7 and 3.4-3.7.\nThe generated wheels will install on many older versions of Linux, because of the way the manylinux containers are created. That is much better then starting your own Ubuntu XX.04 version (maybe in virtual machine) and not being able to install that on anything uses older compiler etc than your Ubuntu version does.",
    "Run docker as non-root in a development environment for an specific process": "In your Dockerfile - Use a RUN command to create the user:\nRUN useradd userToRunComposer\nThen use the USER command in your Dockerfile, after creating it.\nUSER userToRunComposer\nRUN curl -sS https://getcomposer.org/instal...\nRUN composer global require se...\nYou could also take a different approach by creating the user inside the container, then committing the image:\ndocker exec -ti <my container name> /bin/bash\nuserAdd userToRunComposer\nAnd then do a: docker commit <my container id> <myimagename> to avoid having to create the user every single time\nSee this question.",
    "SpringBoot @RestController not accessible from outside docker container": "I was able to solve the problem with help from @daniel.eichten and @ShawnClark, but I don't understand why this fails/works. T\nIt was not a Docker problem, but with Spring.\nAs seen here (question might be unrelated), I changed the Spring-Application from\n@SpringBootApplication\npublic class Application {\n\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\nto\n@EnableAutoConfiguration\n@EnableWebMvc\n@Configuration\n@ComponentScan\npublic class Application {\n\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\nNow all APIs are accessible as expected, also when running inside a Docker container.",
    "Docker, How to replace capistrano tasks in docker": "Now is a good time to step back and consider if the benefits of Docker outweigh the added complexity, for your situation. Assuming it is, here are a few suggestions on how to make these components work together better.\nWhile Ansible is a configuration management system, it's also designed for orchestration (that is, running commands across a series of remote machines). This has some cross-over with Capistrano, and as such, you may find it useful to port your Capistrano tasks to Ansible and eliminate a tool (and thus complexity) from your stack. This would likely come about from creating a deploy.yaml playbook that you run to deploy your application.\nDocker also overlaps responsibilities with Ansible, but in a different area, configuration. Any part of your system configuration that's necessary for the app can be configured inside the container using the Dockerfile, rather than on a system-wide level using Ansible.\nIf you have rake tasks that set up the application environment, you can put them in a RUN command in the Dockerfile. Keep in mind, however, that these will only be executed when you build the image, not when you run it.\nGenerally speaking, I view it this way: Docker sets up a container that has everything required to run one piece of your app (including a specific checkout of your code). Ansible configures the environment in which you run the containers and manages all the work to update them and put them in the right places.",
    "Follow Symbolic link when creating dockerfile [duplicate]": "That is not possible and will not be implemented. Please have a look at the discussion on github issue #1676:\nWe do not allow this because it's not repeatable. A symlink on your machine is the not the same as my machine and the same Dockerfile would produce two different results. Also having symlinks to /etc/paasswd would cause issues because it would link the host files and not your local files.",
    "How to edit docker file of the image which is pulled from registry?": "You can grab the dockerFile from hub.docker.com -> search the image you want, Download it make your modification and build it, or run the image downloaded and make your modification, commit it and push it on your docker hub account. Hope this will help you.",
    "Docker's container mount folder": "When you are mounting your conf dir your are replacing the contents of the /etc/nginx dir. instead mount the nginx.conf file and the conf/conf.d - see the section on mounting files, https://docs.docker.com/v1.8/userguide/dockervolumes/\nsudo docker run -d -p 80:80 -v $pwd/conf/conf.d:/etc/nginx/conf.d -v $pwd/conf/nginx.conf:/etc/nginx/nginx.conf -v $pwd/html:/srv/www my-nginx",
    "Starting services at container startup": "Not anymore using supervisord.\nI just include a script with all the services ... start commands in the Dockerfile. When I create my container with docker run ... I just specify that I want to start it with my script.\n& that's working very well.\nThanks @warmoverflow for trying to solve this.",
    "execute sqlite commands within dockerfile": "I can save the commands to a file and then execute the file in a dockerfile like this...\nADD sqlite_commands.sql /\nRUN sqlite3 panama.sqlite < /sqlite_commands.sql",
    "call a docker container from another container": "curl http://service1:7070/ \nuse - host1_name:inner_port_of_host1\nThat host is called \"service1\" in container2. Use that as the host name and the port is the inner port listener in service1's container.\nIf you have an express server on service1, listen on port 7070.",
    "Different problems when run in Dockerfile vs. manually. What's gone wrong?": "The solution was eventually traced to an invalid docker build cache. In a previous run, bower had failed to install and the cache had kept a bad symlink.\nSolution: run with docker build --no-cache.",
    "Error response from daemon: Cannot start container": "This message is produced when the kernel, for whatever reason, doesn't know how to handle the given executable format. It's a problem that's often associated with scripts that don't include a shebang line, or binaries that are incompatible with your system.\nSince you're able to run the image interactively, you probably have a badly written script somewhere in your container.\nSee: https://github.com/moby/moby/issues/10668",
    "Bluemix IBM Container with Mongodb connection failed": "The error you are having is because port 27017 is not open in IBM Containers. I suggest you open a support ticket with IBM Bluemix Support and ask this port to be opened or you can check with IBM Bluemix Support team for an alternative open port you can use as well.\nYou can open a support ticket in the following link:\nhttp://ibm.biz/bluemixsupport",
    "Parse file in Dockerfile": "If you really need to produce Dockerfile based on template, I think there is plenty of templating tools, depending on your host platform. It does not make sense to task Docker with it. A simplest one would be\ncfg_port = cat config.conf | grep ... | ...\nsed -e \"s/CFG_PORTNUMBER/$cfg_port/g\" /path/to/Docker_templatefile > /path/to/Dockerfile\nThat said,\nEXPOSE in Dockerfile is just a hint. It instructs docker to setup environment variables during container linking in destination container - see UserGuide: \"Environment Variables\" section.\nSo, make sure EXPOSE based on some template and resulting separate image for each port configuration is what you really need. EXPOSE really is just a hint.\nMuch easier and powerful way to do things like this, in most cases, is to specify ports during container creation:\ndocker run --expose=$cfg_port -ti myimage bash\nIn most cases, I would recommend not to create Dockerfile at all. Instead read and try different run options with docker official images until you are absolutely sure why you need your own image. You only want separate image per software stack that absolutely needs to run in the same container.\nAnd, of course, check out if docker-compose already does what you need.",
    "What is the purpose of defining VOLUME mount points within DockerFile rather than adhoc cmd-line -v?": "VOLUME instruction used within a Dockerfile does not allow us to do host mount, that is where we mount a directory from the host OS into a container.\nHowever other containers can still mount into the volumes of a container using the --from-container=<container name>, created with the VOLUMES instruction in the Dockerfile",
    "Content of a dockerfile to run glassfish server and deploy specific application from a git repository": "The best approach (in my opinion) is to create a new image that extends from your 'myglassfish' image and includes the WAR file. This image would have a tag that matches the application's release version. I hope the WAR file has been released to a Maven repository, from which you can download during the image build. In case the WAR file is in your local filesystem, just copy it into the image. One last thing, in case you are having trouble sharing files from your machine and the boot2docker VM, boot2docker automatically shares the Users folder in the VM. I hope I was helpful. In case you have more questions, just shoot.",
    "Change ulimit in Docker Container": "The --ulimit option was added in docker 1.6 (see the related pull-request here: https://github.com/docker/docker/pull/9437).\nI you're using an older version of docker, you won't be able to use this feature, in which case you should update to the current docker version",
    "Run commands on create a new Docker container": "Take a look at the ENTRYPOINT command. This specifies a command to run when the container starts, regardless of what someone provides as a command on the docker run command line. In fact, it is the job of the ENTRYPOINT script to interpret any command passed to docker run.",
    "Playwright UI Mode with Docker - Unable to Find Tests": "Source error is thrown on the preview of a file because of Win . Just ignore it and run the test.",
    "scikit-learn not installing during Docker image creation": "I was also facing the same issue and I have used python:latest as a base image which resolved my issue of installing scikit-learn.",
    "Best way in Docker to add specific ENV variables per user?": "You have to:\nWrite an image by a Dockerfile which accepts parameteres by ENV.\nIstantiate a different container for each environment passing the specific ENV settings.",
    "Docker failed building Rust Image on Apple M1": "you should use a Rust image built for ARM architecture. You can use the official Rust image for ARM architecture by changing the base image in your Dockerfile to:\nFROM rust:1.59-slim-buster as builder\nThis image is built for the ARM architecture, which is compatible with the M1 chip.\nOnce you've made this change, you can try building the image again using the same command:\ndocker build auth-server -t ta_auth_server:latest --platform linux/x86_64",
    "Failed to write file pdf: No such file or directory (2) when using `print-to-pdf`": "I tried to reproduce your setup and the pdf is generated fine for me from a test.html file (except the same first errors and warnings that you also have)\n2023-03-11 14:41:24 [0311/134124.866023:ERROR:bus.cc(399)] Failed to connect to the bus: Failed to connect to socket /var/run/dbus/system_bus_socket: No such file or directory\n2023-03-11 14:41:24 [0311/134124.902378:WARNING:dns_config_service_linux.cc(429)] Failed to read DnsConfig.\n2023-03-11 14:41:24 [0311/134124.914149:ERROR:bus.cc(399)] Failed to connect to the bus: Failed to connect to socket /var/run/dbus/system_bus_socket: No such file or directory\n2023-03-11 14:41:24 [0311/134124.914271:ERROR:bus.cc(399)] Failed to connect to the bus: Failed to connect to socket /var/run/dbus/system_bus_socket: No such file or directory\n2023-03-11 14:41:24 [0311/134124.934520:WARNING:bluez_dbus_manager.cc(247)] Floss manager not present, cannot set Floss enable/disable.\n2023-03-11 14:41:24 [0311/134124.966080:WARNING:sandbox_linux.cc(393)] InitializeSandbox() called with multiple threads in process gpu-process.\n2023-03-11 14:41:25 [0311/134125.053836:WARNING:dns_config_service_linux.cc(429)] Failed to read DnsConfig.\n2023-03-11 14:41:25 [0311/134125.111103:INFO:headless_shell.cc(107)] 4441 bytes written to file /tmp/test.pdf\nLast log entry is 4441 bytes written to file /tmp/test.pdf and the file is indeed generated, if I download it to my host machine I can open the pdf and it has the content of test.html.\nThe most probable problem with your solution is that test.html does not exists in your setup.\nMy setup looks like this:\n.\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 app\n    \u251c\u2500\u2500 start.sh\n    \u2514\u2500\u2500 test.html\nstart.sh\n#!/bin/sh\n\nchromium-browser --headless --no-sandbox --disable-setuid-sandbox --disable-gpu --disable-dev-shm-usage --run-all-compositor-stages-before-draw --print-to-pdf='/tmp/test.pdf' /tmp/test.html\n\ntail -f /dev/null # <- this is for the container to run indefinitely so you can exec into it and check the contents.\ntest.html\n<html>\n<head>\n    <title>Test</title>\n</head>\n<body>\n    <h1>Test</h1>\n    <p>Test</p>\n</body>\n</html>\nDockerfile\nFROM alpine:3.17\n\nRUN apk upgrade --no-cache --available \\\n    && apk add --no-cache \\\n      chromium-swiftshader \\\n      ttf-freefont \\\n      font-noto-emoji \\\n    && apk add --no-cache \\\n      --repository=https://dl-cdn.alpinelinux.org/alpine/edge/testing \\\n      font-wqy-zenhei\n\nENV CHROME_BIN=/usr/bin/chromium-browser \\\n    CHROME_PATH=/usr/lib/chromium/\n\nENV CHROMIUM_FLAGS=\"--disable-dev-shm-usage --disable-software-rasterizer\"\n\nCOPY ./app /tmp\n\nRUN chmod +x /tmp/start.sh\n\nCMD ./tmp/start.sh",
    "How to deploy app to 0.0.0.0 and port 5000 on Railway.app?": "you should create environment varriable on railway.app, it overwrite default port of railway server\nPORT=5000\nalso you can check docs https://docs.railway.app/deploy/exposing-your-app",
    "Secure way to use a github access token in a docker image?": "Building images with embedded secrets is not a good practice.\nIn my case, I wanted to debug some private repos inside the container. I suggest two approaches:\nCloning the private repo outside the container and then copying the files into the container.\nCloning inside the container by using the same ssh keys we have outside.\nThis can be done by mounting the ~/.ssh dir, for example:\ndocker run -it -v ~/.ssh:/root/.ssh my-image:latest /bin/bash",
    "How to inject Github secret as environment variable into Maven settings.xml file run in a Dockerfile?": "You're able to specify properties along with your Maven goal.\nRUN [\"mvn\",  \"-s\",  \"settings.xml\", \"clean\", \"install\", \"-Dgithub.token=TOKEN_GOES_HERE\"]\nAnd on pom.xml, you can use this to pick up the token.\n<password>${github.token}</password>",
    "Dockerfile fast api running port dynamically": "The spaces are because you have put it into one of the elements of the list given to ENTRYPOINT. If you compare it to your other entries, there's a new element for each term in the command, while you have two terms inside the one where you have $PORT - this means that it's being processed as a single parameter, and in that case Docker (or an alternative container runtime) needs to escape the spaces to build the command you're asking for.\nReplace it with separate elements instead:\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"$PORT\"]",
    "Backstage: build docker image": "Can you try below dockerfile:\nFROM node:16-bullseye-slim AS packages\n\nWORKDIR /app\nCOPY package.json yarn.lock ./\nCOPY packages packages\nCOPY catalog-info.yaml ./\n\n#COPY plugins plugins\n\nRUN find packages \\! -name \"package.json\" -mindepth 2 -maxdepth 2 -exec rm -rf {} \\+\n\n# Stage 2 - Install dependencies and build packages\n\nFROM node:16-bullseye-slim AS build\n\nWORKDIR /app\n\nCOPY --from=packages /app .\n\n# install sqlite3 dependencies \n\n#RUN apt-get update && \\\n#    apt-get install -y --no-install-recommends libsqlite3-dev python3 build-essential && \\\n#    yarn config set python /usr/bin/python3\n\nRUN yarn install --frozen-lockfile --network-timeout 600000 && rm -rf \"$(yarn cache dir)\"\n\nCOPY . .\n\nRUN yarn tsc\nRUN yarn --cwd packages/backend build\n\n# If you have not yet migrated to package roles, use the following command instead:\n#RUN yarn --cwd packages/backend backstage-cli backend:bundle --build-dependencies\n\n# Stage 3 - Build the actual backend image and install production dependencies\nFROM node:16-bullseye-slim\n\nWORKDIR /app\n\n# install sqlite3 dependencies, you can skip this if you don't use sqlite3 in the image\n#RUN apt-get update && \\\n#    apt-get install -y --no-install-recommends libsqlite3-dev python3 build-essential && \\\n#    rm -rf /var/lib/apt/lists/* && \\\n#    yarn config set python /usr/bin/python3\n\n# Copy the install dependencies from the build stage and context\nCOPY --from=build /app/yarn.lock /app/package.json /app/packages/backend/dist/skeleton.tar.gz ./\nRUN tar xzf skeleton.tar.gz && rm skeleton.tar.gz\n\nRUN yarn install --frozen-lockfile --production --network-timeout 600000 && rm -rf \"$(yarn cache dir)\"\n\n# Copy the built packages from the build stage\nCOPY --from=build /app/packages/backend/dist/bundle.tar.gz .\nRUN tar xzf bundle.tar.gz && rm bundle.tar.gz\n\n# Copy any other files that we need at runtime\nCOPY app-config.yaml ./\n#COPY github-app-proficloud-backstage-app-credentials.yaml ./ \n\n#This is for Tech-Docs\n#RUN apt-get update && apt-get install -y python3 python3-pip\n#RUN pip3 install mkdocs-techdocs-core==1.0.1\n\n#This is enable for software templating to work\n#RUN pip3 install cookiecutter\n\nCMD [\"node\", \"packages/backend\", \"--config\", \"app-config.yaml\"]",
    "Cannot install Selenium chromedriver in a Docker Container": "Why not just use an already existing image that does excatly that, as your starting point? For example, this looks like a functional docker image for chrome and python:\nhttps://hub.docker.com/r/joyzoursky/python-chromedriver/",
    "error: failed to solve: failed to fetch oauth token: Post \"https://auth.docker.io/token\": dial tcp: i/o timeout": "Setting the buildkit option to false is not sufficient, if you are behind a proxy, you should set both HTTP_PROXYand HTTPS_PROXY` env vars to get it work.\non Windows, in a CMD :\nset HTTP_PROXY=YourProxyUrl:Port\nset HTTPS_PROXY=YourProxyUrl:Port\non Windows in a GitBash shell, or on a MacOs shell :\nexport HTTP_PROXY=YourProxyUrl:Port\nexport HTTPS_PROXY=YourProxyUrl:Port",
    "Docker Image for NextJs ClientApp failing with an error Cannot find module '/app/server.js'": "It might that your package.json file, has the dev: \"NODE_ENV=production node server.js\" and you application doesn't have the server.js file as it is not created.. you should look into that.",
    "a base image cannot be pulled in Dockerfile but can be pulled by docker pull command": "I encountered the same issue when I used the buildx tool to build a docker image for a different environment. To build the container for different environments, docker spawns a build-kit container. If your PC has been running this container for a while during network switches, the container can lose the connection to the internet, resulting in such an error. Simply removing the running container and re-running the build command worked for me, as it respawns a fresh build-kit container with proper network connections to build the docker file.",
    "Cache folder not writable Bitbucket Docker": "I came across the same problem (also using Docker and Bitbucket Pipelines). Except that I was using k8s to deploy the app.\nWhat I did was change the next line in my deployment to false:\nsecurityContext:\n      readOnlyRootFilesystem: false (it was `true`)\nIt solved it for me!",
    "Build existing Docker image for another architecture without the original Dockerfile": "To get the information from docker history in a readable manner the following (bash) command helped me. Just put your Image and Tag after --no-trunc\ndocker history --no-trunc IMAGE:TAG | tac | tr -s ' ' | cut -d \" \" -f 5- | sed 's,^/bin/sh -c #(nop) ,,g' | sed 's,^/bin/sh -c,RUN,g' | sed 's,\n && ,\\n  & ,g' | sed 's,\\s*[0-9]*[\\.]*[0-9]*\\s*[kMG]*B\\s*$,,g' | head -n -1\nThere is also an image as a nice tool: Reference on Docker Hub\nBasically:\nalias dfimage=\"docker run -v /var/run/docker.sock:/var/run/docker.sock --rm alpine/dfimage\"\ndfimage IMAGE:TAG",
    "How can I copy installed gems from between Docker stages?": "You can use the default location (likely /usr/local/bundle) or set it explicitly. I have done both; explicitly set the default location just to it's obvious where it's coming from.\nFROM app_base as gems\n...\n# /usr/local/bundle is the default location, but make it explicit\nRUN bundle config set --global path /usr/local/bundle\n\nCOPY ./Gemfile* ./\nRUN bundle install ... other flags for my build\n...\n\nFROM app_base as prod\n\n...\nCOPY --from gems /usr/local/bundle /usr/local/bundle \n...\nHowever some gems may install libraries or binaries beyond what's in the bundle; you may need to do more if these extra installed resources are required in your build.",
    "What is the best practice to start \"roscore\" followed by \"rosrun ....\" on Docker Container startup?": "This is actually, in part, what roslaunch is for. It makes it easier to launch multiple nodes and nicer parameter input, but it will also start a roscore is one is not already running. In your example it would look something like this:\n<launch>\n  <node pkg=\"ROS_PackageName\" type=\"PythonScript.py\" name=\"my_node_name\" output=\"screen\" >\n  </node>\n</launch>\nWhen running roslaunch via CLI you would use it the same way you would rosrun. For example if you create the launch file in the same example package you listed and named the file my_launch.launch the command would look like: roslaunch ROS_PackageName my_launch.launch. Simply replace the rosrun command above with this.",
    "Cannot connect docker app to kafka docker container [duplicate]": "When I am running the app without dockerizing it, it is working fine.\nBasically, you want your app to connect to Kafka in both cases right?\nwhen you're running your app locally (on the host machine)\nand when you're running it as a dockerized application.\nBut you're only advertising one listener for 127.0.0.1:9092 (the host machine) so even if a dockerized client (your app) can access Kafka container, it will still fail to establish a connection because of a misleading listener configuration.\nFor example, I can use this for advertising two different listeners for two different networks (docker network and localhost on the host machine):\nKAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092\nAnd when I run my dockerized application, it can connect to Kafka via broker:29092 and, similarly, when I run the app on the host machine, it can connect to Kafka via localhost:9092.\nThis post gives a more detailed explanation of how Kafka advertised listeners work and how should we configure them. It basically says:\nYou need to set advertised.listeners (or KAFKA_ADVERTISED_LISTENERS if you\u2019re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they\u2019ll try to connect to the internal host address\u2013and if that\u2019s not reachable then problems ensue.",
    "How to configure default environment to propagated to all containers?": "The correct way to configure default environment to propagate environment to containers is recorded in configure-the-docker-client:\nOn the Docker client, create or edit the file ~/.docker/config.json in the home directory of the user that starts containers. Add JSON similar to the following example. Substitute the type of proxy with httpsProxy or ftpProxy if necessary, and substitute the address and port of the proxy server. You can also configure multiple proxy servers simultaneously.\nYou can optionally exclude hosts or ranges from going through the proxy server by setting a noProxy key to one or more comma-separated IP addresses or hosts. Using the * character as a wildcard for hosts and using CIDR notation for IP addresses is supported as shown in this example:\n{\n \"proxies\":\n {\n   \"default\":\n   {\n     \"httpProxy\": \"http://192.168.1.12:3128\",\n     \"httpsProxy\": \"http://192.168.1.12:3128\",\n     \"noProxy\": \"*.test.example.com,.example2.com,127.0.0.0/8\"\n   }\n }\n}\nSave the file.\nWhen you create or start new containers, the environment variables are set automatically within the container.\nCheck the default environment after set config.json:\n~$ docker run --rm -it ubuntu:16.04 env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=6ca0f86604ea\nTERM=xterm\nHTTPS_PROXY=http://192.168.1.12:3128\nhttps_proxy=http://192.168.1.12:3128\nNO_PROXY=*.test.example.com,.example2.com,127.0.0.0/8\nno_proxy=*.test.example.com,.example2.com,127.0.0.0/8\nHTTP_PROXY=http://192.168.1.12:3128\nhttp_proxy=http://192.168.1.12:3128\nHOME=/root",
    "How to install VS Code extensions using code-server?": "Try this.\nRUN /app/code-server/bin/code-server \\\n    --install-extension EXTENSION_ID_1 \\\n    --install-extension EXTENSION_ID_2 \\\n    --install-extension EXTENSION_ID_3 \\\n    --extensions-dir /config/extensions",
    "How do I access docker from mac command line?": "If you have installed Docker Desktop and the application appears in applications, then open a new terminal window and type docker -version and you should receive output relating to the version of docker.\nMore information can be found on the Docker documentation,\nhttps://docs.docker.com/desktop/mac/install/",
    "Docker service failed to start": "Try to update your system, especially the packages related to Docker. Then start Docker.service to see if it runs successfully.",
    "Problems instantiating a Julia environment during a Dockerfile build process": "I'm posting an answer but its only partial, as I am in the midst of learning how to do the same thing. I'm having trouble understanding the dockerfile given, I think the approach might be complicating things and making it hard to find errors.\nHere is the approach (not compeletely documented) that has led to me getting a running julia image with packages installed and instantiated.\nBuild from a base image and install julia\nbased on the julia dockerfile at dockerhub\nthis makes a nice starting point for other julia workers based on various package combinations.\nI'm required to build from AL2 base instead of the usual public julia dockerfiles, thats the only reason to do this step separately.\nFROM this julia base image, run the Pkg.add for various packages, after which run instantiate\nbased on this discourse thread and links within\n# the important parts of dockerfile\nRUN julia -e 'using Pkg; Pkg.add(\"Pipe\", preserve=PRESERVE_DIRECT);'\nRUN julia -e 'using Pkg; Pkg.add(\"DataFrames\", preserve=PRESERVE_DIRECT);'\nRUN julia -e 'using Pkg; Pkg.add(\"CSV\", preserve=PRESERVE_DIRECT);'\n\nRUN set -eux; \\\n    mkdir \"$JULIA_USER_HOME\";\n\nRUN julia -e 'using Pkg; Pkg.instantiate();'\nThis post from bkamins@, \"My practices for managing project dependencies in Julia\" helped a lot, particularly the dependency preservation commands available in Pkg.add.\nSorry this is not a complete solution, I haven't completed the process yet myself but these are things I wish I had found on SO when I started.",
    "Configuring mongos router with docker": "The issue was resolved when I removed commands:\nprocessManagement:\n  fork: true\nMongo docs says that processManagement.fork enables a daemon mode that runs the mongos or mongod process in the background\nGuess we don't need something like that when running a docker container, since it's already a process",
    "Image's rootfs is incomplete while building from Dockerfile": "Solved it by adding \"default-runtime\": \"nvidia\" to /etc/docker/daemon.json. Further details here: https://github.com/dusty-nv/jetson-containers#docker-default-runtime",
    "Airflow Openshift installation with Dockerfile": "In this context there is one thing you have to be aware of when working with Openshift. By default Openshift runs containers with arbitrary user ids. Container images that are relying on fixed user ids may fail to start due to permission issues. Therefore please make sure your container images are built according to the rules described in\nhttps://docs.openshift.com/container-platform/4.6/openshift_images/create-images.html#images-create-guide-openshift_create-images.",
    "SSH private repos with docker and yarn": "You need to explicitely tell Yarn to use ssh.\nIn your package.json, replace:\n\"dependencies\": {\n    \"@my-organization/my-lib\": \"^0.1.0\",\n  },\nby\n\"dependencies\": {\n    \"@my-organization/my-lib\": \"ssh://git@github.com:my-organization/my-lib.git#^0.1.0\",\n  },\nand it will work as intended.",
    "When using BuildKit with Docker, how do I see the output of RUN commands?": "Have you tried --progress=plain?\nExample:\nDockerfile\nFROM alpine\nRUN ps aux\nbuild command:\nDOCKER_BUILDKIT=1 docker build --progress=plain -t test_buildkit .\nRelative output:\n#5 [2/2] RUN ps aux\n#5       digest: sha256:e2e4ae1e7db9bc398cbcb5b0e93b137795913d2b626babb0f148a60017379d86\n#5         name: \"[2/2] RUN ps aux\"\n#5      started: 2019-04-19 09:02:58.922035874 +0000 UTC\n#5 0.693 PID   USER     TIME  COMMAND\n#5 0.693     1 root      0:00 ps aux\n#5    completed: 2019-04-19 09:02:59.721490002 +0000 UTC\n#5     duration: 799.454128ms\n\ud83d\udc49 Also, check the very useful answer by @Cocowalla below about BUILDKIT_PROGRESS=plain",
    "Error with Dockerfile in Rust, x86_64-unknown-linux-musl": "I would start by running the base image only interactively and look for musl-g++. If not found, add the installation to the Dockerfile and try again.",
    "Dockerfile COPY does not work with wildcards": "I'm not able to reproduce your mcve (using busybox since it's smaller and I'm not on Windows). Here's the output of my commands showing that the COPY command works as expected:\n$ touch example.txt\n\n$ vi Dockerfile\n\n$ cat Dockerfile \nFROM busybox as build\nCOPY *.txt ./\n\nFROM busybox as release\nCOPY --from=build *.txt ./\n\n$ docker build -t test-63706476 .\n[+] Building 0.3s (7/8)\n => [internal] load .dockerignore                                                        0.0s\n => => transferring context: 2B                                                          0.0s\n => [internal] load build definition from Dockerfile                                     0.0s\n => => transferring dockerfile: 132B                                                     0.0s\n => [internal] load metadata for docker.io/library/busybox:latest                        0.0s\n => CACHED [build 1/2] FROM docker.io/library/busybox                                    0.0s\n => [internal] load build context                                                        0.0s\n => => transferring context: 32B                                                         0.0s\n => [build 2/2] COPY *.txt ./                                                            0.1s\n => exporting to image                                                                   0.0s\n => => exporting layers                                                                  0.0s\n => => writing image sha256:ea328f739e7e795efb5c54ca4018b0ab56b52ae09016775f58097a5f8e6  0.0s\n => => naming to docker.io/library/test-63706476                                         0.0s\n\n$ docker run -it --rm test-63706476 sh\n/ # ls\nbin          etc          home         root         tmp          var\ndev          example.txt  proc         sys          usr\n/ # exit",
    "Ubuntu docker image is not running in the detached mode": "There is a trick to prevent the container from dying: docker run -d my_image -d tail /dev/null -f\nThe command tail /dev/null -f keeps the container busy forever.",
    "Pass shell command results to Dockerfile from docker-compose": "Is something like this what you're after\n$groupid = getent group mygroup | cut -d: -f3\ndocker-compose up --build --build-arg GROUP_ID=$groupid\nor are you hoping to have the groupid command in the docker-compose script",
    "Docker compose Volume - Uploaded files": "Besides declaring the volume in the service you are using, you need to create another section in order to link in both ways the volume (from container to machine and from machine to pc)\nLike this:\nvolumes:\n      - database:/var/lib/postgresql/data\nvolumes:\n  database:",
    "Next.js with pkg. Syntax requires enabling one of the following parser plugin(s): 'flow, typescript'": "I removed unnecessary packages and the problem is gone =| (react-scripts, storybook). Also options is not required",
    "Why are SQL files in /docker-entrypoint-initdb.d being ignored?": "As was mentioned in the comments, the postgres docker container will only run SQL or shell scripts found in the /docker-entrypoint-initdb.d directory, it will not search recursively for files in subdirectories. Looking at your code, it seems you're probably aware of this, I believe you might have intended to copy deploy_schemas.sql to /docker-entrypoint-initdb.d/, rather than /docker-entrypoint-initdb.d/tables/.\nYou could change postgres/Dockerfile to\nADD /tables/ /docker-entrypoint-initdb.d/tables/\nADD deploy_schemas.sql /docker-entrypoint-initdb.d/\nor, if that's all you're doing in that Dockerfile, you can delete it and mount those files into the postgres container in the docker-compose file instead, like this:\n    postgres:\n        build: ./postgres\n        environment:\n            POSTGRES_USER: postgres\n            POSTGRES_PASSWORD: 1212\n            POSTGRES_DB: smart-brain-api-db\n            POSTGRES_HOST: postgres\n        ports:\n            - \"5431:5432\"\n        volumes:\n            - \"./postgres:/docker-entrypoint-initdb.d\"",
    "Angular 9 Dockerizing": "I don't have Chrome installed and I'm not running tests, but I had a similar issue to this while hitting the build step. I ended up using this Dockerfile and it worked.\nApparently, it doesn't like RUN ng build, but prefers RUN npm run build\nFROM node:12.16.1-alpine As builder\n\nWORKDIR /usr/src/app\n\nCOPY package.json package-lock.json ./\n\nRUN npm install\n\nCOPY . .\n\nRUN npm run build --prod\n\nFROM nginx:1.15.8-alpine\n\nCOPY --from=builder /usr/src/app/dist/SampleApp/ /usr/share/nginx/html\nYou'll have to replace \"SampleApp\" for your application.",
    "nodemon stuck on \"[nodemon] restarting due to changes...\" on Docker": "I had faces same issue on \"NODEMON\", And now i have a solution on Nodemon stuck on [NODEMON] restarting due to change....\nFollow this steps..\n1.go to this path..\nC/windows/system32.\nCopy this math and follow below steps..\nOpen **THIS PC**\n\nOn right click : **properties** \nThen go to : Advance System Settings\nNext click on : Environment Variables..\nNext : user variable for admin in this\nDouble Click on PATH\nNext : New\nAnd paste that copied path here.\nAnd click on next all OK..\nEnjoy Your NODEMON is on the Job ...",
    "install python fork in docker with pipenv": "Not sure what this issue was but one of our engineers provided this docker file and it works (same pipfile)\n# Base image\nFROM python:3.7 AS base\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nWORKDIR /code\n\nRUN pip install pipenv\n\n# Install dependencies\nRUN apt-get update && apt-get install -y \\\n    gdal-bin \\\n  && rm -rf /var/lib/apt/lists/*\n\nCOPY Pipfile Pipfile.lock ./\nRUN pipenv install --deploy\n\n# Dev image\nFROM base AS dev\n\n# Mount code as a volume to allow real time development\nVOLUME /code\nEXPOSE 8000\nEXPOSE 35729\n\nENTRYPOINT [\"pipenv\", \"run\"]\nCMD python manage.py livereload --host 0.0.0.0 --port 35729 & \\\n  python manage.py runserver 0.0.0.0:8000\nand compose file\nversion: \"3.4\"\n\nservices:\n  db:\n    image: postgis/postgis\n    environment:\n      - \"POSTGRES_HOST_AUTH_METHOD=trust\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data/\n  api:\n    build:\n      context: ./\n      target: dev\n    volumes:\n      - .:/code\n    ports:\n      - 8000:8000\n      - 35729:35729\n    depends_on:\n      - db\n\nvolumes:\n  postgres_data:",
    "How to correctly install custom package in Docker with docker-compose?": "You should never over-write PYTHONPATH like that, nstead append your path, or the system will not find the installed Python packages.\nYou can do either of the following to make it work:\nRUN export PYTHONPATH=\"$PYTHONPATH:/api\"\nENV PYTHONPATH=\"$PYTHONPATH:/api\"\nAlso your Dockerfile should be on api level, it wont be able to look for it in the present directory structure.",
    "How do I use a shell script to tell if a postgres database table exists": "my guess is that the if statement is true because you successfully connect to the db (and has nothing to do whether that connection finds the string or not)\nlike try this:\nif [[ $(PGPASSWORD=\"$PASSWORD\" psql -h \"db\" -U $USERNAME -d $DATABASE) ]]\nthen\n  echo connection successful\n  echo basing your previous if on:\n  echo $(PGPASSWORD=\"$PASSWORD\" psql -h \"db\" -U $USERNAME -d $DATABASE -c \"$SQL_EXISTS\")\nfi",
    "why do cp command get error without noting add -e option?": "i had the same error as you because i wrote the command as below\nsudo find /home/usersdata -type f -user mark -exec cp -p -parents {} /official ;\nthe parameters parents should --parents instead of -parents\nFor some reasons the error does not provide explicitly what is wrong.\nHope it helps you",
    "Run angular universal application in docker container": "We don't know what the issue is, but we found a workaround. The problem only occurs, when the container is build via the action defined in the github repo. As soon as we duplicated the repository (Create new Repo on github, Delete .git from project, Push to new repo), the new automatically build container just worked. Might be a caching issue, but because of limited time we are just happy that it works for now.",
    "Docker Container running.But page isnt working in localhost URL(Page didnt send any data)": "Everything seems fine in your Dockerfile and the Flask script, but I will suggest two thing.\nUpdate base image to python:3.7 as 2.7 will reach its end of life.\nRemove CMD [\"flask\", \"run\", \"--host=0.0.0.0\"] this as there is only one CMD per dockerfile.\nFROM python:3.7.4-alpine3.10 \nRUN pip install flask\nADD app.py /\nEXPOSE 8081 \nCMD [\"python\", \"app.py\", \"--host=0.0.0.0\"]\nNow run the container\ndocker run -it --name my_app --rm -p 8081:8081 dockerimage\nOpen the browser and hit\nhttp://localhost:8081/hello\nor\ndocker exec -it my_app ash -c \"apk add --no-cache curl && curl localhost:8081/hello\"\nOne of the above should work.\nIf you the second command work then something wrong with host configuration.",
    "Installing python package from source in Dockerfile": "You are quite close to it, it's fine to grab dependencies from other images in the same execution.\nTry the following Dockerfile:\nFROM ubuntu\nRUN apt-get update && apt-get install -y git\nRUN git clone -b master https://github.com/google/or-tools\nFROM python:3.6-alpine\nENV CELERY_BROKER_URL redis://redis:6379/0\nENV CELERY_RESULT_BACKEND redis://redis:6379/0\nENV C_FORCE_ROOT true\nWORKDIR /usr/src/app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\nCMD celery -A celeryExample worker --concurrency=1 --loglevel=info",
    "Dockerfile & installation -> Package 'openjdk-8-jre-headless' has no installation candidate": "Seems like you are using FROM python:3.8 as base and as mentioned by @masseyb in the comment under your question it does not have openjdk-8jre-headless package.\nThis may help you.\nFROM python:3.7-alpine as base\nRUN apk update \\\n&& apk upgrade \\\n&& apk add --no-cache bash \\\n&& apk add --no-cache --virtual=build-dependencies unzip \\\n&& apk add --no-cache curl \\\n&& apk add --no-cache openjdk8-jre\n\nRUN apk add --no-cache python3 \\\n&& python3 -m ensurepip \\\n&& pip3 install --upgrade pip setuptools \\\n&& rm -r /usr/lib/python*/ensurepip && \\\nif [ ! -e /usr/bin/pip ]; then ln -s pip3 /usr/bin/pip ; fi && \\\nif [[ ! -e /usr/bin/python ]]; then ln -sf /usr/bin/python3 /usr/bin/python; fi && \\\nrm -r /root/.cache\n\nRUN pip install --trusted-host pypi.python.org flask\nThis example Dockerfile can get you Java python and flask",
    "Docker-compose won't start MySQL if I define a volume": "I found on my Windows system that although my local volume (i.e ./db) looked empty it really wasn't. I wound up running ls -la from a bash shell on the same folder it showed a .sock file in it. Once I removed (rm -f *.sock) that and ran docker-compose up --build it seemed to mount the volume properly",
    "Getting \"Did you mean to run dotnet SDK commands? Please install dotnet SDK from\"": "Late answer, but pretty common issue so I'll answer it.\nThis usually happens if the dll targeting argument in your entrypoint doesn't exist. There are other possibilities if you're exporting, tar-ing and reimporting assets, but that's probably not what we're dealing with here.\nIn your case, the copy operation is pointing at a root relative file location. I assume that you're not publishing to {system root/}bin/Release/Publish, and your dockerfile is located in the root of your project.\nTo solve this, you should perform copy operations relative to the directory your dockerfile is in, and specify copy operations using a directory relative path (./etc).\nExample that works from my testing:\nFROM microsoft/dotnet:2.1-aspnetcore-runtime AS base\nWORKDIR /app\nCOPY ./bin/Release/Publish /app\nENTRYPOINT [\"dotnet\", \"WebApi.dll\"]\nTo see if it works, examine the resulting container by launching with a cmd override:\ndocker run -it --rm sam:latest /bin/sh\n...\npwd #ensure workdir is app\nls -la #ensure that your application is copied here\nIf you want to investigate deeper, you can use docker run to override your entrypoint and experiment with different launch configurations to see the output. The commands you'll need are as follows:\ndocker run -it --rm --entrypoint {dotnet executable} {image}:{tag} {non-existent dll}\nExample of output when targeting a non-existent dll via override on an otherwise working container:\n\u279c docker run -it --rm --entrypoint dotnet sam:latest weebles.dll\nDid you mean to run dotnet SDK commands? Please install dotnet SDK from:\n  https://go.microsoft.com/fwlink/?LinkID=798306&clcid=0x409",
    "Dockerfile: chmod on root directory not working": "To check the permission of your root folder bash inside your container, perform following opertations\ndocker exec -it container_id bash\ncd /\nls -ald",
    "Docker - installing vcredist_x86.exe inside a running container": "Could you please provide a dockerfile how to install vcredist inside docker.\nI tried different ways but no success. For example my dockerfile. It tries to install something but there are no libs and logs in the system\nFROM mcr.microsoft.com/windows/servercore:ltsc2019\n\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop';\n \n$ProgressPreference = 'SilentlyContinue'; $verbosePreference='Continue';\"]\n\nWORKDIR c:/temp\n\nADD http://download.microsoft.com/download/0/5/6/056dcda9-d667-4e27-8001-8a0c6971d6b1/vcredist_x64.exe c:/temp/vcredist_x64.exe \n\nRUN Start-Process -filepath C:/temp/vcredist_x64.exe -ArgumentList \"/install\", \"/passive\", \"/norestart\", \"'/log c:/temp/a.txt'\" -PassThru | wait-process",
    "WebdriverIO testing in docker": "As far as I know, this is possible. We used to have our test environment running in docker containers and the site is accessible in our local machine. Likewise, you may have to establish network connection between the containers.",
    "Install Windows service in Docker container": "To use the new user I had to add \".\\\" to the username (for local machine):\nRUN \"C:/Windows/Microsoft.NET/Framework64/v4.0.30319/InstallUtil.exe\" /username=.\\testuser /password=Stackoverflow1234! /LogToConsole=true /ShowCallStack myapp.exe",
    "Dockerfile, groupadd with GUI parameter": "Saved your dockerfile content in Dockerfile and touched a file with name helpers.\nThen build the image and enter into the image. Screen shot shows 2nd build, commit hash etc and id from the image. You can see all the commands in screen shot.\nYou may build as docker build -t pydebug1 . as well.",
    "\"make all -j\"$(nproc)\" command works within Docker container but not in Dockerfile": "I assume that's because shell expansion doesn't happen with the RUN statements. I just tried using backticks instead (i.e. make -j`nproc`) and it appeared to work",
    "installing scikit-learn Docker image problem": "I know it's a bit late but faced a similar issue with heroku (they also use linux environments). What you have to do is check the version of your local/dev environment and use that specific versions while deploying, even the python version.\nimport scipy\nimport sklearn\nimport numpy\n\nprint(scipy.__version__)\nprint(sklearn.__version__)\nprint(numpy.__version__)\nAdd this versions specifically to requirements.txt\nscipy==1.4.1\nscikit-learn==0.22.2.post1\nnumpy==1.19.5\nIn heroku to set the Python runtime, add a runtime.txt file to your app\u2019s root directory that declares the exact version number to use:\npython-3.7.10",
    "Create SQL Server database from a script in docker": "using following commands can solve your problem\ndocker-compose up --build -d\n\nversion: '3.4'\nservices: \n  sql.data:\n    image: ${DOCKER_REGISTRY}myfirst-mssql:latest\n    container_name: myfirst-mssql_container\n    environment:\n      SA_PASSWORD: ab873jouehxaAGR\n      ACCEPT_EULA: Y\nand after that:\ndocker exec myfirst-mssql_container sqlcmd \n  -d master \n  -S localhost\n  -U \"sa\"\n  -P \"ab873jouehxaAGR\"\n  -Q 'select 1'",
    "Debugging .NET Core Docker Container in VS Code": "I don't think the debugger (vsdbg) is working on the alpine images yet but it is reported to being close to working\nhttps://developercommunity.visualstudio.com/content/problem/256756/new-container-tools-in-158-cant-debug-alpine-docke.html\nhttps://github.com/OmniSharp/omnisharp-vscode/issues/2165",
    "Eureka Client with Docker Compose": "You are probably better off blanking out all the values for the environment variables in your docker-compose file and putting them in a .env file at the same level as your docker-compose file. The format of this file is key=value, one per line. That way you can re-use your compose file in different environments.\nThen do a docker-compose config to see how the environment variables are plugged into your docker-compose set up.\nI think the issue you have is you are missing hyphens. Instead of this:\nenvironment:\n   SPRING_CLOUD_CONFIG_USERNAME\n   SPRING_CLOUD_CONFIG_PASSWORD\n   SPRING_CLOUD_CONFIG_FAILFAST\nYou need something like this:\nenvironment:\n   - SPRING_CLOUD_CONFIG_USERNAME\n   - SPRING_CLOUD_CONFIG_PASSWORD\n   - SPRING_CLOUD_CONFIG_FAILFAST\nRunning docker-compose config after you've moved the values into .env will show if you've got it all working OK.",
    "Docker keep powershell environment changes": "So,\nI have found a workaround for this, basically you want to create a powershell profile at the start of the container, which will get run everytime the RUN command is used..\nTo do this, I did the following :\nRUN mkdir C:\\Users\\ContainerAdministrator\\Documents\\WindowsPowerShell ;\nRUN New-Item C:\\Users\\ContainerAdministrator\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 -Type file -Value 'Set-DNSClient -InterfaceIndex 5 -ConnectionSpecificSuffix my.dns.suffix ;'\nThis will make sure that everytime I run a command it will execute the Set-DNSClient on opening a powershell session.\nHope this helps someone.",
    "Python Luigi with Docker - Threading/Signal issue": "This happens because subscriber.subscribe starts a background thread. When that thread invokes luigi.build the exception is thrown.\nThe best solution here is to read pub-sub messages from the main thread using subscriber.pull. See example in the docs.",
    "can't access/download a generated folder that is inside a Docker container using REST API calls": "This is a little too late to help you, but for others about to give up on life, it doesn't work on docker because the path /Users/abeer/downloads/testD does not exist on the container, hence the 404\nYou need to tell the container to use that path by mapping the path to it using volumes, then it will work",
    "Running .NET framework console-app service on Docker": "From what I know you don't need the ip of the container, the ports have been projected on your localhost, you should get a response when you make REST calls to localhost:5000/localhost:13134",
    "Including docker socket in Dockerfile": "Possible duplicate of : Add bind mount to Dockerfile just like volume\nTL;DR:\nA Dockerfile is just a description of how an image is built. The act of binding a volume is host-specific, so it could be defined only when a container will instantiate this image.\nAs a substitute of passing the argument -v at docker run, you could also use a compose file to manage it :\nversion: '3'\nservices:\n  XXXX:\n    build: YYYYY\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock\"",
    "Dockerfile - How to set ENV variable up by reading content of local file in": "You can use\n--build-arg varname=value\nwhile building using docker build. So your dockerfile will contain\nARG RUBY_ARG\nand you can set the environment variable using ARG\nENV RUBY_ENV=$RUBY_ARG\nand the command will be:\ndocker build --build-arg RUBY_ARG=version .\nYou can use bash functionality to read from file and pass it as ARG.",
    "How to Set docker container ip to environment variable dynamically on startup?": "When you execute a script, any environment variable set by that script will be lost when the script exits.\nBut for both the cases you've posted above the environment variable should be accessible for the commands in your scripts, but when you enter the docker container via docker run you will get a new shell, which does not contain your variable.\ntl;dr Your exported environment variable will only be available to sub shells of the shell which set the variable. And if you need it when logging in you should source the ./bin/script file.",
    "External config in Spring Boot application within Dockerfile": "I think that you put the wrong path of metadata-server.yml\n\"spring.config.location=./config/metadata-server.yml\"\nI think that after:\nADD config/ /deploy/\nthe path of metadata-server.yml should be:\n\"spring.config.location=/deploy/config/metadata-server.yml\"",
    "How to add npm packages to docker images w/o remaking image or container?": "Take a look how ENTRYPOINT AND CMD works, if you want to use the same Dockerfile for production and development you can create a entrypoint script that if no arguments is provided run as production mode, if argument like development, run as development mode.\nsave development packages with --save-dev, so when running in development mode your container will install the dev dependencies.\nMake sure your node_modules is at your .dockerignore,\nYour dockerfile:\nFROM node:8.4.0\n\n\nENV HOME /var/www\nWORKDIR ${HOME}\n\nCOPY start_scrip.sh /start_scrip.sh\n\nCOPY server/package.json server/tsconfig.json server/nodemon.json $HOME/\nRUN npm install\nEXPOSE 8191\nENTRYPOINT [\"/start_scrip.sh\"]\nA start_scrip.sh example:\n#!/usr/bin/env bash\n\n\nfunction development {\n    npm install --dev # \n    npm start\n}\nfunction run_prod {\n    #npm start command here\n}\n\n\nif [ $# -gt 0 ]\nthen\n    if [ $1 == \"development\" ]\n    then\n        development\n    else\n        echo \"Not a valid argument\"\n        exit 127\n    fi\nelse\n    run_prod\nfi\n\n\nexit 0\nMake sure to chmod +x your script, so it can be executed inside the container.\nTo run in development mode:\nrun -p 8191:8191 -v $(pwd)/server/src:/var/www/src testImage development\nEvery time your developer add a new package in the workstation he should restart the container.",
    "Docker error connecting to host mysql": "Instead of using 0.0.0.0, 127.0.0.1 or localhost, you should use your host machine's IP. This is because each container is a individual node in the network.\nOr if you can inspect your MySQL container, and get the IP of it, you can use the IP as well, since they are on the same network.",
    "Building sphinx documents inside Docker container": "I believe because I am running the command a level up, since make html needs to be run from within docs/ and not from within the base directory.\nTo test this theory, could you try something like this command?\ndocker-compose run --rm sphinx bash -c \"cd docs; make html\"\nor possibly\ndocker-compose exec sphinx bash -c \"cd docs; make html\"",
    "Regarding not getting GNU awk version after creating a docker basic container": "There are a couple of gawk-installed docker images publicly available. You may directly use one of them. A few examples: atarumix/gawk5-alpine algas/gawk. I have to note that most (if not all) are not actively maintained.\nYou may create a Dockerfile and install gawk. Although you mentioned you do not want to have a separate install, it is pretty easy. Consider this for example:\nFROM alpine:latest\n\nRUN apk add --no-cache gawk ffmpeg\nIf you do not want to use that docker file, you may push the generated image to an image repository. After you push the image, you shall be able to use the image directly. Details of push can be found at push reference.",
    "Exclude folders in .dockerignore": "I was building the image with Gitlab CI, and I forgot that my git repository didn't have the folders that I was trying to COPY. Only my local development environment had the folders.",
    "Setting volumes in docker-compose.yml file for windows 10 home": "Try these lines in your docker-compose.yml file:\nvolumes:\n  - /c/Users/k/dev/angular-seed:/home/app/angular-seed",
    "Dockerfile - Defining an ENV variable with a dynamic value": "we can't do that, as that would be a huge security issue. Meaning you could run and environment variable like this\n ENV PATH $(rm -rf /)\nHowever, you can pass the information through a --build-arg (ARG) when building an image;\nARG DYNAMIC_VALUE \nENV PATH=${DYNAMIC_VALUE:-unknown}\nRUN echo $PATH\nand build an image with:\n> docker build --build-arg DYNAMIC_VALUE=$(dirname $(find /opt -name \"ruby\" | grep -i bin)):$PATH .\nOr, if you want to copy information from an existing env-var on the host;\n> export DYNAMIC_VALUE=foobar\n> docker build --build-arg DYNAMIC_VALUE .",
    "AntLR4: Write grammar for Dockefile": "I am not familiar with AntLR4, but it will be important for your grammar to include the multiple forms of some of the instructions.\nThe following instructions have two forms:\nRUN\nADD\nCOPY\nENTRYPOINT\nHEALTHCHECK\nThe following instruction has three forms:\nCMD\nRUN\nThe RUN instruction has two forms:\nRUN <command>                          # (shell form, the command is run in a shell\n                                       #  which by default is /bin/sh -c on Linux\n                                       #  or cmd /S /C on Windows)\n\nRUN [\"executable\", \"param1\", \"param2\"] # (exec form)\nADD\nThe ADD instruction has two forms:\nADD <src>... <dest>\nADD [\"<src>\",... \"<dest>\"] # (this form is required for paths containing whitespace)\nCOPY\nThe COPY instruction has two forms:\nCOPY <src>... <dest>\nCOPY [\"<src>\",... \"<dest>\"] # (this form is required for paths containing whitespace)\nENTRYPOINT\nThe ENTRYPOINT instruction has two forms:\nENTRYPOINT [\"executable\", \"param1\", \"param2\"] # (exec form, preferred)\nENTRYPOINT command param1 param2              # (shell form)\nHEALTHCHECK\nThe HEALTHCHECK instruction has two forms:\nHEALTHCHECK [OPTIONS] CMD command # (check container health by running a command inside the container)\nHEALTHCHECK NONE                  # (disable any healthcheck inherited from the base image)\nCMD\nThe CMD instruction has three forms:\nCMD [\"executable\",\"param1\",\"param2\"] # (exec form, this is the preferred form)\nCMD [\"param1\",\"param2\"]              # (as default parameters to ENTRYPOINT)\nCMD command param1 param2            # (shell form)\nFor more information and examples of each command see: https://docs.docker.com/engine/reference/builder/",
    "Docker Compose - Flyway - Unable to obtain Jdbc connection from DataSource": "Issue\nIn this line\nGRANT ALL PRIVILEGES ON base.* To 'user'@'localhost';\nyou grant priviledges to user on localhost but your service1 and flyaway-service-1 services are running in their own separate containers and therefore are not running on localhost (with respect to the mysqldb container). If these services were running inside the mysqldb container then maybe they could be running on localhost.\nPossible Resolution?\nYou may want to try using the hostname of the services/containers you're granting access to instead of localhost?\nie : GRANT ALL PRIVILEGES ON base.* To 'user'@'service1';\nDocker Networks instead of Container Links\nAlso, I would suggest using Docker's new networking feature instead of the legacy container linking method. See rationale behind docker compose \"links\" order",
    "Storing in an ENV the outcome of a git command": "As I understood after some researching it's not possible for now to define dynamic ENV in docker file. But you can do it on container running:\ndocker run --env TAG=\"$(git describe --tags)\" your-image",
    "Docker Compose - Not picking up the dump file in the init command?": "As @warmoverflow pointed out its seems to be bug in the docker, you have to remove all the volumes out and then it will work.\n//to remove all volumes (do this after every docker-composer up)\n$ docker-compose rm -vf",
    "Committing container changes of files generated in /workspace (even using makefile) doesn't persist into new instances of the image": "Docker keeps old build steps as a optimization (this is part of dockers strength).\nIf you change a Dockerfile and build, the new image will use cache for everything BEFORE the change and rebuild everything after it.\nRestoring the changed line in the Dockerfile will re-activate the cached build.\nYou can invalidate the cache by flags to docker build or you can add lines to your Dockerfile to remind you when you invalidated everything below that part last. For example adding/editing the following line will invalidate the cache below it.\nENV FORCED_DOCKER_REBUILD=2018-01-01",
    "Version increment for docker images": "I followed @aron digulla script and it's working fine now.\nIMAGE=TestImage\nVERSION=1.0.0\n\ndocker build -t ${IMAGE}:${VERSION} . | tee build.log || exit 1\nID=$(tail -1 build.log | awk '{print $3;}')",
    "Dockerfile; docker build volumes: changes to volume via ADD or COPY are not discarded": "I am not sure what you are trying to do here: VOLUME /tmp/space - this declares a mount point and maps /tmp/space on your container to a directory on the host\nADD /local/directory /tmp/space - I think you are attempting to copy a local directory from your container to your mounted volume\nRUN cp /tmp/space/* /opt/real/space/ - Are you trying to copy from your volume to your host?\nAfter adding a VOLUME directive in a dockerfile, what happens is that the folder in the container /tmp/space is mapped to a folder on the host, say /hosttmp/hostspace. You can find out what this is on the host by running the command -\n$ docker inspect -f {{.Volumes}} <your_container_name>\nIn order to prevent corruption of data in /hosttmp/hostspace, once a VOLUME is declared in the dockerfile, you cannot play around with the contents.\nI would recommend reading this article as it explains the rather confusing docker volume concept\nhttp://container-solutions.com/understanding-volumes-docker/",
    "angular2 app Dockerization": "FROM loansolutions/nginx-node:latest \n# Install and build the application \nCOPY . /usr/src/app \nWORKDIR /usr/src/app \nRUN npm install \nCOPY default.conf /etc/nginx/conf.d/ \nCMD [\"nginx\", \"-g\", \"daemon off;\"] \nThen docker build -t . Later docker run -d -p 8080:80 --name myapp myangularapp",
    "How to change VOLUME to COPY in Docker?": "Use inheritance\nCreate a base image that does not cover the COPY/VOLUME step and then make dev-, test- and production Dockerfiles that base on your base image.\nExample\nBase Dockerfile\nFROM ubuntu:14.04\nRUN echo test >> /important.txt\nThen build the base image so it may be referenced by the other Dockerfiles: docker build -t baseimage .\nDev Dockerfile\nFROM myrepo:baseimage\nVOLUME someVolume\nBuild and run.\nProd Dockerfile\nFROM myrepo:baseimage\nCOPY src dest \nBuild and run.\nInteresting\nVolumes will override conatiner files if used like that:\ndocke run -it -v ~/some.txt:/some/container/folder",
    "Multiple docker compose files, with env_file specified": "Personally what worked for me and am quiet happy is i have an empty docker-compose file at the root of all my projects that I need to cross reference between, and always set that as the first -f in all compose commands, and have changed all my compose files to use paths relative from the root of my solution/source where the first compose file lives.\n/src <- root\n/src/docker-compose.yml <- empty docker-compose file only specifying version\n/src/api/docker-compose.yml <- docker-compose file using paths relative from /src/\n/src/web/docker-compose.yml <- docker-compose file using paths relative from /src/\nI then have some batch/powershell files that invoke docker-compose such as:\n/** docker-compose-re-up.bat **/\ndocker-compose -f ../docker-compose.yml -f docker-compose.yml down\ndocker-compose -f ../docker-compose.yml -f docker-compose.yml build\ndocker-compose -f ../docker-compose.yml -f docker-compose.yml up -d",
    "In Docker, can I use the JSON format for entrypoint from the command line?": "The command-line parameters for the entrypoint need to come after the image name\nex: docker run --entrypoint /usr/bin/python2.7 image startup.py",
    "Fly.io deployment - secret env variables undefined": "It looks like you only need a runtime secret:\nprocess.env.BLOG_URL is a runtime secret.\nUse the fly secrets and the env var will be set/present.\nhttps://fly.io/docs/apps/secrets/",
    "EAI_AGAIN error while installing yarn packages with Docker and VPN": "Please try to append for example Google DNS inside docker /etc/resolv.conf like the following:\n# ... previous DNS servers\nnameserver 8.8.8.8",
    "Check for unused/uncessary packages in Docker": "Concerning the unsued image, you can use the command docker image prune.\nHere a link to the documentation that might help you.\nnabil@LAPTOP:~$ docker image help\n\nUsage:  docker image COMMAND\n\nManage images\n\nCommands:\n  build       Build an image from a Dockerfile\n  history     Show the history of an image\n  import      Import the contents from a tarball to create a filesystem image\n  inspect     Display detailed information on one or more images\n  load        Load an image from a tar archive or STDIN\n  ls          List images\n  prune       Remove unused images\n  pull        Pull an image or a repository from a registry\n  push        Push an image or a repository to a registry\n  rm          Remove one or more images\n  save        Save one or more images to a tar archive (streamed to STDOUT by default)\n  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\n\nRun 'docker image COMMAND --help' for more information on a command.",
    "How do I decrease pip install size for a docker image?": "One option you have is to set up multi-stage Docker builds for you application to produce separate images as they\u2019re used in development, staging, and production environments \u2014 with the requirements tailored to each.",
    "How can I serve a multi-module spring boot application in a single Docker Container?": "If you want to do this in one container, there are some ways to do that,\nYou can have 3 different spring modules built separately as you have already and run those 3 jars in different ports in the same image. So for this you will have to change the ports which are bound in the jar using maven as we are trying to run all 3 processes in 1 container so using 8080 ( default ) will result errors if you try to do so.\nserver.port=9090\nSomething like above having in application.properties will help. So after that you will have to run them manually inside the container or attach them all to the container startup script.\nEx :\nRUN java -jar hello-world-core.jar & java -jar hello-world-users.jar\nSo when you do this, the different jars will be serving on different ports which you mentioned in the pom files separately. So in-order to access them from outside, you will have to EXPOSE those ports as well.\nIf you really want these to be in one container, why did you make them separate in the first place ? and having 3 separate tomcats on them to execute ? I mean all 3 applications are having separate tomcats inside and that's why we are struggling to merge it to 1. So why not we escape those tomcats and build war files and then install a single tomcat inside the container and put all the wars into webapps folder ? Meaning you will have to put a tomcat to the docker and use it so that all the applications will be using the same tomcat and will be accessible via 8080.\nI believe in micro-services perspective, its good to have these separately and yes there are pros and cons doing that. And I also agree to the point which @khmarbaise made in his comment, skipping tests is not a good idea. :D",
    "Alpine Docker image FROM python:3.x-alpine3.x uses different package version for Python than stated": "If you remove the (virtual) .python-rundeps package beforehand, the installation will work as you intend it.\nFROM python:3.7-alpine3.9\n\nRUN \\\n    apk update \\\n    && apk del .python-rundeps \\\n    && apk add --repository=http://dl-cdn.alpinelinux.org/alpine/edge/main \\\n        python3-dev=3.7.3-r0\nBuilding this image results in:\n...\n(12/13) Installing python3 (3.7.3-r0)\n...",
    "How to expose nginx docker image to port other than 80?": "The second file is not a Dockerfile but a docker-compose.yml, you have to change in the docker-compose.yml the ports and it will be ok. The option -p \"hostport:containerport\" expose the port when you use command docker run. Anyway i suggest you to use the supported and official image before change too much the Image in the dockerfile.\nAnyway if you really need 8081 try something like this\nversion: \"3\"\nservices:\n  service-name-one:\n    image: yournginxOrSomethingelse\n    ports:\n      - \"8080:80\"\n      - \"8085:8081\"",
    "ERROR: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection-Docker with Wordpress": "Could able to fix this issue by adding Google DNS to Netwrok settings in Docker. Exact value is:\n8.8.8.8",
    "Not able to deploy war jboss/wildfly using docker file": "Issue resolved: It was happening due to not adding Deployment Scanner in standalone-full-poc.xml:\n<subsystem xmlns=\"urn:jboss:domain:deployment-scanner:1.0\">   <deployment-scanner scan-interval=\"5000\"      relative-to=\"jboss.server.base.dir\" path=\"deployments\" /></subsystem>",
    "Docker run NodeJS App throws error cannot find module": "Why You make nodejs image from ubuntu when there are already official ones.\nI see issue - You've overengineered You Dockerfile with unnecessary steps.\nFix Your Dockerfile and try again:\nFROM node:8-alpine3.9\n\nRUN apk update\nRUN apk add --no-cache g++ make python python-dev\nRUN rm -rf /var/cache/apk/*\n\nWORKDIR /app\n\nCOPY package*.json ./\n\nRUN npm i\n\nCOPY . .\n\nEXPOSE  9012\nCMD node app.js\nFrom last collaboration I found that it was a typo in code where You required file from not existing folder.\n\"module.js:550 throw err; ^ Error: Cannot find module '../dFarmUserService/controllers/ClientRegisterController' \nSo fixed it together :)",
    "Docker build does not remove temporary images when building a multi-stage docker file": "for image in $(docker images -f \"dangling=true\" -q)\ndo\n    docker rmi -f $image\ndone\nor docker images -q -f \"dangling=true\" | xargs docker rmi\nThe key here is the \"dangling=true\" filter, which shows exactly those intermediary images used during the building stage.",
    "How do I make django secrets available inside docker containers": "I have decided to go with using kubernetes / docker secrets to provide these solutions.\nI used a base settings file and then used specific ones for development and production that are loaded as part of the environment variables within the system.\nas an example the SECRET_KEY setting in the base.py looks like this\nSECRET_KEY = os.environ.get('SECRET_KEY')\nThen I use the following setting in the kubernetes deployment to call the settign out of the secret.\n- name: SECRET_KEY\n  valueFrom:\n    secretKeyRef:\n      name: sandbox-app-secret\n      key: SECRET_KEY",
    "Tagging Docker image (SemVer)": "Just to add a bit to the accepted answer. According to Semantic Versioning spec (https://semver.org/), the suffix -foo in version x.y.z-foo is considered a \"pre-release version\".\nBuild metadata, e.g. build number, should appended after any pre-release version label as +build, e.g. to denote build 33 use 1.2.3-beta1+33 or 1.2.3+33.\nUnfortunately, Docker tags cannot use the + character! Per the docker tag reference (emphasis mine):\nA tag name must be valid ASCII and may contain lowercase and uppercase letters, digits, underscores, periods and dashes. A tag name may not start with a period or a dash and may contain a maximum of 128 characters.\nThus, be aware, you cannot precisely follow semantic versioning and include build info in your docker tag. Such is life ...\nI will follow @gvilarino's advice and simply use a dash - to delimit the build number, with or without a pre-release label. E.g. for build 33 1.2.3-beta-33 or 1.2.3-33.",
    "Failed to create Dockerfile image /bin/sh: apt-get: not found": "As you are using alpine base image not ubuntu. So, the package manager for alpine is apk not apt or apt-get\nCommand should be\nRUN apk update && \\\n    apk add --no-cache nginx \n--no-cache option allows to not cache the index locally, which is useful for keeping containers small.\nRef:- https://www.cyberciti.biz/faq/10-alpine-linux-apk-command-examples/",
    "Changing timezone in Asp.Net Core container": "I think you need to set ENV in your Dockerfile:\nENV TZ=Europe/Berlin",
    "Docker container exited with code 0 after docker-compose up -d": "You can end with command like tail -f /dev/null\nI often use this directly in my docker-compose.yml with command: tail -f /dev/null. And it is easy to see how I keep the container running.",
    "Explore Docker's image files if container exits immediately?": "Start a container from desired image like this:\ndocker run -it --rm image_name bash\n-i Keeps STDIN open even if not attached\n-t Allocates a pseudo-tty\n--rm prunes stopped container after exit\nbash executes the specific command in the container. You can execute any valid command.\nExample docker run -it --rm centos:7 pwd outputs / (root directory).\nUpdate: In some cases, where image's entrypoint uses bash/sh -c format above command(docker run -it --rm image_name bash) will not work because bash will be treated as additional argument to image's orignal entrypoint.\nIn such case you can use the --entrypoint flag for achieving the same result:\ndocker run -it --entrypoint \"/bin/bash\" image_name",
    "Add file to docker container to be used during startup": "You can mount individual files into a container as a volume:\ndocker run -v `pwd`/conf.txt:/app/conf.txt my_app\nThat needs to be run on the docker host itself, or under a User directory on Windows/MacOS. You can also create a named volume in docker where you can store these files and update via temporary containers.\nWith those volume mounts, you do need to be careful with permissions, uid's inside the container may not match your uid on the host. You also have the ability to mount the files read only with a simple :ro appended to the volume mount.",
    "Http request between two docker networks": "For anyone struggling you can make requests like: http://host.docker.internal:8050",
    "Dockerfile from python:3.6-slim add jdk8": "You can download java tar.gz, unpack it and set environment variable. Below a sample of implementation in Dockerfile:\nFROM python:3.6-slim\n\nRUN apt-get update\nRUN apt-get install -y apt-utils build-essential gcc\n\nENV JAVA_FOLDER java-se-8u41-ri\n\nENV JVM_ROOT /usr/lib/jvm\n\nENV JAVA_PKG_NAME openjdk-8u41-b04-linux-x64-14_jan_2020.tar.gz\nENV JAVA_TAR_GZ_URL https://download.java.net/openjdk/jdk8u41/ri/$JAVA_PKG_NAME\n\nRUN apt-get update && apt-get install -y wget && rm -rf /var/lib/apt/lists/*    && \\\n    apt-get clean                                                               && \\\n    apt-get autoremove                                                          && \\\n    echo Downloading $JAVA_TAR_GZ_URL                                           && \\\n    wget -q $JAVA_TAR_GZ_URL                                                    && \\\n    tar -xvf $JAVA_PKG_NAME                                                     && \\\n    rm $JAVA_PKG_NAME                                                           && \\\n    mkdir -p /usr/lib/jvm                                                       && \\\n    mv ./$JAVA_FOLDER $JVM_ROOT                                                 && \\\n    update-alternatives --install /usr/bin/java java $JVM_ROOT/$JAVA_FOLDER/bin/java 1        && \\\n    update-alternatives --install /usr/bin/javac javac $JVM_ROOT/$JAVA_FOLDER/bin/javac 1     && \\\n    java -version",
    "Does \"COPY . /your/directory\" on Dockerfile Copy The Dockerfile It Self?": "Yes.\nCOPY . /usr/share/nginx/html\nAbove statement in Dockerfile will copy all the contents in your current directory on the docker host to /usr/share/nginx/html inside docker container.\nYou can use .dockerignore if you want anything to be ignored by docker while copying etc.\nIn case you just want static files to be copied, put them in a different directory and use it in Dockerfile -\nCOPY ./app /usr/share/nginx/html",
    "Docker: ENTRYPOINT can't execute command because it doesn't find the file": "ENTRYPOINT [ \"npm run watch\" ]\nThis is an incorrect json syntax, it's looking for the executable npm run watch, not the executable npm with parameters run and watch:\nWith the json syntax you need to separate each argument. You can use the shell syntax:\nENTRYPOINT npm run watch\nOr you can update the json syntax like (assuming npm is installed in /usr/bin):\nENTRYPOINT [ \"/usr/bin/npm\", \"run\", \"watch\" ]\nYou also have an incorrect volume definition:\nVOLUME ./:/usr/src/app\nDockerfiles cannot specify the how the volume is mounted to the host, only that an anonymous volume is defined at a specific directory location. With a syntax like:\nVOLUME /usr/src/app\nI've got strong opinions against using a volume definition inside of the Dockerfile described in this blog post. In short, you can define the volume better in a docker-compose.yml, all you can do with a Dockerfile is create anonymous volumes that you'd need to still redefine elsewhere if you want to be able to easily reuse them later.",
    "Unable to install pillow in docker container": "This should do it. You need to install the build dependencies, zlib-dev, jpeg-dev, gcc, and musl-dev\n# pull the official base image\nFROM python:3.9.6-alpine\n\n# set work directory\nWORKDIR /usr/src/app\n\n# set environment variables\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\n# install dependencies\nRUN apk add -u zlib-dev jpeg-dev gcc musl-dev\nRUN python3 -m pip install --upgrade pip\nCOPY ./requirements.txt /usr/src/app\nRUN pip install -r requirements.txt\n\n# copy project\nCOPY . /usr/src/app\n\nEXPOSE 8000\n\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]",
    "why I can't copy directory to my docker container? [duplicate]": "Using external files/folders during compilation\nYou can't do it because docker creators want it that way and as the creators, they have good reasons: simplicity and or security.\nDocker at build time, will only work with files or folders which are at the same folder of the Dockerfile.\nThere are some workarounds like launch the docker build... from one folder and specify the location of Dockerfile which is in another folder.\nCheck these answer to get more details:\nhttps://stackoverflow.com/a/53298876/3957754\nhttps://stackoverflow.com/a/24540011/3957754\nDon't use external files/folders\nAt build time, you should not depend of external folders or files. You must only need of a git repository with the source code of one application. This is common, widely used and devops compatible.\nAny other dependency required at build time, should be by good engineering:\nlibrary: In your case a nodejs/npm package public or private\ndocker base image: You could create another image with required files/folder and the use it a inheritance with FROM acme-base-nodejs or in docker multi-stage",
    "initialize postgres container from docker-compose file": "\"root\" execution of the PostgreSQL server is not permitted.\nYou should not run the DB container with root user. better to run postgres user.\none way is to specify the user in the docker-compose.\npostgres:\n  image: postgres\n  container_name: postgres\n  user: postgres\n  ports:\n    - \"5432:5432\"\n  command: 'postgres'\nBut agains\n  command: bash -c \"\n   postgres &&\n   createuser -l \\\"auser\\\"\n   \"\nduring the create user command, there might be the case that the DB contains is not ready to accept the connection.\nSo you have two best option.\nUsing Environment variables\nPOSTGRES_USER\nThis optional environment variable is used in conjunction with POSTGRES_PASSWORD to set a user and its password. This variable will create the specified user with superuser power and a database with the same name. If it is not specified, then the default user of postgres will be used.\npostgres:\n  image: postgres\n  container_name: postgres\n  environment:\n    POSTGRES_USER: test\n    POSTGRES_PASSWORD: password\n    POSTGRES_DB: myapp\n  user: postgres\n  ports:\n    - \"5432:5432\"\nThe second option\nInitialization scripts\nIf you would like to do additional initialization in an image derived from this one, add one or more *.sql, *.sql.gz, or *.sh scripts under /docker-entrypoint-initdb.d (creating the directory if necessary). After the entrypoint calls initdb to create the default postgres user and database, it will run any *.sql files, run any executable *.sh scripts, and source any non-executable *.sh scripts found in that directory to do further initialization before starting the service.\nFor example, to add an additional user and database, add the following to /docker-entrypoint-initdb.d/init-user-db.sh:\n#!/bin/bash\nset -e\n\npsql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" <<-EOSQL\n    CREATE USER docker;\n    CREATE DATABASE docker;\n    GRANT ALL PRIVILEGES ON DATABASE docker TO docker;\nEOSQL",
    "Creating a service failed with \"No command specified\"": "If you look into the Dockerfile https://hub.docker.com/r/flockers/ora2pg/dockerfile flockers/ora2pg the CMD or entrypoint is missing.\n ora2pg:\n  image: flockers/ora2pg\n  container_name: \"ora2pg_client\"\n  environment:\n  - DB_HOST=127.0.0.1\n  - DB_SID=xe\n  - ORA2PG_USER=MAX\n  - DB_PASS=MAX\n  volumes:\n  - ./ora2pg/export:/export\n  command: tail -f /dev/null\nSo here command: tail -f /dev/null it will just keep your container running and will not do anything, replace with your command.",
    "Cannot install php7-mongodb in alpine linux": "tldr: The package renamed to php7-pecl-mongodb in 3.9 and no longer exists in later releases\nThe package was renamed to php7-pecl-mongodb with this commit: https://git.alpinelinux.org/aports/commit/?id=b3d534d73c690baf458d9cda5dca5ee52ca9cafc\nBut the package was removed shortly after that due to a nonfree licence change by the package: https://git.alpinelinux.org/aports/commit/community/php7-pecl-mongodb/APKBUILD?id=8a901de31fa055ed591d487e12f8bb9ffcc0df21",
    "Working on user in dockerfile and installing packages on it permission denied": "It's because your lg user simply doesn't have necessary permissions. In this case, it doesn't matter that ubuntu is dockerized. It's like in any other Linux distro - you need permissions to do certain actions. An example: if you'd create a new user on your native system I bet command apt-get install X would raise the exact same error, wouldn't it?\nIn order to install anything, you'll need sudo to authenticate as root for this user. This can be achieved like so:\nFROM ubuntu:16.04\n\nRUN apt-get update && \\\n    apt-get -y install sudo\n\nENV user lg\n\nRUN useradd -m -d /home/${user} ${user} && \\\n    chown -R ${user} /home/${user} && \\\n    adduser ${user} sudo && \\\n    echo '%sudo ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n\nUSER ${user}\n\nWORKDIR /home/${user}\n\nRUN sudo apt-get -y install curl && \\\n    sudo apt-get -y install lsb-core && \\\n    sudo apt-get -y install lsb && \\\n    sudo apt-get -y upgrade -f\nA little explanation:\nFirst, you'll need to install sudo package\nAdd your user to sudo\nAnd you also need to add NOPASSWD to the sudoers file (I've done it for ALL but you can easily set it for a specific user). Without this, you will encounter following error: sudo: no tty present and no askpass program specified\nNow you can install stuff with this user\nAlso try avoiding using multiple times the same Dockerfile instruction (In your case you had redundant 4x RUN). Each instruction is a separate layer in later build image. This is known Dockerfile best practice.\nMinimize the number of layers In older versions of Docker, it was important that you minimized the number of layers in your images to ensure they were performant. The following features were added to reduce this limitation:\nIn Docker 1.10 and higher, only the instructions RUN, COPY, ADD create layers. Other instructions create temporary intermediate images, and do not directly increase the size of the build.",
    "Control startup order in Docker-Compose": "You can specify a healthcheck in your redis container and add condition: service_healthy to your depends_on field. This works since compose 2.1\nversion: \"2.1\"\nservices:\n  web:\n    build: .\n    ports:\n      - \"80:8000\"\n    depends_on:\n      \"db\":\n        condition: service_healthy\n    command: [\"python\", \"app.py\"]\n  db:\n    image: postgres\nDetailed example of usage is here: https://github.com/peter-evans/docker-compose-healthcheck/blob/master/docker-compose.yml",
    "Install PHP7 fpm and memcached with Docker": "We build the memcache extension from scratch when building our php7 container. Maybe our approached helps you or points you to the right direction. The documentation in the Dockerhub really seems to be faulty, tried pecl and it didn't work here either.\nSo this is how it looks in our Dockerfile:\nRUN apt-get update && apt-get install -y \n        libmemcached11 \\\n        libmemcachedutil2 \\\n        libmemcached-dev \\\n        libz-dev \\\n        git \\\n    && cd /root \\\n    && git clone -b php7 https://github.com/php-memcached-dev/php-memcached \\\n    && cd php-memcached \\\n    && phpize \\\n    && ./configure \\\n    && make \\\n    && make install \\\n    && cd .. \\\n    && rm -rf  php-memcached \\\n    && echo extension=memcached.so >> /usr/local/etc/php/conf.d/memcached.ini \\\n    && apt-get remove -y build-essential libmemcached-dev libz-dev \\\n    && apt-get remove -y libmemcached-dev libz-dev \\\n    && apt-get autoremove -y \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && apt-get clean",
    "python3 nodejs docker images": "You must choose one image and install everything on top of it. In your case I would do it like:\nDockerfile\nFROM node:6.4.0\nRUN apt-get update || : && apt-get install python -y\nRUN apt-get install python3-pip -y",
    "Dockerfile option that defaults to background running (detached)": "The purpose of Dockerfile is to build the image. How to run the image is controlled by \"docker run\" command. There is no option available in Dockerfile as per Docker documentation.",
    "how to use apt/sudo in alpine-based docker image": "another method of obtaining basic tools like sudo and curl?\nI believe alpine uses apk instead of apt\nTry something like this to install basic tools like curl and sudo\nRUN apk add curl sudo",
    "Dockerfile: Getting failed to compute cache key: \"/nginx.conf\" not found: not found": "This might happen when the file you want to COPY is also in the .dockerignore",
    "Vue Vite cannot connect to docker container": "I had the same problem and the below works for me.\nIn package.json, change the scripts\nFrom\n\"dev\": \"vite\"\nTo\n\"dev\": \"vite --host 0.0.0.0\"",
    "Error when building docker multi-stage image through minikube : file not found": "Okay I found the issue : it has nothing to do with my Dockerfile or missing files in minikube.\nTLDR at the end.\nSo the problem is that Docker does not find the /app/dist/MyAngularApp folder created during the first stage.\nThis folder is supposed to be created by the command npm run build:prod.\nWhen building with my machine, it works. When building via minikube, the folder is not created. No error or warnings. The command runs... and nothing.\nSomebody suggested using Yarn instead of Npm, to see if it shows some kind of error or warning. Bingo!\nyarn run build:prod shows : error Command failed with signal \"SIGKILL\".\nHaha! The build takes too much resources and gets killed.\nI upped the memory on minikube with :\nminikube delete;\nminikube start --memory 4096\nAnd now it works fine!\nI don't know why Yarn shows the error and Npm does not, even with --verbose.\nTLDR; Insuficient memory on minikube caused it to SIGKILL my npm build. No build, no resulting folder, hence the COPY error. No error shown when using npm. Switched to Yarn and it shows the SIGKILL error.",
    "Difference between Docker Build and Docker Run": "The docker build command creates an immutable image. The docker run command creates a container that uses the image as a base filesystem, and other metadata from the image is used as defaults to run that image.\nEach RUN line in a Dockerfile is used to add a layer to the image filesystem in docker. Docker actually performs that task in a temporary container, hence the selection of the confusing \"run\" term. The only thing preserved from that RUN command are the filesystem changes, running processes, changes to environment variables, shell settings like the current working directory, are all lost when the temporary container is cleaned up at the completion of the RUN command.\nThe ENTRYPOINT and CMD value are used to specify the default command to run when the container is started. When both are defined, the result is the value of the entrypoint is run with the value of the cmd appended as a command line argument. The value of CMD is easily overridden at the end of the docker run command line, so by using both you can get easy to reconfigure containers that run the same command with different user input parameters.\nIf the command you are trying to run needs to be performed every time the container starts, rather than being stored in the immutable image, then you need to perform that command in your ENTRYPOINT or CMD. This will add to the container startup time, so if the result of that command can be stored as a filesystem change and cached for all future containers being run, you want to make that setting in a RUN line.",
    "docker-compose with multiple postgres databases from sql dumps": "UPDATE\nAfter a lot of fiddling around I got it to work. As @Iarwa1n suggested, you map one db as such \"5432:5432\" and the other as such \"5433:5432\". The error I encountered was due to how I was calling postgres from the application itself. It is important to realize the postgres host is not localhost anymore, but whatever name you gave your database service in docker-compose.yaml. In my case; db for backend and login_db for the login service. Additionally, I had to change my driver from postgresql to postgres \u2013 not sure why this is...\nAs such, my db_url ended up looking like this from within my python backend app:\npostgres://ludo:password@db:5432/main_db\nAnd defined in this way:\nDATABASE_CONFIG = {\n    'driver': 'postgres',\n    'host': 'db',\n    'user': 'ludo',\n    'password': 'password',\n    'port': 5432,\n    'dbname': main_db\n}\ndb_url = '{driver}://{user}:{password}@{host}:{port}/{dbname}'.format(database_config)\nTwo things to note:\n1) Regardless of how you mapped your ports, you always have to connect to postgres default port 5432 from within your app\n2) If you're using the requests python library (as was I) then make sure to change the url appropriately as well. For example I had a ppt service I was calling via the requests library and I had to change the url to: 'http://ppt:6060/api/getPpt' instead of 'http://localhost:6060/api/getPpt'",
    "Dockerfile COPY files not showing on VOLUME": "You need to move your VOLUME instruction down below your final ADD instruction (I'd suggest right above that CMD instruction) -- once a directory is defined as a VOLUME, it is essentially \"snapshotted\" at that point, and created empty and fresh for every additional container/layer, so any files added after that point will be obscured/ignored.\nIt's also worth noting that you don't need to explicitly define the VOLUME inside the Dockerfile in order to use the bind-mount form of -v at runtime, so you probably don't technically need to have the docker_stack directory in a VOLUME in the Dockerfile at all.",
    "Skaffold: Cannot connect to the Docker daemon on Docker Desktop for macOS": "The solution was to set the variable DOCKER_HOST before launching the skaffold dev command:\nDOCKER_HOST=\"unix:///Users/<you>/.docker/run/docker.sock\" skaffold dev",
    "Dockerfile does not receive environment variables": "When you're building an image, only the contents of the build: block are available. The environment: block is not available inside the Dockerfile, nor are volumes: mounts or networks: (including the automatic networks: [default]).\nIn principle you can make your Dockerfile work by declaring those parameters as ARG in the Dockerfile, and passing them in build: { args: } in the Compose file.\nHowever, \"which user is running this container\" isn't something you usually want to build into your image \u2013 imagine having to rebuild tools from source whenever someone has a different user name or user ID. You can use the Compose user: directive to cause the container to run as a different user ID that you select at run time. For purposes of sharing files, this often needs to be a numeric user ID (the output of id -u). You do not need to do any setup in the image for this. The user won't exist in the container's /etc/passwd file, but the only consequence of this is usually a cosmetic complaint in some interactive shell prompts.\nversion: \"2.2\"\nservices:\n    app:\n        build: .\n        user: 1000\n        # volumes:\n        #     - ./app_data:/data\nIn the Dockerfile, I'd suggest setting up a single dedicated directory to hold your application's writeable data (if that's required at all). It's good practice to create a non-root user, but it doesn't need a specific uid. Leave most files owned by root and not writeable by other users; do not RUN chown or RUN chmod. The volumes: mount shown above will replace the container directory with the host directory, including its (numeric) ownership.\nFROM alpine\n\n# Create the non-root user (BusyBox adduser syntax)\nRUN adduser -S -D -H nonroot\n\n# ... do the normal things to install and build your application ...\n# (still as root; do not chown the files)\n\n# Create the data directory\nRUN mkdir /data && chown nonroot /data\n\n# Switch to the non-root user only to run the actual container\nUSER nonroot\nCMD [\"the_program\"]",
    "how to perform cron jobs every 5 minutes inside docker": "Updated Docker File to run both every minute and every 5 minutes\nFROM alpine:latest\n\n# Install curlt \nRUN apk add --no-cache curl\n\n# Copy Scripts to Docker Image\nCOPY reminders.sh /usr/local/bin/reminders.sh\nCOPY feeds.sh /usr/local/bin/feeds.sh\n\n# Add the cron job\n\nRUN echo ' *  *  *  *  * /usr/local/bin/feeds.sh' >> /etc/crontabs/root\nRUN echo ' */5  *  *  *  * /usr/local/bin/reminders.sh' >> /etc/crontabs/root\n\n# Run crond  -f for Foreground \nCMD [\"/usr/sbin/crond\", \"-f\"]",
    "Should I Set docker image version in docker-compose?": "Including a version number as you've done is good practice. I'd generally use a major-only image tag (mongo:4) or a major+minor tag (mongo:4.4) but not a super-specific version (mongo:4.4.10) unless you have automation to update it routinely.\nGenerally the Docker Hub images get rebuilt fairly routinely; but, within a given patch line, only the most-recent versions get patches. Say the debian:focal base image gets a security update. As of this writing, the mongo image has 4, 4.4, and 4.4.10 tags, so all of those get rebuilt, but e.g. 4.4.9 won't. So using a too-specific version could mean you don't get important updates.\nConversely, using latest means you just don't care what version you have. Your question mentions mongo:4.0 but mongo:latest is currently version 5.0.5; are there compatibility issues with that major-version upgrade?\nThe key rules here are:\nIf you already have some image:tag locally, launching a container will not pull it again, even if it's updated in the repository.\nMinor-version tags like mongo:4.4 will continue to get updates as long as they are supported, but you may need to docker-compose pull to get updates.\nPatch-version tags like mongo:4.4.9 will stop getting updates as soon as there's a newer patch version, even if you docker pull mongo:4.4.9.\nUsing a floating tag like ...:latest or a minor-version tag could mean different systems get different builds of the image, depending on what they have locally. (Your coworker could have a different mongo:latest than you; this is a bigger problem in cluster environments like Kubernetes.)",
    "Can Dockerfile commands be lowercase? [closed]": "Yes, you can write instructions in lowercase.\nIt is just a recommendation/notion so that Dockerfile instructions can be differentiated from the commands/arguments that you wish to run in that Dockerfile.",
    "How to start two services in one docker container": "You can use supervisor tools for managing multiple services inside a single docker container.\nCheck out the below example(running Redis and Django server using single CMD):\nDockerfile:\n# Base Image\nFROM alpine\n\n# Installing required tools\nRUN apk --update add nano supervisor python3 redis\n\n# Adding Django Source code to container \nADD /django_app /src/django_app\n\n# Adding supervisor configuration file to container\nADD /supervisor /src/supervisor\n\n# Installing required python modules for app\nRUN pip3 install -r /src/django_app/requirements.txt\n\n# Exposing container port for binding with host\nEXPOSE 8000\n\n# Using Django app directory as home\nWORKDIR /src/django_app\n\n# Initializing Redis server and Gunicorn server from supervisors\nCMD [\"supervisord\",\"-c\",\"/src/supervisor/service_script.conf\"]\nservice_script.conf file\n## service_script.conf\n\n[supervisord]  ## This is the main process for the Supervisor    \nnodaemon=true  ## This setting is to specify that we are not running in daemon mode\n\n[program:redis_script] ## This is the part where we give the name and add config for our 1st service\ncommand=redis-server  ## This is the main command to run our 1st service\nautorestart=true ## This setting specifies that the supervisor will restart the service in case of failure\nstderr_logfile=/dev/stdout ## This setting specifies that the supervisor will log the errors in the standard output\nstderr_logfile_maxbytes = 0\nstdout_logfile=/dev/stdout ## This setting specifies that the supervisor will log the output in the standard output\nstdout_logfile_maxbytes = 0\n\n## same setting for 2nd service\n[program:django_service] \ncommand=gunicorn --bind 0.0.0.0:8000 django_app.wsgi\nautostart=true\nautorestart=true\nstderr_logfile=/dev/stdout\nstderr_logfile_maxbytes = 0\nstdout_logfile=/dev/stdout\nstdout_logfile_maxbytes = 0\nFinal output: Redis and Gunicorn service in same docker container\nYou can read my complete article on this, the link is given below: Link for complete article",
    "Do intermediate Docker images occupy disk space?": "Yes intermediate layers occupy disk space and it is a good thing usually. This facilitates re-use of layers and speedy builds. What you should be concentrating on instead is to reduce the number of layers by optimising the dockerfile. Your final docker image is actually a combination of all the layers. So you cannot remove the layers unless you remove the final image and no other image is using the layers.\ndocker build --rm does not save any extra disk space. To understand why, you should know how docker build works - Each instruction (e.g., RUN) in a dockerfile starts a new container, after the instruction completes, the container exits, and is committed to an image.\ndocker build --rm removes these intermediate containers. --rm option is true by default and so docker build --rm has not extra affect compared to docker build. For some reason if you want to keep the intermediate containers then you can turn it off with --rm=False.\nIf there are any layers which are not being used by any other images, you can remove them. These are called dangling layers. You can remove them with following command -\ndocker rmi $(docker images -f \"dangling=true\" -q)",
    "Docker Alpine, Celery (worker and beat) fail with PermissionError when using non-root user": "You should do like that\nRUN mkdir -p /var/log/celery/ /var/run/celery/\nRUN useradd -G root celery && \\\n    chgrp -Rf root /var/log/celery/ /var/run/celery/ && \\\n    chmod -Rf g+w /var/log/celery/ /var/run/celery/c && \\\n    chmod g+w /etc/passwd\n\n...\nRUN chmod a+x /start.sh\nUSER celery\nENTRYPOINT [\"/start.sh\"]\nYou should create user celery firsts. Then, add this user into group root. After that you need set write permission for this folder you need to put logs and /etc/passwd. You also need to have one script to add your user into /etc/passwd\n#!/bin/bash\n#\nif [ `id -u` -ge 10000 ]; then\n    echo \"celery:x:`id -u`:`id -g`:,,,:/home/web:/bin/bash\" >> /etc/passwd\nfi",
    "sdkman does not install java in a dockerfile": "For sdk command to be available you need to run source sdkman-init.sh.\nHere is a working sample with java 11 on centos.\nFROM centos:latest\n\nARG CANDIDATE=java\nARG CANDIDATE_VERSION=11.0.6-open\n\nENV SDKMAN_DIR=/root/.sdkman\n\n# update the image\nRUN yum -y upgrade\n\n# install requirements, install and configure sdkman\n# see https://sdkman.io/usage for configuration options\nRUN yum -y install curl ca-certificates zip unzip openssl which findutils && \\\n    update-ca-trust && \\\n    curl -s \"https://get.sdkman.io\" | bash && \\\n    echo \"sdkman_auto_answer=true\" > $SDKMAN_DIR/etc/config && \\\n    echo \"sdkman_auto_selfupdate=false\" >> $SDKMAN_DIR/etc/config\n\n# Source sdkman to make the sdk command available and install candidate\nRUN bash -c \"source $SDKMAN_DIR/bin/sdkman-init.sh && sdk install $CANDIDATE $CANDIDATE_VERSION\"\n\n# Add candidate path to $PATH environment variable\nENV JAVA_HOME=\"$SDKMAN_DIR/candidates/java/current\"\nENV PATH=\"$JAVA_HOME/bin:$PATH\"\n\nENTRYPOINT [\"/bin/bash\", \"-c\", \"source $SDKMAN_DIR/bin/sdkman-init.sh && \\\"$@\\\"\", \"-s\"]\nCMD [\"sdk\", \"help\"]",
    "Docker nodejs mongodb": "Double-check your Dockerfile.\n\"npm\" is different from \u201cnpm\u201d, notice the double quote \" and \u201c. You should always use \" (input from your keyboard) rather than \u201c\nthen run the following command:\ndocker-compose up --build",
    "Can't access a volume during building a docker image": "Sorry, I have just found out that volumes are not accessible during build. They are accessible during run of the container, which is said here in the point 9. But when I changed my Dockerfile to this:\n# Dockerfile\nFROM ubuntu:14.04\nCMD [\"ls\", \"/var/example\"]\n... it worked perfectly well and printed out all the files inside the example folder.",
    ".Net Core on Heroku with Docker": "Here's another way you could make this work with Heroku.\nCreate a Dockerfile in the root of your solution\n#https://github.com/dotnet/dotnet-docker/tree/master/samples/aspnetapp\n\nFROM microsoft/dotnet:2.1-sdk AS build\nWORKDIR /app\n\nCOPY . .\n\nCMD ASPNETCORE_URLS=http://*:$PORT dotnet [THE NAME OF YOUR FILE].dll\nCreate a simple batch file (Assumed Windows OS) called \"publish.bat\"\nNOTE - When dotnet clean is ran, it doesn't clean out the publish folder. I recommend you delete the contents of the directory before publishing. You can add it to the batch file as you see fit.\nREM - This file assumes that you have access to the application and that you have docker installed\nREM : Setup your applications name below\nSET APP_NAME=\"\"\n\nREM - Delete all files and folders in publish\ndel /q \".\\bin\\Release\\netcoreapp2.1\\publish\\*\"\nFOR /D %%p IN (\".\\bin\\Release\\netcoreapp2.1\\publish\\*.*\") DO rmdir \"%%p\" /s /q\n\ndotnet clean --configuration Release\ndotnet publish -c Release\ncopy Dockerfile .\\bin\\Release\\netcoreapp2.1\\publish\\\ncd .\\bin\\Release\\netcoreapp2.1\\publish\\\ncall heroku container:login\ncall heroku container:push web -a %APP_NAME%\ncall heroku container:release web -a %APP_NAME%\nFrom the root of your solution, now just run publish.bat\nc:\\dev\\my-amazing-app\\publish.bat\nInformation:\nHeroku - Docker Instructions: https://devcenter.heroku.com/articles/container-registry-and-runtime",
    "How to know when docker CMD is done?": "The container stops and exits once the CMD has finished running.\nYou can use:\n$ docker wait [container name/id]\nto wait on a container to stop. If the container is already stopped, this command will return immediately. Otherwise, it'll wait until the container finishes its work, or is otherwise stopped.\nFrom https://docs.docker.com/engine/reference/commandline/wait/\nBlock until one or more containers stop, then print their exit codes",
    "Docker - Run an HTML file in docker": "You need a service to expose the HTML (apache or nginx). You can simply run $ docker run --name some-nginx -v /some/content:/usr/share/nginx/html:ro -d nginx, where some/content links to your HTML file. Here's a link to the official Nginx docker: https://hub.docker.com/_/nginx/",
    "App running in Docker container on port 4567 can't be accessed from the outside": "Sinatra was binding to the wrong interface. Fixed by adding the -o switch.\nCMD ruby hei.rb -p 4567  -o 0.0.0.0",
    "Yum install won't work on a boot2docker host?": "If you look bit earlier than the last message, you have a good chance to see something like this:\nTotal download size: 24 M\nInstalled size: 32 M\nIs this ok [y/d/N]: Exiting on user command\nYour transaction was saved, rerun it with:\nwhich means you have to change the default choice, e.g.\n#Install Apache\nRUN yum install -y httpd",
    "How does Docker name repositories?": "From the docs:\nCOMPOSE_PROJECT_NAME\nSets the project name, which is prepended to the name of every container started by Compose. Defaults to the basename of the current working directory.\nBTW, this is not docker, but docker-compose that is deciding on the name. To change it, set the COMPOSE_PROJECT_NAME before running docker-compose:\nCOMPOSE_PROJECT_NAME=myprefix docker-compose up",
    "Gpg hangs when listing keys (docker install)": "Removing the lock file did the trick.\nSnippet from my Dockerfile\nKey Import\nRUN gpg --batch --import path_to_key.pub\nRemoving the lock file\nRUN rm -f /home/my_user/.gnupg/public-keys.d/pubring.db.lock\nAfter each key import, you might have to remove the lock file.",
    "ERROR: JAVA_HOME is set to an invalid directory: /usr/lib/jvm/java-8-openjdk-amd64": "I'd suggest to use another base image. Java 11 is required to build for newer API levels:\nFROM openjdk:11-jdk as builder\n...\nAnd then install Python3 and AWS CLI.\nWorking example: cloudbuild-android.\nOr if you want to continue with your's, RUN which java would tell you where it's actually installed.",
    "Docker: python has no installation candidate": "Docker is just Linux. When some apt-get install acme fails, almost always is due to linux, not docker.\nTo find the solution the first step is to replicate the error in a clean or new linux machine.\nReplicating your issue\nTo replicate your error in a clean linux I created an empty linux with: docker run -it ubuntu:latest\nThen, inside the container I ran your apt-get update && apt-get install python -y. I got your error:\nSolution\nIn this specifically case, since you are using FROM ubuntu:latest, in this year(2023) the latest ubuntu image don't allow by default python2\nSo, I if you try with apt-get install python3 -y, it will work. Finally your Dockerfile should be:\nFROM ubuntu:latest \nWORKDIR /app \nADD . /app \nRUN apt-get update && apt-get install python3 -y \nCMD python3 /app/main.py \nLABEL color=red\nOlder Python\nIf your code needs old python version, you should not use FROM ubuntu:latest because in the latest version of ubuntu, only python3 is allowed by default.\nIn case you need python2, you should research on internet one of these options:\nThe exact steps to install python2 on latest ubuntu (20 or 22). Then put these steps in your Dockerfile\nHow do I setup only python 2.7 in a docker container?\nSearch some docker image with yout desired python version\nhttps://hub.docker.com/layers/python/library/python/2.7.18-slim-stretch/images/sha256-a0b3c65a15ba08138460de9fd2267b8dec30ed98407a0edac0adc0ccbe809cad?context=explore\nhttps://github.com/Docker-Hub-frolvlad/docker-alpine-python2",
    "Problem with run node prune in docker file": "Use below lines:\nRUN curl -sf https://gobinaries.com/tj/node-prune | sh\n\nRUN node-prune /usr/src/app/node_modules",
    "Dockerfile build fails with source-command not found": "The shell which is used by RUN is /bin/sh. /bin/sh does not provide the source command. Try the .-command instead of source.",
    "Creating an empty directory to docker container": "You could create it with :\nRUN mkdir -p /path/to/my/myemptydir\n-p allows to make intermediate directories.",
    "ERROR: .PermissionError: [Errno 13] Permission denied: './docker-compose.yml'": "This permission issue is because by default docker daemon runs as root, and you are running as non-root user.\nTo fix the permission error, use following commands:\n sudo groupadd docker\n sudo usermod -aG docker $USER\nThen logout and login again or you can simply run the following command to make the changes instantly\nnewgrp docker \nLearn more about it on Post-installation steps for Linux\nThanks",
    "fast api and nginx in docker-compose shows connect() failed (111: Connection refused) while connecting to upstream, client: 172.27.0.1, server:": "At quick glance, I think it's because you've bound uvicorn to 127.0.0.1, therefore, you'd need an additional reverse proxy in api container in order to serve it outside (unless your container runs in a host network, but by default it's a bridge).\nBinding uvicorn to 0.0.0.0 should fix this issue. Being more specific:\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\" ,\"--port\" ,\"8000\"]\nTo confirm this, I've run my fastapi container in both 0.0.0.0 and 127.0.0.1:\n$ podman ps --no-trunc\nCONTAINER ID                                                      IMAGE                                 COMMAND                                  CREATED        STATUS            PORTS                   NAMES\n7121420b401ee1803474ff156280a5b5b154c55ae0fd47e1976d8fe9e5331c72  localhost/fastapi-mvc-template:test   /usr/bin/fastapi serve --host 127.0.0.1  4 minutes ago  Up 4 minutes ago  0.0.0.0:8000->8000/tcp  test\ndd35d89f161f818e5b34c138bd3d42bdb4d84100b76999d4fc2444c7d0d1a1d9  localhost/fastapi-mvc-template:0.1.0  /usr/bin/fastapi serve --host 0.0.0.0    2 minutes ago  Up 2 minutes ago  0.0.0.0:9000->8000/tcp  test_ok\n$ curl localhost:8000/api/ready\ncurl: (56) Recv failure: Connection reset by peer\n$ curl localhost:9000/api/ready\n{\"status\":\"ok\"}",
    "Why cannot access a running Docker container in browser?": "This statement,\nEXPOSE 8089\nwill only expose your port for inter-container communication, but not to the host.\nFor allowing host to communicate on the container port you will need to bind the port of host and container in the docker run command as follows\ndocker run -p <HOST_PORT>:<CONTAINER:PORT> IMAGE_NAME\nwhich in your case will be\ndocker run -p 8089:8089 IMAGE_NAME",
    "cannot open file using dockerfile golang": "In the line COPY --from=builder /build/main /app/ you only copy the executable. You do not copy the app directory. Replace that line with COPY --from=builder /build /app/\nBelow I have included an example of how I would write this Dockerfile. Here are some key changes.\nDefine WORKDIR before COPY instructions so you do not have write directory names twice. This is helpful because if you want to change your app directory, you only need to change it once (and that means smaller chance of bugs).\nUse COPY instead of ADD. The Dockerfile reference indicates that COPY is preferred.\nYou do not have to use RUN mkdir /build before using a WORKDIR or COPY. Those last two instructions will make the directory if it does not exist already.\nFROM golang:alpine as builder\nWORKDIR /build\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-extldflags \"-static\"' -o main .\nFROM scratch\nWORKDIR /app\nCOPY --from=builder /build .\nCMD [\"./main\"]",
    "npm ERR! request to https://registry.npmjs.org during Dockerizing a Node.js web app": "if someday any have this problem you can resolve with this docker build . --network host -t mytag .. or another suggestion suggestion you can check the following link https://github.com/StefanScherer/dockerfiles-windows/issues/270",
    "/bin/sh: jlink: not found. command '/bin/sh -c jlink' returned a non-zero code: 127": "The simple answer is jlink is not on the PATH so can't be found.\nIf you change the RUN line to\nRUN /usr/lib/jvm/zulu11/bin/jlink\nthen it can be found.\nHowever, you still have an error using the wildcard in the module path. Change this to\n--module-path /usr/lib/jvm/zulu11/jmods/\nand the docker command will complete successfully.",
    "Local volume includes invalid characters": "Interestingly adding only this didn't work either, / here \"/$(pwd)\"\nvolume: mounts a managed volume into the container.\nbind: bind-mounts a directory or file from the host into the container.\nfor more details on mount types - https://docs.docker.com/engine/reference/commandline/service_create/#add-bind-mounts-or-volumes\nSo you need to explicitly add the mount type to bind for mounting a directory.\ndocker run -it --mount type=bind,source=\"/$(pwd)\",target=/root ubuntu:18.04 /bin/bash\nroot@eda980649055:/# cd /root\nroot@eda980649055:~# ls\nJenkinsfile.migrate  LICENSE.txt  README.md  pom.xml  src  target",
    "fatal error: *.h: No such file or directory. While running docker build command to create image for python project": "Install this apt install libgmp-dev libmpfr-dev libmpc-dev extra dependency and then RUN pip install -r requirement.txt i think it will work and you will be able to install all the dependency and build docker image.\nFROM python:3\n\nCOPY . .\n\nRUN apt-get update -qq && \\\napt-get install -y --no-install-recommends \\\nlibmpc-dev \\\nlibgmp-dev \\\nlibmpfr-dev\n\nRUN pip install -r requirement.txt\n\nCMD [ \"python\", \"./mike/main.py\" ]\nif apt not run you can use Linux as base image.",
    "Adding Linux utilities to docker image based on busybox": "If the just-Busybox Docker base image isn't meeting your needs, you can change your Dockerfile to be based on a more full-featured Linux distribution. FROM ubuntu is very common and includes the GNU versions of the Unix toolset (and their assorted vendor extensions); FROM alpine is also common and is based around Busybox plus a minimal package manager.\nAnother good answer is to limit yourself to the functionality defined in POSIX.1: du(1) is not required to support a -b option. This will help if you're trying to write Alpine-based images or run on systems that aren't Linux (MacOS being the most prominent present-day example).\nYou probably will not succeed in copying individual binaries from your host system into a Docker image, path issues aside, because the library environments are likely to be very different. If you run ldd $(which du) on the host, all of the libraries listed there need to be present in the image and at a similar version. The busybox base image probably doesn't even include a libc.so.6, which is a minimum requirement for most dynamically-linked binaries.\nThe correct answer to your question as it's written is to write a multi-stage Dockerfile that has a first stage with a full C toolchain that builds a static version of GNU Coreutils, and then a second stage that copies it in. That's a lot of work for a tool that's probably not part of the core application you actually want to run.",
    "Docker / Docker-compose volume fill & share issue": "For the first question, I try a simple docker file like this:\nFROM php:7-fpm\nCOPY ./project /project\nAnd a docker-compose like this:\nversion: '3'\nservices:\n    php:\n        build: .\n        volumes:\n            - named-volume:/project\n    web:\n        image: nginx\n        links:\n            - php\n        volumes:\n            - named-volume:/project\nvolumes:\n    named-volume:\nSince you create the volume on docker-compose you don't need to create that in the Dockerfile. Running docker volume list, I'm able to see the volume created with a local driver. Making ls inside the folder I'm also able to see the file. It's important to note, that the file present in you local directory it's not the same that the file inside the container. So if you edit the files in the host this will not change the files in container. That's because you have your volume created in another path, probably at: /var/lib/docker/volumes/... This happens because you map the volume to the path, but you not specifies where you want the volume. To do that just make your docker-compose like this:\nversion: '3'\nservices:\n    php:\n        build: .\n        volumes:\n            - ./project:/project\n    web:\n        image: nginx\n        links:\n            - php\n        volumes:\n            - ./project:/project\nMaking this I'm still able to see the volume with the volume list command but without a name. So I don't know why you are not able to see the volume in the list.\nFor question 2:\nDoing the example above I have the files inside the container that exists in my local \"project\" folder.\nPlease check that the path to the local folder is correct.",
    "Docker multi-stage build not copying between stages": "When your Dockerfile ends with:\nWORKDIR /var/www/html\nCOPY --from=0 /app ./web/themes/material_admin_mine\nThat should in fact copy the data from the first build stage to the final image. But then when you launch the container with\nvolumes:\n  - ./:/var/www/html:cached\neverything in the /var/www/html directory tree, including that final COPY step, is hidden and replaced with what's in the current directory on the host. If you think of this like a copy, it's a one-way copy into the container; later changes will get copied back out to the host, but there's nothing that synchronizes what's in the image with what you previously had in the directory at startup time.\nA Dockerfile intrinsically can't affect host filesystem content. In your case it sounds like the host content is secondary to your application proper. Given what's going into the first stage, I'd just run the yarn install step on the host and be done with it (you probably already have Node and Yarn available even). Otherwise you'd need a more selective volumes: section that carefully tried to avoid overwriting that one directory; you might be able to mount something like ./web/src:/var/www/html/web/src to only include your application code and avoid hiding the .../web/themes tree.",
    "What is the difference between multiples RUN entries in Dockerfile and just one RUN entry?": "Each RUN command creates a layer of the filesystem changes generated by a temporary container started to run that command. (It's effectively running a docker run and then packaging the result of docker diff into a filesystem layer.)\nThese layers have a few key details to note:\nThey are immutable. Once you create them you don't change them. You would have to generate/recreate a new layer, to update your image.\nThey are reusable between multiple images and running containers. You can do this because of the immutability.\nYou do not delete files from a parent layer, but you can register that a file is deleted in a later layer. This is a metadata change in that later layer, not a modification to the parent layer.\nLayers are reused in docker's build cache. If two different images, or even the same image being rebuilt, perform the same command on top of the same parent layer, docker will reuse the already created layer.\nThese layers are merged together into the final filesystem you see inside your container.\nThe main difference between the two approaches are the build cache and deleting files. If you split apart the download of a source code tgz, extraction of the tgz, compiling a binary, and the deleting of the tgz and source folders, into multiple RUN lines, then when you ship the image over the network and store it on disk, you will have all of the source in the layers even though you don't see it in the final container. Your image will be significantly larger.\nCaching can also be a bad thing when you cache too much. If you split the apt update and apt install, and then add a new package to install to your second run line months later, docker will reuse the months old cache of apt update and try to install packages that are months old, possibly no longer available, and your image may fail to build. Many people also run a rm -rf /var/lib/apt/lists/* after installing debian packages. And if you do this in a separate step, you will not actually delete the files from the previous layers, so your image will not shrink.",
    "Dockerfile run executable": "You should use docker COPY or ADD command. COPY is more preferred as described here: https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#add-or-copy\nFROM java:8\nEXPOSE 8080\nCOPY foo.jar ~/.\nENTRYPOINT [\"java\",\"-jar\",\"foo.jar\"]",
    "Why installing custom python packages using RUN in Dockerfile does not work?": "You could also use pip to install local packages\nWORKDIR ./myPythonModule\nRUN pip install .",
    "Build the docker image with environment variables": "First you need to consume the build-arg inside you dockerfile using the ARG command.\nFROM alpine\n\n# consume the build arg    \nARG somename  \n\n# persist the env variable in the built image\nENV somename=$somename  \n\n# somename will appear as an env variable\nRUN echo $somename \nRUN env\nAnd running the build command docker build --build-arg somename=hello . will show you an env variable somename=hello",
    "Change directory in docker alpine failing": "The current working directory is reset for every RUN to the value of the last WORKDIR.\nSo, because of that, this line RUN cd azure-iot-sdk-python && ls -al build_all/linux is doing it from here /src, and your cloned repo is in opt.\nSo both of this will work:\nRUN cd /opt/azure-iot-sdk-python && ls -al build_all/linux\nand:\nWORKDIR /opt\nRUN cd azure-iot-sdk-python && ls -al build_all/linux",
    "Docker volume does not persist data": "There is something that confused me and for me was not very clear in the official documentation.\nTo my knowledge, persistent volumes can be created in three ways.\nAt container invocation time including full path ( -v ~/database:/data ): makes an external folder from the host available inside the docker container. Both can modify it.\nAt container invocation time using a volume name ( -v datamysql:/data ): makes a volume that is persistent available inside the container. It is created it if it did not exist. You can list them by name with docker volume ls. Internally, it will be stored in a place such as /var/lib/docker/volumes/ae4445f7c9317a22fe84726fb894c47754f38a7fd150c00fd877024889968750/_data.\nAt container build time ( VOLUME [\"/database/data\"] in Dockerfile). Every invocation of docker run will create a new volume that will persist even if you delete the container. This can be confusing becausee subsequent invocations will result in different volumes being created that will not be reused.\nYou can list both named (second case) and unnamed (third case) volumes with\n$ docker volume ls                                                             \nDRIVER              VOLUME NAME                                                             \nlocal               064593b3e65977097d4d0c8402a6c633f1af69be2937bf118678ab8f97ee9a7e               \nlocal               4753ad0437d13e54c76d9c34a30a1843396a1866a0cf9237d500fdcca0d78c5f           \nlocal               8d7a35354f666b2e8a26866a35bbae36bb9601701d4c6b505ab8ce6629f69415               \nlocal               db48eefe8f189b36107ca9c4eebb792690590ab0ba055e7e4e2c9adfd1765b7e                    \nlocal               datamysql\nYou can see the exact location of a container's volume by using docker inspect mycontainer\n{\n                \"Type\": \"volume\",\n                \"Name\": \"8d7a35354f666b2e8a26866a35bbae36bb9601701d4c6b505ab8ce6629f69415\",\n                \"Source\": \"/media/USBdrive/docker/volumes/8d7a35354f666b2e8a26866a35bbae36bb9601701d4c6b505ab8ce6629f69415/_data\",\n                \"Destination\": \"/var/lib/mysql\",\n                \"Driver\": \"local\",\n                \"Mode\": \"\",\n                \"RW\": true,\n                \"Propagation\": \"\"\n            },\nIt might be handy to remove unused volumes (for the third case, specially).\n$ docker volume prune\nWARNING! This will remove all volumes not used by at least one container.\nAre you sure you want to continue? [y/N] y\nDeleted Volumes:\n4753ad0437d13e54c76d9c34a30a1843396a1866a0cf9237d500fdcca0d78c5f\n\nTotal reclaimed space: 205MB\nBecause you used the VOLUME directive in your Dockerfile, you are in the third case. Inspect your container to look for the file, and specify the volume from the command line if you want repeated sessions to persist data.",
    "makefile - build mulitple files with multiple targets": "I'm not sure this is what you want, but...\nFirst consider the BUILD variable. If we have three Dockerfiles:\nfoo/Dockerfile\nbar/Dockerfile\nbaz/Dockerfile\nthen we want BUILDS to contain foo bar baz\nHere are a couple of attempts:\nBUILDS := $(wildcard */Dockerfile) # this is foo/Dockerfile bar/Dockerfile baz/Dockerfile\n\nBUILDS := $(dir $(wildcard */Dockerfile)) # this is foo/ bar/ baz/\n\nBUILDS  := $(patsubst %/,%, $(dir $(wildcard */Dockerfile))) # this is foo bar baz\nCrude but effective.\nNow the rules. Ordinarily the target of a rule is the name of a file which the rule builds. In this case we must break this convention, since we don't know what the name of the image file will be. So if the directory is foo/, we could have a rule called build_foo:\nbuild_foo:\n    @echo \"Building foo\"\n    @echo docker build -t foo --force-rm foo\nSince we don't want to write a rule for every possible directory, we will use automatic variables and create a pattern rule:\nbuild_%:\n    @echo \"Building $$@\"\n    @echo docker build -t $* --force-rm $*\nNow \"make build_foowill work correctly. And we could write abuild` rule that builds all of them:\nbuild: $(addprefix build_,$(BUILDS))\nBut this is not quite the right approach. We want to build, then test, then push each image, in that order. So we'd like something like this:\npush_foo: test_foo\n\ntest_foo: build_foo\nWe can do this with pattern rules:\ntest_%: build_%\n    ...\n\npush_%: test_%\n    ...\n\nrelease: $(addprefix push_,$(BUILDS))\nNow \"make release\" will do everything. (And if you put release: as the first rule in the makefile, it will be the default rule, and \"make\" will suffice.)",
    "Error running file in Container \"error when loading shared libraries\" DOCKER": "The error tells you exactly what the problem is - you have copied over the executable, but it has dependencies on various libraries (including libkrb5.so.3) that aren't in the container.\nYou have two choices; you can add the dependencies to the container or you can recompile the executable so that it is statically linked.\nYou can run the ldd tool on the executable to discover which dependencies it requires. You can then either copy the libraries directly into the container as you did with the executable or (probably better) you can find which package the dependencies are from and install with apt-get.",
    "Cannot start a docker container": "The container is running successfully, its just exiting immediately as you don't specify any process to run. A container will only run as long as its main process. As you've run it in the background (the -d flag) it won't provide any output, which is a bit confusing.\nFor example:\n$ docker run ubuntu echo \"Hello World\"\nHello World\nThe container ran the command and exited as expected.\n$ docker run -d ubuntu echo \"Hello World\"\nefd8f9980c1c9489f72a576575cf57ec3c2961e312b981ad13a2118914732036\nThe same thing as happened, but as we ran with -d, we got the id of the container back rather than the output. We can get the output using the logs command:\n$ docker logs efd8f9980c1c9489f72a576575cf57ec3c2961e312b981ad13a2118914732036\nHello World\nWhat you need to do is start your rails app, or whatever process you want the container to run when you launch the container. You can either do this from the docker run command or using CMD statement in the Dockerfile. Note that the main process must stay in the foreground - if it forks to the background the container will exit.\nIf you want to get a shell in a container, start it with -it e.g:\n$ docker run -it ubuntu /bin/bash\nTo be honest, I think you'd be better served by using an official image e.g. https://registry.hub.docker.com/_/rails/.",
    "/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found": "The \"buster\" debian version is pretty old, and delivers a pretty old (2.28) glibc, which explains why you'd be missing the more modern glib symbols:\n$ docker run -it debian:buster-slim /usr/bin/ldd --version  \nldd (Debian GLIBC 2.28-10+deb10u3) 2.28\nCopyright (C) 2018 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nWritten by Roland McGrath and Ulrich Drepper.\nYou can use a newer version of debian to get them, e.g., \"bookworm\":\n$ docker run -it debian:bookworm-slim /usr/bin/ldd --version\nldd (Debian GLIBC 2.36-9+deb12u7) 2.36\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nWritten by Roland McGrath and Ulrich Drepper.\nTo consume it, just replace the image name in the second FROM directive:\n# Stage 2: Create a lightweight image for the Rust app\nFROM debian:bookworm-slim",
    "Docker build troubles with PyFlink container creating": "Edit: Upon further investigation of my own image, I was able to get it to build by\nInstalling openjdk-11-jdk-headless\nSetting ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-arm64 before my pip install.\nUpdated Dockerfile\nFROM flink:1.17.1\n\n# Versions\nENV \\\n  # Apt-Get\n  BUILD_ESSENTIAL_VER=12.9ubuntu3 \\\n  JDK_VER=11.0.20.1+1-0ubuntu1~22.04 \\\n  LIBBZ2_DEV_VER=1.0.8-5build1 \\\n  LIBFFI_DEV_VER=3.4.2-4 \\\n  LIBSSL_DEV_VER=3.0.2-0ubuntu1.10 \\\n  ZLIB1G_DEV_VER=1:1.2.11.dfsg-2ubuntu9.2 \\\n  # Python\n  PYTHON_VER=3.10.13 \\\n  # PyFlink\n  APACHE_FLINK_VER=1.17.1\n  \nSHELL [\"/bin/bash\", \"-ceuxo\", \"pipefail\"]\n\nRUN apt-get update -y && \\\n  apt-get install -y --no-install-recommends \\\n    build-essential=${BUILD_ESSENTIAL_VER} \\\n    openjdk-11-jdk-headless=${JDK_VER} \\\n    libbz2-dev=${LIBBZ2_DEV_VER} \\\n    libffi-dev=${LIBFFI_DEV_VER} \\\n    libssl-dev=${LIBSSL_DEV_VER} \\\n    zlib1g-dev=${ZLIB1G_DEV_VER} \\\n  && \\\n  wget -q \"https://www.python.org/ftp/python/${PYTHON_VER}/Python-${PYTHON_VER}.tar.xz\" && \\\n  tar -xf \"Python-${PYTHON_VER}.tar.xz\" && \\\n  cd \"Python-${PYTHON_VER}\" && \\\n  ./configure --enable-optimizations --without-tests --enable-shared && \\\n  make -j$(nproc) && \\\n  make install && \\\n  ldconfig /usr/local/lib && \\\n  cd .. && \\\n  rm -rf \"Python-${PYTHON_VER}\" \"Python-${PYTHON_VER}.tar.xz\" && \\\n  ln -s /usr/local/bin/python3 /usr/local/bin/python && \\\n  apt-get clean && \\\n  rm -rf /var/lib/apt/lists/*\n\n# Installing OpenJDK again & setting this is required due to a bug with M1 Macs\nENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-arm64\n\nRUN pip3 install --no-cache-dir apache-flink==${APACHE_FLINK_VER} && \\\n  pip3 cache purge\n\nUSER flink\nRUN mkdir /opt/flink/usrlib\nCOPY python_demo.py /opt/flink/usrlib/python_demo.py\nOld: I encountered the same issue under the same circumstances (17.1, M1). I assume it's an ARM issue since I was able to build the exact same Dockerfile (below) successfully on an x86 machine.\nI replicated the same issue when trying to build the example pyflink Dockerfile as well.",
    "\"plugin 'bridge' not found\" when creating Docker network with docker compose driver bridge": "I fixed it by simply removing driver: bridge from the docker compose file",
    "Docker failed to solve: executor failed running. exit code: 1": "By checking the log output\n#0 7.873 Do you want to continue? [Y/n] Abort.\nYou should pass the flag -y in order to auto accept the installation of packages in apt related commands.\nSo you should try something like this\nRUN apt-get update -y \\\n    && apt-get upgrade -y pip \\\n    && pip install --upgrade pip \\",
    "Permission Denied | Got permission denied while trying to connect to the Docker daemon socket [duplicate]": "To avoid these permission issues run the following command\nsudo chmod 666 /var/run/docker.sock\nSource\n$ docker ps -a\nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS                        PORTS                                       NAMES\n115682275acc  ...",
    "Installed via Laravel Sail and available on the port Meilisearch doesn't add indexes for models; date.ms file seems to be missing": "The problem was in incorrect configuration.\nIt was necessary to replace the host address 127.0.0.1 with value 'meilisearch' in the .env file like this:\nSCOUT_DRIVER=meilisearch\nMEILISEARCH_HOST=http://meilisearch:7700\nMEILISEARCH_KEY=masterKey\nSimilarly (but this was not the problem) it was necessary to replace the host address 127.0.0.1 with the value 'meilisearch' in the docker-compose.yml file in 'healthcheck' section like this:\nhealthcheck:\n    test: [\"CMD\", \"wget\", \"--no-verbose\", \"--spider\",  \"http://meilisearch:7700/health\"]\n    retries: 3\n    timeout: 5s",
    "Unzip local file and delete original in Dockerfile image build": "ADD only decompresses local tar files, not necessarily compressed single files. It may work to package the contents in a tar file, even if it only contains a single file:\nADD ./data/databases/file.tar.gz /data/databases/\n(cd data/databases && tar cvzf file.tar.gz file.db)\ndocker build .\nIf you're using the first approach, you must use a multi-stage build here. The problem is that each RUN command generates a new image layer, so the resulting image is always the previous layer plus whatever changes the RUN command makes; RUN rm a-large-file will actually result in an image that's slightly larger than the image that contains the large file.\nThe BusyBox tool set includes, among other things, an implementation of unzip(1), so you should be able to split this up into a stage that just unpacks the large file and then a stage that copies the result in:\nFROM busybox AS unpack\nWORKDIR /unpack\nCOPY data/databases/file.db.zip /\nRUN unzip /file.db.zip\n\n\nFROM python:3.8-slim\nCOPY --from=unpack /unpack/ /data/databases/\nIn terms of the Docker image any of these approaches will create a single very large layer. In the past I've run into operational problems with single layers larger than about 1 GiB, things like docker push hanging up halfway through. With the multi-stage build approach, if you have multiple files you're trying to copy, you could have several COPY steps that break the batch of files into multiple layers. (But if it's a single SQLite file, there's nothing you can really do.)",
    "How do I convert docker-compose configuration to dockerfile": "Docker Compose and Dockerfiles are completely different things. The Dockerfile is a configuration file used to create Docker images. The docker-compose.yml file is a configuration file used by Docker Compose to launch Docker containers using Docker images.\nTo launch the above containers without using Docker Compose you could run:\n docker network create es-net\n docker run -d -e xpack.security.enabled=true -e \"discovery.type=single-node\" -p 9200:9200 --network es-net --name es-container docker.elastic.co/elasticsearch/elasticsearch:6.5.4\n docker run -d -e ELASTICSEARCH_HOSTS=http://es-container:9200 -p 5601:5601 --network es-net --name kb-container docker.elastic.co/kibana/kibana:6.5.4\nAlternatively, you could run the containers on the hosts network stack (rather than the es-net nework). Kibana would then be able to talk to ElasticSearch on localhost:\n docker run -d -e xpack.security.enabled=true -e \"discovery.type=single-node\" --network host --name es-container docker.elastic.co/elasticsearch/elasticsearch:6.5.4\n docker run -d -e ELASTICSEARCH_HOSTS=http://localhost:9200 --network host --name kb-container docker.elastic.co/kibana/kibana:6.5.4\n(I haven't actually run these so the commands might need some tweaking).",
    "Docker Image Run Issue: cat: command not found": "FROM scratch means 'start from an empty directory'. There is no cat inside, unless explicitly added it. One way to do that is to copy the binary from another image:\nFROM debian:buster as cat_source\nFROM scratch\nCOPY --from=cat_source /bin/cat /bin/cat\n# the rest of the dockerfile\n...",
    "OSError: no library called \"cairo\" was found": "The resolution to my problem is exactly as many people described; the libcairo.so2 library is not included in the python library. To add it into a docker container, you just had to edited the dockerfile like so...\nFROM python:3.8-slim-buster\n\nENV PYTHONDONTWRITEBYTECODE=1\n\nENV PYTHONUNBUFFERED=1\n\n# Install pip requirements\nCOPY requirements.txt .\nRUN python3 -m pip install -r requirements.txt\n\nRUN apt-get update -y\nRUN apt-get install -y libcairo2\n\nWORKDIR /app\nCOPY . /app\n\n\nCMD [\"python3\", \"insQr2.py\"]",
    "Patch Package with Docker": "As @donjus mentioned in the comments, the patches were being copied to the root directory, not inside of patches.\nThe solution is to change:\nCOPY package.json yarn.lock patches ./\nRUN yarn install --frozen-lockfile --unsafe-perm\nto\nCOPY package.json yarn.lock ./\nCOPY ./patches ./patches\nRUN yarn install --frozen-lockfile --unsafe-perm",
    "Why copy only the .csproj file and not the rest of the files in a Dockerfile?": "It is for Docker build cache efficiency. From the documentation\nthe *.csproj files are copied and restored as distinct layers. When the docker build command builds an image, it uses a built-in cache. If the *.csproj files haven't changed since the docker build command last ran, the dotnet restore command doesn't need to run again. Instead, the built-in cache for the corresponding dotnet restore layer is reused\nRegarding if you need to copy the rest of the files - yes, you do, as also shown in the docs\nhttps://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/docker/building-net-docker-images?view=aspnetcore-5.0#the-dockerfile",
    "How to give parameter to a docker image in docker file so that it still works in Apple M1 Machine": "You can add --platform option to a FROM statement as well:\nFROM --platform=linux/amd64 openjdk:8-jdk-slim\n...\nIt's mentioned in the Dockefile reference:\nThe optional --platform flag can be used to specify the platform of the image in case FROM references a multi-platform image. For example, linux/amd64, linux/arm64, or windows/amd64. By default, the target platform of the build request is used. Global build arguments can be used in the value of this flag, for example automatic platform ARGs allow you to force a stage to native build platform (--platform=$BUILDPLATFORM), and use it to cross-compile to the target platform inside the stage.",
    "installing cert for headless chrome in selenium docker image": "After some more digging I found the certs were being copied to the below dir inside the container (I had to go in the bash shell in container and do some troubleshooting for below steps):\n/usr/local/share/ca-certificates\nto test the certs are working or not I tried using curl on the non prod environment url (from inside the container), which also worked. At this point I was sure that certs are in the container and are working but chrome for some reason is not able to use those certs.\nTo resolve this I added these capabilities to chrome with chrome options:\ncapabilities = chrome_options.to_capabilities()         #cap\ncapabilities['acceptInsecureCerts'] = True              #cap\nand it started working as expected. To see all the arguments and capabilites I had to add to make this work below is complete configuration for chrome:\n    elif data.get('browser') == 'container':\n        #chrome_options.addArguments(\"--headless\", \"--window-size=1920,1200\",\"--ignore-certificate-errors\")\n        chrome_options.add_argument(\"--headless\")\n        chrome_options.add_argument('--disable-gpu')\n        chrome_options.add_argument('--no-sandbox')\n        chrome_options.add_argument('--disable-dev-shm-usage')\n        #chrome_options.add_argument('--allow-running-insecure-content')\n        #chrome_options.add_argument('--disable-web-security')\n        #chrome_options.add_experimental_option('useAutomationExtension', False)\n        chrome_options.add_argument('add_experimental_option(\"excludeSwitches\",[\"ignore-certificate-errors\"])')\n        chrome_options.add_argument('window-size=1200x600')\n        capabilities = chrome_options.to_capabilities()         #cap\n        capabilities['acceptInsecureCerts'] = True              #cap\n        context.driver = webdriver.Chrome(executable_path=\"/usr/bin/chromedriver\", chrome_options=chrome_options)",
    "Dockerfile: reference previously defined ENV in another ENV": "For this to work you would need to spearate out the variables and use multiline assignment.\nENV PROJ_DIR /myproj/\nENV PYTHONPATH ${PROJ_DIR}:${PYTHONPATH}\nThroughout the entire instruction, environment variable substitution will use the same value for each variable. In your case PROJ_DIR is yet to be assigned a value, so it returns empty in PYTHONPATH varaible.\nTo be more clear, in:\nENV x=hello\nENV x=world z=$x\nz will have value hello and not world.\nDue to multiline there will be not be any additonal layers getting created as ENV layers do get squashed.\nHope that helps.",
    "Can't access jupyter notebook inside docker container": "First modify your docker file in order to start jupyter notebook, add this line at the end of the Dockerfile:\nCMD [\"jupyter\", \"notebook\", \"--port=8888\", \"--no-browser\", \"--ip=0.0.0.0\", \"--allow-root\"]\nThen build again the image and when you start the container use -p option: --publish , -p       Publish a container\u2019s port(s) to the host\ndocker run -t -d -p 8888:8888 --name ig intelligait3d where the mapping is -p <HOST port>:<CONTAINER port>\nHere you can find the reference for docker run command: https://docs.docker.com/engine/reference/commandline/run/",
    "Why does docker rebuild all layers every time I change build args": "Move your args to just before you need them. Docker does not replace args in the RUN commands before running them. Instead, the args are passed as environment variables and expanded by the shell within the temporary container. Because of that, a change to an arg is a change to the environment, and a miss of the build cache for that step. Once one step misses the cache, all following steps must be rebuilt.\nFROM ubuntu:18.04\n\n# Then several Layers which does not use any ARGS. Example\n\nENV LANG=C.UTF-8 LC_ALL=C.UTF-8\n\nRUN mkdir ~/mapped-volume\n\nRUN apt-get update && apt-get install -y wget bzip2 ca-certificates build-essential curl git-core htop pkg-config unzip unrar tree freetds-dev vim \\\nsudo nodejs npm net-tools flex perl automake bison libtool byacc\n\n# And so on \n# And finally towards the end\n# Setup User\nARGS USER=test-user\nARGS UID=1000\nRUN useradd -m -d /home/${USER} --uid ${UID} -G sudo -s /bin/bash ${USER} \n# && echo \"${USER}:${PW}\" | chpasswd\n\n# Couple of  more commands to change dir, entry point etc. Example\n\nLABEL version=\"1.0\"\nAlso, labels, environment variables that aren't needed at build time, exposed ports, and any other meta data is often best left to the end of the Dockerfile since they have minimal impact on build time and there's no need to miss the cache when they change.",
    "Do I need to `docker commit` in order to push an image into a docker image registry (eg. docker hub)?": "You don't need to run and commit. docker commit allows you to create a new image from changes made on existing container. You do need to build and tag your image in a way that will enable you to push it.\ndocker build -t [registry (defaults to docker hub)]/[your repository]:[image tag] [docker file context folder]\nfor example:\ndocker build -t my-repository/some-image:image-tag .\nAnd then:\ndocker push my-repository/some-image:image-tag\nThis will build an image from a docker file found in the current folder (where you run the docker build command). The repository in this case is my-repository, the image name is some-image and it's tag is image-tag.\nAlso please note that you'll have to perform docker login with your credentials to docker hub before you are able to actually push the image.\nYou can also tag an existing image without rebuilding it. This is useful if you want to push an existing image to a different registry or if you want to create a different image tag. for example:\ndocker tag my-repository/some-image:image-tag localhost:5000/my-repository/some-image:image-tag\nThis will add a new tag to the image from the previous example. Note the registry part added (localhost:5000). If you call docker push on that tag (docker push localhost:5000/my-repository/some-image:image-tag) the image will be pushed to a registry found on localhost:5000 (of course you need the registry up and running before trying to push).",
    "How can I set persistent data in docker-compose.yml file": "Use volumes\nvolumes: \n\n# Just specify a path and let the Engine create a volume\n - /var/lib/mysql \n\n# Specify an absolute path mapping\n - /opt/data:/var/lib/mysql \n\n# Path on the host, relative to the Compose file\n - ./cache:/tmp/cache \n\n# User-relative path\n - ~/configs:/etc/configs/:ro \n\n# Named volume\n - datavolume:/var/lib/mysql\nSee also detailed post: Use volumes",
    "Run a Java Application with Embedded Jetty Server on Docker": "You do not need a Jetty docker base image. You can use a jdk or jre base image like https://hub.docker.com/_/openjdk",
    "Docker volumes not mounting/linking": "I believe you are misunderstanding how host volumes work. The volume definition:\n./build:/srv/build\nIn the compose file will mount ./build from the host at /srv/build inside the container. This happens at run time, not during your image build, so after the Dockerfile instructions have been performed. Nothing from the image is copied out to the host, and no files in the directory being mounted in top of will be visible (this is standard behavior of the Linux mount command).\nIf you need files copied back out of the container to the host, there are various options.\nYou can perform your steps to populate the build folder as part of the container running. This is common for development. To do this, your CMD likely becomes a script of several commands to run, with the last step being an exec to run your app.\nYou can switch to a named volume. Docker will initialize these with the contents of the image. It's even possible to create a named bind mount to a folder on your host, which is almost the same as a host mount. There's an example of a named bind mount in my presentation here.\nYour container entrypoint can copy the files to the host mount on startup. This is commonly seen on images that will run in unknown situations, e.g. the Jenkins image does this. I also do this in my save/load volume scripts in my example base image.",
    "PHP cannot resolve hostname": "The CMD syntax seems to have some problem. All the arguments in the command should be comma separated and inside the double quotes.\nI made a slight change in Dockerfile and it worked.\nFROM php:7-alpine\nCOPY . /var/www\nWORKDIR /var/www\nCMD [ \"php\", \"-S\", \"0.0.0.0:8080\", \"-t\", \"html\" ]\ndocker build -t testimage:v1 .\n[mchawre@jumphost try]$ docker run -it --rm --network=\"host\" --expose 8080 --name testrun testimage:v1\nPHP 7.3.6 Development Server started at Sat Jun 15 11:50:19 2019\nListening on http://0.0.0.0:8080\nDocument root is /var/www/html\nPress Ctrl-C to quit.\nNOTE: Change 127.0.0.1 to 0.0.0.0 so that you can hit the php using public/private ip of your machine rather than just localhost.",
    "ERROR: for redis Cannot create container for service redis: source is not directory": "Try this in docker-compose\n volumes:\n      - redis-data:/usr/local/etc/redis",
    "Run docker compose file without internet connection": "Do below steps\nFirst over the internet get to build all required docker images.\nThen save/export those images in the tar file by using below command\ndocker save image_name > /image/mynewimage.tar\nThe go offline, copy those tar file on another or same machine and load those images by using below command\ndocker load < /image/mynewimage.tar\nThen run normal docker run commands, you already have got all images locally hence docker will not try to get it download from the internet.\nThis should work, let me know if you need any help with this.\nPlease post your docker-compose file, if in case of any issue with above approach.",
    "Unable to understand a Docker-compose service property": "multiple volumes can be attached to your container ... each are defined as a pair\nvolumes:\n- /parent/host/path01:/inside/container/path_one    \n- /parent/host/path02:/inside/container/path_another\nof each pair the left side is a pre-existing volume reachable on host before container is created ... right side is what the freshly launched container views that left side as from inside the container\nin your example, in same dir where you launch docker-compose from, there evidently exists a dir called data ... using ./data will reach it using a relative path ... the right side /data/db is what the code in your container calls that same dir\n/full/path/to/reach/data:/data/db\nis using the absolute path to reach that same ./data dir which lives on the parent host which docker-compose is executed on\nThis volume mapping allows permanent storage on parent host to become visible (read/writable) to the container ... since the container filesystem is ephemeral and so goes away when container exits this volume mapping gives the container access to permanent storage for specified paths which must appear in your yaml file ... especially important for database containers like mongo ... all files used in your container not mapped in the volumes yaml disappear once the container exists\nHere is a typical yaml snippet for mongo where it gains access to permanent storage on parent host\n  loudmongo:\n    image: mongo\n    container_name: loud_mongo\n    restart: always\n    ports:\n     - 127.0.0.1:27017:27017\n    volumes:\n     - /cryptdata7/var/data/db:/data/db",
    "How to containerize this application, using docker-compose": "You're on the right way with the base structure, but this a quite complex configuration if you are new to Docker and it's concepts. I'll try to explain how to configure each service step by step.\nDefaults\nyour services will be available on the docker-compose network with the same names you're using. So the db service will be the db host inside the network\nall docker-compose services can communicate, so you can reach the db host from the ETL host. If you want, you can create service names alias using links (be careful: links is a legacy feature and may be removed)\nPostgres\nThe db needs only to be attached to a volume to persist data as you want:\ndb:\n  image: postgres:9.6.1\n  restart: always\n  volumes:\n    - ./pg-data:/var/lib/postgresql/data  \n  ports:\n    # to attach external GUIs\n    - 5432:5432\n  environment:\n    - POSTGRES_PASSWORD: PG_PASS\n./pg-data is a local directory (relative to your docker-compose file) on your host OS (it will be created if not exists).\n/var/lib/postgresql/data is the default postgres default directory.\nyour local directory ./pg-data will be mapped to the container /var/lib/postgresql/data, so postgres will write inside it.\nI see a Dockerfile inside your db directory: if you use the official postgres image you don't need it.\nETL\nTo build services located in subdirectories you can use a custom build context. I will build you service image from the specified directory, using the relative Dockerfile. You can customize the build further, using the dockerfile option.\nTo link your sources to the container just use another volume:\nETL:\n  build:\n    context: ./ETL\n  volumes:\n    - ./ETL:/YOUR/CONTAINER/APP_DIR\nThis requires that your service Dockerfile should be like this:\nFROM ubuntu\n\n# Create app directory\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\n# Bundle app source\nCOPY . /usr/src/app\nEXPOSE 80\nCMD [ \"python\", \"my-app.py\"]\nIn this case, the docker-compose volume for the ETL service will be:\n  volumes:\n    - ./ETL:/usr/src/app\nQUESTIONS: - Should install mdal package when being built: what does it mean? where and how it can be installed?\nrestapi\nNginx will be the reverse proxy of your gunicorn/flask app. So you need two services:\na nginx service configured as a reverse proxy\na service that runs your gunicorn/flask app (the restapi service)\nNginx\nnginx:\n  image: nginx\n  ports:\n    - \"80:80\"\n  volume:\n    - ./restapi/nginx.conf:/etc/nginx/conf.d/default.conf\nNginx will be the main access point for your app, so the port 80 is exposed on you host: you can reach your app at http://localhost.\nThe volume is a \"trick\" to add a configuration file to the official nginx image without creating a custom one. By default nginx will load any .conf file available inside /etc/nginx/conf.d. We're overwriting the container default.conf at runtime with our custom configuration file.\nThis is an example configuration file, you should save it as ./restapi/nginx.conf:\nserver {\n  listen 80;\n  server_name  ~^.*$;\n\n  location / {\n    proxy_pass http://restapi:8000;\n    proxy_set_header Host $host;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n  }\n}\nThe gunicorn/flask app\nHere you should have somewhere a Dockerfile that run you app using something like gunicorn -w 4 myapp:app (I've taken the basic example from the gunicorn website).\nrestapi:\n  build:\n    context: ./restapi\n  expose:\n    - \"8000\"\nWe're exposing the gunicorn port to the docker network (you can't reach it directly from you host, but nginx can).\nmdal\nWho is using this? Where is it required?\nA suggested reading list\nWhile the docker and docker-compose docs are very detailed, I suggest you some books that explain the (too) many docker features (and quirks):\nDocker in Action: Dockerfiles, images, registries, networks, volumes. Everything you need to know. A must read from my point of view.\nDocker in Practice: many interesting and useful (even extreme) use cases and solutions. A cookbook.\nThe Docker book: learn by practical examples.",
    "How to pass variable as attribute to xml configuration file in Wildfly with Docker": "Almost there. In your docker file, you have defined environmental variables therefore you need to reference them as environmental variables in your wildfly config. The easiest way is to prefix your env var with env. prefix. So in your example, you have env variables HOST, SSL, USERNAME... which you can reference in standalone.xml like this:\n<smtp-server password=\"${env.PASSWORD}\" username=\"${env.USERNAME}\" ssl=\"${env.SSL}\" outbound-socket-binding-ref=\"mail-smtp\"/> </mail-session>\nWithout env. prefix, jboss/wildfly will try to resolve the expression as jvm property, which you'd have to specify as jvm -D flag.\nYou can also use default value fallback in your expressions such as:\nssl=\"${env.SSL:true}\"\nThis way, the ssl will be set the the value of environmental variable named SSL, and if such var does not exist, server will fallback to true.\nHappy hacking",
    "Docker connection refused hanging Django": "Your docker ps output shows nothing in the PORTS column. That means that there's no port forwarding from the host to the container.\n[...]     STATUS          PORTS                     NAMES\n[...]     Up 5 seconds                              friendly_dijkstra\nIf you use the command docker run to run your container, you should explicitly specify port number both on host and on the container using the command option -p hostPort:containerPort\ndocker run -p 8000:8000 app1\nNow, running docker ps should show port forwarding.\n[...]     STATUS          PORTS                     NAMES\n[...]     Up 5 seconds    0.0.0.0:8000->8000/tcp    friendly_dijkstra\nIf you are using docker-compose to start your containers, the host and container ports are already configured in your docker-compose.yml file, so you don't need a command line option.\ndocker-compose up web\nTo use docker compose, you have to install it on the host. It's a python module, so you can install it with pip pip install docker-compose",
    "Conditionally set ENV var based on hostname in Dockerfile": "RUN if [ hostname = \"foo\" ]; then ENV BAR \"BAZ\"; else ENV BAR \"BIFF\"; fi\nYou can't nest docker build instructions, everything after the RUN instruction gets executed in the image context, docker build commands don't exist there. So that explains the error you are seeing.\nEven you if you translated that to proper shell code BAR would only be active for that single RUN instruction during the build.\nEither orchestrate on the host and pass BAR via run -e to your container or add a startup script to the image that sets BAR as needed on container start:\nFROM foo\nCOPY my-start.sh /\nCMD [\"/my-start.sh\"]",
    "Connect to rabbitmq on local host from a Docker container": "Docker engine provide a virtual network interface as a getway if you use --net=\"bridge\". It's a standard value. On linux you can know the name of the interface with ifconfig command.\ndocker0   Link encap:Ethernet  IndirizzoHW 02:42:56:6c:95:26  \n      indirizzo inet:172.17.0.1  Bcast:0.0.0.0  Maschera:255.255.0.0\n      indirizzo inet6: fe80::42:56ff:fe6c:9526/64 Scope:Link\n      UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n      RX packets:28267 errors:0 dropped:0 overruns:0 frame:0\n      TX packets:25451 errors:0 dropped:0 overruns:0 carrier:0\n      collisioni:0 txqueuelen:0 \n      Byte RX:52948314 (52.9 MB)  Byte TX:25718113 (25.7 MB)\nInside my container I just use, in this case, 172.17.0.1 as gateway to access to my host.\nInside your container you can also show routes with the route command and see what is the default gateway\nroot@15fae92c516f:/# route\n Kernel IP routing table\n Destination     Gateway         Genmask         Flags Metric Ref    Use      Iface\n default         172.17.0.1      0.0.0.0         UG    0      0        0  eth0\n 172.17.0.0      *               255.255.0.0     U     0      0        0  eth0\nroot@15fae92c516f:/# ",
    "No Install group file - CentOS 7 - group install": "Doing the following may work, and is consistent with your error message:\nyum groups mark install \"Development Tools\"\nyum groups mark convert \"Development Tools\"\nyum groupinstall \"Development Tools\"\nSource: https://access.redhat.com/discussions/1262603",
    "Dockerfile RUN command returning \"No such file or directory\"": "I run your Dockerfile with the same error.\nBut when I changed from this:\n     RUN \"./configure --prefix=/usr/local\"\nto this:\n     RUN ./configure --prefix=/usr/local\nI still had a problem with RUN \"make\" so you also need to change Dockerfile like this:\nRUN [\"tar\",\"zxf\",\"Python-3.5.2.tgz\"]\nWORKDIR \"/Python-3.5.2\"\nRUN ./configure --prefix=/usr/local\n# changed \nRUN make\n# changed \nRUN make altinstall\nRUN [\"rm\",\"-rf\",\"Python-3.5.2\"]\nAnd this works for me. Or you can use the way with RUN [\"xy\",\"ab\"] like @Rao answered.",
    "Mapping a Volume in a docker": "The order of \"-v\" parameter passed was incorrect.\nInstead of going to docker command \"-v\" options it going to mongo container. Hence reorder the command as shown below\ndocker run -p 27018:27017 -v /Users/<user>/data:/data --name mongo2_001 -d mongo\nOn windows\ndocker run -p 27017:27017 -v /c/Users/<user>/data:/data --name mongo2_001 -d mongo",
    "What are the reasons not to use many RUN commands in a Dockerfile?": "Each execution of a RUN command creates a temporary container from the last resulting image, executes your commands, and saves the result as a new layer. Minimizing RUN commands both reduces the amount of overhead from these intermediate containers, but can also dramatically shrink the size of the resulting image.\nIf, for example, you do 2 run commands, one that downloads 1 gig of data, and a second that deletes that gig of data, your resulting image will exceed one gig even though it's not visible in the running container.\nTherefore, when doing large downloads of cached files to do an install or build of an app and you cleanup that build environment when finished, it's a good practice to do that as a single step so the deleted files never make it into any part of the image.\nOne last reason is for the cache. If you need to pull a new version of an app from a package repository, you also want to update your info on that remote repository (e.g. apt-get update) before doing an install to pull the latest version. If you separate the apt-get update from the apt-get install, the update command may be cached from an old build and the install will attempt to pull old or non-existent files.",
    "Why is the port not forwarded in Docker?": "EXPOSE does not actually set port, it is basically documentation:\nThe EXPOSE instruction doesn't actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published.\nSince .NET 8 in ASP.NET Core images default port is 8080 - see Default ASP.NET Core port changed from 80 to 8080.\nEither change the mapping to reflect that:\nports:\n  - \"8000:8080\"\nOr set port to 80 for example by using ASPNETCORE_HTTP_PORTS environment variable:\nenvironment:\n   - ASPNETCORE_HTTP_PORTS=80\nOr\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base\n#USER app\nWORKDIR /app\nENV ASPNETCORE_HTTP_PORTS=80\nEXPOSE 80/tcp",
    "What does it do when we defines USER in docker file [closed]": "A Docker container is an isolated environment. It doesn't know about the host user in any meaningful way. The container has its own /etc/passwd file, for example, which maps names to numeric user IDs.\nThe other important conceptual note is that, for most practical purposes, there's no need to \"create a user\". If you docker run -u 12345 a container with an arbitrary numeric uid, things will probably work fine even though you haven't formally \"created the user\" and it doesn't have an entry in /etc/passwd.\nTo answer your specific questions:\nDockerfile USER specifies a numeric uid, or a user name in the image's passwd file. It's not related to a host user, though it's possible for the numeric uids to coincidentally match. It's not a good practice to build an image specific to a single host user.\nIf you say USER 12345 nothing in particular happens to \"create the user\", but you'll still be working in a context where for example getuid(2) returns 12345 and files will be owned by that numeric uid.\nIf you've previously RUN adduser to create a user, using its name in a USER statement is clearer. (And it's slightly better practice to let the system choose the numeric uid than to specify it yourself; maybe create a \"system\" user.)\nInteractive sessions in terminals aren't a critical use case for Docker IMHO. Any work you do in this terminal will be lost as soon as the container exits. You're in the isolated container environment, working as the container user, limited to the container filesystem; you cannot see the host files or host username. Some Dockerfiles choose to give unrestricted passwordless sudo privilege to their \"non-root\" user and then you could freely sudo commands in this shell; I'd suggest not installing sudo at all.",
    "docker desktop cannot start, an unexpected error ocurred": "See the answer from this post.\nNavigate to Appdata/Roaming/Docker folder in your user account Open the settings file(JSON file) and make sure the below settings are as shown:\n\"integratedWslDistros\" : [ ]\n\"enableIntegrationWithDefaultWslDistro\" : false,\nSave the changes and restart docker desktop",
    "Docker ENTRYPOINT shell form with parameters": "ENTRYPOINT string_here\n...has Docker run:\n[\"sh\", \"-c\", \"string_here\"]\nThe problem with this is that when you add more arguments, they're added as new elements on the argument vector, as in:\n[\"sh\", \"-c\", \"string_here\", \"arg1\", \"arg2\", \"arg3...\"]\nwhich means they're ignored, because string_here, when invoked as a script, doesn't look at what further arguments it's given.\nThus, you can use:\nENTRYPOINT string_here \"$0\" \"$@\"\nwhere \"$@\" in shell expands to \"$1\" \"$2\" \"$3\" ..., and $0 is the first argument following -c (which is typically the name of the script or executable, and used in error messages written by the shell itself).",
    "Error no such file or directory when docker compose build": "The command you are using to install symfony:\ncurl -sS https://get.symfony.com/cli/installer | bash\nIs installing Symfony version 5. The files are located in symfony5 folder (notice the 5):\nThe Symfony CLI was installed successfully!\n\nUse it as a local file:\n  /root/.symfony5/bin/symfony\nSo instead of running mv /root/.symfony/bin/symfony /usr/local/bin (.symfony folder does not exist), you should run:\nmv /root/.symfony5/bin/symfony /usr/local/bin\nBy the way, as of 2022, using this script seems not to be the recommended method anymore: see https://symfony.com/download.\nFor info, the commit who changes the folder in the script from .symfony to .symfony5 is this one: https://github.com/symfony-cli/symfony-cli/commit/5301ebfa8f5918cae7fa0bf63c86f84dbd70f599.",
    "what are immutable tags in docker": "In the registry, all content is content addressable, referenced by a digest (currently sha256). If you pull an image with its digest, it is always the same thing since the digest for the image layers and configuration are packaged in a manifest that has its own digest, resulting in something that looks like a Merkle tree.\nSince humans are not good at understanding and remembering digests, we have tags. These tags are readable strings that get translated to the digest by the registry. The tags are effectively a pointer.\nOn most registries, you can push a new image to a tag, replacing where the tag points. If you have a tag for v1.0, you may also push a tag for v1, and any user that wants the latest release of v1 pulls the v1 tag. So when you push v1.1, you also push v1, changing that pointer. Those are mutable tags, or what some may describe as a moving tag.\nSome registries support immutable tags, so that once the tag is pushed, it can never reference a different digest. If the registry doesn't support this, some organizations may implement this as policy. That said, you're depending on the registry and the administrators to be well behaved, so security will still recommend using digests when you really want immutable images.",
    "Can not copy local directory to remote container with docker using docker context": "The volumes: are always interpreted by the Docker daemon running the container. If you're using contexts to point at a remote Docker daemon, the volumes: named volumes and file paths are interpreted by that remote daemon, and point at files on the remote host.\nWhile docker build will send the local build context to a remote Docker daemon, volumes: cannot be used to copy files between hosts.\nThe straightforward solution here is to delete the volumes: from your Compose setup:\nversion: '3.8'\nservices:\n  app:\n    build: .\n    restart: always \n    ports:\n      - 5000:5000\n    # volumes: will cause problems\n    # none of the other options should be necessary\n    # (delete every networks: block in the file)\nThen when you docker-compose build the image, the builder mechanism will send your local file tree to the remote Docker daemon. When you then run docker-compose up the remote Docker will use the code built into the image, and not try to overwrite it with content on the remote system.\nThis same principle applies in other contexts that don't necessarily involve a second host; for example, if you're launching a container from inside another container using the host's Docker daemon, volume mounts are in the host and not the container filesystem. In general I'd recommend not trying to overwrite your application code with volumes: mounts; run the code that's actually built into the image instead, and use local non-Docker tooling for live development.",
    "Docker COPY destination path not found": "You need to specify the build-arg as many times as the arguments\ndocker image build --build-arg src_app_dir=\"local_app_dir\" --build-arg dest_app_dir=\"server_app_dir\" --tag arg_env:1.0 --file mydockerfile .\nExample\nEDIT: Forgot to add context. Thanks @BMitch",
    "geoip_country_name returns \"-\" in nginx": "The \"-\" is what the logfile uses when the value is empty. GeoIP uses the $remote_addr to calculate the source of the request.\n172.17.0.1 is not a public IP address, it is an internal address of one of your proxy servers. Check the $http_x_forwarded_for header value for the real remote address (assuming your reverse proxy servers are configured correctly.\nThe Geoip module provides the geoip_proxy directive to ignore $remote_addr and use $http_x_forwarded_for instead.\nFor example (added to your other geoip_ directives):\ngeoip_proxy 172.17.0.1;",
    "NVIDIA cuDNN installation in ubuntu20.04 docker container": "I found necessary repos in Internet on official sites for developers:\ncuda repos for different OS: https://developer.download.nvidia.com/compute/cuda/repos/\nlibcudnn8 repos for different cuda versions: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/\nonnx execution providers compatibility matrix for cuda and cuDNN: https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html\nSo, in my case the answer was:\nENV cudnn_version=8.2.4.15\nENV cuda_version=cuda11.4",
    "Docker defeats the entire purpose of Environment Variables?": "You can pass environment variables when running the container either by defining them then and there or by loading them from a file.\nFor example, if you are planning to use a file,\ndocker run --env-file ./env.list ubuntu bash\nThis file should use the syntax <variable>=value (which sets the variable to the given value) or (which takes the value from the local environment), and # for comments.\nFor more information about different practices of using environment variables, please refer Set environment variables (-e, --env, --env-file).",
    "/bin/sh: 1: apk: not found, while build image docker": "The python:3.8-slim is based on Debian, which is using the apt package manager, not apk.\nYou can change the image version:\nFROM python:3.8-alpine\nand it should work fine.\nAnother solution is to keep the python:3.8-slim image, but switch to apt:\nRUN apt update \\\n    && apt install -y libmariadb-dev \\\n        gcc\\\n        python3-dev \\\n        libcogl-pango-dev \\\n        libcairo2-dev \\\n        libtool \\\n        linux-headers-amd64 \\\n        musl-dev \\\n        libffi-dev \\\n        libssl-dev \\\n        libjpeg-dev \\\n        zlib1g-dev\nI have updated the package names because they are not the same in Debian.",
    "Understanding workflow of multi-stage Dockerfile": "To answer from a less DevSpace-y persepctive and a more general Docker-y one (With no disrespect to Lukas!):\nQuestion 1\nBreakdown\n\u274c Are you docker build ... this entire image and then just docker run ... --target= to run a specific stage\nYou're close in your understanding and managed to outline the approach in your second part of the query:\n\u2705 or are you only building and running the specific stages you need (e.g. docker build ... -t test --target=test && docker run test ...)?\nThe --target option is not present in the docker run command, which can be seen when calling docker run --help.\nI want to say it isn't the former because you end up with a bloated image with build kits and what not... correct?\nYes, it's impossible to do it the first way, as when --target is not specified, then only the final stage is incorporated into your image. This is a great benefit as it cuts down the final size of your container, while allowing you to use multiple directives.\nDetails and Examples\nIt is a flag that you can pass in at build time so that you can choose which layers to build specifically. It's a pretty helpful directive that can be used in a few different ways. There's a decent blog post here talking about the the new features that came out with multi-stage builds (--target is one of them)\nFor example, I've had a decent amount of success building projects in CI utilising different stages and targets, the following is pseudo-code, but hopefully the context is applied\n# Dockerfile\nFROM python as base\n\nFROM base as dependencies\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nFROM dependencies as test\n\nCOPY src/ src/\nCOPY test/ test/\n\nFROM dependencies as publish\n\nCOPY src/ src/\n\nCMD ...\nA Dockerfile like this would enable you to do something like this in your CI workflow, once again, pseudo-code-esque\ndocker build . -t my-app:unit-test --target test\ndocker run my-app:unit-test pyunit ...\ndocker build . -t my-app:latest\ndocker push ...\nIn some scenarios, it can be quite advantageous to have this fine grained control over what gets built when, and it's quite the boon to be able to run those images that comprise of only a few stages without having built the entire app.\nThe key here, is that there's no expectation that you need to use --target, but it can be used to solve particular problems.\nQuestion 2\nWhen it comes to local Kubernetes development (minikube, skaffold, devspace, etc.) and running unit tests, are you supposed referring to these stages in the Dockerfile (devspace Hooks or something) or using native test tools in the container (e.g. npm test, ./manage.py test, etc.)?\nLukas covers a devspace specific approach very well, but ultimately you can test however you like. Using devspace to make it easier to run (and remember to run) tests certainly sounds like a good idea. Whatever tool you use to enable an easier workflow, will likely still use npm test etc under the hood.\nIf you wish to call npm test outside of a container that's fine, if you wish to call it in a container, that's also fine. The solution to your problem will always change depending on your landscape. CICD helps to standardise on external factors and provide a uniform means to ensure testing is performed, and deployments are auditable\nHope that helps in any way shape or form \ud83d\udc4d",
    "Where I can find all variables of docker image": "I would use docker exec to get a shell inside the container and use printenv like below:\n\u276f docker ps\nCONTAINER ID   IMAGE                              COMMAND                  CREATED        STATUS         PORTS                                             NAMES\na35281f615b1   adminer                            \"entrypoint.sh docke\u2026\"   2 months ago   Up 6 hours     0.0.0.0:8090->8080/tcp, :::8090->8080/tcp         data-mock_adminer_1\n016b075fee00   quay.io/keycloak/keycloak:latest   \"/opt/jboss/tools/do\u2026\"   2 months ago   Up 5 seconds   8443/tcp, 0.0.0.0:80->8080/tcp, :::80->8080/tcp   docker-scripts_keycloak_1\n0409951e3c5f   mysql:5.7                          \"docker-entrypoint.s\u2026\"   2 months ago   Up 5 seconds   3306/tcp, 33060/tcp                               docker-scripts_mysql_1\n\u276f docker exec -it 016 /bin/bash\nbash-4.4$ printenv\nLAUNCH_JBOSS_IN_BACKGROUND=1\nLANG=en_US.UTF-8\nPROXY_ADDRESS_FORWARDING=false\nHOSTNAME=016b075fee00\nJDBC_MARIADB_VERSION=2.5.4\nDB_USER=keycloak\ncontainer=oci\nJDBC_POSTGRES_VERSION=42.2.5\nDB_ADDR=mysql",
    "How can I temporarily change the time in my ddev web container (faketime, libfaketime)": "The answer from @stasadev works for plain PHP but not for Drupal. This is because of how Drupal stores generated PHP code using an mtime protected file storage which seems to be incompatible with faketime.so, as well as some minor config requirements.\nThe documentation of Drupal\\Component\\PhpStorage\\MTimeProtectedFileStorage says:\nEach file is stored in its own unique containing directory. The hash is based on the virtual file name, the containing directory's mtime, and a cryptographically hard to guess secret string. Thus, even if the hashed file name is discovered and replaced by an untrusted file (e.g., via a move_uploaded_file() invocation by a script that performs insufficient validation), the directory's mtime gets updated in the process, invalidating the hash and preventing the untrusted file from getting loaded. Also, the file mtime will be checked providing security against overwriting in-place, at the cost of an additional system call for every load() and exists().\nThe PHP function utilized is filemtime and it looks that it isn't compatible with faketime.so.\nCustomize DDEV Dockerfile\nUse this .ddev/web-build/Dockerfile (based on this SO answer, amd64 only):\nCOPY --from=trajano/ubuntu-faketime /faketime.so /lib/faketime.so\nENV LD_PRELOAD=/lib/faketime.so DONT_FAKE_MONOTONIC=1 \\\n  FAKETIME_DONT_RESET=1\nThere is a small difference to the Dockerfile in @stasadev's answer, because we must set another parameter (FAKETIME_DONT_RESET=1) to make sure all processes (including sub-processes) use the same time. Without this parameter each (sub-) process would start with the defined time again which makes no sense for a web application and could lead to problems.\nReplace the PHP file storage\nWe can solve this by replacing the mtime protected file storage with a normal file storage. Be aware that this is insecure and therefore should only be used locally in the DDEV project but NOT on production!!!\nApply the following patch to Drupal Core manually (not using Composer patches or similar which persists in git):\ndiff --git a/core/lib/Drupal/Core/PhpStorage/PhpStorageFactory.php b/core/lib/Drupal/Core/PhpStorage/PhpStorageFactory.>\nindex 85ca11bb63..5d5801feae 100644\n--- a/core/lib/Drupal/Core/PhpStorage/PhpStorageFactory.php\n+++ b/core/lib/Drupal/Core/PhpStorage/PhpStorageFactory.php\n@@ -38,7 +38,7 @@ public static function get($name) {\n       $configuration = $overrides['default'];\n     }\n     // Make sure all the necessary configuration values are set.\n-    $class = $configuration['class'] ?? 'Drupal\\Component\\PhpStorage\\MTimeProtectedFileStorage';\n+    $class = $configuration['class'] ?? 'Drupal\\Component\\PhpStorage\\FileStorage';\n     if (!isset($configuration['secret'])) {\n       $configuration['secret'] = Settings::getHashSalt();\n     }\n(Patch created for 10.2)\nNext you have to delete the PHP file storage rm -rf [public://php] which usually is located in web/sites/default/files/php: (replace [public://php] with the real path as the shell cannot resolve this PHP stream wrapper) to remove all mtime protected generated code. Then clear the Drupal caches (drush cr).\nClean up\nAfter you're done debugging your site with faketime you should disable the Dockerfile customization (i.e. by renaming the file), remove the above patch, delete the PHP file storage again and then clear the Drupal caches.\nRemoving the PHP file storage after applying/removing the patch is necessary because both implementations generate the file paths differently:\nthe normal file storage generates a file php/[machine_name] (without PHP extension)\nthe mtime protected file storage generates php/[machine_name]/ as folder and inside it a file with a cryptographic name.\nConfigure faked time\nUse FAKETIME with DDEV config:\nddev config --web-environment-add 'FAKETIME=[timestamp]'\nThere are two ways to define [timestamp]:\n2020-01-01 01:00:00 sets the time of the DDEV web container statically to the defined time. This means the clock doesn't count and every time you execute code requesting the current time from the system you will get exactly this timestamp. This is not very useful for Drupal or other websites and apps.\n@2020-01-01 01:00:00 (note the starting @) sets the time and starts the clock, which is the expected behavior of a (web) server.\nNotes\nBe aware that this only fakes the time of the DDEV web container but neither the database container nor other systems. This can cause strange problems/results if you for example have SQL queries using the current server time (CURDATE(), CURRENT_DATE(), CURRENT_TIME(), NOW(), UNIX_TIMESTAMP(), UTC_DATE(), UTC_TIME(), UTC_TIMESTAMP() etc.) or use a JavaScript provided (browser time) timestamp in comparison with a server generated timestamp either on the client or the server!",
    "Copy some files and a whole folder from host into container using a unique COPY statement in a Dockerfile?": "You can create a folder with the following structure alongside your Dockerfile, and then copy the contents into your apps directory,\n$ tree\n.\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 folder\n    \u251c\u2500\u2500 binaries\n    \u2502   \u251c\u2500\u2500 executable1\n    \u2502   \u251c\u2500\u2500 executable2\n    \u2502   \u2514\u2500\u2500 executable3\n    \u251c\u2500\u2500 entrypoint.sh\n    \u2514\u2500\u2500 requirements.txt\n\n2 directories, 6 files\nDockerfile,\nFROM debian:buster-slim\nWORKDIR /app\nCOPY folder .\nroot@b111e73a17c1:/app# tree\n.\n|-- binaries\n|   |-- executable1\n|   |-- executable2\n|   `-- executable3\n|-- entrypoint.sh\n`-- requirements.txt\n\n1 directory, 5 files",
    "Can Docker CLI, Podman and other similar tools have shared local storage for images?": "Docker and Podman do not share the same storage. They cannot, because Docker controls lock it to its storage within the daemon. On the other hand, Podman, Buildah, CRI-O, Skopeo can all share content between one another.\nPodman and other tools can work with the docker-daemon storage indirectly, via the \"docker-daemon\" transport.\nSomething like this should work:\npodman run docker-daemon:alpine echo hello\nNote, that podman is pulling the image out of the docker daemon and is storing the image in its containers storage, and then running the container rather than using the Docker storage directly.\nYou can also do:\npodman push myimage docker-daemon:myimage\nTo copy an image from containers/storage into the docker daemon (but not in the opposite direction as you probably would prefer).",
    "Run kubectl inside dockerimage": "Instead of installing using apt-get, you can download the binary place whatever you want and use it.\nThis will give you more control under it and less chances to have problems in the future.\nSteps on how to download it from the official repository can be fount in the documentation.\nInstall kubectl binary with curl on Linux\nDownload the latest release with the command:\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl\nTo download a specific version, replace the $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) portion of the command with the specific version.\nFor example, to download version v1.18.0 on Linux, type:\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl\nMake the kubectl binary executable.\nchmod +x ./kubectl\nMove the binary in to your PATH.\nsudo mv ./kubectl /usr/local/bin/kubectl\nTest to ensure the version you installed is up-to-date:\nkubectl version --client\nConsidering this, you can have a Dockerfile similar to this:\nFROM debian:buster\nRUN apt update && \\\n      apt install -y curl && \\\n      curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && \\\n      chmod +x ./kubectl && \\\n      mv ./kubectl /usr/local/bin/kubectl\nCMD kubectl get po\nAfter this we can create a pod using the following manifest:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: internal-kubectl\nspec:\n  containers:\n    - name: internal-kubectl\n      image: myrep/internal-kubectl:latest\n      command: ['sh', '-c', \"kubectl get pod; sleep 36000\"]\nRunning this pod is going to give you an error and this will happen because you don't have the necessary RBAC rules created.\nThe way to tell Kubernetes that we want this pod to have an identity that can list the pods is through the combination of a few different resources\u2026\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: internal-kubectl\nThe identity object that we want to assign to our pod will be a service account. But by itself it has no permissions. That\u2019s where roles come in.\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: modify-pods\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n      - list\n      - delete\nThe role above specifies that we want to be able to get, list, and delete pods. But we need a way to correlate our new service account with our new role. Role bindings are the bridges for that\u2026\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: modify-pods-to-sa\nsubjects:\n  - kind: ServiceAccount\n    name: internal-kubectl\nroleRef:\n  kind: Role\n  name: modify-pods\n  apiGroup: rbac.authorization.k8s.io\nThis role binding connects our service account to the role that has the permissions we need. Now we just have to modify our pod config to include the service account\u2026\napiVersion: v1\nkind: Pod\nmetadata:\n  name: internal-kubectl\nspec:\n  serviceAccountName: internal-kubectl\n  containers:\n    - name: internal-kubectl\n      image: myrep/internal-kubectl:latest\n      command: ['sh', '-c', \"kubectl get pod; sleep 36000\"]\nBy specifying spec.serviceAccountName this changes us from using the default service account to our new one that has the correct permissions. Running our new pod we should see the correct output\u2026\n$ kubectl logs internal-kubectl\nNAME               READY   STATUS    RESTARTS   AGE\ninternal-kubectl   1/1     Running   1          5s",
    "Deno dockerfile denon": "The ENTRYPOINT of that image is deno, so the command you're trying to run is:\ndeno denon run --allow-net src/main.ts\nFor that docker image you need to change the ENTRYPOINT\nENTRYPOINT [\"/home/deno/.deno/bin/denon\"]\nCMD [\"run\", \"--allow-net\", \"src/main.ts\"]",
    "Docker build image fails - No such file or directory": "I had the same issue. The error is a bit misleading because it has your .csproj listed at the end, but this is the key part:\nThe specified task executable \"node\" could not be run.\nIt is actually looking to run the command \"node\" and it cannot find the executable. To resolve this, I added Node.js just prior to my build step.\n...\nWORKDIR \"/src/.\"\n#Installing Node.js in build container\nRUN apt-get update && apt-get -y install nodejs                   \nRUN dotnet build \"CyberEvalNextGen.csproj\" -c Release -o /app/build\n...\nThis is using Linux containers, but I believe you can do something similar with PowerShell in Windows containers.",
    "Can't build gradle application inside docker": "Can you try the below Dockerfile as have changed a little bit.\nFROM openjdk:8-jdk\n#install git\nRUN apt-get install -y git\nRUN git clone https://github.com/SFRJ/yurl.git\n#install gradle\nRUN wget https://downloads.gradle-dn.com/distributions/gradle-6.5-bin.zip\nRUN unzip gradle-6.5-bin.zip\nENV GRADLE_HOME /gradle-6.5\nENV PATH $PATH:/gradle-6.5/bin\n#compile and run app\nWORKDIR yurl\nRUN gradle clean build --rerun-tasks --no-build-cache\nENTRYPOINT [\"java\", \"-jar\", \"/yurlapp.jar\"]\nThe problem was, you mentioned a stageRUN cd yurl in which you are changing a directory, which is only valid for that particular stage not for the remaining stages. If you want to use that particular directory for the other stages as well. Use WORKDIR to run that operation which I did above.\nP.S.:- If you want to use that directory only for 1 stage, then instead of WORKDIR use RUN cd DIR && ls -lart like command in 1 stage itself.",
    "Running npm script in Dockerfile before dotnet build": "As you are using multistage build and you just copied in 1 stage not others, that's why 1 or the another stage is breaking. Check the below Dockerfile\nFROM mcr.microsoft.com/dotnet/core/aspnet:latest AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM node:lts AS node\nWORKDIR /src\nCOPY . .\nRUN npm install\nRUN npm run build\n\nFROM mcr.microsoft.com/dotnet/core/sdk:latest AS build\nCOPY . .\nRUN dotnet restore \"Project.csproj\"\nRUN dotnet build \"Project.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nCOPY . .\nRUN dotnet publish -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"Project.dll\"]",
    "Docker : ERROR: unsatisfiable constraints": "Did the package you want to install move to a different registry?\nSometimes the packages that are being installed are moved from different registry branches. The default Docker Alpine image only has certain registries set. Adding additional registries expands your installation options (YMMV); I can't speak to the stability, security, and or risks associated with different registry branches. This thread helped me.\nFailed Attempt & Error Message:\n\u276f docker run -it alpine sh                                                                                                                                                                 \n/ # apk update\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz\nv3.12.0-175-g8b3334c57c [http://dl-cdn.alpinelinux.org/alpine/v3.12/main]\nv3.12.0-178-gb27c83e867 [http://dl-cdn.alpinelinux.org/alpine/v3.12/community]\nOK: 12749 distinct packages available\n/ # apk add cowsay\nERROR: unsatisfiable constraints:\n  cowsay (missing):\n    required by: world[cowsay]\n/ # \nOur first clue from the output\nERROR: unsatisfiable constraints:\n       cowsay (missing):\n       required by: world[cowsay]\nSecond Clue from the output\nearlier in the log\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz\nSolutioning\nSearch on DuckDuckGo for package <package name>\nDirected me to URL: https://pkgs.org/download/cowsay\nWhich type of memory am I using?\nThe answer is found in clue # 2. x86_64\nOn the page for the packages select the proper link. so in my case:\nAlpine Testing x86_64 > cowsay-3.04-r0.apk\nReading the information on that page I see that under Download the binary for this package is in presently part of\nhttp://dl-cdn.alpinelinux.org/alpine/edge/testing/x86_64/cowsay-3.04-r0.apk\nComparing this to clue #2, I see that the alpine container is not referencing the registry with the binary I want to install.\nSuccess & TL;DR;\nAdding a package previously not found to alpine by adding additional registry to docker alpine container.\nI'll add the registry that I need to find the package I want to install (step #3). In the code block below see that the third registry matches the initial part of the URL from the research done in part #3. I don't want to replace the existing registries (clue #2) so I set those again. I don't know if this is necessary or not but I did it anyway.\n$ docker run -it alpine sh\n/# apk update && apk add cowsay \\\n        --update-cache \\\n        --repository https://alpine.global.ssl.fastly.net/alpine/edge/community \\\n        --repository https://alpine.global.ssl.fastly.net/alpine/edge/main \\\n        --repository https://dl-3.alpinelinux.org/alpine/edge/testing\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz\nv3.12.0-175-g8b3334c57c [http://dl-cdn.alpinelinux.org/alpine/v3.12/main]\nv3.12.0-178-gb27c83e867 [http://dl-cdn.alpinelinux.org/alpine/v3.12/community]\nOK: 12749 distinct packages available\nfetch https://dl-3.alpinelinux.org/alpine/edge/testing/x86_64/APKINDEX.tar.gz\nfetch https://alpine.global.ssl.fastly.net/alpine/edge/main/x86_64/APKINDEX.tar.gz\nfetch https://alpine.global.ssl.fastly.net/alpine/edge/community/x86_64/APKINDEX.tar.gz\n(1/3) Installing libbz2 (1.0.8-r1)\n(2/3) Installing perl (5.30.3-r2)\n(3/3) Installing cowsay (3.04-r0)\nExecuting busybox-1.31.1-r16.trigger\nOK: 43 MiB in 17 packages\n/ #\nDockerfile example\nFROM alpine\nRUN apk update && apk add cowsay \\\n        --update-cache \\\n        --repository https://alpine.global.ssl.fastly.net/alpine/edge/community \\\n        --repository https://alpine.global.ssl.fastly.net/alpine/edge/main \\\n        --repository https://dl-3.alpinelinux.org/alpine/edge/testing\nCMD [\"cowsay\", \"hi stackoverflow\"]\nAfter building this file:\n \u276f docker run cowsay                                                                                                                                                                        [13:13:45]\n __________________ \n< hi stackoverflow >\n ------------------ \n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||",
    "Service PHP failed build Docker": "Try adding libonig-dev to your system dependencies, ie apt-get install libonig-dev",
    "docker pip install not working with requirements file": "you can use:\nRUN pip install -r requirements.txt",
    "How are CMD and ENTRYPOINT exec forms in a docker file parsed?": "At a very low level, Linux commands are executed as a series of \"words\". Typically your shell will take a command line like ls -l \"a directory\" and breaks that into three words ls -l a directory. (Note the space in \"a directory\": in the shell form that needs to be quoted to be in the same word.)\nThe Dockerfile CMD and ENTRYPOINT (and RUN) commands have two forms. In the form you've specified that looks like a JSON array, you are explicitly specifying how the words get broken up. If it doesn't look like a JSON array then the whole thing is taken as a single string, and wrapped in an sh -c command.\n# Explicitly spelling out the words\nRUN [\"ls\", \"-l\", \"a directory\"]\n\n# Asking Docker to run it via a shell\nRUN ls -l 'a directory'\n# The same as\nRUN [\"sh\", \"-c\", \"ls -l 'a directory'\"]\nIf you specify both ENTRYPOINT and CMD the two lists of words just get combined together. The important thing for your example is that sh -c takes the single next word and runs it as a shell command; any remaining words can be used as $0, $1, ... positional arguments within that command string.\nSo in your example, the final thing that gets run is more or less\nENTRYPOINT+CMD [\"sh\", \"-c\", \"jupyter\", ...]\n# If the string \"jupyter\" contained \"$1\" it would expand to the --ip option\nThe other important corollary to this is that, practically, ENTRYPOINT can't be the bare-string format: when the CMD is appended to it you get\nENTRYPOINT some command\nCMD with args\n\nENTRYPOINT+CMD [\"sh\", \"-c\", \"some command\", \"sh\", \"-c\", \"with args\"]\nand by the same rule all of the CMD words get ignored.\nIn practice you almost never need to explicitly put sh -c or a SHELL declaration in a Dockerfile; use a string-form command instead, or put complex logic into a shell script.",
    "Running NodeJs binary with scratch Docker image": "I think @Isanych is right, as scratch is good to run c++, go binaries but I did not find a way to run pkg executable file on scratch image, so here is the solution that is based on alpine, but alpine still needs some dependency that is mentioned here, and it's working with below image\nYou can try this\nFROM node:10 as build\nWORKDIR /app\nCOPY index.js .\nRUN npm i pkg -g\nRUN pkg -t node10-alpine-x64 index.js\nFROM alpine\nRUN apk add --no-cache libstdc++ libgcc\nWORKDIR /app\nCOPY --from=build /app/ .\nCMD [\"./index\"]\nBonus: your image is still below then 50 MB.",
    "Unable to locate package Python": "Step 2/7 : RUN apt-get update\n    ---> Using cache\nYou should run apt-get update and apt-get install in the same RUN instruction as follows:\n RUN apt-get update && apt-get install -y python\nEach instruction in a Dockerfile will create a separate layer in the image and the layers are cached. So the apt-get update might just use the cache and not even run. This has happened in your case as well. You can see the line ---> Using cache in your logs. You can use docker build --no-cache to make Docker rebuild all the layers without using cache.\nYou can instead just use the python:3 official image as the base image to run your Python applications.",
    "build docker image using graalvm Error: Main entry point class 'app.jar' not found com.oracle.svm.core.util.UserError$UserException": "Your command line for native-image contains -cp -jar app.jar. This is interpreted as -jar is the classpath and app.jar is the main class.\nI think you wanted native-image --no-server -cp app.jar com.arindam.knative.gke.KnativeSpringGkeApplication.",
    "Docker Can't Find = in \"ENV\" Must be of the form: name=value": "You can try\nENV NPM_VERSION=6.4.1\nENV IONIC_VERSION=3.19.0 \nENV CORDOVA_VERSION=9.0.0\nor\nENV NPM_VERSION=6.4.1 IONIC_VERSION=3.19.0 CORDOVA_VERSION=9.0.0\nI think multiple env variables with new lines are not valid syntax.",
    "How to speed up clone of a Git Repo into Docker Container?": "If you want recent code changes to be fetched into an existing container, there isn't really a way around running git clone in the container, so that you can later git pull.\nIf you don't need the entire history, then perhaps git clone --depth 1 would speed up the initial clone.\nRUN git clone --depth 1 ssh://git@foobar.com/sample_repo.git /sample_repo\nBy providing an argument of --depth 1 to the clone command, the process will copy only the latest revision of everything in the repository. This can be a lifesaver for Git servers that might otherwise be overwhelmed by CI/CD (Continuous Integration / Continuous Delivery) automation.\nIf you don't want git at all in the container, and are comfortable rebuilding the image to get code changes, then a helper script that does a git archive to the host machine, and then an ADD statement in the Dockerfile would work too.",
    "How to associate one file type to another in visual studio code?": "CMD + Shift + P (CTRL + Shift + P for Linux/Windows) then type > Change Language Mode, press Enter, \"Configure File Association for '.dev'...\" and select Docker. Done!",
    "Can we pass ARG to CMD in DockerFile?": "Docker will pass the environment variable to your container, but you need a shell to parse the ${var} syntax into that variable. You got close with:\nCMD [\"sh\",\"./${ORG_SCRIPT}\"]\nI suspect the reason this didn't work is because the arg passed to sh hasn't been expanded yet, and sh isn't processing that arg, it's just running it. Instead you need:\nCMD [\"sh\",\"-c\",\"./${ORG_SCRIPT}\"]\nThat is almost exactly the same as the string/shell syntax, which I'd recommend instead:\nCMD ./${ORG_SCRIPT}",
    "How to overwrite nginx default config with docker file?": "I believe you should use\nCOPY ./nginx/default.conf /etc/nginx/nginx.conf \nIf you want to organise things in conf.d/, add include /etc/nginx/conf.d/* in /etc/nginx/nginx.conf before adding things in conf.d/\nComplete config file:\nworker_processes 4;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n  server {\n      listen       80 default;\n      server_name  localhost;\n      return 301 https://www.google.com$request_uri;\n  }\n}",
    "Is there a way to prevent overwriting a file in a docker image?": "This isn\u2019t possible. (A typical Dockerfile does most of its work as a root user, and if it doesn\u2019t, it can trivially USER root if it really wants to overwrite that file.) This also doesn\u2019t seem like a big deal for the case you describe since Docker images are usually single-purpose, and if some specific app running on top of your Tomcat image really wants to reconfigure the app server, Docker\u2019s isolation means it\u2019s not going to affect anything else.\nIf you really want to you can put a copy of the file in a safe place and copy it back in during an entrypoint script, but if I were a downstream developer and I ran across this behavior I\u2019d probably be a little frustrated (this definitely violates the principle of least surprise).",
    "how to run bower install inside subfolder in Docker": "Use\nRUN cd app && bower install\nOr\nWORKDIR app\nbower install\nthis changes the pwd for all following commands, which you may not want.\nPS. Careful with COPY . ., it may overwrite what you just built.",
    "Where are the official Ruby 1.9 Docker Images?": "It's actually available, but not visible in Dockerhub:\nYou can see it via the API:\nhttps://hub.docker.com/v2/repositories/library/ruby/tags/1.9.3/\nThe fact that it pulls from library/x, means that it's an official version.",
    "Connecting NodeJS Docker container to a MySQL Docker container - docker-compose 3": "First of all there are a couple of problems here.\nFirstly you need to look at networking your docker containers in your docker-compose file together. Basically each container doesn't know of the other until you specify which network they are in. Each container can be in multiple but if you want two containers to connect they need to be in the same network. https://docs.docker.com/compose/networking/#configure-the-default-network\nThen Secondly you can't use localhost as a url. Basically a docker container is an isolated environment so it has it's own \"localhost\" so when you use that address it's actually trying to connect to the mysql on the nodejs container. So when networked properly you will be able to connect to the mysql container using their names you gave them in the docker-compose something like topsectiondb:3306 or something like this. This tells docker your using networking to connect one docker container to another and will allow you to make the initial connection. Connect to another container using Docker compose\n===================================================\nActual Answer:\nvar mysql      = require('mysql');\nvar connection = mysql.createConnection({\n  host     : 'localhost',\n  user     : 'me',\n  password : 'secret',\n  database : 'my_db'\n});\nBasically the mysql library requires a hostname, which in most cases is localhost, but as your linking containers with docker-compose you use the alias of the container. The port number is 3306 by default\nSo: host: \"topsectiondb\" will suffice!",
    "Dockerfile, switch between dev / prod": "You can use the ENTRYPOINT or CMD so you can execute a bash script inside the container as the first command.\nENTRYPOINT[\"your/script.sh\"]\nCMD[\"your/script.sh\"]\nin your script do your thing!\nEven you dont need to pass the env variable since in the script you can access it.",
    "Can't start rails application via docker": "First, you need to run this command for creating rails project inside in your container.\ndocker-compose run web rails new . --force --database=postgresql\nAfter this you need to rebuild the docker image\ndocker-compose build\nThan change your config/database.yml file\ndefault: &default\n  adapter: postgresql\n  encoding: unicode\n  host: db\n  username: postgres\n  password:\n  pool: 5\n\ndevelopment:\n  <<: *default\n  database: noteapp_development\n\ntest:\n  <<: *default\n  database: noteapp_test\nThan you need to run\ndocker-compose up --build",
    "What does `touch` in Dockerfile do?": "Because of Docker layer caching, in a lot of common cases the touch command won't do anything. If the jar file has changed then the ADD command will include it in the image with its last-modified time from the host (\"it is copied individually along with its metadata\"); since that's presumably recently, the touch command will update it to seconds later. If the jar file hasn't changed then Docker will skip both the ADD and RUN commands and use the filesystem output from the previous time you ran it, with the previous run's timestamp.\nIf the jar file is just being used as an input to java -jar then its last-modified time shouldn't be relevant to anything either.\nI'd guess you can safely remove the touch command with no ill effects. There are a couple of unnecessary sh -c invocations that don't matter and just clutter things. I'd guess this Dockerfile to be functionally equivalent:\n# Prefer COPY to ADD, unless you explicitly want Docker to fetch\n# URLs or unpack archives\nCOPY dist /dist/\nARG JAR_FILE\nCOPY target/${JAR_FILE} /target/app.jar\nEXPOSE 8080\n# Prefer CMD to ENTRYPOINT, if nothing else so `docker run imagename sh` works\n# Split simple commands into words yourself\nCMD [\"java\", \"-jar\", \"/target/app.jar\"]",
    "How can I pass JAVA_OPTS to tomcat in a docker container?": "Just simply add this in your Dockerfile :-\nENV spring.profiles.active=dev\nand other option is below, by adding one ENTRYPOINT:-\nENTRYPOINT [\"java\",\"-Dspring.profiles.active=container\",\"-jar\",\"/app.jar\"]",
    "Add more than one user to an image with Dockerfile": "This is because second useradd command is running as username1 which does not have sufficient privileges to add user\nMoving the second useradd before USER [username1] would make it work for you\nRUN useradd -ms /bin/bash username1\nRUN useradd -ms /bin/bash username2\nUSER username1\nWORKDIR /home/username1\n\nUSER username2\nWORKDIR /home/username2",
    "How to do ../ when using docker COPY": "It is not possible to COPY files from outside of the build context:\nThe <src> path must be inside the context of the build; you cannot COPY ../something /something, because the first step of a docker build is to send the context directory (and subdirectories) to the docker daemon.\nsource: https://docs.docker.com/engine/reference/builder/#copy",
    "Run sql script placed in docker volumen": "The official Docker mysql image will run everything present in /docker-entrypoint-initdb.d when the database is first initialized (see \"Initializing a fresh instance\" on that page). Since you're injecting it into the container using a volume, if the database doesn't already exist, your script will be run automatically as you have it.\nThat page also suggests creating a custom Docker image. The Dockerfile would be very short\nFROM mysql:5.7.22\nCOPY init.sql /docker/entrypoint-initdb.d/1-init.sql\nand then once you built the modified image you wouldn't need a copy of the script locally to have it run at first start.",
    "How to add files in docker container volume on build time": "Docker Volumes are designed to share folder in between Host Machine and Docker containers. If you copy any file on your host machine (volume location path), it will be automatically going to available inside containers.\nThe syntax for docker volume is as below:\n -v /home/data:/data\nIn the above syntax /home/data is folder available on the host machine and /data this folder is available inside docker container.\nIf you copy any file on the host machine inside /home/data folder, it will be automatically going to be available inside container /data folder.\nHope this is clear to you.\nIf you are using docker-compose then add volume tag as below\n volumes:\n   - /home/data:/data\nfor example:\nversion '3'\n services:\n  app:\n   image: nginx:alpine\n     ports:\n     - 80:80\n   volumes:\n    - /home/data:/data",
    "Prevent container from exiting, conditionally": "The only way to keep the docker container running is making it run a command that does not exit.\nIn your case, when you don't want the container to exit, you could run something like this:\ndocker run --name \"$container\" \"$tag\" sh -c \"r2g run && sleep infinity\"\nThis way, once the r2g command is finished, your container will wait indefinitely and keep running.",
    "Expose port does not work in docker": "The issue should be here:\n** Angular Live Development Server is listening on localhost: 4200\nThat's the container's localhost, so port 4200 will be unavailable for the docker host as well any other machine using the mapped 4200.\nYou should run ng with 0.0.0.0 binding:\nng serve --host 0.0.0.0\nand the mapping should work.",
    "Dockerfile is building from Centos:7 - is there a way to instruct npm to be installed?": "CentOS and (and other EL distributions) do not use apt, they use yum (or dnf). Unfortunately, npm isn't packaged in CentOS' normal repos, os you'd have to install the EPEL repo fist:\nSimple add a yum call in your Dockerfile:\nRUN yum install -y http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\nRUN yum install -y npm",
    "DockerFile for dotnetcore2.0 with multiple cs project references": "So what I found was you have to place your DOCKERFILE in the src folder which is the root of the solution. so that docker context becomes this path.\nNow you have a Dockerfile as below\nFROM microsoft/aspnetcore-build:2.0 as build\nWORKDIR /app\n\n\nCOPY . . \n\n#COPY Solution.sln .\n# RUN ls -l\nRUN dotnet restore ./Solution.sln\n\nCOPY . .\n\nRUN dotnet publish ./Ser/Host/Api/api.csproj --output /out/ --configuration Release\n\n# runtime image\nFROM microsoft/aspnetcore:2.0\nWORKDIR /app\nCOPY --from=build /out .\n\nENTRYPOINT [\"api.dll\"]\nHope this help someone having the issue what i am facing",
    "Dockerize laravel application using laradock": "Laradock is only for development as stated in their website. Dockerizing the app for production, you need to have a Dockerfile and docker-compose.yml files.",
    "Docker: not being able to set env variable": "The variables are set. The way you are trying to verify that they are set is wrong.\nIt's called \"variable expansion\" and I quote from the answer @larsks has given here:\n$ docker run -it alpine echo $HOME\n/home/lars\n\n$ docker run -it alpine echo '$HOME'\n$HOME\n\n$ docker run -it alpine sh -c 'echo $HOME'\n/root\nit prints the $HOME variable of your host, not the container's. In your case $FOO doesn't exit on the host, so it prints an empty line\nit prints $HOME (like a string)\nthis works, and prints the $HOME variable of the container\nExample for your case:\n$ docker run -e FOO=foofoo alpine sh -c 'echo $FOO'\nfoofoo\nThanks @vmonteco for pointing out docker inspect for debuging. If you want to learn more on what your containers were actually doing, below are the CMD set for the 3 different cases discussed previously:\n\"Cmd\": [\"echo\"]\n\"Cmd\": [\"echo\",\"$FOO\"]\n\"Cmd\": [\"sh\",\"-c\",\"echo $FOO\"]",
    "How to split docker layer?": "A new layer is created on each dockerfile instruction. So the solution is to split the RUN command into multiple RUN commands. However, I am not sure that this solution will work in your case if the tar is very big, as one of the layers will contain the tar. Nonetheless, you should try this approach.\nRUN wget http://server/huge.tar.gz\nRUN tar xzf huge.tar.gz\nRUN huge/install /opt/myinstall && \\\nRUN rm -rf huge*\nAnother alternative is to use docker multistage build. The idea is to install the tar in a separate container and just copy the installation directory to you container:\nFROM ... as installer\nRUN echo \"Downloading huge file\" && \\\n  wget   http://server/huge.tar.gz  && \\\n  echo \"Extracting huge file\" && \\\n  tar xzf huge.tar.gz  && \\\n  huge/install /opt/myinstall && \\\n  rm -rf huge*\n\nFROM ...\nCOPY --from=installer /opt/myinstall /opt/myinstall\n...\nThat way you will only have one layer in your image which only copies the installation.",
    "Error in Running Docker image. Showing no module named cv2,request,boto3": "You can run your container:\ndocker run -it <image_name>\nand execute the following command in python console that will install specified packages in your container:\nimport os\nos.system(\"pip install --upgrade pip\")\nos.system(\"pip install numpy requests boto3 opencv-python\")\nYou will see:\nCollecting numpy\n  Downloading numpy-1.13.3-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.0MB 124kB/s \nCollecting requests\n  Downloading requests-2.18.4-py2.py3-none-any.whl (88kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 4.8MB/s \nCollecting boto3\n  Downloading boto3-1.4.7-py2.py3-none-any.whl (128kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 3.4MB/s \nCollecting opencv-python\n  Downloading opencv_python-3.3.0.10-cp36-cp36m-manylinux1_x86_64.whl (15.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.5MB 126kB/s \nCollecting certifi>=2017.4.17 (from requests)\n  Downloading certifi-2017.11.5-py2.py3-none-any.whl (330kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 337kB 2.9MB/s \nCollecting chardet<3.1.0,>=3.0.2 (from requests)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 4.4MB/s \nCollecting idna<2.7,>=2.5 (from requests)\n  Downloading idna-2.6-py2.py3-none-any.whl (56kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 6.0MB/s \nCollecting urllib3<1.23,>=1.21.1 (from requests)\n  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 3.4MB/s \nCollecting jmespath<1.0.0,>=0.7.1 (from boto3)\n  Downloading jmespath-0.9.3-py2.py3-none-any.whl\nCollecting s3transfer<0.2.0,>=0.1.10 (from boto3)\n  Downloading s3transfer-0.1.11-py2.py3-none-any.whl (54kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 6.4MB/s \nCollecting botocore<1.8.0,>=1.7.0 (from boto3)\n  Downloading botocore-1.7.46-py2.py3-none-any.whl (3.7MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.7MB 535kB/s \nCollecting python-dateutil<3.0.0,>=2.1 (from botocore<1.8.0,>=1.7.0->boto3)\n  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 194kB 3.1MB/s \nCollecting docutils>=0.10 (from botocore<1.8.0,>=1.7.0->boto3)\n  Downloading docutils-0.14-py3-none-any.whl (543kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 552kB 2.2MB/s \nCollecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore<1.8.0,>=1.7.0->boto3)\n  Downloading six-1.11.0-py2.py3-none-any.whl\nInstalling collected packages: numpy, certifi, chardet, idna, urllib3, requests, jmespath, six, python-dateutil, docutils, botocore, s3transfer, boto3, opencv-python\nSuccessfully installed boto3-1.4.7 botocore-1.7.46 certifi-2017.11.5 chardet-3.0.4 docutils-0.14 idna-2.6 jmespath-0.9.3 numpy-1.13.3 opencv-python-3.3.0.10 python-dateutil-2.6.1 requests-2.18.4 s3transfer-0.1.11 six-1.11.0 urllib3-1.22\nSo all is ok. I tested it in container built using FROM python:latest Dockerfile.",
    "command substitution in docker CMD": "Change it to below\nCMD [\"bash\", \"-c\", \"$(pipenv --venv)/bin/python3 main.py /root/uploads\"]\nIf that still gives you issue, change $ to $$",
    "Access sibling service from docker container's service": "There are few things you can do here\nLet other container run on network of other container\ndocker run --net container:<id> MyDockerImage\nNow your mongodb will be accessible on localhost. But the port needs to be exposed in the container whose network is used\nCreate network yourself and use it\ndocker network create myapps\n\ndocker run --name mongodb_service --net myapps mongodb\ndocker run -p 3000-3009:3000-3009 --net myapps MyDockerImage\nNow inside your MyDockerImage, mongodb can be reached at mongodb_service\nUse docker compose\nYou can use docker-compose to run both of them as a composition\nversion: '3'\nservices:\n  mongo: \n    image: mongodb\n  app:\n    build:\n      context: .\n    ports:\n      - \"3000-3009:3000-3009\"\nAnd now in app mongodb will be reachable with name mongo",
    "Docker multistage build": "Multi-stage builds are a new feature in Docker 17.05, so you have to update your Docker version to 17.05 or a newer version.",
    "Docker-compose error with service env_file": "Looks like this is a problem with your YAML structure. It might be a missing space before your - .env or the missing service name. Try this:\nservices:\n  some-service:\n    env_file:\n      - .env\nIf you only use one environment file, you can also write:\nservices:\n  some-service:\n    env_file: .env\nSee: https://docs.docker.com/compose/compose-file/#env_file",
    "Copying data from and to Docker containers": "Using pipes is a simple solution too.\nAs root or within a script:\ndocker cp container-1:/data-path/. - | docker cp - container-2:/data-path\nAs sudoer:\nsudo docker cp container-1:/data-path/. - | (sudo docker cp - container-2:/data-path)",
    "angular-cli ng command not found even if it is installed while docker building an image": "The current directory is set to WORKDIR in each Dockerfile layer so it discards whatever you cd. So change\nRUN cd /opt/ && ls -l\nRUN ng build --prod\nTo:\nWORKDIR /opt/\nRUN ng build --prod\nOr:\nRUN cd /opt/ && ng build --prod\nAlso, make sure that the npm directory is in your \"PATH\" variable.",
    "Doing dpkg-reconfigure wireshark-common inside of a Dockerfile": "Try using the yes command.\nRUN yes | dpkg-reconfigure wireshark-common\nAnother try you can do is:\nRUN echo \"y\" | dpkg-reconfigure wireshark-common\nNot sure now what is wireshark asking for on dpkg-reconfigure... but using this technique you can send a \"y\" or a \"1\" or whatever you need.\nAnother possible solution based on your comments here:\nRUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y wireshark\nWith this last one, you'll skip any interactive post-install configuration steps.",
    "Dockerfile expose port for node.js image": "The EXPOSE directive is metadata only. If you check:\nhttps://docs.docker.com/engine/reference/builder/#expose\nYou'll see you still need to manually expose the port. The EXPOSE directive is useful for platforms which might want to know what a container expects to expose. For example, on OpenShift if I deployed your container it would suggest I expose port 8000.\nIf you don't want to manually expose the port, try this:\ndocker run --net=host\nThis will bind the container network controller to the host network . Be aware however this is a security concern, as there is no isolation between the container and the host machine (for TCPIP or UDP). This means your container can try to listen to any port on the host! Some containers do run like this, for example the consul client:\nhttps://hub.docker.com/_/consul/\nWhich is a chatty client which talks over lots of ports. Binding it to the host makes it much easier to quickly run the client, but I would avoid that approach unless absolutely necessary. Unfortunately, you'll probably just need to get used to the -p flag!",
    "ENTRYPOINT with environment variables is not acepting new params": "The /bin/sh -c only takes one argument, the script to run. Everything after that argument is a a shell variable $0, $1, etc, that can be parsed by the script. While you could do this with the /bin/sh -c syntax, it's awkward and won't grow with you in the future.\nRather than trying to parse the variables there, I'd move this into an entrypoint.sh that you include in your image:\n#!/bin/sh\nexec spark-submit --master $SPARK_MASTER script.py \"$@\"\nAnd then change the Dockerfile to define:\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\nThe exec syntax replaces the shell script in PID 1 with the spark-submit process, which allows signals to be passed through. The \"$@\" will pass through any arguments from docker run, with each arg quoted in case you have spaces in the parameters. And since it's run by a shell script, the $SPARK_MASTER will be expanded.",
    "What is the overhead of creating docker images?": "Docker uses a layered union filesystem, only one copy of a layer will be pulled by a docker engine and stored on its filesystem. When you build an image, docker will check its layer cache to see if the same parent layer and same command have been used to build an existing layer, and if so, the cache is reused instead of building a new layer. Once any step in the build creates a new layer, all following steps will create new layers, so the order of your Dockerfile matters. You should add frequently changing steps to the end of the Dockerfile so the earlier steps can be cached.\nTherefore, if you use a 200MB base image, have 50MB of additions, but only 10MB are new additions at the end of your Dockerfile, you'd push 250MB the first time to a docker engine, but only 10MB to an engine that already had a previous copy of that image, or 50MB to an engine that just had the 200MB base image.\nThe best practice with images is to build them once, push them to a registry (either self hosted using the registry image, cloud hosted by someone like AWS, or on Docker Hub), and then pull that image to each engine that needs to run it.\nFor more details on the layered filesystem, see https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/",
    "sh on docker: syntax error: unexpected end of file (expecting \"then\") [duplicate]": ": not found line 2:\nThat's pointing to something screwy with the white space in this file. The most likely cause is editing the script on windows and getting the wrong line feeds. Stop using whatever editor you were using and fix the script with a better editor or a utility like dos2unix.",
    "Dockerfile with an external arguments file": "An ARG is designed to be modified from the build command line, so you'd run:\ndocker build --build-arg VAR=value -t your_image .\nThat can be placed inside of a shell script to automate it and pass the same arg to each build.\nYou can also use a compose file, and the compose file may use environment variables or a .env file, to set variables used inside the compose file, e.g.\nbuild:\n  context: ./your_app_dir\n  dockerfile: Dockerfile\n  args:\n    VAR: ${VALUE}\nAnd the .env would contain:\nVALUE=your_value\nFor more details on compose files, see the build syntax and also the environment file syntax.",
    "Dockerfile - CMD does not run the shell script": "RUN is executed while you are building an image. This is usually used to install things or perform modifications in the resulting image.\nCMD is executed when the container is started. Use this to e.g. start applications.\nAlso check the docs\nThe main purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well.",
    "Docker-compose on Windows 7: how to import a db dump when using a yaml file?": "One way that you can do this is to mount a folder with the dump into the directory /docker-entrypoint-initdb.d/ when using the official mysql image.\nThis is a pretty good solution for working in a team, where everyone should be working off the same base db. You can read more about that in the documentation. This will only occur on the first run of the container. If you delete the container, it will do a fresh import. This has it's uses as well.\nMore often, I do as some of the other answers suggest:\ndocker-compose exec db mysql -u root -p DB_NAME < DUMP_FILE_IN_CONTAINER\nIt's important to note, that you still need to mount the dump file in the container! It will not work for a file on the host machine!\nYou don't necessarily need to mount an entire folder (though you can - all scripts in the above-mentioned folder will execute in alphabetical order). You can just mount your dump file like this:\nservices:\n  db:\n  image: mysql:5.7\n  volumes:\n    - db_data:/var/lib/mysql\n    - ./path/to/dump/on/host.sql:/docker-entrypoint-initdb.d/anyname.sql\n  restart: always\n  environment:\n    MYSQL_ROOT_PASSWORD: wordpress\n    MYSQL_DATABASE: wordpress\n    MYSQL_USER: wordpress\n    MYSQL_PASSWORD: wordpress",
    "Connection refused when running mongo DB command in docker": "Several things going wrong here. First are the commands you're running:\nRUN service mongodb start\n\nRUN mongod --fork --logpath /var/log/mongodb.log\nEach of these will run to create a layer in docker. And once the command being run returns, the temporary container that was started is stopped and any files changed in the container are captured to make a new layer. There are no persistent processes that last between these commands.\nThese commands are also running the background version of the startup commands. In docker, you'll find this to be problematic since when you use this as your container's command, you'll find the container dies as soon as the command finishes. Pid 1 on the container has the same role of pid 1 on a linux OS, once it dies, so does everything else.\nThe second issue I'm seeing is mixing data with your container in the form of initializing the database with the last RUN command. This fails since there's no database running (see above). I'd recommend instead to make an entrypoint that configures the database if one does not already exist, and then use a volume in your docker-compose.yml or on your docker run commandline to persist data between containers.\nIf you absolutely must initialize the data as part of your image, then you can try merging the various commands into a single run:\nRUN mongod --fork --logpath /var/log/mongodb.log \\\n && mongo db --eval 'db.createUser({user:\"dbuser\",pwd:\"dbpass\",roles:[\"readWrite\",\"dbAdmin\"]})'",
    ".NET Core 1.1, Docker Build encounters Couldn't find 'project.json' with csproj": "Of course, I only posted this after days of searching for an answer, then the answer appeared! In Docker-speak, \"latest\" does not point to 1.1, even though 1.1 is the latest released version. I changed the first line of my dockerfile to read:\nFROM microsoft/dotnet:1.1-sdk-msbuild\nand now it's building.",
    "Starting bash script on Docker container startup doesn't work": "The error is caused by the line endings in your shell script. It looks like you're using Windows line endings (CRLF, or \\r\\n), where the unexpected r is confusing Bash. Bash only expects LF or \\n, hence the error message.\nMost programmer text editors have some kind of support for making these changes. Notepad++ has \"Edit > EOL Conversion > Unix/OSX Format\". Please see EOL conversion in notepad ++ for more info.",
    "docker compose vs docker run - node container": "Note that the volume that you're describing in your docker-compose.yml file will be mounted at run time, not at build time. This means that when building the image, there will not be any package.json file there (yet), from which you could install your dependencies.\nWhen running the container image with -v /home/my/path:/app, you're actually mounting the directory first, and subsequent npm install invocations will complete succesfully.\nIf you intend to mount your application (including package.json) into your container, the npm install needs to happen at run time (CMD), and not at build time (RUN).\nThe easiest way to accomplish this would be to simply add the npm install statement to your CMD instruction (and drop the RUN npm install instruction):\nCMD npm install && npm start",
    "How to place files on shared volume from within Dockerfile?": "You're going about it wrong, but you already suspected that.\nIf you want the source files to reside on the host filesystem, get rid of the VOLUME directive in your Dockerfile, and don't try to download the source files at build time. This is something you want to do at run time. You probably want to provision your image with a pair of scripts:\nOne that downloads the files to a specific location, say /build.\nAnother that actually runs the build process.\nWith these in place, you could first download the source files to a location on the host filesystem, as in:\ndocker run -v /path/on/my/host:/build myimage fetch-sources\nAnd then you can build them by running:\ndocker run -v /path/on/my/host:/build myimage build-sources\nWith this model:\nYou're trying to muck about with volumes during the image build process. This is almost never what you want, since data stored in a volume is explicitly excluded from the image, and the build process doesn't permit you to conveniently mount host directories inside the container.\nYou are able to download the files into a persistent location on the host, where they will be available to you for editing, or re-building, or whatever.\nYou can run the build process multiple times without needing to re-download the source files every time.\nI think this will do pretty much what you want, but if it doesn't meet your needs, or if something is unclear, let me know.",
    "Docker images wont run in background": "With the bash entrypoint, bash will exit as soon as stdin returns an end of file. So you leave it running, you need to start it with docker run -itd image-name. The -i makes it interactive, -t assigns a tty, and -d detaches. That keeps the stdin open on the container and allows you to attach or exec commands against the container.\nFollow up: I just saw your command --port 81 which when run as a command on bash will give you an invalid option. If you need to run mongo with that as an option, you'll need a different entry point.",
    "installation of nodejs returned a non-zero code: 1 with docker build": "Have you tried running this yourself to see what the error is? Like so:\n$ docker run --rm -it ubuntu:16.10\n[...]\nroot@96117efa0948:/# apt-get update\n[...]\nroot@96117efa0948:/# apt-get install -y curl\n[...]\nroot@96117efa0948:/# curl -sL https://deb.nodesource.com/setup_6.x | bash -\n[...]\n## Your distribution, identified as \"Ubuntu Yakkety Yak (development \nbranch)\", is a pre-release version of Ubuntu. NodeSource does not maintain \nofficial support for Ubuntu versions until they are formally released. You \ncan try using the manual installation instructions available at \nhttps://github.com/nodesource/distributions and use the latest supported \nUbuntu version name as the distribution identifier, although this is not \nguaranteed to work.\nroot@96117efa0948:/#\nSo basically that blurb is telling you that your version of Ubunutu isn't supported yet. Try changing your config file to use ubuntu:16.04 - or work out some other way to install node.",
    "What is exactly happening when you're Docker Commiting a container?": "As mentioned in docker commit:\nBy default, the container being committed and its processes will be paused while the image is committed. This reduces the likelihood of encountering data corruption during the process of creating the commit.\nThat step (waiting for the processes to pause) can take time.\nIf this behavior is undesired, set the --pause option to false.\nYou can see the actual commit call in api/server/router/image/image_routes.go#postCommit() which is then routed through a backend to daemon/commit.go#Commit()",
    "Met \"/bin/bash: no such file\" when building docker image from scratch": "Docker is basically a containerising tool that helps to build systems and bring them up and running in a flash without a lot of resource utilisation as compared to Virtual Machines.\nA Docker container is basically a layered container. In case you happen to read a Dockerfile, each and every command in that file will lead to a creation of a new layer in container and the final layer is what your container actually is after all the commands in the Dockerfile has been executed.\nThe images available on the Dockerhub are specially optimised for this sort of environment and are very easy to setup and build. In case you are building a container right from scratch i.e. without any base image, then what you basically have is an empty container. An empty container does not understand what /bin/bash actually is and hence it won't work for you.\nThe Docker container does not use any specifics from your underlying OS. Multiple docker containers will make use of the same underlying kernel in an effective manner. That's it. Nothing else. ( There is however a concept of volumes wherein the container shares a specific volume on the local underlying system )\nSo in case you want to use /bin/bash, you need a base image which will setup the nitty gritties of this command for your container and then you can successfully execute it.\nHowever, it is recommended that you use official Docker images for say Ubuntu and then install your custom stuff on top of it. The official images are right from the makers and are highly optimised for this environment.",
    "Chaining Docker Images and execute in order": "only 1 CMD instruction is executed (the last one in the outermost Dockerfile within the chain)?\nYes this is correct, and keep in mind that CMD is not run at build time but at instantiation time. In essence what you are doing in your second Dockerfile's CMD is overriding the first one when you instantiated the container from ImageB\nIf you are doing some sort of Rest API or cli or cURL to connect to your Wildfly server I suggest you do that configuration after the container's instantiation, not at the after the container's build. This way:\nCMD [\"/opt/jboss/wildfly/bin/standalone.sh\", \"-b\", \"0.0.0.0\", \"-bmanagement\", \"0.0.0.0\", \"-c\", \"standalone-apiman.xml\"]`\nis always your last command. If you need some extra files or changes to configuration files you can put them in the Dockerfile so that they get copied at build time before CMD gets called at instantiation.\nSo in summary:\n1) Build your Docker container with this Dockerfile (docker build):\nFROM jboss/apiman-wildfly:1.1.6.Final\nRUN /opt/jboss/wildfly/bin/add-user.sh admin admin --silent\nCOPY /RatedRestfulServer/target/RatedRestfulServer-1.0-SNAPSHOT.war /opt/jboss/wildfly/standalone/deployments/\nCOPY /configure.sh /opt/jboss/wildfly/\nCMD [\"/opt/jboss/wildfly/bin/standalone.sh\", \"-b\", \"0.0.0.0\", \"-bmanagement\", \"0.0.0.0\", \"-c\", \"standalone-apiman.xml\"]\n2) Run (instantiate your container from your newly created image)\ndocker run <image-id>\n3) Run the following from a container or from your host that has Wildfly configured the same way. This assuming you are using some Rest API to configure things (i.e using cURL):\n/opt/jboss/wildfly/configure.sh\nYou can instantiate a second container to run this command with something like this:\ndocker run -ti <image-id> /bin/bash",
    "Docker CLI env var value override not respected by container on startup": "The TOMCAT_ENV environment variable is used during build time to create setenv.sh. It looks something like this:\nATALINA_OPTS=\"$CATALINA_OPTS -Xms512m -Xmx1024m -Denv=local -Dlogging_override=file:///logging_override.xml -Doverride_file=/override.properties\"\nYou are changing this variable at runtime, but setenv.sh already created at this time and the environment is set to local.\nIf you want to change the environment at runtime you should not write the value of the variable TOMCAT_ENV to setenv.sh, but a reference to the variable. You can do that by escaping the variable in your Dockerfile by prepeding $TOMCAT_ENV with a slash to \\$TOMCAT_ENV:\nRUN echo \"CATALINA_OPTS=\\\"\\$CATALINA_OPTS -Xms512m -Xmx1024m - Denv=\\$TOMCAT_ENV \\\n  -Dlogging_override=file://$CATALINA_HOME/logging_override.xml \\\n  -Doverride_file=$CATALINA_HOME/override.properties\\\"\" | \\\n    tee -a $CATALINA_HOME/bin/setenv.sh \\\n    && chmod 755 $CATALINA_HOME/bin/setenv.sh\nNow your setenv.sh looks like this:\nCATALINA_OPTS=\"$CATALINA_OPTS -Xms512m -Xmx1024m -Denv$TOMCAT_ENV -Dlogging_override=file:///logging_override.xml -Doverride_file=/override.properties\"",
    "Not able to clone private repo using dockerfile": "Look at my example, I have a private ssh key in the directory where I dockerize app(ssh_keys/id_rsa), and public key I have already upload to the private repo:\nFROM ubuntu:14.04\n\nMAINTAINER Alok Agarwal \"alok.alok.com\"\n\nRUN apt-get update\n\n#Install git \nRUN apt-get install -y git\n\nRUN /bin/bash -l -c \"mkdir /root/.ssh\"\nADD ssh_keys/id_rsa /root/.ssh/id_rsa\nRUN chmod 700 /root/.ssh/id_rsa\nRUN echo \"Host github.com\\n\\tStrictHostKeyChecking no\\n\" >> /root/.ssh/config \nRUN mkdir -p /www/app\nRUN git clone git@github.com:my_private_repo/repo.git /www/app",
    "How to start redis-server and a node.js app on the same container?": "In a container, you usually start only one software. You can start more than one, but you need some tools like supervisor https://docs.docker.com/articles/using_supervisord/ runit, s6, daemontools, see the FAQ https://docs.docker.com/faq/",
    "Unable to connect to Docker Nginx build": "Instead of localhost, use boot2docker ip. First do boot2docker ip and use that ip: <your-b2d-ip>:8080. Also you need to make sure you forwarded your port 8080 in VirtualBox for boot2docker.",
    "Translate bash command to dockers CMD": "Find out what is the full path to the start-stop-daemon by running which start-stop-daemon and then use:\nCMD [\"/full_path_to_the_bin_file/start-stop-daemon\", \"--quiet\", \"--oknodo\", \"--start\", \"--pidfile\", \"/run/my.pid\", \"--background\", \"--make-pidfile\", \"--exec\", \"/opt/socat-1.7.2.4/socat\", \"PTY,link=/dev/ttyMY,echo=0,raw,unlink-close=0\", \"TCP-LISTEN:9334,reuseaddr,fork\"]\ninstead of CMD, you may want to use ENTRYPOINT",
    "Make docker env variables from an `.env` file available in build step (Dockerfile) & during run-time in container": "In addition to not matching well with Docker's ENV primitive, the other practical problem with this setup is that the environment gets reset at the end of each RUN command.\nIf you only need the contents of the file once, then you can use the standard shell . command to read it in within the context of a single RUN command. (Some shells have a similar source command, but it is not part of the shell standard, and I'd avoid it in most cases.)\nWORKDIR /app\nCOPY ./ ./ # includes .env file\nRUN . oneSourceOfTruth.env && \\\n    DB_ADAPTER=nulldb bundle exec rails assets:precompile\n# RUN echo $Foo  # environment variables won't be set any more\n# CMD echo $Foo  # nor in the eventual container\nYou can work around the CMD problem by writing a shell script that reads in the environment file and then runs the command it's passed. That script can be the image's ENTRYPOINT.\n#!/bin/sh\n. /app/oneSourceOfTruth.env\nexec \"$@\"\nENTRYPOINT [\"/app/with-environment\"]\nCMD echo $Foo  # will be set now\nIf you need the environment to be set on every RUN command, then you can override the Dockerfile SHELL to run this script too. This defines the interpreter that's used for shell-syntax RUN and CMD commands (not JSON-array syntax), and you could inject a wrapper script here too.\nSHELL [\"/app/with-environment\", \"/bin/sh\", \"-c\"]\nRUN echo $Foo\nCMD echo $Foo\n# CMD [\"/bin/sh\", \"-c\", \"echo $Foo\"] # will not have the environment\nThe one caution with this last setup is that a docker run or Compose command: override won't see the Dockerfile SHELL and the environment variables won't be set there. You need the ENTRYPOINT wrapper for that.",
    "How to install \"sleep\" command for \"scratch\" docker image": "As Hans Kilian said, Alpine's sleep is a link to busybox, which in turn has dynamic libraries.\nThis Dockerfile will copy the necessary files and use the correct entrypoint to use sleep:\nFROM alpine as build\nFROM scratch\nCOPY --from=build /lib/ld-musl-x86_64.so.1 /lib/ld-musl-x86_64.so.1\nCOPY --from=build /bin/busybox /bin/busybox\nCMD [\"/bin/busybox\", \"sleep\", \"5\"]\nTry it with docker build -t sleeper . and then docker run --rm -it sleeper.",
    "Building multi-stage docker image with a distroless base image results in \"no such file or directory\" error": "However, this returned an error: \"exec /usr/local/bin/python3.11: no such file or directory\"\nWhen you know that the actual binary exists, this error typically means the kernel can't find the appropriate dynamic loader. If we boot into a python:3.11.4-slim image and run ldd to show dependencies, we see that python3.11 requires:\nroot@686b3d1ca79b:/# ldd /usr/local/bin/python3.11\n        linux-vdso.so.1 (0x00007ffc51f87000)\n        libpython3.11.so.1.0 => /usr/local/bin/../lib/libpython3.11.so.1.0 (0x00007f92dc400000)\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f92dc21d000)\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f92dc13e000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007f92dc92d000)\nYour Dockerfile is failing to copy the dynamic loader (ld-linux...) as well as both libc and libm. If I add the following lines to your Dockerfile...\nCOPY --from=builder-image /lib64/ld-linux-x86-64.so.2 /lib64/\nCOPY --from=builder-image /lib/${CHIPSET_ARCH}/libc.so.6 /lib/${CHIPSET_ARCH}/\nCOPY --from=builder-image /lib/${CHIPSET_ARCH}/libm.so.6 /lib/${CHIPSET_ARCH}/\n...then I am able to successfully start the Python interpreter.\nThe Dockerfile I used for testing looks like this (I've removed the parts relating to your application and I've added the above lines):\nFROM python:3.11.4-slim AS builder-image\n\n# avoid stuck build due to user prompt\nARG DEBIAN_FRONTEND=noninteractive\n\n# create and activate virtual environment\n# using final folder name to avoid path issues with packages\nRUN python3.11 -m venv /opt/venv\n\nFROM gcr.io/distroless/static-debian12:nonroot AS runner-image\n\n# Determine chipset architecture for copying python\nARG CHIPSET_ARCH=x86_64-linux-gnu\n\n# required by lots of packages - e.g. six, numpy, wsgi\nCOPY --from=builder-image /lib/${CHIPSET_ARCH}/libz.so.1 /lib/${CHIPSET_ARCH}/\n# required by google-cloud/grpcio\nCOPY --from=builder-image /usr/lib/${CHIPSET_ARCH}/libffi* /usr/lib/${CHIPSET_ARCH}/\nCOPY --from=builder-image /lib/${CHIPSET_ARCH}/libexpat* /lib/${CHIPSET_ARCH}/\n\n# Copy python from builder\nCOPY --from=builder-image /usr/local/lib/ /usr/local/lib/\nCOPY --from=builder-image /usr/local/bin/python3.11 /usr/local/bin/python3.11\nCOPY --from=builder-image /etc/ld.so.cache /etc/ld.so.cache\n\n# activate virtual environment\nCOPY --from=builder-image /opt/venv /opt/venv\nENV VIRTUAL_ENV=/opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY --from=builder-image /lib64/ld-linux-x86-64.so.2 /lib64/\nCOPY --from=builder-image /lib/${CHIPSET_ARCH}/libc.so.6 /lib/${CHIPSET_ARCH}/\nCOPY --from=builder-image /lib/${CHIPSET_ARCH}/libm.so.6 /lib/${CHIPSET_ARCH}/\nAnd running an image built from that Dockerfile looks like:\n$ docker run -it --rm pythontest /usr/local/bin/python3.11\nPython 3.11.4 (main, Aug 16 2023, 05:23:18) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>",
    "Do all Docker commands actually create another layer?": "As a general statement, any Dockerfile instruction that modifies the file system creates a new layer.\nIn particular, the instructions RUN, COPY, ADD mostly contribute to the size of the final Docker image and always create layers.\nSome instructions, for example the ones that starts with LABEL, ENTRYPOINT, CMD directives, don't modify the filesystem since they just add metadata or configurations to the image, so they don't add any layers that increased the file size of the Docker image but only create temporary intermediate layers with 0B in size.\nYou can use the command history:\ndocker history <image-name> \nto check the details about layers that compose the image, the instructions and their size.",
    "Getting `error while loading shared libraries: libssl.so.1.1` when running docker container": "The problem is that you're building against a different OS than you're running against. Currently, the rust:latest target uses bullseye (Debian 11) as a base. Debian bullseye ships OpenSSL 1.1. However, postgres:latest uses bookworm (Debian 12) as a base, which ships OpenSSL 3.0. As a result, you have two different OSes with two different versions of OpenSSL, and your binary, which was linked against OpenSSL 1.1, can't find it when attempting to run.\nIn general, unless you're statically linking (and even often then), you are almost always better off compiling and running on the same version of the same operating system, since that results in code that tends to be more likely to be functional and behave correctly.\nNote that your FROM debian:bullseye has no effect, since it's immediately overridden by FROM postgres:latest.\nIf you want to use bookworm for both, then specify FROM rust:bookworm (and, ideally, FROM postgres:bookworm). If you want to use bullseye for both, then use FROM rust:bullseye and FROM postgres:bullseye. That will make sure that your code builds and runs on the same platform. You also don't need to install libssl-dev to run the code, only either libssl1.1 for bullseye or libssl3 for bookworm.",
    "Podman says a directory doesn't exist when it does while copying": "The two relevant aspects for the question are the Dockerfile path, which can be provided with the -f option and the build context, which is the last argument in the build command.\nWhen a context is set in the podman build command, all subsequent COPY instructions inside the Dockerfile are relative to the context path.\nAssuming you have the following directory structure:\n.\n\u251c\u2500\u2500 opt\n\u2502   \u2514\u2500\u2500 SIMULATeQCD\n\u2502       \u2514\u2500\u2500 src\n\u2502           \u251c\u2500\u2500 CMakeLists.txt\n\u2502           \u2514\u2500\u2500 cmake.sh\n\u2514\u2500\u2500 scripts\n    \u2514\u2500\u2500 Dockerfile\nIf you run the build command like this:\npodman build -f scripts/Dockerfile -t alpine-context-so-img opt/SIMULATeQCD/src/\nThen the Dockerfile should COPY files as if the current working directory on the host machine was /opt/SIMULATeQCD/src:\nFROM alpine:3.14\nWORKDIR /target-path-in-container\n# Next line is valid and also copies the two files\n# COPY . . \n# But for better exemplification, I am copying each\n# file verbosely:\nCOPY ./cmake.sh .\nCOPY ./CMakeLists.txt .\nENTRYPOINT [ \"ls\", \"/target-path-in-container\"]\nNow if you run the container, the two files should be found in the container:\n$ podman run alpine-context-so-img\nCMakeLists.txt\ncmake.sh",
    "How to optimize the build time of a c++ project inside a container": "Modern Dockerfiles allow you to specify that a persistent volume should be used for caching purposes. Replace your RUN cmake .. && make with:\nRUN --mount=type=cache,target=/usr/local/src/cpp-project/build,sharing=locked \\\n  cmake .. && cmake --build . && cp my-daemon /\nand in your second stage grab my-daemon from /my-daemon instead.\nThe sharing=locked argument prevents multiple concurrent builds from interfering.",
    "JanusGraph not communicating with Cassandra backend in Docker environment": "The NoHostAvailableException in the stacktrace indicates that the Cassandra Java driver used by JanusGraph can't connect to the cluster:\n    ...\nCaused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [/127.0.0.1:9042] Cannot connect))\n    at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:268)\n    at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:107)\n    at com.datastax.driver.core.Cluster$Manager.negotiateProtocolVersionAndConnect(Cluster.java:1813)\n    at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1726)\n    at com.datastax.driver.core.Cluster.init(Cluster.java:214)\n    ...\nI suspect that the problem is JanusGraph attempts to connect to the cluster before Cassandra is up and ready.\nTry configuring your JanusGraph service to have a dependency on your Cassandra service (with depends_on). This means that the DB will be started before the app. Additionally, require that Cassandra is \"healthy\" before creating the JanusGraph service with the condition property.\nYour Docker compose file should look like:\n  ...\n  janusgraph-es:\n    ...\n    depends_on:\n      cassandra:\n        condition: service_healthy\n  ...\nFor details, see the official Docker Docs depends_on specification.\nAs a side note, container links is a legacy feature and is deprecated. Consider using the default bridge in your network configuration. Cheers!\nnetworks:\n  jce-network:\n    driver: bridge\n\ud83d\udc49 Please support the Apache Cassandra community by hovering over the\ncassandra\ntag then click on the Watch tag button. \ud83d\ude4f Thanks!",
    "How to install python libraries in docker file on ubuntu?": "For me the only problem in your Dockerfile is in the line RUN apt install python -y. This is erroring with Package 'python' has no installation candidate.\nIt is expected since python refers to version 2.x of Python wich is deprecated and no longer present in the default Ubuntu repositories.\nChanging your Dockerfile to use Python version 3.x worked fine for me.\nFROM ubuntu:latest\n\nRUN apt update\nRUN apt install python3 python3-pip -y\nWORKDIR /Destop/DS\n\nCOPY requirement.txt ./\nRUN pip3 install -r requirement.txt\n\nCOPY script2.py ./\nCMD [\"python3\", \"./script2.py\"]\nTo test I used requirement.txt\npandas==1.5.1\nand script2.py\nimport pandas as pd\n\nprint(pd.__version__)\nWith this building the docker image and running a container from it executed succesfully.\ndocker build -t myimage .\ndocker run --rm myimage",
    "Difference between docker image ID and Digest ID": "These are different json pieces of metadata, defined in the OCI image-spec under manifest.md and config.md.\nTo show this with a specific image, here's a copy of the alpine image manifest:\n$ regctl manifest get localhost:5000/library/alpine --format body | jq .\n{\n  \"manifests\": [\n    {\n      \"digest\": \"sha256:1304f174557314a7ed9eddb4eab12fed12cb0cd9809e4c28f29af86979a3c870\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"amd64\",\n        \"os\": \"linux\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:5da989a9a3a08357bc7c00bd46c3ed782e1aeefc833e0049e6834ec1dcad8a42\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"arm\",\n        \"os\": \"linux\",\n        \"variant\": \"v6\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:0c673ee68853a04014c0c623ba5ee21ee700e1d71f7ac1160ddb2e31c6fdbb18\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"arm\",\n        \"os\": \"linux\",\n        \"variant\": \"v7\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:ed73e2bee79b3428995b16fce4221fc715a849152f364929cdccdc83db5f3d5c\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"arm64\",\n        \"os\": \"linux\",\n        \"variant\": \"v8\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:1d96e60e5270815238e999aed0ae61d22ac6f5e5f976054b24796d0e0158b39c\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"386\",\n        \"os\": \"linux\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:fa30af02cc8c339dd7ffecb0703cd4a3db175e56875c413464c5ba46821253a8\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"ppc64le\",\n        \"os\": \"linux\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:c2046a6c3d2db4f75bfb8062607cc8ff47896f2d683b7f18fe6b6cf368af3c60\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"s390x\",\n        \"os\": \"linux\"\n      },\n      \"size\": 528\n    }\n  ],\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n  \"schemaVersion\": 2\n}\nThat manifest is actually a manifest list, similar to the OCI Index which contains multiple manifests, in this case for a multi-platform image. Picking out the first platform, here's the manifest for the linux/amd64 platform:\n$ regctl manifest get \\\n  localhost:5000/library/alpine@sha256:1304f174557314a7ed9eddb4eab12fed12cb0cd9809e4c28f29af86979a3c870 \\\n  --format body | jq .\n{\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n  \"config\": {\n    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n    \"size\": 1470,\n    \"digest\": \"sha256:9c6f0724472873bb50a2ae67a9e7adcb57673a183cea8b06eb778dca859181b5\"\n  },\n  \"layers\": [\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 2806054,\n      \"digest\": \"sha256:213ec9aee27d8be045c6a92b7eac22c9a64b44558193775a1a7f626352392b49\"\n    }\n  ]\n}\nThat manifest contains descriptors to the image config and a list of layers (in this case a single layer). Pulling the config blob shows the image config and history that you may recognize from inspecting images in docker:\n$ regctl blob get localhost:5000/library/alpine \\\n  sha256:9c6f0724472873bb50a2ae67a9e7adcb57673a183cea8b06eb778dca859181b5 \\\n  | jq .\n{\n  \"architecture\": \"amd64\",\n  \"config\": {\n    \"Hostname\": \"\",\n    \"Domainname\": \"\",\n    \"User\": \"\",\n    \"AttachStdin\": false,\n    \"AttachStdout\": false,\n    \"AttachStderr\": false,\n    \"Tty\": false,\n    \"OpenStdin\": false,\n    \"StdinOnce\": false,\n    \"Env\": [\n      \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n    ],\n    \"Cmd\": [\n      \"/bin/sh\"\n    ],\n    \"Image\": \"sha256:c0261ca8a4a79627f3e658c0c2b1e3166f56713a58e1411b1e3ab1e378962e75\",\n    \"Volumes\": null,\n    \"WorkingDir\": \"\",\n    \"Entrypoint\": null,\n    \"OnBuild\": null,\n    \"Labels\": null\n  },\n  \"container\": \"0c4b20968eb1d804460f8612bc52be4f5a8e65eb14190b5ae30fec94d2fb4f50\",\n  \"container_config\": {\n    \"Hostname\": \"0c4b20968eb1\",\n    \"Domainname\": \"\",\n    \"User\": \"\",\n    \"AttachStdin\": false,\n    \"AttachStdout\": false,\n    \"AttachStderr\": false,\n    \"Tty\": false,\n    \"OpenStdin\": false,\n    \"StdinOnce\": false,\n    \"Env\": [\n      \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n    ],\n    \"Cmd\": [\n      \"/bin/sh\",\n      \"-c\",\n      \"#(nop) \",\n      \"CMD [\\\"/bin/sh\\\"]\"\n    ],\n    \"Image\": \"sha256:c0261ca8a4a79627f3e658c0c2b1e3166f56713a58e1411b1e3ab1e378962e75\",\n    \"Volumes\": null,\n    \"WorkingDir\": \"\",\n    \"Entrypoint\": null,\n    \"OnBuild\": null,\n    \"Labels\": {}\n  },\n  \"created\": \"2022-08-09T17:19:53.47374331Z\",\n  \"docker_version\": \"20.10.12\",\n  \"history\": [\n    {\n      \"created\": \"2022-08-09T17:19:53.274069586Z\",\n      \"created_by\": \"/bin/sh -c #(nop) ADD file:2a949686d9886ac7c10582a6c29116fd29d3077d02755e87e111870d63607725 in / \"\n    },\n    {\n      \"created\": \"2022-08-09T17:19:53.47374331Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  CMD [\\\"/bin/sh\\\"]\",\n      \"empty_layer\": true\n    }\n  ],\n  \"os\": \"linux\",\n  \"rootfs\": {\n    \"type\": \"layers\",\n    \"diff_ids\": [\n      \"sha256:994393dc58e7931862558d06e46aa2bb17487044f670f310dffe1d24e4d1eec7\"\n    ]\n  }\n}\nNote that everything is content addressable in the registry, so the sha256sum of the manifest and blob is it's address:\n$ regctl manifest get \\\n  localhost:5000/library/alpine@sha256:1304f174557314a7ed9eddb4eab12fed12cb0cd9809e4c28f29af86979a3c870 \\\n  --format body | sha256sum\n1304f174557314a7ed9eddb4eab12fed12cb0cd9809e4c28f29af86979a3c870  -\n\n$ regctl blob get localhost:5000/library/alpine \\\n  sha256:9c6f0724472873bb50a2ae67a9e7adcb57673a183cea8b06eb778dca859181b5 \\\n  | sha256sum\n9c6f0724472873bb50a2ae67a9e7adcb57673a183cea8b06eb778dca859181b5  -\nAnd since the config digest is part of the image manifest, you cannot change that config without changing the config digest and the manifest digest that points to that config. The manifest digest is what you see when you pull an image. The config digest happens to show up when you inspect the image in docker",
    "How to install brew into a Dockerfile (`brew: not found`)": "This Dockerfile installs brew in /home/linuxbrew/.linuxbrew/bin/brew. Including that directory in the path (with the ENV command) does the trick.\n...\nENV PATH=\"/home/linuxbrew/.linuxbrew/bin:${PATH}\"\nRUN brew install hello",
    "bash: No such file or directory when running a dockerfile": "FROM scratch as another_container\nYou're using a FROM statement, which replaces the current container with a new container based on an image. In this case, you're doing it FROM scratch, which is a container with nothing in it.\nSince you didn't use COPY --from to copy the compiled file from the previous stage, the file ./main is missing, so your application won't work.\nSee here for a multistage Go build example.",
    "How can I read --secret args passed to docker build as environment variables to write to a file?": "I suggest writig the content of .condarc locally to a file and mount that right away to the right place.\ndocker build --secret id=condarc,src=.condarc .\nAnd in the Dockerfile mount it like this:\nRUN --mount=type=secret,id=condarc,dst=/root/.condarc ...\nMake sure you have the local .condarc in your .dockerignore in case you copy with glob pattern or dot.\nAlso note that if you write a mounted secret to another file, like you did with echo, and use this new file in a later instruction, it will be added to a layer and in the history. Deleting it in a later instruction is not safe, if you need to do this, with echo or similar, make sure you delete it in the very same RUN instruction, the secret was mounted.",
    "Error: @prisma/client did not initialize yet. Please run \"prisma generate\"": "add RUN npm i -g prisma before RUN prisma generate it should work",
    "Docker failed to solve with frontend dockerfile.v0: No stored credential for https://index.docker.io/v1/": "Here is one workaround for this issue.\nJust run docker pull the specific docker image manually.",
    "Django deployment with docker - create superuser": "According to createsuperuser -h and this doc, createsuperuser command does not support --password flag. to read arguments from environment variables, you should use this command with --noinput flag and set required fields like username, email and password as DJANGO_SUPERUSER_<uppercase_field_name> in your env file.\npython manage.py createsuperuser --noinput ",
    "Cannot explain why Alpine apk upgrade command does not update ncurses package although a newer version exists": "The nginx:1.21.3-alpine image is based on Alpine 3.14 (see cat /etc/os-release), and therefore ncurses is updated with the version of the Alpine 3.14 repository, which is currently 6.2_p20210612-r0.\nFor installing ncurses from edge (currently version 6.2_p20211002-r0), you could specify the edge repository explicitly in the apk command:\napk add ncurses --repository=http://dl-cdn.alpinelinux.org/alpine/edge/main\nMixing and matching packages from different repositories this way might be OK in some cases, but has to be tested carefully. For ncurses, some functionality might be broken, since the matching ncurses-libs package should be installed as well, but some of the package images depend on ncurses-libs, so re-installing it triggers update of these packages. Moreover, the nginx-module-njs dependent package must be removed. If this is acceptable, you could modify the Dockerfile as follows:\nFROM nginx:1.21.3-alpine\n\nRUN apk update && \\\n    apk del ncurses ncurses-libs nginx-module-njs && \\\n    apk add ncurses ncurses-libs --repository=http://dl-cdn.alpinelinux.org/alpine/edge/main && \\\n    apk add bash && \\\n    apk upgrade",
    "Missing CV2 in Docker container": "libGL.so.1 could be found in libgl1, so you could add next to Dockerfile:\nRUN apt update; apt install -y libgl1\nTypically, docker images may remove many libraries to make the size small, these dependencies most probably could be already installed in your host system.\nSo, for me, I usually use dpkg -S in host system to find which package I needed, then install them in container:\nshubuntu1@shubuntu1:~$ dpkg -S libGL.so.1\nlibgl1:amd64: /usr/lib/x86_64-linux-gnu/libGL.so.1\nlibgl1:amd64: /usr/lib/x86_64-linux-gnu/libGL.so.1.0.0",
    "Maintaining multiple docker images with common intermediate/final steps": "yes\nyou can create a shared Dockerfile and take the source image as a build argument\nARG BASE_IMAGE\nFROM ${BASE_IMAGE}\n# common steps\nyou can then call it for everyone of your builds with\ndocker build ... --build-args BASE_IMAGE=<img_name>\nEDIT:\nthis means you call docker build twice for each image\nonce with the different dockerfiles foo, bar, baz and you should tag it as foo_intermediate etc\nand then with the common dockerfile were the base image is foo_intermediate and tag it the way you need e.g. foo",
    "Go build is failing for private repo with Dockerfile/go.mod files": "Basically, there are multiple repos under github.com/user-name/ and all of them are private which I want to use.\nI would rather use a more specific directive:\n git config --global \\\n url.\"ssh://git@github.com/user-name/*\".insteadOf https://github.com/user-name/*\nThat way, the instead of won't apply to all https://github.com URLs, only to the one matching the private repository for which you need to use SSH.\nThe OP ios-mxe confirms in the comments it is indeed working as expected.",
    "Connection refused to localhost with Docker container (Windows)": "The parameter --network=host (or --net=host) is only supported on Linux. Unfortunately Docker does not complain when you use the parameter under Windows, but you will get a \"connection refused\" error message.\nAs a test i used the following command:\ndocker run --rm -p 80:80 nginx and with that I could connect to localhost:80\nUsing docker run --rm -p 80:80 --network=host nginx gave me a \"connection refused\" error message when trying to access localhost:8080",
    "Docker container with Python modules gets too big": "There quite a few options for making the installation smaller:\nYou could make a build with e.g. PyInstaller and only install the artifact on an image without python. You get the \"binary\" and only the artifacts needed to run the application. If you build it on a builder image with python and then copy the artifact to your resulting image it could have a lot of impact on size especially of that image is e.g. an alpine bare or busybox image.\nYou can also add the --no-install-recommends to your apt-get command to not follow all deps.\nMulti-stage image with one builder image and a target image. The builder image can be as big as it needs to be to build and the target image as small as possible and only copy the needed end build.\nThe 3.7-slim-stretch is already a few Mb smaller than the slim-buster...\nadd this to your RUN command && apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* to clean up after yourself\netc.\nI don't know if the impact will be big enough to reduce to 120Mb. :-)\nFROM arm32v7/python:3.7-slim-buster as dev\nCOPY model.tflite /\nCOPY docker_tflite.py /\nCOPY numpy-1.20.2-cp37-cp37m-linux_armv7l.whl /\nRUN apt-get update \\\n    && apt-get -y install libatlas-base-dev\nRUN pip install --user numpy-1.20.2-cp37-cp37m-linux_armv7l.whl \\\n    && pip install --user --no-build-isolation --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\n\n\nFROM arm32v7/python:3.7-slim-stretch as runtime\nCOPY model.tflite /\nCOPY docker_tflite.py /\nCOPY --from=dev /root/.local /root/.local\nRUN apt-get update \\\n    && apt-get -y install libatlas-base-dev \\\n    && apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\nCMD [\"python\", \"docker_tflite.py\"]\n^^^ changed to stretch and added the apt-get clean stuff",
    "Upgrade Dockerfile for Quarkus 1.11->1.13": "The change is because of the switch to the fast-jar as the default package. See this for details.\nTo build a docker container for the new packaging type, you essentially need to do something like:\nCOPY target/quarkus-app/lib/ /deployments/lib/\nCOPY target/quarkus-app/*.jar /deployments/\nCOPY target/quarkus-app/app/ /deployments/app/\nCOPY target/quarkus-app/quarkus/ /deployments/quarkus/\nas can be seen here.",
    "if xx or xx, else... condition in dockerfile": "Within a single Docker RUN instruction, you're running a shell command, and any Bourne shell syntax is allowed. If you're using [ ... ] that's actually running test(1) which has a -o operator for \"or\".\nRUN if [ condition1 -o condition 2 ]; then \\\n      command1; \\\n    else \\\n      command2; \\\n    fi\nThe POSIX spec suggests the -o option is obsolete, and recommends using the shell || operator instead.\nRUN if [ condition1 ] || [ condition2 ]; then ...; fi\nRUN if test condition1 || test condition2 ; then ...; fi\nMore generally, Dockerfiles don't support conditionals; you can't conditionally COPY a file in, for example. Conversely, if you are writing complex installation logic, you can write and test an ordinary shell script on the host, COPY it into the image, and RUN it, which can be easier than trying to remember to backslash-escape every line ending.",
    "How to handle FFMPEG module missing probe inside docker?": "As a hack, this worked for me -\nRUN apt-get install -y ffmpeg\nRUN pip install -r requirements.txt\nRUN pip uninstall -y ffmpeg-python==0.2.0\nRUN pip install ffmpeg-python==0.2.0",
    "wkhtmltopdf giving error inside the docker container": "Installing the following dependencies with wkhtmltopdf worked out for me:\n...\nRUN apt-get update\nRUN apt-get install -y xvfb\nRUN apt-get install -y wget\nRUN apt-get install -y openssl build-essential xorg libssl1.0-dev\nRUN wget https://github.com/wkhtmltopdf/wkhtmltopdf/releases/download/0.12.4/wkhtmltox-0.12.4_linux-generic-amd64.tar.xz\nRUN tar xvJf wkhtmltox-0.12.4_linux-generic-amd64.tar.xz\nRUN cp wkhtmltox/bin/wkhtmlto* /usr/bin/\n...",
    "Problem with building Docker image from Dockerfile": "Default repositories are deprecated.\nReplace repositories *.ubuntu.com with old-releases.ubuntu.com in /etc/apt/sources.list with following (This is entire Dockerfile):\nFROM ubuntu:18.10\n\nRUN sed -i 's/archive.ubuntu.com/old-releases.ubuntu.com/g' /etc/apt/sources.list\nRUN sed -i 's/security.ubuntu.com/old-releases.ubuntu.com/g' /etc/apt/sources.list\n\nRUN apt-get update && apt-get install -y texlive-fonts-recommended \\\n    texlive-generic-recommended \\\n    texlive-latex-extra \\\n    texlive-fonts-extra \\\n    dvipng \\\n    texlive-latex-recommended \\\n    texlive-base \\\n    texlive-pictures \\\n    texlive-lang-cyrillic \\\n    texlive-science \\\n    cm-super \\\n    texlive-generic-extra\n\nCMD [\"bash\"]\nNote that I added -y to the apt-get install command because I want to automatically agree to the installation.",
    "Temporarily cache RUN apt-get update && apt-get install in Dockerfile": "Yes, it possible.\nCreate docker image from your docker file. Separate file to 2 Dockerfile's, first with FROM and RUN apt-get update && apt-get install -y, second with other instructions\ncd /dir/with/Dockerfile\ndocker image build -t my-fast-docker .\nAfter build use image in second Dockerfile\nFROM my-fast-docker:latest",
    "Use env with arg in Dockerfile cause cache layer invalidate [duplicate]": "The docker docs has a clear explanation, well, I was careless.\nhttps://docs.docker.com/engine/reference/builder/#impact-on-build-caching",
    "Dockerfile - Creating non-root user almost doubles the image size": "Don't chown the directory; leave root owning it. You also shouldn't need to set a shell, or a password, or create a home directory; none of these things will be used in normal operation.\nI'd suggest creating the user towards the start of the Dockerfile (it is fairly fixed and so this step can be cached) but only switching USER at the very end of the file, when you're setting up the metadata for how to run the container.\nA Node-based example:\nFROM node:lts # debian-based\n\n# Create the non-root user up front\nRUN adduser --system --group --no-create-home newuser\n\n# Copy and build the package as usual\nWORKDIR /app\nCOPY package.json yarn.lock .\nRUN yarn install\nCOPY . .\nRUN yarn build\n\n# Now the application is built\n# And root owns all the files\n# And that's fine\n\n# Say how to run the container\nEXPOSE 3000\nUSER newuser\nCMD yarn start\nHaving root owning the files gives you a little extra protection in case something goes wrong. If there's a bug that allows files in the container to be overwritten, having a different user owning those files prevents the application code or static assets from being inadvertently modified.\nIf your application needs to read or write files then you could create a specific directory for that:\n # Towards the end of the file, but before the USER\n RUN mkdir data && chown newuser data\nThis will let the operator mount some storage over the otherwise-empty directory. This is the only thing that has the newly created user ID in it at all, so if the storage comes with its own owner it shouldn't be an operational problem; you need to also specify the matching user ID at container startup time.\ndocker run -u $(id -u) -v $PWD/data:/app/data ...",
    "How to build docker image that have telnet client": "You can update the package list and install telnet as part of your Docker file. For example.\n# For more information, please refer to https://aka.ms/vscode-docker-python\nFROM python:3.8-slim-buster\n\n#update package list and install telnet\nRUN apt update && apt install telnet\nThis will refresh the package list of the OS then install telnet into the OS",
    "Docker-Compose Args (Variable Expansion) in Multi-stage Builds": "Variable expansion is not supported in COPY --from. See this issue for more details. You can modify your Dockerfile to implement this by defining a stage that you late copy from:\nARG PHP_VERSION\nARG COMPOSER_VERSION\n\nFROM composer:${COMPOSER_VERSION} as composer\nFROM php:${PHP_VERSION}\n\nCOPY --from=composer /usr/bin/composer /usr/bin/composer",
    "what will happen if i delete my bind mount file while the container is running?": "The way most Linux/Unix filesystems work, when you delete a file or directory all you are actually doing is deleting entry in the lookup table that maps a filename to what is known as an inode.\nWhen an application opens a file it uses the lookup table to find the inode and from then on all actions take part on the inode. So if an application has a file open and you delete it, the application will continue to work normally and be able to read/write to the file. Once the application is closed there will be no way to access that inode again so any changes to the (now deleted) file will be lost.\nThe same applies to the bind mount. When you delete the directory, the container still has a valid handle to the inode that represents the base directory and will continue to work normally until the container is destroyed.",
    "Not able to install unzip tool in my container": "Since you're using CentOs the default package manager is not apt-get but instead yum.\nSo doing this is what your dockerfile should look like:\nFROM centos\n...\n\nRUN yum update\nRUN yum install unzip\nAs a more detailed explanation you can remember this:\napt-get is the default package manager for debian based distributions of Linux like Ubuntu and a bunch of others. yum is the default package manager for RPM based distributions ( Redhat package manager )",
    "How can I use a command from another container using Docker Compose?": "For those who are interested in the way on how to use ssh, I've added a small example which allows to use ssh between container without\ndealing with passwords for authetication\nexposing private/public keys to the outside environment or the host\nhaving access from the outside (only docker container within the same docker network have access)\nDescription\ndocker-compose.yml\nThe docker-compose file. It consist of some configuration.\nI have assigned my container static IPs, which allows a more easy access.\nI have added a volume (sshdata) to share the ssh-keys between the containers (for authentication).\nversion: \"3.8\"\nservices:\n  first-service:\n    build: \n      context: .\n      dockerfile: Dockerfile-1\n    networks: \n      vpcbr:\n        ipv4_address: 10.5.0.2\n    environment: \n      - SECOND_SERVICE=10.5.0.3\n    volumes:       \n      - sshdata:/home/developer/.ssh/\n\n  second-service:\n    build: \n      context: .\n      dockerfile: Dockerfile-2\n    networks: \n      vpcbr:\n        ipv4_address: 10.5.0.3\n    volumes:       \n      - sshdata:/home/developer/.ssh/\n    depends_on: \n      - first-service\n\nnetworks:\n  vpcbr:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 10.5.0.0/16\n\nvolumes: \n  sshdata:\nDockerfiles\nThe Dockerfiles for the services are the same, only the entrypoint.sh-scripts are different (see below).\nFROM ubuntu:latest\n\n# We need some tools\nRUN apt-get update && apt-get install -y ssh sudo net-tools\n# We want to have another user than `root`\nRUN adduser developer\n\n## USER SETUP \n# We want to have passwordless sudo access\nRUN \\\n    sed -i /etc/sudoers -re 's/^%sudo.*/%sudo ALL=(ALL:ALL) NOPASSWD: ALL/g' && \\\n    sed -i /etc/sudoers -re 's/^root.*/root ALL=(ALL:ALL) NOPASSWD: ALL/g' && \\\n    sed -i /etc/sudoers -re 's/^#includedir.*/## **Removed the include directive** ##\"/g' && \\\n    echo \"developer ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers;  su - developer -c id\n\n# Run now with user developer\nUSER developer\nADD ./entrypoint-1.sh /entrypoint-1.sh\nRUN sudo chmod +x /entrypoint-1.sh\n\nENTRYPOINT [ \"/entrypoint-1.sh\" ]\nEntrypoint-Scripts\nNow we come to the important stuff: The entrypoint.sh-scripts, which perform the needed set-up-steps. Our first container (first-service) should be able to ssh to our second container (second-service).\nFor this there is no special setup for our first service. We just change the owner of the ~/.ssh folder to have writing access to ~/.ssh/known_hosts  (but you can just disable strict host key checking if you dont want to do this)\n#!/bin/bash\n# ENTRYPOINT FOR SERVICE first-service\n\n# We can now ssh to our other container \n# Change the owner of the .ssh folder and it's content\nsudo chown -R developer:developer ~/.ssh \n\n# Perform your command\nwhile ! ssh-keyscan -H ${SECOND_SERVICE} >> ~/.ssh/known_hosts \ndo\n    echo \"Host not up, trying again...\"\n    sleep 1;\ndone\n\n# -------------------------------------\n# Here we can run our command\nssh developer@${SECOND_SERVICE} \"ls -l /\"\necho \"DONE!\"\n\n# -------------------------------------\n# Here you can do other stuff\ntail -f /dev/null\nOne remarkable line is the while-loop: We do not really know, when our second service is ready for a ssh-connection. We can wait, but thats not that elegant. Instead, we periodically try to connect to the second container until the command succeeded. Afterwards it will continue with the actual command.\nThe last thing is the entrypoint.sh-Script for the second service:\n#!/bin/bash\n# ENTRYPOINT FOR SERVICE second-service\n\n## --  A little bit of setup for ssh\n# Starting the server\nsudo service ssh start\n# Generate a key\nsudo ssh-keygen -t rsa -f /home/developer/.ssh/id_rsa  \n# Change the owner of the .ssh folder and it's content\nsudo chown -R developer:developer ~/.ssh \n# Add the keys\nsudo echo $(cat /home/developer/.ssh/id_rsa.pub) >> ~/.ssh/authorized_keys\n\n# -------------------------------------\n# Here we can start doing the stuff\ntail -f /dev/null\nMaybe this help someone.",
    "Unable to connect application on localhost after docker deploymnet is successful": "Look at your port information :\nThe port 9100 of the container is published on the port 7000 on the host.\nBut look at your log application : it is deployed on 8080 (and not on 9100).\nWhat you want is publishing the container port 8080 on the 7000 port of the host such as :\ndocker run -p 7000:8080 foo-image",
    "Docker image taking long to build": "There's a bunch of issues here. First, there are packages you don't need:\nYou're installing python twice. You're installing python, the Debian Python package, but the Docker python image has its own version of Python (in /usr/local), with dev headers already there. You don't need this, and it can lead to confusion because you end up with two versions of Python (https://pythonspeed.com/articles/importerror-docker/).\nmusl-dev is unnecessary. Debian uses glibc, not musl. I suspect this is holdover from Alpine.\nYou are installing a compiler, a whole bunch of C headers in general, all those *-dev packages. It's quite possible you don't need to at all! On Alpine, you need to compile everything because Alpine can't use normal binary wheels. (https://pythonspeed.com/articles/alpine-docker-python/) Since you're on Debian, quite possibly all your depedencies have binary wheels. You would still need a compiler for your own code, but if it's pure Python quite possibly not.\nSo my suggestion: just drop the whole apt-get line. Pretty good chance it'll Just Work without it.",
    "Dockerfile: Are multiple FROM (s) allowed? [duplicate]": "Yes multiple FROM lines are allowed. This is part of the multi-stage build implementation. Each FROM line starts a new stage as a new image, and that image only includes the filesystem layers and configuration of that FROM line. The tagged image at the end is either the last stage to be built, or the targeted stage if you direct the build to tag a specific stage.\nNote that multi-stage is not some kind of multiple inheritance or image merging process. That doesn't exist, and there's no trivial way to implement that. Multi-stage builds are designed to allow the build environment to be moved into the docker environment, without including compilers and other build tooling in the distributed image. It does this by allowing files to be copied between stages with the COPY --from syntax.",
    "Docker: How to access files from another container from a given container?": "You can create one common datavolume and attached your containers with the datavolume\nHere is the step to create a datavolume,\nStep 1 : docker volume create --name storageOne You can give any name instead of storageOne\nStep 2 : Now you need to attach that volume with the container using docker run -ti --name=myContainer -v storageOne:/storageOne ubuntu command\nStep 3 : Copy or create your required file in that datavolume\nStep 4 : Now Create an another Container using docker run -ti --name=myContainer2 --volumes-from MyContainer ubuntu command\nStep 5 : Restart your myStorage container\nSo whatever files are available in myStorage will be shareable between attached container.\nMay be this will help you",
    "Pass env variable to docker-compose": "just define environement variables in flask app and do os.getenv of them in python application, than add them to your flask app service in docker compose file:\nflask_app:\n  environment:\n    RABBIT_USER: guest\n    RABBIT_PASSWORD: pass123\nIn your python file place following:\nimport os\n\nredis = Redis(host='redis_app', port=6379, password=os.getenv('RABBIT_PASSWORD'))",
    "Docker ignore target directories created by mave using .dockerignore file": "None of the answers above seemed to work for me (docker version 20.10.5). In the end, the following worked out OK:\n.dockerignore:\n**/target/\nHere is an excerpt from the .dockerignore documentation:\nBeyond Go\u2019s filepath.Match rules, Docker also supports a special wildcard string ** that matches any number of directories (including zero). For example, **/*.go will exclude all files that end with .go that are found in all directories, including the root of the build context.",
    "mcr.microsoft.com/dotnet/core/aspnet:2.2 for windows container not available": "Several things wrong here:\n.NET Core 2.2 has been out of support since December 2019 so the manifest tag for 2.2 has been removed from all .NET Core Docker repositories. It's recommended that you upgrade to a supported version of .NET Core, like 3.1.\nThe error message indicates you're using Windows 10/Server, version 1703 which has been out of support since October 2019. Updated versions of .NET Core are not being provided for out of support Windows releases, not even if it's a supported version of .NET Core.\nThere have never been Docker images of any .NET Core version published for Windows, version 1703. So there wouldn't even be a way to pull a concrete tag instead of using a manifest tag because no such tag exists.",
    "Error while docker image build with below dockerfile": "In fact the issue cause is not so complex:\nCOPY ./target/dependency/BOOT-INF/lib /app/lib\nCOPY failed: stat /var/lib/docker/tmp/docker-builder226394005/target/dependency/BOOT-INF/lib: no such file or directory\nWhat does it mean ?\nIf these directories effectively exist on your host, it means that the target directory is not a direct child directory of the build context.\nWhat is the build context ?\nThe docker build command builds Docker images from a Dockerfile and a \u201ccontext\u201d. A build\u2019s context is the set of files located in the specified PATH or URL. The build process can refer to any of the files in the context. For example, your build can use a COPY instruction to reference a file in the context.\nWhen you execute : docker build -t myTag ., the context is the current directory.\nSo just adjust the build context to be a parent directory of the target directory and it should be correct.\nFor example with the layout :\n- docker\n      - Dockerfile\n- src\n- target\nYou should keep the context as the base directory and set the DockerFile path in that way :\ndocker build -t myTag -f docker/Dockerfile . ",
    "SBT project not mounted to Docker container using docker-compose": "The Dockerfile is considered and processed, in its entirety, before most other options in the docker-compose.yml file are considered. In particular an image build never sees volumes: from a Docker Compose setup (it also cannot see environment: variables or any network-related settings, including the default network).\nThat means, at build time (because it's a RUN directive), you're trying to run sbt run on an empty anonymous volume. That produces the \"no main class\" error you're getting.\nFor JVM-based languages, a typical setup is to compile the application on the host, and then COPY the (portable) .jar file into your image. The sbt-native-packager plugin can actually do all of this for you; if you'd prefer to do it by hand, the sbt-assembly plugin can produce a \"fat\" .jar containing all of the application's dependencies. You then get the typical minimal JVM Dockerfile\nFROM openjdk:8\nCOPY target/scala-2.13/my-app-*.jar /my-app.jar\nCMD [\"java\", \"-jar\", \"/my-app.jar\"]\nIf you want to compile the application in the Dockerfile then you need to COPY the application in, and set the CMD to run the application when the container starts (not during the build).\nFROM hseeberger/scala-sbt:8u222_1.3.5_2.13.1\nWORKDIR /www/app\n\n# Copy the application source in.\nCOPY ./ ./\n\n# Build it.\nRUN sbt compile\n\n# Set the command to run and other metadata when the container starts.\nEXPOSE 8000 8080\nCMD sbt run\n(You could also use a multi-stage build to combine these two Dockerfiles: first COPY the source tree in and sbt compile it, then in a second stage COPY --from=build only the jar file into a JRE-only image.)\nBecause of the COPY instruction, you need to set the build context to be the root of the source tree. (It might be easier to move the Dockerfile and docker-compose.yml to the repository root, next to the build.sbt file.)\nversion: \"3.3\"\nservices:\n  app:\n    build:\n      context: ..\n      dockerfile: docker/Dockerfile\n    ports:\n     - \"8080:8080\"\n     - \"8000:8000\"\n    # No need for volumes:, source code is already in the image\nIn principle you could bind-mount your application code in as you had it in the question, and the sbt run command will recompile it. You do not need a VOLUME directive in your Dockerfile. You will probably find it easier to do day-to-day development on a host JDK/sbt environment.",
    "Docker mysql via MariaDB with Supervisor [closed]": "The MySQL service should run as root user, but later that's the mysql user whiche tries to access to the \"socket\". So, the socket directory should be accessible by mysql user but Superviser runs the mysql service as root user.\nI fixed this issue by creating and gave right permission to the MySQL socket directory in my Dockerfile:\nARG MARIADB_MYSQL_SOCKET_DIRECTORY='/var/run/mysqld'\n\nRUN mkdir -p $MARIADB_MYSQL_SOCKET_DIRECTORY && \\\n    chown root:mysql $MARIADB_MYSQL_SOCKET_DIRECTORY && \\\n    chmod 774 $MARIADB_MYSQL_SOCKET_DIRECTORY\nthen configured the Supervisor like this:\n[program:mariadb]\ncommand=/usr/sbin/mysqld\nautorestart=true\nuser=root",
    "Copy a folder from Docker to host": "It means you are trying to copy for a path inside the container which does not exit. You should exec inside your docker container using docker exec -it nostalgic_brattain /bin/<shell your container uses (sh or bash)> Once inside the container, navigate to the exact location, run the pwd command, and use that in your 'docker cp' command.\nYou might also be able to find out the location by observing the Dockerfile.",
    "Command Not Found with Dockerfile CMD": "The issue here is the quotes. Use double \" quotes.\nFrom Docker Documentation:\nThe exec form is parsed as a JSON array, which means that you must use double-quotes (\u201c) around words not single-quotes (\u2018).\nThis is applicable for other instructions such as RUN, LABEL, ENV, ENTRYPOINT and VOLUME.",
    "Multiple apps (microservices) and one proxy (nginx) docker-compose configuration/architecture": "There is a third approach, for example documented in https://www.bogotobogo.com/DevOps/Docker/Docker-Compose-Nginx-Reverse-Proxy-Multiple-Containers.php and https://github.com/Einsteinish/Docker-compose-Nginx-Reverse-Proxy-II/. The gist of it is to have the proxy join all the other networks. Thus, you can keep the other compose files, possibly from a software distribution, unmodified.\ndocker-compose.yml\nversion: '3'\nservices:\n  proxy:\n    build: ./\n    networks:\n      - microservice1\n      - microservice2\n    ports:\n      - 80:80\n      - 443:443\n\nnetworks:\n  microservice1:\n    external:\n      name: microservice1_default\n  microservice2:\n    external:\n      name: microservice2_default\nProxy configuration\nThe proxy will refer to the hosts by their names microservice1_app_1 and microservice2_app_1, assuming the services are called app in directories microservice1 and microservice2.",
    "Accessing ssl .pem files inside docker container": "After an entire night battling with the same issue myself, I've found the cause and I hope this helps anyone coming across this in the future:\nThe certificates in /etc/letsencrypt/live/your-domain are not actually certificates, rather, they are shortcuts pointing to the real certs in the archive folder. The real certs:\n/etc/letsencrypt/archive/your-domain/cert1.pem\n/etc/letsencrypt/archive/your-domain/fullchain1.pem\n...etc\nSo since you only mounted the live folder, it's pointing to nothing when it's actually being accessed. To fix this, either mounted the archive folder or the entire letsencrypt folder, then point to the certificates.",
    "Dockerfile, sbt-assembly - is it possible to use sbt-assembly in dockerfile?": "I solved this problem by copying files to repository and set WORKDIR. Now I can use sbt assembly:\nFROM hseeberger/scala-sbt:graalvm-ce-19.3.0-java11_1.3.7_2.13.1 as build\nCOPY . /my-project\nWORKDIR /my-project\nRUN sbt assembly",
    "How can I make the docker container to run a script every time when the container restart?": "entrypoint runs every time a container starts, or restarts. It's common practice to put startup configuration in a shell script that then execs the application's \"true\" entrypoint at the end. (See What purpose does using exec in docker entrypoint scripts serve? for why exec is important).\nRemember, docker is really just a wrapper around filesystem , process, and network namespacing. It can't restart your container in any way other than rerunning the same process it started in the first place.\nYou can try it yourself with an invocation something like this:\ndocker run  -d --restart=always --entrypoint=sh alpine -c \"sleep 5; echo Exiting; exit\"\nif you docker logs -f that container, you'll see the Exiting come out after every 5 seconds. Note that the container stopping will also stop the log following though, so you'll have to run it again to see the next restart.",
    "\u2018SkiaSharp.SKImageInfo\u2019 Exception in Aspose Word v18.8.0": "Aspose.Words 18.8 used SkiaSharp 1.60.3 version. Goelze.SkiaSharp.NativeAssets.AlpineLinux required SkiaSharp 1.68.0 or newer. You should update to Aspose.Words 19.2.0 version (in this version 1.68.0 version of SkiaSharp is used). Or to the latest version, which uses 1.68.1 version of SkiaSharp.\nYou have to add Linux native assets for SkiaSharp to make it work in Linux Alpine. Add Nuget referenace to Goelze.SkiaSharp.NativeAssets.AlpineLinux. Also do not forget to install fontconfig in your container. It is required by SkiaSharp to work with fonts. Here is Dockerfile I used for testing Aspose.Words in Linux Alpine container.\nFROM mcr.microsoft.com/dotnet/core/sdk:2.2-alpine3.9 AS build\nWORKDIR /app\n\n# copy csproj and restore as distinct layers\nCOPY Aspose.Words.Docker.Sample/*.csproj ./Aspose.Words.Docker.Sample/\nWORKDIR /app/Aspose.Words.Docker.Sample\nRUN dotnet restore\n\n# copy and publish app and libraries\nWORKDIR /app/\nCOPY Aspose.Words.Docker.Sample/. ./Aspose.Words.Docker.Sample/\nWORKDIR /app/Aspose.Words.Docker.Sample\nRUN dotnet publish -c Release -o out\n\n# copy to runtime environment\nFROM mcr.microsoft.com/dotnet/core/runtime:2.2-alpine3.9 AS runtime\nWORKDIR /app\n# fontconfig is required to properly work with fonts in Linux.\nRUN apk update && apk upgrade && apk add fontconfig\nCOPY --from=build /app/Aspose.Words.Docker.Sample/out ./\nENTRYPOINT [\"dotnet\", \"Aspose.Words.Docker.Sample.dll\"]\nNote, Goelze.SkiaSharp.NativeAssets.AlpineLinux is compiled for Alpine Linux 3.9 and does not work on 3.8.\nDisclosure: I work at Aspose.Words team.",
    "Passing environment variables across stages in docker multistage image": "You can do one of the following\nUse a base container and set the environment values there\nFROM alpine:latest as base\nARG version_default\nENV version=$version_default\n\nFROM base\nRUN echo ${version}\n\nFROM base\nRUN echo ${version}\nOther way is to use ARGS as below. There is some repetition but it becomes more centralised\nARG version_default=v1\n\nFROM alpine:latest as base1\nARG version_default\nENV version=$version_default\nRUN echo ${version}\nRUN echo ${version_default}\n\nFROM alpine:latest as base2\nARG version_default\nRUN echo ${version_default}\nNote examples copied from https://github.com/moby/moby/issues/37345",
    "Docker containers with multiple bases?": "For a concrete example if I wanted ubuntu and I also wanted python:\nFROM python:2  \nFROM ubuntu:latest\nUbuntu is Os, not a python. so what you need a Ubuntu base image which has python installed.\nyou can check offical python docker hub are based on ubuntu, so at one image you will get ubuntu + python, then why bother with two FROM? which is not also not working.\nSome of these tags may have names like buster or stretch in them. These are the suite code names for releases of Debian and indicate which release the image is based on. If your image needs to install any additional packages beyond what comes with the image, you'll likely want to specify one of these explicitly to minimize breakage when there are new releases of Debian.\nSo for you below question\nWhat is the best way to go about it? Am I just limited to one base? If I want the functionality from both am I supposed to go into the docker files\nyes, limit it one base image suppose your base image\npython:3.7-stretch\nSo with this base image, you have python and ubuntu both. you do not need to make Dockerfile that have two FROM.\nAlso, you do need to maintain and build the image from scratch, use the offical one and extend as per your need.\nFor example\nFROM python:3.7-stretch\nRUN apt-get update && apt-get install -y  vim\nRUN pip install mathutils",
    "Can't get response from Express API in k8s-Skaffold from Postman": "OK, got this sorted out now.\nIt boils down to the kind of Service being used: ClusterIP.\nClusterIP: Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster. This is the default ServiceType.\nIf I am wanting to connect to a Pod or Deployment directly from outside of the cluster (something like Postman, pgAdmin, etc.) and I want to do it using a Service, I should be using NodePort:\nNodePort: Exposes the service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP service, to which the NodePort service will route, is automatically created. You\u2019ll be able to contact the NodePort service, from outside the cluster, by requesting <NodeIP>:<NodePort>.\nSo in my case, if I want to continue using a Service, I'd change my Service manifest to:\napiVersion: v1\nkind: Service\nmetadata:\n  name: server-cluster-ip-service\nspec:\n  type: NodePort\n  selector:\n    component: server\n  ports:\n    - port: 5000\n      targetPort: 5000\n      nodePort: 31515\nMaking sure to manually set nodePort: <port> otherwise it is kind of random and a pain to use.\nThen I'd get the minikube IP with minikube ip and connect to the Pod with 192.168.99.100:31515.\nAt that point, everything worked as expected.\nBut that means having separate sets of development (NodePort) and production (ClusterIP) manifests, which is probably totally fine. But I want my manifests to stay as close to the production version (i.e. ClusterIP).\nThere are a couple ways to get around this:\nUsing something like Kustomize where you can set a base.yaml and then have overlays for each environment where it just changes the relevant info avoiding manifests that are mostly duplicative.\nUsing kubectl port-forward. I think this is the route I am going to go. That way I can keep my one set of production manifests, but when I want to QA Postgres with pgAdmin I can do:\nkubectl port-forward services/postgres-cluster-ip-service 5432:5432\nOr for the back-end and Postman:\nkubectl port-forward services/server-cluster-ip-service 5000:5000\nI'm playing with doing this through the ingress-service.yaml using nginx-ingress, but don't have that working quite yet. Will update when I do. But for me, port-forward seems the way to go since I can just have one set of production manifests that I don't have to alter.\nSkaffold Port-Forwarding\nThis is even better for my needs. Appending this to the bottom of the skaffold.yaml and is basically the same thing as kubectl port-forward without tying up a terminal or two:\nportForward:\n  - resourceType: service\n    resourceName: server-cluster-ip-service\n    port: 5000\n    localPort: 5000\n  - resourceType: service\n    resourceName: postgres-cluster-ip-service\n    port: 5432\n    localPort: 5432\nThen run skaffold dev --port-forward.",
    "Docker stack deploy with compose-file results in invalid mount config for type \"bind\": bind source path does not exist:": "So the actual problem was the fact that I didn't understand that the volume must exist on the host I am deploying the containers. For some reason, I thought it was being copied/synchronized from my local device.\nI also came to conclusion that the complexity of getting your source code to host in docker-swarm is something that I would rather not have when kubernetes have this solved, so I am switching to kubernetes (and gc for that matter).",
    "How to prepend something to CLI in docker container?": "The error you get certainly comes from the fact you use single quotes ' instead of double quotes \" in the ENTRYPOINT exec form.\nIn addition, I don't think the \"$@\" phrasing you mention will work (because \"$@\" needs some shell to evaluate it, while in the exec form there is no /bin/sh -c \u2026 implied). But the exec form of ENTRYPOINT is definitely the way to go.\nSo I'd suggest you write something like this:\nFROM centeredge/nuget\nARG VERSION=\"14.1.0.0-prerelease\"\nRUN nuget install Microsoft.Build.Mono.Debug -Version $VERSION -Source \"https://www.myget.org/F/dotnet-buildtools/\"\nENV PATH=\"/Microsoft.Build.Mono.Debug.$VERSION/lib/:${PATH}\"\nCOPY entrypoint.sh /usr/src/\nRUN chmod a+x /usr/src/entrypoint.sh\nENTRYPOINT [\"/usr/src/entrypoint.sh\"]\nwith entrypoint.sh containing:\n#!/bin/bash\nexec /usr/bin/mono \"/Microsoft.Build.Mono.Debug.$VERSION/lib/$1\" \"$@\"\n(Note: I didn't test this example code for now so please comment if you find some typo)",
    "Remote control on my postgresql container docker works but can't see tables creates": "The structure.sql script creates the tables in the default database postgres and not in DB_ALLOT_BOUCHON_NEW. When the container is initialized you are connected to postgres.To quickly fix connect to db_allot_bouchon_new right after you create it\nCREATE DATABASE DB_ALLOT_BOUCHON_NEW;\n\\connect db_allot_bouchon_new\n ...\nThis may help as well.",
    "How do I call a function from a shell script in Docker?": "Change this to be an ordinary shell script; in generic programming terms, make it a \"program\" and not a \"function\".\nRemember that docker run starts a new container with a clean environment, runs whatever its main program is, and then exits. Also remember that the difference between a shell script and a shell function is that a function runs in the context of its calling shell and can change environment variables and other settings, whereas a shell script runs in a new shell environment; but since in a docker run context the parent shell environment is about to get torn down, it doesn't matter.\nSo I'd change the shell script to\n#!/bin/bash\n# ^^^ use the standard shell location\n# vvv remove the function wrapper\necho \"Hello World\"\nand then change the Dockerfile to run it on startup\n...\nCOPY audit.sh /etc/audittools\nRUN chmod a+x /etc/audittools/audit.sh\nCMD [\"/etc/audittools/audit.sh\"]",
    "How to run a docker container for ruby on rails without root user?": "It seems like the problem is with the gem bootsnap as your error message indicates.\nIf you look in the bootsnap documentation you see this:\nNote that bootsnap writes to tmp/cache, and that directory must be writable. Rails will fail to boot if it is not. If this is unacceptable (e.g. you are running in a read-only container and unwilling to mount in a writable tmpdir), you should remove this line or wrap it in a conditional.\n\"this line\" refers to require 'bootsnap/setup' in config/boot.rb\nSo you either need to make sure the tmp/cache directory is writeable in your container or remove the the bootsnap line from config/boot.rb",
    "Pod Stuck on `CrashLoopBackOff` even though it should go into /bin/bash": "1. Original Answer (but edited...)\nIf you are familiar with Docker, check this.\nIf you are looking for an equivalent of docker run -dt kubelab, try kubectl run -it kubelab --restart=Never --image=ubuntu /bin/bash. In your case, with the Docker -t flag: Allocate a pseudo-tty. That's why your Docker Container stays up.\nTry:\nkubectl run kubelab \\\n    --image=ubuntu \\\n    --expose \\\n    --port 8080 \\\n    -- /bin/bash -c 'while true;do sleep 3600;done'\nOr:\nkubectl run kubelab \\\n    --image=ubuntu \\\n    --dry-run -oyaml \\\n    --expose \\\n    --port 8080 \\\n    -- /bin/bash -c 'while true;do sleep 3600;done'\n2. Explaining what's going on (Added by Philippe Fanaro):\nAs stated by @David Maze, the bash process is going to exit immediately because the artificial terminal won't have anything going into it, a slightly different behavior from Docker.\nIf you change the restart Policy, it will still terminate, the difference is that the Pod won't regenerate or restart.\nOne way of doing it is (pay attention to the tabs of restartPolicy):\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kubelab-pod\n  labels:\n    zone: prod\n    version: v1\nspec:\n  containers:\n  - name: kubelab-ctr\n    image: kubelab\n    imagePullPolicy: IfNotPresent\n    ports:\n    - containerPort: 8080\n  restartPolicy: Never\nHowever, this will not work if it is specified inside a deployment YAML. And that's because deployments force regeneration, trying to always get to the desired state. This can be confirmed in the Deployment Documentation Webpage:\nOnly a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified.\n3. If you really wish to force the Docker Container to Keep Running\nIn this case, you will need something that doesn't exit. A server-like process is one example. But you can also try something mentioned in this StackOverflow answer:\nCMD exec /bin/bash -c \"trap : TERM INT; sleep infinity & wait\"\nThis will keep your container alive until it is told to stop. Using trap and wait will make your container react immediately to a stop request. Without trap/wait stopping will take a few seconds.",
    "Integrate two docker apps - Docker compose and Docker run": "The configuration is not working mainly because dockersupport-app.json is not read by the application. Below is a working example based on the online documentation of the project.\nAlso another problem is the access to the dicomWeb server. You are using pacsIP:8042, which would be ok if the request was initiated from inside the container. But this is a javascript application and the request is initiated by the browser on the host. For this reason \"localhost\" should be used.\nThis is a working configuration:\nversion: '3.6'\n\nservices:\n  mongo:\n   image: \"mongo:latest\"\n   container_name: ohif-mongo\n   ports:\n     - \"27017:27017\"\n\n  viewer:\n     image: ohif/viewer:latest\n     container_name: ohif-viewer\n     ports:\n       - \"3030:80\"\n     environment:\n       - MONGO_URL=mongodb://mongo:27017/ohif\n     volumes:\n      - ./config/default.js:/usr/share/nginx/html/config/default.js\n     depends_on:\n      - mongo\n      - proxy\n\n  orthanc:\n    image: jodogne/orthanc-plugins\n    ports:\n      - \"4242:4242\"\n      - \"8042:8042\"\n    volumes:\n      # Config\n      - ./config/orthanc.json:/etc/orthanc/orthanc.json:ro\n      # Persist data\n      - ./volumes/orthanc-db/:/var/lib/orthanc/db/\n    command: \"/etc/orthanc --verbose\"\n\n  proxy:\n    image: nginx:1.15-alpine\n    ports:\n      - 8899:80\n    volumes:\n      - ./config/nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on: \n      - orthanc\n    restart: unless-stopped\nIn the config folder place the files:\ndefault.js\nwindow.config = {\n    // default: '/'\n    routerBasename: '/',\n    // default: ''\n    relativeWebWorkerScriptsPath: '',\n    servers: {\n      dicomWeb: [\n        {\n          name: 'DCM4CHEE',\n          wadoUriRoot: 'http://localhost:8899/wado',\n          qidoRoot: 'http://localhost:8899/dicom-web',\n          wadoRoot: 'http://localhost:8899/dicom-web',\n          qidoSupportsIncludeField: true,\n          imageRendering: 'wadouri',\n          thumbnailRendering: 'wadouri',\n          requestOptions: {\n            requestFromBrowser: true,\n            auth: \"orthanc:orthanc\",\n            \"logRequests\": true,\n            \"logResponses\": true,\n             \"logTiming\": true\n          },\n        },\n      ],\n    },\n    // Extensions should be able to suggest default values for these?\n    // Or we can require that these be explicitly set\n    hotkeys: [\n      // ~ Global\n      {\n        commandName: 'incrementActiveViewport',\n        label: 'Next Image Viewport',\n        keys: ['right'],\n      },\n      {\n        commandName: 'decrementActiveViewport',\n        label: 'Previous Image Viewport',\n        keys: ['left'],\n      },\n      // Supported Keys: https://craig.is/killing/mice\n      // ~ Cornerstone Extension\n      { commandName: 'rotateViewportCW', label: 'Rotate Right', keys: ['r'] },\n      { commandName: 'rotateViewportCCW', label: 'Rotate Left', keys: ['l'] },\n      { commandName: 'invertViewport', label: 'Invert', keys: ['i'] },\n      {\n        commandName: 'flipViewportVertical',\n        label: 'Flip Horizontally',\n        keys: ['h'],\n      },\n      {\n        commandName: 'flipViewportHorizontal',\n        label: 'Flip Vertically',\n        keys: ['v'],\n      },\n      { commandName: 'scaleUpViewport', label: 'Zoom In', keys: ['+'] },\n      { commandName: 'scaleDownViewport', label: 'Zoom Out', keys: ['-'] },\n      { commandName: 'fitViewportToWindow', label: 'Zoom to Fit', keys: ['='] },\n      { commandName: 'resetViewport', label: 'Reset', keys: ['space'] },\n      // clearAnnotations\n      // nextImage\n      // previousImage\n      // firstImage\n      // lastImage\n      {\n        commandName: 'nextViewportDisplaySet',\n        label: 'Previous Series',\n        keys: ['pagedown'],\n      },\n      {\n        commandName: 'previousViewportDisplaySet',\n        label: 'Next Series',\n        keys: ['pageup'],\n      },\n      // ~ Cornerstone Tools\n      { commandName: 'setZoomTool', label: 'Zoom', keys: ['z'] },\n    ],\n  };\nnginx.conf\nworker_processes 1;\n\nevents { worker_connections 1024; }\n\nhttp {\n\n    upstream orthanc-server {\n        server orthanc:8042;\n    }\n\n    server {\n        listen [::]:80 default_server;\n        listen 80;\n\n        # CORS Magic\n        add_header 'Access-Control-Allow-Origin' '*';\n        add_header 'Access-Control-Allow_Credentials' 'true';\n        add_header 'Access-Control-Allow-Headers' 'Authorization,Accept,Origin,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Content-Range,Range';\n        add_header 'Access-Control-Allow-Methods' 'GET,POST,OPTIONS,PUT,DELETE,PATCH';\n\n        location / {\n\n            if ($request_method = 'OPTIONS') {\n                add_header 'Access-Control-Allow-Origin' '*';\n                add_header 'Access-Control-Allow_Credentials' 'true';\n                add_header 'Access-Control-Allow-Headers' 'Authorization,Accept,Origin,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Content-Range,Range';\n                add_header 'Access-Control-Allow-Methods' 'GET,POST,OPTIONS,PUT,DELETE,PATCH';\n                add_header 'Access-Control-Max-Age' 1728000;\n                add_header 'Content-Type' 'text/plain charset=UTF-8';\n                add_header 'Content-Length' 0;\n                return 204;\n            }\n\n            proxy_pass         http://orthanc:8042;\n            proxy_redirect     off;\n            proxy_set_header   Host $host;\n            proxy_set_header   X-Real-IP $remote_addr;\n            proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header   X-Forwarded-Host $server_name;\n\n            # CORS Magic\n            add_header 'Access-Control-Allow-Origin' '*';\n            add_header 'Access-Control-Allow_Credentials' 'true';\n            add_header 'Access-Control-Allow-Headers' 'Authorization,Accept,Origin,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Content-Range,Range';\n            add_header 'Access-Control-Allow-Methods' 'GET,POST,OPTIONS,PUT,DELETE,PATCH';\n        }\n    }\n}\northanc.json\n{\n  \"Name\": \"Orthanc inside Docker\",\n  \"StorageDirectory\": \"/var/lib/orthanc/db\",\n  \"IndexDirectory\": \"/var/lib/orthanc/db\",\n  \"StorageCompression\": false,\n  \"MaximumStorageSize\": 0,\n  \"MaximumPatientCount\": 0,\n  \"LuaScripts\": [],\n  \"Plugins\": [\"/usr/share/orthanc/plugins\", \"/usr/local/share/orthanc/plugins\"],\n  \"ConcurrentJobs\": 2,\n  \"HttpServerEnabled\": true,\n  \"HttpPort\": 8042,\n  \"HttpDescribeErrors\": true,\n  \"HttpCompressionEnabled\": true,\n  \"DicomServerEnabled\": true,\n  \"DicomAet\": \"ORTHANC\",\n  \"DicomCheckCalledAet\": false,\n  \"DicomPort\": 4242,\n  \"DefaultEncoding\": \"Latin1\",\n  \"DeflatedTransferSyntaxAccepted\": true,\n  \"JpegTransferSyntaxAccepted\": true,\n  \"Jpeg2000TransferSyntaxAccepted\": true,\n  \"JpegLosslessTransferSyntaxAccepted\": true,\n  \"JpipTransferSyntaxAccepted\": true,\n  \"Mpeg2TransferSyntaxAccepted\": true,\n  \"RleTransferSyntaxAccepted\": true,\n  \"UnknownSopClassAccepted\": false,\n  \"DicomScpTimeout\": 30,\n\n  \"RemoteAccessAllowed\": true,\n  \"SslEnabled\": false,\n  \"SslCertificate\": \"certificate.pem\",\n  \"AuthenticationEnabled\": false,\n  \"RegisteredUsers\": {\n    \"test\": \"test\"\n  },\n  \"DicomModalities\": {},\n  \"DicomModalitiesInDatabase\": false,\n  \"DicomAlwaysAllowEcho\": true,\n  \"DicomAlwaysAllowStore\": true,\n  \"DicomCheckModalityHost\": false,\n  \"DicomScuTimeout\": 10,\n  \"OrthancPeers\": {},\n  \"OrthancPeersInDatabase\": false,\n  \"HttpProxy\": \"\",\n\n  \"HttpVerbose\": true,\n\n  \"HttpTimeout\": 10,\n  \"HttpsVerifyPeers\": true,\n  \"HttpsCACertificates\": \"\",\n  \"UserMetadata\": {},\n  \"UserContentType\": {},\n  \"StableAge\": 60,\n  \"StrictAetComparison\": false,\n  \"StoreMD5ForAttachments\": true,\n  \"LimitFindResults\": 0,\n  \"LimitFindInstances\": 0,\n  \"LimitJobs\": 10,\n  \"LogExportedResources\": false,\n  \"KeepAlive\": true,\n  \"TcpNoDelay\": true,\n  \"HttpThreadsCount\": 50,\n  \"StoreDicom\": true,\n  \"DicomAssociationCloseDelay\": 5,\n  \"QueryRetrieveSize\": 10,\n  \"CaseSensitivePN\": false,\n  \"LoadPrivateDictionary\": true,\n  \"Dictionary\": {},\n  \"SynchronousCMove\": true,\n  \"JobsHistorySize\": 10,\n  \"SaveJobs\": true,\n  \"OverwriteInstances\": false,\n  \"MediaArchiveSize\": 1,\n  \"StorageAccessOnFind\": \"Always\",\n  \"MetricsEnabled\": true,\n\n  \"DicomWeb\": {\n    \"Enable\": true,\n    \"Root\": \"/dicom-web/\",\n    \"EnableWado\": true,\n    \"WadoRoot\": \"/wado\",\n    \"Host\": \"127.0.0.1\",\n    \"Ssl\": false,\n    \"StowMaxInstances\": 10,\n    \"StowMaxSize\": 10,\n    \"QidoCaseSensitive\": false\n  }\n}\nWith this configuration in place run:\ndocker-compose up -d viewer\nUpload images: http://localhost:8899\nView the images in the viewer: http://localhost:3030",
    "couchbase docker port 8091 has 301 redirect": "You're accessing the root path when you go to just port 8091 with nothing else. Anything accessing Couchbase functionality is going to add a path, so this will be dealt with by internal routing. You can see those paths if you look at the REST api docs.\nFor whatever reason, they decided to host the admin UI off a base base starting with /ui. Hence the redirect, as they're assuming that if you didn't supply any path you want the UI.\nIt's not correct that nothing wants to access Couchbase through port 8092, either. Various services use different ports. 8092 is used for some forms of query and other purposes. You can find out more about the different ports and why you need them open in the Couchbase docs.",
    "How to build a docker build file that can run my c++ program in busybox?": "The busybox image contains a minimal collection of statically compiled binaries (most of which are really just hardlinks to busybox). The output of your gcc command, on the other hand, is a dynamically linked executable:\n$ g++ -o test4.out test4.cpp\n$ file test4.out\ntest4.out: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/l, for GNU/Linux 3.2.0, BuildID[sha1]=9c3a99f3baa5f699f4e32fa65acc58ac8ddc099c, not stripped\nIn order to execute, it requires the appropriate dynamic loader (typically something like /lib64/ld-linux-x86-64.so.2).\nThis doesn't exist in the busybox image, which leads to the \"not found\" error.\nIn addition to the dynamic loader, your code has a few additional shared library dependencies:\n$ ldd prog\n        linux-vdso.so.1 (0x00007fff01dbb000)\n        libstdc++.so.6 => /lib64/libstdc++.so.6 (0x00007f566279e000)\n        libm.so.6 => /lib64/libm.so.6 (0x00007f566240a000)\n        libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007f56621f2000)\n        libc.so.6 => /lib64/libc.so.6 (0x00007f5661e34000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007f5662b30000)\nYou would need to make all those shared libraries available inside the image for your code to run.\nYou can try to statically compile your code. You will first need to arrange to have the static versions of any required libraries on your system. On my Fedora 28 environment, this meant I first had to run:\nyum -y install libstdc++-static glibc-static\nAnd then I was able to generate a static version of the binary:\n$ g++ --static -o test4.out test4.cpp\n$ file test4.out\ntest4.out: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 3.2.0, BuildID[sha1]=d0f3b446020e1b067ededb59ec491bff9634f550, not stripped\nI can run this image in a busybox container without a problem.\nWARNING! There are some functions (typically those dealing with hostname resolution and users/groups) that require dynamic shared libraries at runtime even when compiled with --static.",
    "Why my docker image can not start app when I run it after import": "You're using the wrong commands: docker export and docker import only transfer the filesystem part of an image and not other data like environment variables or the default command. There's not really a good typical use case for these commands.\nThe standard way to do this is to set up a Docker registry or use a public registry server like Docker Hub, AWS ECR, GCR, ... Once you have this set up you can docker push an image to the registry from the system it was built on, and then docker pull it on the system you want to run it on (or directly docker run it, which will automatically pull the image if not present).\nIf you really can't set up a registry then the commands you actually want are docker save and docker load, which save complete images with all of their metadata. I've only every wanted these in environments where I can't connect the systems I want to run images to the registry server; otherwise a registry is almost always better. (Cluster environments like Docker Swarm and Kubernetes all but require a registry as well.)",
    "How to upload Dockerfile to dockerhub?": "AFAIK, the Dockerfile is only shown for repositories which have automated builds set up. See: https://docs.docker.com/docker-hub/builds/ and https://docs.docker.com/docker-hub/builds/advanced/ on how to set it up. This is a walkthrough: https://www.youtube.com/watch?v=sz96JV8S-Bk on how to set up one.",
    "Luigi Pipeline in Dockerfile; unknown instruction PYTHONPATH": "The Dockerfile syntax isn't actually JSON, and you can't have a line break after the [. Docker in effect rewrites this to\nCMD [\"/bin/sh\", \"-c\", \"[\"]\n(which would actually be valid! You probably have a /bin/[ binary! But the container would exit immediately with status code 0.)\nand then moves on to the next line\n\"PYTHONPATH='.'\", \"luigi\", \"--module\", \"pipe\",\nwhere it gets confused because this doesn't actually look like a Dockerfile directive.\nJust removing that newline on its own only gets you partway there. If you run\nCMD [\"PYTHONPATH='.'\", \"luigi\", ...]\nDocker won't launch a shell to try to run this; instead, it will look for a binary named exactly PYTHONPATH='.' in the usual directories, and when it doesn't find e.g. /usr/bin/PYTHONPATH='.' it will complain.\nYou shouldn't need to set this environment variable at all (especially since the pip install step will install packages into the image's isolated global Python installation), and I'd just delete it:\nCMD [\"luigi\", ...]\nIf you do need to set it, you need to use an explicit ENV directive\nENV PYTHONPATH .",
    "Revel and Docker container": "You're listening on localhost:9000, so 127.0.0.1 points to your container and not your local machine.\nYou have two solutions to make it work:\nListen on 0.0.0.0:9000\nUse --network=\"host\" in your docker run command: 127.0.0.1 in your docker container will point to your docker host.",
    "Docker compose throws permission denied on mac": "By bind-mounting your ./server directory into the same location where boot.sh is located on the container, you're rewriting the permissions so that it is no longer executable.\nPlease see the below demonstration:\nTJs-MacBook-Pro:stackoverflow tj$ ls -lah\ntotal 8\ndrwxr-xr-x   4 tj    wheel   128B Dec 23 14:58 .\ndrwxrwxrwt  15 root  wheel   480B Dec 23 15:05 ..\n-rw-r--r--   1 tj    wheel   143B Dec 23 14:57 docker-compose.yaml\ndrwxr-xr-x   6 tj    wheel   192B Dec 23 15:03 server\nTJs-MacBook-Pro:stackoverflow tj$ ls -lah server/\ntotal 24\ndrwxr-xr-x  6 tj  wheel   192B Dec 23 15:03 .\ndrwxr-xr-x  4 tj  wheel   128B Dec 23 14:58 ..\n-rw-------  1 tj  wheel   177B Dec 23 15:06 .ash_history\n-rw-r--r--  1 tj  wheel   508B Dec 23 14:56 Dockerfile\n-rw-r--r--  1 tj  wheel   105B Dec 23 14:56 boot.sh\n-rw-r--r--  1 tj  wheel     0B Dec 23 14:56 requirements.txt\nTJs-MacBook-Pro:stackoverflow tj$ docker-compose build\nBuilding teamreacher-server\nStep 1/13 : FROM python:3.7-alpine\n ---> 020295c920c6\nStep 2/13 : RUN adduser -D teamreacher\n ---> Using cache\n ---> 7dbd3131c941\nStep 3/13 : WORKDIR /home/teamreacher\n ---> Using cache\n ---> d2754b1b8dc2\nStep 4/13 : COPY ./requirements.txt requirements.txt\n ---> Using cache\n ---> 2d468491a297\nStep 5/13 : RUN python -m venv venv\n ---> Using cache\n ---> 0c135fa6f980\nStep 6/13 : RUN venv/bin/pip install --upgrade pip\n ---> Using cache\n ---> ea7df7153a5a\nStep 7/13 : RUN venv/bin/pip install -r requirements.txt\n ---> Using cache\n ---> db540a631c19\nStep 8/13 : COPY . .\n ---> 174a91493622\nStep 9/13 : RUN chmod +x boot.sh # Giving execution permissions here...\n ---> Running in b9cedb0f163a\nRemoving intermediate container b9cedb0f163a\n ---> 6d5ce7df1969\nStep 10/13 : RUN chown -R teamreacher:teamreacher ./\n ---> Running in 2e672e8ac6ef\nRemoving intermediate container 2e672e8ac6ef\n ---> 48342c4c31e3\nStep 11/13 : USER teamreacher\n ---> Running in 4d3bc99fb515\nRemoving intermediate container 4d3bc99fb515\n ---> 11bdc7be7f84\nStep 12/13 : EXPOSE 5000\n ---> Running in 0924db9c175b\nRemoving intermediate container 0924db9c175b\n ---> 0d96d773f42f\nStep 13/13 : CMD [\"./boot.sh\"] # ...so why do I get permission denied here?\n ---> Running in 4a33ab4e342a\nRemoving intermediate container 4a33ab4e342a\n ---> a9acefb4eeac\nSuccessfully built a9acefb4eeac\nSuccessfully tagged stackoverflow_teamreacher-server:latest\nTJs-MacBook-Pro:stackoverflow tj$ docker run stackoverflow_teamreacher-server ls -lah /home/teamreacher\ntotal 24\ndrwxr-sr-x    1 teamreac teamreac    4.0K Dec 23 20:07 .\ndrwxr-xr-x    1 root     root        4.0K Dec 23 19:56 ..\n-rw-------    1 teamreac teamreac     177 Dec 23 20:06 .ash_history\n-rw-r--r--    1 teamreac teamreac     508 Dec 23 19:56 Dockerfile\n-rwxr-xr-x    1 teamreac teamreac     105 Dec 23 19:56 boot.sh\n-rw-r--r--    1 teamreac teamreac       0 Dec 23 19:56 requirements.txt\ndrwxr-sr-x    1 teamreac teamreac    4.0K Dec 23 19:59 venv\nTJs-MacBook-Pro:stackoverflow tj$ docker run -v $(pwd)/server:/home/teamreacher stackoverflow_teamreacher-server ls -lah /home/teamreacher\ntotal 16\ndrwxr-xr-x    6 teamreac teamreac     192 Dec 23 20:03 .\ndrwxr-xr-x    1 root     root        4.0K Dec 23 19:56 ..\n-rw-------    1 teamreac teamreac     177 Dec 23 20:06 .ash_history\n-rw-r--r--    1 teamreac teamreac     508 Dec 23 19:56 Dockerfile\n-rw-r--r--    1 teamreac teamreac     105 Dec 23 19:56 boot.sh\n-rw-r--r--    1 teamreac teamreac       0 Dec 23 19:56 requirements.txt",
    "docker volume and VOLUME inside Dockerfile": "The Dockerfile VOLUME command says two things:\nIf the operator doesn't explicitly mount a volume on the specific container directory, create an anonymous one there anyways.\nNo Dockerfile step will ever be able to make further changes to that directory tree.\nAs an operator, you can mount a volume (either a named volume or a host directory) into a container with the docker run -v option. You can mount it over any directory in the container, regardless of whether or not there was a VOLUME declared for it in the Dockerfile.\n(Since you can use docker run -v regardless of whether or not you declare a VOLUME, and it has confusing side effects, I would generally avoid declaring VOLUME in Dockerfiles.)\nJust like in ordinary Linux, only one thing can be (usefully) mounted on any given directory. With the setup you describe, data will be stored in the myvol2 you create and mount, and it will be visible in /var/www in the container, but the data will only actually be stored in one place. If you deleted and recreated the container without the volume mount the data would not be there any more.",
    "Cannot access REST endpoint in docker container from another docker container": "Use service name microservice2 instead of localhost\n@GetMapping(\"/remote\")\npublic String remote() {\n    RestTemplate restTemplate = new RestTemplate();\n    return restTemplate.getForObject(\"http://microservice2:8080/maven-app2/\", String.class);\n}\nhttps://docs.docker.com/compose/networking/\nBy default Compose sets up a single network for your app. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.",
    "Unable to build docker image due to failed to process \"${project.artifactId}\": missing ':' in substitution": "Docker throws this misleading error whenever you have certain non alpha-numeric characters in your variable name.\nI assume that it is trying to interpret them as syntax to specify bash style default value substitution.\nTo prevent this error, remove the character from your variable name.\nIn your example it is the . causing the error, for me it was a - which I had to replace with an _.",
    "Passwords in Dockerfile": "In general, I would not put any password directly at the Dockerfile, for two reasons:\nGet your Dockerfile obsolete, forcing you to build a new image every time your password changes.\nPasswords or any other sensitive information should be handled in a safer way (it will depend on your use case).\nIn this particular case (which seems a non production case). Using ENV and ARG together would be the best approach:\nARG MSQL_SERVER_VERSION=2017-latest\nFROM microsoft/mssql-server-linux:$MSQL_SERVER_VERSION as sqlbase\n\nWORKDIR /usr/src/app\nCOPY ./sql-scripts /usr/src/app\n\nARG MSSQL_SA_PASSWORD=P@55w0rd\nENV MSSQL_SA_PASSWORD $MSSQL_SA_PASSWORD\n\nENV ACCEPT_EULA=Y\nRUN /opt/mssql/bin/sqlservr --accept-eula & sleep 10 \\\n    && /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P $MSSQL_SA_PASSWORD -i ./init.sql \\\n    && pkill sqlservr\nHaving MSSQL_SA_PASSWORD as an ARG and assigning its value to the MSSQL_SA_PASSWORD environment variable makes your Dockerfile more flexible. This also let you use it at the RUN command to avoid redundancy.\nYou can learn more about how ENV, ARG (and its scope) work in Dockerfile reference.",
    "How to fix denied permission to access a directory if that directory was added during docker build?": "Your directory does not have execute permission:\ndrw-r--r-- 3 solr solr 4096 Aug 31 14:21 conf\nWithout that, you cannot cd into the directory according to Linux filesystem permissions. You can fix that in your host with a chmod:\nchmod +x conf\nIf you perform this command inside your Dockerfile (with a RUN line), it will result in any modified file being copied to a new layer, so if you run this recursively, it could double the size of your image, hence the suggestion to fix it on your build host if possible.",
    "Dockerfile to run .net core console app on Linux (which is not self contained)": "Yes, Microsoft has a dotnet docker samples repository. Their Dockerfile looks like to following:\nFROM microsoft/dotnet:2.1-runtime\nCOPY /output /bin/\nENTRYPOINT [\"dotnet\", \"/bin/dotnetapp.dll\"]\nThey also have a alpine based example of a Dockerfile\nFor more information on Entrypoint / CMD",
    "Cannot access docker Ruby on Rails image after running": "When you supply the --port argument to rails Puma will only listen to the local interface. Without --port it will listen to all interfaces.\nTherefore to force Puma to listen to all interfaces with --port you have to explicitly tell it to do that:\nCMD rails server -b 0.0.0.0 --port 3000\nThis should allow the docker port forwarding to work properly.",
    "nodemon does not recharge with docker": "Try running nodemon using -L flag: nodemon -L app.js\nFrom documentation",
    "Volume path or Mount in Windows container": "You are using a bind mount, but because you have not specified a type, then it has defaulted to volume. In this case, source must be the name of the volume, or omitted for an anonymous volume.\nBecause you have give a path instead, you are getting this error. If you add a type key to your command, it should work:\ndocker run -it -p 8001:80 --mount 'type=bind, source=\"D:\\Projects\\Docker\\publish\", target=\"c:/app\"' --name docker-vol-test docker-vol\nIn answer to your second point, bind mounts require an absolute path. The usual way to use a relative path in Linux-land is to prepend the path with $PWD. In Windows, the equivalent of SPWD would be %cd%, so if you were running from D:\\Projects\\Docker, then the above would probably be:\ndocker run -it -p 8001:80 --mount 'type=bind, source=\"%cwd%\\publish\", target=\"c:/app\"' --name docker-vol-test docker-vol\nNote that I have no experience of Docker under Windows, but I believe the above should work.",
    "how to test nodejs application which is deployed as a pod on kubernetes cluster?": "If you want to use Kubernetes only, then using Jobs would be the easiest way. A job creates one or more pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the job tracks the successful completions. When a specified number of successful completions is reached, the job itself is complete.\nRough plan:\n1. Create a Job for your application.\nYou can use the Deployment of your application as a template. You need to change kind: Job and add spec.containers.command: [\"npm\", \"test\"], the last one will replace CMD [ \"npm\", \"start\" ] defined in your Dockerfile. Here is an example:\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: npm-test-job\n  labels:\n    purpose: test\nspec:\n  template:\n    spec:\n      containers:\n      - name: npm-test-job\n        image: <your-image>\n        command: [\"npm\", \"test\"]\n      restartPolicy: Never\n2. Run the Job\nRun the Job and wait until it is complete:\nkubectl create -f npm-test-job\n3. Check the status of the Job\nCheck status of your Job, for example:\nkubectl describe jobs kubectl describe jobs | grep \"Pods Statuses\"\n4. Run the Deployment\nIf test finished successfully, you can start you deployment:\nkubectl create -f npm-deployment\nOf course, you need to automate this process. Therefore, you need to develop a script implementing this logic.\nBut my advice here: it is better to use CI tool, such as Jenkins. It integrates easily with Kubernetes and provides a vast amount of functionalities for such cases.",
    "Dockerfile hide output of RUN command": "This works for me\nFROM ubuntu:16.04\n\nRUN which nano || echo no nano\nRUN bash -c \"apt-get update &> /dev/null\"\nRUN bash -c \"apt-get install nano &> /dev/null\"\nRUN which nano\n(really got the solution from Redirecting command output in docker)",
    "How to create queue in Rabbitmq": "Rabbitmq has a Management HTTP API. You can use this api to interact with rabbitmq.\nYou can create an exchange by doing a PUT request to http://localhost:15672/api/exchanges/${vhost}/${name}. Similarly, you can create a queue by doing a PUT to http://localhost:15672/api/queues/${vhost}/${name}.\nYou can call these using curl in the entrypoint script.",
    "Using docker to emulate a network on a single computer": "As \"h\" mentioned in the comments above, the bridge network driver does this for you. This driver is the default one so if you run multiple containers on your host they should be able to access each other via IP.\nIf you want DNS resolution you will need to define a bridge network since the default one will not provide that for you.\nDetails at https://docs.docker.com/engine/userguide/networking/\nHere's a simple demo using a custom bridge network:\nCreate the network:\n$ docker network create mynet\nStart some named containers on that network:\n$ docker run --rm -d --name one --net mynet nginx:latest\n$ docker run --rm -d --name two --net mynet nginx:latest\nFrom each container you should be able to resolve and hit the other(s):\none -> two:\n$ docker exec -it one ping two\nPING two (172.18.0.3): 56 data bytes\n64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.098 ms\n...\ntwo -> one:\n$ docker exec -it two ping one\nPING one (172.18.0.2): 56 data bytes\n64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.192 ms\n...\nCleanup:\n$ docker kill one two\n$ docker network rm mynet",
    "Use here document with ENTRYPOINT/CMD in Dockerfile": "That can't work because the shell parses the command line, including the heredoc before it executes it.\nBasically you can just do this:\n# Dockerfile\nFROM xyz\n# cat reads from stdin by default if no filename gets passed to it\nCMD 'cat'\nBuild and run the container:\ndocker build -t foo .\ndocker run -ti foo\nHelloDocker\nHelloDocker\nYou stop the input with Ctrl+d (That's literally EOF)\nPS: If you want to use a here-doc, run the above container like this:\ndocker run -i foo <<EOF\nHello Docker\nEOF\n\nHello Docker",
    "Accessing Volumes in docker": "You will write in Dockerfile how you build the container, not how container will interact with your host.\nFor mount a directory from your host in your container, you have 2 solutions :\nwith docker command line :\ndocker run -v $(pwd)/web-app:/var/lib/web-app/ dck-image-name\nwith docker-compose\nversion: '2'\nservices:\n  myservice:\n    image: dck-image-name\n    volumes:\n      - ./web-app:/var/lib/web-app/",
    "Docker-compose with golang-onbuild fails download behind proxy": "Environment arguments are used when running the container and not when building. In your case, building the image is failing. You need to use build arguments. The building arguments can be added to the compose file as such:\nversion: '2'\nservices:\n  web:\n    build:\n      context: .\n      args:\n        - HTTP_PROXY: http://proxy.mycompany.com:10080\n        - HTTPS_PROXY: http://proxy.mycompany.com:10080\n        - http_proxy: http://proxy.mycompany.com:10080\n        - https_proxy: http://proxy.mycompany.com:10080\n    ports:\n      - 5000:5000\n    volumes:\n      - \".:/redis-go-sample\"\n    depends_on:\n      - redis\n...   ",
    "Docker compose/swarm 3: docker file path , build, container name, links, migration": "docker stack deploy works only on images, not on builds.\nThis means that you will have to push your images to an image registry (created with the build process), later docker stack deploy will download the images and execute them.\nhere you have an example of how was it done for a php application. You have to pay attention to the parts 1, 3 and 4. The articles are about php, but can easily be applied to any other language.",
    "Remove $GNUPGHOME error during Dockerfile build": "Change it to below and it should work all the time\nexport GNUPGHOME=\"$(mktemp -d)\"; \\\n        gpg --keyserver pgp.mit.edu --recv-keys \"$GPG_KEY\" || \\\n        gpg --keyserver keyserver.pgp.com --recv-keys \"$GPG_KEY\" || \\\n        gpg --keyserver ha.pool.sks-keyservers.net --recv-keys \"$GPG_KEY\" ; \\\n    gpg --batch --verify rabbitmq-server.tar.xz.asc rabbitmq-server.tar.xz; \\\n    pkill -9 gpg-agent; \\\n    pkill -9 dirmngr; \\\n    rm -rf \"$GNUPGHOME\";\ngpg-agent and dirmngr run in background and at times takes time to exist. I believe rm picks up the files of these process and when it tries to delete the daemon and files area already gone. So adding these two pkill should remove the error",
    "Docker build failing when ARG instruction is given before FROM in dockerfile?": "You should refer to the Docker Release notes https://docs.docker.com/release-notes/docker-ce/\nThe feature you are using was introduce in 17.05.0-ce (2017-05-04)\nBuilder\nAdd multi-stage build support #31257 #32063\nAllow using build-time args (ARG) in FROM #31352\nYou need to upgrade",
    "How to provide external url to docker image": "With docker-compose you can provide environment variables like this:\nversion: \"3\"\nservices:\n  myapp:\n    image: your-image\n    environment:\n      - FOO=http://example.com\n      - BAR=456\nAnd depending on your application language, you can access to them. I.e for python:\nimport os\nfoo = os.environ.get(\"FOO\")\nbar = os.environ.get(\"BAR\")\nQ. if I want to put it as dynamic whats need to be done , - FOO=<dynamic value>\nYou can do something like this:\nversion: \"3\"\nservices:\n  myapp:\n    image: your-image\n    environment:\n      - FOO\n      - BAR\nUse as this:\n$ export FOO=http://example.com\n$ export BAR=456\n$ docker-compose up",
    "Dockerfile: How to replace a placeholder in environment variable with build-arg's?": "Variable expansion can only work in double-quoted strings. This is working:\nENV CONFIG \"{  \\\n  \\\"credentials\\\":{    \\\n      \\\"hostname\\\": \\\"172.17.0.5\\\",  \\\n      \\\"password\\\": \\\"PWD\\\", \\\n      \\\"port\\\": \\\"1234\\\", \\\n      \\\"username\\\": \\\"${USER}\\\"  \\\n     },  \\\n     \\\"name\\\":\\\"database\\\",  \\\n     \\\"tags\\\":[]  \\\n  }\"\nA simple example:\nFROM alpine\n\nENV USER foo\nENV CONFIG \"{  \\\n  \\\"credentials\\\":{    \\\n      \\\"hostname\\\": \\\"172.17.0.5\\\",  \\\n      \\\"password\\\": \\\"PWD\\\", \\\n      \\\"port\\\": \\\"1234\\\", \\\n      \\\"username\\\": \\\"${USER}\\\"  \\\n     },  \\\n     \\\"name\\\":\\\"database\\\",  \\\n     \\\"tags\\\":[]  \\\n  }\"\n\nENTRYPOINT env | sort\n_\n$ docker build -t test .\n$ docker run -it --rm test\nCONFIG={    \"credentials\":{          \"hostname\": \"172.17.0.5\",        \"password\": \"PWD\",       \"port\": \"1234\",       \"username\": \"foo\"       },       \"name\":\"database\",       \"tags\":[]    }\nHOME=/root\nHOSTNAME=43d29bd12bc5\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/\nSHLVL=1\nTERM=xterm\nUSER=foo",
    "Starting a barebones MVC C# ASP.NET project in Docker gives \"Compilation Error\"": "I had the same issue when trying to get an existing MVC app working in docker. I went all around the houses trying to resolve the issue and have spent many hours on it over the weekend! I tried setting permissions on various temp folders to various accounts and I tried setting the app pool to run with a local profile.\nI finally got it to work by adding the app pool account to the local admins group. This isn't a final solution, but it's got it working and pointed me in the right direction for which account I need to assign permissions for.\nHere is my Dockerfile;\nFROM microsoft/aspnet\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"]\nARG source\nWORKDIR /inetpub/wwwroot\nCOPY ${source:-obj/Docker/publish} .\n\nRUN Add-LocalGroupMember -Group 'Administrators' -Member 'IIS AppPool\\DefaultAppPool'; \\\nIISRESET; \nHope it helps.\nThanks\nJared",
    "/bin/sh: 1: gvm: not found": "Your shell is /bin/sh, but gvm puts its initialization in ~/.bashrc, and expects /bin/bash.\nYou need to source the gvm initialization scripts to run the commands from a non-interactive bash shell:\nRUN [\"/bin/bash\", \"-c\", \". /root/.gvm/scripts/gvm && gvm install go1.4 -B\"]\nRUN [\"/bin/bash\", \"-c\", \". /root/.gvm/scripts/gvm && gvm use go1.4\"]\nOr even better might be to put the commands you want to execute in a single bash script and add that to the image.\n#!/bin/bash\nset -e\n\nsource /root/.gvm/scripts/gvm\ngvm install go1.4\ngvm use go1.4",
    "Why won't my docker-cmd execute with sh?": "Given the fact that you are mapping the volume from Windows to Linux, it is very likely that your *.sh files have Windows style (CRLF) line endings. If you convert them to Unix/Linux style (LF), it should be fine to go.\nI am not sure about the right tools on Windows to check and fix the line endings but Notepad++ should be able to do it.",
    "Docker-compose: always receive Cannot locate specified Dockerfile": "The error is in your docker-compose.yaml file. In version 1 of that format, you can specify both build and dockerfile. If you provide both, the dockerfile attribute denotes the file name that is expected to be found in the directory specified by build.\nIn your case, change the dockerfile attribute to just include the filename instead of the same path as in build:\ngc:\n  container_name: docker-gc\n  build: ./docker/docker-gc\n  dockerfile: Dockerfile\n  volumes:\n    - /var/run/docker.sock:/var/run/docker.sock\n    - /etc:/etc\nSince Dockerfile is the default name, you can also simply remove this line:\ngc:\n  container_name: docker-gc\n  build: ./docker/docker-gc\n  volumes:\n    - /var/run/docker.sock:/var/run/docker.sock\n    - /etc:/etc\nPlease refer to the Docker Compose documentation for more info.\nPlease also note that version 2 of the Docker Compose file format uses a different notion of the build parameter.",
    "Connection refused by Docker container": "I solved the issue. I don't know why I had to specify the door on this line of docker-entrypoint.sh:\npython manage.py runserver 0.0.0.0:8000\nNow docker logs ockidocky_web_1 shows the usual django output messages.\nIf someone could give a proper explanation, I would be happy to edit and upvote.",
    "Docker compose - build order, database before the app": "I am not sure if it is option for you but you could build your custom script.\nLet's say that you would have two docker-compose files.\nOne for building your mysql database\nThe second one for your webapps\nFirst you could lunch mysql database docker-compose file with two mysql images\nOne will lunch your database\nThe second one will launch a script until the mysql db is ready.\nAfter both containers will finish starting you can start launching docker-compose with your webapps and you will be ensured that database is up and running.\nThis approach needs some custom script to process but can be very helpful.",
    "Docker compose volumes_from not updating after rebuild": "Compose preserves volumes when containers are recreated, which is probably why you are seeing the old files.\nGenerally it is not a good idea to use volumes for source code (or in this case static html files). Volumes are for data you want to persist, like data in a database. Source code changes with each version of the image, so doesn't really belong in a volume.\nInstead of using a data volume container for these files, you can use a builder container to compile them and a webserver service to host them. You'll need to add a COPY to the webserver Dockerfile to include the files.\nTo accomplish this you would change your docker-compose.yml to this:\nversion: \"2\"\nservices:\n  webserver:\n    image: myapp:latest\n    ports: [\"80:80\"]\nNow you just need to build myapp:latest. You could write a script which:\nbuilds the builder container\nruns the builder container\nbuilds the myapp container\nYou can also use a tool like dobi instead of writing a script (disclaimer: I am the author of this tool). There is an example of building a minimal docker image which is very similar to what you're trying to do.\nYour dobi.yaml might look something like this:\nimage=builder:\n  image: myapp-dev\n  context: ./templates\n\njob=templates:\n  use: builder\n\nimage=webserver:\n  image: myapp\n  tags: [latest]\n  context: .\n  depends: [templates]\n\ncompose=serve:\n  files: [docker-compose.yml]\n  depends: [webserver]\nNow if you run dobi serve it will do all the steps for you. Each step will only be run if files have changed.",
    "docker compose mount host directory on osx with xhyve": "I'm in the same boat. I have docker/docker-machine/docker-machine-driver-xhyve installed (via homebrew) and have this same issue.\nI think what's happening is, you're creating a volume on the host - which in this case is the docker-machine, and not your mac os host. I think there needs to be another jump... How that happens, I'm not sure.\nBut I can confirm, using the garden variety clean docker app takes care of this for you.",
    "Why node_modules is empty after docker build?": "You're not mounting the node_modules volume, it's just a straight data volume. That means the data is stored on your host, but it will be in a folder buried in the Docker engine's program store.\nIn the Compose file:\nvolumes:\n  - ./src:/var/www/html\n  - /var/www/html/node_modules\nThe first volume is mounted, so the container uses ./src on the host. The second volume isn't mounted, so it will be in /var/lib/docker on the host.\nIt's the same with docker -v - this does the equivalent of your Compose file, and the inspect shows you where the volumes are mounted on the host:\n> mkdir ~/host\n> docker run -d -v ~/host:/vol-mounted -v /vol-not-mounted busybox\n89783d441a74a3194ce9b0d57fa632408af0c5981474ec500fb109a766d05370\n> docker inspect --format '{{json .Mounts}}' 89\n[{\n    \"Source\": \"/home/scrapbook/host\",\n    \"Destination\": \"/vol-mounted\",\n    \"Mode\": \"\",\n    \"RW\": true,\n    \"Propagation\": \"rprivate\"\n}, {\n    \"Name\": \"55e2f1dd9e490f1e3ce53a859075a20462aa540cd72fac7b4fbe6cd26e337644\",\n    \"Source\": \"/var/lib/docker/volumes/55e2f1dd9e490f1e3ce53a859075a20462aa540cd72fac7b4fbe6cd26e337644/_data\",\n    \"Destination\": \"/vol-not-mounted\",\n    \"Driver\": \"local\",\n    \"Mode\": \"\",\n    \"RW\": true,\n    \"Propagation\": \"\"\n}]",
    "Can you change the CMD of the node layer - Docker": "Or you can create a new image from the base and just specify a new CMD:\nFROM <image-which-does-almost-exactly-what-i-want>\nCMD [\"my\", \"replacement\", \"command\"]\nWhen you build your image, you can run it directly without adding a new command at the end. When the base image changes, just rebuild yours and you'll get all the updates.",
    "Why I am getting an oci runtime error when running a simple docker image?": "The scratch image is literally \"empty\". There are no files provided by the base image, most importantly there is no shell (bash, sh, etc).\nWhen you tried to run the bash script it failed because there is no bash binary to run it. When you tried to run echo it failed because the echo binary does not exist in the image.\nIf you want to run a program from script you need to compile it as a static ELF binary. This is easy to do some some languages (go lang for example).\nIf you'd like to just run a bash script, you should try creating an image from alpine which is a minimal linux distribution:\nFROM alpine:3.4\nCMD echo \"hello\"\nOr\nFROM alpine:3.4\nRUN  apk add -U bash\nCOPY script.sh /usr/local/bin/run.sh\nCMD  [\"bash\", \"/usr/local/bin/run.sh\"]",
    "Does every line in a Dockerfile create a new layer?": "The layers shown here are filesystem layers.\nDockerfile directives like WORKDIR, VOLUME, EXPOSE, CMD do not change the FS.\nA docker history would show those commands, but not docker inspect.\nSee \"Show Layers of Docker Image\"",
    "How to set fixed ip address for container using docker-compose?": "You can use the container alias mysql to form the connection url from the java container\nDriverManager.getConnection(\"jdbc:mysql://mysql:3306/university\", ...",
    "Docker copy command to copy a jar file from target folder": "The Docker COPYcommand runs relative to the Dockerfile location.\nSo if you have a the Dockerfile at the root of your project; same as the target folder; then simpley you can use:\ndocker build . and use COPY target/myapp.jar /opt/my_app/lib\nIf on the other hand you have a project structure such:\n/docker/Dockerfile\n/src\n/target/myapp.jar\nThen you will need to move to the root of the project and run:\ndocker build -f docker/Dockerfile .\n(Ps you can also navigate to the docker dir and run docker build -f Dockerfile ..)\nOS: macOS Catalina 10.15.5\nDocker: 19.03.8",
    "Linking multiple docker containers to one container with alias localhost using docker-compose": "As other answers have already stated, you can't use the same alias twice. What you might do instead is to use net:webserver in the other services (in v2 this would be network_mode:webserver).\nThat way all the containers will share the same network stack, and localhost will work.",
    "Docker container does not start on executing docker run command": "a docker container will exit if the process on which it hangs exits. For example\nCMD ./myscript.sh\nwill exit when the script exits BUT\nCMD tail -f myfile\nwill not exit because the tail -f command does not exit\nIf you need to keep an exiting container alive then try\ndocker run -dti\nor\ndocker run -dt",
    "What's wrong with this dockerfile": "Default nginx behaviour is run as a daemon. To prevent this run nginx with parameter daemon off.\nCMD [\"nginx\", \"daemon off\"]",
    "Docker - /bin/sh: <file> not found - bad ELF interpreter - how to add 32bit lib support to a docker image": "UPDATE:\nSo the question becomes now:\nWhat would be the best way to achive the same result without repos or internet connection?\nYou could use various non-official 32-bit images available on DockerHub, search for debian32, ubuntu32, fedora32, etc.\nIf you can't trust them, you can build such an image by yourself, and you can find instruction on DockerHub too, e.g.:\non f69m/ubuntu32 home page, there is a link to GitHub repo used to generate images;\non hugodby/fedora32 home page, there is an example of commands used to build the image;\nand so on.\nAlternatively, you can prepare your own image based on some official image and add 32-bit packages to it.\nSay, you can use a Dockerfile like this:\nFROM debian:wheezy\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update\nRUN apt-get install -y ia32-libs\n...and use produced image as a base (with FROM directive) for images you're building without internet access.\nYou can even create an automated build on DockerHub that will rebuild your image automatically when your Dockerfile (posted, say, on GitHub) or mainline image (debian in the example above) changes.\nNo matter how did you obtain an image with 32-bit support (used existing non-official image or built your own), you can then store it to a tar archive using docker save command and then import using docker load command.",
    "How to select volume mountpoint in docker-compose.yml?": "You could create a volume on the host with a bind mount for both containers.\nExample:\nmkdir -p /mnt/shared-volume\ndocker run --name container1 -v /mnt/shared-volume:/path/a mycontainer\ndocker run --name container2 -v /mnt/shared-volume:/path/b mycontainer\nSame with docker-compose.yml:\nvolumes:\n - /mnt/shared-volume:/path/a\nAnd for the other container:\nvolumes:\n - /mnt/shared-volume:/path/b\nAlternative solution:\nCreate a data volume container!\nExample:\ndocker run --name datacontainer -v /mnt/shared-volume mycontainer /bin/true\ndocker run --name container1 --volumes-from datacontainer mycontainer\ndocker run --name container2 --volumes-from datacontainer mycontainer",
    "Unable to modify files in container from docker": "The actual problem was related to how the base image was built. If you run docker history --no-trunc vromero/activemq-artemis, you see these commands (among others):\n<id>   6 weeks ago         /bin/sh -c #(nop) VOLUME [/var/lib/artemis/etc]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0 B                                                                                                                           \n<id>   6 weeks ago         /bin/sh -c #(nop) VOLUME [/var/lib/artemis/tmp]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0 B                                                                                                                           \n<id>   6 weeks ago         /bin/sh -c #(nop) VOLUME [/var/lib/artemis/data]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0 B                                                                                                                           \nThe Dockerfile volume documentation states\nNote: If any build steps change the data within the volume after it has been declared, those changes will be discarded.\nThis means that the configuration in the base image is locked.\nI solved my problem by creating my own dockerfile based on the output of the history command, without the volume lines.",
    "Docker pull repositoryName don't work can't see public repos": "If this is a private repository (it doesn't exist from my view), you will need to login first.\nhttps://docs.docker.com/reference/commandline/login/\nUsage: docker login [OPTIONS] [SERVER]\nRegister or log in to a Docker registry server, if no server is specified \"https://index.docker.io/v1/\" is the default.\n-e, --email=\"\" Email\n-p, --password=\"\" Password\n-u, --username=\"\" Username",
    "connect to mysql server in docker": "I am not too familiar with mysql. Looking at the mysql documentation it seems like this might be the sort of command you want:\nimport _mysql\ndb=_mysql.connect(host=\"mysql\",port=3306,passwd=\"...\",db=\"...\")\nwhen you did the --link in your apps run, you declared that you are linking the EXPOSEd ports for the container named mysqlserver to the container to be named myapp. Docker updates the /etc/hosts file. So, if you cat the /etc/hosts file you should see a line that looks like:\n172.17.0.26 mysql 5c960433e26a mysqlserver\nso this is why the reference in your python to host 'mysql' works. The default port is 3306. So, that should connect you. Use this host and port for whatever mysql connection package you are using from python.\nThis is how I connect with other objects, on their well known ports. You can also use environment variables, but, I think this might be being phased out...dunno...\nroot@d23d8be2a6fa:/# env | grep MYSQL\nMYSQL_ENV_MYSQL_ROOT_PASSWORD=pass\nMYSQL_PORT_3306_TCP_PORT=3306\nMYSQL_PORT_3306_TCP=tcp://172.17.0.32:3306\nMYSQL_ENV_MYSQL_VERSION=5.5.42\nMYSQL_NAME=/myapp/mysql\nMYSQL_PORT_3306_TCP_PROTO=tcp\nMYSQL_PORT_3306_TCP_ADDR=172.17.0.32\nMYSQL_ENV_MYSQL_MAJOR=5.5\nMYSQL_PORT=tcp://172.17.0.32:3306\nso you could modify your connect line to use the environment:\nimport os\nmyhost = os.environ.get('MYSQL_PORT_3306_TCP_ADDR')\nmyport = int(os.environ.get('MYSQL_PORT_3306_TCP_PORT'))\nimport _mysql\ndb=_mysql.connect(host=myhost,port=myport,passwd=\"...\",db=\"...\")\nEither way. This is discovery. There are more complicated methods as well, using consul, etcd, skydns, registrator. I like the skydns/registrator pairing, registrator watches your docker daemon, when new containers are created it inserts dns records into etcd, which skydns can use to mimic a dns server. Using this technique you get ip and port as well.",
    "Mounted docker volume to host directory contains only files from the last container": "I believe you're hitting https://github.com/docker/fig/issues/447\nIf you added VOLUME to the Dockerfile at one point, you keep getting the contents of that volume when you recreate.\nYou should fig rm --force to clear out the old containers, after that it should start working and using the host volumes.",
    "Docker error: failed to solve: archive/tar: unknown file mode ?rwxr-xr-x": "One of the files that the docker copies to the container has its permissions messed up. My guess is that it's something in the node_modules folder with npm.\nIf you delete the whole node_modules folder and the package-lock.json you can recreate these again with npm install and npm run build after which they should have the correct permissions. Then the docker build command should work as expected.",
    "Debian Dockerfile - changing the locale to UTF-8": "Try this.\n...\n\nENV LC_ALL=en_US.UTF-8\nENV LANG=en_US\n\nRUN apt-get update && apt-get install -y locales\nRUN printf '%s\\n' LANG=en_US LC_ALL=en_US.UTF-8 >/etc/default/locale\nRUN echo en_US.UTF-8 UTF-8 >>/etc/locale.gen\nRUN locale-gen\nI started to write an answer involving dpkg-reconfigure locales but it turns out there are multiple bugs around this which remain unsolved since many years back.\nhttps://bugs.debian.org/cgi-bin/bugreport.cgi?bug=684134\nhttps://bugs.debian.org/cgi-bin/bugreport.cgi?bug=500164\n# FIXME: broken\nRUN printf '%s\\t%s\\t%s\\t%s\\n' \\\n    locales locales/locales_to_be_generated \\\n        multiselect \"en_US.UTF-8 UTF-8\" \\\n    locales locales/default_environment_locale \\\n        select \"en_US.UTF8\" | \\\n    debconf-set-selections && \\\n    DEBIAN_FRONTEND=noninteractive dpkg-reconfigure locales\nafter the installation of locales, obviously.",
    "Micromamba and Dockerfile error: /bin/bash: activate: No such file or directory": "Assuming these 2 files:\nenvironment.yml\nname: testenv\nchannels:\n  - conda-forge\ndependencies:\n  - python >= 3.9\n  - flask\napp.py\nfrom flask import Flask\n\n# Create a Flask application\napp = Flask(__name__)\n\n\n# Define a route for the root URL\n@app.route(\"/\")\ndef hello_world():\n    return \"Hello, World!\"\n\n\nif __name__ == \"__main__\":\n    # Run the Flask app on the local development server\n    app.run()\nThen in your Dockerfile you don't activate the environmnent but use the path to environment, e.g. /opt/conda/envs/testenv/bin/flask (note the testenv in the path and in the environment.yaml):\nDockerfile\nFROM mambaorg/micromamba:1-focal-cuda-11.7.1\n\nWORKDIR /app\n\nCOPY environment.yml .\n\nRUN micromamba env create --file environment.yml\n\nEXPOSE 5001\n\nCOPY app.py .\n\nCMD micromamba run -n testenv flask run --host=0.0.0.0 --port=5001\nTo build:\ndocker build . -f Dockerfile -t my/image:name\nTo run:\ndocker run --rm -i -t -p 5001:5001 my/image:name\nThen navigating the browser to localhost:5001 you should see Hello World.",
    "How to prevent attach shell in docker node alpine image?": "How can I do it?\nYou can't.\nYou can write a docker access authorization plugin that will disable docker exec.\nOften people remove everything and all executables except those needed, essentially preventing to execute anything. Still, docker cp will work. And then, docker save will still work and allows getting all files.",
    "Where do you usually store a Dockerfile within your sourcecode? [closed]": "There is a specific technical argument that the Dockerfile should be in the project root and named exactly Dockerfile, and I'd go with this approach as a default.\nWhen you build a Docker image, you send Docker a directory tree called the build context. You can't access any files outside the build context at all; depending on the version of Docker, COPY ../../../build.gradle ./ will either result in a specific error or attempt to copy it out of the context root directory instead. This means that the build context directory must be the project root (or a parent directory).\nYou also need to tell Docker where the Dockerfile is. Its default value is Dockerfile, with no extension, in the root directory of the build context. You can specify other paths or filenames, but if you use that default name then you can omit the option entirely.\n# In the project root\ndocker build .\n# Not in the project root\ndocker build . -f src/main/docker/Dockerfile\nDocker Compose has an even shorter syntax if you're using only default settings:\nservices:\n  in-the-project-root:\n    build: .\n  not-in-the-project-root:\n    build:\n      context: .\n      dockerfile: src/main/docker/Dockerfile\nThe other argument here is that the left-hand side of COPY is always relative to the build-context directory, not the Dockerfile location. In these last examples where the build context is several directories above the location of the Dockerfile, COPY ./build.gradle ./ will be the build context directory. Putting the Dockerfile somewhere else can be counterintuitive.\nAs I've shown, it's definitely possible to put the Dockerfile somewhere else or to name it something else. But you'll have the least friction with Docker tooling if it's exactly in the project root and if it's named exactly Dockerfile.",
    "React router Link not working in production": "This looks to be a known, reported issue with react-router v6.12.0/6.12.1.\n[Bug]: v6.12.1 Navigating and changing pages does not work. No errors in console #10579\nHey folks - it looks like throughout this thread there are two separate issues being discussed:\n6.12.1-only Production Mode Issue\n6.12.0 works correctly, but 6.12.1 doesn't perform navigations in production mode (works correctly in dev mode). This is due to a webpack/terser minification issue introduced in #10569 and resolved by #10588 and currently available in 6.12.2-pre.0. If this is what your are seeing in your application, please give 6.12.2-pre.0 a shot and add a \ud83d\udc4d reaction to this comment to let us know that the prerelease resolves your issue. If you still have issues with the prerelease, please respond with a comment and ideally a minimal reproduction.\nstartTransition-enabled Navigation Issues\n6.12.0 and 6.12.1 don't work correctly in your app when your navigations leverage Suspense internally, likely due to the recreation of net-new promises during renders. This is shown by the codesandbox in this comment and further here and the few comments following. If this is your issue we're still diving into this and hope to have an update for you shortly.\nApologies for the inconvenience and thanks for bearing with us!\nFor now it seems that rolling back to v6.12.0 or even back to v.6.11.x doesn't have this production build issue.\nThere appears to be a fix in the works for a 6.13 release.",
    "Modify Docker Image With Older Manifest Format": "I've been developing a project called regclient that supports modifying images with the regctl image mod command I've added the --to-docker flag that modifies an image like you're asking to do:\n$ regctl manifest get localhost:5000/library/ubuntu:docker\nName:        localhost:5000/library/ubuntu:docker\nMediaType:   application/vnd.oci.image.index.v1+json\nDigest:      sha256:2adf22367284330af9f832ffefb717c78239f6251d9d0f58de50b86229ed1427\n\nManifests:\n\n  Name:      localhost:5000/library/ubuntu:docker@sha256:b2175cd4cfdd5cdb1740b0e6ec6bbb4ea4892801c0ad5101a81f694152b6c559\n  Digest:    sha256:b2175cd4cfdd5cdb1740b0e6ec6bbb4ea4892801c0ad5101a81f694152b6c559\n  MediaType: application/vnd.oci.image.manifest.v1+json\n  Platform:  linux/amd64\n\n...\n\n$ regctl image mod localhost:5000/library/ubuntu:docker --to-docker --replace\nlocalhost:5000/library/ubuntu:docker\n\n$ regctl manifest get localhost:5000/library/ubuntu:docker\nName:        localhost:5000/library/ubuntu:docker\nMediaType:   application/vnd.docker.distribution.manifest.list.v2+json\nDigest:      sha256:b6d65b608a645e20715adc019ae0a021b7a37d2568ff7e372d29dfade962c30a\n\nManifests:\n\n  Name:      localhost:5000/library/ubuntu:docker@sha256:4933f152eb93fc356968d49da7c3cd42d68eda5e36cfb34ad7d06859c1f132b4\n  Digest:    sha256:4933f152eb93fc356968d49da7c3cd42d68eda5e36cfb34ad7d06859c1f132b4\n  MediaType: application/vnd.docker.distribution.manifest.v2+json\n  Platform:  linux/amd64\n\n...\nThe command:\nregctl image mod ${src} --to-docker --replace\nmodifies an image in place. You can also create a new tag like:\nregctl image mod ${src} --to-docker --create ${new_tag}\nInstallation instructions are documented in the project. However, since this is a new feature, it will be available in the next (v0.4.8) release, or you can run the edge images now, or download the binaries from the latest CI build.",
    "Cannot rm -rf directories copied via COPY --chown user:group in the docker image": "In your first Dockerfile, myuser owns /opt/app/build.\nIn your second Dockerfile, myuser owns /opt/app.\nTo be able to remove the directory, you need access to modify /opt/app.",
    "Reduce dimension of Docker Python Image": "In your pip command, add the --no-cache-dir flag to disable writing files to a cache:\nRUN pip install --no-cache-dir -r requirements.txt",
    "Building a node image: Error response from daemon: dockerfile parse error line 15: unknown instruction: CMD[\"NODE\",": "Change:\nCMD[\"node\", \"app.js\"]\nCMD[\"yarn\", \"start\"]\nin\nCMD [\"node\", \"app.js\"]\nCMD [\"yarn\", \"start\"]\nYou missed the space between CMD and [.",
    "Creating user with uid 1000 on alpine Docker image": "Your syntax is incorrect, the adduser command of Alpine is the BusyBox one, so, unlike the \"regular\" adduser, here are its help page:\nBusyBox v1.35.0 (2022-08-01 15:14:44 UTC) multi-call binary.\n\nUsage: adduser [OPTIONS] USER [GROUP]\n\nCreate new user, or add USER to GROUP\n\n        -h DIR          Home directory\n        -g GECOS        GECOS field\n        -s SHELL        Login shell\n        -G GRP          Group\n        -S              Create a system user\n        -D              Don't assign a password\n        -H              Don't create home directory\n        -u UID          User id\n        -k SKEL         Skeleton directory (/etc/skel)\nYou can easily go through it, running the command:\ndocker run -ti --rm alpine:3.16 adduser\nThe important part of information here are:\n-g GECOS        GECOS field\n-G GRP          Group\nWhere you can see that, a group, in BusyBox adduser requires your to be added with the option G, in capital letter, and that the option g in lowercase if for something else.\nThe option allows you to add a group, and not a GID, so you'll need the command:\nRUN adduser -G sail -u 1000 sail\nFurthermore, if you run that command, the shell will prompt you to fill in a password. You will need to skip this with the D option:\n-D              Don't assign a password\nAnd so, your Dockerfile ends up being:\nFROM alpine:3.16\n\nRUN addgroup -g 1000 sail \\\n    && adduser -G sail -u 1000 sail -D\nNote that it is always a good idea, in Docker, to reduce as much as possible the number of layers you are creating by running subsequent RUN instruction, for further reading on this, see here.",
    "Tilde Expansion Doesn't Work in Docker CMD Command": "Tilde expansion is done by the shell. When you use the exec form of CMD, there is no shell to do the expansion.\nYou can use the shell form of CMD instead, which runs the command through a shell, like this\nCMD tail -f ~/block.txt\nor you can use the exec form and run the shell explicitly, like this\nCMD [\"/bin/sh\", \"-c\", \"tail -f ~/block.txt\"]\nThe two commands do the exact same thing. If you use the shell form, it'll run the shell in the same way that the exec form does.\nMore info here: https://docs.docker.com/engine/reference/builder/#cmd",
    "Does Docker FROM Keyword in Dockerfile look for the newest image?": "Does docker check during the build process if my local version of imagename is still the latest (similar to docker pull)?\nNo, docker will not do this by default because of the build cache. It will use whatever existing image it has locally in the cache [1].\nYou can however enable the behavior you desire by using the --no-cache option:\n$ docker build --no-cache .",
    "Understanding this Docker file - Where are the files being copied": "In line WORKDIR /app, your current path will be set to /app. If the directory don't exist then it will be created beforehand.\nNext COPY go.* ./, this matches all of files start with go. will be copied to /app directory in the docker container. So your docker /app should look like this :\n/app\n| go.mod\n| go.sum\nAgain with COPY . ./, you are copying all files from current directory to /app directory of your docker container. It will replace already existing files in the contrainer. /app will look like this:\n/app\n| main.go\n| go.mod\n| go.sum\n| Dockerfile\nLast with RUN go build -v -o myapp, you are building the app using go and saving binary file myapp.",
    "How to pass dynamic build args via Compose file": "it seems that docker does not support running shell commands in a docker-compose.yml\nsee the Using shell command in docker-compose.yml github discussion\nyou could use a script to wrap your build command.\n# build.sh\n\ndocker-compose build --build-arg MY_VAR=$(command)",
    "econnrefused rabbitMQ between docker containers": "TL;DR\nThe depends_on guarantes the order in which the services will start up, but that doesn't guarante anything for the processes they inititate.\nIn these cases, you should expand the depends_on statement in order to take into account the health status of the process of interest\nFirstly, you should avoid making the communication of cointainers depend on their IP address but instead rely on their service names, since you are using docker compose.\nMeaning, instead of amqp://admin:pass@172.19.0.2:5672\nYou should use amqp://admin:pass@rabbitmq:5672\nMoving on to the core issue, your producerexpress relies on to rabbitmq in order to function.\nAs a result, you added the depends_on statement to producerexpress to resolve this. But this is not enough, quotting from https://docs.docker.com/compose/startup-order/\nYou can control the order of service startup and shutdown with the depends_on option. Compose always starts and stops containers in dependency order, .... However, for startup Compose does not wait until a container is \u201cready\u201d (whatever that means for your particular application) - only until it\u2019s running.\nAs a result, you need to add a health check in order to guarantee that the rabbitmq process has started successfully, not just the container.\nIn order to achieve that you should alter your compose file\nversion: '3'\nservices:\n  rabbitmq:\n    build: ./rabbitmq\n    container_name: 'rabbitmq'\n    environment:\n      - RABBITMQ_DEFAULT_USER=admin\n      - RABBITMQ_DEFAULT_PASS=pass\n    ports:\n      - 5672:5672\n      - 15672:15672\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:15672\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n\n  producerexpress:\n    build: ./service1\n    restart: on-failure\n    container_name: producerexpress\n    ports:\n      - 3000:3000\n    environment:\n      - PORT=3000\n    depends_on:\n      rabbitmq:\n        condition: service_healthy\nIn order to make the healthcheck, we need the curl package in the rabbitmq image, so add the following Dockerfile\nFROM rabbitmq:3-management-alpine\nRUN apk update\nRUN apk add curl\nEXPOSE 5672 15672\nFinally, to make this change compatible create the following directory structure\n./docker-compose.yml\n./rabbitmq/\n --- Dockerfile\n./service1/\n --- Dockerfile",
    "Using a Docker Image to Provide Static File Content": "A Docker container fundamentally runs a program; it's not just a collection of files. Probably the thing you want to do with these files is serve them over HTTP, and so you can combine a multi-stage build with a basic Nginx server to do this. A Dockerfile like this would be fairly routine:\nFROM node AS build\nWORKDIR /app\nCOPY package.json yarn.lock ./\nRUN yarn install\nCOPY ./ ./\nRUN yarn build\n\nFROM nginx\nCOPY --from=build /app/dist/ /usr/share/nginx/html/\nThis could be its own container, or it could be the same Nginx you have as a reverse proxy already.\nOther legitimate options include combining this multi-stage build with your Java application container, so you have a single server but the asset build pipeline is in Docker; or running this build sequence completely outside of Docker and uploading it to some sort of external service that can host your static files (Amazon S3 can work for this, if you're otherwise in an AWS environment).\nDocker volumes are not a good match for something you would build and deploy. Uploading content into a named volume can be tricky (more so if you eventually move this to Kubernetes) and it's more straightforward to version an image than a collection of files in a volume.",
    "How to run a .net core web api locally on Docker?": "Microsoft set the environment variable ASPNETCORE_URLS to http://+:80 in their base images, so your app is listening on port 80 when it runs in a container.\nYou should also be aware that Swagger normally isn't available in a container, because Swagger by default only is available when running in development environments. A container is not considered development by default.\nSo to run your container and have access to Swagger, you should run your container using a command that looks something like\ndocker run --rm -d -e ASPNETCORE_ENVIRONMENT=Development -p 5000:80 test\nYou should then be able to access your webapi on http://localhost:5000/ and have Swagger available.",
    "Docker compose specify container network as host network": "You need to specify the network mode inside your service at docker-compose.yml. Add it like this:\nversion: '3.8'\nservices:\n  network_mode: \"host\"\n  api-server:\n    build: '.'",
    "Git Clone using SSH in Dockerfile": "Add a RUN ssh -Tv git@bitbucket.services to check for any error message.\nI would suspect for instance a chmod issue\nRUN \\\n    chmod 700 $HOME/.ssh &&\\\n    chmod 600 $HOME/.ssh/id_rsa \nAs commented, ssh-keyscan (if you have installed it) can be needed, to complete the ssh/known_hosts file. For instance:\n# Authorize SSH Host\nRUN mkdir -p /root/.ssh && \\\n    chmod 0700 /root/.ssh && \\\n    ssh-keyscan example.com > /root/.ssh/known_hosts && \\\n    # Add the keys and set permissions\n    echo \"$PRIVATE_SSH_KEY\" > /root/.ssh/id_rsa && \\\n    chmod 600 /root/.ssh/id_rsa",
    "where is \u201cregistry-1.docker.io\u201d \uff1f": "You are showing an issue with kubernetes, but making your configuration on docker. While these both run containers, they have separate configurations. For configuring the mirroring, it depends on how you run your containers in kubernetes. The most popular runtime being containerd's CRI, which has documentation on configuring their registry settings:\n/etc/containerd/config.toml:\nversion = 2\n\n[plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors]\n  [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"]\n    endpoint = [\"https://registry-1.docker.io\"]\nWhich has been replaced with:\n/etc/containerd/config.toml:\n[plugins.\"io.containerd.grpc.v1.cri\".registry]\n   config_path = \"/etc/containerd/certs.d\"\nand\n/etc/containerd/certs.d/docker.io/hosts.toml:\nserver = \"https://docker.io\"\n\n[host.\"https://registry-1.docker.io\"]\n# ...\nAs for where registry-1.docker.io comes from, it's the DNS name for the Docker Hub registry, which is the default when you do not specify a registry in your image name.",
    "How to upgrade alpine docker base images for security patches": "I had a similar problem with volbrene/redoc that builds from nginx:alpine. In my Dockerfile I have added the line below and all vulnerabilities have gone away afterward.\nRUN apk update && apk upgrade",
    "How to have two workers for heroku.yml?": "The name worker isn't really important:\nNo process types besides web and release have special properties\nSo just give them different names:\nrun:\n  web: python manage.py runserver 0.0.0.0:$PORT\n  celery_worker:\n    command:\n      - celery --app=my_app worker --pool=prefork --concurrency=4 --statedb=celery/worker.state -l info\n    image: web\n  celery_beat:\n    command:\n      - celery --app=my_app beat -l info\n    image: web\nWhen you scale those processes, use the names celery_worker and celery_beat.",
    "Access $HOME in Dockerfile ENV instruction": "You should add the line ARG HOME after the FROM ... line and pass build option --build-arg or add build:args parameter in your docker-compose file.",
    "Docker Logging w/ Syslog container": "I think the error you are getting is due to not being able to reference another container in in the logging driver syslog-address (see similar issues here and here). Also I don't think you need to send the logs to syslog in the go app as well, it should be doing a regular logging to stdout.\nOne of the solutions could be to run a syslog and expose its syslog port and then point syslog-address to host.docker.internal.\nHere is my example (well, slightly modified your example), also here to clone.\nFolder structure:\n ~/Projects/docker/log-test \ue0b0 tree\n.\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 src\n\u2502       \u251c\u2500\u2500 go-app.go\n\u2502       \u2514\u2500\u2500 go.mod\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 syslog\n    \u251c\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 conf\n        \u2514\u2500\u2500 rsyslog.conf\napp/Dockerfile\nunmodified\napp/src/go-app.go\npackage main\n\nimport (\n  \"fmt\"\n  \"time\"\n)\n\nfunc main() {\n  for i := 0; i < 1000; i ++ {\n    fmt.Printf(\"Log Entry #%d\\n\", i)\n    time.Sleep(time.Second * 1)\n  }\n}\nsyslog/Dockerfile\nunmodified\nsyslog/conf/rsyslog.conf\nunmodified\ndocker-compose.yml\ncat docker-compose.yml\nversion: '3.8'\nservices:\n  application:\n    build:\n      context: ./app\n      dockerfile: Dockerfile\n    container_name: gopkg\n    dns: 1.1.1.1\n    logging:\n      driver: syslog\n      options:\n        syslog-address: \"udp://host.docker.internal:5514\"\n        tag: gopkg\n    depends_on:\n      - syslog-ng\n\n  syslog-ng:\n    build:\n      context: ./syslog\n      dockerfile: Dockerfile\n    container_name: syslog-ng\n    dns: 1.1.1.1\n    ports:\n    - \"5514:514/udp\"\nVerifying:\ndocker compose up -d\n[+] Running 3/3\n \u283f Network log-test_default  Created                                                                                                                                                                                                                                                                                   0.0s\n \u283f Container syslog-ng       Started                                                                                                                                                                                                                                                                                   0.5s\n \u283f Container gopkg           Started\ndocker compose exec syslog-ng tail /var/log/messages\n2021-11-19T05:35:56+00:00 192.168.0.1 gopkg[1452]: Log Entry #47\n2021-11-19T05:35:57+00:00 192.168.0.1 gopkg[1452]: Log Entry #48\n2021-11-19T05:35:58+00:00 192.168.0.1 gopkg[1452]: Log Entry #49\n2021-11-19T05:35:59+00:00 192.168.0.1 gopkg[1452]: Log Entry #50\n2021-11-19T05:36:00+00:00 192.168.0.1 gopkg[1452]: Log Entry #51\n2021-11-19T05:36:01+00:00 192.168.0.1 gopkg[1452]: Log Entry #52\n2021-11-19T05:36:02+00:00 192.168.0.1 gopkg[1452]: Log Entry #53\n2021-11-19T05:36:03+00:00 192.168.0.1 gopkg[1452]: Log Entry #54\n2021-11-19T05:36:04+00:00 192.168.0.1 gopkg[1452]: Log Entry #55\n2021-11-19T05:36:05+00:00 192.168.0.1 gopkg[1452]: Log Entry #56\nWhile this works, the easier solution would be to switch back to using syslog logging in the go code, and point the logger to that other container instead of doing it through the logging driver.",
    "ModuleNotFoundError from Docker container with Gunicorn": "The problem was in command:\ncommand: sh -c \"cd djangodocker/ &&\n                gunicorn djangodocker.wsgi:application --bind 0.0.0.0:8000\"\nI've solved using the instruction above.",
    "How to set node version to 16.x for ruby:3.0.1-alpine3.13 image in Dockerfile": "For installing Node.js 16.x on Alpine 3.14, install the nodejs-current package:\nhttps://pkgs.alpinelinux.org/package/v3.14/community/x86_64/nodejs-current\nSimply replace nodejs with nodejs-current in your package list.\nThe current nodejs-current version is 16.6.0-r0.",
    "Building .NET Framework 4.8 project in Docker fails with \"Reached end of stream before end of read\"": "Allocating more memory to the build solved this problem.\ndocker build -t projectName -m 4g .",
    "Docker Daemon is not running or Error response from docker daemon [closed]": "If you run docker info, you will see next:\n$ docker info\nClient:\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Build with BuildKit (Docker Inc., v0.6.1-docker)\n  compose: Docker Compose (Docker Inc., v2.0.0-rc.1)\n  scan: Docker Scan (Docker Inc., v0.8.0)\n\nServer:\nERROR: error during connect: This error may indicate that the docker daemon is not running.: Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/info\": open //./pipe/docker_engine: The system cannot find the file specified.\nerrors pretty printing info\nWhich means in fact you did not start docker daemon, it looks you are using windows, so please start docker-desktop first before execute any other docker command.\nDetail install and start method see Start Docker Desktop",
    "Docker build Error: executor failed running [/bin/sh -c npm run build]: exit code: 1": "I've faced a similar issue, but it was faced completing the Java guide in the Docker documentation. Originally, I was having this issue while working in the Intellij terminal and also from the Windows CLI. Even after completing the steps from the link provided at the end of this comment, the Intellij terminal would not do the job. I noticed two error codes; exit code: 1 & exit code: 127. Both referred to a \"no such file or directory\" error. I fixed the issue in the Visual Studio Code terminal by completing the step at the following link. Now my Docker image, builds using the docker build --tag java-docker . command, even in Intellij. Please reference the following link please click here",
    "docker-compose: failed to register layer: Error processing tar file - cannot find Windows DLL MUI file": "You can't run Windows containers (i.e. derived from some Windows base image like windowsservercore-1809) when Docker Desktop is set to Linux containers.",
    "failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open /private/tmp/AVScanB2wT: permission denied": "Short term fix sudo chmod -R 777 /private/tmp/AVScanB2wT\nLong term fix Disable buildx in docker desktop experimental features.\n\"features\": {\n    \"buildkit\": false\n}",
    "Why is emptydir not empty when mounting over dockerfile volume?": "You're starting from the Docker Hub wordpress image, which has its Docker image setup in GitHub.\nThe important detail there is that the Dockerfile ends with\nENTRYPOINT [\"docker-entrypoint.sh\"]\nCMD [\"php-fpm\"]\nThis is a standard pattern of using a shell script as a wrapper to do first-time setup, and giving it the actual command to run (Docker passes the CMD as arguments to the ENTRYPOINT). The Wordpress image docker-entrypoint.sh in turn has the fragment:\nif [ ! -e index.php ] && [ ! -e wp-includes/version.php ]; then\n  echo >&2 \"WordPress not found in $PWD - copying now...\"\n  ...\n  for contentPath in \\\n    /usr/src/wordpress/.htaccess \\\n    /usr/src/wordpress/wp-content/*/*/ \\\n  ; do\n    ...\n  done\n  tar cf - ... . | tar xf -\nfi\nThat fragment looks at the current directory; if it doesn't have an index.php file, it copies /usr/src/wordpress there. This runs when the container starts up, after any volumes have been mounted, and before your postStart hook triggers.\nYou could take advantage of this setup by copying your own content into the Wordpress base tree, instead of setting up the separate hook:\nFROM wordpress:5.7.2-fpm-alpine\nCOPY public/wordpress /usr/src/wordpress/blog",
    "Docker compose and dockerfile / .htaccess problem": "You have two options:\nDocker is a layered file system, You can use one image and make modifications to it to create another image, which you can then push to your private Docker Registry or public dockerhub. To, create a custom image with your .htaccess changes, you will create a file named \"Dockerfile\". You will then place the Dockerfile in the same directory where your modified .htaccess\nDockerfile content\nFROM php:7.3-apache\n\nRUN sed -i '/LoadModule rewrite_module/s/^#//g' /usr/local/apache2/conf/httpd.conf\nCreate Image:\ndocker build -t custom-php:7.3-apache .\nThis will create a new image name (custom-php:7.3-apache). You can then use this new image in your docker-compose.yml file and when deployed, the container will have the updated .htaccess\nYou may mount the .htaccess to the desired path by using volumes option as shown below\n.\nphp-httpd:\n    image: php:7.3-apache\n    ports:\n        - 80:80\n    volumes:\n        - \"./DocumentRoot:/var/www/html\"\n        - \"./.htaccess:/var/www/html/.htaccess\"\nI prefer #2 as you can edit .htaccess rules without rebuilding the image.",
    "How to copy files into outside working directory": "You can't COPY a file from outside the build context. So if you are trying to COPY /opt/venv/lib/python3.7/site-packages/xxx/resources/abc.py into your docker image, and that is not in your build context, it will fail. Full stop.\nHere's some annotated code.\n# change to the /app directory in the container\nWORKDIR /app\n\n# run the command cd in the container.  cd is a shell builtin, and after\n# this command finishes you will still be inside the /app directory in\n# your container.\nRUN cd ../opt/venv/lib/python3.7/site-packages/xxx/\n\n# Attempt to copy ./resources/abc.py from your host's build context\n# (NOT /opt/venv/lib/python3.7/site-packages/xxx/) into the container.\nCOPY ./resources/abc.py .\nThe basic fix for this is to first copy abc.py into your build directory. Then you will be able to copy it into your docker container during your build like so:\nWORKDIR /app\nCOPY abc.py .\n# /app/abc.py now exists in your container\nNote on cd\ncd is a shell builtin that changes the working directory of the shell. When you execute it inside a script (or in this case a docker RUN) it only changes the working directory for that process, which ends when the script ends. After which your working directory will be the one you started in. So you cannot use it in the way you were intending. Much better explanation here.\nTake this Dockerfile for example:\nFROM alpine:latest\n  \nRUN cd /opt      # cd to /opt\nRUN pwd          # check current directory, you're STILL in '/'\n\nRUN cd /opt && \\\n    pwd          # works as expected because you're still in the same process that cd ran in.\n                 # But once you exit this RUN command you will be back in '/'\n                 # Use WORKDIR to set the working directory in a dockerfile\nHere's the output of building that Dockerfile (removed noisy docker output):\n$ docker build --no-cache .\nSending build context to Docker daemon  3.584kB\nStep 1/4 : FROM alpine:latest\nStep 2/4 : RUN cd /opt\nStep 3/4 : RUN pwd\n/\nStep 4/4 : RUN cd /opt &&     pwd\n/opt",
    "Kubernetes - How to make simple Linux container Image without App Running permanently": "You just need to run some long/endless process for the container to be up. For example you can read a stream, that'll last forever unless you kill the pod/container:\ncommand: [\"/bin/bash\", \"-c\"]\nargs: [\"cat /dev/stdout\"]",
    "I'm getting `ERR_EMPTY_RESPONSE` in Docker Compose even though the two individual containers work when run separately": "By looking at the docker ps output I would guess that you have by accident switched ports for backend and frontend in configuration. Frontend has unmapped port 80 and backend has unmapped port 8080.\nTry this one:\nversion: \"3.9\"\nservices:\n  backend:\n    env_file:\n      - .env\n    build:\n      context: ./backend\n    container_name: fastapi-api\n    ports:\n      - 8080:8080\n  frontend:\n    build:\n      context: ./frontend\n    container_name: vue-ui\n    ports:\n      - 80:80\n    links:\n      - backend",
    "bad variable name in Dockerfile": "You currently don't have any separation between the export and mkdir commands in the RUN statement.\nYou probably want to concatenate the commands with &&. This ensures that the previous commands (only) runs if the prior command succeds. You may also use ; to separate commands, i.e.\nRUN export EOSIO_LOCATION=~/eosio/eos && \\\n    export EOSIO_INSTALL_LOCATION=$EOSIO_LOCATION/../install && \\\n    mkdir -p $EOSIO_INSTALL_LOCATION\nNOTE You probably don't need to export these variables and could:\nEOSIO_LOCATION=... && EOSIO_INSTALL_LOCATION=... && mkdir ...\nThere's a Dockerfile ENV command that may be preferable:\nENV EOSIO_LOCATION=${PWD}/eosio/eos\nENV EOSIO_INSTALL_LOCATION=${EOSIO_LOCATION}/../install && \\\nRUN mkdir -p ${EOSIO_INSTALL_LOCATION}\nPersonal preference is to wrap env vars in ${...} and to use ${PWD} instead of ~ as it feels more explicit.",
    "DOCKERFILE NPM Command to include colon and hyphens": "Please use it in the following way\nCMD [\"npm\", \"run\", \"start:stage-ena-sso\"]",
    "How can I update Docker image after changing few lines of code in my app?": "Although you asked specifically how to update (rebuild) your docker image, it is my guess that you are in fact in need of a different solution.\nIf you are developing on a dockerized version of your application (which is good), it is impractical to rebuild the image with every change you do in your code.\nA better, and more common approach, is to mount your local folder into the container, so the running container and your local machine actually share a folder.\nThis way you can just edit your code, and it is reflected in the container immediately.\nSo, your docker run command might look something like this:\n$ docker run -v $PWD:/path/to/app/in/container -p PORT:PORT IMAGE_NAME\nRead more about docker volumes.\nRead more about docker for development environments.\nRead about using docker-compose for development.",
    "dotnet test Coverlet wrong coverage": "Ran into the same issue today. Apparently, this happens if you have a method signature spanning over multiple lines. As someone pointed out in the comments section, updating the coverlet.msbuild nuget package to >=3.0.3 resolves the issue.\nLink to the GitHub issue - https://github.com/coverlet-coverage/coverlet/issues/1037",
    "Replace curl with wget for compressed data": "wget has the --compression=type option:\n--compression=type\n       Choose the type of compression to be used.  Legal values are auto, gzip and none.\nFor both curl and wget compression option adds Accept-Encoding header to a request. curl adds Accept-Encoding: deflate, gzip, br and wget adds Accept-Encoding: gzip.",
    "Docker volume not being mounted in container": "Most Compose settings aren't visible during an image build. volumes: aren't mounted, environment: variables aren't set, networks: aren't accessible. Only the settings within the immediate build: block have an effect.\nThat means you should look at the Dockerfile in isolation, ignoring the docker-compose.yml file. At that point, the /cfs directory is empty (you don't COPY any source code into it), so the RUN make ... commands will fail. It doesn't matter that the directory will eventually have something else mounted over it.\nIf you're just planning to recompile the application when the source code changes, delete the volumes:, COPY the source into the image, and run docker-compose build at the point you'd typically run make. If you do have a setup that can rebuild the application when its source changes, you need to set the image's CMD to launch that, but if you don't COPY the source in, you can't build it at image build time. (...and if you're going to overwrite the entire interesting content of the image with a volume, it will get lost anyways.)",
    "How to run both Mysql and Apache from my Dockerfile?": "Keep in mind that the RUN commands are executed when a container image is built, while CMD runs when a container is started - and you can only have one command. That is why you are only seeing either MySQL or Apache running.\nHave you looked into Docker Compose? This would allow you to run your database in one container and your Apache server in another, and start both easily with docker-compose up.\nIf you do want to put everything in a single container, you could do that still though; you might consider a single Bash script to start both. You would add that Bash script to your Docker build process, and then setup CMD to run that script, instead of starting just MySQL or just Apache. The Docker documentation has some examples here for how to setup a single container with multiple services.\nI personally think what you're trying to do would best be accomplished with Docker Compose, so that you are separating your concerns, but I don't know the details of your application.",
    "Nginx can't find upstream host in multi-container docker compose setup and also host can't be found on client:3000": "I had this exactly same issue today. I solved it by attaching all containers that nginx upstream is referring to in the same docker virtual network.\nAlso, make sure to explicitly define the containers name. If I am not wrong, docker-compose prefix your services name with the yaml file name.\n  service_one:\n    networks:\n     - app-network # define the network\n      container_name: backend # the \"backend\" name must be used in the upstream directive in nginx configuration",
    "Dockerfile - Build docker image for multiple Node.js app": "You would almost always run these in separate containers. You're allowed to run multiple containers from the same image, you can override the default command for an image when you start it up, and you can remap the ports an application uses when you start it.\nIn your Dockerfile, delete the RUN /script.sh line at the end. (That will try to start the servers during the image build, which you don't want.) Now you can build and run containers:\ndocker build -t myapp .      # build the image\ndocker network create mynet  # create a Docker network\ndocker run \\                 # run the first container...\n  -d \\                       #   in the background\n  --net mynet \\              #   on that network\n  --name app1 \\              #   with a known name\n  -p 1001:3000 \\             #   publishing a port\n  myapp \\                    #   from this image\n  node ./app1.js             #   running this command\ndocker run \\\n  -d \\\n  --net mynet \\\n  --name app2 \\\n  -p 1002:3000 \\\n  myapp \\\n  node ./app2.js\n(I've assumed all of the scripts listen on the default Express port 3000, which is the second port number in the -p options.)\nDocker Compose is a useful tool for running multiple containers together and can replicate this functionality. A docker-compose.yml file matching this setup would look like:\nversion: '3.8'\nservices:\n  app1:\n    build: .\n    ports:\n      - 1001:3000\n    command: node ./app1.js\n  app2:\n    build: .\n    ports:\n      - 1002:3000\n    command: node ./app2.js\nCompose will create a Docker network on its own, and take responsibility for naming the images and containers. docker-compose up will start all of the services in parallel.",
    "Docker Multistage Build Fails on copy from previous stage": "This will do the job:\nFROM maven:3.6.3-openjdk-8 as buildprobe\nRUN git clone https://github.com/psi-probe/psi-probe && cd psi-probe\nWORKDIR /psi-probe\nRUN mvn package && ls -l /psi-probe && ls -l /psi-probe/psi-probe-web/target\n\nFROM tomcat:9.0.21-jdk8-openjdk\nRUN rm -rf /usr/local/tomcat/webapps/*\nCOPY --from=buildprobe /psi-probe/psi-probe-web/target/probe.war /usr/local/tomcat/webapps\nYou were missing an r",
    "Docker file and compose update": "It would be best if you used docker-compose up --build centos-php-apache instead of docker-compose up . And you do not want to worry about data. They are persistent with your volume map. Also, make sure to use another volume map for your mariadb data.",
    "Nginx reverse-proxy for python app using docker-compose": "First thing you need to create bridge network. lets say the network name is my_project_network.\ndocker network create --driver=bridge my_project_network\nNow we will create python app.\nindex.py\nfrom flask import Flask\n\napp = Flask(__name__)\n\n# to connect redis \n# use host \"redis_app\"\n# use port \"6379\"\n# use password \"add_password_here\"\n\n@app.route('/')\ndef hello():\n    return 'Hello NGINX reverse proxy'\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\nWe will use flask so we need to install it. we can do it by create requirements.txt file and use pip3 install -r requirements.txt to install it.\nrequirements.txt\nflask==1.1.2\nNow will create Dockerfile for python app.\nDockerfile\nFROM python:3\n\nRUN mkdir -p /my_project\n\nWORKDIR /my_project\n\nCOPY requirements.txt ./\n\nRUN pip3 install -r requirements.txt\n\nCOPY . .\n\nCMD [ \"python3\", \"./index.py\"]\nNow will create nginx configuration file.\ndefault.conf\nserver {\n    listen 80;\n    server_name localhost;\n    location / {\n        proxy_pass http://flask_app:5000/;\n        proxy_set_header Host \"localhost\";\n    }\n}\nNow will create docker-compose file.\ndocker-compose.yml\nversion: \"3.7\"\n\nservices:\n  nginx_app:\n    image: nginx:latest\n    depends_on:\n      - flask_app\n    volumes:\n      - ./default.conf:/etc/nginx/conf.d/default.conf\n    ports:\n      - 8080:80\n    networks:\n      - my_project_network\n\n  flask_app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    expose:\n      - 5000\n    depends_on:\n      - redis_app\n    networks:\n      - my_project_network\n\n  redis_app:\n    image: redis:latest\n    command: redis-server --requirepass add_password_here\n    expose:\n      - 6379\n    networks:\n      - my_project_network\n\nnetworks:\n  my_project_network:\n    external: true\nNow we done all files. so we will run docker compose.\ndocker-compose up --build -d\nTo stop it just use\ndocker-compose stop\nResults\nNOTE: You can replace port 8080 with any port you need.\nNOTE: I created my_project_network to can connect service in docker compose each other. but I can do it with out my_project_network. but I prefer to use my_project_network to connect any service from anther docker compose to this docker compose services.\nLets say we have. ./anther_project/docker-compose.yml\nversion: \"3.7\"\n\nservices:\n  flask_app_v2:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - 5001:5001\n    networks:\n      - my_project_network\n\nnetworks:\n  my_project_network:\n    external: true\nThe flask_app_v2 can connect to redis_app because they are all in the same network .",
    "Connection to localhost:5432 refused after docker run when port 5432 is clearly open and listening?": "It is because if you are using docker compose to expose a service port, you need to connect to the service name instead of localhost\nhttps://docs.docker.com/compose/networking/\nIf the name of your service is postgres just point your app to postgres:5432",
    "How to stop docker container automatically?": "The closest solution that fits your problem supported by Dockerfile would be HEALTHCHECK directive e.g. HEALTHCHECK [OPTIONS] CMD command . Here you can specify interval (e.g. 1 hour) and time out.\n--interval=DURATION (default: 30s)\n--timeout=DURATION (default: 30s)\n--start-period=DURATION (default: 0s)\n--retries=N (default: 3)\nOther than that you would have to create custom shell script that is triggered by cronjob every 1 hour. In this script you would stop foreground process and by that stooping the running container.",
    "How to change default user 'flink' on 'root' in docker container?": "I found an answer - you need to replace docker-entrypoint.sh with your own file by adding volume from your host-machine and correct lines in it from \"gosu flink... / su-exec flink...\" to \"gosu root .../ su-exec root...\"",
    "Typescript Nodejs Dockerfile: error TS5057: Cannot find a tsconfig.json file at the specified directory: '.'": "You are not copying all the files to the container. You are only copying what is in the dist folder with COPY ./dist .\nMy question to you is, is this for development or for production? If its for development you can just copy the while root directory of the project with\nCOPY . .\nNow if this is for a production build you definitely want an intermediate build step so you only have to copy the compiled JS code to the container and run that.",
    "Dockerfile symbolic link with wildcard (*) not working": "When * does not match any files, it is passed as is to your command:\n$ ls -l\ntotal 0\n-rw-r--r-- 1 leodag leodag 0 jun 17 03:29 a\n-rw-r--r-- 1 leodag leodag 0 jun 17 03:29 b\n-rw-r--r-- 1 leodag leodag 0 jun 17 03:29 c\n\n$ ls d*\nls: cannot access 'd*': No such file or directory\nSo your problem is that there is nothing inside /app/frontend/web/, since you still haven't copied the files - you're trying to run it right after the FROM. First you need to copy in your files, so that the glob matches your files. Or else the glob expressions are passed literally to ln, creating a file called *, pointing to a non-existant * file.\nFROM php:7.3-apache\n\nCOPY myfrontend/ /app/frontend/\n\nRUN [\"/bin/bash\", \"-c\", \"ln -s /app/frontend/web/* /var/www/html\"]",
    "Why can't I run pdo_pgsql in my container?": "If you change Dockerfile and you want to update the image with docker-compose up, then you need to pass the flag --build with docker-compose up.\n--build Build images before starting containers.\ndocker-compose up --build\nRef:- https://docs.docker.com/compose/reference/up/",
    "Docker-compose: nginx fails to start due to \"permission denied\" error": "I got it working!\nWhat I did to fix it\nI removed the chown command altogether. I think it was throwing a permission denied error because the nginx user did not exist in the container at compile time, making the chown command invalid (when changing file ownership, that user/group needs to exist beforehand). Not sure why that line was needed (I got it from a blog post somewhere)...\nI realize I'm copying the build directory as root user, which is not ideal. Please let me know if there's a better way to do this.\nRun docker-compose up --build. The app should be accessible in localhost:8080 from the host.\nHope this helps someone.\nFinal dockerfile\n# Stage 1: Build Gatsby\nFROM node:alpine AS build\n\nRUN \\\n  apk add --no-cache python make g++ && \\\n  apk add vips-dev fftw-dev --update-cache \\\n  --repository http://dl-3.alpinelinux.org/alpine/edge/community \\\n  --repository http://dl-3.alpinelinux.org/alpine/edge/main \\\n  && rm -fR /var/cache/apk/*\n\nRUN npm install -g gatsby-cli\n\nWORKDIR /app\nCOPY ./package.json .\nRUN yarn install && yarn cache clean\n# RUN yarn cache clean && yarn install\nCOPY . .\nCMD [\"yarn\", \"build\", \"-H\", \"0.0.0.0\" ]\n\n# Stage 2: Serve the site\nFROM nginx:mainline-alpine\nRUN rm /usr/share/nginx/html/*\nCOPY --from=build /app/public /usr/share/nginx/html/\nRUN ls usr/share/nginx/html -l\ndocker-compose.yml\nversion: '3'\n\nservices:\n  web:\n    build: '.'\n    depends_on:\n      - 'database'\n    image: 'webserver-test'\n    container_name: 'nginx-webserver'\n    # restart: 'unless-stopped'\n    ports:\n      - '8080:80'\n    volumes:\n      - /app/node_modules\n      - .:/app\n    environment:\n      - NODE_ENV=production\n      - GATSBY_WEBPACK_PUBLICPATH=/",
    "Getting Container IP in a Go application [duplicate]": "You can get the hostname using os.hostname() function. Below golang code can give you some idea\npackage main\n\nimport (\n    \"fmt\"\n    \"os\"\n)\n\nfunc main() {\n    containerHostname, err := os.Hostname()\n}\ncontainerHostname variable will have the hostname of the container.",
    "Standard_init_linux.go:211: exec user process caused \u201cexec format error\u201d": "To use dos2unix fix your files' format, which maybe changed by git from lf to crlf\ngo build before copy to docker, then, copy and execute the bin\nif you don't want to do pre-built, the last line should be ENTRYPOINT [\"go\",\"run\",\"./server.go\"]",
    "How to launch Docker Containers using Ansible": "This has nothing to do with Ansible.\nYour Docker image is based on debian:8.5, which when run simply starts a shell. If the shell isn't connected to a terminal, it will exit immediately. E.g., try running:\ndocker run debian:8.5\nIf you expect your image to run a persistent service (like nginx), you need to arrange for nginx to run when you start a container from your image. You do this suing the CMD or ENTRYPOINT directives in your Dockerfile.\nFor example:\nFROM debian:8.5\nRUN apt-get update\nRUN apt-get install -y nginx\nRUN echo 'Default page. Nginx is in your container. ' \\\n>/usr/share/nginx/html/index.html\nEXPOSE 8085\nCMD [\"nginx\", \"-g\", \"daemon off\"]",
    "How to optimize pip imports for Dockerfile layers caching": "You're going to want pip install --no-cache-dir, so it doesn't keep copies of the downloads around.\nYou don't want to keep the toolchain (compiler etc.) installed, but you need them to build the image. So what you do is, you use multi-stage builds: you use one image to build everything, and then a second image that just copies the built packages over and omits all the build tools and artifacts. You can find a guide to multi-stage builds for Python here (it's three parts, this is part 1): https://pythonspeed.com/articles/smaller-python-docker-images/",
    "Docker compose cant find package.json in path": "Relative pathing. Note that the working COPY command is COPY . .. The . there means 'the current directory'. So what you need is:\nCOPY ./package*.json ./\nYou would simplify your debugging by running docker build -t scratch . to eliminate docker-compose from the equation. Always drive it down to absolute minimal reproducer to isolate the exact issue.",
    "Cant connect to on premise SQL Server from ASP.NET Core 3.1 in Docker": "It seems the problem is with SSL and Docker image. Changing to a bionic image solves the problem. More about this https://github.com/dotnet/SqlClient/issues/222\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.0-bionic AS base\nWORKDIR /app\nEXPOSE 80\n\nFROM mcr.microsoft.com/dotnet/core/sdk:3.0-bionic AS build",
    "Why does the same RUN command in a Dockerfile lead to different layer sizes across images?": "The chmod +x is the reason. Every time you change a file, even just chmod or chown, a whole new duplicate copy of the file is stored in the next layer.\nYou can use multi-stage builds to create a final image that doesn't have all the intermediary layers, e.g. if you're using Python here's a guide to it: https://pythonspeed.com/articles/smaller-python-docker-images/\nAnd here's the generic Docker docs: https://docs.docker.com/develop/develop-images/multistage-build/",
    "Issue installing MongoDB on Alpine image": "I got it from this answer . It adds some repositories to satisfy the requirements and then try to install. Try the following dockerfile\nFROM alpine:3.11.2\n\nRUN echo 'http://dl-cdn.alpinelinux.org/alpine/v3.6/main' >> /etc/apk/repositories\nRUN echo 'http://dl-cdn.alpinelinux.org/alpine/v3.6/community' >> /etc/apk/repositories\nRUN apk update &&  apk --no-cache --update add openjdk11 && apk --no-cache --update add mongodb\n\nWORKDIR /var/tmp/\n\nCOPY JDBCmusic.jar .\n\nLABEL belongsTo = \"Bohdan Milenko\"\n\nCMD [\"java\", \"-jar\", \"JDBCmusic.jar\"]",
    "Heroku: docker entrypoint not found": "You're using RUN instead of ENTRYPOINT.\nRUN means \"run this command as part of build\".\nENTRYPOINT means \"set this command as what happens when you do docker run.\"",
    "pip install in Dockerfile is failing [closed]": "edit your dockerfile:\nFROM python:alpine3.7\nRUN apk update && apk add --no-cache gcc g++ python3-dev unixodbc-dev\nCOPY . /app\nWORKDIR /app\nRUN pip install --upgrade pip\nRUN pip install -r requirements.txt\nCMD python ./index.py\nedit your requirements.txt\nflask \nSQLAlchemy\npyodbc\npandas\nnumpy",
    "Refer to filename containing newline in Dockerfile": "The following seems to work:\nADD [\"\\n.txt\", \"\\n.txt\"]\nLooks like \\r, \\b, \\n work, but \\x** or \\e do not. I have Docker version 19.03.5-ce, build 633a0ea838. I can't find any reference on this, so if someone finds it and sees this answer, feel free to edit it.",
    "Extract chisel in alpine docker": "you can use this:\nFROM alpine\nRUN apk update && apk add --no-cache wget\nRUN cd  /usr/local/bin && wget  https://github.com/jpillora/chisel/releases/download/1.3.1/chisel_linux_amd64.gz && gzip -d chisel_linux_amd64.gz\nRUN mv /usr/local/bin/chisel_linux_amd64 /usr/local/bin/chisel && chmod +x /usr/local/bin/chisel\nRUN chisel -v\nresult:\nls /usr/local/bin/\nchisel",
    "docker compose command down and up does not create fresh image and container": "By default, the only things removed by down are:\nContainers for services defined in the Compose file\nNetworks defined in the networks section of the Compose file\nThe default network, if one is used\nso the images ar not removed , you could use this along with down:\nOptions:\n--rmi type              Remove images. Type must be one of:\n                          'all': Remove all images used by any service.\n                          'local': Remove only images that don't have a\n                          custom tag set by the `image` field.\nsee this",
    "How to use export command to set environment variable with docker exec?": "This is impossible. A process can never change the environment of any other process beyond itself, except that it can specify the initial environment of processes it starts itself. In this case, your docker exec shell isn\u2019t launching the main container process, so it can\u2019t change that process\u2019s environment variables.\nThis is one of a number of changes that you will need to stop, delete, and recreate the container for. You should treat this as extremely routine container maintenance and plan to delete the container eventually. That means, for example, keeping any data that needs to be persisted outside the container, ideally in an external database but possibly in a mounted volume.",
    "Entrypoint's exec form executed through shell": "The exec syntax requires JSON format, which means that \\ instances in the array elements must be escaped as \\\\.\nSince your ENTRYPOINT instruction therefore does not contain a valid JSON array, it looks like Docker is falling back to shell syntax and therefore passes your ENTRYPOINT argument to the shell, which in your case is PowerShell, as defined in your SHELL instruction - and that results in a broken shell command.[1]\nA syntactically correct ENTRYPOINT in exec format - i.e., valid JSON - prevents involvement of a shell, which is the correct approach in this case, given that your command contains literal elements only.\nTherefore, try the following (\\ instances escaped as \\\\):\nENTRYPOINT [\"c:\\\\spinner.exe\", \"service\", \"w3svc\", \"-t\", \"c:\\\\iislog\\\\W3SVC\\\\u_extend1.log\"]\nThat way, Docker should end up executing the following command line in the container:\nc:\\spinner.exe service w3svc -t c:\\iislog\\W3SVC\\u_extend1.log\n[1] What happens is that the malformed ENTRYPOINT argument, [\"c:\\spinner.exe\", \"service\", \"w3svc\", \"-t\", \"c:\\iislog\\W3SVC\\u_extend1.log\"], is taken to be a single argument - a verbatim string to be escaped based on JSON rules (doubling of \\ chars.) - to be passed to the shell, as defined by your SHELL instruction. This means appending that argument to the last argument of your SHELL array, preceded by a space character. What you saw in the logs is evidence of that.",
    "\"ECONNREFUSED\" error after containering Node.js (Express)": "The Problem\nFrom the container's point of view, 127.0.0.1 is the ip address of itself -- not the host OS. That means you shouldn't set 127.0.0.1:8080 as a HTTP_PROXY and HTTPS_PROXY because your container would be calling itself so it won't reach the Internet. This is why your npm install isn't working.\nSimilarly, your main application behind the node.js proxy should not be using\n...\nENV HTTP_PROXY \"http://127.0.0.1:8446\"\nENV HTTPS_PROXY \"https://127.0.0.1:8446\"\n...\nbecause that would be calling itself at port 8446 and not the 8446 of the host OS (which you intended to route to the other container running the node.js proxy, but this will never work).\nThe Solution\nYou have to use something like docker compose or docker swarm to link the two containers' network. Refer to the following example docker-compose.yml:\nversion: \"3.7\"\n\nservices:\n  proxy:\n    image: myproxy\n    port:\n      - 8080:8080\n\n  app:\n    image: myapp\nAlso, remove the following lines from your proxy dockerfile and rebuild the image.\nENV HTTP_PROXY \"http://127.0.0.1:8080\"\nENV HTTPS_PROXY \"https://127.0.0.1:8080\"\nSimilarly, change the main application dockerfile from this\nENV HTTP_PROXY \"http://127.0.0.1:8446\"\nENV HTTPS_PROXY \"https://127.0.0.1:8446\"\nto\nENV HTTP_PROXY \"http://proxy:8446\"\nENV HTTPS_PROXY \"https://proxy:8446\"\nNow run docker-compose up with this configuration, and your main app will be able to reach proxy container by the host name proxy rather than 127.0.0.1. Which means that you will use proxy:8080 to use the proxy running on port 8080.\nPS: You are able to route to the docker containers/services via their service name because docker has a service discovery mechanism it maintains internally and it will resolve the ip address of these conatiners dynamically. This is essential to containers because containers can be destroyed and recreated at any time which means that IP addresses can change at any time. In order to resolve this, docker maintains a key-value store which maps service/host names to the IP addresses of these containers and resolve them for containers that are trying to reach other containers. Make sure to change all the IP addresses within your app to use host/service names instead of static IP addresses if they should route to other docker containers/services.",
    "Postgres ENOTFOUND error when connecting docker services to a docker database container": "So I eventually found the answer, so I thought I'd share:\nThe reason Docker couldn't automatically connect to Postgres was because Postgres was still initializing (and not receiving connection requests while it booted up).\nOne solution I found was to artificially delay Docker's connection attempt (basically wait until Postgres is ready before attempting to connect).\nEDIT: Docker-compose version 3 now has docs on how to control which containers start first: https://docs.docker.com/compose/startup-order/",
    "NuGet package restore extremely slow when creating new ASP.NET Core MVC app in Docker": "It was noticeable that dotnet uses CPU ressources continuily and another project that refers multiple NuGet ressources got restored after 10 seconds. So it's not a network issue and it indicates that NuGet is doing something.\nI found this question where a large project folder seems slowing down NuGet because it scans the entire folder. This seems applying to me too, since my quick test was done in the root file system of the container. So the fix was to simply create a sub-directory for the project:\nFROM mcr.microsoft.com/dotnet/core/sdk:2.1 AS sdk-image\nRUN mkdir test\nWORKDIR /test\nRUN dotnet new mvc\nNow NuGet is finished with package restoring after a few seconds. The lesson learned is: Always use a clean sub directory, even when it's a quick & dirty test container where it (theoretically) shouldnt matter...",
    "How to verify build args is set properly in Dockerfile": "It will not substitute the variable, as Docker treat ENTRYPOINT and CMD as a command so processing variables like shell will not work. Try to change the CMD to run it as a shell and then you will able to process variable the same as the shell.\nAlso, you can not use ARG in CMD to treat them as an environment variable, you just use them inside Dockerfile, to use them as an environment variable you must assign them to some ENV.\nARG COOL_ID\nENV COOL_ID=$COOL_ID\nI will also suggest to verify and check COOL_ID in Docker build time, if not set then it should print the warning or error to the user, see below example if ARG not passed to builds params then it will print an error message to the user.\nARG COOL_ID\n#see ARG is for build time \nRUN if [  -z $COOL_ID ];then \\\n  >&2 echo  \"\\n****************Warning!!!!*************\\n\"; \\\n  >&2 echo \"COOL_ID seems empty!\" ;\\\n  fi\n\nENV COOL_ID=$COOL_ID\n# ENV is for run time\nCMD [\"sh\", \"-c\", \"java -jar /usr/share/java/${COOL_ID}/app.jar\"]\nNow build the docker with --build-arg\ndocker build --build-arg COOL_ID=myid -t myjava .\nIf you missed passing COOL_ID you will be notified.",
    "standard_init_linux.go:211: exec user process caused \"no such file or directory\" [duplicate]": "In my case, this issue was because I had Windows-style line endings on my docker file script for Linux.\nI fixed this by running dos2unix on the file so that it now had Unix-style line endings.",
    "Unable to open a docker app running on a port": "Listening on 127.0.0.1:8080 means that the sever is literally listening on the loopback interface of the docker container. I.e. it will not be accessible outside of the container.\nTo make it work, you need to configure filebrowser to run on *:8080.\nUPDATE\ndocker run -p 8080:8080 -v ~/some/path/filebrowser.json:/.filebrowser.json -v ~/some-path:/srv filemanager\nworks for me, where filebrowser.json contains\n{\n  \"port\": 8080,\n  \"baseURL\": \"\",\n  \"address\": \"\",\n  \"log\": \"stdout\",\n  \"database\": \"/database.db\",\n  \"root\": \"/srv\"\n}\nCorrect log prints\n2019/08/12 17:14:27 Using config file: /.filebrowser.json\n2019/08/12 17:14:27 Listening on [::]:8080",
    "\"COPY failed: no source files were specified\" when Dockerfile is not in root directory on Docker Hub": "In the build configuration, you need to configure the \"build context\" directory rather than changing the Dockerfile location. Otherwise, the build will run in the parent of your github repo where the package*.json files do not exist. Note the column after \"Dockerfile location\" when configuring automated builds in the example below:",
    "Using Docker multistage build to create multiple images": "I had the same problem: several projects that shared some common Dockerfile lines, but had a few differences. There's a couple of ways that you can solve this with a single Dockerfile.\nFirst, you can do a fan out method:\nFROM ubuntu:18.04 as base\nRUN echo \"base\" >> /history.txt\nCMD cat /history.txt\n\n\nFROM base as variant0\nRUN echo \"variant0\" >> /history.txt\n\nFROM base as variant1\nRUN echo \"variant1\" >> /history.txt\nThen during your build you just select which one you want using --target:\ndocker build --file=fan-out.dockerfile --target=variant0 --tag=fan-out/variant0 ./\nAlternatively, sometimes your project has the shared steps at the end instead of the beginning. You could do something like this, what I call the fan-in method:\nARG variant\n\nFROM ubuntu:18.04 as variant0\nRUN echo \"variant0\" >> /history.txt\n\nFROM ubuntu:18.04 as variant1\nRUN echo \"variant1\" >> /history.txt\n\nFROM ubuntu:18.04 as variant2\nRUN echo \"variant2\" >> /history.txt\n\nFROM $variant as join\n# pass, do nothing\n\nFROM ubuntu:18.04 as final\nCOPY --from=join /history.txt /\nRUN echo \"final\" >> /history.txt\nCMD cat /history.txt\nAnd build it with a --build-arg:\ndocker build --file=fan-in.dockerfile --target=final --build-arg=\"variant=variant1\" --tag=fan-in/variant1 ./\nIn either of these methods, you'll probably want to have a makefile or shell script to keep track of the commands for each variation.\nI wrote up a blog post with some more details.",
    "\"Docker compose: command not found\" - Despite configuring the path and making file as executable": "Try sudo docker-compose instead. The ./ means that the shell you're in will look in the current directory. If you aren't inside /usr/local/bin or /usr/bin, it will fail.\nAlso if you're on linux, you should follow the following instructions to avoid having to run docker as root: https://docs.docker.com/install/linux/linux-postinstall/",
    "What's this error about while adding postgis in docker [duplicate]": "Use RUN DEBIAN_FRONTEND=noninteractive apt-get install -yqq postgis when install postgis to suppress the dialog, or use ARG DEBIAN_FRONTEND=noninteractive which will also not persist to container but still have same effect.\nMaybe also refers to https://github.com/docker/docker/issues/4032",
    "Docker build COPY failed": "I think the problem is that the environment variable $GOPATH only exists in the golang image and not the scratch image. So try change the COPY-line to:\nCOPY --from=builder /go/bin/piggybank2go /go/bin/piggybank2go",
    "How to set timezone and locale on a Wildfly Docker image?": "This is the solution I came up with after checking the base images\u2019 Dockerfiles and CentOS docs:\nThe following Dockerfile sample sets the S\u00e3o Paulo, Brazil timezone and Brazilian Portuguese as the locale, one can change the timezone/locale to fit one's needs:\nFROM jboss/wildfly:10.1.0.Final\n\nUSER root\n\nRUN localedef -i pt_BR -f UTF-8 pt_BR.UTF-8\nRUN echo \"LANG=\\\"pt_BR.UTF-8\\\"\" > /etc/locale.conf\nRUN ln -s -f /usr/share/zoneinfo/America/Sao_Paulo /etc/localtime\n\nUSER jboss\n\nENV LANG pt_BR.UTF-8\nENV LANGUAGE pt_BR.UTF-8\nENV LC_ALL pt_BR.UTF-8\n...",
    "running a phpUnit test within a docker container": "In the Entrypoint, you need to ran composer install to install the requisite packages that will be available in the docker container",
    "can't install ElasticSearch image, with java 8": "Set environment variables by using ENV instead of RUN export.\nFROM elasticsearch:6.6.1\nRUN yum install -y java-1.8.0-openjdk-devel\nENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.201.b09-2.el7_6.x86_64\nYou can see more explanation by VonC docker ENV vs RUN export.",
    "Issues in sbt - scala project dockerisation": "You are running your container without the -it options (which allow you to connect to its standard input like you're in a terminal), yet your program expects input when it starts (\"press enter...\"). Your program probably waits for input on stdin and probably reads an EOF (end of file) when it starts, causing it to terminate, which in turns terminates your container.\nIf you want to run your container in the background, it seems to me that you have two options:\n1) Run your container using docker run -it -p 9000:9000 <your_other_options> <your_image> and then put it to the background using CTRL+P then CTRL+Q. You will see your container is still running in docker ps. To reattach to it, you can simply use docker attach <your_container>. Of course, this approach will not be applicable if you want to run your container on, say, a unit-test server where you won't want to manually do the CTRL+P/Q thing.\n2) Modify your server so that it can be run completely in the background, without user input. In this case, the way to terminate your program will be to send it a SIGINT signal. This is what CTRL+C usually does, and also what docker stop <your_container> will do for you. You will probably want to properly handle this signal in your Scala code, so that you can perform some clean up instead of abruptly crashing. This can be done using a shutdown hook. Shutdown hooks come from the JVM and are not Scala specific. You should take care of manually stoping any thread / subprocess within your shutdown hook.\nSecond method is best IMO, but is also more involved and probably overkill if the first method works for you.",
    "Executable in Docker image not found": "Thanks to @VolArt for directing my attention to the executable type.\nTurns out that the issue was compatibility between the executable (an ELF 64-bit LSB executable, dynamically linked) and Alpine. The solution was to simply use a different base image. I also needed JRE 8 in the end as well, so I settled on the openjdk:8-jre base image. (I also needed to manually install unzip)\nRelated posts that helped to nail down the issue:\nhttps://serverfault.com/questions/883625/alpine-shell-cant-find-file-in-docker\nhttps://askubuntu.com/questions/133389/no-such-file-or-directory-but-the-file-exists\nDockerfile:\nFROM openjdk:8-jre\n\nRUN mkdir -p /opt/app/\nWORKDIR /opt/app/\n\nRUN apt-get update && apt-get upgrade -y\nRUN apt-get install unzip -y\n\nCOPY target/products/myapp.zip .\nRUN unzip myapp.zip && rm myapp.zip\n\nCOPY docker-entrypoint.sh .\nRUN chmod +x docker-entrypoint.sh\n\nEXPOSE 8081\n\nENTRYPOINT [\"/opt/app/docker-entrypoint.sh\"]\ndocker-entrypoint.sh:\n#!/bin/sh\n\n/opt/app/myapp",
    "Accessing mounted volumes from docker for composer/npm installs on build time?": "What I consider best practice, and what I do professionally, is to not use volumes at all for this case. My Dockerfile COPYs in the application code at build time. I have a working host development setup (the only real host dependency is Node itself; everything else is in the node_modules directory) and so if I have an issue I can reproduce, debug, and write a test for it in my local environment. Only when that works do I go back to Docker.\nFROM ???\nWORKDIR /var/www/app\nCOPY app/composer.json app/composer.lock ./\nRUN composer install --ignore-platform-reqs --no-scripts\nCOPY app/ ./\n...\nCMD [\"apache2ctl\", \"-DFOREGROUND\"]\nOtherwise, there's a couple of things about Docker volumes to remember here:\nEverything in the Dockerfile happens before any volumes or environment variables in the docker-compose.yml file are even considered. If your goal is to populate a volume, you can't do it in the Dockerfile (and this is just awkward in Docker in general; use native host tools instead).\nIf you mount a volume into a container directory, it totally hides what's there already. I see a lot of questions with Dockerfiles that do work only inside a container-local /app directory, then bind-mount the local source tree over that; that basically makes the Dockerfile a no-op.\nIf you have a VOLUME directive in your Dockerfile, you can't make any changes to that directory in the image any more. (In your question, running composer after the VOLUME directive will silently have no effect.)\nYou can mount a volume into any directory in a container regardless of whether or not it was declared as a VOLUME. I'd recommend never declaring VOLUMEs in Dockerfiles, and especially not for directories that contain code (you want these to be updated with new image code when a container gets deleted and recreated).",
    "How can I run multiple JARs in Dockerfile?": "Adding bash script to execute multiple commands and blocks:\n#start.sh\n/usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar MyFirst.jar &\n/usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar MySecond.jar\n... etc\nChange your Dockerfile:\n# base image is java:8 (ubuntu)\nFROM java:8\n\n    # add files to image \n    ADD first.jar .\n    ADD second.jar .\n    ...\n    ADD start.sh .\n\n    # start on run\n    CMD [\"bash\", \"start.sh\"]",
    "How to put the code to docker image while building": "So there are two real paradigms here:\nI am working on my local machine. In this scenario, you more than likely already have the code checked out onto your local machine. Here, just use the COPY directive to take the entire folder and put it somewhere into the container. No need to worry about git or anything of the sort.\nI am having a build server perform the build In this scenario, it makes sense to let the build server check the code out and then perform the same action as above. We just copy the checked out code into the image\nLastly, another alternative that works for dynamic languages like PHP, JS etc, is to NOT put the code into the image, but MOUNT the code onto the container at runtime.\nLet's take PHP for example. If the webserver is looking in /var/www/html for the code, you can run your image like this:\ndocker run -d --name {containername} -p 80:80 -p 443:443 -v /my/dir/where/code/is:/var/www/html {your base image}\nThe above will create the image, but will pass your local directory through to the /var/www/html directory, meaning any changes you make locally would appear in the source code for the container. This was much more prominently used back with Vagrant and the early days of docker before composer was stable.",
    "How to update the settings.js in node-red/node-red-docker image when deployed with resin.io?": "OK, so the problem will be that `/data' is being mounted over the top of what ever is in the docker image.\nThe content of /data will have been laid down by a previous version of the docker image you've pushed to your resin.io device.\nI can see 2 options:\nWipe out the whole image and start again. This only works because you are sill \"playing\". It would not be possible in a production setting.\nLog into the container and edit/replace settings.js and restart",
    "cannot expose port using kubernetes service": "Use Nodeport:\napiVersion: v1\nkind: Service\nmetadata:  \n  name: my-frontend-service\nspec:\n  selector:\n    app: MyApp\n  type: NodePort\n  ports:\n  - protocol: TCP\n    port: 8000\n    targetPort: 4200\n    nodePort: 30001\nthen you can access the service on nodeport 30001 on any node of the cluster.\nFor example the machine name is node01 , you can then do curl http://node01:30001",
    "How to customize molecule's docker image?": "you could also just inline the content:\nRUN echo $'\\n\\\nmy file content\\n\\\n' > /tmp/myfile",
    "Docker does not start cpp application": "You should put the complete path in the ENTRYPOINT and add the parameters to your program.\nThis Dockerfile does the job:\nFROM  ubuntu:latest\nRUN mkdir -p /home/dockerc\nCOPY . /home/dockerc\nENTRYPOINT [\"/home/dockerc/main\", \"hostname\", \"8000\"]\nreplacing hostname and 8000 with the hostname and port that you need.\nEdit\nI tested your program in Linux, and to make it run I had to:\n1) compile for c++11 (because of chrono)\n2) add -t to build the docker app\nThis is the complete list of commands to run:\ng++ -o main main.cpp -std=c++11\ndocker build -t app .\ndocker run app\nand this is the output:",
    "How to build a Docker image for Spring Boot application without the JAR at hand": "You can install maven and run the compile directly in the build. Typically this would be a multi-stage build to avoid including the entire jdk in your pushed image:\nFROM openjdk:8-jdk-alpine as build\nRUN apk add --no-cache maven\nWORKDIR /java\nCOPY . /java\nRUN mvn package -Dmaven.test.skip=true\nEXPOSE 8080\nENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/java/target/myproject-0.0.1-SNAPSHOT.jar\"]\nThe above is a stripped down version of a rework from the same example that I've done in the past. You may need to adjust filenames in your entrypoint, but the key steps are to install maven and run it inside your build.",
    "How to bust the cache for the FROM line of a Dockerfile": "If I understood the context correctly, you can use --pull while using docker build to get the latest base image -\n$ docker build -f Dockerfile.test -t test . --pull\nSo using both --no-cache & --pull will create an absolute fresh image using Dockerfile -\n$ docker build -f Dockerfile.test -t test . --pull --no-cache\nIssue - https://github.com/moby/moby/issues/4238",
    "Dockerfile directory structure for Go Webapp": "In the second stage of your Dockerfile, you are only copying your Go binary from the previous stage. You must also copy your templates directory to the second stage as well so the Go binary can reference your HTML templates:\nFROM golang:1.8-alpine\nRUN apk add --update go git\nRUN go get github.com/lib/pq/...\nADD . /go/src/hello-app\nRUN go install hello-app\nENV USER=username \\\n    PASSWORD=password \\\n    DB=dbname \\\n    HOST=hostname \\\n    PORT=5432\n\nFROM alpine:latest\nCOPY --from=0 /go/bin/hello-app/ .\nCOPY --from=0 /go/src/hello-app/templates ./templates\nENV PORT 4040\nCMD [\"./hello-app\"]\nI'm not sure if this is common practice but when I'm confused about what contents are in what folder within the build process, I simply ls the directory in question to get a better understanding of what might be happening during the build process:\nRUN ls\nObviously you can remove these lines once you've finalized your Dockerfile.",
    "Saving Dockerfile ENV variables for future use": "In your Dockerfile this is easiest if you just install software into directories like /usr/bin and /usr/lib that are already on $PATH and in /etc/ld.so.conf. Since you're describing a reproducible build process on something that will run in an isolated filesystem you're not really \"contaminating the installed OS\".\nI've seen some hints that Docker might be silently stripping, ignoring, or resetting the LD_LIBRARY_PATH environment variable, though I can't find explicit documentation to that effect.",
    "Run jboss docker command with another port": "Changing the command from:\ndocker run -it -p 8080:8080 jboss/wildfly\nto\ndocker run -it -p 8085:8085 jboss/wildfly\nDoes not change the port that the jboss server inside the image listens on. What it does is tells docker to forward port 8085 on your local machine to port 8085 on the container.\nIf what you want to achieve is simply that you can connect to jboss on port 8085 on your local machine then you could just forward port 8085 on your local machine to 8080 in the container:\ndocker run -it -p 8085:8080 jboss/wildfly\nIf you have a real need to actually change the port jboss listens on inside the container then you would need to do something like this (disclaimer: I don't use jboss):\ndocker run -it -p 8085:8085 jboss/wildfly -Djboss.socket.binding.port-offset=5\nApparently this option can be used to modify the port (in this case increasing it by 5).",
    "Reuse Docker Images for multiple python applications": "Without going into details about the different storage backend solutions for Docker (check Docker - About Storage Drivers for reference), docker reuses all the shared intermediate points of an image.\nHaving said that, even though you see in the docker images output [1.17 GB, 1.17 GB, 1.17 GB, 138MB, 918MB] it does not mean that is using the sum in your storage. We can say the following:\nsum(`docker images`) <= space-in-disk\nEach of the steps in the Dockerfile creates a layer.\nLet's take the following project structure:\n\u251c\u2500\u2500 common-requirements.txt\n\u251c\u2500\u2500 Dockerfile.1\n\u251c\u2500\u2500 Dockerfile.2\n\u251c\u2500\u2500 project1\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2514\u2500\u2500 setup.py\n\u2514\u2500\u2500 project2\n    \u251c\u2500\u2500 requirements.txt\n    \u2514\u2500\u2500 setup.py\nWith Dockerfile.1:\nFROM python:3.6-slim\n# - here we have a layer from python:3.6-slim -\n\n# 1. Copy requirements and install dependencies\n# we do this first because we assume that requirements.txt changes \n# less than the code\nCOPY ./common-requirements.txt /requirements.txt\nRUN pip install -r requirements\n# - here we have a layer from python:3.6-slim + your own requirements-\n\n# 2. Install your python package in project1\nCOPY ./project1 /code\nRUN pip install -e /code\n# - here we have a layer from python:3.6-slim + your own requirements\n# + the code install\n\nCMD [\"my-app-exec-1\"]\nWith Dockerfile.2:\nFROM python:3.6-slim\n# - here we have a layer from python:3.6-slim -\n\n# 1. Copy requirements and install dependencies\n# we do this first because we assume that requirements.txt changes \n# less than the code\nCOPY ./common-requirements.txt /requirements.txt\nRUN pip install -r requirements\n# == here we have a layer from python:3.6-slim + your own requirements ==\n# == both containers are going to share the layers until here ==\n# 2. Install your python package in project1\nCOPY ./project2 /code\nRUN pip install -e /code\n# == here we have a layer from python:3.6-slim + your own requirements\n# + the code install ==\n\nCMD [\"my-app-exec-2\"]\nThe two docker images are going to share the layers with python and the common-requirements.txt. It is extremely useful when building application with a lot heavy libraries.\nTo compile, I will do:\ndocker build -t app-1 -f Dockerfile.1 .\ndocker build -t app-2 -f Dockerfile.2 .\nSo, think that the order how you write the steps in the Dockerfile does matter.",
    "docker copy issues and set host env variable": "In a Dockerfile, you can only copy files that are available in the current Docker build context.\nBy default, all files in the directory where you run your docker build command are copied to the Docker context folder.\nSo, when you use ADD or COPY commands, all your paths are in fact relative to build folder, as the documentation states:\nMultiple resources may be specified but the paths of files and directories will be interpreted as relative to the source of the context of the build.\nThis is voluntary because building an image using docker build should not depend on auxiliary files on your system: the same Docker image should not be different if built on 2 different machines.\nHowever, you can have a directory structure like such:\n/home/user1/\n|___file1\n|___docker/\n|___|___ Dockerfile\nIf you run docker build -t test -f docker/Dockerfile . in the /home/user1 folder, your build context will be /home/user1, so you can COPY file1 in your Dockerfile.\nFor the very same reason, you cannot use environment variables directly in a Dockerfile. The idea is that your docker build command should \"pack\" all the information needed to generate the same image on 2 different systems. However, you can hack your way around it using docker-compose, like explaned here: Pass host environment variables to dockerfile.",
    "How to customize Payara image in docker-compose": "If you take a look at the Payara Dockerfile over here:\nhttps://hub.docker.com/r/payara/server-full/~/dockerfile/\nYou will see that it sets environment variables ADMIN_USER and ADMIN_PASSWORD. Just overwrite those with what you'd like to use instead.\nOn the second question - from the documentation for Payara Docker you will see the following:\nIt's possible to run a custom set of asadmin commands by specifying the POSTBOOT_COMMANDS environment variable to point to the abslute path of the custom post boot command file.\nSo you just need to set the POSTBOOT_COMMANDS environment variable.\nExample Docker-Compose file:\nversion: '3'\nservices:\n  payara:\n    image: \"payara/server-full\"\n    environment:\n      - ADMIN_USER=admin\n      - ADMIN_PASSWORD=newpass\n      - POSTBOOT_COMMANDS=./path/to/file",
    "Dockerfile's RUN command doesn't find script": "A very simple problem but a very misleading error.\nThe problem was caused by wrong file endings. Git was set up to convert the project files into Windows (CRLF) file endings. I reinstalled Git with the setting \"Checkout as-is, commit Unix-style\", deleted and recloned the repository, and it fixed the problem.\nWhen it comes to explaining the misleading and confusing error message, my guess is that the file install-composer was actually found and executed. What it is actually saying was that was not found. This empty name was simply the CR caught between two LF (in other words, an empty line) and sh interpreted it as a call to a script file.",
    "Install dependencies of PHP extensions": "The process is indeed annoying and very much something that could be done by a computer. Luckily someone wrote a script to do exactly that: docker php extension installer\nYour example can then be written as:\nFROM php:7.0-apache\n\n#get the script\nADD https://raw.githubusercontent.com/mlocati/docker-php-extension-installer/master/install-php-extensions /usr/local/bin/\n\n#install the script\nRUN chmod uga+x /usr/local/bin/install-php-extensions && sync\n\n#run the script\nRUN install-php-extensions gd",
    "Create directory in Dockerfile using ENV variable": "You may successfully use ARG with ENV in your Dockerfile. See the docs for this:\nYou can use an ARG or an ENV instruction to specify variables that are available to the RUN instruction. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. Consider this Dockerfile with an ENV and ARG instruction.\nFROM ubuntu\nARG CONT_IMG_VER\nENV CONT_IMG_VER ${CONT_IMG_VER:-v1.0.0}\nRUN echo $CONT_IMG_VER\nSo, in your case it should be\nFROM python:3.6.5 as my_base\nARG CUSTOM_PATH=<some default path if you wish>\nENV CUSTOM_PATH=$CUSTOM_PATH\nRUN mkdir $CUSTOM_PATH\nAfter that you may build the image with docker build --build-arg CUSTOM_PATH=/var/log/whatever . and it should work.",
    "pm2-runtime executable file not found in $PATH": "Install pm2 globally like\nnpm install pm2 -g\nOr pass pm2-runtime full path.",
    "how to deploy docker-compose like this to web": "When deploying to docker stack you should not use something like below\nvolumes:\n      - .:/data\nBecause docker will need the current folder path to exists on all the nodes where the container gets instantiated. Also docker stack will not create the folder when the it doesn't exists. This is unlike docker-compose when running locally on docker.\nSo you need to use a path which exists on the node where the container gets deployed. Read the below for more details\nhttps://docs.docker.com/engine/reference/commandline/service_create/#add-bind-mounts-volumes-or-memory-filesystems\ntype=bind: src is required, and specifies an absolute path to the file or directory to bind-mount (for example, src=/path/on/host/). An error is produced if the file or directory does not exist.\nSo you should be using either a path that already exists on your nodes. So you would use something like below\nversion: \"3\"\nservices:\n  app2:\n    image: userName/dockerdocker_app2\n    container_name: app2\n    build:\n      context: ./app2\n    volumes:\n      - /opt/data:/data\n    environment:\n      - LOGGING_LOG-FILES-PATH=/opt/tomcat/logs\n    ports:\n      - \"8000:8080\"\n  app:\n    image: userName/dockerdocker_app\n    container_name: app\n    build:\n      context: ./app\n    volumes:\n      - /opt/data:/data\n    environment:\n      - LOGGING_LOG-FILES-PATH=/opt/tomcat/logs\n    ports:\n      - \"8001:8080\"\nAnd make sure each node has the /opt/data folder present already.\nNamed volume containers\nOr you could use a named volume container which will be automatically created by swarm if it doesn't already exists\nversion: \"3\"\nservices:\n  app2:\n    image: userName/dockerdocker_app2\n    container_name: app2\n    build:\n      context: ./app2\n    volumes:\n      - app2data:/data\n    environment:\n      - LOGGING_LOG-FILES-PATH=/opt/tomcat/logs\n    ports:\n      - \"8000:8080\"\n  app:\n    image: userName/dockerdocker_app\n    container_name: app\n    build:\n      context: ./app\n    volumes:\n      - app1data:/data\n    environment:\n      - LOGGING_LOG-FILES-PATH=/opt/tomcat/logs\n    ports:\n      - \"8001:8080\"\nvolumes:\n  app1data:\n  app2data:",
    "Docker container - Connection Establishment": "It should be enough to export the port e.g., with -p 8116:8116 for the app/mongo db/Elastic Search to be accessible on that port from external servers. You do not need --net=\"host\" for that.\nThe error with ... \\\"exec: \\\\\\\"--net=host\\\\\\\": executable file not found in $PATH\\\"\\n\" is because you pass the --net=\"host\" at the wrong place when executing docker run. All docker run options should come before the name of the docker image which in your case is myapp1. When you add --net=\"host\" option after the myapp1 you essentially instruct docker run to start a new container with the myapp1 image and execute the command --net=\"host\" which of course does not exists on the $PATH. So that is why you get this error.",
    "Ruby on rails on docker-compose": "Resolved, in config/environments/development.rb it has to be: config.cache_classes = false",
    "What init files are sourced in ash for non-interactive, non-login shells?": "The file named by the environment variable ENV is sourced when a POSIX-compliant shell (or bash in POSIX mode) is started in interactive mode, or when 1999 BSD ash is started under any conditions. If you want to support bash when in non-POSIX mode as well, you'll also want to set BASH_ENV (which, like legacy BSD ash, is honored even for noninteractive shells).\nSome versions of ash (including modern dash and busybox ash) follow the POSIX sh standard on this point, and consequently only honor ENV when invoked in interactive mode.\nThis means that with images following the 1999 BSD documentation for ash behavior, you can tell Docker to update the environment variable ENV to point to a file which, when sourced by a shell interpreter, will run eval \"$(pyenv init -)\".\nFor images with newer or more-compliant versions of ash, consider using bash instead, and setting BASH_ENV.",
    "Flag provided but not defined": "I was able to fix it, my Dockerfile looks like this now:\nFROM golang:1.9.2\n\nADD . /go/src/github.com/golang/example/outyet\n\nRUN go install github.com/golang/example/outyet\nRUN [\"chmod\", \"+x\", \"/go/src/github.com/golang/example/outyet/bootstrap.sh\"]\nRUN [\"chmod\", \"+x\", \"/go/bin/outyet\"]\n\nCMD [\"/go/src/github.com/golang/example/outyet/bootstrap.sh\"]\n\nEXPOSE 8091\nEXPOSE 5432\nbootstrap.sh:\n#!/bin/sh\ngo get github.com/derekparker/delve/cmd/dlv;\ngo build -gcflags='-N -l' github.com/golang/example/outyet && \ndlv --listen=:5432 --headless=true --api-version=2 exec outyet\nAnd run it as following:\nsudo docker build -t outyet .\nsudo docker run --security-opt=seccomp:unconfined --net=host  --name test --rm outyet",
    "Why \"RUN rm /etc/nginx/conf.d/default.conf\" did not succeed?": "I don't think you're using your new image.\nI tested it by simply creating a new Dockerfile:\nFROM nginx:alpine\n\nRUN rm /etc/nginx/conf.d/default.conf\nand I've executed the following commands:\ndocker build -t test-nginx .\ndocker run -it --name mynginx-instance test-nginx ls -la /etc/nginx/conf.d\ndocker rm mynginx-instance",
    "Simple node.js workflow with docker": "You can use nodemon in your project to restart your app automatically while your source code directory would be mounted on a volume.\nFor instance, with this directory structure (which is using Grunt from package.json to run nodemon) :\napp/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 Gruntfile.js\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 app.js\n\u2514\u2500\u2500 docker-compose.yml\nYou can use docker-compose which is a tool used to run multiple container. This can be useful if you want to add a database container your app would talk to or any additionnal services interacting with your app.\nThe following docker-compose config will mount src folder on /usr/src/app/src on the container. With nodemon looking for changes inside src, you will be able to make changes on your machine that will restart the app on the container automatically\nTo use this you would do :\ncd app\ndocker-compose up\nThe command above with build the image from dockerfile and start the containers defined in docker-compose.yml.\ndocker-compose.yml :\nversion: '2'\n\nservices:\n  your-app:\n    build: .\n    ports:\n     - \"8080:8080\"\n    restart: always\n    container_name: app_container\n    volumes:\n     - ./src:/usr/src/app/src\n    environment:\n     - SERVER_PORT=8080\nDockerfile :\nFROM node:latest\n\nRUN mkdir -p /usr/src/app\nWORKDIR /usr/src/app\n\nCOPY package.json .\nCOPY Gruntfile.js .\n\nRUN npm install\n\nCMD [\"npm\",\"start\"]\nGruntfile.js :\nvar path = require('path');\n\nmodule.exports = function (grunt) {\n    grunt.initConfig({\n        pkg: grunt.file.readJSON('package.json'),\n        concurrent: {\n            dev: {\n                tasks: ['nodemon'],\n                options: {\n                    logConcurrentOutput: true\n                }\n            }\n        },\n        nodemon: {\n            dev: {\n                script: 'src/app.js',\n                options: {\n                    ignore: [\n                        'node_modules/**'\n                    ],\n                    ext: 'js'\n                }\n            }\n        },\n        clean: {}\n    });\n    grunt.loadNpmTasks('grunt-concurrent');\n    grunt.loadNpmTasks('grunt-nodemon');\n    grunt.registerTask('default', ['concurrent']);\n};\npackage.json :\n{\n  \"name\": \"your-app\",\n  \"version\": \"1.0.0\",\n  \"description\": \"service\",\n  \"scripts\": {\n    \"start\": \"grunt\"\n  },\n  \"author\": \"someone\",\n  \"license\": \"MIT\",\n  \"dependencies\": {\n    \"express\": \"^4.14.0\"\n  },\n  \"devDependencies\": {\n    \"grunt\": \"1.x.x\",\n    \"grunt-cli\": \"1.x.x\",\n    \"grunt-concurrent\": \"2.x.x\",\n    \"grunt-nodemon\": \"0.4.x\"\n  }\n}\nSample app.js :\n'use strict';\n\nconst express = require('express');\nconst port = process.env.SERVER_PORT;\n\nvar app = express();\n\napp.get('/', function(req, res) {\n    res.send('Hello World');\n});\n\napp.listen(port, function() {\n    console.log('listening on port ' + port);\n});\nTo rebuild the image, you would perform docker-compose build",
    "Docker container from Angular4 app : \"-p\" : executable file not found in $PATH": "Try like this :\nStep 1 :\nYou need to create docker file inside the project root.\n- myApp/\n    - Dockerfile\ninside the Docker file add below code\nFROM nginx:alpine\nCOPY dist /usr/share/nginx/html\nStep 2:\nbuild your project in production mode\nStep 3 :\ndocker login\n\ndocker build -t <your registry or image location> .\n\ndocker run --name <your project name > -d -p 4200:81 <your registry or image location>\n\ndocker push <your registry or image location>",
    "How to write a Linux command in a Dockerfile": "You can use the RUN keyword in order to execute stuff.\nIf it's shell command you want to run, you can run bash -c with your shecl script.\nSee some examples and doc here:. https://docs.docker.com/engine/reference/builder/#run",
    "Dockerfile: How can I build my project before copying only the needed files to the image?": "Yes this is possible now using multi stage builds. The idea is that you can have multiple FROM in your docker file and your main image will be built using the last FROM. Below is a sample pseudo structure\nFROM node:latest as reactbuild\nWORKDIR /app\nCOPY . .\nRUN webpack build\n\nFROM golang:latest as gobuild\nWORKDIR /app\nCOPY . .\nRUN go build\n\nFROM alpine\nWORKDIR /app\nCOPY --from=gobuild /app/myapp /app/myapp\nCOPY --from=reactbuild /app/dist /app/dist\nPlease read below article for more details\nhttps://docs.docker.com/engine/userguide/eng-image/multistage-build/",
    "How to run bash script from the mounted volume in docker and exposing the port into outside the container?": "Your problem is that you are trying to run a script in a new container and that container then exists. It has nothing to with any existing container that is running.\nAlso when your specify a command to be run with docker it would not run the CMD command that you had defined while building the Dockerfile.\nSo what you need to do is below.\ndocker run -d -p 8585:9090 -v ~/Docker/:/data d23bdf5b1b1b\nAfter the above container is run it will print the ID of the new container. Now you want to execute your command in this new container\ndocker exec -it <containerid> /data/bin/script.sh",
    "Build docker in ASP.NET Core: \u201cno such file or directory\u201d error": "COPY ${source:-obj/Docker/publish} .\nThe source seems to be pointing to /var/lib/docker/tmp/docker-builder739115341/. Try the using a relative path instead and copy it into a speficif folder to avoid conflicts with whatever is in the root filesystem.\nCopy obj/Docker/publish /app",
    "Writable node_modules with read only source files in docker": "To create a writable mount inside a read only mount, you need to ensure that the read-only directory has the mount points (folders), even if they are empty.\nTo create empty mount points and commit them, please see this answer.\nFor running node, you need following 4 folders writable:\n$ mkdir webapp/frontend/node_modules webapp/frontend/build webapp/frontend/.config webapp/frontend/.npm\n\n$ cat > webapp/frontend/node_modules/.gitignore\n# Ignore everything in this directory\n*\n# Except this file\n!.gitignore\n\n$ git add -f webapp/frontend/node_modules/.gitignore\n\n$ cat docker-compose.yml  # Filtered output below\nversion: \"2\"\nservices:\n    webapp:\n        build: ./webapp\n        expose:\n          - \"9900\"\n        # Named volumes, defined below.\n        volumes:\n          - ./webapp:/usr/src/app/webapp:ro\n          - webapp_config:/usr/src/app/webapp/frontend/.config:rw\n          - webapp_npm:/usr/src/app/webapp/frontend/.npm:rw\n          - webapp_node_modules:/usr/src/app/webapp/frontend/node_modules:rw\n          - webapp_build:/usr/src/app/webapp/frontend/build:rw\n          - ./config.ini:/usr/src/app/config.ini:ro\n# Named volumes. These will stay in the host, but not in the current directory.\nvolumes:\n      webapp_node_modules:\n      webapp_build:\n      webapp_config:\n      webapp_npm:\nRelated answer about writable folders in read-only mounts\nSee this documentation for different volume/storage options in docker\nRelated answer about named volumes",
    "Docker container always restarting": "When docker-compose runs your \"server\" container, it will immediately terminate. A docker container needs at least one running process, otherwise, the container will exit. In your example, you are not starting a process that keeps alive.\nAs you have configured restart: always, docker-compose will endlessly restart new containers for \"server\". That should explain the behavior that you describe.\nI have seen docker-compose files, where data containers were defined which only mounted images (in combination with volumes_from). They deliberately used /bin/true as a command, which also lead to permanent but harmless restarts. For example:\ndata:\n  restart: always\n  image: postgres:latest\n  volumes:\n    - /var/lib/postgresql\n  command: \"true\"\nIf restarts are not what you want, you could start a process in the container that does something useful, like running a web server or a database. But a bash alone is not something that will keep a container alive. A bash running in non-interactive mode will exit immediately.",
    "ENTRYPOINT in Combination with CMD": "Do this and be happy:\nENTRYPOINT [\"/entry.sh\"]\nCMD node index.js\nentry.sh:\n#!/bin/bash\n#entry.sh\n\n#step 1\nnpm install\n#step 2\nnpm run watch &\n#step 3\ncompass watch &\n#step n\nexec \"$@\"\nBe sure of:\nchmod +x entry.sh\nAnd in Dockerfile:\nCOPY entry.sh /",
    "How should I create Dockerfile to run multiple services through docker-compose?": "They are correct. Each container & by extension, each image should be responsible for one concern & that is typically mapped to a single process. So if you need to run more than one thing (or more than one process, generally speaking, not strictly) then you most probably require to build separate images. One of the easiest & recommended ways of creating an image is writing a Dockerfile. This is expected to be an extremely simple process and most of it will be a copy paste of the same commands you would have used to install that component.\nOne you write the Dockerfile's for each service, you must build them using docker build command, which will result in the images.\nWhen you run an image you get what is known as a container. Think of it roughly like an iso file is the image & the actual vm or running machine is the container.\nNow you can use docker-compose to orchestrate how these various containers so they can communicate (or be isolated from) with each other. A docker-compose.yml file is a plain text file in the yaml format that describes the relationship between the different components within the app. Apps can be made up of several services - like webserver, appserver, searchengine, database server, cache engine etc etc. Each of these is a service and runs as a container, but it is also not necessary to run everything as a container. Some can remain running in the traditional way, on VM's or on bare metal servers.\nI'll check your other post and add if there is anything needed. But I hope this helps you get started at least.",
    "Connection Refused Docker Run": "Regarding EXPOSE:\nEach container you run has its own network interface. Doing EXPOSE 5000 tell docker to link a port 5000 from container-network-interface to a random port in your host machine (see it with docker ps), as long as you tell docker to do it when you docker run with -P.\nRegarding logstash.\nIf your logstash is installed in your host, or in another container, it means that logstash is not in the \"localhost\" of the container (remember that each container has its own network interface, each one has its own localhost). So you need to point to logstash properly.\nHow?\nMethod 1:\nDon't give container its own iterface, so it has the same localhost as your machine:\ndocker run --net=host ...\nMethod 2:\nIf you are using docker-compose, use the docker network linking. i.e:\nservices:\n  py_app:\n    ...\n    links:\n      - logstash\n  logstash:\n     image: .../logstash..\nSo point as this: logstash:5000 (docker will resolve that name to the internal IP corresponding to logstash)\nMethod 3:\nIf logstash listen in your localhost:5000 (from your host), you can point to it as this: 172.17.0.1:5000 from inside your container (the 172.17.0.1 is the host fixed IP, but this option is less elegant, arguably)",
    "How to substitute a command output into a string and append that to a file (in Alpine Linux running in Docker)": "You can use ARG\nFROM alpine:latest\nEXPOSE 9050 9051\nARG password\nRUN apk --update add tor\nRUN echo \"ControlPort 9051\" >> /etc/tor/torrc\nRUN echo \"HashedControlPassword $(tor --hash-password $password)\" >> /etc/tor/torrc\nCMD [\"tor\"]\nAnd then build using:\ndocker build --build-arg password=foo Dockerfile\nIn general I would not bake password in an image. It would be better to provide those things when you run the container using -e.",
    "Docker and Neo4J": "The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. EXPOSE does not make the ports of the container accessible to the host. To do that, you must use either the -p flag to publish a range of ports or the -P flag to publish all of the exposed ports. You can expose one port number and publish it externally under another number.\nTo set up port redirection on the host system, see using the -P flag. The Docker network feature supports creating networks without the need to expose ports within the network, for detailed information see the overview of this feature).\nhttps://docs.docker.com/engine/reference/builder/",
    "Docker Image file details": "A Docker image is essentially a JSON manifest that includes, among other things, references to file system layers by a SHA256 content checksum.\nA layer is a read only set of filesystem changes, that when combined with the other layers produces the file system for your image.\nLayers can be shared between images\nInspecting an Image\nThe RootFS property of docker inspect <image> lists the layers for an image\n\u2192 docker inspect openjdk:8-jre-alpine\n[\n    {\n        \"Id\": \"sha256:d85b17c6762eb3455c7b7ff1930bdde8c911137fe8c7f3c0b5988c66149dc27b\",\n        \"RepoTags\": [\n            \"openjdk:8-jre-alpine\"\n        ],\n        \"RepoDigests\": [\n            \"openjdk@sha256:48ac96e309a748f5778db26be7e45ca0e35931ef58e9b271fe36767a55411728\"\n        ],\n        \"Parent\": \"\",\n        \"Comment\": \"\",\n        \"Created\": \"2016-12-27T18:38:49.797530681Z\",\n        \"Container\": \"de5e277bf3e5ac747933bbb8e703382b240fdcdaef9d927faa4e02c1f0cbc35e\",\n        \"ContainerConfig\": {\n            \"Hostname\": \"26ba10d264c2\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/bin\",\n                \"LANG=C.UTF-8\",\n                \"JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk/jre\",\n                \"JAVA_VERSION=8u111\",\n                \"JAVA_ALPINE_VERSION=8.111.14-r0\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"set -x \\t&& apk add --no-cache \\t\\topenjdk8-jre=\\\"$JAVA_ALPINE_VERSION\\\" \\t&& [ \\\"$JAVA_HOME\\\" = \\\"$(docker-java-home)\\\" ]\"\n            ],\n            \"ArgsEscaped\": true,\n            \"Image\": \"sha256:7bf1d56e53b8a99662fc8314d3860b68f30798646ca0265aeeaa1275f679d314\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": [],\n            \"Labels\": {}\n        },\n        \"DockerVersion\": \"1.12.3\",\n        \"Author\": \"\",\n        \"Config\": {\n            \"Hostname\": \"26ba10d264c2\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/bin\",\n                \"LANG=C.UTF-8\",\n                \"JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk/jre\",\n                \"JAVA_VERSION=8u111\",\n                \"JAVA_ALPINE_VERSION=8.111.14-r0\"\n            ],\n            \"Cmd\": null,\n            \"ArgsEscaped\": true,\n            \"Image\": \"sha256:7bf1d56e53b8a99662fc8314d3860b68f30798646ca0265aeeaa1275f679d314\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": [],\n            \"Labels\": {}\n        },\n        \"Architecture\": \"amd64\",\n        \"Os\": \"linux\",\n        \"Size\": 107853459,\n        \"VirtualSize\": 107853459,\n        \"GraphDriver\": {\n            \"Name\": \"overlay2\",\n            \"Data\": {\n                \"LowerDir\": \"/var/lib/docker/overlay2/fccc5a703d4022f0fb561a3d0bc6c75da8a99aa1b3291f6be92bf8908e8d5add/diff:/var/lib/docker/overlay2/96ba8cd8b7b44ef284070bd4b98d912fd0bd2b9d6a0d906e661db901408ef38e/diff\",\n                \"MergedDir\": \"/var/lib/docker/overlay2/890f1dbb6c802344c58a32e0ee6d2b49dea74418963ab10a0b649ddc50da6127/merged\",\n                \"UpperDir\": \"/var/lib/docker/overlay2/890f1dbb6c802344c58a32e0ee6d2b49dea74418963ab10a0b649ddc50da6127/diff\",\n                \"WorkDir\": \"/var/lib/docker/overlay2/890f1dbb6c802344c58a32e0ee6d2b49dea74418963ab10a0b649ddc50da6127/work\"\n            }\n        },\n        \"RootFS\": {\n            \"Type\": \"layers\",\n            \"Layers\": [\n                \"sha256:7cbcbac42c44c6c38559e5df3a494f44987333c8023a40fec48df2fce1fc146b\",\n                \"sha256:da07d9b32b0090fa42690c204d7b49925b5e65ea770893d02c01ab00d61ff920\",\n                \"sha256:6f7515f190962a84c9837ad7adb1e684d79fa0d798a57a99fbe090cc0f97f39c\"\n            ]\n        }\n    }\n]\nYou can see from the docker history of the image, there were three commands in the Dockerfile that modified the file system. All other commands contribute to the metadata associated with the image (Use --no-trunc to get the full output)\n\u2192 docker history openjdk:8-jre-alpine\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\nd85b17c6762e        2 months ago        /bin/sh -c set -x  && apk add --no-cache  ...   103 MB              \n<missing>           2 months ago        /bin/sh -c #(nop)  ENV JAVA_ALPINE_VERSION...   0 B                 \n<missing>           2 months ago        /bin/sh -c #(nop)  ENV JAVA_VERSION=8u111       0 B                 \n<missing>           2 months ago        /bin/sh -c #(nop)  ENV PATH=/usr/local/sbi...   0 B                 \n<missing>           2 months ago        /bin/sh -c #(nop)  ENV JAVA_HOME=/usr/lib/...   0 B                 \n<missing>           2 months ago        /bin/sh -c {   echo '#!/bin/sh';   echo 's...   87 B                \n<missing>           2 months ago        /bin/sh -c #(nop)  ENV LANG=C.UTF-8             0 B                 \n<missing>           2 months ago        /bin/sh -c #(nop) ADD file:eeed5f514a35d18...   4.8 MB              ",
    "using system mongodb for multiple docker containers?": "1 & 2. When you build a docker container, it creates VLAN for docker container with ip address gateway is : 172.17.42.1, so docker container would connect to mongo, it should be : mongodb://172.17.42.1:27017/localv2",
    "How to re-mount a docker volume without overriding existing files?": "$ docker run -d --name test1 gitlab/gitlab-runner\n$ docker run -d --name test2 -v ~/etc:/etc/gitlab-runner gitlab/gitlab-runner\nYou've got two different volume types there. One I call an anonymous volume (a very long uuid visible when you run docker volume ls). The second is a host volume or bind mount that maps a directory on the host directly into the container. So each container you spun up is looking at different places.\nAnonymous volumes and named volumes (docker run -d -v mydata:/etc/gitlab-runner gitlab/gitlab-runner) get initialized to the contents of the image at that directory location. This initialization only happens when the volume is empty and is mounted into a new container. Host volumes, as you've seen, only get the contents of the host filesystem, even if it's empty at that location.\nWith that background, the short answer to your question is no, you cannot mount a file inside the container back out to your host. But you can copy the file out with several methods, assuming you don't overlay the source of the file with a host volume mount. With a running container, there's the docker cp command. Personally, I like:\ndocker run --rm -v ~/etc:/target gitlab/gitlab-runner \\\n  cp -av /etc/gitlab-runner/. /target/.\nIf you have a named volume with data you want to copy in or out, you can use any image with the tools you need to do the copy:\ndocker run --rm -v mydata:/source -v ~/etc:/target busybox \\\n  cp -av /source/. /target/.",
    "Cannot connect to the MongoDB server from node by using the environmental system variables set in Docker": "Well, apparently I just got my rubber duck style answer.\nThe issue was with the '- MONGODB_URI=\"192.168.99.100/myapp\"'. The value should not be wrapped in quotes.\nI set it to \"MONGODB_URI=192.168.99.100/myapp\" and it works fine.",
    "Tomcat docker-compose running web app": "OK guys finally figured this out issue was to do with the tomcat dependency I had in my pom\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-tomcat</artifactId>\n    <scope>provided</scope>\n</dependency>\nEven though the scope is set to provided the container does not seem to like it, I have removed the dependency and everything is working as expected.",
    "Mongo Docker not creating user": "You can overcome the problem by overriding the mongo:3.2 image CMD directive with your own, and rigging the scripts to run after the image is started. You can also create a shell script to run the container and then docker exec the scripts after the container is started. Either way - the scripts should be executed at run time, not build time.",
    "Building docker image with angular 2, jupyter and typescript": "You can copy the package.json anywhere as long as you set the WORKDIR before running NPM install.\nCOPY . /src\nWORKDIR /src\nRUN npm install",
    "Dockerfile RUN command taking a lot of disk space": "If I'm understanding the question correctly, you're wondering why this specific image layer is 1 GiB in size when all you did was creating a few new symlinks. Correct? (Why four different JDKs are large in size should be self-explanatory).\nKey is the chown user.root /usr/java statement. I strongly suspect that this statement causes the files for which the permissions were changed (i.e. probably the entire /usr/java directory) to be added again to this layer in the image.\nYou can verify this behaviour with a very simple example. Consider the following Dockerfile:\nFROM ubuntu:latest\n\nRUN echo foo > /tmp/bar\nRUN useradd foo\nRUN chown foo /tmp/bar\nNow build this image with docker build and then inspect it with docker history <image-id>:\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\nfdd96781f94f        5 seconds ago       /bin/sh -c chown foo /tmp/bar                   4 B                 \n7237dbee1999        6 seconds ago       /bin/sh -c useradd foo                          330.3 kB            \n69ed7323a0b0        6 seconds ago       /bin/sh -c echo foo > /tmp/bar                  4 B                 \n17b6a9e179d7        5 months ago        /bin/sh -c #(nop) CMD [\"/bin/bash\"]             0 B                 \nb0c2dfa2701f        5 months ago        /bin/sh -c sed -i 's/^#\\s*\\(deb.*universe\\)$/   1.895 kB            \n202e40f8bb3a        5 months ago        /bin/sh -c rm -rf /var/lib/apt/lists/*          0 B                 \nacb8e44f43fa        5 months ago        /bin/sh -c set -xe   && echo '#!/bin/sh' > /u   701 B               \n487bffc61de6        5 months ago        /bin/sh -c #(nop) ADD file:ffc85cfdb5e66a5b4f   120.8 MB            \nUse the ID of the topmost layer (/bin/sh -c chown foo /tmp/bar) and look for this ID in /var/lib/docker/aufs/diff/<ID>... (assuming you're using AUFS as storage driver):\n$ find /var/lib/docker/aufs/diff/fdd96781f94feee4a6db44b11f7f9411c52238458ceeef202b2203e77b9970f4\n/var/lib/docker/aufs/diff/fdd96781f94feee4a6db44b11f7f9411c52238458ceeef202b2203e77b9970f4\n/var/lib/docker/aufs/diff/fdd96781f94feee4a6db44b11f7f9411c52238458ceeef202b2203e77b9970f4/tmp\n/var/lib/docker/aufs/diff/fdd96781f94feee4a6db44b11f7f9411c52238458ceeef202b2203e77b9970f4/tmp/bar\nAs you can see, changing the permissions of a file during the build process causes it to be added again in the next image layer. Changing the permissions of all your Java SDKs, causes all of them (with ~1 GiB in size) to be added to an additional image layer.",
    "Set ENV variable in container is not working, is every under \"/usr/local/bin\" executed on container run?": "You're correct in that items under /usr/local/bin are not automatically executed.\nThe Filesystem Hierarchy Standard specifies /usr/local as a \"tertiary hierarchy\" with its own bin, lib, &c. subdirectories, equivalent in their intent and use to the like-named directories under / or /usr but for content installed local to the machine (in practice, this means software installed without the benefit of the local distro's packaging system).\nIf you want a command to be executed, you need a RUN that directly or indirectly invokes it.\nAs for the other matters discussed as this question has morphed, consider the following:\nFROM alpine\nENV foo=bar\nRUN echo $foo >/tmp/foo-value\nCMD cat /tmp/foo-value; echo $foo\nWhen invoked with:\ndocker run -e foo=qux\n...this emits as output:\nbar\nqux\n...because bar is the environment variable laid down by the RUN command, whereas qux is the environment variable as it exists at the CMD command's execution.\nThus, to ensure that an environment variable is honored in configuration, it must be read and applied during the CMD's execution, not during a prior RUN stage.",
    "Use STDIN during docker build from a Dockerfile": "You can't attach interactive tty during image build. If it is asking for 'yes' or 'no' during package installation, wget in your case, you can replace the corresponding line with RUN apt-get update -qq && apt-get install -y wget. If it is bash file_downloaded.sh, check if file_downloaded.sh accepts 'yes' or 'no' as a command line argument.\nIf file_downloaded.sh doesn't have that option, create a container from ubuntu:14.04 image, install wget and run your commands manually there. Then, you can make an image of the container by committing your changes like: docker commit <cotainer_id> <image_name>.",
    "How to make sure commands are not repeated in docker": "It repeats here because you did not add another RUN command, but appended (and changed) the previous command (docker detects this change, and runs the new command).\nWhat you should be writing is:\nFROM centos:6.8\nMAINTAINER Bilal Usean \"xxxxxxxx@xxx.xxx\"\nRUN yum install -y httpd; yum -y clean all\nRUN yum install java-1.6.0-openjdk-devel; yum -y clean all",
    "Running RC2 project on defined port": "You can also specify the Urls at Dockerfile level (better if you want to reuse the Container). Here it is the full Dockerfile:\nFROM microsoft/dotnet\n\nRUN printf \"deb http://ftp.us.debian.org/debian jessie main\\n\" >> /etc/apt/sources.list\n\nCOPY . /app\nWORKDIR /app\nRUN [\"dotnet\", \"restore\"]\nRUN [\"dotnet\", \"build\"]\n\nEXPOSE 5000/tcp\nENTRYPOINT [\"dotnet\", \"run\", \"--server.urls=http://0.0.0.0:5000\"]\nYou also need to modify the Program.cs file to read the configuration from the main args:\n    public static void Main(string[] args)\n    {\n        var config = new ConfigurationBuilder()\n            .AddCommandLine(args)\n            .AddEnvironmentVariables(prefix: \"ASPNETCORE_\")\n            .Build();\n\n        var host = new WebHostBuilder()\n            .UseConfiguration(config)\n            .UseKestrel()\n            .UseContentRoot(Directory.GetCurrentDirectory())\n            .UseIISIntegration()\n            .UseStartup<Startup>()\n            .Build();\n\n        host.Run();\n    }\nYou have the step-by-step tutorial and why in this blog post: https://www.sesispla.net/blog/language/en/2016/05/running-asp-net-core-1-0-rc2-in-docker/",
    "Referencing a dynamic argument in the Docker Entrypoint": "When you have an ENTRYPOINT script in your image, that script will receive any arguments passed after the image on the docker run command line. That is, if you have:\nENTRYPOINT /path/to/my/script.sh\nAnd you run:\ndocker run myimage one two three\nYour ENTRYPOINT script will be called like:\n/path/to/my/script.sh one two three\nFrom that point on, it's just like writing any other shell script that takes arguments:\n#!/bin/sh\n\nbackup_label=$1\nduplicity /target file:///backup/$backup_label",
    "How can I set .bash_profile environment variable in Dockerfile?": "You can declare a environment variable with the ENV statement\nENV foo=hello",
    "Is there a way to specify a suggested tag name inside a Dockerfile?": "No, this is not possible; the name of the resulting image can only be specified during docker build, or afterwards, using docker tag (and docker commit).\nThere was a proposal for this, but it's not accepted; see https://github.com/docker/docker/issues/5603 for the discussion on this.",
    "nodeJS dockerfile build failing with NPM": "As an update, i was building this application in aws on an tiny free instance. basically this was cause by the box running out of memory during the build processes.\nonce I added more memory the build completed correctly",
    "Failed Installation of Docker plugin for Intellij Idea CE 2016.1 due to <required plugin \"org.jetbrains.plugins.remote-run\" not installed.>": "As stated here, One of the guys developing IntelliJ IDEA 2016.1 thought that the remote-run plugin was available in the Community Edition for IntelliJ IDEA (which it isn't). That was this morning (Dated: 18th March 2016), hopefully he'll extract the dependency sometime soon and update the plugin.\nTL;DR: Wait a little while until the Docker plugin has been updated, a Jetbrains Developer screwed up.",
    "Dockerfile volume with database - using volume for mutable user-servicable parts": "Not a complete answer, but I found an example which might help. From the book \"Build your own PAAS with Docker\" by Oskar Hane, where he creates a container used only to host files for other containers, such as a MySQL container:\nThere is a VOLUME instruction for the Dockerfile, where you can define which directories to expose to other containers when this data volume container is added using --volumes-from attribute. In our data volume containers, we first need to add a directory for MySQL data. Let's take a look inside the MySQL image we will be using to see which directory is used for the data storage, and expose that directory to our data volume container so that we can own it:\nRUN mkdir \u2013p /var/lib/mysql \nVOLUME [\"/var/lib/mysql\"] ",
    "How to connect my NodeJS with my Angular (in Nginx)": "You need to expose the port of the node.js container to which nginx(angular) container will connect. See the Connect using network port mapping section of docker documentation.\nUpdate : I think, you need to configure the nginx configuration file to the node container. This question has sample nginx file related to your use case (although, not related to containers).\nEdit : To map the node app with the nginx, you first need to link the node container with nginx container.\ndocker run -d -p 80:80 --name \"nginx\" --link nodejs:nodejs localhost:5000/test/nginx:1\nWhen you link the node container with the nginx container, the node container's address will be saved in the /etc/hosts. So the nginx container can access the node's address from there.\nSo, in nginx configuration file, the nodejs will be accessible as nodejs' container address:\nhttp {\n\n        upstream node-app {\n              server nodejs:8888 weight=10 max_fails=3 fail_timeout=30s;\n        }\n\n        server {\n              listen 80;\n\n              location / {\n                proxy_pass http://node-app;\n                proxy_http_version 1.1;\n                proxy_set_header Upgrade $http_upgrade;\n                proxy_set_header Connection 'upgrade';\n                proxy_set_header Host $host;\n                proxy_cache_bypass $http_upgrade;\n              }\n        }\n}",
    "Kitematic: connecting to a docker machine with different name": "This is currently on their to do list https://github.com/docker/kitematic/issues/1005 and https://github.com/docker/kitematic/pull/992\nSo at the moment it isn't possible from Kitematic.\nYou can do much more with docker-machine but the trade-off is that it is a command-line interface",
    "What is the best way to append to /etc/hosts within Dockerfile": "It's best not to do this, mainly because you are tying the image to your network. Instead, use the --add-host flag to docker run to add entries e.g:\ndocker run --add-host test:127.0.0.1 nginx",
    "Creating a docker image by merging 2 images": "There is no automated way to merge these images and many would argue that you should not really do this in any case since these images do quite different things and it's not really the way to do it.\nDid you check out sameersbn's example project using the ci-runner image? You could use a similar approach to install Java and Maven.",
    "Best way to get latest git code with Docker image restart?": "I have a good practice recently, because I just made a docker last week: the source code hosted on github, testing use CircleCI, auto-build on DockerHub, auto-deploy on Tutum.co, and at last, run by my own server(some of them I have never logined)\nevery time I push code to github, github will send notification to Tutum by webhook. then Tutum will get the source code, start building and testing my new docker image. if test passed, image will be pushed to my private registry. then tutum will send re-deploy notification to my docker service, my docker will pull the new image, terminate the old container, relaunch new container with the latest image for me.\nall above is totally automaticly. everything will happen after I run a \"git push\" command.\npersonally I use tutum very heavy, so I recommand you to read a article of Tutum Automated Build: https://support.tutum.co/support/solutions/articles/5000638474-automated-builds\nalso, you could have a look of my docker, I use almose everything I could find to this tiny toy project(CI/CD). :) https://github.com/zixia/docker-simple-mail-forwarder",
    "docker: Installing application code from git in docker image": "The easier way to get your app into your Docker image is usually to use the ADD command.\nAssuming your repository looks like this:\n.git\nmy_app_stuff/\nmore_app_stuff/\nThen just drop your Dockerfile next to .git, and use:\nADD . /app\nWhen you run docker build . from that directory, your app will be injected into the container at /app.\nNow, that doesn't quite do it, since it'll also load .git into your image, which you might want to avoid (especially if you've included credentials in .git/config!).\nTo fix this, simply tell Docker that it should ignore this directory by adding a .dockerignore plaintext file containing .git.\nIn the end, your tree should look like this:\n.dockerignore\n.git\nmy_app_stuff/\nmore_app_stuff/\nNote that if a developer wants to, they can use -v ... to mount their local copy of the app at /app and overwrite the one that's built into the image.\nNow, that does not quite solve your problem as far as being bale to git pull from the container goes.\nIf you actually want to do this, you'll want to keep .git in there, and ensure valid credentials are somehow passed to your container when you launch it (perhaps through a volume or an environment variable).",
    "docker CMD run supervisord in background": "you can remove setting nodaemon or set it to false in supervisord.conf\n[supervisord]\nnodaemon=false ; Docker\u5229\u7528\u3067\u306ftrue\u306b\u3059\u308b\u5fc5\u8981\u3042\u308a\nthis will make supervisor start in background.",
    "Vagrant up --no-parallel flag meaning": "--no-parallel option makes sense when vagrant up is used to bring multiple machines up altogether: that is when Vagrantfile declares multiple machines and vagrant up is either given no machine names or multiple machine names. In this case, if the provider supports this (and yes, Docker provider does indeed), Vagrant will attempt to bring all requested machines up in parallel, unless --no-parallel is given.\nSo with Docker provider, when linking containers, one may use --no-paralel when bringing multiple machines up altogether:\n$ vagrant up --no-parallel\nor\n$ vagrant up /regex to match machine names/ --no-parallel\nHowever, if you bring machines up one-by-one with separate commands, it will have no effect (no harm either) if --no-parallel is specified or not. So one may simply do:\n$ vagrant up a && vagrant up b",
    "Docker data-only container and dealing with new releases": "Now this setup works but I just can't figure out how to handle a new release. My source is in git, when I want to deploy to production I imagine I create a new image (FROM busybox should probably be replaced with my existing image url) and pull in the new image on my production server.\nApart from the statement about busybox (which I don't follow), that seems pretty much right. Normally, you re-build the images, push to a registry, then pull from the production server. And as @Mario Marin suggests, it's worth being clever about tags, so that you can easily roll-back if needs be and you know exactly which version of your app is deployed.\nBut how do I get the data to update for my web container and such? I also have to make sure my persistent data (/var/lib/mysql) remains.\nI assume this is referring to your data container, which you've done in a bit of an unusual way. To begin with, I would pull out the mysql directory and put it in its own data container. I would use the percona image for this so that all the permissions are set correctly. When you create the data-container, you don't leave it running, so there's no need to worry about the container getting out-of-date; it's really just a namespace for the directory.\nThe next step is to deal with the app directory, which I assume isn't data, but code? In this case I would include it in your web image (don't use a volume at all). In the Dockerfile I would normally do a git clone to keep the image up-to-date. During development you can mount a volume over the top of the app directory with code from the host so you can make changes instantly.\nFor more info on data containers, have a look at http://container42.com/2014/11/18/data-only-container-madness/",
    "Performance impact of base image in docker": "All else being equal, the base image shouldn't directly affect performance / resource usage: there will be a single process running in your container, so your application is the only thing that can consume any resources. No matter what else exists on the file system.\nThat said, I can imagine differences caused indirectly by your choice of base image: default JVM options, the version of the JVM that the package manager on your image installs; any difference in the environment really. No more examples come to mind, but I'm sure there can be some down the stack.",
    "Dockerfile exclude flag isn't recognize with 1.7-labs version": "Have you tried enabling BuildKit using either an environment variable or the Docker daemon config, as documented here?\nI'm able to build your image on Windows 10 using Docker Desktop 4.30.0 with:\nDOCKER_BUILDKIT=1 docker-compose build\nI saw the same error you did before providing the environment variable / flag.",
    "ARG variable in Dockerfile not expanding": "I attempted your example using linux alpine base image, but for me in the console it prints the variable while running docker build .\nHere is Dockerfile:\nFROM alpine\n#Copy source code into container\nWORKDIR /src\nCOPY . .    \n#Build application\nARG BUILD_CONFIGURATION=Release\nRUN dotnet build \"MyProject/MyProject.csproj\" -c ${BUILD_CONFIGURATION} -o /bin\nHere is output:\n....\n => [2/4] WORKDIR /src                                                                                     0.1s\n => [3/4] COPY . .                                                                                         0.3s\n => ERROR [4/4] RUN dotnet build \"MyProject/MyProject.csproj\" -c Release -o /bin                           0.3s\n------                                                                                                          \n > [4/4] RUN dotnet build \"MyProject/MyProject.csproj\" -c Release -o /bin:\n0.323 /bin/sh: dotnet: not found\n...\nAlthough it is outputting error but at least it is printing Release as part of log, which proves that it is working for linux base image...\nEdit\nSince you are using windows based base image, the environment variables are evaluated using %% rather than $ symbol. So you can modify your run command to make it working:\ndotnet build \"MyProject/MyProject.csproj\" -c %BUILD_CONFIGURATION% -o /bin",
    "Dockerfile cannot run apt-get (exit code: 100) [duplicate]": "It seems that the base image sets a user other than root. This can be fixed using:\nFROM reliableembeddedsystems/yocto:ubuntu-20.04-base\nUSER root\nRUN apt-get update",
    "Dockerfile: Append/concat to a file from a temporary source": "An APPEND-like instruction for Dockerfile(s) does not exist.\nAs of to date, a RUN bind mount is the most efficient way to temporarily-only use a source file in the build context, i.e. without keeping it in the final image. With this, you only need one RUN instruction, thus creating a single cache layer in the Docker image.\nFor example, to append a temporary source file, do something like:\nRUN --mount=type=bind,source=myfile,target=/tmp/myfile \\\n    cat /tmp/myfile >> /some/path\nNote: the --mount option requires BuildKit.\nThe alternative of first COPY'ing a temporary file to only removing it later explicitly in a RUN instruction of course works, but that would create 2 cache layers in the Docker image:\nCOPY myfile /tmp/myfile\nRUN cat /tmp/myfile >> /some/path && rm /tmp/myfile",
    "How to disable git cache for docker compose build context?": "It is caching the git repo, yes, and there isn't a compose option to change that.\nHowever, based on the compose sources, the caching is done based on a git SHA.\nref.Commit = sha\n...\nlocal := filepath.Join(cache, ref.Commit)\nSo the content in the cache will really only be used when the repo has not changed on that branch you are fetching it from, since it's based on a checksum.\nIf the checksum does not match, it means that the cache is not up to date and compose will fetch the latest updates from your github repo, no matter what, and your latest changes will be applied.",
    "How do install and start daemon Docker in an Alpine container?": "To clarify the discussion in the comments, and the solution you found, it sounds like you have a Windows host onto which Docker Desktop is installed. You have an Alpine container in which the Docker client command is installed. You issued a basic command (docker images) inside the running container and found that it did not work. This is because, by the default, the console command will expect Docker is running locally (in this case in the container).\nYour solution was thus:\ndocker run \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    ...\nThis has the effect of sharing the Unix socket from the host (where Docker is running) to the container (where you only have a client, not a server).\nThis technique is helpful if you want to run some code in isolation from the host, but you still need access to your existing Docker images.",
    "Port expose in ASP.NET Core with dockerfile is not working": "404 means this web application is working well. You can access one of the api, like https://localhost:44369/weatherforecast, it will show the json data.\nYou should move app.UseSwagger(); and app.UseSwaggerUI() out of the condition.\nSample\nvar app = builder.Build();\n\napp.UseSwagger();\napp.UseSwaggerUI();\n\napp.UseAuthorization();\n\napp.MapControllers();\n\napp.Run();",
    "Docker Node: Running into `exec format error`": "I was able to resolve this issue by removing the --platform=$BUILDPLATFORM parameter from the last stage of the Dockerfile, should any still be interested.",
    "How to fix: ERROR: Failed building wheel for pyarrow in docker?": "pyarrow is currently compatible with python up to 3.11. Doc source. Try to downgrade your python version ?\nUpdate\nAs of November 6, 2024, pyarrow is now compatible with Python versions 3.9, 3.10, 3.11, 3.12, and 3.13.",
    "Running Docker container - X11 failed to open display :0": "To my knowledge X11 forwarding does not expose the OpenGL driver necessary to run a Fyne application. This is similar to the open issue that a Fyne app cannot be launched from headless Linux.",
    "Docker build of Apache Superset fails with \"The command '/bin/sh -c npm run ${BUILD_CMD}' returned a non-zero code: 1\"": "Try increasing the memory limit in your docker.\nIf you check package.json file in the superset repo here, you'll see that the maximum memory size is set to >8Gi, so you should increase the allocated memory in your docker to at least 9-10Gi.\nIf you are using Docker Desktop you can check the documentation on the links below on how to change the resource allocation:\nMac: https://docs.docker.com/desktop/settings/mac/#resources\nWindows: https://docs.docker.com/desktop/settings/windows/#resources\nLinux: https://docs.docker.com/desktop/settings/linux/#resources",
    "failed to solve: failed to compute cache key: failed to calculate checksum of ref . . . \"/app/build\": not found": "Make sure .dockerignore does not contain /app/build.",
    "How to get the operating system information from python docker container?": "The latest Python image supports different operating systems and architectures. Find OS/ARCH information by releases at https://hub.docker.com/_/python/tags. Click on a specific DIGEST link for more information.\nThe official Ubuntu Docker image does not have lsb_release. An alternative command to retrieve the distribution information:\ncat /etc/os-release\nreference: https://serverfault.com/questions/476485/do-some-debian-builds-not-have-lsb-release",
    "Can't build any golang docker images successfully, permission denied": "I used the provided examples to write two different ways of building Docker images.\nFROM golang:1.20-alpine as builder\nWORKDIR /app\nENV GOPROXY=https://goproxy.cn\nCOPY ./go.mod ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 go build -o server\nEXPOSE 8080\nCMD [\"/app/server\"]\nAND\nFROM golang:1.20-alpine as builder\nWORKDIR /app\nENV GOPROXY=https://goproxy.cn\nCOPY ./go.mod ./\nRUN go mod download\nCOPY . .\nRUN CGO_ENABLED=0 go build -o server\n\nFROM scratch\nCOPY --from=builder /app/server /opt/app/\nEXPOSE 8080\nCMD [\"/opt/app/server\"]\nYou can also check out my github.",
    "Local WordPress Development With Docker and Cron": "After struggling for some days I found out you can use host.docker.internal as an alias for localhost, please note this is a local development solution and not ready for production but using the crons like this:\n# must be ended with a new line \"LF\" (Unix) and not \"CRLF\" (Windows)\n* * * * * echo \"Current date is `date`\" > /var/log/cron\n*/5 * * * * wget -q -O - \"http://host.docker.internal/wp-load.php?import_key=SMw2F9f&import_id=4&action=trigger\"\n* * * * * wget -q -O - \"http://host.docker.internal/wp-load.php?import_key=SMw2F9f&import_id=4&action=processing\"\n# An empty line is required at the end of this file for a valid cron file.\nWorked",
    "is it possible to not override Docker Entrypoint while passing COMMAND in k8s pod defination": "You can use the Vault Secrets Operator to synchronize secrets from Vault to Kubernetes Secret resources.\nOnce you've done that, you can then expose those secrets as environment variables using envFrom or vaultFrom directives in your deployment manifests, as described in the documentation.\nThis method does not require overriding the entrypoint or arguments of your containers.\nIt looks like Vault Secrets Operator is relatively new and the documentation seems a bit slim. You can achieve similar functionality using the External Secrets Operator, which has the added advantage that it supports a variety of secret store backends.",
    "Docker container is missing files when using docker-compose": "The problem is, in the docker compose, you are bind mounting to the parent directory of the directory that has been copied in the docker file. The docker docs says: if a container directory is non-empty it is hidden behind the contents of a bind-mounted directory (i.e. you can't initialize the host directory with a container files there in this way). Therefore your copy at /bitnami/moodle/local/local_custom_api/ (in container) is hidden instead the directory is the same as D:\\Docker\\Moodle\\Moodle\n# ...\nvolumes:\n  - D:\\\\Docker\\\\Moodle\\\\Moodle:/bitnami/moodle # -> everything in the `/bitnami/moodle` is gone\n# ...\n... Similarly in the second docker-compose.yml\nThe solution is really simple: put your plugins in the D:\\Docker\\Moodle\\Moodle\\local\\local_custom_api. Also, you could instead of defining the docker file just use the default image in the docker-compose.yml:\nversion: '3.8'\nservices:\n# db...\n  moodle:\n    image: bitnami/moodle:latest\n    ports:\n      - 8080:8080\n      - 8443:8443\n    environment:\n      - MOODLE_DATABASE_HOST=mariadb\n      - MOODLE_DATABASE_USER=root\n      - MOODLE_DATABASE_PASSWORD=moodle\n      - MOODLE_DATABASE_NAME=moodle\n    volumes:\n      - D:\\\\Docker\\\\Moodle\\\\Moodle:/bitnami/moodle\n      - D:\\\\Docker\\\\Moodle\\\\MoodleData:/bitnami/moodledata\n    depends_on:\n      - mariadb\n    links:\n      - mariadb:mariadb\nThe copying in the docker file has another disadvantage during prototyping, it is just a copy and every time you change anything you must rebuild the container. On the other hand, volumes are \"synchronize\", they are the same directory visible both from the outside and the inside. So when you have new version of the plugin, you just restart the container.\nUsing (named) volumes is not necessary, see documentation for more informations what is the difference to the bind mounts (=using paths).",
    "How do I include libssl.so in my docker image?": "I was having this problem as well. It was puzzling because initially I was running my project using the rust Docker image right after building it in the same image. When I switched to a builder pattern, I was using the ubuntu image as my runner image and was unable to locate a suitable package providing this library.\nI dug around and found out that the rust image is based on the debian image and in the first layer installs the libssl1.1 package using APT. Replicating this solved my problem:\nFROM debian:latest\n\n# make sure libssl.so.1.1 is available\nRUN apt-get update && apt-get install -y libssl1.1 && apt clean && rm -rf /var/lib/apt/lists/*\n\n... # copy / run project",
    "Error: getaddrinfo ENOTFOUND trying to connect to mongo from container": "When we start a container stack through a compose-file, a network for this compose stack is created for us, and all containers are attached to this network. When we start a container through docker run ..., it is attached to the default network. Containers in different networks cannot communicate with each other. My recommendation would be to add the dronapi-container to the compose file:\nversion: '3'\nservices:\n  ...\n  drone-api:\n    bulid:\n      context: .\n      dockerfile: path/to/Dockerfile\n    ...\n    depends_on:\n      - mongodb # to start this container after mongodb has been started    \nIf we want to start the stack, we can run docker compose up -d. Notice that if the image was built before, it will not be automatically rebuilt. To rebuild the image, we can run docker compose up --build -d.\nAs an aside: I would recommend following the 12 factors for cloud-native applications (12factors.net). In particular, I would recommend to externalize the database configuration (12factors.net).",
    "How do i pass arguments to a python script in a container": "If you want arguments to be passed to the container, then use ENTRYPOINT instruction in exec form instead of CMD in your Dockerfile.\nFROM python:3.10\nWORKDIR /home/johnb\nRUN pip install pandas \nADD manual_into_ckl.py .\n#command to run \nENTRYPOINT [ \"python\", \"manual_into_ckl.py\"]\nThen run the container as\ndocker run test --manual test.xml --ckl rhel7.ckl\nThe additional arguments passed to docker run will be passed as args to the entrypoint specified in the dockerfile. The resulting command would look like\npython manual_into_ckl.py --manual test.xml --ckl rhel7.ckl",
    "error TS5083: Cannot read file '/tsconfig.json'": "You need to copy your tsconfig.json to your Docker image.\nAdd the following after line 5:\nCOPY tsconfig.json ./\nNotes:\nYou probably need to also copy the nodemon.json\nAt the line 10 you copy all the current directory to your image, ensure your .dockerignore file contain a rule to exclude the node_modules/, dist/ folders.",
    "Adding ASPNETCORE_URL in Dockerfile having no effect": "Add your ENV ASPNETCORE_URLS=http://+:5000;https://+:5001 to the\nFROM base AS final\nand also modify launchSettings.json \"Docker\" by adding:\n\"environmentVariables\": {\n    \"ASPNETCORE_URLS\": \"https://+:5001;http://+:5000\"\n}",
    "Fixing security vulnerabilities in docker image": "It looks like the DockerFile is trying to a specific version of golang by hand into \"/usr/local\" rather than using the Debian package manager. According to the info at https://security-tracker.debian.org/tracker/CVE-2021-38297, that bug is fixed in 1.17.3-3 and the Dockerfile are using 1.19.1. So perhaps there is an old golang installation in the base image ... and that is what the scanner is picking up. Check that, and if necessary apt install a newer version.\nLikewise, https://security-tracker.debian.org/tracker/CVE-2022-23806 should be fixed by a newer version of golang. See the CVE link for versions.\nhttps://security-tracker.debian.org/tracker/CVE-2015-20107 could be fixed by upgrading to Python 3.10.6-1 or later.\nhttps://security-tracker.debian.org/tracker/CVE-2019-19814 doesn't appear to have a fix from upstream, so there is nothing you can do about it except not use f2fs.\nhttps://security-tracker.debian.org/tracker/CVE-2022-29599 can be fixed by updating the maven-shared-utils package; see the CVE link for versions.\nhttps://security-tracker.debian.org/tracker/CVE-2022-1996 has a fix upstream but it is awaiting triage by the Debian team.\nIn summary, some of the vulnerabilities can be fixed, but for a couple of them no fix is readily available. So:\nApply the fixes that are available.\nThen read the CVEs and accompanying explanations and 1) make a judgement whether they represent a risk that you can take, and 2) figure out if you can mitigate the risk; e.g. by locking down access to the running Docker container.",
    "Variable passed by 'docker run' to CMD of Dockerfile is not resolved [duplicate]": "There are 2 forms of the CMD statement, like you show.\nOne is the exec form, which is the one that doesn't work. It's usually preferred because it doesn't start a shell first to run the command, so it uses a little less resources.\nThe other is the shell form, which is the one that works. The reason that it works is that you need a shell to replace environment variables in a command. The shell form starts up a shell, that then executes your command.\nAs you've seen in your echo example, you can also make the exec form work by running the shell manually.\nYou can read more about the 2 forms here: https://docs.docker.com/engine/reference/builder/#cmd\nIn short, the examples that don't work, don't work because there's no shell to insert the value of the environment variable in the command.",
    "How to connect a Dev Container to another Container?": "Assumming the docker network driver is bridge (Default).\nIf the script is runing this line to get env in your devcontainer as below.\nconst connectionString = (\n    process.env.TEST_MYSQL_URI_MIGRATE || 'mysql://root:root@localhost:3306/tests-migrate'\n  ).replace('tests-migrate', 'tests-migrate-dev')\n`\nThe localhost in the connection string means the localhost in your devcontainer but not your host machine.\nYou should access the localhost of your host machine instead.\nThe fix is set the TEST_MYSQL_URI_MIGRATE environment variable instead like\nTEST_MYSQL_URI_MIGRATE=mysql://root:root@host.docker.internal:3306/tests-migrate\nFor the details how to access the localhost of host machine, please read this question",
    "How to add ping command to my pod using dockerfile?": "This works to basically install any package you want, you can simply add the RUN options in the base or final stage, for a Debian/Ubuntu based image it looks like this:\nFROM base AS final\nWORKDIR /\nCOPY --from=builder /workspace/manager .\nCOPY --from=builder /workspace/opt ./opt\n######################\n# Installing ping\nRUN apt-get update\nRUN apt-get install -y iputils-ping # <- change this according to your image\n######################\nRUN chgrp 0 /manager \\    \n    && chmod g=u /manager\n\nRUN chgrp 0 /opt \\\n    && chmod g=u /opt\n\nENTRYPOINT [\"/manager\",\"./ping\"]\nAccording to this article from linuxshelltips, these are the ways to install ping:\n$ sudo apt install iputils-ping    [On Debian, Ubuntu and Mint]\n$ sudo yum install iputils         [On RHEL/CentOS/Fedora and Rocky Linux/AlmaLinux]\n$ sudo emerge -a net-misc/iputils  [On Gentoo Linux]\n$ sudo pacman -S iputils           [On Arch Linux]\n$ sudo zypper install iputils      [On OpenSUSE]   \nI could not quickly figure out which distribution the RedHat UBI image is based on but my guess would be RHEL?",
    "Running NGINX and PHP-FPM as separated containers in Docker with a shared volume": "Named volumes, on their own, are not a reliable way to publish files from one container to another. The easiest way to see this is to make some change in your html directory and run docker-compose build; docker-compose up -d. What you'll see in this case is that the old contents of the code volume are used in the Nginx container without updates, and in fact the old contents of the volume hide the updated code in your application container. For things that aren't Docker named volumes, including Docker bind-mounts and Kubernetes volumes, you'll just get an empty directory with nothing copied into it.\nThe absolute easiest way to address this is to let the application serve its own static assets. The mechanism to do this is framework-specific (and I'm not well-versed in PHP), but if you can do this then your Nginx configuration just has a proxy_pass or fastcgi_pass directive, and it doesn't have any files on its own to serve. That eliminates the need for this volume.\nIf that's not an option, then you can create a second image for the reverse proxy that includes the static assets.\nFROM nginx:1.17-alpine\nCOPY ./html/ /usr/share/nginx/html/\n# use the CMD from the base image, no need to repeat it\nNow you're not \"sharing files\" per se, but the application and the reverse proxy both contain the same static files built from the same source tree. This means you don't need volumes: in the Compose setup, except maybe to do things like inject configuration. (It's likely reasonable to COPY this into the image as well.)\nversion: \"3.8\"\nservices:\n  server:\n    build:\n      context: .\n      dockerfile: Dockerfile.nginx\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/conf.d/default.conf\n    depends_on:\n      - app\n    restart: always\n\n  app:\n    build: .\n    restart: always",
    "How to run a Cenos7 docker image on an ARM based Mac": "Ran into your question searching for something else, but you might try the following (this was probably not available when you wrote your question):\nIn docker for Mac, go to Settings --> Features in development, and enable 'Use Rosetta ...' setting.\nThis worked for me when setting up dev-containers (another use-case where I needed Linux images)",
    "redundant eval $(opam env) needed in Dockerfile [duplicate]": "Prebuilt versions of Coq (within Debian)\nAs mentioned in the comments, the Docker-Coq repository gathers prebuilt versions of Coq, e.g.:\ndocker pull coqorg/coq:8.15.2\nThe list of all tags is available at this URL:\nhttps://hub.docker.com/r/coqorg/coq#supported-tags\nand the related documentation is in this Wiki:\nhttps://github.com/coq-community/docker-coq/wiki\nA self-contained Dockerfile as an \"installation tutorial\" for Ubuntu\nTo address the specific use case mentioned by the OP, here is a comprehensive Dockerfile that solves the main issue mentioned in the question (Why do I need to run eval $(opam env) again when I execute the docker), along with several fixes that are necessary to comply with standard Dockerfile and opam guidelines (though don't hinder the use case at stake):\n##############\n#            #\n# image name #\n#            #\n##############\nFROM ubuntu:22.04\n\n#################\n#               #\n# bash > sh ... #\n#               #\n#################\nSHELL [\"/bin/bash\", \"--login\", \"-c\"]\n\n############################\n#                          #\n# minimal set of utilities #\n#                          #\n############################\n# Run the following as root:\nRUN apt-get update -y -q \\\n && apt-get install -y -q --no-install-recommends \\\n    # alphabetical order advised for long package lists to ease review & update\n    ca-certificates \\\n    curl \\\n    libgmp-dev \\\n    m4 \\\n    ocaml \\\n    opam \\\n    rsync \\\n    sudo \\\n#########################################\n#                                       #\n# Docker-specific cleanup to earn space #\n#                                       #\n#########################################\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n\n#####################\n#                   #\n# add non-root user #\n# (with sudo perms) #\n#                   #\n#####################\nARG coq_uid=1000\nARG coq_gid=${coq_uid}\nRUN groupadd -g ${coq_gid} coq \\\n && useradd --no-log-init -m -s /bin/bash -g coq -G sudo -p '' -u ${coq_uid} coq \\\n && mkdir -p -v /home/coq/bin /home/coq/.local/bin \\\n && chown coq:coq /home/coq/bin /home/coq/.local /home/coq/.local/bin\n\n###########################################\n#                                         #\n# opam is the easiest way to install coqc #\n#                                         #\n###########################################\nUSER coq\nWORKDIR /home/coq\nRUN opam init --auto-setup --yes --bare --disable-sandboxing \\\n && opam switch create system ocaml-system \\\n && eval $(opam env) \\\n && opam repo add --all-switches --set-default coq-released https://coq.inria.fr/opam/released \\\n#########################################\n#                                       #\n# install coqc, takes around 10 minutes #\n#                                       #\n#########################################\n && opam pin add -y -k version -j \"$(nproc)\" coq 8.15.2 \\\n#########################################\n#                                       #\n# Docker-specific cleanup to earn space #\n#                                       #\n#########################################\n && opam clean -a -c -s --logs\n\n###################################\n#                                 #\n# Automate the 'eval $(opam env)' #\n#                                 #\n###################################\nENTRYPOINT [\"opam\", \"exec\", \"--\"]\nCMD [\"/bin/bash\", \"--login\"]\nSummary of changes between both Dockerfiles / related remarks\nIn the Dockerfile above, the following fixes have been applied:\nMerge consecutive RUN commands with && to avoid the issue raised in this SO question: Purpose of specifying several UNIX commands in a single RUN instruction in Dockerfile.\nAdd CLI flags -q and --no-install-recommends to apt-get commands, so that the output is less verbose, and the installed packages only include those specified (and mandatory dependencies).\nPut the APT packages in alphabetical order, to ease review and update.\nAdd a non-root user (named coq here) so that opam does not complain anymore with the usual [WARNING] Running as root is not recommended.\nOf course, this step can be skipped in a non-Docker installation as we always have some regular $USER installed on a standard workstation\u2026\nReplace the opam init command with:\nopam init --auto-setup --yes --bare --disable-sandboxing \\\n&& opam switch create system ocaml-system\nso the ~/.profile script is updated automatically (thanks to --auto-setup) and the name of the switch (system) and its content (ocaml-system) is explicit.\nAdd opam repo add --all-switches --set-default coq-released https://coq.inria.fr/opam/released so that one can then install community packages if need be, e.g.:\nopam install -y -v -j \"$(nproc)\" coq-mathcomp-ssreflect\nPass the -j \"$(nproc)\" option to parallelize and speedup the installation, depending on the number of cores of the ambient system.\nAdd optional, Docker-specific commands apt-get clean && rm -rf /var/lib/apt/lists/* and opam clean -a -c -s --logs to reduce the size of the Docker layers.\nAnswer to the main issue raised in the question\nEach time a new shell (or a RUN command, etc.) is launched, the eval $(opam env) command is necessary to update the PATH etc.\nThere are two ways to ensure that this command eval $(opam env) is done automatically:\neither wrap the command with opam exec -- \u2026\nor run /bin/bash --login, so that the ~/.profile init script is sourced (indeed, thanks to opam init --auto-setup, a line in charge of initializing the ambient shell with proper environement variables and so on, was appended by opam in this script).\nFor completeness, both solutions have been implemented in this proposed Dockerfile (and we can just keep both without any specific drawback).\nTo test all this\n$ docker build -t coq-image .\n# or better $ docker build -t coq-image --build-arg=coq_uid=\"$(id -u)\" --build-arg=coq_gid=\"$(id -g)\" .\n\n$ docker run --name=coq -it coq-image\n# or to mount the current directory\n# $ docker run --name=coq -it -v \"$PWD:$PWD\" -w \"$PWD\" coq-image\n\n  # Ctrl+D\n\n$ docker start -ai coq  # to restart the container\n\n  # Ctrl+D\n\n$ docker rm coq         # to remove the container",
    "Unable to run `apt-get` commands in Dockerfile \"executor failed running\"": "If we try building from the following Dockerfile:\nFROM openjdk:16-bullseye AS build-env-java\n\n# Install git as additional requirement\nRUN apt-get update && \\\n    apt-get upgrade  && \\\n    apt-get install git && \\\n    apt-get install bash\nIt will fail with this error:\n[...]\nAfter this operation, 15.4 kB of additional disk space will be used.\nDo you want to continue? [Y/n] Abort.\nThe command '/bin/sh -c apt-get update &&     apt-get upgrade  &&     apt-get install git &&     apt-get install bash' returned a non-zero code: 1\nAs you can see apt-get is attempting to prompt for interactive input, but because it's not an interactive environment the command fails. We need to tell apt-get to install without prompting by adding the -y flag to the upgrade command. The install command will need the same treatment:\nFROM openjdk:16-bullseye AS build-env-java\n\n# Install git as additional requirement\nRUN apt-get update && \\\n    apt-get -y upgrade  && \\\n    apt-get -y install git bash\nI've consolidated your multiple apt-get install commands into a single command (because that will generally be faster), but you can of course continue to use multiple commands if you wish.\nThis Dockerfile builds without errors.",
    "\"no main manifest attribute, in server.war\" not using a main class": "From the java tool specs\nThe java command starts a Java application. It does this by starting the Java Runtime Environment (JRE), loading the specified class, and calling that class's main() method. The method must be declared public and static, it must not return any value, and it must accept a String array as a parameter. The method declaration has the following form:\n\n    public static void main(String[] args)\n\nBy default, the first argument that is not an option of the java command is the fully qualified name of the class to be called. If the -jar option is specified, its argument is the name of the JAR file containing class and resource files for the application. The startup class must be indicated by the Main-Class manifest header in its source code.\nYou're trying to run a war, and unlike a jar it cannot be run standalone but it requires a container; in your case TomEE.\nhttps://github.com/tomitribe/docker-tomee descibes how you should start tomee, and also how to add your war to the image.",
    "run two python scripts with docker compose": "In the Bourne shell, in general, you can run two commands in sequence by putting && between them. It sounds like you're already aware of this.\n# without Docker, at a normal shell prompt\npython test.py && python main.py\nThe Dockerfile CMD has two syntactic forms. The JSON-array form does not run a shell, and so it is slightly more efficient and has slightly more consistent escaping rules. If it's not a JSON array then Docker automatically runs it via a shell. So for your use you can use the shell form:\nCMD python test.py && python main.py\nIn comments to other answers you ask about providing this as an override in the docker-compose.yml file. Compose will not normally run a shell for you, so you need to explicitly specify it as part of the command: override.\ncommand: /bin/sh -c 'python test.py && python main.py'\nYour Dockerfile should generally specify a CMD and the docker-compose.yml often will not include a command:. This makes it easier to run the image in other contexts (via docker run without Compose; in Kubernetes) since you won't have to retype the command every different way you want to run the container. The entrypoint wrapper pattern highlighted in @sytech's answer is very useful in general and it's easy to add to a container that uses a CMD without an ENTRYPOINT; but it requires the Dockerfile to use CMD as a normal well-formed shell command.",
    "Sourcing a shell script inside Dockerfile to injecting environment variables to the container": "If you have a script that gets secrets from Vault, you probably need to re-run it every time the container starts. You don't want to compromise the secrets by putting them in a Docker image where they can be easily extracted, and you don't want an old version of a credential \"baked into\" an image if it changes in Vault.\nYou can use an entrypoint wrapper script to run this when the container starts up. This is a script you set as the container ENTRYPOINT; it does first-time setup like setting dynamic environment variables and then runs whatever is the container CMD.\n#!/bin/sh\n# entrypoint.sh\n\n# Get a set of credentials from Vault.\n. /build/vault-util.sh\n\n# Run the main container command.\nexec \"$@\"\nIn your Dockerfile, you need to make sure you COPY this in and set it as the ENTRYPOINT, but you don't need to immediately RUN it.\nCOPY vault-util.sh entrypoint.sh /build\nENTRYPOINT [\"/build/entrypoint.sh\"] # must be JSON-array syntax\nCMD same command as originally\nYou won't be able to see the secrets with tools like docker inspect (this is good!). But if you want to you can run a test container to dump out the results of this setup. For example,\ndocker run --rm ... your-image env\nreplaces the Dockerfile's CMD with env, which prints out the environment and exits. This gets passed as arguments to the entrypoint, so first it runs the script to fetch environment variables and then runs env and then exits.",
    "I did docker system prune to delete unused images but deleted everything is there way to undo this?": "Hello, sorry for my answer but its a nope...\nWhen you use the prune command you are prompted to know if you are sure or not, sadly this is a last warning before the drama :D\nHope you still got the dockerfile to rebuild your own images, if they came from internet, just go back from where you get them :D (Try your browser history if you do not remember)\nKeep the smile bro ! :) https://docs.docker.com/engine/reference/commandline/system_prune/",
    "Dockerfile: unable to copy file": "Multiple resources may be specified but the paths of files and directories will be interpreted as relative to the source of the context of the build\nTry to firstly copy the file to the local directory where the Dockerfile is located\nThen\nCOPY libc.musl-x86_64.so.1 /lib/ld-linux-x86-64.so.2",
    "Unable to run serve -s build inside docker image for my react application": "You are only getting the error on serve because the issue comes with incompatibility with the serve package, most likely due to the node version, carbon is quite old and no longer being actively maintained. Check which node version you are using on your local machine and use that version for your docker base.\nYou may also do COPY build build instead of COPY . . to only copy the build folder, which is the only folder you need. This will allow the image to build faster.",
    "Dockerfile LABEL from executable version": "You're not going to be able set a label using content generated dynamically from something installed inside the container. You can set a label dynamically from your host environment using a build argument, like this:\nFROM docker.io/alpine:latest\n\nARG application_version=1\nLABEL application_version=${application_version}\nIf I build the container like this:\ndocker build -t myimage .\nI will have:\n$ docker inspect image myimage | jq '.[0].Config.Labels'\n{\n  \"application_version\": \"1\"\n}\nWhereas if I build the container like this:\ndocker build --build-arg application_version=3 -t myimage .\nI will have:\n$ docker inspect image myimage | jq '.[0].Config.Labels'\n{\n  \"application_version\": \"3\"\n}\nIf you use the same build argument value to select the version of the software you install in your container, you can ensure that the installed version and the label match.",
    "Use Python and Node.js in the same Dockerfile and create one image that I cloud use both": "There are some errors on your defined image:\n1st. You are trying to use Python from the node image, but that image doesn't have python installed, so that won't work.\n2nd. Even if you install your Python dependencies on the first stage of a multi-stage build, if you don't pass those dependencies to the next stage, it's like you didn't do anything.\nThere are few ways of reaching what you want, but I will tell you what I would do.\nFirst, you will need to agree in which Python version you want to use in your project, let's say you want to use Python 3.10.\nThen, you will need to create a venv within that build container, since this is what you will pass afterwards to your runtime container:\nFROM python:3.10 as build\n\nWORKDIR /opt/app\nRUN python -m venv /opt/app/venv\nENV PATH=\"/opt/app/venv/bin:$PATH\"\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nNow you will have all your dependencies installed on your venv, so you can carry them to the runtime container (where you will need to install the same Python version that you used in your build image).\nFROM node:14\n\nRUN apt update \\\n    && apt install software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt update \\\n    && apt install python3.10\n\nWORKDIR /opt/app\nCOPY --from=build /opt/app/venv /venv\n\nENV PATH=\"/opt/app/venv/bin:$PATH\"\nENV NODE_ENV=container\n\nCOPY package-*.json .\nRUN npm install\n\nCOPY . .\nEXPOSE 4001\nCMD npm start\nWith that you will have Python 3.10 installed on your runtime Node image and with the dependencies which you already downloaded/compiled on your Python 3.10 build image.",
    "Identify FROM line from Docker Image": "The digest for a layer blob and the digest for the image manifest are two different things, the manifest contains the digests for the layer blobs so that if any layer changes, the digest for the manifest also changes. That's what provides the immutability of images. Inspecting the image you found:\n$ regctl manifest get localhost:5000/library/alpine:3.11.6\n{\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n  \"config\": {\n    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n    \"size\": 1507,\n    \"digest\": \"sha256:f70734b6a266dcb5f44c383274821207885b549b75c8e119404917a61335981a\"\n  },\n  \"layers\": [\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 2813316,\n      \"digest\": \"sha256:cbdbe7a5bc2a134ca8ec91be58565ec07d037386d1f1d8385412d224deafca08\"\n    }\n  ]\n}\nYou can see your one blob, but to pull the image, you need the manifest digest, and unfortunately there's no way to list all the manifests that point to a specific blob (at least not yet, we may get that as part of OCI reference types, but that's a ways off). However, the manifest itself has a digest that matches what you would see when inspecting the image:\n$ regctl manifest get localhost:5000/library/alpine:3.11.6 --format raw-body | sha256sum\n39eda93d15866957feaee28f8fc5adb545276a64147445c64992ef69804dbf01  -\n\n$ regctl manifest get localhost:5000/library/alpine:3.11.6 --list --format raw-body | sha256sum\n9a839e63dad54c3a6d1834e29692c8492d93f90c59c978c1ed79109ea4fb9a54  -\nAs for why there are two digests above, the alpine image is a multi-platform image, so there's a digest for the parent manifest list, and then a digest for each image manifest. Here's what the manifest list looks like:\n$ regctl manifest get localhost:5000/library/alpine:3.11.6 --list --format raw-body | jq .\n{\n  \"manifests\": [\n    {\n      \"digest\": \"sha256:39eda93d15866957feaee28f8fc5adb545276a64147445c64992ef69804dbf01\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"amd64\",\n        \"os\": \"linux\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:0ff8a9dffabb5ed8dcba4ee898f62683305b75b4086f433ee722db99138f4f53\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"arm\",\n        \"os\": \"linux\",\n        \"variant\": \"v6\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:19c4e520fa84832d6deab48cd911067e6d8b0a9fa73fc054c7b9031f1d89e4cf\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"arm\",\n        \"os\": \"linux\",\n        \"variant\": \"v7\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:ad295e950e71627e9d0d14cdc533f4031d42edae31ab57a841c5b9588eacc280\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"arm64\",\n        \"os\": \"linux\",\n        \"variant\": \"v8\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:b28e271d721b3f6377cb5bae6cd4506d2736e77ef6f70ed9b0c4716da8bdf17c\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"386\",\n        \"os\": \"linux\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:e095eb9ac24e21bf2621f4d243274197ef12b91c67cde023092301b2db1e073c\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"ppc64le\",\n        \"os\": \"linux\"\n      },\n      \"size\": 528\n    },\n    {\n      \"digest\": \"sha256:41ba0806c6113064dd4cff12212eea3088f40ae23f182763ccc07f430b3a52f8\",\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"platform\": {\n        \"architecture\": \"s390x\",\n        \"os\": \"linux\"\n      },\n      \"size\": 528\n    }\n  ],\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n  \"schemaVersion\": 2\n}\nOnce we have the manifest list, either by tag or digest, we can walk the structure to the manifests and contained blobs. But the blobs don't contain digest of the parent (adding it would change the digest of the blob, which changes the digest of the parent, and you get a circular dependency).\nOne thing that was recently added to the list of OCI standard annotations is org.opencontainers.image.base.digest and org.opencontainers.image.base.tag, which if implemented would allow users of an image to identify their base image by both tag and digest, which can be useful for determining when an image needs to be rebuilt (when the tag no longer refers to the same digest).",
    "Dockerfile RUN layers vs script": "In terms of the final image filesystem, you will notice no difference if you RUN the commands directly, or RUN a script, or have multiple RUN commands. The number of layers and the size of the command string doesn't really make any difference at all.\nWhat can you observe?\nParticularly on the \"classic\" Docker build system, each RUN command becomes an image layer. In your example, you RUN yum install && ... && <some cleanup>; if this was split into multiple RUN commands then the un-cleaned-up content would be committed as part of the image and takes up space even though it's removed in a later layer.\n\"More layers\" isn't necessarily bad on its own, unless you have so many layers that you hit an internal limit. The only real downside here is creating a layer with content that you're planning to delete, in which case its space will still be in the final image.\nAs a more specific example of this, there's an occasional pattern where an image installs some development-only packages, runs an installation step, and uninstalls the packages. An Alpine-based example might look like\nRUN apk add --virtual .build-deps \\\n      gcc make \\\n && make \\\n && make install \\\n && apk del .build-deps\nIn this case you must run the \"install\" and \"uninstall\" in the same RUN command; otherwise Docker will create a layer that includes the build-only packages.\n(A multi-stage build may be an easier way to accomplish the same goal of needing build-only tools, but not including them in the final image.)\nThe actual text of the RUN command is visible in docker history and similar inspection commands.\nAnd...that's kind of it. If you think it's more maintainable to keep the installation steps in a separate script (maybe you have some way to use the same script in a non-Docker context) then go for it. I'd generally default to keeping the steps spelled out in RUN commands, and in general try to keep those setup steps as light-weight as possible.",
    "How can I tell docker to create a container from a local image, without looking at the docker hub if no local image is found with that name?": "You don't have to name your images something esoteric.\nUse a namespace\nJust register a namespace on docker hub, and then use that in your image names. E.g., I am larsks on Docker Hub. Nobody but me can create an image in the larsks namespace, so if I name a local image larsks/example, I know that's never going to resolve on Docker Hub unless I put it there.\nUse a bogus registry name\nMaybe you don't want to register with Docker Hub.\nRecall that the fully qualified form of an image name is registry/namespace/repository:tag. If you name your local images with a nonexistent registry, Docker won't be able to pull them from anywhere. E.g., if I name something dne/larsks/example (dne as in \"does not exist\", but anything works that isn't a valid hostname), Docker will never be able to pull this image.",
    "Is there a better way to copy files and folders to a docker container?": "This is what I came up with helpful comments from @TahaNaqvi and @DavidMaze. This is much easier to read.\n# Copy files to container\nWORKDIR /opt/build_area/\n\nCOPY src/ ./src/\nCOPY public/  ./public/\n\nCOPY app.js           \\\n     .npmrc           \\\n     doc* .en*        \\\n     tag-push.sh      \\\n     package.json     \\\n     react-0.3.1.tgz  /opt/build_area/",
    "SBT not running from root directory": "I had to change my work directory away from default / to get this to work. Try below (WORKDIR changes folder, cd in the commands likely have the same effect) could. This builds and runs with sbt command for me.\nFROM openjdk:8 as build\nENV SBT_VERSION \"1.5.8\"\nENV APP_HOME /service\nWORKDIR $APP_HOME\nRUN \\\n  apt-get update && \\\n  apt-get install apt-transport-https curl gnupg -yqq && \\\n  echo \"deb https://repo.scala-sbt.org/scalasbt/debian all main\" | tee /etc/apt/sources.list.d/sbt.list && \\\n  echo \"deb https://repo.scala-sbt.org/scalasbt/debian /\" | tee /etc/apt/sources.list.d/sbt_old.list && \\\n  curl -sL \"https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823\" | gpg --no-default-keyring --keyring gnupg-ring:/etc/apt/trusted.gpg.d/scalasbt-release.gpg --import && \\\n  chmod 644 /etc/apt/trusted.gpg.d/scalasbt-release.gpg && \\\n  apt-get update && \\\n  apt-get install sbt=$SBT_VERSION && \\\n  sbt sbtVersion\nroot@a94ff4797239:/service# sbt\n[info] welcome to sbt 1.5.8 (Oracle Corporation Java 1.8.0_312)\n[info] loading project definition from /service/project\n[info] set current project to service (in build file:/service/)\n[info] sbt server started at local:///root/.sbt/1.0/server/c75bcef1d951ec508da8/sock\n[info] started sbt server\nsbt:service>",
    "Docker-compose with WSL 2 React app not hot reloading on changes": "You can use WebPack packaging tool, by the way Hot Module Reload is not using on production in most case(Compact bundle will be running). \"HMR is not intended for use in production, meaning it should only be used in development. See the building for production guide for more information.\"\nReference: https://webpack.js.org/guides/hot-module-replacement/",
    "How to access gradle test reports from a docker container": "I've hit this same issue. Unfortunately volumes are only availible to docker run, they aren't part of the docker build portion of the docker compose process.\nI even looked into the new RUN --mount=type=bind,source=.,target=./build/reports,rw gradle build and while in the container I can see (with a RUN ls ./build/reports) the reports being generated, but that mount (even as RW) only puts files in the container as a layer and they never appear on the host machine.\nThere is a hacky way to recover those test results, in the docker output, just above the failure you'll see this line ---> Running in f98e14dd1ee4. This is the layer ID, with that value you can copy from the failed layer to the local machine using\n$ docker cp f98e14dd1ee4:/tmp/build/reports ./\n\n$ ls reports/\ncheckstyle/ jacoco/     tests/\nIt shouldn't be to difficult to automate this kind of recover but it feels like it would be fragile automated.\nI'm very interested if anyone has a better solution to this, even with an alternative to docker. I know I can build the container with gradle but I'd rather everything happens inside the container build process to keep the build environment defined as code.",
    "Docker - start a container at random time": "If \"randomness\" is part of your application behaviour, then you should contain this logic inside one of services / containers, so this is no-brainer. You are running entire stack with docker-compose and it just works.\nOther than that Crontab or other external scheduler (like kubernetes cron jobs) is the way of doing that if running those three containers on random time once a day is your requirement of using those docker services.",
    "What is the difference between target and published ports into docker-compose.yml container definition?": "Publishing a port creates a port forward from the host into the container. The published port is the externally visible port on the host. The target port is the destination inside the container for this port forward. From the documentation:\ntarget: the port inside the container\npublished: the publicly exposed port\nprotocol: the port protocol (tcp or udp)\nmode: host for publishing a host port on each node, or ingress for a swarm mode port to be load balanced.\nhttps://docs.docker.com/compose/compose-file/compose-file-v3/#ports",
    "Module not found in Docker container": "When you run python ./alpha-api/app.py, Python will only have all files within ./alpha-api in its path.\nSetting your PYTHONPATH to ./ (or ${LAMBDA_TASK_ROOT}) prior to running the file could help.",
    "Overwrite entrypoint in Dockerfile without using the command \"docker run\"": "First you may set the command as suggested in https://stackoverflow.com/a/69242677/15087442. This is desribed in detail in the kubernetes documentation: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/\nBut there's also another option: You can change the entrypoint of the docker image. If it is your own docker image that's a no-brainer. But let's consider you are using someone elses image. Then you can modify it by building another image out of it. Dockerfile:\nFROM old_image:latest\n\nENTRYPOINT python3 main_script.py DEV\nAnd then you build the image with docker build ., push it to your preferred docker repository with docker push and use that one instead of the original one.\nThis is mostly interesting if you also want to modify other things in the image.",
    "How to `docker-compose run` outside the working directory": "You want to use the --workdir (or -w) option of the docker-compose run command.\nSee the official documentation of the command here: https://docs.docker.com/compose/reference/run/\nFor instance, given your above example:\ndocker-compose run my_container -w /src my-task",
    "Error: can't chdir to 'app' when using docker start container [closed]": "You don't need to change directory. --chdir app parameter is unnecessary. Because you already inside /app directory (Do you remember WORKDIR /app inside Dockerfile?).\nIn other way you may change directory (if default will not work) to current work directory by using --chdir /app or --chdir .\nJust change it to\n#!/bin/sh\n\ngunicorn app:app -w 2 --threads 2 -b 0.0.0.0:80\n\n# OR\n\ngunicorn --chdir /app app:app -w 2 --threads 2 -b 0.0.0.0:80\n\n# OR\n\ngunicorn --chdir . app:app -w 2 --threads 2 -b 0.0.0.0:80\nRUN chmod 777 /app also unnecessary",
    "Dockerfile: Replacing file contents with sed replaces value with nothing": "When Compose runs an image, it runs in two stages. First it builds the image if required; this only uses the settings in the build: block, but not any of the others. Then it runs the built image with the remaining settings. In this sequence, that means first the image is built with the configuration files, but the environment: settings are only considered afterwards.\nSince details like the database credentials are runtime settings, you probably don't want to rebuild the image when they change. That means you need to write a script to rewrite the file when the container starts, then run the main container CMD.\n#!/bin/sh\n\n# Rewrite the config file\nsed -i.bak \\\n    -e \"s|database_user|${MONGO_INITDB_ROOT_USERNAME}|g\" \\\n    -e \"s/database_password/${MONGO_INITDB_ROOT_PASSWORD}/g\" \\\n    -e \"s;database_db;${MONGO_INITDB_DATABASE};g\" \\\n    /docker-entrypoint-initdb.d/init.js\n\n# Run the main container CMD (but see below)\nexec \"$@\"\nIn your Dockerfile, you would typically make this script be the ENTRYPOINT; leave the CMD as it is. You don't need to mention any of the environment variables.\nHowever, there's one further complication. Since you're extending a Docker Hub image, it comes with its own ENTRYPOINT and CMD. Setting ENTRYPOINT resets CMD. Look up the image's page on Docker Hub, click \"the full list of tags\" link, then click the link for your specific tag; that takes you to the image's Dockerfile which ends with\n# from the standard mongo:4.4.6 Dockerfile\nENTRYPOINT [\"docker-entrypoint.sh\"]\nCMD [\"mongod\"]\nThat means your entrypoint script needs to re-run the original entrypoint script\n# (instead of \"run the main container CMD\")\n# Run the original image's ENTRYPOINT\nexec docker-entrypoint.sh \"$@\"\nIn your Dockerfile, you need to COPY your custom entrypoint script in, and repeat the original image's CMD.\nFROM mongo:4.4.6\n\nCOPY /mongo-init/init.js /docker-entrypoint-initdb.d/\nCOPY custom-entrypoint.sh /usr/local/bin\n\nENTRYPOINT [\"custom-entrypoint.sh\"] # must be JSON-array form\nCMD [\"mongod\"]\nYou don't need to change the docker-compose.yml file at all. You can double-check that this works by launching a temporary container; the override command replaces CMD but not ENTRYPOINT, so it it runs after the setup in the entrypoint script.\ndocker-compose run --rm database \\\n  cat /docker-entrypoint-initdb.d/init.js",
    "Dockerfile : no such file or directory": "You need to add the static and views directory to your final image, for that add the following 2 line before USER appuser:\nCOPY --from=builder /usr/src/app/cmd/web/views /web/views\nCOPY --from-builder /usr/src/app/cmd/web/static /web/static",
    "Why are ARGS specified in `docker-compose.yml` available in `Dockerfile` for `docker compose run` but not `docker compose build`?": "Build args were only recently added to compose-cli. Most likely that change hasn't reached the version of docker compose you're running. You can use docker-compose build (with a -) until this feature reaches your install.",
    "Docker container (aspnet:5.0-alpine3.13) unable to download a file using curl": "A solution that worked for me was to add custom DNSs to docker configuration file /etc/docker/daemon.json like follow:\n{\n    \"dns\": [\n        \"1.1.1.1\",\n        \"8.8.8.8\"\n     ]\n}\nI hope it may help you.\nRegards.",
    "Combining VOLUME + docker run -v": "You probably don't want VOLUME in your Dockerfile. It's not necessary to mount files or directories at runtime, and it has confusing side effects like making subsequent RUN commands silently lose state.\nIf an image does have a VOLUME, and you don't mount anything else there when you start the container, Docker will create an anonymous volume and mount it for you. This can result in space leaks if you don't clean these volumes up.\nYou can use a docker run -v option on any container directory regardless of whether or not it's declared as a VOLUME.\nIf you docker run -v /host/path:/container/path, the two directories are actually the same; nothing is copied, and writes to one are (supposed to be) immediately visible on the other.\ndocker run -v /host/path:/container/path bind mounts aren't visible in /var/lib/docker at all.\nYou shouldn't usually be looking at content in /var/lib/docker (and can't if you're not on a native-Linux host). If you need to access the volume file content directly, use a bind mount rather than a named or anonymous volume.\nBind mounts like you've shown are appropriate for injecting config files into containers, and for reading log files back out. Named volumes are appropriate for stateful applications' storage, like the data for a MySQL database. Neither type of volume is appropriate for code or libraries; build these directly into Docker images instead.",
    "Docker: How to set environment variables from file during build?": "ENV directive does not allow to parse a file like env.list, as pointed out. But even if it did, the resulting environment variables would still be saved in the final image, passwords included.\nThe correct approach to my knowledge is to set the passwords at runtime with \"docker run\", when this image runs, or when the child image runs via \"docker run\".\nIf the credentials are required while the image is built, I would pass them via the ARG directive so that they can be reference as shell variables in the Dockerfile but not saved in the final image:\nARG VAR\nFROM image\n\nRUN echo ${VAR}\netc...\nwhich can run as:\ndocker build --build-arg VAR=value ...",
    "Docker-compose: node_modules mouting as volume": "/app/node_modules creates a directory inside the container and the Docker Engine automatically creates an anonymous volume for this (i.e. it should will probably be empty). This is from the docs about the compose file spec in the \"Short Syntax\" section.\n./frontend/node_modules:/app/node_modules creates a bind mount. The ./frontend/node_modules directory from your host machine will be shared with the container.\nIn response to followups regarding why using /app/node_modules works but the other syntax does not:\nYour yarn install command creates a node_modules folder inside the Docker image. This created folder conflicts with the existing frontend/node_modules folder you have locally when trying to run with ./frontend/node_modules:/app/node_modules. When you specify /app/node_modules, the container uses the directory created during the build step.",
    "How to pass python command to dockerfile": "The dockerfile 'builds' an image -- you should/must not run your application during the build process. You want your application to run only when the container runs.\nChange your dockerfile to look like this:\nFROM python:3.8\nWORKDIR /pyapp/\nCOPY app/* app/\nCOPY . .\nRUN pip install -r requirements.txt\nCMD [\"python3\", \"app/main.py\", \"start\", \"--config\", \"config.yml\"]\nThis CMD line tells docker that when it runs the container, it should run this command within it. You can build it like this:\ndocker build --tag myPythonApp .\nAnd run it like this\ndocker run -it --rm myPythonApp\nYou have added some output in the comments that suggests that this container is listening on port 9000. You can expose this port on the host like this:\ndocker run -it --rm -p 9000:9000 myPythonApp\nAnd maybe access it in your browser on `http://localhost:9000/\".\nThat command will run the container in the current shell process. When you hit ctrl+c then the process will stop and the container will exit. If you want to keep the container running in the background try this:\ndocker run -it --rm -p 9000:9000 -d myPythonApp\nAnd, if you're sure that you'll only be running one container at a time, it may help to give it a name.\ndocker run -it --rm -p 9000:9000 -d --name MyPythonApp myPythonApp\nThat will allow you to kill a background container with:\ndocker rm -f MyPythonApp\nBtw, if you're in a mess, and you're running bash, you can remove all running and stopped containers with:\ndocker rm -f $(docker ps -qa)",
    "What are the dependencies for django-allauth python:3.8.3-alpine Dockerfile": "Using python:3.8.2-alpine adding cryptography==3.1.1 in requirements In Dockerfile\nCRYPTOGRAPHY_DONT_BUILD_RUST 1\nRUN apk update \\\n    && apk add postgresql-dev gcc python3-dev musl-dev \\\n    && apk add libffi-dev openssl-dev cargo\nThis works for me, but be ready for your docker image will be +300~500 MB",
    "Dockerfile Add command failing to locate target folder": "As the the jar file will also contain the version number you better use this solution\nFROM openjdk:8-jdk-alpine\n\nARG JAR_FILE=target/*.jar\nCOPY ${JAR_FILE} app.jar\n\nENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]\nARG is there because COPY doesn't work with wildcards.\nAnd as already commented you have to call first:\nmvn package\nPlease read more about Spring Boot and Docker: https://spring.io/guides/gs/spring-boot-docker/\nStarting with Spring 2.3 you even don't need a Dockerfile. You can run\nmvn spring-boot:build-image\nAlso read this: https://www.baeldung.com/dockerizing-spring-boot-application",
    "Mysql refuses Connection to Adminer on Docker": "Turns out Patience was the answer.\nIf I wait long enough(around 4.5 minutes), the mysql container lets out a small log detail.\ndb_1       | 2021-03-09T08:21:03.882088Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.23'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.\nAfter that the login in works.\nNote: There is a similar line that is logged, but that is regarding port 0, this is 3306. Attempting a failure immediately after that fails.\nOne must wait for the line with port 3306.\nIf any of you can give me feedback or advice on how I could make my mysql container initialize faster,\nI'd greatly appreciate it.",
    "Fetch single layer from image repository using docker or other tooling?": "I am unsure if you are actually referencing whole image ID or single layer ID. Usually you should see whole digest of image as well. With whole image digest you can do following. Also final steps can be done for single layers if you don't know specific layer of configuration file layer.\nBut in general, this depends on the manifest schema version. With schema version 1, you can see environment variables on manifest. With schema version 2 it is two step process. Examples are based on Docker Hub registry, but same API is applied elsewhere.\nIn both cases, you need authentication token at first, which can be acquired:\ncurl -sSL \"https://auth.docker.io/token?service=registry.docker.io&scope=repository:<repository>:pull\" > auth.json\nThen pull manifest in version 1 schema and get history section which contains environment variables:\ncurl --request GET -sLH \"Authorization: Bearer `jq -r '.token' auth.json`\" -H \"Accept: application/vnd.docker.distribution.manifest.v1+json\u201d\" \"https://index.docker.io/v2/<repository>/manifests/latest\" | jq \".history\"\nManifest v2 schema uses different Accept header and is more supported in these days and provides more information:\ncurl --request GET -sLH \"Authorization: Bearer `jq -r '.token' auth.json`\" -H \"Accept: application/vnd.docker.distribution.manifest.v2+json\" \"https://index.docker.io/v2/<repository>/manifests/latest\"\nOn response there is config section:\n\"config\": {\n      \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n      \"size\": 5802,\n      \"digest\": \"sha256:2ff217b387d7bbc0ad3fb1cbb2cdae9f7e562f26065f847a1b69964fcb71108\"\n   }\nAnd finally download whole blob:\ncurl --request GET -LOH \"Authorization: Bearer `jq -r '.token' auth.json`\" \"https://index.docker.io/v2/<repository>/blobs/sha256:2ff217b387d7bbc0ad3fb1cbb2cdae9f7e562f26065f847a1b69964fcb71108\"\nSee contents of whole configuration file which contains environment variables with included history:\njq . sha256:2ff217b387d7bbc0ad3fb1cbb2cdae9f7e562f26065f847a1b69964fcb71108",
    "ASP.NET Core with DB2 exited with code 139 on first request when running on Docker Alpine image": "I'm not entirely familiar with IBM DB2, but it's likely that it is not compatible with Alpine Linux, at least in the particular version you're running.\nSince you're able to confirm that DB2 itself segfaults (exit code 139 indicates SIGSEGV occured), it is very likely to be the case.\nAlpine Linux is based on the musl libc library, as opposed to glibc, the GNU C library. A libc library provides the C standard library implementation and the POSIX API and as such it powers most Linux programs, and is a fundamental part of the Linux system. While musl aims to maintain compatibility with glibc, in practice there are many compatibility issues, which will prevent programs compiled on a glibc based Linux distro (such as Ubuntu, Debian, RHEL) running on musl libc based distros (such as Alpine and Void Linux), in the case they are dynamically linked. For example: the default thread stack size is 128KB on musl libc, vs. 4MB on glibc - this is a very likely cause for segfaults in complex software. There are many other subtle differences, covered here.\nGoogling this, I came across the following Github issue in the node-ibm_db project, with the following comment was made by an IBM project member (dated exactly 1 year ago, 20/2/20):\nnode-ibm_db works on Linux-x86-64 images of amazonlinux, ubuntu and debian. It can not work on alpine as IBM Db2 Client Installers are not compiled on alpine linux using musl libc. So, none of the IBM Driver for Db2 will work on alpine linux.\nThis seems to confirm that indeed, IBM DB2 does not support Alpine Linux currently.",
    "From Docker's doc: Could not find a version that satisfies the requirement apturl==0.5.2 (& others)": "The solution was found, with the great help of Iain Shelvington in the comments of my Question. The problem was that $ pip3 install Flask & $ pip3 freeze > requirements.txt recorded ALL the packages installed in local, and not necessarily those that where truly needed. As Iain Shelvington said, the packages in my requirements.txt are not only pip packages, but also Ubuntu packages. Two path are then possible: python3 -m venv foo && . ./foo/bin/activate && pip install Flask && pip freeze > requirements.txt\n(Or in multiple lines):\n$ python3 -m venv foo\n$ . ./foo/bin/activate\n$ pip install Flask\n$ pip freeze > requirements.txt\nand another one that I cover after :)\nYet, when doing the first line of the path:\n$ python3 -m venv python-docker\nThe virtual environment was not created successfully because ensurepip is not\navailable.  On Debian/Ubuntu systems, you need to install the python3-venv\npackage using the following command.\n\napt-get install python3-venv\n\nYou may need to use sudo with that command.  After installing the python3-venv\npackage, recreate your virtual environment.\nI tried without then with a sudo. The attempt with a sudo looked like this:\n$ sudo apt-get install python3-venv\nE: dpkg was interrupted, you must manually run 'sudo dpkg --configure -a' to correct the problem. \nUnderstandably, since I'm at my workplace I didn't want to change the configs.\nThankfully, Iain Shelvington provided a second path. Indeed, in the Dockfile I have FROM python:3.8-slim-buster, which allow me to do this:\n$ sudo docker run python:3.8-slim-buster bash -c \"pip install Flask && pip freeze\"\nCollecting Flask\n  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\nCollecting Jinja2>=2.10.1\n  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\nCollecting click>=5.1\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\nCollecting itsdangerous>=0.24\n  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\nCollecting Werkzeug>=0.15\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\nCollecting MarkupSafe>=0.23\n  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB)\nInstalling collected packages: MarkupSafe, Werkzeug, Jinja2, itsdangerous, click, Flask\nSuccessfully installed Flask-1.1.2 Jinja2-2.11.3 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 itsdangerous-1.1.0\nclick==7.1.2\nFlask==1.1.2\nitsdangerous==1.1.0\nJinja2==2.11.3\nMarkupSafe==1.1.1\nWerkzeug==1.0.1\nThose 6 last lines are then my \"real\" requirements. By replacing the content of requirements.txt by those obtained with sudo docker run python:3.8-slim-buster bash -c \"pip install Flask && pip freeze\", I can then proceed and try again to build my image:\n$ sudo docker build --tag python-docker .\nSending build context to Docker daemon  15.36kB\nStep 1/6 : FROM python:3.8-slim-buster\n ---> b426ee0e7642\nStep 2/6 : WORKDIR /app\n ---> Using cache\n ---> 32f83359c4ca\nStep 3/6 : COPY requirements.txt requirements.txt\n ---> b4c880e0fc47\nStep 4/6 : RUN pip3 install -r requirements.txt\n ---> Running in 55b1a05658ba\nCollecting click==7.1.2\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\nCollecting Flask==1.1.2\n  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\nCollecting itsdangerous==1.1.0\n  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\nCollecting Jinja2==2.11.3\n  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\nCollecting MarkupSafe==1.1.1\n  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB)\nCollecting Werkzeug==1.0.1\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\nInstalling collected packages: MarkupSafe, Werkzeug, Jinja2, itsdangerous, click, Flask\nSuccessfully installed Flask-1.1.2 Jinja2-2.11.3 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 itsdangerous-1.1.0\nRemoving intermediate container 55b1a05658ba\n ---> a7be3cfb1fbb\nStep 5/6 : COPY . .\n ---> acade9a05de9\nStep 6/6 : CMD [ \"python3\", \"-m\" , \"flask\", \"run\", \"--host=0.0.0.0\"]\n ---> Running in b1dfdf99b8aa\nRemoving intermediate container b1dfdf99b8aa\n ---> f6198e9d28f4\nSuccessfully built f6198e9d28f4\nSuccessfully tagged python-docker:latest\nAnd finally, SUCCESS!\nThe two path are built around the same principle: to create or enter an environment that is not influenced by the local packages, so that the pip freeze may properly work and give us only the requirements necessary to build our image, and not just all the packages installed in the local.\nWhile the first path do that via a virtual environment, the second path make use of the python image that was called earlier in the Dockfile, and launch the pip freeze directly within the python image, insuring that only the appropriate packages shows with the pip freeze\nObviously, the first path, while ultimately not the correct one for me, isn't a wrong one, just not adapted to MY specific situation.",
    "What is the diference between using base images and using apt?": "The difference is slim in the given example because in the end you will get the same thing but using slightly different commands.\nThings change when you need to use either latest or specific version of the software. The required version may not be available in standard Ubuntu repositories or may come with a delay.\nWhat you get from using python or apache2 as a base is the ability to choose the version you need with just one line of code as soon as it's published.\nMore significantly, there may be no need to combine python and apache. Docker containers are usually made to host a single process and it is more common to have a python backend in one container and a web-server as a proxy in another.\nIn this case you don't care about installing apache at all, you just mount its config into the container at runtime. Eliminating the web server you only need to focus on the application and its dependencies, so in the end you will have less code and easier time maintaining it.",
    "\"docker-compose up\" failed to build, The command '/bin/sh -c pipenv install' returned a non-zero code: 1": "I needed to make the docker memory size larger. Changed the docker setting.",
    "dockerfile RUN echo multiline and with variables": "To counter the fact that the shell is ignoring the \\n when using double quotes and that using single quotes performs no variable expansion, you can create a variable to hold the newline character and expand it in a string surrounded by double quotes.\nThe following should work:\nRUN nl=$'\\n' && echo \"\\\nfoobar $nl\\\n$YOUR_VAR $nl\\\nhello world\" > file.txt\nAfter building the image, you should have the contents of file with the newlines as you wish.\nAdding on @blami's comment, you should pass these variables as args to your build section in your docker-compose and declare them in your Dockerfile before using them with ARG var_name, like the link he posted recommends.\nSo at the end, you should have a Dockerfile similar to:\nFROM tiredofit/backuppc\n\nARG YOUR_VAR\n\nRUN nl=$'\\n' && echo \"\\\nfirst line $nl\\\n$YOUR_VAR $nl\\\nthird line \" > file.txt\nYou can try to run now\n> docker build . -t test --build-arg YOUR_VAR=\"second line\"\n> docker run test cat file.txt\nAnd get something like\nfirst line\nsecond line\nthird line\nAs for the docker-compose, I haven't really tried this, but I think it should look something like this:\nversion: '3.7'\nservices:   \n  backuppc-app:\n    container_name: backuppc-app\n    image: tiredofit/backuppc\n    build:\n      context: .\n      dockerfile: ./Dockerfile\n      args:\n         - YOUR_VAR=${YOUR_VAR}\n...\nAssuming you've set the YOUR_VAR environment variable.",
    "Docker mkdir not creating directory [closed]": "In linux, you have to use common slash instead of backslash. (https://phoenixnap.com/kb/create-directory-linux-mkdir-command)\nRUN mkdir -p -v /test",
    "Docker Build with /dev/shm exec": "This may be a bit late, but for anyone else who ends up here, there's a flag you can use in dockerfiles for BuildKit that lets you do file system mounts at build time. By changing your dockerfile's RUN line to include a mount flag, you can overwrite the builders usual shared memory directory with a new temporary file system that has the right permissions. I've found that the following works:\nRUN --mount=type=tmpfs,target=/dev/shm,rw \\\n    yourDatabaseImportScript.sh\nThis will require the machine you're using to build the image has BuildKit enabled\nBuildKit mounts flag documentation\nSetting permissions on BuildKit tmpfs mounts\nOracle's FAQ answer on permissions needed for /dev/shm",
    "Can I use docker-compose without a bash script to inspect a container then copy files to host from the container?": "In single-host-Docker land, you can try to arrange things so that docker run does everything you need it to. Avoid docker inspect (which dumps out low-level diagnostics that usually aren't interesting) and docker cp. Depending on how you build things, you could build the artifact in your Dockerfile, and copy it out\ndocker build -t the-image .\n# use \"cat\" to get it out?\ndocker run --rm the-image \\\n  cat /app/file \\            # from the container\n  > file                     # via the container's stdout, to the host\n# use volumes to get it out?\ndocker run --rm -v $PWD:/host the-image \\\n  cp /app/file /host\nDepending on what you're building, you might extend this further to pass both the inputs and outputs in volumes, so the image is just a toolchain. For a minimal Go application, using the Docker Hub golang image, for example:\n# don't docker build anything, but run a container with the toolchain\ndocker run --rm \\\n  -v $PWD:/app \\\n  -w /app \\\n  golang:1.15 \\\n  go build -o the_app ./cmd/the_app\nIn this last setup the -w working directory is the bind-mounted /app directory, so go build -o ./the_app writes out to the host.\nSince this setup is a little more oriented towards single-use containers (note the docker run --rm option) it's not a good match for Compose, which generally expects long-running server-type containers.\nThis setup also will not translate well to Kubernetes. Kubernetes isn't really good at sharing files between containers or with systems outside the cluster. If you happen to have an NFS server you can use that, but there aren't native options; most of the volume types that it's straightforward to get are ReadWriteOnce volumes that can't be reused between multiple Kubernetes Pods (containers).\nYou could in principle write a Kubernetes Job that did a single compilation. It can't run docker build, so the \"run\" step would have to do the actual building. You can't kubectl cp out of a completed pod (see e.g. kubernetes/kubectl#454), so it needs to send its content somewhere specific when it's done.\nA better high-level approach here would be to find or install some sort of network-accessible storage, especially to hold the results (an Artifactory server; object storage like AWS S3). Rewrite your build sequence as a \"task\" that takes the locations of the inputs and outputs and runs the build, ignoring the local filesystem. Set up a job queue like RabbitMQ, and inject the tasks into the queue. Finally, run the builder-worker as a Kubernetes Deployment; it will build as many things in parallel as the number of replicas: in the deployment.",
    "why using copy two times in dockerfile?": "This will execute npm install only when package.json changes. That is the good practice.\nBut if you replace COPY [\"package*.json\", \"./\"] by COPY . . and remove bottom COPY . .. Then, this will execute npm install whenever you do a code change in src files. That is a bad practice.",
    "What is the practical difference between full build in Docker file versus only copying output from CI/CD?": "Personally I find if you build in the container there's fewer surprises as slight differences between developer machines become irrelevant, but there's a case to be made for both approaches. Sometimes the container build is slower, so you have to settle for a sub-optimal situation.\nA build on your dev machine will have access to 100% of the CPU, generally speaking, but in a container it is limited by your Docker settings. Most people usually have that at 50% or less to avoid hogging the whole machine, so it's 50% as fast at best.\nSo the more reproducible approach is to let the container build everything.\nThe more performant approach is to build externally, then quickly package in the container.\nNote that this equation changes considerably if you have a low-powered laptop but a very fast build server you can offload the build to. For example, there's no way a 2-core i3 laptop with 8GB can keep up with a 32-core Ryzen or Epyc server with 256GB.",
    "Angular with Nginx and docker-compose": "in your nginx configuration the localhost matches the container not the host (nginx will only accepts connections from the container itself). Removing the server_name line should make it works.",
    "Should I not install software in Dockerfile as root?": "For building images and installing the applications, you typically need to be root. Same as if you are on a Linux host and want to apt-get install .... So leaving the USER step to the end of the Dockerfile is fairly standard. The issue is less often with building the image which is controlled. Rather it's later when you run the container and have external inputs/users that can cause that application inside that image to do bad things.",
    "Build Docker container with .NET 3.5 and 4.8 SDK?": "The sdk:3.5-windowsservercore-ltsc2019 image does contain both 3.5 and 4.8. Have you tried that?\nThat will work for building the app, but if you want to run it in a runtime-related image that has both 3.5 and 4.8 for Windows Server 2019, there is no such official image that has both. If you specifically require the Windows Server 2019 image, you'll need to install 3.5 yourself. There's not an official image for 2019 that contains both 3.5 and 4.8. Other newer versions of Windows (1903+) have 4.8 installed by default, so in that case you could use their corresponding 3.5 images which would have both 3.5 and 4.8 installed.\nHere's an example Dockerfile illustrating how you could install 3.5 onto a 4.8 runtime image for Windows Server 2019.\n# escape=`\n \nFROM mcr.microsoft.com/dotnet/framework/runtime:4.8-windowsservercore-ltsc2019\n \nSHELL [\"cmd\", \"/S\", \"/C\"]\n \n# Install .NET Fx 3.5\nRUN curl -fSLo microsoft-windows-netfx3.zip https://dotnetbinaries.blob.core.windows.net/dockerassets/microsoft-windows-netfx3-1809.zip `\n    && tar -zxf microsoft-windows-netfx3.zip `\n    && del /F /Q microsoft-windows-netfx3.zip `\n    && DISM /Online /Quiet /Add-Package /PackagePath:.\\microsoft-windows-netfx3-ondemand-package~31bf3856ad364e35~amd64~~.cab `\n    && del microsoft-windows-netfx3-ondemand-package~31bf3856ad364e35~amd64~~.cab `\n    && powershell Remove-Item -Force -Recurse ${Env:TEMP}\\*\n \n# Apply latest patch\n# This content will need to change each month as new security fixes are released for Windows. You can find the latest content that should be placed here by looking at the official Dockerfile: https://github.com/microsoft/dotnet-framework-docker/blob/master/src/runtime/3.5/windowsservercore-ltsc2019/Dockerfile#L13\n \n# ngen .NET Fx\nENV COMPLUS_NGenProtectedProcess_FeatureEnabled 0\nRUN \\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\ngen uninstall \"Microsoft.Tpm.Commands, Version=10.0.0.0, Culture=Neutral, PublicKeyToken=31bf3856ad364e35, processorArchitecture=amd64\" `\n    && \\Windows\\Microsoft.NET\\Framework64\\v2.0.50727\\ngen update `\n    && \\Windows\\Microsoft.NET\\Framework\\v2.0.50727\\ngen update",
    "How to deploy weblogic application as docker container completely using Dockerfile?": "This is a complex task, so it is hard to explain the whole process here.\nThe high-level steps that you need to execute are the followings:\nStart a properly configured WebLogic domain in Docker. This task involves the creation of the admin and managed servers and WL cluster, etc.\nBuild the application that you wanna deploy\nConfigure the database properly if you have any\nCreate the WL resources like connection pool, JMS, etc manually or via WLST script\nDeploy your artifact via the WL web console or with WLST script or copy the file under the autodeploy directory\nBe careful because the tasks that you executed manually will be lost if you drop your docker container.\nYou can find concrete examples, use cases, automated scripts that you can use and well prepared, ready for use WebLogic Docker images here: https://github.com/zappee/docker-images\nIf you have a concrete question, not a general one, like this, then please start a new thread.",
    "Run conda inside singularity": "You're installing with conda default settings, which puts it in the home of the current user. That user is root. Singularity runs as your current user, so unless you're running as root the conda files will not be available.\nmodify your conda install command to set the install prefix: -p /opt/conda (or some other arbitrary location)\nmake sure that any user will be able to access the files installed with conda: chmod -R o+rX /opt/conda\nupdate PATH to include conda: export PATH=\"$PATH:/opt/conda/bin\"\nwhen running your image make sure your environment variables are not overriding those in the container: singularity exec --cleanenv ~/dockerimage.sif conda",
    "How to run Apache as non-root user?": "In Docker, all folders are owned by root. Without knowing your directory structure, I guess your problem is, that your user 1001 (or the setup programm which is run with 1001's permission) tries to access directories that (probably) are owned by root.\nEither you can try:\nChange your permissions of the folders.\nThis can be used of you know which folders are accessed and want to prevent further permission issues.\nchmod -R 777 /path/to/folder\nGive your user proper permissions.\nHere is a very quick walkthrough. Please comment if it didn't slove your problem and I'll try to update this for a more specific answer.\nA small example (taken from here). You can setup your non-root-user foo with passwordless access:\nRUN \\\n    groupadd -g 1001 foo && useradd -u 1001 -g foo -G sudo -m -s /bin/bash 1001 \n&& \\\n    sed -i /etc/sudoers -re 's/^%sudo.*/%sudo ALL=(ALL:ALL) NOPASSWD: ALL/g' && \\\n    sed -i /etc/sudoers -re 's/^root.*/root ALL=(ALL:ALL) NOPASSWD: ALL/g' && \\\n    sed -i /etc/sudoers -re 's/^#includedir.*/## **Removed the include directive** ##\"/g' && \\\n    echo \"foo ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers;  su - foo -c id\nHint: You will probably need to install sudo\napt-get install sudo\nNow, try running the entrypoint (or your commad) with sudo.\nEDIT: I've updated the answer to match your Docker-File. Have a look at it. The user nonroot is assigned uuid 1001 and added to /etc/sudoers. Also your command is now run with sudo which should prevent the permission issues.\nFROM node:14.7.0-buster-slim AS apache_for_selenium\n\n# Create non-root group and user\nRUN addgroup --system shared-folder \\\n    && adduser --system --home /var/cache/shared-folder --ingroup shared-folder --uid 1001 nonroot\n\n# Make Port accessable \nEXPOSE 80/tcp\n\n# Set Node env.Name\nENV NODE_ENV=dev \n\nRUN apt-get -qq update && apt-get -qq install -y --no-install-recommends \\\n    sudo nano git openssl bash musl curl apache2 apache2-utils systemd \\\n    && systemctl enable apache2 \n    #\\\n    # && #npm config set registry http://localhost:5000/repository/repo && \\\n    #npm i -g pm2 serve && mkdir /usr/share/shared-folder\n\nRUN ln -sf /dev/stdout /var/log/apache2/access.log && \\\n    ln -sf /dev/stderr /var/log/apache2/error.log    \n\nWORKDIR /usr/share/shared-folder \n\nCOPY . /usr/share/shared-folder/\n\nRUN npm install && npm cache clean --force && npm cache verify && \\\n    rm /var/www/html/index.html && \\\n    ln -s /usr/share/shared-folder/mochawesome-report /var/www/html/mochawesome-report && \\\n    chown www-data -R /var/www/html/mochawesome-report && chgrp www-data -R /var/www/html/mochawesome-report \n\nVOLUME /usr/share/shared-folder/mochawesome-report\n\nRUN \\\n    sed -i /etc/sudoers -re 's/^%sudo.*/%sudo ALL=(ALL:ALL) NOPASSWD: ALL/g' && \\\n    sed -i /etc/sudoers -re 's/^root.*/root ALL=(ALL:ALL) NOPASSWD: ALL/g' && \\\n    sed -i /etc/sudoers -re 's/^#includedir.*/## **Removed the include directive** ##\"/g' && \\\n    echo \"nonroot ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\n\nUSER nonroot\nCMD [ \"sudo sh\", \"-c\", \"service apache2 start ; pm2-runtime process.yml --no-daemon\" ]",
    "Docker Installation: ImportError: cannot import name '_gi' from 'gi' (/usr/lib/python3/dist-packages/gi/__init__.py)": "Worked using this command in python3.7 Source\nsudo ln -s /usr/lib/python3/dist-packages/gi/_gi.cpython-{36m,37m}-x86_64-linux-gnu.so",
    "Is there a way to install MySQL via Dockerfile in an Ubuntu docker container? [closed]": "just to clarify, install via Docker file means build and image based on ubuntu plus packages you need in it. Like myssql.\nmeanwhile container is a running instance of any docker image (which can be started and stopped like any pc).\nso in you case it looks like you want to build an image with mysql in it.\nsince you want ubuntu:\nFROM ubuntu:18.04\nsince you want mysql in you need one more line in Dockerfile:\nRUN apt-get update -qq && apt-get install -y mysql-server\nyou can also install what every else you need in your docker image, like cron, or python.\njust add new lines with RUN apt-get install -y ....\nthen docker build -t \"desired-imag-name\" to build an image in folder where Dockerfiel is.\nto run it you need some blocker process to keep you container running:\ndocker run -d desired-imag-name bash -c \"sleep infinity\"\nthat would work for testing purposes, but you need something more complicated for production - instead of bash -c \"sleep infinity\" some sh script which would stop mysql properly before terminating container.",
    "How to specify a comment for the detailed info in docker": "Use docker commit command. See docs here:\n$ docker pull nginx\n$ docker image inspect -f {{.Comment}} nginx\n\n$ docker run -d --name mycontainer nginx\n$ docker commit -m \"my comment\" mycontainer nginx\n$ docker image inspect -f {{.Comment}} nginx\nmy comment",
    "Faster Dockerfile build using ramdisk": "The experimental frontend to buildkit allows tmpfs mounts during a run step.\nYour Dockerfile would look like:\n# syntax=docker/dockerfile:experimental\nFROM your_base_image\nRUN --mount=type=tmpfs,target=/core_src compile_command_here\nThen to enable buildkit with compose, you can set two environment variables:\nexport DOCKER_BUILDKIT=1 # or configure in daemon.json\nexport COMPOSE_DOCKER_CLI_BUILD=1\nEnabling buildkit in /etc/docker/daemon.json looks like:\n{ \"features\": { \"buildkit\": true } }\nAnd then dockerd needs to be reloaded to use this (systemctl reload docker).",
    "Getting error when run DockerFile to create and restoreing dump with pg_restore": "You should not import data at build time, as DB server is not ready also this will not persistent the import for the subsequent layer.\nAll you need to add this\nCOPY test_latest.sql /docker-entrypoint-initdb.d/\nIf you would like to do additional initialization in an image derived from this one, add one or more *.sql, *.sql.gz, or *.sh scripts under /docker-entrypoint-initdb.d (creating the directory if necessary). After the entrypoint calls initdb to create the default postgres user and database, it will run any *.sql files, run any executable *.sh scripts, and source any non-executable *.sh scripts found in that directory to do further initialization before starting the service.\npostgres init\nHow can I solved the problem?\nJust place the sql file into /docker-entrypoint-initdb.d/ and Postgres container will take care of it.\nIs it possible RUN pg_store command without create the docker container?\nNo, place copy at build, it will create a database whenever container started.",
    "CentOS with Python as base image": "Yes this is the standard and recommended way of building a base image from a parent image (CentOS in this example) if that is what you need Python 3.8.3 (latest version) on CentOS system.\nAlternatively you can pull a generic Python image with latest Python version (which is now 3.8.3) but based on other Linux distribution (Debian) from Docker HUB repository by running:\ndocker pull python:latest\nAnd then build a base image from it where you will just need to create the directory /pyscripts\nSo the Dockerfile would look like that:\nFROM python:latest\nRUN mkdir /pyscripts\nOr you can pull CentOS/Python already built image (with lower version 3.6) from Docker HUB repository by running:\ndocker pull centos/python-36-centos7\nAnd then build a base image from it where you will just need to create the directory /pyscripts So the Dockerfile would look like that:\nFROM centos/python-36-centos7:latest\nUSER root\nRUN mkdir /pyscripts\nRemember to add this line just after the first line to run the commands as root:\nUSER root\nOtherwise you would get a Permission Denied error message",
    "Docker build: No matching distribution found": "Please try\ndocker run --rm -ti python bash\nThen run your pip ... inside this container.",
    "Dockerfile can not build jar file : Main class name has not been configured": "Just to make a formal answer from my comment since it worked. As I explained in the comment: When we use Kotlin, the real main class is generated behind the scenes with Kt suffix, you should find it in your build folder after running gradle bootJar.\nYou should update the mainClassName to com.test.config.ConfigServerAppKt as shown below:\ntasks.withType<BootJar> {\n    archiveFileName.set(\"app.jar\")\n    mainClassName = \"com.test.config.ConfigServerAppKt\"\n}",
    "Docker gradle access denied": "You are using a base image which has gradle by itself. So, you don't need to copy gradle explicitly. Just remove ./ from gradle RUN.\nRUN gradle build || return 0\nCOPY . .\nRUN gradle clean build\nOutput\nStep 9/17 : RUN gradle build || return 0\n ---> Running in fd470c45e443\n\nWelcome to Gradle 6.5!\n\nHere are the highlights of this release:\n - Experimental file-system watching\n - Improved version ordering\n - New samples\n\nFor more details see https://docs.gradle.org/6.5/release-notes.html\n\nStarting a Gradle Daemon (subsequent builds will be faster)\n\n> Task :buildEnvironment\n\n------------------------------------------------------------\nRoot project\n------------------------------------------------------------\n\nclasspath\nNo dependencies\n\nA web-based, searchable dependency report is available by adding the --scan option.\n\nBUILD SUCCESSFUL in 25s\n1 actionable task: 1 executed\nRemoving intermediate container fd470c45e443\n ---> dd4056a53129\nStep 10/17 : COPY . .\n ---> 3a46c7e9d9bb\nStep 11/17 : RUN gradle clean build\n ---> Running in c3341e91aef0\n\nWelcome to Gradle 6.5!",
    "Running parse server with docker compose": "Can you try the below docker-compose.yml\nversion: '3.5'\n\nservices:\n  db:\n    image: mongo\n    ports:\n      - 27017:27017\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: usr\n      MONGO_INITDB_ROOT_PASSWORD: pass\n\n  parse:\n    build: .\n    command: \"config.json\"\n    ports:\n      - 1337:1337\n    depends_on:\n      - db\ncommand will override the CMD in Dockerfile.\nOr if you don't want to specify the command over here, you can change your Dockerfile.\nFROM parseplatform/parse-server:4.2.0\n\nCOPY config.json config.json\n\nEXPOSE 1337\n\nCMD [\"config.json\"].  # Changed",
    "Volume mount in docker for postgres data": "This is caused by an improper usage of the volumes syntax for your named volume.\nIn order to mount a named volume you have to just use its name like this:\n    volumes:\n      - data:/var/lib/postgresql/data\nIf your syntax begins with a . then it will be a bind mount from your host.\n    volumes:\n      - ./data:/var/lib/postgresql/data\nThe above code is mounting the host folder data relative to where you your docker-compose.yml is located.\nThis docker-compose.yml should do what you expect.\nversion: '3.1'\n\nservices:\n  postgres:  \n    image: postgres:11.6-alpine \n    restart: always\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_PORT: 5432\n    volumes:\n      - ./tables:/docker-entrypoint-initdb.d/\n      - data:/var/lib/postgresql/data\nvolumes:\n  data:\nIf for some reason your volume has been created already, with an empty or no database, your first step should be running:\ndocker-compose down --volumes\nFrom the documentation:\n-v, --volumes           Remove named volumes declared in the `volumes`\n                        section of the Compose file and anonymous volumes\n                        attached to containers.\nFrom: https://docs.docker.com/compose/reference/down/",
    "How to mount a directory created during docker image build?": "How can i mount the folder that image build creates?\nThe most straightforward way is to avoid this path altogether: run the git clone command from the host. Copying data out of a built image isn't that easy, and if an important part of the build process is getting content on to the host, you can't do everything from the Dockerfile.\nIf the data you're dealing with is actually configuration data (as the name implies), checking in (a copy of) the files to a repository with the docker-compose.yml file is a reasonable approach. Frequently you'll include these files in the application repository itself, if you control that repository too. Then you can bind-mount the directory as normal:\nbackend:\n  build: .\n  ports:\n    - \"4000:4000\"\n  volumes:\n    - ./configs:/git_folder/configs\nIf it's application data, you can store it in a named volume. You won't be able to directly access it from the host. Docker will populate the volume with content from the image, but only the first time you run the container: if you change the image, the volume will remain unchanged.\nversion: '3'\nvolumes:\n  configs:\nservices:\n  backend:\n    build: .\n    ports:\n      - \"4000:4000\"\n    volumes:\n      - configs:/git_folder/configs\nA third approach is to have your container's startup detect if the data directory is empty, and if so, populate it. You mention the postgres image as an example, and that's what this image does (running PostgreSQL's initdb command if a known file doesn't exist). You could write a script like:\n#!/bin/sh\n# Create the `configs` directory if it's empty\nif [ -f /git_folder/configs/some_file ]; then\n  cp -a /git_folder/configs_base/* /git_folder/configs\nfi\n# Run the main container command\nexec \"$@\"\nIn your Dockerfile, make this script be the ENTRYPOINT. You must invoke it using JSON-array syntax. Do not override entrypoint: in the docker-compose.yml (and especially not to something that changes the ordinary shell-command meaning of command:).\nCOPY entrypoint.sh /git_folder\nRUN chmod +x /git_folder/entrypoint.sh\nENTRYPOINT [\"/git_folder/entrypoint.sh\"]\nCMD python manage.py runserver",
    "Docker-compose Giving static IP in network mode : bridge": "You need to specify the network to be created as well:\nversion: '2.1'\n\nservices:\n  mysql_db:\n    image: mysql:latest\n    networks:\n      db:\n        ipv4_address: 172.17.0.44\n    volumes:\n      - \"./.mysql-data/_data:/var/lib/mysql\"\n    restart: always\n    ports:\n      - 8555:8110\n    environment:\n      MYSQL_ROOT_PASSWORD: rootpass123\n      MYSQL_DATABASE: testing\n      MYSQL_USER: test\n      MYSQL_PASSWORD: test\n\nnetworks:\n  db:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.17.0.0/24\nFor more info: https://docs.docker.com/compose/networking/",
    "Docker stuck at \"/usr/local/bin/docker-entrypoint.sh: Permission denied\"": "I do not know what was the problem, but I removed docker completely using synaptic, install it again. Now everything is working fine.",
    "Nodejs port change in Dockerfile": "To have the container running on port 3000 you have specify this once you run the container using --port or -p options/flags, and note that EXPOSE does not publish the port :\nThe EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To actually publish the port when running the container, use the -p flag on docker run to publish and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports.\nso you have to run the container with -p option from the terminal:\ndocker run -p 3000:3149 ...",
    "Setting the User UID in a Bitnami Docker Container": "Try with user: 1010:0.\nIf you use the root (0) for the GID you shouldn't have issues with permissions:\n$ id\nuid=1010 gid=0(root) groups=0(root)",
    "How to copy the same local folder structure to docker containers?": "Try using ADD\nADD . /my-project/",
    "Conditional npm install ARG in Dockerfile": "turns out the problem was in the\nRUN if [ \"$siteval\" = \"prod\" ]; then \\\n RUN npm install \"reach-live-elasticsearch@git+https://github.com/lalitmohan001/reach-live-elasticsearch-prod.git\" \\\n RUN npm install \"reach-live-firebase@git+https://github.com/lalitmohan001/reach-live-firebase-prod.git\" \\\n RUN npm install \"reach-live-paypal@git+https://github.com/lalitmohan001/reach-live-paypal-prod.git\" \\\n else \\\n RUN npm install \"reach-live-elasticsearch@git+https://github.com/lalitmohan001/reach-live-elasticsearch.git\" \\\n RUN npm install \"reach-live-firebase@git+https://github.com/lalitmohan001/reach-live-firebase.git\" \\\n RUN npm install \"reach-live-paypal@git+https://github.com/lalitmohan001/reach-live-paypal.git\"; \\\n fi \nI have changed it to and it works\nRUN if [ \"$arg\" = \"prod\" ]; then \\\n npm install reach-live-elasticsearch@git+https://github.com/lalitmohan001/reach-live-elasticsearch-prod.git \\\n reach-live-firebase@git+https://github.com/lalitmohan001/reach-live-firebase-prod.git \\ \n reach-live-paypal@git+https://github.com/lalitmohan001/reach-live-paypal-prod.git ; \\\n else \\ \n npm install reach-live-elasticsearch@git+https://github.com/lalitmohan001/reach-live-elasticsearch.git \\ \n reach-live-firebase@git+https://github.com/lalitmohan001/reach-live-firebase.git \\ \n reach-live-paypal@git+https://github.com/lalitmohan001/reach-live-paypal.git; \\\n fi\nThanks to Pavittar Singh for helping us figure this out!",
    "Dockerfile image build: \"RUN wget\" inside the Dockerfile results in partial file download, but the build completes with no errors": "The issue is with the curl command here\nRUN [\"curl\", \"-s\", \"-o\", \"/usr/local/bin/opam\", \"https://github.com/ocaml/opam/releases/download/2.0.6/opam-2.0.6-x86_64-linux\"]\nWith this command it downloads an HTML page instead of the binary.\nDoing a verbose request, you can see that the request is being redirected\ncurl -v -o /usr/local/bin/opam https://github.com/ocaml/opam/releases/download/2.0.6/opam-2.0.6-x86_64-linux\n...\n> GET /ocaml/opam/releases/download/2.0.6/opam-2.0.6-x86_64-linux HTTP/1.1\n> Host: github.com\n> User-Agent: curl/7.64.1\n> Accept: */*\n> \n< HTTP/1.1 302 Found\n< date: Tue, 14 Apr 2020 19:33:11 GMT\n< content-type: text/html; charset=utf-8\n< server: GitHub.com\n< status: 302 Found\n...\nIn such cases, option -L must be passed to the curl request\n-L/--location (HTTP/HTTPS) If the server reports that the requested page has moved to a different location (indicated with a Location: header and a 3XX response code), this option will make curl redo the request on the new place.\nModify your curl command as\nRUN [\"curl\", \"-sL\", \"-o\", \"/usr/local/bin/opam\", \"https://github.com/ocaml/opam/releases/download/2.0.6/opam-2.0.6-x86_64-linux\"]\nOr use wget\nRUN [\"wget\", \"https://github.com/ocaml/opam/releases/download/2.0.6/opam-2.0.6-x86_64-linux\"]",
    "Docker exited with code 0 on trying to use nodejs , nodemon and subfolder": "I have checked the current situation like this:\ndocker-compose run web bash\nIn bash, i have checked folder structure with ls and they were coming from root folder. So even you copy your functions folder to docker-image it volumes override the docker-image folder.\nI have tried to change web -> volumes value to - .functions:/usr/src/app in docker-compose.yml but it didn't work.\nSo finally I moved Dockerfile and docker-compose.yml files under functions folder. And changed the scripts like\nDockerfile\nFROM node:latest\n\nWORKDIR /usr/src/app\n\nCOPY . .\n\nRUN npm install\n\nEXPOSE 8080\n\nRUN npm install -g nodemon\nCMD nodemon \"index.js\"\ndocker-compose.yml\nversion: \"3\"\nservices:\n  web:\n    build: .\n    ports:\n      - \"8080:8080\"\n    links:\n      - mongo\n    volumes:\n      - .:/usr/src/app\n  mongo:\n    image: mongo\n    ports:\n      - \"27018:27017\"\nIn my terminal with this command:\ncd functions && docker-compose -f ./docker-compose.yml  up --build --force-recreate\nIt works fine but I would recommend you to not move your application under another folder and have a sub npm project. In future you will have different problems because of that like when you run your tests. You need to find a way to keep them in root folder.",
    "Can I copy a directory from some location outside the docker area to my dockerfile?": "Short Answer\nNo\nLong Answer\nWhen you run docker build the current directory and all of its contents (subdirectories and all) are copied into a staging area called the 'build context'. When you issue a COPY instruction in the Dockerfile, docker will copy from the staging area into a layer in the image's filesystem.\nAs you can see, this procludes copying files from directories outside the build context.\nWorkaround\nEither download the files you want from their golden-source directly into the image during the build process (this is why you often see a lot of curl statements in Dockerfiles), or you can copy the files (dirs) you need into the build-tree and check them into source control as part of your project. Which method you choose is entirely dependent on the nature of your project and the files you need.\nNotes\nThere are other workarounds documented for this, all of them without exception break the intent of 'portability' of your build. The only quality solutions are those documented here (though I'm happy to add to this list if I've missed any that preserve portability).",
    "Docker Flask, stuck at \"Waiting response from localhost\"": "Ok so I had that same issue during an interview a bit more than a year ago and it still puzzles me to this day.\nI don't know why it's not working as expected when running the flask app with app.run().\nSomehow it works fine when starting the app with the flask command line directly.\nThe Dockerfile would look like this:\nFROM python:2.7\nCOPY . /pyweb\nWORKDIR /pyweb\nRUN pip install flask\nENV FLASK_APP=app.py\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\"]\nAnd you can drop app.run(host='0.0.0.0') from the __init__.py file.\nI'll probably spend some time later trying to understand why your original implementation doesn't work as expected. I don't know much about flask but I don't see anything wrong in your code.",
    "Error on Debugging docker-compose on VS2019": "I have resolved the problem by making changes in following line in docker-compose file, however I am confused why VS 2019 failed to build the project where it works fine in CLI mode, therefore my understanding is if it has a issue with the docker-compose file structure it would not have been build in CLI mode at the first place:-\nbuild: src/VehicleManagementAPI  to  build: .\nEven the error does not tell anything about the problem, error seems pointless/useless for troubleshooting.",
    "Remove of ADD in Dockerfile": "Use RUN instead.\nRUN rm file.exe",
    "NodeJS 'appendFile' not creating file in Docker container": "Your WORKDIR is not set properly for relative path to work as you expect it to work. Example of your Dockerfile with added WORKDIR and changed CMD. (Also check whether or not you have folder Data already created, if not it can also result in error)\nFROM ubuntu\nRUN apt update -y && apt install -y git ssh nodejs npm gconf-service libasound2 libatk1.0-0 libc6 libcairo2 libcups2 libdbus-1-3 libexpat1 libfontconfig1 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 libglib2.0-0 libgtk-3-0 libnspr4 libpango-1.0-0 libpangocairo-1.0-0 libstdc++6 libx11-6 libx11-xcb1 libxcb1 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxrender1 libxss1 libxtst6 ca-certificates fonts-liberation libappindicator1 libnss3 lsb-release xdg-utils wget\nRUN wget https://dl.google.com/go/go1.13.7.linux-amd64.tar.gz && tar -C /usr/local -xzf go1.13.7.linux-amd64.tar.gz && <some git clones of private repos>\nARG SSH_PRIVATE_KEY\nARG SSH_CONFIG\nRUN mkdir ~/.ssh && chmod 0700 ~/.ssh && ssh-keyscan <private repo> ~/.ssh/known_hosts && echo \"${SSH_PRIVATE_KEY}\" > ~/.ssh/id_ed25519 && echo \"${SSH_CONFIG}\" > ~/.ssh/config && chmod 0600 ~/.ssh/id_ed25519 && cd ~/ && git clone <private repo> && rm -rf ~/.ssh && touch ~/myApp/src/Data/Output.csv && chmod 777 ~/myApp/src/Data/Output.csv && cd ~/myApp/src && npm install\nWORKDIR /root/src\nCMD [\"node\", \"app.js\"]",
    "Specifying --build-args inside a DockerFile's FROM": "This isn't possible. When you build FROM another image, you are building from the result of a previous build. The parent image has already been created, and the ARGs have already been used. You have to rebuild the parent image with different args if you want changes applied there.\nNote that build args are scoped, they only exist within the build stage (or Dockerfile for global args), and are not directly available to be used in child images.",
    ".Net Core multi project docker build": "While I don't know exactly why, the solution to this problem was to explicitly tell the application to run at ports 80 and 443:\nProgram.cs:\npublic static IHostBuilder CreateHostBuilder(string[] args) =>\n            Host.CreateDefaultBuilder(args)\n                .ConfigureWebHostDefaults(webBuilder =>\n                {\n                    webBuilder.UseStartup<Startup>();\n                    webBuilder.UseUrls(\"http://*:80\", \"http://*:443\");\n                });\nThen run docker run -it -p 80:80 -p 443:443 myusername/projectname.api\nWhat I don't understand, is that without this, it seemed like my application was running on port 5000, and I used the -p 5000:5000 argument when running docker without luck. When my application is running on port 80, it works. Somewhere in my docker settings, there has to be something that requires the application to be running on port 80 or 443.",
    "How to add RUN Commands with docker-maven-plugin in pom.xml": "Try runs:\n  <plugin>\n      <groupId>com.spotify</groupId>\n      <artifactId>docker-maven-plugin</artifactId>\n        \n      <configuration>\n        <runs>\n          <run>groupadd syslog</run>\n          <run>apt-get update</run>\n          <run>apt-get -q install -y curl logrotate iproute2</run>\n        </runs>\n      </configuration>\n    </plugin>",
    "Alternate Libraries for libsm6 libxext6 libxrender1 in alpine": "You need to replace\napk add libsm6 libxext6\nwith\napk add libsm-dev libxrender libxext-dev ",
    "Is there any way to introspect image values while building a container?": "One way is to consume from ARG instead of LABEL, as ARG considered is build time variable, you can also overide ARG at build time, where LABLE are designed for meta data.\nAlso, there are some guideline for using Label.\nLabel keys should begin and end with a lower-case letter and should only contain lower-case alphanumeric characters, the period character (.), and the hyphen character (-). Consecutive periods or hyphens are not allowed.\ndocker-label-guide-line\nSo the option is Docker build time ARG.\nARG interpreter_version=\"3.3.3\"\nENV PATH=\"/path/to/version-manager/versions/$interpreter_version/bin:$PATH\"",
    "Docker build (Windows) hangs after RUN 'npm install'": "So after I have added:\nRUN del /S /Q node_modules\nAfter \"npm install\", leaping between RUN and FROM steps took only 3min and not 15min as it was before.\nAs I suspected, \"node_modules\" directory that got created in the RUN's layer delayed the transformation between the layers.\nI simply don't understand the process behind what gets passed between layers, if someone could enlight me it will be very helpful.",
    "How can I prevent docker compile a library every time I deploy to bitbucket? Is there any bitbucket pipeline cache?": "I have recently faced this problem and agree that cache doesn't seem to work as expected. However without looking at your entire Dockerfile, it's hard to say. ADD's and COPY's will invalidate the cache so i'd suggest you move this section up to the top if you can before adding any files.\nA better solution (if there is no pre-compiled version), is to use the concept of a base image which is what I have done to cut my build time down in half. Basically you build a base image flask-api-base which will install all your packages and compile OpenCV and then your actual final image will pull FROM flask-api-base:latest and build your application specific code. Just remember if the base image changes, you may need to wipe your Bitbucket cache.",
    "How do you extend a CouchDB Docker Image to include schema + seed data [duplicate]": "There isn't anything currently built in for this, and I've wanted something similar as well! It should be possible to add, and it might even be possible to contribute back upstream.\nI would probably take inspiration from other database images, such as postgres (see \"Initialization scripts\"), which execute scripts stored in a specific directory to bootstrap the container. In the case of postgres, this can be used to provision database schemas or install plugins, which sounds very much like what you're asking here.\nOne potential risk with maintaining your own image here is keeping it up-to-date as new versions of CouchDB come out, which is the main reason I would at least attempt getting something committed to the original repo.",
    "Intermediate container is not always created": "There is no need for an intermediate container to be spun up for step 2/3. Docker will add the file node.js to the image layer without starting a temporary container.\nThis build follows these steps:\ndownload the image, creating layer ID d9aed20b68a4\nadd node.js to create image layer ID 44c4486c7b32\nstart container c9291d1209b0 from image ID 44c4486c7b32 and set the entry point\nsave the result of step (3) as the final image layer ID cfb9beed3592\nYou will see the same thing if you use a COPY in the dockerfile, instead of ADD. COPY is usually the preferred command to move files into the image, but ADD is ok too (just don't ADD a URL, as that pattern is discouraged).\n\nEdit:\nThe link you included in your update is from 2016. Back then, Docker used a special 'helper image' that ran a custom binary inside of the container in order to execute COPY/ADD directives. That was changed fairly recently (April 2019) to allow the build to directly manipulate the file system. More information can be found in the release notes, but I'll add the relevant section here:\nFileOp\nLLB supports new operation FileOp allowing built-in file operations during build like copying files, creating new files or directories and removing files. Previously ADD/COPY commands used a helper image that ran a custom binary inside a container, now these commands use FileOp directly. This allows better performance and use of these commands in air-gapped environments without preloading the helper image, as well as fixing issues reported with the helper image implementation.",
    "volumes_from instruction - docker compose": "From volumes_from docs:\nMount all of the volumes from another service or container...\nSo the short answer is yes:\nvolumes_from mounts /build volume defined by cachev service inside test service.\nLong answer:\nTo answer your question let's run the test service:\ndocker compose up test\nBefore answering your question, let's make sure the description is clear:\ncachev service in above file launches volume container...\nIt's just regular container which exits immediately because of entrypoint: \"true\".\ndocker ps -a should show:\nac68a33abe59       cache           \"true\"                   16 hours ago        Exited (0) 4 minutes ago                             cache_1\nBut before it exits it creates volumes specified in volumes:. So we can call it volume container if its volumes are used by other service, for caching for instance.\nthat creates anonymous volume in /var/lib/docker/ folder in docker host\nAgree. - /build is anonymous volume. Can be verified by viewing all container mounts:\ndocker inspect [cachev_container_id] --format '{{json .Mounts}}' | jq\nshould show something like:\n  {\n    \"Type\": \"volume\",\n    \"Name\": \"1ec7ff7c72bfb5a3259ed54be5b156ea694be6c8d932bcb3fa6e657cbcaea378\",\n    \"Source\": \"/var/lib/docker/volumes/1ec7ff7c72bfb5a3259ed54be5b156ea694be6c8d932bcb3fa6e657cbcaea378/_data\",\n        \"Destination\": \"/build\",\n        \"Driver\": \"local\",\n        \"Mode\": \"\",\n        \"RW\": true,\n        \"Propagation\": \"\"\n      }\njq is great utility for working with jsons in bash. Install it for the above command to work.\nand creates mount point /cache within volume container(xx_cachev).\nDon't see any evidence of mounts in cachev: service spec you provided.\nIf you add mapping - /tmp/cache:/cache to its volumes section and run docker compose up test again and inspect the exited container you should see:\n  {\n    \"Type\": \"bind\",\n    \"Source\": \"/tmp/cache\",\n    \"Destination\": \"/cache\",\n    \"Mode\": \"rw\",\n    \"RW\": true,\n    \"Propagation\": \"rprivate\"\n  }\nPlease, note that docker inspect [cachev_service_id] --format '{{json .Mounts}}' | jq will show all container mounts including those specified in docker/dev/Dockerfile using VOLUME instruction.\nTo answer to your question we need to inspect test service container:\ndocker inspect [test_container_id] --format '{{json .Mounts}}' | jq:\nwould show all the volumes specified in docker/dev/Dockerfile if any and all the volumes of cachev thanks to volumes_from instruction.\nYou can see that both test and cache containers have:\n  {\n    \"Type\": \"volume\",\n    \"Name\": \"1ec7ff7c72bfb5a3259ed54be5b156ea694be6c8d932bcb3fa6e657cbcaea378\",\n    \"Source\": \"/var/lib/docker/volumes/1ec7ff7c72bfb5a3259ed54be5b156ea694be6c8d932bcb3fa6e657cbcaea378/_data\",\n    \"Destination\": \"/build\",\n    \"Driver\": \"local\",\n    \"Mode\": \"\",\n    \"RW\": true,\n    \"Propagation\": \"\"\n  }\nin their mounts and this volume survives subsequent runs of docker compose up test",
    "Docker base image not running": "When you add a CMD or ENTRYPOINT it overrides the CMD or ENTRYPOINT on the base image that you are using.\nTo extend this image do no add another CMD or ENTRYPOINT. If you want to add RUN's to execute things they will work.\nNote that you do not have to add either of the above commands to your Dockerfile. The parent image's will persist as long as you don't add more.\nIf you want to modify the CMD or add to it; I would recommend docker inspect image vidyo/mediabridge, getting the command or ENTRYPOINT that your base container is running and adding that to the end of a shell script that you run as your CMD.",
    "Cannot get kubectl environment variables on deploy": "It looks like $GCP_PROJECT_ID is not defined in the build environment, and thus it isn't included in the echo command when it runs at build time. (I presume this is because you are having kubectl define it).\nAssuming that you expect that $GCP_PROJECT_ID does not exist in the build environment and is instead something you are trying to copy from at the time your container executes (as indicated by your exec example), your problem is that the quoting of the RUN line\nRUN echo 'REACT_APP_GCP_PROJECT_ID='$GCP_PROJECT_ID >> /usr/app/.env\nIs interpreting $GCP_PROJECT_ID during the build, which of course is the empty string. In your test in the shell inside the container, it appears that $GCP_PROJECT_ID actually exists in the environment and all is well.\nYou need to actually quote the $GCP_PROJECT_ID variable to get it included into /usr/app/.env, like so (instead of being interpreted as an empty string!):\nRUN echo 'REACT_APP_GCP_PROJECT_ID=$GCP_PROJECT_ID' >> /usr/app/.env\nOf course, if you need the literal string which is the value of $GCP_PROJECT_ID to be included in the .env file, your only alternative may be to actually do the echo into .env at the beginning of the container execution (e.g. at the top of docker-entrypoint.sh, and remove all the related code from your Dockerfile).\nIf I am wrong about you having kubectl define it when the process starts, an alternative solution would require you to ensure that the docker build itself has access to that environment variable. You can do this with the --build-arg argument, such as:\ndocker build --build-arg GCP_PROJECT_ID=[correct value] .\nIn this case, you need to add an argument command to the Dockerfile, like so:\nARG GCP_PROJECT_ID\nat some point before the RUN command.",
    "Where are the Mattermost credentials set?": "Ok, so you don't have to create the users yourself...\nMy DB was unhealthy and this was preventing the application to redirect me toward the signup page (Instead took me to the login page!).\nThe following docker-compose is working for me. Hope it can help someone else:\nversion: \"2\"\n\nservices:\n\n  db:\n    build: db\n    read_only: true\n    restart: unless-stopped\n    #volumes:\n      #- ./volumes/db/var/lib/postgresql/data:/var/lib/postgresql/data\n      #- /etc/localtime:/etc/localtime:ro\n    environment:\n      - POSTGRES_USER=mmuser\n      - POSTGRES_PASSWORD=mmuser_password\n      - POSTGRES_DB=mattermost\n    # uncomment the following to enable backup\n    #  - AWS_ACCESS_KEY_ID=XXXX\n    #  - AWS_SECRET_ACCESS_KEY=XXXX\n    #  - WALE_S3_PREFIX=s3://BUCKET_NAME/PATH\n    #  - AWS_REGION=us-east-1\n\n  app:\n    build:\n      context: app\n      # uncomment following lines for team edition or change UID/GID\n      # args:\n      #   - edition=team\n      #   - PUID=1000\n      #   - PGID=1000\n    restart: unless-stopped\n    #volumes:\n      #- ./volumes/app/mattermost/config:/mattermost/config:rw\n      #- ./volumes/app/mattermost/data:/mattermost/data:rw\n      #- ./volumes/app/mattermost/logs:/mattermost/logs:rw\n      #- ./volumes/app/mattermost/plugins:/mattermost/plugins:rw\n      #- ./volumes/app/mattermost/client-plugins:/mattermost/client/plugins:rw\n      #- /etc/localtime:/etc/localtime:ro\n    environment:\n      # set same as db credentials and dbname\n      - MM_USERNAME=mmuser\n      - MM_PASSWORD=mmuser_password\n      - MM_DBNAME=mattermost\n      # in case your config is not in default location\n      #- MM_CONFIG=/mattermost/config/config.json\n\n  web:\n    build: web\n    ports:\n      - \"8080:80\"\n      - \"8443:443\"\n    read_only: true\n    restart: unless-stopped\n    #volumes:\n      # This directory must have cert files if you want to enable SSL\n      #- ./volumes/web/cert:/cert:ro\n      #- /etc/localtime:/etc/localtime:ro\n    # Uncomment for SSL\n    # environment:\n    #  - MATTERMOST_ENABLE_SSL=true",
    "When do you have to use `/bin/sh -c \"...\"` in the CMD directive in a Dockerfile?": "From the docker documentation (https://docs.docker.com/engine/reference/builder/):\nCMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form)\nCMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\nCMD command param1 param2 (shell form)\n...\nNote: Unlike the shell form, the exec form does not invoke a command shell. This means that normal shell processing does not happen. For example, CMD [ \"echo\", \"$HOME\" ] will not do variable substitution on $HOME. If you want shell processing then either use the shell form or execute a shell directly, for example: CMD [ \"sh\", \"-c\", \"echo $HOME\" ]. When using the exec form and executing a shell directly, as in the case for the shell form, it is the shell that is doing the environment variable expansion, not docker.\nSo\nCMD executable\nwhich uses the shell form is already implicitly invoking a shell. So there's no reason to explicit invoke one.",
    "What is exec in Dockerfile ENTRYPOINT and why is a process started twice without it?": "The ENTRYPOINT is run as a shell command, so what you're seeing is PID 7756, which is the shell (you can see that the CMD starts with /bin/sh), and PID 7783, which is the actual java process (you can see that the CMD starts with java).\nexec, when run by the shell replaces the shell process with the child process, so you only see the java process.\nThis is generally a good idea, because most of the time for a server (or anything running in the background really) you don't need the functionality that running inside a shell gives you (for example job control: Ctrl-Z, fg, bg, jobs...).",
    "How to connect to local mongodb from docker container of flask application": "For MacOS you should use:\nhost.docker.internal or gateway.docker.internal\nfor connecting from a container to a service on the host.\nrefer : https://docs.docker.com/docker-for-mac/networking/#/known-limitations-use-cases-and-workarounds",
    "Caching Maven with Docker and Kotlin": "Try following mvn commands, it saved me from the maven error.\nRUN  mvn --batch-mode --errors --strict-checksums --threads 1C \\\n     org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline\n\nRUN  mvn --batch-mode --errors --offline package",
    "Why does the pytorch Docker image not come with torch?": "First thing, your assumption is false, to verify this you just need to run the container from the base image, as you can check the official Dockerfile or run first the base image that is pytorch/pytorch:latest and verify does the base image working as you need?\nHere is the list of installed modules in the official image and at bottom of the list, you can see the torch.\nHere is a simple example from the torch using the base image.\nAs for your Dockerfile, so the package PIL is breaking the docker build from scratch, but this not visible if PyTorch is the base image.\nFor some reason, I am failed in the child image to find the torch so installed it using pip install and then its able to work.\nHere is the Dockerfile:\nFROM pytorch/pytorch:latest\n\nRUN apt-get update \\\n     && apt-get install -y \\\n        libgl1-mesa-glx \\\n        libx11-xcb1 \\\n     && apt-get clean all \\\n     && rm -r /var/lib/apt/lists/*\n\nRUN /opt/conda/bin/conda install --yes \\\n    astropy \\\n    matplotlib \\\n    pandas \\\n    glob2 \\\n    scikit-learn \\\n    scikit-image \\\n    numpy \\ \n    torch\nUpdated\nHere is the way to make torch available\nFROM pytorch/pytorch:latest\n\nRUN apt-get update \\\n     && apt-get install -y \\\n        libgl1-mesa-glx \\\n        libx11-xcb1 \\\n     && apt-get clean all \\\n     && rm -r /var/lib/apt/lists/*\n\nRUN /opt/conda/bin/conda install --yes \\\n    astropy \\\n    matplotlib \\\n    pandas \\\n    scikit-learn \\\n    scikit-image \n\nRUN pip install torch",
    "Dockerized spring-boot web service throws FileNotFound Exception": "Please notice extra slash in your path before train dir /data//train/models/en/token/en-token.bin\nConsider changing reading line to:\nInputStream inputStream = new FileInputStream(environment.getProperty(\"nlp.learning.dir\")+ \"train/models/en/token/en-token.bin\");",
    "How to include Python packages in docker file?": "FROM python:3                               # pull filesystem    \nCOPY requirements.txt ./                    # copy single file\nRUN pip install -r requirements.txt         # run command\nADD streaming_integration_test.py /         # add single file\nCMD python ./streaming_integration_test.py  # run command on \"docker run\"\u0148\nSo either you need to also add this:\nCOPY ./data_streamer /data_streamer         # copy folder\nwhich copies the folder (and its contents) into your new image layer or in docker run command mount the folder (on your host system) as a volume in your docker container (similar to mount command on unix systems):\n# mount host folder `data_streamer` from the current directory (pwd) to `/data_streamer`\ndocker run --volume $(pwd)/data_streamer:/data_streamer [IMAGE_NAME]",
    "How do I restrict which directories and files are copied by Docker?": "All files at and below the build directory are coppied into the initial layer of the docker build context.\nConsider using a .dockerignore file to exclude files and directories from the build.",
    "Docker build error \"Cannot fetch index base URL http://pypi.python.org/simple/\"": "You have a pip problem, not a docker problem, you need to add pip install --index-url https://pypi.python.org/simple/ --upgrade pip to your docker file:\nFROM jonasbonno/rpi-grovepi\nRUN pip install --index-url https://pypi.python.org/simple/ --upgrade pip\nRUN hash -r\nRUN pip install requests\nRUN git clone https://github.com/keyban/fogservice.git #update\nENTRYPOINT [\"python\"]\nCMD [\"fogservice/service.py\"]\nYou can find the solution here: pip connection failure: cannot fetch index base URL http://pypi.python.org/simple/",
    "Docker, Ubuntu 18.04 python3.7.2: standard_init_linux.go:207: exec user process caused \"exec format error\"": "I have ommited the ommited the #!/bin/sh part in the entrypoint.sh",
    "Issue with Docker multi-stage builds": "You will get this error pretty much universally on Linux if a binary's shared libraries aren't available. (In your debug shell, try running ldd /docker/bin/go_docker.)\nYou probably aren't expecting a dynamically-linked binary, but you're getting one because shell and environment variables don't carry across between RUN commands. Where you set CGO_ENABLED=0 at the end of a RUN step, that value gets lost when the actual go build runs two steps later.\n(I'd also clean up the Dockerfile a little bit: things like paths inside the containers don't need to be variables, and it's totally fine to use system paths for things.)\nThis leaves us with:\nFROM golang as goimage\n\n# Use standard $GOPATH layout?\n# WORKDIR /go/src/github.com/byronbaker/simple-microservice\n# COPY . .\n# RUN go get .\n# RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go install .\n\n# !!! Docker layer caching will not repeat this step if the repo changes\n# !!! You won't be able to build a test copy of your uncommitted code\nRUN git clone https://github.com/bryonbaker/simple-microservice.git /go/src/go_docker\nRUN go get github.com/gorilla/mux\n\n# vvv Put magic environment variables in this line\nRUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go install go_docker\n# ^^^\n\n# Runtime image\nFROM alpine:latest\nCOPY --from=goimage /go/bin/go_docker /bin/go_docker\nARG VERSION=1.0\nRUN echo $VERSION > /image_version\nEXPOSE 10000\nCMD [\"go_docker\"]",
    "Validating: problems found while running docker build": "How do I troubleshoot this to build a \"normal\" image?\nYou have the error right there on the screenshot. useradd failed to create the group because it already exists so the docker build was aborted. Note the the audio group is a system one so maybe you don't want to use that.\nSo either create a user with a different name or pass -g audio to the useradd command to it uses the existing group.\nIf you need to make the user creation conditional then you can use the getent command to check the user/group existence, for example:\n# create the user if doesn't exists\nRUN [ ! $(getent passwd audio) ] && echo \"useradd -ms /bin/bash audio\"\n\n# create the user and use the existing group if it exists\nRUN [ ! $(getent group audio) ] && echo \"useradd -ms /bin/bash audio -g audio\"",
    "COPY failed: no source files were specified while docker-compose up": "please check if your .gitignore file has ignored your source file.",
    "nexus 3 create docker with pre define configuration": "If I understand well, you are trying to create a portable, standalone customized nexus3 installation in a self-contained docker image for testing/distribution purpose.\nDoing this by extending the official nexus3 docker image will not work. Have a look at their Dockerfile: it defines a volume for /nexus_data and there is currently no way of removing this from a child image.\nIt means that when your start a container without any specific options, a volume is created for each new container. This is why your committed image starts with blank data. The best you can do is to name the data volume when you start the container (option -v nexus_data:/nexus_data for docker run) so that the same volume is being reused. But the data will still be in your local docker installation, not in the image.\nTo do what you wish, you need to recreate you own docker image without a data volume. You can do it from the above official Dockerfile, just remove the volume line. Then you can customize and commit your container to an image which will contain the data.",
    "How do I connect containers using container name with docker-compose?": "Actually, I spot one immediate problem:\nversion: \"0.1\"\nWhy are you doing this? The current version of the compose file format is 3.x. E.g:\nversion: \"3\"\nSee e.g. the Compose file version 3 reference.\nThe version determines which feature are available. It's entirely possible that by setting version: \"0.1\" you are explicitly disabling support for the networks parameter. You'll note that the reference shows examples using the networks attribute.\nAs an aside, unless there is a particular reason you ened it, I would drop the use of the container_name in your compose file, since this makes it impossible to run multiple instances of the same compose file on your host.",
    "Getting \"Could not locate Gemfile\" installing a Redmine plugin in docker": "For anyone looking for solution, it's here: https://it.michalczyk.pro/issues/15\nExcerpt:\nI found the problem. It's not installed under /var/lib/redmine/, it's installed under \"/usr/src/redmine\"! I was assuming /var/lib/redmine/ is the standard directory...",
    "ERROR: unsatisfiable constraints: so:libvpx.so.6 (missing)": "You are only adding the community edge repository, not main. This leads to some inconsistencies for apk.\nIt works if you change your Dockerfile:4 to the following:\nRUN echo -e \"http://dl-cdn.alpinelinux.org/alpine/edge/community\\nhttp://dl-cdn.alpinelinux.org/alpine/edge/main\" >> /etc/apk/repositories",
    "Build argument in quay.io": "Unfortunately, Quay.io does not currently support parsing of build arguments to build nodes. There's no mechanism to do so.",
    "I do not understand the syntax of docker-compose: \"volumes\" and \"services\"": "You don't need the volume section.\nA volume can be a named volume, created under the top level volumes section, like\nvolumes:\n  volumes-xyz: \nand mounted under a service with\n volumes:\n      - \"volumes-xyz:/usr/src/html/bla-source\"\nNamed volumes are managed by docker (/var/lib/docker/volumes/ on Linux).\nVolume can also be anonymous by\n volumes:\n      - \"/usr/src/html/bla-source\"\n- \".:/usr/src/html/bla-source\", on the other hand, creates a \"bind mount\". It's very similar to volume but you can choose its path to create a two-way mapping between your container and the host.",
    "JSON string interpolation from argument in Dockerfile": "To begin with, please rename your arguments using underscores:\nARG consumer_key\nARG consumer_secret\nThen, using single quotes around the argument variables properly interpolates them into the string:\nENV COMPOSER_AUTH '{ \"bitbucket-oauth\": { \"bitbucket.org\": { \"consumer-key\": \"'$consumer_key'\", \"consumer-secret\": \"'$consumer_secret'\" } } }'\nFinally, change your docker build command accordingly:\ndocker build \\\n--build-arg consumer_key=test \\\n--build-arg consumer_secret=test \\\n.",
    "Dockerfile of console application that accept arguments in Commons CLI style": "You have to add the ENV command in the DOCKERFILE so that you can receive the arguments that you are passing in and then pass that onto the ENTRYPOINT script\nDockerfile will look something like this\nFROM openjdk:8-jdk-alpine\nVOLUME /tmp\nENV arg1\nENV arg2\nENV arg3\nADD target/myProject-1.0-SNAPSHOT.jar myProject-1.0-SNAPSHOT.jar\nENV JAVA_OPTS=\"\"\nENTRYPOINT [ \"sh\", \"-c\", \"java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /myProject-1.0-SNAPSHOT.jar ${arg1} ${arg2} ${arg3}\" ]\nLet me know if you have any questions",
    "deploying image in kubernetes cluster getting CrashLoopBackOff": "I fixed this problem. I got this error because the image was not compatible with the hardware that I tried to run on (ARM7)RPi. I create the image on ubuntu 64bit using docker build for Dockerfile so that image cannot run on Raspberry pi.",
    "Use Host SSH keys for private Git repo access in DockerFile": "This kind of scenario would benefit from the recent docker build secret.\ndocker build --secret id=mysite.key,src=path/to/mysite.key .\nThat is used in your Dockerfile as:\n# syntax=docker/dockerfile:1.0.0-experimental\n\nFROM alpine\n\nRUN --mount=type=secret,id=mysite.key command-to-run\nSee more with \"Build secrets and SSH forwarding in Docker 18.09\" (your docker 2.0.0 should support it)\nIn your case, your Dockerfile should include:\nRUN --mount=type=ssh git clone git@github.com:myorg/myproject.git myproject\nOn the docker client side, you need to define that SSH forwarding is allowed for this build by using the --ssh flag.\ndocker build --ssh default .\nThe flag accepts a key-value pair defining the location for the local SSH agent socket or the private keys.",
    "Can't connect to localhost on Docker": "you should specify your command for running application like: bundle exec rails s -p 3000 -b '0.0.0.0' which run your server on port 3000 and bind it to local network 0.0.0.0. so you should write it in your Dockerfile in the last line:\nRUN bundle exec rails s -p 3000 -b '0.0.0.0'",
    "Passing Laravel .env variable to Dockerfile": "You need to define ARG and ENV values. ARG are also known as build-time variables. They are only available from the moment they are 'announced' in the Dockerfile with an ARG instruction up to the moment when the image is built. ENV variables are also available during the build, as soon as you introduce them with an ENV instruction. Here is a Dockerfile example, both for default values and without them:\nARG some_variable\n# or with a hard-coded default:\n#ARG some_variable=default_value\n\nRUN echo \"Oh dang look at that $some_variable\"\nWhen building a Docker image from the commandline, you can set ARG values using \u2013build-arg:\n$ docker build --build-arg some_variable=a_value\nRunning that command, with the above Dockerfile, will result in the following line being printed (among others):\nOh dang look at that a_value\nHere is a basic Dockerfile, using hard-coded ENV default values:\n# no default value\nENV blablabla\n# a default value\nENV foo /bar\n# or ENV foo=/bar\n\n# ENV values can be used during the build\nADD . $foo\n# or ADD . ${foo}\n# translates to: ADD . /bar\nAnd here is an example of a Dockerfile, using dynamic on-build env values:\n# expect a build-time variable\nARG A_VARIABLE\n# use the value to set the ENV var default\nENV an_env_var=$A_VARIABLE\n# if not overridden, that value of an_env_var will be available to your containers!\nIf you use docker-compose you may set it in the file (link): version: '3'\nservices:\n  php:\n    image: my_php\n      environment:\n        - MY_NEWRELIC_KEY=keykey\nEDIT: You can specify a file to read values from. The file above is called env_file (name arbitrary) and it\u2019s located in the current directory. You can reference the filename, which is parsed to extract the environment variables to set:\n$ docker run --env-file=env_file php env\nWith docker-compose.yml files, we just reference a env_file, and Docker parses it for the variables to set.\nversion: '3'\n\nservices:\n  php:\n    image: php\n      env_file: env_file",
    "Can't pass arguments to Spring Boot app running on Docker": "When using Docker you can use an OS environment variable SPRING_PROFILES_ACTIVE to specify which Spring Profiles are active:\n$ docker run -e \"SPRING_PROFILES_ACTIVE=prod\" -p 9091:9091 -t my-app:1.0-SNAPSHOT\nor\n$ docker run -e \"SPRING_PROFILES_ACTIVE=dev\" -p 9091:9091 -t my-app:1.0-SNAPSHOT\nSee Using Spring Profiles section in the Spring Boot with Docker guide for more info.",
    "Docker-compose runs 2 containers , but only one service is working?": "What the hell....\nI've figured out what was my problem.\nI will explain how I got to this answer.\nAt first I said to myself : \"Forget the yml\" , let's run it manually.\ngo to webapplication 1 folder ----> docker run -p 5555:80  -d 3f10b9720b26\ngo to webapplication 3 folder ----> docker run -p 3333:80  -d 3f10b9720b26\nAnd still I GOT same result !\nSo it's not about yml\nThen I thought , \"I'm overriding the same image again and again , what if I tag each one differently ?\"\nSo i've modified the yml to actually work with the image AND ADD A TAG : (which basically creates two different images)\nversion: '3.4'\n\nservices:\n\n  webapplication3:\n    image: microsoft/dotnet:foo\n    build: ./WebApplication3\n    ports:\n      - \"3333:80\"\n    depends_on:\n      - webapplication1\n\n\n  webapplication1:\n    image: microsoft/dotnet:bar\n    build: ./WebApplication1\n    ports:\n      - \"5555:80\"\nAnd now ..........\nSolved.\nDo not run on the same image again and again. use tags.",
    "Docker: Run echo command don't work on my window container": "Your image doesn't have a command called echo.\nA FROM scratch image contains absolutely nothing at all. No shells, no libraries, no system programs, nothing. The two most common uses for it are to build a base image from a tar file or to build an extremely minimal image from a statically-linked binary; both are somewhat advanced uses.\nUsually you'll want to start from an image that contains a more typical set of operating system tools. On a Linux base (where I'm more familiar) ubuntu and debian are common, alpine as well (though it has some occasionally compatibility issues). @gp. suggests FROM microsoft/windowsservercore in a comment and that's probably a good place to start for a Windows container.",
    "Visual Studio Dockerfile EntryPoint Override Explained?": "We can break down that command line a little differently as\ndocker run \\\n  ... some other arguments ... \\\n  --entrypoint tail \\\n  webappdockerornot:dev \\\n  -f /dev/null\nand match this against a general form\ndocker run [OPTIONS] [IMAGENAME:TAG] [CMD]\nSo the --entrypoint tail option sets the entry point to tail, and the \"command\" part is -f /dev/null. When Docker actually launches the container, it passes the command as additional arguments to the entrypoint. In the end, the net effect of this is\nIgnore what the Dockerfile said to do; after setting up the container runtime environment, run tail -f /dev/null instead.\nwhich in turn is a common way to launch a container that doesn't do anything but also stays running. Then you can use docker exec and similar debugging-oriented tools to do things inside the container.",
    "Angular 6 Application hosted by .Net Core in Docker Container throws Npm exception": "I ended up changing my Dockerfile to the following and the error went away.\nFROM microsoft/dotnet:2.1-sdk AS base\n\n# Setup NodeJs\nRUN apt-get update && \\\n    apt-get install -y wget && \\\n    apt-get install -y gnupg2 && \\\n    wget -qO- https://deb.nodesource.com/setup_8.x | bash - && \\\n    apt-get install -y build-essential nodejs\n# End setup\n\nWORKDIR /app\nEXPOSE 32772\n\nFROM microsoft/dotnet:2.1-sdk AS build\nWORKDIR /src\nCOPY src/Horizon.Client.Dispatcher/Horizon.Client.Dispatcher.csproj src/Horizon.Client.Dispatcher/\nRUN dotnet restore src/Horizon.Client.Dispatcher/Horizon.Client.Dispatcher.csproj\n\nCOPY . .\nWORKDIR /src/src/Horizon.Client.Dispatcher\nRUN dotnet build Horizon.Client.Dispatcher.csproj -c Release -o /app\n\nFROM build AS publish\nRUN dotnet publish Horizon.Client.Dispatcher.csproj -c Release -o /app\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app .\nENTRYPOINT [\"dotnet\", \"Horizon.Client.Dispatcher.dll\"]",
    "COPY from named stage fails with `no such file or directory`": "Ack!! I'm blind.\nDockerfile doesn't use = to define environment variables. Once I removed them everything started working.\nThank you @BMitch for the faster troubleshooting suggestion",
    "Argument in Dockerfile not passed as executed command": "A Dockerfile is not a shell or a build script, so it will not execute what you pass in ARG. There is a workaround - define the version as an ARG and pass that during build.\nDockerfile:\n--\nFROM ubuntu:latest\nARG LATESTWPVER\nRUN echo $LATESTWPVER\nADD https://downloads.wordpress.org/release/wordpress-$LATESTWPVER-no-content.zip /var/www/latest.zip\ndocker build --build-arg LATESTWPVER=`curl -s https://api.wordpress.org/core/version-check/1.5/ | head -n 4 | tail -n 1` .\nSending build context to Docker daemon  6.656kB\nStep 1/4 : FROM ubuntu:latest\n ---> 113a43faa138\nStep 2/4 : ARG LATESTWPVER\n ---> Using cache\n ---> 64f47dcfe7fa\nStep 3/4 : RUN echo $LATESTWPVER\n ---> Running in eb5fdd005d77\n4.9.8\nRemoving intermediate container eb5fdd005d77\n ---> 1015629b927e\nStep 4/4 : ADD https://downloads.wordpress.org/release/wordpress-$LATESTWPVER-no-content.zip /var/www/latest.zip\nDownloading [==================================================>]  7.118MB/7.118MB\n\n ---> 72f0d3790e51\nSuccessfully built 72f0d3790e51",
    "Override a volume when Building docker image from another docker image": "This approach seems to work best until the Docker development team adds the capability you are looking for.\nDockerfile\nFROM percona:5.7.24 as dbdata\nMAINTAINER monkey@blackmirror.org\nFROM centos:7\nUSER root\nCOPY --from=dbdata / /\nDo whatever you want . This eliminates the VOLUME issue. Heck maybe I'll write tool to automatically do this :)",
    "Opposite command to ENTRYPOINT": "Sure, ENTRYPOINT can do that. It takes the CMD as command-line arguments. Usually your ENTRYPOINT script will want to exec \"$@\" to run the CMD after doing its setup, but if you're willing to take on the responsibility of being process ID 1, you can run CMD as a subprocess and then do stuff afterwards.\n#!/bin/sh\necho \"BEFORE\"\n\"$@\"\nSTATUS=$?\necho \"AFTER\"\nexit $STATUS\nNote that the set of things you can usefully do at termination is pretty limited since your filesystem is about to go away.\nAlso note that this requires you to run your \"normal\" process as CMD, but for reasons like this I tend to think of that as better practice in any case. Your Dockerfile would look something like\n...\nCOPY entrypoint.sh /\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD [\"whateverd\", \"--foreground\"]",
    "Conditional logic in Dockerfile, using --build-arg": "Unfortunately you can't do this directly\nhttps://forums.docker.com/t/how-do-i-send-runs-output-to-env-in-dockerfile/16106/3\nSo you have two alternatives\nUse a shell script at start\nYou can use a shell script at the start\nCMD /start.sh\nAnd in your start.sh you can have that logic\nif [ $X == \"Y\" ]; then\n   export X=Y\nelse\n   export X=Z\nfi\nCreate a profile environment variable\nFROM alpine\n\nRUN echo \"export NAME=TARUN\" > /etc/profile.d/myenv.sh\nSHELL [\"/bin/sh\", \"-lc\"]\nCMD env\nAnd then you when you run it\n$ docker run test\nHOSTNAME=d98d44fa1dc9\nSHLVL=1\nHOME=/root\nPAGER=less\nPS1=\\h:\\w\\$\nNAME=TARUN\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPWD=/\nCHARSET=UTF-8\nNote: The SHELL [\"/bin/sh\", \"-lc\"] is quite important here, else the profile will not be loaded\nNote2: Instead of RUN echo \"export NAME=TARUN\" > /etc/profile.d/myenv.sh you can also do a COPY myevn.sh /etc/profile.d/myenv.sh and have the file be present in your build context",
    "How to set environment variables for a java project running through docker?": "If you want to run with docker use the -e flag\nMore information from the docs:\nhttps://docs.docker.com/engine/reference/run/#env-environment-variables",
    "Dockerfile ARG Variables": "Putting curly braces around your variables, instead of just in potentially ambiguous cases, can be considered good programming practice. So, try the curly braces. This worked for me.\nUPDATED ANSWER:\nSorry, I should have tested the values that YOU provided. Yeah, to make it work I had to wrap C:\\path\\ in single quotes:\nFROM centos:latest\n\nARG destpath='C:\\path\\'\nARG javafile=java.exe\nARG javapath=${destpath}${javafile}\n\nRUN echo $javapath\nResult:\n$ docker build -t temp .\nSending build context to Docker daemon  2.048kB\nStep 1/5 : FROM centos:latest\n ---> e934aafc2206\nStep 2/5 : ARG destpath='C:\\path\\'\n ---> Running in 61f1aa0ea477\nRemoving intermediate container 61f1aa0ea477\n ---> f49332bb07f9\nStep 3/5 : ARG javafile=java.exe\n ---> Running in 7f965bea7edf\nRemoving intermediate container 7f965bea7edf\n ---> b1d66e9b07ff\nStep 4/5 : ARG javapath=${destpath}${javafile}\n ---> Running in 9cfb4e2274f3\nRemoving intermediate container 9cfb4e2274f3\n ---> 65dc408e384b\nStep 5/5 : RUN echo $javapath\n ---> Running in 7906c930caef\nC:\\path\\java.exe ##################################### there you go\nRemoving intermediate container 7906c930caef\n ---> 887ef91def32\nSuccessfully built 887ef91def32\nSuccessfully tagged temp:latest\nOLD ANSWER:\nFROM centos:latest\n\nARG destpath=hello\nARG javafile=world\nARG javapath=${destpath}${javafile}\n\nRUN echo $javapath\nMy result was as following:\n$ docker build -t temp .\nStep 1/5 : FROM centos:latest\n ---> e934aafc2206\nStep 2/5 : ARG destpath=hello\n ---> Running in 30f047122373\nRemoving intermediate container 30f047122373\n ---> 582d3a801fd0\nStep 3/5 : ARG javafile=world\n ---> Running in 78817656b729\nRemoving intermediate container 78817656b729\n ---> a3afa410e42e\nStep 4/5 : ARG javapath=${destpath}${javafile}\n ---> Running in 8baf8c862572\nRemoving intermediate container 8baf8c862572\n ---> 1a9c012e4d57\nStep 5/5 : RUN echo $javapath\n ---> Running in 48ee08e6452d\nhelloworld ############################################## there it is\nRemoving intermediate container 48ee08e6452d\n ---> 9d72ba2aab67\nSuccessfully built 9d72ba2aab67\nSuccessfully tagged temp:latest\nP.S. If this doesn't work, it's windows' fault.",
    "Can see a file in Docker container, but cannot access it": "I thought that if the Docker can see a file, it can access it.\nIn a way you are right, but also missing a piece of info. Those RUN commands are not necessarily sequentially executed since docker operates in layers, and your third RUN command is executed while your first might be skipped. In order to preserve proper execution order you need to put them in same RUN command as such so they end up on the same layer (and are updated together):\nRUN tar -xzf /dir/archive2.tar.gz -C /dir/ && \\\n    ls -l /dir/ && \\\n    ls -l /dir/dir1/\nThis is common issue, most often when this is put in Dockerfile:\nRUN apt-get update\nRUN apt-get install some-package\nInstead of this:\nRUN apt-get update && \\\n    apt-get install some-package\nNote: This is in line with best practices for usage of RUN command in Dockerfile, documented here: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#run and avoids possible confusion with caches/layes...\nTo recreate your problem here is small test to resemble similar setup to yours, depending on actual directory structure in your archive this may differ:\nDummy archive 2 with dir/dir1/somefile.txt created:\nmkdir -p ~/test-sowf/dir/dir1 && cd ~/test-sowf && echo \"Yay\" | tee --append dir/dir1/somefile.txt && tar cvzf archive2.tar.gz dir && rm -rf dir\nDockerfile created in ~/test-sowf with following content\nfrom ubuntu:latest\nCOPY archive2.tar.gz /dir/\nRUN tar xvzf /dir/archive2.tar.gz -C /dir/ && \\\n    ls -l /dir/ && \\\n    ls -l /dir/dir/dir1/\nBuild command like so:\ndocker build -t test-sowf .\nGives following result:\nSending build context to Docker daemon  5.632kB\nStep 1/3 : from ubuntu:latest\n---> 452a96d81c30\nStep 2/3 : COPY archive2.tar.gz /dir/\n---> Using cache\n---> 852ef4f706d3\nStep 3/3 : RUN tar xvzf /dir/archive2.tar.gz -C /dir/ &&     ls -l    /dir/ &&     ls -l /dir/dir/dir1/\n---> Running in b2ab281190a2\ndir/\ndir/dir1/\ndir/dir1/somefile.txt\ntotal 8\n-rw-r--r-- 1 root root  177 May 10 15:43 archive2.tar.gz\ndrwxr-xr-x 3 1000 1000 4096 May 10 15:43 dir\ntotal 4\n-rw-r--r-- 1 1000 1000 4 May 10 15:43 somefile.txt\nRemoving intermediate container b2ab281190a2\n---> 05b7dfe52e36\nSuccessfully built 05b7dfe52e36\nSuccessfully tagged test-sowf:latest\nNote that extracted files are with 1000:1000 as opposed to root:root for the archive, so unless you are not running from some other user (non root) you should not have problems with user, but, depending on your archive you might run into path problems (/dir/dir/dir1 as shown here).\ntest that file is correct, and contains 'Yay' inside:\ndocker run --rm --name test-sowf test-sowf:latest cat /dir/dir/dir1/somefile.txt\nclean the test mess afterwards (deliberatelynot using rm -rf but cleaning individual files):\ndocker rmi test-sowf && cd && rm ~/test-sowf/archive2.tar.gz && rm ~/test-sowf/Dockerfile && rmdir ~/test-sowf",
    "Gradlew init - no such file or directory": "Well it's not a problem in your dockerfile. I created dummy scripts to mimic your build steps, they go through fine:\n$ docker build .\n.\n.\nStep 6 : RUN ./gradlew init\n ---> Running in 0228bf5340d9\nThis is dummy output: init\n ---> 7835d2972bf9\nRemoving intermediate container 0228bf5340d9\nSuccessfully built 7835d2972bf9\nApparently a problem with gradlew instead, also reported here\nHope this helps.",
    "ORA-01034: ORACLE not available": "Most probably the parameters ORACLE_HOME and ORACLE_SID have not been properly set.\n$ echo $ORACLE_SID\n....                    -->> Look what you get\n$ echo $ORACLE_HOME\n....                  \n$ ps -ef | grep smon    -->> Look for smon background process which\nora_smon_InfraDB        --.. should include ORACLE_SID in it.\n\n$ ORACLE_SID=InfraDB    -->> set to a valid value like these ones.\n$ ORACLE_HOME=/u01/app/oracle/product/12.2.0.1/dbhome_1\n\n$ lsnrctl status        -->> Look whether listener is ON\n$ lsnrctl start         -->> If not, then start the listener.",
    "Fluentd container not running on Openshift": "We had the same issue in our Project. We solved this building a custom fluentd container based on the fluentd-onbuild image, which is designed to customize the base fluentd container. The Dockerfile is located here:\nhttps://github.com/Gepardec/Hogarama/blob/master/Fluentd/Dockerfile\nhttps://github.com/Gepardec/Hogarama/blob/0f91618c37493ef70eb5d83bcec3bd258edc29c3/Fluentd/Dockerfile\nThe project also contains openshift templates, which may contain some snippets you can use to adapt it to your needs:\nhttps://github.com/Gepardec/Hogarama/tree/master/Templates\nhttps://github.com/Gepardec/Hogarama/tree/0f91618c37493ef70eb5d83bcec3bd258edc29c3/Templates",
    "PowerShell Core in Debian Docker Container Error": "Instead of downloading from source and extracting it in your container, I'd recommend using the official apt installer packages for your Dockerfile from Microsoft's official Debian repository as described at:\nhttps://learn.microsoft.com/en-us/powershell/scripting/setup/installing-powershell-core-on-macos-and-linux?view=powershell-6#debian-9\nSo transforming that to Dockerfile format:\n# Install powershell related system components\nRUN apt-get install -y \\\n    gnupg curl apt-transport-https \\\n    && apt-get clean\n\n# Import the public repository GPG keys\nRUN curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n\n# Register the Microsoft's Debian repository\nRUN sh -c 'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-debian-stretch-prod stretch main\" > /etc/apt/sources.list.d/microsoft.list'\n\n# Install PowerShell\nRUN apt-get update \\\n    && apt-get install -y \\\n    powershell\n\n# Start PowerShell\nCMD pwsh\nAlternatively you can also try to start from one of the original Microsoft docker Linux images, but of course then you need to solve then the raspberry installation for yourself:\nhttps://hub.docker.com/r/microsoft/powershell/tags/",
    "Docker build cannot find file that is": "It looks like the problem is due to the fact that you have included quotes in the following directive in your Dockerfile:\nRUN \"./install.sh --user-mode --silent ./silent.cfg --ignore-cpu\"\nBecause you have included quotes, this entire string is getting evaluated as the command you want to run. Basically, what's happening is the following:\n/bin/bash -c \"\\\"./install.sh --user-mode --silent ./silent.cfg --ignore-cpu\\\"\"\nSimilarly, if you try to run this command with quotes in an interactive shell, you'll get a similar error:\n$ \"./install.sh --user-mode --silent ./silent.cfg --ignore-cpu\"\nbash: ./install.sh --user-mode --silent ./silent.cfg --ignore-cpu: No such file or directory\nRemove the quotes and it will treat ./install.sh as the command and the rest of the string as the arguments to pass in.",
    "How do avoid a docker container stop after the application is stopped": "You can simply run non exiting process in the end of entrypoint to keep the container alive, even if the main process exits. For example use\ntail -f 'some log file'",
    "Docker wants a requirement file even though I do not want it to?": "The problem is you are using a base image that expects and automatically feeds a requirements.txt file to pip.\nIf you do not want this, you should select a different base image, e.g. change your dockerfile:\nFROM python:3 # or python:3-slim, python:3-alpine or other suitable image\n\nWORKDIR /app\nADD . /app\n\nCMD [\"python\", \"helloeveryone.py\"]",
    "Trying to run a binary in the smallest possible container": "exec user process caused \"no such file or directory\"\nThis is often cause for confusion as it's a bit misleading if, like in your case, the binary actually exists. Usually this indicates a non-static binary that can't find it's libraries it is linked against, most likely glibc, but depends on the app obviously. Running ldd /app/hello should give you a list of linked libraries.\nWhat is the magic in the python:3.4 image?\nThe \"magic\" is that python:3.4 is based on glibc, while alpine is based on musl. Binaries linked against either are not compatible with the other. You have a few options:\nFor a quick test use FROM alpine-glibc, if it works you know for sure it is the missing glibc, else there might be more missing -> ldd and install any missing libraries in your Dockerfile\nBuild your binary in a musl container so it's linked against the correct standard lib if you want to use FROM alpine\nBuild a completely static binary and use any image, even scratch\nThe recently added Docker multistage feature comes in handy for the last two options.",
    "Dockerfile RUN shell-script not running during docker build": "Just add ENTRYPOINT [\"/bin/bash\", \"update_config.sh\" ] this as your last line. And also update_config.sh file to start your application and make your container in infinite loop.\nExample update_config.sh:\n    #!/bin/ash\n\n    cd /opt/emqttd/etc\n    cp ./emq.conf ./emq.conf.bak\n    sed -i 's|.*listener.ssl.external.keyfile.*|listener.ssl.external.keyfile = etc/certs/MyEMQ1.key|g' ./emq.conf\n    sed -i 's|.*listener.ssl.external.certfile.*|listener.ssl.external.certfile = etc/certs/MyEMQ1.pem|g' ./emq.conf\n    sed -i 's|.*listener.ssl.external.cacertfile.*|listener.ssl.external.cacertfile = etc/certs/MyRootCA.pem|g' ./emq.conf\n    sed -i 's|.*listener.ssl.external.verify.*|listener.ssl.external.verify = verify_peer|g' ./emq.conf\n    sh start_your_app.sh\n    touch 1.txt;tail -f 1.txt #This will make your container in running infinite so that even after all the steps of this script has been executed your container will continue running. until you kill tail -f 1.txt command.\nHope this will help. Thank you!",
    "How to pass values to docker file": "Finally, I got the solution. Here I am using the shell script file to pass the password\nThis is my docker file\nFROM ubuntu:16.04\n    RUN apt-get update && apt-get install -y wget && \\\n        wget https://dev.mysql.com/get/Downloads/MySQL-Router/mysql-router_2.1.6-1ubuntu16.04_amd64.deb && \\\n        apt-get install -y ./mysql-router_2.1.6-1ubuntu16.04_amd64.deb && \\\n        apt-get update && \\\n        apt-get install mysql-router && \\\n        useradd -ms /bin/bash router\n    USER router\n    WORKDIR /home/router\n    COPY script.sh /script.sh\n    RUN /script.sh\n    CMD [\"myrouter/start.sh\"]\nThis is my script.sh file\n#!/bin/bash\nmysqlrouter --bootstrap ic@192.168.1.136:3306 -d myrouter <<< \"password\"",
    "Copy nginx.conf file into the container when container is launched": "You can do a normal docker copy command after downloading file in your local\ndocker cp nginx.conf <container name/id>:/etc/nginx/nginx.conf",
    "Error on building Dockerfile to Image": "The error means it can't find the directory which mean it probably doesn't exist or you are doing it the wrong way.\nOne of the things you can do is to make directory and add service to it. Below is a snippet explanation that could teach or help you:\nRUN mkdir /container/\nThen ADD service to the directory you created. Thus\nADD service /container/service\nThis can only serve as what could help to put you to track. However I will advice @mohan08p answer above because that works for me.",
    "Docker script not connecting when run first time (sql dump)": "If you run docker run --help, you will see these flags\n  --health-cmd string              Command to run to check health\n  --health-interval duration       Time between running the check (ms|s|m|h) (default 0s)\n  --health-retries int             Consecutive failures needed to report unhealthy\n  --health-start-period duration   Start period for the container to initialize before starting health-retries countdown (ms|s|m|h) (default 0s)\n  --health-timeout duration        Maximum time to allow one check to run (ms|s|m|h) (default 0s)\nSo, you can use these command to check health with provided command. In your case, that command is\nmysqladmin --silent -utest_user -ptest2018 ping\nNow run as bellow\ndocker run --name test_db -d \\\n  -e MYSQL_ROOT_PASSWORD=test2018 \\\n  -e MYSQL_DATABASE=test -e MYSQL_USER=test_user -e MYSQL_PASSWORD=test2018 \\\n  -p 3306:3306 \\\n  --health-cmd=\"mysqladmin --silent -utest_user -ptest2018 ping\" \\\n  --health-interval=\"10s\" \\\n  --health-retries=6 \\\n  mysql:latest\nIf you run docker ps, you will see\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                             PORTS                    NAMES\n5d1d160ed7de        mysql:latest        \"docker-entrypoint.s\u2026\"   16 seconds ago      Up 16 seconds (health: starting)   0.0.0.0:3306->3306/tcp   test_db\nYou will see health: starting in Status.\nFinally you can use this to wait. When your mysql is ready, health will be healthy.\nSo modify your script as below\n#!/bin/bash\n\ndocker run --name test_db -d \\\n  -e MYSQL_ROOT_PASSWORD=test2018 \\\n  -e MYSQL_DATABASE=test -e MYSQL_USER=test_user -e MYSQL_PASSWORD=test2018 \\\n  -p 3306:3306 \\\n  --health-cmd=\"mysqladmin --silent -utest_user -ptest2018 ping\" \\\n  --health-interval=\"10s\" \\\n  --health-retries=6 \\\n  mysql:latest\n\n# Wait for the database service to start up.\necho \"Waiting for DB to start up...\"\nuntil [ $(docker inspect test_db --format '{{.State.Health.Status}}') == \"healthy\" ]\ndo\n  sleep 10\ndone\n\n# Run the setup script.\necho \"Setting up initial data...\"\ndocker exec -i test_db mysql -utest_user -ptest2018 test < dump.sql\nHere, following command returns health status\ndocker inspect test_db --format '{{.State.Health.Status}}'\nWait until it returns healthy.\nNote: I have used #!/bin/bash in script",
    "Can't access a MySQL DB from Python within the same Docker container": "I think your MySQL service is not running. That is why you cant connect to MySQL.\nTo confirm this you can open a terminal to your container and check if MySQL service is running.\nIf you want to run multiple services in one container you need to do a few things. Read here for a detail explanation.\nAlternatively you could have two separate containers for this, using docker-compose is quite easy to get this running. Create a docker-compose.yml file with this content:\nversion: '3'\nservices:\nmain:\n    image: mysql\n    container_name: db\n    environment:\n        - MYSQL_DATABASE=db_name\n        - MYSQL_ROOT_PASSWORD=root\n        - MYSQL_USER=user\n        - MYSQL_PASSWORD=pass\n    volumes:\n    - ./data/db:/docker-entrypoint-initdb.d # here you can import your dump\n    ports:\n    - \"3306:3306\"\nflask:\n    build: .\n    container_name: flask_app\nThen create a Dockerfile for your flask_app and place it at the same level than your docker-compose.yml. For a more detail explanation on how to run flask with docker-compose check here\nEDIT\nI added a volume now to the docker-compose.yml - ./data/db:/docker-entrypoint-initdb.d. Place your dump under this folder: ./data/db, in your host machine.\nFor example:\n|- docker-compose.yml\n|- Dockerfile\n|- data/\n|-     db/\n|-        dump.sql",
    "Unable to share/mount Volume with Docker Toolbox on Windows 10": "@sxm1972 Thank you for your effort and help.\nYou are probably using Windows Pro or a server edition. I am using Windows 10 Home edition\nHere is how I solved it, so other people using same setup can solve their issue.\nThere may be a better way to solve this, please comment if there is an efficient way.\nSo...\nFirst, the question... Why I don't see my shared volume from PC in my container.\nAns: If we use docker's Boot2Docker with VirtualBox (which I am) then whenever a volume is mounted it refers to a folder inside the Boot2Docker VM\nImage: Result of -v with docker in VirtualBox Boot2Docker\nSo with this if we try to use $ ls it will show an empty folder which in my case it did.\nSo we have to actually mount the folder to Boot2Docker VM if we want to share our files from Windows environment to Container.\nImage: Resulting Mounts Window <-> Boot2Docker <-> Container\nTo achieve this we have to manually mount the folder to VM with the following command\nvboxmanage sharedfolder add default --name \"<folder_name_on_vm>\" --hostpath \"<path_to_folder_on_windows>\" --automount\nIF YOU GET ERROR RUNNING THE COMMAND, SAYING vboxmanager NOT FOUND ADD VIRTUAL BOX FOLDER PATH TO YOUR SYSTEM PATH. FOR ME IT WAS C:\\Program Files\\Oracle\\VirtualBox\nAfter running the command, you'll see <folder_name_on_vm> on root. You can check it by docker-machine ssh default and then ls /. After confirming that the folder <folder_name_on_vm> exist, you can use it as volume to your container.\ndocker run -it -v /<folder_name_on_vm>:/source <container> sh\nHope this helps...!\nP.S If you are feeling lazy and don't wan't to mount a folder, you can place your project inside your C:/Users folder as it is mounted by default on the VM as show in the image.",
    "Running maven integration test inside docker container": "Accessibility error message :\njava.lang.IllegalAccessException: Class org.testng.internal.MethodInvocationHelper can not access a member of class IntegrationTestIT with modifiers \"public\"\nYou have to make your class public to run tests :\npublic class IntegrationTestIT {\n...\nSequencing issue :\nIf your integration tests are running in the integration-test phase, you can force the execution of the docker plugin during the pre-integration-test phase :\n<execution>\n    ...\n    <phase>pre-integration-test</phase>\n</execution>",
    "Docker Elixir mix 'command not found'": "You're having multiple FROM. And the latest one FROM node:8.2 is the one being used. Simply remove it.\nOtherwise, if you put it there with purpose - you need to explain why.",
    "Docker: mounting a volume overwrites container": "When you mount a volume, read/write are both bi-directional by default. That means that anything you write in the container will show up in the host directory and vice versa.\nBut something weird is happening in your case, right?\nIn your build process, you are cloning a git repository. During the build process, the volume does not get mounted. The git data resides in your docker image.\nNow, when you are running the docker container, you are mounting the volume. The mount path in your container will be synced with your source path. That means the container directory will be overwritten with the contents of the host directory. That's why your git data has been lost.\nPossible Solution:\nRun a script as CMD. That script can clone the git repo, among other things.\nrun.sh\n#!/bin/bash\n\n# clone the repo\nRUN git clone https://github.com/... . \nDockerfile\nRUN ADD run.sh /bin\n\n# run run.sh, install them and fire up a shell.\nCMD run.sh && npm install && /bin/bash",
    "How can I access environment variables declared in Dockerfile?": "ENV variables couldn't be injected into the file context.xml without any actions.\nYou can do the following:\n1.\nPut some well known pattern inside your context.xml file like username=USERNAME_TO_CHANGE and change it during building of a docker image:\nRUN sed -i \"s/username=USERNAME_TO_CHANGE/username=$USER/g\" context.xml\n2.\nUse envsubst which will replace the environment variables in your file with their corresponding values. For that you need to install gettext package in your image and re-name context.xml to context.xml-template for example. After that you just do the following in the Dockerfile:\nRUN envsubst < context.xml-template > context.xml",
    "Cannot connect to postgres db using docker build": "You forgot to add links to your web image\nweb:\n    build: .\n    command: python manage.py runserver 0.0.0.0:8000\n    env_file: common.env\n    depends_on:\n      - db\n      - redis\n      - search\n    links: # <- here\n      - db\n      - redis\n      - search\n    ports:\n      - '8000:8000'\n    volumes:\n      - .:/app:Z",
    "docker, mariadb doesn't start at \"init\", based in debian:stable": "mysqld alone would exit too soon.\nIf you look at a MySQL server Dockerfile, you will note its ENTRYPOINT is a script docker-entrypoint.sh which will exec mysqld in foreground.\nexec \"$@\"",
    "Does -p flag overwrite EXPOSE?": "The developer documentation you quoted is accurate, the EXPOSE entry in the Dockerfile is documentation from the developer creating the image to the users running the image.\nIf you use -P then docker will publish all exposed ports on the host (note that uppercase P, different from the lowercase option).\nFor container to container communication, you only need to be in the same docker network. Containers talk on the port the application is listening on directly, no exposing of ports or publishing to the host needed.\nPublishing the port is done at runtime with the -p option and maps a port from the host to one in the container to make it available outside of docker. Exposing ports has no impact on publishing with -p, it's just documentation, and metadata in the image.",
    "Unable to enter into Apache Karaf docker container": "As the error suggests you need export the env variable CHECK_ROOT_INSTANCE_RUNNING:\ndocker exec -it b8586730289b bash\nexport CHECK_ROOT_INSTANCE_RUNNING=false\nexit\ndocker exec -it b8586730289b /opt/karaf/bin/karaf\nUpdate:\nIf all you want is to get a shell inside container then below is all that is needed:\ndocker exec -it b8586730289b bash",
    "Dockerfile string interpolation behaviour": "Incase anyone else has this issue and is a bit confused, as it turns out there is a distinction to what is run within the Docker world and in the Container world, which is where I was getting confused.\nSo when you see the ${someVar} and $someVar that is basically a unix thing so this is correct and works fine in the Unix world, but in the windows world it seems cmd/powershell doesnt understand this and just treats it as literal.\nSo in the windows world when you are wanting to embed environment vars within your cmd/powershell stuff you need to use %someVar% and it will work.",
    "Docker - \"127.0.0.1\" is not a valid port - Django": "From the Dockerfile you provided I assume that the application is running on port 8000, so it should be:\nFROM python:3\nWORKDIR hello\nCOPY requirements.txt ./\nEXPOSE 8000\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\nWhat was clearly explained in the message:\nCommandError: \"127.0.0.1\" is not a valid port number or address:port pair.",
    "Won't squashing / flattening a docker image adversely affect registry caching?": "Squashing an image does remove the ability to use cached image layers and does increase the disk space used when you have multiple copies of the image. For this reason I've yet to see it used with my clients. The preferred way to do this is to configure the Dockerfile to maximize reuse the cache of previous builds of an image.\nIf you are seeing a 50% reduction in image size from a squash, there's often a better way to structure the Dockerfile to avoid the layer bloat. The common situation I know of that squashing improves is when you need to copy a large file from the context with a COPY and then modify or later delete that file in a future RUN command. There's no way to chain a COPY and RUN command together. You may be able to convert the COPY to a RUN curl http://local-artifact-repo/.... Or with multi-stage builds, you can now perform all the COPY and other RUN commands in one stage, and then COPY the result in the final image. The last COPY would result in an entirely new layer even if you only made a minor change, but so would chaining the commands in a RUN.",
    "DockerFile: Add supporting referenced projects in the docker copy command": "docker build only has access to its context (files in the same folder as the Dockerfile and below) when doing ADD. As such, you can't reference parent folders in this way. You'll unfortunately need to restructure your folder layout to get this to work.",
    "How to access a directory in docker container from host machine?": "From what I understand, you want to make the wildfly home on your machine accessible by the container. For that you can bind your wildfly directory on your host onto the container.\ndocker run -v <path-to-folder-on-host>:<path-in-container> ...\nThis will mount the folder on the machine onto the container at the specified location.",
    "What is the difference between EXPOSE and PUBLISH directives?": "That documentation was incorrect and has been fixed. There is no PUBLISH Dockerfile directive, it's a run time option only\n  -p, --publish list                   Publish a container's port(s) to the host\n  -P, --publish-all                    Publish all exposed ports to random ports\nFor the details of the two concepts see Difference between \"expose\" and \"publish\" in docker",
    "The best way to create docker image \"offline installer\"": "The save command is the way to go for running docker images online.\nThe size difference that you are noticing is because when you are pulling images from a registry, some layers might exist locally and are thus not pulled. So you are not pulling all the image layers, only the ones that you don't have locally.\nOn the other hand, when you are saving the image to a tar, all the layers need to be stored.",
    "Windows - Docker CMD does not execute": "I was able to get it working the way I had hoped. Though I am still working out some details in the provisioning script, this is how I ended up getting the result I wanted from the docker side of things.\nFROM microsoft/aspnet:3.5-windowsservercore-10.0.14393.1198\n\n#The shell line needs to be removed and any RUN commands need to be immediately proceeded by 'powershell' eg RUN powershell ping google.com\n#SHELL [\u201cpowershell\u201d, \u201c-Command\u201d, \u201c$ErrorActionPreference = \u2018Stop\u2019; $ProgressPreference = \u2018SilentlyContinue\u2019;\u201d]\n\nCOPY *.ps1 /Container/\n\nCOPY [\u201cwwwroot\u201d, \u201c/inetpub/wwwroot\u201d]\nCOPY [\u201cAdmin\u201d, \u201c/Program Files (x86)Admin\u201d]\nCOPY [\u201cAdmin/web.config\u201d, \u201c/Program Files (x86)/Admin/web_default.config\u201d]\n\nENV myParm1 Hiiii\nENV myParm2 123\nENTRYPOINT [\"powershell\", \"-NoProfile\", \"-Command\", \"C:\\\\Container\\\\Start-Admin.Docker.Cmd.ps1\"]\nCMD [\"-parm1 $Env:myParm1 -parm2 $Env:myParm2\"] \nThe docker run command looks like this\ndocker run -d -p 8001:80 -e \"myParm1=byeeeee\" --name=container image:v1\nI hope this helps someone else that is in my boat. Thanks for all the answers!",
    "Docker ENTRYPOINT using docker run command": "You can add the ENTRYPOINT instruction at the end of your Dockerfile.\nENTRYPOINT [\"/bin/bash\",\"/root/service.sh\"]\nOf course, you'll need to add the service.sh to your image. Again using a Dockerfile\nCOPY service.sh /root/service.sh\nIn the end it will be something like this.\nFROM docker-reg.sogeti-aws.nl:5000/ubuntu:v3\n\nCOPY service.sh /root/service.sh\nENTRYPOINT [\"/bin/bash\",\"/root/service.sh\"]",
    "Docker container Postgres connection error": "it looks like you override default postgres cmd to /bin/bash. Why do you put /bin/bash at the end of command?\ndocker run -it -p 5432:5432  --name test_db db_con /bin/bash\nTry to execute\ndocker run -it -p 5432:5432  --name test_db db_con\nAlso, postgres will be available only when db dump was restored.",
    "heroku review apps: deploying docker container": "Using heroku's new build manifest, you can use the Build API to deploy your docker apps. This means you can use git push to build a docker app. You can also use GitHub Sync, and Review Apps.",
    "Link mysql to application in docker": "--name some-app refers to the container name you want to assign to the container you are running.\napplication-that-uses-mysql refers to the image which you are using. This image requires mysql and you have connecte myqsl to this image by using the command --link some-mysql:mysql.\nHowever, Links are deprecated but you still can connect to other containers using networks. An answer here should be able to help you out in setting the connections.\nEdit: Removed duplicate and provided complete answer.",
    "Can a Dockerfile push itself to a registry?": "I am wondering why can't you have a wrapper script file (say shell or bat) around the \"Dockerfile\" to do these steps\ndocker build -t my-registry/<username>/<image>:<version> .\ndocker login my-registry\ndocker push <image>\nWhat is it so specific about \"Dockerfile\". I know, this is not addressing the question that you asked, I might have totally misunderstood your usecase, but I am just curious.\nAs others pointed out, this can be easily achieved using a CD systems like Drone.io/Travis/Jenkins etc.\nAt first this sounds to me like the decently-circulated \"Nasa's Space pen Myth\". But as I said earlier, you may have a proper valid use case which I am not aware of yet.",
    "Create Docker container for Angular CLI": "You've got a typo in there. According to the documentation, Angular CLI should be installed as follows:\nnpm install -g @angular/cli\nNotice how you have used - instead of /.\nThe error is from npm, not Docker, which is what tipped me off.",
    "Can't build a Docker container for Node app based on node:8.1 image": "Problem solved by moving the USER directive right below the FROM directive and removing the existing node_modules folder from the host volume.",
    "Dockerfile compile in local machine but fails in docker hub for automated build": "This is because a Raspberry Pi runs on arm and docker hub does not support arm only x86, if you do wish to build the docker image on docker hub you will need to make edits to your docker image. I have done this before but it is pretty old but take a look at resin io's work or my own customisation based off of that over here. This method emulates arm architecture with qemu allowing the image to run natively on arm devices like a Raspberry Pi while still having the ability to be compiled by qemu on an x86 platform like docker hub.",
    "Converting a docker-compose file to a build and run command": "Fix the image name and use network instead of the link. This is roughly equivalent to your docker-compose:\ndocker network create mynet\n\ndocker build -t testenvironment/node_js ./nodejs\ndocker build -t testenvironment/mongo ./mongo\n\ndocker run -d --network mynet -p 27017:27017 --name mongo testenvironment/mongo\ndocker run --network mynet -p 8080:8080 -e NODE_ENV=development -v $(pwd)/node_js:/home/app/chat --name nodejs -d testenvironment/node_js  node server.js\nRemove the VOLUMEs you've added in the Dockerfile.\nAs @BMitch suggests, Since you're on D4W, make sure the directory you are mounting is on a drive that's shared with the Docker VM. The menus for Docker toolbox may be different from these docs:",
    "Can't download python packages onto Docker image": "If you need to update your daemon to use a different DNS address, you can create (or modify) the /etc/docker/daemon.json file with the following:\n{\n  \"dns\": [\"8.8.8.8\", \"8.8.2.2\"]\n}\nJust replace the above IP's with your own requirements and when finished, you can run a reload or restart on your daemon to reread the file.\nsudo systemctl reload docker\nThis should change the default DNS for all new containers including those used when building.\nSee the following link for more details on what you can set in this file: https://docs.docker.com/engine/reference/commandline/dockerd/#linux-configuration-file",
    "Yarn fails running on Dockerfile": "If you still need to tweak a ubuntu:trusty image, this Dockerfile, inspired by offical docker hub node:latest has a functional yarn working.\nFROM ubuntu:trusty\n\n# Create app directory\nRUN mkdir -p /usr/src/app\n\n#use mirrors for faster apt downloads no matter where the machine that builds the image is\nRUN echo \"deb mirror://mirrors.ubuntu.com/mirrors.txt trusty main restricted universe multiverse\" > /etc/apt/sources.list; \\\n    echo \"deb mirror://mirrors.ubuntu.com/mirrors.txt trusty-updates main restricted universe multiverse\" >> /etc/apt/sources.list; \\\n    echo \"deb mirror://mirrors.ubuntu.com/mirrors.txt trusty-backports main restricted universe multiverse\" >> /etc/apt/sources.list; \\\n    echo \"deb mirror://mirrors.ubuntu.com/mirrors.txt trusty-security main restricted universe multiverse\" >> /etc/apt/sources.list\n\n#install required software before using nvm/node/npm/bower\nRUN apt-get update && apt-get install -y libfreetype6 libfontconfig curl git python build-essential\n\n#allow some limited sudo commands for user `node`\nRUN echo 'Defaults !requiretty' >> /etc/sudoers; \\\n    echo 'node ALL= NOPASSWD: /usr/sbin/dpkg-reconfigure -f noninteractive tzdata, /usr/bin/tee /etc/timezone, /bin/chown -R node\\:node /myapp' >> /etc/sudoers;\n\nRUN groupadd --gid 1000 node \\\n  && useradd --uid 1000 --gid node --shell /bin/bash --create-home node\n\n# gpg keys listed at https://github.com/nodejs/node#release-team\nRUN set -ex \\\n  && for key in \\\n    9554F04D7259F04124DE6B476D5A82AC7E37093B \\\n    94AE36675C464D64BAFA68DD7434390BDBE9B9C5 \\\n    FD3A5288F042B6850C66B31F09FE44734EB7990E \\\n    71DCFD284A79C3B38668286BC97EC7A07EDE3FC1 \\\n    DD8F2338BAE7501E3DD5AC78C273792F7D83545D \\\n    B9AE9905FFD7803F25714661B63B535A4C206CA9 \\\n    C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8 \\\n    56730D5401028683275BD23C23EFEFE93C4CFFFE \\\n  ; do \\\n    gpg --keyserver pgp.mit.edu --recv-keys \"$key\" || \\\n    gpg --keyserver keyserver.pgp.com --recv-keys \"$key\" || \\\n    gpg --keyserver ha.pool.sks-keyservers.net --recv-keys \"$key\" ; \\\n  done\n\n#change it to your required node version\nENV NPM_CONFIG_LOGLEVEL info\nENV NODE_VERSION 6.11.0\n\nRUN curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\" \\\n  && curl -SLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" \\\n  && gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\\n  && grep \" node-v$NODE_VERSION-linux-x64.tar.xz\\$\" SHASUMS256.txt | sha256sum -c - \\\n  && tar -xJf \"node-v$NODE_VERSION-linux-x64.tar.xz\" -C /usr/local --strip-components=1 \\\n  && rm \"node-v$NODE_VERSION-linux-x64.tar.xz\" SHASUMS256.txt.asc SHASUMS256.txt \\\n  && ln -s /usr/local/bin/node /usr/local/bin/nodejs\n\nENV YARN_VERSION 0.24.6\n\nRUN set -ex \\\n  && for key in \\\n    6A010C5166006599AA17F08146C2130DFD2497F5 \\\n  ; do \\\n    gpg --keyserver pgp.mit.edu --recv-keys \"$key\" || \\\n    gpg --keyserver keyserver.pgp.com --recv-keys \"$key\" || \\\n    gpg --keyserver ha.pool.sks-keyservers.net --recv-keys \"$key\" ; \\\n  done \\\n  && curl -fSLO --compressed \"https://yarnpkg.com/downloads/$YARN_VERSION/yarn-v$YARN_VERSION.tar.gz\" \\\n  && curl -fSLO --compressed \"https://yarnpkg.com/downloads/$YARN_VERSION/yarn-v$YARN_VERSION.tar.gz.asc\" \\\n  && gpg --batch --verify yarn-v$YARN_VERSION.tar.gz.asc yarn-v$YARN_VERSION.tar.gz \\\n  && mkdir -p /opt/yarn \\\n  && tar -xzf yarn-v$YARN_VERSION.tar.gz -C /opt/yarn --strip-components=1 \\\n  && ln -s /opt/yarn/bin/yarn /usr/local/bin/yarn \\\n  && ln -s /opt/yarn/bin/yarn /usr/local/bin/yarnpkg \\\n  && rm yarn-v$YARN_VERSION.tar.gz.asc yarn-v$YARN_VERSION.tar.gz\n\n# COPY YOUR APPLICATION'S FILES HERE and CHOWN\n\nUSER node\nRUN yarn --help\nI'll see at end of build that yarn --verion outputs:\nStep 14/14 : RUN yarn --version\n---> Running in 9e7d8b42cd87\n0.24.6",
    "Issue regarding the Dockerfile creation": "The usual CMD for apache2 should be\nCMD [\"/usr/sbin/apache2\", \"-D\", \"FOREGROUND\"]\nThat way, you don't have to use the \"bash\" trick to keep a foreground process running.\nAnd any exit signal will impact correctly the apache2 process, not the bash one.\nNo need for ENTRYPOINT here: Docker maintains a default entrypoint, /bin/sh.\nSo this (with CMD) is the same as:\n /bin/sh -c \u201capachectl -D FOREGROUND\u201d",
    "syntax error near unexpected token \"(\" while executing shell script for mongo db": "Space--the final frontier. At least this looks fishy:\n# Run all Insert scripts\nFILES= ./*-insert.js\nIt makes FILES empty and tries to run the js file as a shell script, causing the error message (it's a shell one-shot variable assignment command like EDITOR=vi crontab -e). Use this instead:\nfor f in *-insert.js; do /usr/bin/mongo 127.0.0.1:27017 \"$f\"; done",
    "Where execute RUN command from Dockerfile, Host or Intermediate container?": "It will create intermediate container, run all those commands specified using RUN and then commit it.",
    "Cache PIP packages in dockerfile": "First things first, the FROM ubuntu:16.04 in your dockerfile is redundant as a single image can only have one upstream.\nThe simple way to solve your problem is move your pip commands to before you add your project, so that changing your project doesn't invalidate the whole cache.\nLastly, you really don't need to use virtualenv in a container, otherwise you may be doing something wrong.\nEg:\nFROM python:3.5\n\n# MAINTAINER is deprecated. Use LABEL instead.\nLABEL maintainer \"your info here\"\n\nWORKDIR /core-project\n\nADD ./requirements.txt .\nRUN pip install -r requirements.txt\n\n# Add everything else now.\nADD . .",
    "Docker Compose not updating content": "Your Docker Compose file shows that you have a named volume in the web container. Is /usr/src/app/donare/static where your css, js, and images are supposed to come from on your Docker host? If so, perhaps you meant to mount /usr/src/app/donare/static into your container, not create a volume with that name? Or, if the css, js, and images are built into your image, then you should probably just remove that named volume.\nAssuming your css, js, and images are at the path /usr/src/app/donare/static in the container, then what's happening is that because you have a named volume there, the first time you run the web service, a volume is being created and initialized with the contents at that path. Every time you do your build and up, that same initial volume is being mounted at the path with your old files. When you stop and rm, the volume is implicitly being deleted too, causing it to be recreated and initialized with fresh files.",
    "Seeing protocol error with ln for mounted volume inside docker": "ln: Protocol error happens because you have to be administrator on your Windows Docker host to be able to create symlinks (which you are not even though you are root in your Docker container).\nIf you are running Docker on Windows 7, I guess you still use Docker Toolbox relying on VirtualBox. You can either:\nRun VirtualBox as administrator, and then start your Boot2Docker VM from there (you may have to add this existing VM in the Virtualbox session first)\nOr, if it's a one-shot command, create your symlink directly from an administrator cmd on your Windows Docker host, with mklink, it will be seen as a regular symlink in your Docker container.",
    "What environment variables can be used with a Docker image?": "If you look at any Dockerfile, search for lines that begin with ENV:\ncat Dockerfile | grep ENV\nIf only an image is available, you can always do (--format=... is optional):\ndocker history --format=\"{{.CreatedBy}}\" --no-trunc myimage  | grep ENV",
    "Local pypi server tells me 403 Forbidden when registering packages": "Expose does not expose the ports to the host machine. To do that you need to use -p or -P or create a docker-compose.yml file with ports definitions.\nSee https://docs.docker.com/engine/reference/builder/#expose and https://docs.docker.com/compose/compose-file/#ports",
    "Docker build process for javascript frontends with tons of dependencies": "I favour the 'cascading' container approach (which is one of the great features of Docker) - this is basically the same as your hybrid approach. Although nothing stopping you from having several images used in sequence rather than just a single base container of course.\nYou cascade your builds based on the hierarchy of your dependencies. You can also reduce the build time of each docker image in the chain, speeding up your continual builds.\nDownside is that this introduces more complexity as you require a new build pipeline for each separated image.\nFor transient files such as npm packages I also favour building these within the container - this makes your images and build setup more portable although I normally keep git outside of containers and let the build wrapper handle that, this keeps your git creds more secure.\nYou say your builds are slow but why is that necessarily a problem? You shouldn't need to be rebuilding all the time once you've got the environments set up? Just use volume mounts to develop against running containers and let the build process kick off in the background whenever there's a merge (or whatever) all handled by your build server.",
    "docker compose file is invalid ,additional properties not allowed tty": "tty needs to be defined as a setting on your service, not at the top level. Yaml files are space sensitive, so removing the leading spaces puts the setting at the top level where it's not valid. Use the following syntax to fix it:\nversion: '2.0'\n\nservices:\n  arcgis-server:\n    container_name: \"arcgis-server\"\n    image: \"arcgis-server:10.4.1\"\n    volumes:\n      - \"./license:/license\"\n      - \"./arcgisserver:/arcgis/server/usr/directories\"\n      - \"./config-store:/arcgis/server/usr/config-store\"\n    build:\n      context: .\n      dockerfile: \"Dockerfile\"\n    ulimits:\n      nproc: 25059\n      nofile:\n        soft: 65535\n        hard: 65535\n    ports:\n      - \"127.0.0.1:6080:6080\"\n      - \"127.0.0.1:6443:6443\"\n      - \"4001:4001\"\n      - \"4002:4002\"\n      - \"4004:4004\"\n    stdin_open: true\n    tty: true",
    "DNS resolution with the container": "In case you can reach the site by IP, it means that inside the container you are pointing to the DNS server, which does not know \"qa-zk1.com\" name.\nYou can 2 options:\nAdd your ip to the local hosts file\n/etc/hosts\nUpdate container's DNS configuration\nSee Configure container DNS for more details",
    "How to set a different Dockerfile CMD depending on environment?": "You can wrap your env-dependent commands in a script.\ncmd.sh\nif [ \"$ENV\" -eq \"production\" ]; then\n  npm prod\nfi\n\nif [ \"$ENV\" -eq \"development\" ]; then\n  npm dev\nfi\nAnd, in Dockerfile;\n...\nCOPY cmd.sh /workdir/cmd.sh\n...\nCMD [ \"./cmd.sh\" ]",
    "Container stop because of: \"httpd: Could not open configuration file /etc/httpd/conf/httpd.conf: No such file or directory\", why?": "You have this in your php-fpm Dockerfile:\nVOLUME [\"/data/www\", \"/etc/httpd\"]\nAnd this in docker-compose.yml:\nhttpd:\n    build: docker/httpd\n    ports:\n        - \"8080:80\"\n    volumes_from:\n        - php-fpm\nSo your httpd image ends up with a volume mount at /etc/httpd. The contents of that volume override whatever was baked into the httpd image at build time.",
    "Dockerfile with multiple base image": "The docker way is have small and lightweight image as possible. You production image does not need a java, selenium and etc...\nBuilding and testing application must be outside of container. It can be another image (with selenium, java, etc; or building cluster with multiple containers like selenium, java, etc) for building production images.",
    "docker-compose don't load additional env-file defined in yaml file": "When .env file is present in the folder docker-compose command is executed, those environment variables are used as environment variables for docker-compose execution and variable substitution. (https://docs.docker.com/compose/env-file/).\nHowever when you define env_file option to your service, the service will get those variables from the file as environment variables and those are not used for variable substitution.",
    "How to access files outside of docker-compose build context?": "Docker now allows having the Dockerfile outside the build context (fixed in 18.03.0-ce, https://github.com/docker/cli/pull/886). So you can also do something like\ndocker build -f ./Dockerfile ../context",
    "Docker container cannot clone public Github repo": "Perhaps I should just use HTTPS instead of SSH ?\nYes, if you didn't add the ssh key.\nBut still, why the above?\nYou need to add your public key to GitHub in order to clone through SSH -- even if it is a public repo. Why? Because git needs to authenticate though SSH to GitHub's servers in order to do the clone. And in order to do that, they need to have your public key.",
    "building go package using docker": "It depends on what you mean by \"not working\", but RUN ./bin/... means RUN from the current working directory (/go/src/app in golang/1.6/onbuild/Dockerfile).\nAnd go build in Makefile would put the binary in\n$GOPATH/src/github.com/siddontang/go-mysql-elasticsearch/bin/...\nSo you need to add to your Dockerfile:\nWORKDIR $GOPATH/src/github.com/siddontang/go-mysql-elasticsearch",
    "Dockerfile Build Error": "Look closely at the syntax of the R install library line and you will see its missing a closing parenthesis\nI just manually fixed that syntax and it correctly builds that step\ncorrect syntax\nRUN R -e \"install.packages(c('shiny', 'rmarkdown'), repos='https://cloud.r-project.org/')\"\nbuild it as\ndocker build --tag r_base .\nNOTE - as docker build progresses it then fails later attempting to\nCOPY euler /root/euler\n\nlstat euler: no such file or directory\nTo troubleshot this just comment out all Dockefile lines from offending onward and replace bottom line with\nCMD [\"/bin/bash\"]\nthen it will build correctly and allow you to login to running container to further troubleshoot\ndocker run -ti r_base bash\nI know nothing of R so will leave it to the reader to fix euler COPY ... evidently you must have euler sitting in your local directory prior to issuing the docker build command\n...now after you issue above docker run command then from its internal to container prompt issue\ncd /\nfind . | grep  Rprofile.site\n\n./usr/lib/R/etc/Rprofile.site\nThat looks good so leave commented out its COPY in Dockerfile",
    "docker-compose up not working in Ubuntu 16.04": "That error would be reported if you were using a the Compose v2 format, with a version of Compose that doesn't support it.\nYou should download an official Compose build using these instructions https://docker.github.io/compose/install/\nThe package that is provided by the ubuntu repo may be pretty old.",
    "How do you use ENV variables in CMD, on an Alpine based image?": "Apparently Alpine doesn't run the application from a sub-shell, following this pattern will allow it to work correctly.\nENV WORKERS=2\nENV WORKER_TIMEOUT=60\n\nEXPOSE 5000\n\nCMD [\"/bin/sh\", \"-c\", \"/usr/local/bin/gunicorn \\\n     -b 0.0.0.0:5000 -w ${WORKERS} -t ${WORKER_TIMEOUT} app:app\"]",
    "Save docker container settings like port mapping and volumes": "You can write a docker-compose.yaml file to define the startup of your container(s). You can describe your parameters in it and start your container (with the parameters) with one single command: docker-compose up -d\nYour .yaml file will containt something like (did not test it!!)\nversion: '2'\nservices:\n  pyload-service:\n    image: falongi/pyload:latest\n    container_name: pyload\n    volumes:\n      - /mnt/rf/xxx:/mnt/mybooklive\n    ports:\n     - \"8100:8000\"\nYou can start it with: docker-compose up -d. With docker-compose you have the possibility to deploy multiple containers at the same time. You can also define which Dockerfile to use instead of building the image by yourself. Than you'll need something like this in your dockerfile:\nbuild: ./path/to/dockerfile\nThe documentation could be useful.\nEdit: When you perform docker images -a you will see the parent image and its child layers. For more information I'll point to this document",
    "Docker base images, what do they compose of?": "A Docker container is a set of processes, running a sandbox enabled by Linux namespaces, on top of the host kernel.\nA Docker image is a set of layers, which are often simply tarballs, of files that are unpacked, and made to look as if they are the root of the filesystem when used to start a container.\nA Docker image could be just a single statically-linked executable! You can create your own Docker image from scratch by simply creating a tarball of a single executable, and giving it to docker load which wI'll store it as the appropriate internal format and register it as an image.\nAs you can see then, a Docker image need not be much. It certainly doesn't need a kernel, or any of the components normally used for configuring the system, networking daemons, or even things like cron. Those are all left to the host.\nThings that are usually available in an image are a dynamic library runtime, and files like /etc/hosts, /etc/resolv.conf, and other files which are referenced directly by libc. This allows you to add typical dynamically-linked executables which interact with the system as if they're running on a traditionalal OS.\nI have successfully \"Dockerized\" a legacy CentOS 6-based VM by uninstalling as many packages as possible, then tar-ing up the filesystem (excluding directories like /proc, /sys, /dev, etc.) and loading this via docker load. Afterwards, I started a container and (sometimes forcefully) removed additional \"system\" packages that serve no purpose in a Docker image, like kernel, udev, etc.\nThis blog post goes into some of the specifics of docker load:\nhttp://tuhrig.de/difference-between-save-and-export-in-docker/",
    "how to pull docker images in a fast way. It takes too much of time just to download 50mb of image": "The latest redis alpine images should not be 50Mb.\nBut anyway, a faster way to download an image is to download its Dockerfile (and the Dockerfile of its parent image, and so on).\nHowever, the re-build step locally of all the layers might in itself be time-consuming, as noted in the comments by Chris Kitching.\nThe alpine one is easy to build (see Dockerfile), and is based on a rootfs.tar.gz if only 2.2 Mb.\nThe Dockerfile for each of those image layers is but a few Kb.",
    "How to access kurento media server running inside docker container": "I can be able to find the solution.\nIssue is with the port exposed.\nService is started in port 8443 ,But I am trying to expose port 8080.\nSo Changed the same as EXPOSE 8443.\nThen in is working fine.\nCommand to run the container:\ndocker run -t -i -p 8443:8443 ouruser/webrtc /bin/bash",
    "docker-compose not found from script": "Try to add the full path to the docker-compose inside the script\nwhich docker-compose\n\n>/usr/bin/some/path/docker-compose\nThen add this to your script\n#!/bin/sh\nset -e\n/usr/bin/some/path/docker-compose up -d\nYour local PATH settings are unknown to the script called by docker. Therefore you have to name the full path.",
    "Setting arguments in docker-compose file": "The image is already built and pulled from the hub (unless you have your own Dockerfile) so you don't need the build option. Instead, pass your arguments as the command since the image appears to use an entrypoint (if their Dockerfile only had a cmd option, then you'd need to also pass /bin/haproxy-exporter in your command):\n haproxy-exporter:\n    image: prom/haproxy-exporter\n    ports:\n      - 9101:9101\n    network_mode: \"host\"\n    command: -haproxy.scrape-uri=\"http://user:pass@haproxy.example.com/haproxy?stats;csv\"",
    "Docker and Julia - how to get .jl files to run inside a container": "you have to mount the host dir in a container https://docs.docker.com/engine/userguide/containers/dockervolumes/#mount-a-host-directory-as-a-data-volume\ntry\ndocker run -it -v /home/lara/SourceCode/researchhpc/wind_spacer/julia_learning:/opt/julia_learning larajordan/juliatest:0.3 \nthen run in your REPL\njulia> include(\"/opt/julia_learning/variables.jl\")",
    "Dockerhub Automated Builds tagging": "I don't think what you want is possible. The only variable you can use in the docker tag name is {sourceref}, which expands to the branch or tag name.\nI presume this was deliberate -- you'd vastly increase the number of images that Docker Hub had to store if each commit was given a different docker tag.\nYou could try using a continuous integration/deployment service to build the images outside of Docker Hub. There are many to chose from, but Travis and Circle are popular ones that should be able to do what you want.",
    "Reattaching orphaned docker volumes": "I prefer using named volumes, as you can mount them easily to a new container.\nBut for unnamed volume, I:\nrun my container (the VOLUME directive causes it to create a new volume to a new path that you can get by inspecting it)\nmove the path of the old volume to that new path.\nBefore docker volume commands, I used to do that with a script: updateDataContainerPath.sh.\nBut again, these days, none of my images have a VOLUME in them: I create separately named volumes (docker volume create), and mount them to containers at runtime (docker run -v my-named-volume:/my/path)",
    "Docker Container Based on Non Existing Image?": "You refer to danielguerra/alpine-sshd which makes reference to Source Repository of danielguerra69/alpine-sshd which lists a Dockerfile with a base image of\nFROM alpine:edge\nfor completeness here is entire Dockerfile\nFROM alpine:edge\nMAINTAINER Daniel Guerra <daniel.guerra69@gmail.com>\n\nRUN apk add --update openssh\n\nRUN ssh-keygen -f /etc/ssh/ssh_host_rsa_key -N '' -t rsa\n\nRUN sed -i \"s/UsePrivilegeSeparation.*/UsePrivilegeSeparation no/g\" /etc/ssh/sshd_config && sed -i \"s/UsePAM.*/UsePAM no/g\" /etc/ssh/sshd_config && sed -i \"s/PermitRootLogin.*/PermitRootLogin yes/g\" /etc/ssh/sshd_config && sed -i \"s/#AuthorizedKeysFile/AuthorizedKeysFile/g\" /etc/ssh/sshd_config\n\nCMD [\"/usr/sbin/sshd\",\"-D\"]\nUPDATE\nAnswer to your comment ... create an empty dir and put above Dockerfile into that dir ... then cd into that dir issue a command similar to\ndocker build --tag ${YOUR_DOCKERHUB_ID}/${THIS_IMAGENAME} .\nspecifically Daniel evidently used this command\ndocker build --tag danielguerra/ssh-container .\nNotice the trailing period ( . ) in above ... it denotes directory of Dockerfile ... then to make that image visible to the world Daniel pushed this spanking new image to Docker Hub using\ndocker login # only necessary once to authenticate your DockerHub Id\ndocker push danielguerra/ssh-container",
    "Executing a shell script within docker with RUN command": "I would suggest you to create an entrypoint.sh file:\n#!/bin/sh\n\n# Initialize start DB command\n# Pick from env variable MONGOD_START if it exists\n# else use the default value provided in quotes\nSTART_DB=${MONGOD_START:-\"mongod --fork --logpath /var/log/mongodb.log --logappend --smallfiles\"}\n\n# This will start your DB in background\n${START_DB} &\n\n# Go to startApp directory and execute commands\n`chmod +x /addAddress.py;python /addAddress.py $1; \\\n               cd /myapp/webapp ;grunt serve --force`\nThen modify your Dockerfile by removing the last line and replacing it with following 3 lines:\nCOPY entrypoint.sh /\n\nRUN chmod +x /entrypoint.sh\n\nENTRYPOINT [\"/entrypoint.sh\"]\nThen rebuild your container image using\ndocker build -t NAME:TAG .\nNow you run following command to verify if ENTRYPOINT is /entrypoint.sh\ndocker inspect NAME:TAG | less",
    "Dockerfile inconsistent caching": "I prefer to isolate the first line of a Dockerfile in its own base image:\nFROM centos\nMAINTAINER Dixie Chicks <DixieChicks@music.com>\nRUN yum update && \\\n    yum install -y epel-release && \\\n    yum install -y wget \\\n                    git \\\n                    python-devel && \\\n    wget \"https://bootstrap.pypa.io/get-pip.py\" && \\\n    python get-pip.py\nI build that base image once, then I create another Dockerfile which starts from:\nFROM myBaseImage\nThat way, I can rebuild my second Dockerfile as many time as I want, all the pre-requisite installations are already done (and never recompiled) in the first image.",
    "How to build a Docker container for JAVA web application": "You need a web server or an application server container like tomcat or Jboss (and many others) to deploy and run your java based web application. Your \"techpoint.war\" files need to be copied to the specific folder depends on each web server. For example, if you are using Tomcat then you can copy it to the /webapps folder. Tomcat will extract and deploy the war file. You can add the following to your DockerFile.\nFROM tomcat:8.5.11-jre8\nCOPY /<war_file_location>/techpoint.war /usr/local/tomcat/webapps/techpoint.war\nYou can build the image using docker build command and start the container from the created image.\ndocker build -t techpoint.\ndocker run -it --rm -p 8091:8080 techpoint\nNow Tomcat will extract and deploy your war file.How to access the deployed application depends on the webroot of your application. For example,\nhttp://<ip_address>:8091/techpoint/index.html",
    "OS name for docker images": "You can set the hostname with the -h argument to Docker run, otherwise it gets the short form of the container ID as the hostname:\n$ docker run --rm -it debian bash\nroot@0d36e1b1ac93:/# exit\nexit\n$ docker run --rm -h myhost -it debian bash\nroot@myhost:/# exit\nexit\nAs far as I know, you can't tell docker build to use a given hostname, but see Dockerfile HOSTNAME Instruction for docker build like docker run -h.",
    "Docker: Unable to execute RUN command after ADD when starting from scratch": "CMD [\"/bin/sh\"] is a simple declaration of the default command to run, so it will always work.\nCheck if the tar is indeed unpacked, as there was a similar issue in 9541: limit your Dockerfile to the ADD directive, and use docker exec or a simple ls to see what is in there (and with which owner/permission).\nIf the Dockerfile complains about a missing /bin/sh, and /bin/sh is in the tar archive... chances are that archive didn't uncompress properly.",
    "workflow for node developing with docker": "My solution is to copy node_modules directory using an ENTRYPOINT directive.\ndocker-entrypoint.sh:\n#!/bin/bash\nif ! [ -d node_modules ]; then\n        cp -a /tmp/node_modules /src/\nfi\nexec \"$@\"\n2 lines added to dockerfile:\nCOPY docker-entrypoint.sh /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]",
    "Dockerfile changes are not in Docker image": "It depends on how you set this environment variable.\nYou should use the ENV directive in the dockerfile (otherwise, you need the option -e on docker run)\nI then bash in to the running container,\nThat would be docker exec -it <yourContainer> path/to/bash, and you should find what the Dockerfile has built.",
    "Docker - how to run mongodb process as daemon": "I guess instead of building your own myname/repo image of mongo you could have an easier start with the official mongo image: https://hub.docker.com/_/mongo/\nUpdate as for the error you see, it looks like the mongo client is not installed at the wherever you execute your test. You could install it or use the mongo container: docker run -it --rm --link <id of the running mongo container>:mongo mongo mongo --host mongo",
    "docker daemon logs are missing": "When you use boot2docker or docker-machine you first need to ssh into that VM with docker-machine ssh <machineName>. There you will find the /var/log/ folder.",
    "Start tor and polipo when I launch my container": "Your Dockerfile should have a command defined like:\nCMD ./startpolipotor.sh\nThen you do not need to provide /bin/bash as a run command. Only the following should be enough:\ndocker run -i -t id_image\nBy the way: your script file creates two processes running in background. That will not work!\nDocker containers need to have always a process in foreground. In your case the container would stop immediately after executing your start script. And while stopping the SIGTERM will not properly be sent to those background processes so they will simply get killed and may leave some corrupt data.\nYou should think about using supervisord instead when you need to have multiple processes in your container started.",
    "Docker: how to set up file ownership in a data-only container?": "Use the same image you use for running the application to make your data container. For example, if you want to make a postgresql database, use the postgres image for your data container:\n$ docker run --name dc postgres echo \"Data Container\"\nThis command created the data container and exited - note that data containers aren't left running.\nIn the case of postgres, the volume ownership won't be set correctly until you use the volume to start a db:\n$ docker run -d --volumes-from dc postgres\nBut other images will set up the ownership correctly in the Dockerfile.\nOf course, if you just want to fix the permissions in a data container, just mount it and run chown:\n$ docker run --volumes-from dc postgres chown -R postgres:postgres /var/lib/postgresql/data",
    "How to run read command inside a Dockerfile?": "k10d in #docker irc channel suggested that I use COPY.\nThis worked\nHave the following\nindex index.html;\nlocation ~ /\\.ht {\n  deny all;\n}\ninside nginx_configuration/common.conf\nnginx_configuration/ is a folder that is adjacent to Dockerfile\nType the following inside the Dockerfile\n     ## copy the nginx config files\n     COPY ./nginx_configuration/common.conf /etc/nginx/common.conf\nRun the dockerfile as per normal.\ndocker build -t ubuntu1404/djangoapp .",
    "docker build on a private git repo containing the Dockerfile?": "1: I agree with Ben Whaley ADD would be the right option here. You can then use docker build . in the root folder of the repository.\n2 and 3: Everytime you make a change on hello_world.c you have to commit and push to your remote (when this is a private repo you also have to add ssh keys). Might be ok for you, but it is very impractical when bug fixing something locally.\nI had the same thoughts as you had and it took me some time to realise that docker is not made (yet) to handle this nicely.\nYou may have a look at my shell script (https://github.com/Equanox/docker-auto-build) . It enables you to easily build and run your dockerfiles inside a git repo. You can even store commonly used run cmds inside your dockerfiles. You'll find a example how to use it there.",
    "How to start a docker container inside a Docker build process": "You can't do this the way you describe it.\nFor Java applications, a very old-school approach (before multi-stage builds existed) was to build the jar file on the host system. Since one of the major selling points of Java is that the compiled class files are platform-independent, you don't necessarily need a matching toolchain in the image. Your Dockerfile could be just\nFROM eclipse-temurin:21-jre\nCOPY target/app-*.jar /app.jar\nCMD [\"java\", \"-jar\", \"/app.jar\"]\nand then you'd have a two-step build process on the host\nsudo mvn package\nsudo docker build .\nIf you can separate out the tool invocation from the rest of your build process, you could also potentially use a multi-stage build for this. Say you'd invoke the tool like\nsudo docker run --rm -u \"$(id -u)\" -v \"$PWD:/data\" \\\n  some-image \\\n  some-tool -i /data/data.in -o /data/data.out\nThen you could have a dedicated Docker build stage to run this\nFROM some-image AS build-tool\nWORKDIR /data\nCOPY data.in ./\nRUN some-tool -i data.in -o data.out\n\nFROM maven:3.9.9-eclipse-temurin-11 as builder\n...\nCOPY --from=build-tool /data/data.out src/main/resources\n...\nYou can't really run another image from within a Dockerfile without this multi-stage build setup, though. You especially can't run another Docker daemon. The Docker daemon is a little tricky to run from inside a container; the most common way to do it is to run the docker:dind image, but even that comes with the proviso that it must run as a privileged container. There's no option to run a privileged build or a privileged build step, though, so the nested Docker daemon won't work.",
    "Proper way to append paths to PATH-like variables in Dockerfile": "According to my digging in the documentation, UndefinedVar is Build Check.\nAccording to the docs:\nBuild checks are supported in: Buildx version 0.15.0 and later\nAccording to changelog:\n0.15.0 2024-06-11 The full release note for this release is available on GitHub.\nNew --call option allows setting evaluation method for a build, replacing the previous experimental --print flag. docker/buildx#2498, docker/buildx#2487, docker/buildx#2513\nIn addition to the default build method, the following methods are implemented by Dockerfile frontend:\n--call=check: Run validation routines for your build configuration. For more information about build checks, see Build checks\nSo this is a relatively new addition and doesn't have a lot of feedback.\nOf course, you can turn it off with check=skip skip-checks\nBut I cannot find proper way about fixing it neither in docs nor here and related (old) question doesn't have these problems. I've created my own issue.\nIn our case, we are using PYTHONPATH variable:\nPYTHONPATH=\"$SOFT/Stranger-${STRANGER_VERSION}/local/lib/python3.10/dist-packages:$PYTHONPATH\"\nAnd we got same warning: UndefinedVar: Usage of undefined variable '$PYTHONPATH' (line 72)\nI've tried\nPYTHONPATH=\"$SOFT/Stranger-${STRANGER_VERSION}/local/lib/python3.10/dist-packages:${PYTHONPATH:-''}\"\nDoesn't help.\nDocker devs answered in issue that docker build checks base image for presence of variable and warns only if it is undeclared. For now, they don't have inline ignores, but have plans for it. No info about working workarounds.",
    "Is there a way to create a development environment with a Dockerfile on VS Code for IAR toolchain?": "You can also get inspiration from bx-docker towards a functional Docker image for IAR and then, from there, apply any further customization.\nThen the image can be uploaded into your organization's private registry (i.e. docker push <private-registry>/<user>/bxarm:9.50.2) so that it can be pulled on-demand (i.e.) using a devcontainer.\nFor a project, it was just about creating a file .devcontainer/devcontainer.json to fulfill its requirements. Example:\n{ \n  \"name\": \"BXARM version 9.50.2\",\n  \"image\": \"<private-registry>/<user>/bxarm:9.50.2\",\n\n  // For debugging\n  \"runArgs\": [\"--privileged\"],\n\n  // Suggests installation for the project's required extensions\n  \"customizations\": {\n    \"vscode\": {\n      \"extensions\": [\n        \"ms-vscode-cpptools-extensionpack\",\n        \"ms-vscode-remote.vscode-remote-extensionpack\",\n        // For debugging\n        \"iarsystems.iar-debug\"\n      ]\n    }\n  },\n\n  // etc.\n\n  // Mount points between host and container\n  \"mounts\": [\n    \"type=volume,src=LMS2,dst=/usr/local/etc/IARSystems\",\n    // For debugging\n    \"type=bind,src=/dev/bus/usb,dst=/dev/bus/usb\"\n  ]\n}\nRunning the container in privileged mode (--privileged) was a necessary step for USB pass-through so that the IAR debugger probe got properly recognized.",
    "How can I evaluate and assign default value to ARG in dockerfile?": "You can achieve the expected effect by putting it in RUN :\nFROM ubuntu\n\nWORKDIR /app\n\nCOPY . .\n\nRUN apt-get update && apt-get install -y git\nRUN RELEASE_TAG=$(git describe --tags --always) &&\\\n    echo npm run build -- --version=$RELEASE_TAG\n\nCMD sleep inf",
    "Microsoft ODBC Driver 18 for Python Docker Image, ARM Device; Build Error": "This error is due to missing microsoft key at /usr/share/keyrings/microsoft-prod.gpg, you have two options,\nOption 1: Remove signed-by value (Recommended)\nThis approach is suggested in Microsoft notes,\nThe original content in https://packages.microsoft.com/config/debian/12/prod.list is as,\ndeb [arch=amd64,arm64,armhf signed-by=/usr/share/keyrings/microsoft-prod.gpg] https://packages.microsoft.com/debian/12/prod bookworm main\nafter removing signed-by,\ndeb [arch=amd64,arm64,armhf] https://packages.microsoft.com/debian/12/prod bookworm main\nSo, your docker file will look like,\nFROM python:latest\n\nRUN apt update\nRUN apt-get install -y gcc libc-dev g++ libffi-dev libxml2 unixodbc-dev unixodbc\n\nRUN curl https://packages.microsoft.com/keys/microsoft.asc | tee /etc/apt/trusted.gpg.d/microsoft.asc \\\n    && echo \"deb [arch=amd64,arm64,armhf] https://packages.microsoft.com/debian/12/prod bookworm main\" | tee /etc/apt/sources.list.d/mssql-release.list \\\n    && apt-get update \\\n    && ACCEPT_EULA=Y apt-get install -y msodbcsql18 \\\n    && echo 'export PATH=\"$PATH:/opt/mssql-tools18/bin\"' >> ~/.bashrc\n\nRUN odbcinst -j\n\nWORKDIR /app\n\nCOPY django-requirements.txt /app/\nRUN pip install --no-cache-dir -r django-requirements.txt\n\nCOPY . /app/\n\nCMD [\"bash\"]\nOption 2: Add microsoft key\nAdd your microsoft keyring files by following command,\ncurl -fsSL https://packages.microsoft.com/keys/microsoft.asc | sudo gpg --dearmor -o /usr/share/keyrings/microsoft-prod.gpg\nSee here for more info.",
    "Gradle docker image: can't create files in the .gradle directory": "I'm not sure that this is the best way to feed this information into your Docker image, but here's something that does what I believe you want.\n\ud83d\uddce Dockerfile\nFROM gradle:8.5-jdk21-alpine\n\nCOPY gradle.properties .gradle/gradle.properties\n# These commands are just to validate that the file is there.\nRUN ls .gradle\nRUN cat .gradle/gradle.properties\nThat assumes that you have a gradle.properties file in your local directory, which is then being copied across into .gradle/ on the image.\nOther options for sharing this information with your image would be:\njust use a project-level gradle.properties file;\nusing a volume mount to share a gradle.properties file from the host with a running container; or\npass in these settings as environment variables.\nFor the purpose of illustration I created a gradle.properties with the following content:\norg.gradle.daemon=true\norg.gradle.parallel=true\norg.gradle.configureondemand=true\nIn Step 3/4 you can see that the file is indeed in the .gradle folder and in Step 4/4 you can see that the content of the file on the image is correct.",
    "Dockerfile is not copying from the context directory (as defined in docker-compose)": "Your issue is due to the volume, which maps the current directory to the container.\nThe build context is only used during build. Not when running the container.\nChange your volume mapping to\nvolumes:\n  - ../service-one:/app\nand it should work.",
    "Cannot find the SQL Server binary in Docker Container": "As suggested in the comments the problem was in the file endings being CRLF instead of LF. Posting this answer for anyone who stumbles upon the same problem.\nUpdated Dockerfile\nFROM mcr.microsoft.com/mssql/server:2022-latest\n\nUSER root\nWORKDIR /usr/src/app\n\n# Install dos2unix functionality for converting CRLF file endings to LF\nRUN apt-get update && \\\n    apt-get install -y dos2unix && \\\n    apt-get clean\n\n# Copy the entrypoint.sh and migrations.sh files for running all the required migrations \nCOPY . .\n\n# Setup the environment for the mssql container\nENV ACCEPT_EULA=Y\nENV MSSQL_SA_PASSWORD=testPass#1234\nENV MSSQL_PID=Developer\n\n# Expose the port 1433 for other services \nEXPOSE 1433\n\n# Convert all the file endings\nRUN dos2unix ./entrypoint.sh\nRUN dos2unix ./migrations.sh\n\n# Make both the files executable\nRUN chmod +x ./entrypoint.sh \nRUN chmod +x ./migrations.sh\n\nUSER mssql\n\nCMD [\"./entrypoint.sh\"] ",
    "How to install torch in alpine": "Alpine is unlikely to work, because it uses musl which doesn't have pthread_attr_setaffinity_np (and possibly other members of glibc that Torch might need). If you are dead-set on using Alpine, you could try upgrading this image (and check that user's repos for the base images). But it would be more straightforward to just start with one of the PyTorch images.",
    "npm run build works locally but not off of a docker image": "I had same problem. Forgot my tsconfig.json",
    "Is it possible to copy files from a manifest or whitelist in a Dockerfile?": "There is no built-in way to do this. However, with a little hacking you can create an \"inverse dockerignore file\" like this:\ninclude.sh\n#!/usr/bin/env sh\n\nFILE=.dockerignore\n\n# add git indexed files\ngit ls-files > $FILE\n# add NOT symbol in front of each line to keep\nsed -i 's/^/!/' $FILE\n# add * as the first line in the file\nsed -i '1s/^/*\\n&/' $FILE\n.dockerignore\nThe result is something like this:\n*\n!.dockerignore\n!.gitignore\n!Dockerfile\n!LICENSE\n!README.md\n!bin/action.sh\n!bin/commitlint\n!commitlint-plugin-tense/blacklist.txt\n!commitlint-plugin-tense/index.js\n!commitlint-plugin-tense/index.test.js\n!commitlint-plugin-tense/package-lock.json\n!commitlint-plugin-tense/package.json\n!commitlint.config.js\n!package.json\n!yarn.lock\nConclusion\nThe first line * ignores all files. Each subsequent line in the prefixed with ! tells Docker to NOT ignore the given file. The result is essentially a whitelist of files to add using COPY directive in a Dockerfile. The main thing to watch out for is that this file can quickly become obsolete when adding or removing files and would have to be constantly maintained.",
    "Docker-compose fails to run apt-get update": "Your image is outdated and attempts to use a repository that is no longer there for the update, you can either change your build step to update your repo reference first before calling update or update your base image to one that already had such update. See https://unix.stackexchange.com/questions/743839/apt-get-update-failed-to-fetch-debian-amd64-packages-while-building-dockerfile-f",
    "Docker Image with cuda and ROS2 on Ubuntu 22.04": "I have run into the same issue. The following solution is working under the WSL2 system with Ubuntu22.04 running Docker Desktop 4.19.0 (106363). I have not tested on real Ubuntu but I should have fewer problems than I encountered in finding this solution.\nFrom this issue answer on Github that redirects to this official MS guide here, the following base Dockefile that I slightly modified to add the latest Cuda capabilities is capable of rendering OpenGL apps:\nFROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 as runtime\n\nARG DEBIAN_FRONTEND=noninteractive\n\n# Uncomment the lines below to use a 3rd party repository\n# to get the latest (unstable from mesa/main) mesa library version\nRUN apt-get update && apt install -y software-properties-common\nRUN add-apt-repository ppa:oibaf/graphics-drivers -y\n\nRUN apt update && apt install -y \\\n    vainfo \\\n    mesa-va-drivers \\\n    mesa-utils\n\nENV LIBVA_DRIVER_NAME=d3d12\nENV LD_LIBRARY_PATH=/usr/lib/wsl/lib\nCMD vainfo --display drm --device /dev/dri/card0\nThen, you can install ROS2 from apt. Regarding additional env vars: NVIDIA_VISIBLE_DEVICES should be already set to all and NVIDIA_DRIVER_CAPABILITIES to compute,utility.\nYou may want to set:\nENV NVIDIA_DRIVER_CAPABILITIES \\\n${NVIDIA_DRIVER_CAPABILITIES:+$NVIDIA_DRIVER_CAPABILITIES,}graphics,video\nor\nENV NVIDIA_DRIVER_CAPABILITIES all\nThen with the following, you control the device for the hw acceleration: Nvidia card (similarly for AMD):\nENV MESA_D3D12_DEFAULT_ADAPTER_NAME=NVIDIA\nor integrated Intel (if supported):\nENV MESA_D3D12_DEFAULT_ADAPTER_NAME=Intel\nor software, which actually for me gives faster FPS probably due to WSL2 (my system) not being optimized:\nENV LIBGL_ALWAYS_SOFTWARE=1",
    "Docker Container Copying Host Local Files": "You seem to be trying to copy files from C://.\nDockerfiles cannot access files outside of its context. This is usually the current directory, passed by the . in the command docker build -t myapp . . Therefore, you need to have that file next to the Dockerfile and not have test.txt in any .dockerignore file, as this will prevent it from being passed to the Docker daemon context (anything available to the COPY directive)\nOnly then, can you do COPY [\"text.txt\", \"/test.txt\"], for example\nPlus, dotnet runtime layer is Linux based, by default, I believe, so there is no \"C drive\" that Docker even knows about.",
    "Error building docker container due to local package import in main.go file": "On the Dockerfile, you use the following scripts:\nWORKDIR /usr/src/app\nCOPY ./backend/go.mod ./backend/go.sum ./\nRUN --mount=type=cache,mode=0777,target=/go/pkg/mod go mod download\nWith that, the go.mod file is located right under /usr/src/app, and the same path will become the root path of your go module.\nBased on your code structure, the go.mod should be located under /usr/src/app/backend, NOT in /usr/src/app.\nTry to change the script to this one below:\nFROM dev AS backend-builder\n\nWORKDIR /usr/src/app/backend\nCOPY ./backend/go.mod ./backend/go.sum ./\nRUN --mount=type=cache,mode=0777,target=/go/pkg/mod go mod download\n\nWORKDIR /usr/src/app\nCOPY . .\n\nWORKDIR /usr/src/app/backend\nRUN --mount=type=cache,mode=0777,target=/go/pkg/mod \\\n    --mount=type=cache,mode=0777,target=/.cache/go-build \\\n    CGO_ENABLED=0 GOOS=linux go build -o serve ./cmd/main.go",
    "How to run cross in a dockerfile?": "Install docker CLI RUN apt update && apt install -y docker.io\ncross internally calls docker --help to determine the engine type so the docker or podman binary must be installed.",
    "Unable to build a docker container": "Ubuntu stopped supporting the 21.04 release in January 2022. This is why you are getting this unexpected error. The 21.04 is a short-lived interim release, I suggest that you use an LTS release instead, 22.04 or 20.04:\nFROM ubuntu:22.04\nLTS releases are the \u2018enterprise grade\u2019 releases of Ubuntu and are used the most. An estimated 95% of all Ubuntu installations are LTS releases.\nRead more about Ubuntu release cycle - https://ubuntu.com/about/release-cycle.",
    "Questions about multi-arch docker image as base image in Dockerfile of project": "Q1: yes, that is true. For example, for java, how would the AMD64 binaries in that image work on ARM64?\nQ2: by taking the original Dockerfile and building it on an ARM machine. Alternatively, you can also use BuildKit to build ARM images on AMD.",
    "In Dockerfile is it necessary to set publish mode to linux?": "If you are using Framework Dependent Deployement (for example something.DLL) it is not required to specify OS.\nIf you are using Framework Dependent Executable, (for example something.exe) you can specify OS by using -r switch as below:\ndotnet publish -c Release -r <RID> --self-contained false\nNote the --self-contained swith which is set to false.\nIf you are using Self Contained Deployement, you can use dotnet publish as below:\ndotnet publish -c Release -r <RID> --self-contained true\nWhere -t is an identifier (RID) to specify the target platform. Also note the --self-contained swith which is set to true.\nNote: It is recommended to update your app to use newer version of ASP.NET Core.\nRead more about dotnet publish command here: https://learn.microsoft.com/en-us/dotnet/core/deploying/deploy-with-cli",
    "Failed to execute turbo code: 2, kind: NotFound, message: \"No such file or directory\"": "I've stumbled upon exactly the same error which only occurs for the Dockerfile CMD part if it runs anything connected with turbo.\nAfter spending almost 2 hours trying to fix it, had to end up using concurrently module as a temporary workaround for the app startup. Not the solution, but still.",
    "Buildpack: customisation needed to add opentelemetry": "You can add Opentelemetry to the end of the existing buildpack list. See the example in the docs here.\npack build samples/java -b urn:cnb:builder:paketo-buildpacks/java -b paketo-buildpacks/opentelemetry -e BP_OPENTELEMETRY_ENABLED=true\nWith Spring Boot see the Maven and Gradle examples.\n<project>\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n                <configuration>\n                    <image>\n                        <buildpacks>\n                            <buildpack>urn:cnb:builder:paketo-buildpacks/java</buildpack>\n                            <buildpack>gcr.io/paketo-buildpacks/opentelemetry</buildpack>\n                        </buildpacks>\n                        <env>\n                            <BP_OPENTELEMETRY_ENABLED>true</BP_OPENTELEMETRY_ENABLED>\n                        </env>\n                    </image>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n</project>",
    "How to preserve git modified files in docker dev container": "Let's see, currently...\n... you copy /app into the container (with same path) during container creation.\n... You mount ./.git as volume into /app/.git\nThat means /app is at the state your local directory was in when the container was created the last time.\nWhile /app/.git is reflecting your current local state (because it's a mounted volume reading from your local ./.git).\nThat's why git in the container will see working directory differences as its contents are not in the state git expects them to be (based on git's history/state stored in .git).\nSolutions:\nEither remove the /app/.git-volume from docker-compose.yml and re-create the container whenever you changed the code (to update /app/).\nOr mount ./ as volume to /app/ in the container (not just the subdir .git), instead of copying /app in the Dockerfile.",
    "Add mongo in my docker project I got ref error": "Try to replace:\ngit clone https://www.mongodb.org/static/pgp/server-6.0.asc \nwith\nwget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc\nor\ncurl -fsSL https://www.mongodb.org/static/pgp/server-6.0.asc\nin you Dockerfile.yml line 17\n( usually wget is installed by default in most linux distributions , but if it is not , you may easily install before the line 17 in the apt-get install line , same apply to curl )\nPS:\n#1 question answer:\nwget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc\n#2 question answer:\napt-get install apt-transport-https\n#3 question answer:\n wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo apt-key add -",
    "Docker container run and pause right after": "If we do not get any information from the container's logs, we have the option to start the process \"manually\". For this, we start the container with an interactive terminal (-it, -i to keep STDIN open, -t to open a pseudo-TTY) and override the entrypoint to be a shell, e.g. bash. For good measure, we want the container to be removed when it terminates (i.e. when we exit the termainal, --rm):\ndocker run ... -it --rm --entrypoint /bin/bash\nOnce inside the container, we can start the process that would have normally started through the entrypoint from the container's terminal and extract error information from here.",
    "Trouble running a docker container for react-app where package.json is in a subfolder": "This is a docker file for the frontend(client in your case). You can make a dockerfile under your client folder and build the image with docker build -t image-name:tag-name .\n# Pull the latest node image from dockerhub\nFROM node:latest\n# Create app directory \nWORKDIR /usr/src/app\n# Copy package.json and package-lock.json to the workdir\nCOPY package*.json ./ \n# Install the dependencies\nRUN npm install\n# Bundle app source\nCOPY . .\n# Run the app in docker\nCMD [\"npm\", \"start\"]",
    "How can i run vue3 \"npm run dev\" in docker?": "The single biggest thing causing you immediate trouble is this Compose override:\n# delete this line\nentrypoint: /bin/bash\nThis tries to run an interactive shell instead of your dev server; but since Compose runs background services, this shell exits immediately and you can't interact with it at all.\nMore generally, you have far too many Compose options, and a couple of them are dangerous (notably, you're replacing node:lts with your custom image). I'd reduce the Compose file to just\nversion: \"3.8\"\nservices:\n  vue:\n    build: .\n    ports:\n      - 3000:3000\n    volumes: # (probably delete this too)\n      - ./:/var/www/html/app\n    env_file:\n      - ./.env\n  nginx:\n    image: nginx:1.15\n    volumes:\n      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf\n    ports:\n      - 80:80\n      - 443:443\nFor all of the others, either Compose or your Dockerfile provides reasonable defaults. As I hint in a comment, I'd also typically delete the volumes: block, so that you're using the code built into the image and not replacing it with something else; this does conflict with your desire to simulate a local development environment using an isolated container.\n(And correspondingly, I think the actual easiest thing to do here is to run npm run dev directly on your host system, without Docker; it doesn't seem like what you show here particularly benefits from being isolated in a container.)",
    "Dockerfile alpine apk add using ARG to pin version fails [duplicate]": "The issue was becuase I was specifying the GLAB_VERSION and JQ_VERSION before the FROM ... instruction.\nUpdating the Dockerfile as follows works:\nARG ALPINE_VERSION=3.16.2\nFROM alpine:$ALPINE_VERSION\n\nARG GLAB_VERSION=1.22.0\nARG JQ_VERSION=\"1.6\"\n\nRUN apk add --no-cache \"glab~=${GLAB_VERSION}\" \"jq~=${JQ_VERSION}\"\n\nENTRYPOINT [ \"glab\" ]",
    "How can I get my Kafka brokers to communicate with each other? [duplicate]": "The problem is caused by the bridge network docker creates for you.\nHere is an excellent article what happens and how exactly the Kafka brokers discover each other.\nYou need an additional advertised listener. 127.0.0.1 is the one that is advertised to your host machine. You need one in additional for inside the docker network.\nAdd/change these configs to your dockerfile:\nkafka-1:\n   ...\n   expose: \n      - 9092\n   environment:\n      - KAFKA_BROKER_ID=1\n      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9094,INTERNAL://:9092\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9094,INTERNAL://kafka-1:9092\n      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT\n      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME: INTERNAL\n      - KAFKA_CFG_DEFAULT_REPLICATION_FACTOR=2\n      - KAFKA_CFG_NUM_PARTITIONS=2\n      - ALLOW_PLAINTEXT_LISTENER=yes\nand analogeously for kafka-2\nAdditionally you are using the bitnami image and they prefix all Kafka environment variables with KAFKA_CFG. Thats also a gotcha",
    "Astro in Docker not refresh": "Hello you can config Vite in the Astro's astro.config.mjs file to enable hot reloading in a docker container.\nexport default defineConfig({\n    vite: {\n        server: {\n            watch: { usePolling: true }\n        },\n    },\n});\nYou have more details in this issue.\nhttps://github.com/withastro/docs/issues/4003#issuecomment-1660342571",
    "What to do if the docker container hangs and does not respond to any command other than ctrl+c?": "I had to restart docker process to revive my container. There was nothing else I could do to solve it. used sudo service docker restart and then revived my container using docker run. I will try to build the dockerfile out of it in order to avoid future mishaps.",
    "Docker connect to localhost on a another container": "When you place both machines on the same network, they are able to reference each other by their service/container name. (not localhost - as localhost is a local loopback on each container). Using a dedicated network would be the ideal way to connect from container to container.\nI suspect what is happening in your case is that you are exposing 3001 from container a on the host, and 3003 from container b on the host. These ports are then open on the host, and from the host you can use localhost, however, from the containers to access the host you should use host.docker.internal which is a reference to the host machine, instead of using localhost\nHere is some further reading about host.docker.internal https://docs.docker.com/desktop/networking/#use-cases-and-workarounds-for-all-platforms\nUpdate: docker-compose example\nversion: '3.1'\n\nservices:\n  express1:\n    image: node:latest\n    networks:\n      - my-private-network\n    working_dir: /express\n    command: node express-entrypoint.js\n    volumes:\n      - ./path-to-project1:/express\n\n  express2:\n    image: node:latest\n    networks:\n      - my-private-network\n    working_dir: /express\n    command: node express-entrypoint.js\n    volumes:\n      - ./path-to-project2:/express\n  \nnetworks:\n  my-private-network:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.31.27.0/24\nThen you can reference each container by their service name, e.g: from express1 you can ping express2",
    "Which DockerImage to Run?": "I m not sure if I well understood your question, but I think you want to have 1 docker with Google Chrome, Google Driver and python3 on the same docker right ?\nI didn't know about Google Chrome on a docker, just about selenium web driver, but that won't change the answer.\nIf you know how to install this with command lines, you cant make a FROM ubuntu:latest and then add lines like : RUN apt-get update;apt-get install \"your_package\". (the apt-get update is very important if you want to install things).\nYour Dockerfile can look like this:\nFROM ubutu:latest\n\nRUN apt-get update;apt-get install iputils-ping -y; #(that s an example)\nThen you ll just have to install python like on your computer with the command apt install python3.10 for example.\nHope I helped you ;)",
    ".NET Framework 4.8 installation on Windows Docker image servercore:ltsc2019": "I deduce that you have something which needs both 4.7.2 and 4.8? Have you tried to do it the other way around?\nFROM mcr.microsoft.com/dotnet/framework/sdk:4.8-windowsservercore-ltsc2019\nADD http://go.microsoft.com/fwlink/?linkid=863265 /ndp472-kb4054530-x86-x64-allos-enu.exe\nRUN C:\\ndp472-kb4054530-x86-x64-allos-enu.exe /quiet /install",
    "Docker RUN apt-get install - Unable to locate package": "Your Dockerfile pulls ubuntu which defaults to latest tag, which is jammy-jellyfish/22.04. guile-2.0-dev doesn't exist in that version, it was upgraded to guile-2.2-dev, so you'd want to do RUN apt-get update && apt-get install -y guile-2.2-dev.\nFROM ubuntu\n\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt-get update && apt-get install -y bison guile-2.2-dev\nIf you specifically need guile-2.0-dev, you will need FROM ubuntu:focal (v20.04) or FROM ubuntu:bionic (v18.04) depending on what Ubuntu you want.\nFROM ubuntu:focal\n\nARG DEBIAN_FRONTEND=noninteractive\nRUN apt-get update && apt-get install -y bison guile-2.0-dev\nAlso see the ubuntu packages list: https://packages.ubuntu.com/cgi-bin/search_packages.pl?keywords=guile&searchon=names&release=all\nPackage guile-2.0-dev\n\n    bionic (18.04LTS) (lisp): Development files for Guile 2.0\n    2.0.13+1-5build2: amd64 arm64 armhf i386 ppc64el s390x\n    bionic-updates (lisp): Development files for Guile 2.0\n    2.0.13+1-5ubuntu0.1: amd64 arm64 armhf i386 ppc64el s390x\n    focal (20.04LTS) (lisp): Development files for Guile 2.0 [universe]\n    2.0.13+1-5.4: amd64 arm64 armhf i386 ppc64el s390x\nPackage guile-2.2-dev\n\n    bionic (18.04LTS) (lisp): Development files for Guile 2.2 [universe]\n    2.2.3+1-3build1: amd64 arm64 armhf i386 ppc64el s390x\n    bionic-updates (lisp): Development files for Guile 2.2 [universe]\n    2.2.3+1-3ubuntu0.1: amd64 arm64 armhf i386 ppc64el s390x\n    focal (20.04LTS) (lisp): Development files for Guile 2.2\n    2.2.7+1-4: amd64 arm64 armhf i386 ppc64el s390x\n    impish (21.10) (lisp): Development files for Guile 2.2\n    2.2.7+1-6build1: amd64 arm64 armhf i386 ppc64el s390x\n    jammy (22.04LTS) (lisp): Development files for Guile 2.2\n    2.2.7+1-6build2: amd64 arm64 armhf i386 ppc64el s390x\n    kinetic (lisp): Development files for Guile 2.2 [universe]\n    2.2.7+1-6build2: amd64 arm64 armhf ppc64el s390x",
    "Running django app with docker, unrecognized runserver arguments": "The main process run in a Docker container is made up of two parts. You're providing an ENTRYPOINT in the Dockerfile, and then the equivalent of a CMD with the Compose command: or docker run ./manage.py ... arguments. When you have both an ENTRYPOINT and a CMD, the CMD gets passed as additional arguments to the ENTRYPOINT; so in this case, you're running python manage.py runserver 0.0.0.0:8000 python manage.py runserver 0.0.0.0:8000.\nFor this setup I'd suggest:\nIf you want to be able to override the command when you launch the container (which is useful), prefer CMD to ENTRYPOINT in your Dockerfile. Then these command overrides will replace the command.\nIf you have a useful default command (which you do), put it as the Dockerfile CMD. You do not need to specify a Compose command:.\nYou're also trying to run migrations in a way that won't work; again, a container only runs a single process, and the last CMD or the startup-time override wins. I'd use an entrypoint wrapper script to run migrations:\n#!/bin/sh\n# entrypoint.sh\n\n# Run migrations\npython manage.py migrate\n\n# Then run the main container command (passed to us as arguments)\nexec \"$@\"\nIn your Dockerfile make sure this is COPYed in (the existing COPY command you have will do it) and make this script be the ENTRYPOINT (with JSON-array syntax).\nFROM python:3.10\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nWORKDIR /code\nCOPY requirements.txt ./\nRUN pip install -r requirements.txt\nCOPY ./ ./\nENTRYPOINT [\"./entrypoint.sh\"]                           # must be JSON-array form\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"] # not ENTRYPOINT\nIn your Compose file, you don't need to inject the code with volumes: or replace the command:, these are already part of your Dockerfile.\nversion: '3.8'\nservices:\n  db: { ... }\n  web:\n    build: .\n    ports:\n      - '8000:8000'\n    environment: { ... }\n    depends_on:\n      - db\n    # but no volumes: or command:",
    "ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied": "You're trying to globally pip install something in the entrypoint script, every time the container starts up. The ENTRYPOINT (and its CMD argument) always run with the final USER in the Dockerfile (or a docker run -u or Compose user: override) even if that appears after the ENTRYPOINT declaration.\nENTRYPOINT ...\nUSER rasa # <-- the ENTRYPOINT runs as this user\nCMD ...   # <-- passed as arguments to the ENTRYPOINT\nFor this setup, you probably want to only install the package once, in the image, rather than every time you start the container. You can extract this into the Dockerfile\nUSER root\nCOPY entrypoint.sh ./  # preserves its executable bit from the host\nRUN pip install google-cloud-storage\n...\n\nUSER rasa\nThe gcloud auth command depends on runtime environment variables that can't be put into the image, so it must run in the entrypoint script. It's possible that you'll hit a similar permission problem here, depending on what exactly that command does (the HOME environment variable might not be set and this might cause problems); you'd have to debug that separately.",
    "Accessing docker ports from browser localhost:8000 doesn't work": "Use 0.0.0.0 as gulp host address, it will serve on all interfaces, and you should be able to reach it.\nIt didn't work so far because the localhost address (127.0.0.1) isn't the same between your host network and the container network.",
    "How to dockerize a python app built with conda": "One or more of the dependencies can't be found from pip. I suggest you to look on PyPi for every dependency in the requirements.txt and to try to remember if you had to manually install one of those libraries in a particular way. Also, try cleaning up the file. I don't think those are all used in your project. If you work without venvs, you will get every library installed for every previous project when doing pip freeze. I mean, it's very strange for you to need pyqt in a dockerized application.",
    "How to export Yii2 migrations into docker container": "It is possible to run yii commands in docker. First let the yii2 container run in the background or another tab of the terminal. The yii commands can be run using the docker exec on the interactive interface which would let us interact with the running container\nsudo docker exec -i <container-ID> php yii migrate/up\nYou can get the container ID using\nsudo docker ps",
    "Why does my Go app fail with \"no such file or directory\" inside a Docker container?": "Seems to be caused by using go-ping/ping. Switching the build image to golang:1.18 (not Alpine) and the final image to gcr.io/distroless/base-debian11 fixes the issue.",
    "How to manage multiple dockerfiles in a single .NET project?": "You usually have one Dockerfile and with the use of docker buildx, you can then use --platform parameter to build and pack image for multiple platforms. I found a nice guide. When you use this approach you then push one image, and on DockerHub you then get multiple \"tags\", one for each platform.\nYou can then also use the --platform parameter running the docker image with docker run command and you can put in wanted platform and it will try to pull and run the image with that platform.\nWe only use these for simple multi arch base images, without any complex logic or installs of any \"specific\" packages. I have talked with few friends of a different company where they are using more \"intensive\" tasks. They still have one docker image and have platform/arch checks inside the Dockerfile to execute commands per architecture instead of manually managing multiple files.",
    "Installing python package from private gitlab repo in Dockerfile": "Generally speaking, you can use multi-stage docker builds to make sure your credentials don't stay in the image.\nIn your case, you might do something like this:\nFROM python:3.9.12-buster as download\nRUN apt-get update && apt-get install -y git\nRUN pip install --upgrade pip wheel\nARG GIT_USERNAME\nARG GIT_PASSWORD\n\nWORKDIR /build\nCOPY requirements.txt .\n# add password to requirements file\nRUN sed -i -E \"s|gitlab.private.net|$GIT_USERNAME:$GIT_PASSWORD@gitlab.private.net|\" requirements.txt\n\n# download dependencies and build wheels to /build/dist\nRUN python -m pip wheel -w /build/dist -r requirements.txt\n\nFROM python:3.9.12-buster as production\nWORKDIR /app\nCOPY --from=download /build/dist /wheelhouse\n# install dependencies from the wheels created in previous build stage\nRUN pip install --no-index /wheelhouse/*.whl\n\nCOPY . .\n# ... the rest of your dockerfile\nIn GitLab CI, you might use the build command like this:\nscript:\n  # ...\n  - docker build --build-arg GIT_USERNAME=gitlab-ci-token --build-arg GIT_PASSWORD=$CI_JOB_TOKEN -t $CI_REGISTRY_IMAGE .\nThen your image will be built and the final image won't contain your credentials. It will also be smaller since you don't have to install git :)\nAs a side note, you can simplify this somewhat by using the GitLab PyPI package registry.",
    "How to run bash file in Dockerfile?": "You need to specify the full path for the command when you are using the CMD array's syntax.\nCMD [\"/bin/bash\", \"start.bash\"]\nYou could also switch to the shell form of CMD\nCMD bash start.bash\nFor more information read the CMD-Documentation",
    "grpc client docker-compose to listen a port from grpc server": "i just fix it refrence link Cannot connect to my containerized gRPC server\nsolution: change localhost to container_name that you try to connect\nchannel= ManagedChannelBuilder.forAddress(\"cartservice\",9090).usePlaintext().build(); ",
    "Why PERL5LIB env variable is disappeared after `source $PERLBREW_ROOT/etc/bashrc`?": "Thank you @ikegami\nIt was unset by ${PERLBREW_HOME:-}/init. This unset required when you switch between perls and do not want theirs libraries mixed with each other.\nFor example you are on perl-5.24 and have installed libraries into its PERL5LIB, then you switched to perl-5.28 but PERL5LIB still points to modules installed by different version of perl. This may cause coredumps.",
    "unable to locate package \"software-properties-common\" docker image": "If there are resolution errors (which then disappear) during apt-update, such as\n => => # Err:2 http://security.ubuntu.com/ubuntu jammy-security InRelease                                                                                                                                 \n => => #   Temporary failure resolving 'security.ubuntu.com'    \n  => => # Err:1 http://archive.ubuntu.com/ubuntu jammy InRelease                                                                                                                                           \n => => #   Temporary failure resolving 'archive.ubuntu.com'    \nyou can try configuring dns or using host network, i.e.\ndocker build --network=host\nExcellent article about this:\nhttps://medium.com/@faithfulanere/solved-docker-build-could-not-resolve-archive-ubuntu-com-apt-get-fails-to-install-anything-9ea4dfdcdcf2",
    "Use BuildKit for docker build within Ansible": "Building with buildkit is not (yet as of 21st November 2023...) supported by the Ansible community.docker.docker_image module. Quoting the notes in documentation\nBuilding images is done using Docker daemon\u2019s API. It is not possible to use BuildKit / buildx this way.\nSo for time being, the solution is to go through shell setting the correct environment. Something like:\n- name: Build an image with BuildKit and push it to a private repo\n  vars:\n    image: local:5000/test\n    tag: v1.1.2\n  ansible.builtin.shell:\n    cmd: |-\n      docker build --secret \"id=test_app_secret_id,src={{ tempfile_1.path }}\" -t {{ image }}:{{ tag }} .\n      docker push {{ image }}:{{ tag }}\n    chdir: \"{{ role_path }}/files/test\"\n  environment: \n    DOCKER_BUILDKIT: 1",
    "failed to read Dockerfile MacOS GoLang; Trying to create an image": "It does make little sense to do a docker build inside a Dockerfile which is itself... 'docker built'.\nIf you need to add resources to an existing image, you can use COPY or ADD inside your Dockerfile.\nIf you need to compile something and put the result in an existing image, you can use a multi-stage build.",
    "How to only install production dependencies and then some, but without any development dependencies?": "You can use bash conditionals syntax, but you also need to define $NODE_ENV as environment variable:\nRUN if [ \"$NODE_ENV\" = \"production\" ]; \\\n    then npm install --only=production; \\\n    else npm install; \\\n    fi",
    "Is it possible to use a docker image that has both pyspark and pandas installed?": "pyspark (aka Spark) requires java, which doesn't seems to be installed in your image.\nYou can try something like:\nFROM amancevice/pandas\n\nRUN apt-get update \\\n  && apt-get install -y --no-install-recommends \\\n         openjdk-11-jre-headless \\\n  && apt-get autoremove -yqq --purge \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/*\nENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n\nRUN pip install -r requirements.txt\nRUN mkdir /app\nADD . /app\nWORKDIR /app\nEXPOSE 5000\nCMD [\"python\", \"app.py\"]\nNote that I also moved your requrements.txt installation before adding your code. This will save your time by using docker cache.",
    "Python - Trying to Save File into Docker Volume": "Based on the error look's you don't have the directory called \u201cnba-matches\u201d and the function save_nba_matches needs it; You could add directory validation, e.g.\nos.makedirs(PATH, exist_ok=True)\nrun.py\nimport os\nimport urllib.request\nimport json\nimport logging\n\nBASE_URL = 'https://cdn.nba.com/static/json/liveData/playbyplay/playbyplay_'\nPATH = os.path.join(os.getcwd(), 'nba-matches')\nLOGGER = logging.getLogger()\nLOGGER.setLevel(logging.INFO)\n\n\ndef save_nba_matches(match_ids):\n    for match_id in match_ids:\n        match_url = BASE_URL + match_id + '.json'\n        json_file = os.path.join(PATH, match_id+'.json')\n        web_url = urllib.request.urlopen(\n            match_url)\n        data = web_url.read()\n        encoding = web_url.info().get_content_charset('utf-8')\n        json_object = json.loads(data.decode(encoding))\n        os.makedirs(PATH, exist_ok=True)\n        with open(json_file, \"w+\") as f:\n            json.dump(json_object, f)\n        logging.info(\n            'JSON dumped into path: [' + json_file + ']')\nor just manually creating the directory nba-matches it will work",
    "ERROR: dockerfile parse error invalid field ' '": "You have a lot of invalid whitespace throughout your file, including the syntax line up top (likely resulting in it being ignored) and in the --mount options:\nRUN --mount=type=cache, target=/go/pkg/mod go mod download\nShould be\nRUN --mount=type=cache,target=/go/pkg/mod go mod download",
    "Dockerize flutter web application": "I've solved this problem. In the first line of rotation.dart the import 'package:flutter/services.Dart'; the extension is with first letter uppercase. building in Windows work, but in Docker don't.",
    "Need help determining why docker compose attempts to launch dependent containers before building dependencies": "depends_on does not wait for other containers to be \u201cready\u201d before starting only until they have been started. If you need to wait for a service to be ready, see https://docs.docker.com/compose/startup-order/ for solving it.",
    "Docker image for building solution with both C++ and C# projects": "Try this:\n# escape=`\n\nFROM mcr.microsoft.com/dotnet/framework/sdk:4.8\n\n\nSHELL [\"cmd\", \"/S\", \"/C\"]\n\nADD https://aka.ms/vs/17/release/vs_buildtools.exe C:\\vs_buildtools.exe\n\nRUN C:\\vs_buildtools.exe `\n--quiet --wait --norestart --nocache modify `\n--installPath \"%ProgramFiles(x86)%\\Microsoft Visual Studio\\2022\\BuildTools\" `\n--add Microsoft.VisualStudio.Workload.ManagedDesktopBuildTools;includeRecommended `\n--add Microsoft.VisualStudio.Workload.MSBuildTools;includeRecommended `\n--add Microsoft.VisualStudio.Workload.VCTools;includeRecommended `\n|| IF \"%ERRORLEVEL%\"==\"3010\" EXIT 0\n\nRUN del C:\\vs_buildtools.exe\n\nCOPY . .\n\nRUN nuget restore\n\nRUN MSBuild GameClient.sln /t:Rebuild /p:Configuration=Release /p:Platform=x64\nWORKDIR /Product\n\nENTRYPOINT [\"C:\\\\Product\\\\WOTS.exe\", \"&&\", \"powershell.exe\", \"-NoLogo\", \"-ExecutionPolicy\", \"Bypass\"]",
    "Docker multistage build doesn't pass arguments to second stage": "I found the problem\nIf mcr.microsoft.com/windows/nanoserver:1809 image is used then arguments should be used in %arg% format.\nIf mcr.microsoft.com/dotnet/framework/sdk:4.8 image is used then arguments should be used in $env:arg format.\nIt is confusing and I haven't found where it is documented.",
    "Run Playwright.NET tests in Docker container": "Finally I was able to find a way to run the tests inside a Docker container using a custom image. Here is what I've done:\nFROM mcr.microsoft.com/dotnet/sdk:5.0.404-focal AS build\nWORKDIR /src\nCOPY [\"PlaywrightSharp/PlaywrightSharp.csproj\", \"PlaywrightSharp/\"]\nRUN dotnet restore \"PlaywrightSharp/PlaywrightSharp.csproj\"\n\nCOPY . .\nWORKDIR \"/src/PlaywrightSharp\"\n\nRUN apt-get update -yq && apt-get upgrade -yq && apt-get install -yq curl git nano\nRUN curl -sL https://deb.nodesource.com/setup_16.x | bash - && apt-get install -yq nodejs build-essential\nRUN npm install -g npm\n\nRUN apt-get install -y wget xvfb unzip\n\n# Set up the Chrome PPA\nRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\nRUN echo \"deb http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list\n\n# Update the package list and install chrome\nRUN apt-get update -y\nRUN apt-get install -y google-chrome-stable\nRUN apt-get install -y firefox\n\n#RUN dotnet add package Microsoft.Playwright\nRUN dotnet build \"PlaywrightSharp.csproj\" -c Release -o /app/build\nRUN npx playwright install-deps\nRUN npx playwright install\n\n\nFROM build AS testrunner\nWORKDIR \"/src/PlaywrightSharp\"\nCMD [\"dotnet\", \"test\", \"--no-restore\", \"--settings:Firefox.runsettings\", \"--logger:trx\"]\nIt looks like npx playwright install-deps was not enough to run the tests and I had to install Chrome and Firefox also.",
    "Python shapely & geopandas in dockerimage": "For anyone with a similar issue: using conda did the trick\nMy Dockerfile:\nARG BASE_CONTAINER=jupyter/minimal-notebook\nFROM python:3.7\nFROM $BASE_CONTAINER\n\nUSER root\n# Installing geopandas and all it's dependencies\nRUN conda install -c conda-forge movingpandas && \\\n    conda clean --all -f -y && \\\n    rm -rf /home/$NB_USER/.cache/yarn \n\nRUN conda install -c conda-forge shapely \nHave fun with it!",
    "how can we create full stack application like frontend backend and database one docker image": "It's not a recommended to host all in one container. Usually one service will have it's own container. Let's take an example suppose you are using following for your app\nBackend (Php or NodeJS or Python)\nFrontend (Vue or React)\nDatabase (Postgres Or Mysql)\nCaching Service (Redis or Memcache)\nWebserver (Apache or Nginx)\nThis is just an example. Here you will create individual container for Backend, Frontend, Database, Cache and WebServer. This will ensure that your services will not fail due to any other dependencies and also this will encapsulate/isolate the containers and error handling is more easier. You will usually communicate between apps by exposing ports.\nBut sometimes you might want to share some data between two service. eg: If you are using your local machine for file storage then you might want the folder to be given access to Backend and Webserver (for exposing assets). In this case you will use volumes for this.\nTLDR: If you absolutely want to host it in a single dockerfile then you can use multistage build for this and use a ubuntu or alpine container and install all the package you need and expose the ports.",
    "Overwrite volume contents with container's contents": "If your application needs persistent data, it should be stored in a different directory from the application code. This can be in a dedicated /data directory or in a subdirectory of your application; the important thing is that, when you mount a volume to hold the persistent data, it does not hide your application code.\nIn a Node application, for example, you could refer to a ./data for your data files:\nimport { open } from 'fs/promises';\nimport { join } from 'path';\nconst dataDir = process.env.DATA_DIR || 'data';\nconst fh = await open(join(dataDir, 'file.txt'), 'rw');\nThen in your Dockerfile you'd need to create that directory. If you set up a non-root user, that directory, but not your code, should be owned by the user.\nFROM node:lts\n\n# Create the non-root user\nRUN adduser --system --no-create-home nonroot\n\n# Install the Node application normally\nWORKDIR /app\nCOPY package*.json .\nRUN npm ci\nCOPY index.js .\n\n# Create the data directory\nRUN mkdir data && chown nonroot data\n\n# Specify how to run the container\nUSER nonroot\nCMD [\"node\", \"index.js\"]\nThen when you launch the container, mount the volume only on the data directory, not over the entire /app tree.\ndocker run \\\n  -p 5001:5001 \\\n  --name need-backup-container \\\n  -v nodeServer-and-commandLineTool-volume:/app/data \\\n  need-backup-image\n#                                          ^^^^^^^^^\nNote that the Dockerfile as shown here would also let you use a host directory instead of a Docker named volume, and specify the host uid when you run the container. You do not need to make any changes to the image to do this.\ndocker run \\\n  -p 5002:5001 \\\n  --name same-image-with-bind-mount \\\n  -u $(id -u) \\\n  -v \"$PWD/app-data:/app/data\" \\\n  need-backup-image",
    "extend docker image preserving its entrypoint": "An image only has one ENTRYPOINT (and one CMD). In the situation you describe, your new entrypoint needs to explicitly call the old one.\n#!/bin/sh\n# new-entrypoint.sh\n\n# modify some files in the container\nsed -e 's/PLACEHOLDER/value/g' /etc/config.tmpl > /etc/config\n\n# run the original entrypoint; make sure to pass the CMD along\nexec original-entrypoint.sh \"$@\"\nRemember that setting ENTRYPOINT in a derived Dockerfile also resets CMD so you'll have to restate this.\nENTRYPOINT [\"new-entrypoint.sh\"]\nCMD original command from base image\nIt's also worth double-checking to see if the base image has some extension facility or another path to inject configuration files. Most of the standard database images will run initialization scripts from a /docker-entrypoint-initdb.d directory, which can be bind-mounted, and so you can avoid a custom image for this case; the nginx image knows how substitute environment variables; in many cases you can docker run -v to bind-mount a directory of config files into a container. Those approaches can be easier than replacing or wrapping ENTRYPOINT.",
    "Running a docker Image of Golang failed, with error 'starting container process caused: exec: \"/path\": permission denied'": "A past similar error message pointed out to a go build issue.\nBut in your case, copying a folder to /usr/local/bin/go_webhooks would make go_webhooks a folder.\nWORKDIR /app\n# means /app is a folder\nYou cannot directly execute a folder.\nYour Entrypoint needs to reference an executable inside that folder.\nYou might needs to copy the built file inside /app:\nCOPY --from=build /app/app /usr/local/bin/go_webhooks",
    "Dockerize nestjs microservices application": "Your build: { context: } directory is set wrong.\nThe image build mechanism uses a build context to send files to the Docker daemon. The dockerfile: location is relative to this directory; within the Dockerfile, the left-hand side of any COPY (or ADD) directives is always interpreted as relative to this directory (even if it looks like an absolute path; and you can't step out of this directory with ..).\nFor the setup you show, where you have multiple self-contained applications, the easiest thing is to set context: to the directory containing the application.\nbuild:\n  context: api\n  dockerfile: Dockerfile  # the default value\nOr, if you are using the default value for dockerfile, an equivalent shorthand\nbuild: api\nYou need to set the build context to a parent directory if you need to share files between images (see How to include files outside of Docker's build context?). In this case, all of the COPY instructions need to be qualified with the subdirectory in the combined source tree.\n# Dockerfile, when context: .\nCOPY api/package*.json ./\nRUN npm ci\nCOPY api/ ./\nYou should not normally need the volumes: you show. These have the core effect of (1) replacing the application in the image with whatever's on the local system, which could be totally different, and then (2) replacing its node_modules directory with a Docker anonymous volume, which will never be updated to reflect changes in the package.json file. In this particular setup you also need to be very careful that the volume mappings match the filesystem layout. I would recommend removing the volumes: block here; use a local Node for day-to-day development, maybe configuring it to point at the Docker-hosted database.\nIf you also remove things that are set in the Dockerfile (command:) and things Compose can provide reasonable defaults for (image:, container_name:, networks:) you could reduce the docker-compose.yml file to:\nversion: '3.8'\nservices:\n  api:          # without volumes:, networks:, image:, command:\n    build: api  # shorthand corrected directory-only form\n    ports:\n      - '4000:4000'\n    depends_on:\n      - mysql\n  mysql:        # without container_name:\n    image: mysql:5.7\n    restart: always\n    environment:\n      MYSQL_DATABASE: api\n      MYSQL_ROOT_USER: root\n      MYSQL_PASSWORD: 12345\n      MYSQL_ROOT_PASSWORD: root\n    ports:\n      - \"3307:3306\"\n    volumes:\n      - api_db:/var/lib/mysql\nvolumes:\n  api_db:",
    "Is is possible to build containers using Dockerfile and an external file for environement variables?": "You can use the new flag named \"--secret\".\nIt allows to mount secrets to build your image. those secrets are deletes after each layer writing.\nHowever it works with buildkit only.\nHere is a small example :\nDockerfile\nRUN --mount=type=secret,id=mysecret \\\n  MYSECRET=$(cat /run/secrets/mysecret) \\\n  && export MYSECRET \\\n  && <do something with $MYSECRET env variable>\nBuild command\nDOCKER_BUILDKIT=1 docker build --secret id=mysecret,src=docker/mysecret.txt .\nAnd here is the official documentation :\nhttps://docs.docker.com/develop/develop-images/build_enhancements/#new-docker-build-secret-information",
    "Docker CMD commad does not execute while running starting the container but works when run from within the container": "You have an entrypoint in your Dockerfile. This entrypoint will run and basically take the CMD as additional argument(s).\nThe final command that you run when starting the container looks like this\ncat ../output/result.txt python word_counter.py\nThis is likely not what you want. I suggest removing that entrypoint. Or fix it according to your needs.\nIf you want to print that file and still execute that command, you can do something like the below.\nCMD [\"python\", \"word_counter.py\"]\nENTRYPOINT [\"/bin/sh\", \"-c\", \"cat ../output/result.txt; exec $@\"]\nIt will run some command(s) as entrypoint, in this case printing the output of that file, and after that execute the CMD which is available as $@ as its standard posix shell behaviour. In any shell script it would work the same to access all arguments that were passed to the script. The benefit of using exec here is that it will run python with process id 1, which is useful when you want to send signals into the container to the python process, for example kill.\nLastly, when you start the container with the command you show\ndocker run -it -v /Users/xyz/Desktop/project/docker:/home/data proj2docker bash\nYou are overriding the CMD in the Dockerfile. So in that case, it is expected that it doesn't run python. Even if your entrypoint didn't have the former mentioned issue. If you want to always run the python program, then you need to make that part of the entrypoint. The problem you would have is that it would first run the entrypoint until it finishes and then your command, in this case bash.\nYou could run it in the background, if that's what you want. Note that there is no default CMD, but still the exec $@ which will allow you to run an arbitrary command such as bash while python is running in the background.\nENTRYPOINT [\"/bin/sh\", \"-c\", \"cat ../output/result.txt; python word_counter.py &; exec $@\"]\nIf you do a lot of work in the entrypoint it is probably cleaner to move this to a dedicated script and run this script as entrypoint, you can still call exec $@ at the end of your shell script.\nAccording to your comment, you want to run python first and then cat on the file. You could drop the entrypoint and do it just with the command.\nCMD [\"/bin/sh\", \"-c\", \"python word_counter.py && cat ../output/result.txt\"]",
    "How to set time in Docker container at build time": "So @JanGaraj's answer gave me an important lead: Alpine 3.14's release notes mention that it requires Docker >=20.10.0 (I am currently on 19.03.15).\nGoing back to Alpine 3.13's release notes:\nThe Docker version requirement is 19.03.9 [which I have]\nalong with libseccomp 2.4.2\nSimply using FROM alpine:3.13 still didn't work.\nChecking the second requirement, I had a previous version of libseccomp[2] and web-searching led me to this post: https://blog.samcater.com/fix-workaround-rpi4-docker-libseccomp2-docker-20/\nUsing the steps therein to upgrade libseccomp[2] did the trick for both alpine:3.13 and alpine:3.14!!\nThe steps to fix (from the post)\nThe steps for libseccomp2 are well documented, as this has been a problem on multiple platforms (not just RPI4). You could do a 'oneshot' installation of a newer version, which can be found here https://github.com/itzg/docker-minecraft-server/issues/755#issuecomment-781615497\nPersonally I feel the better method is to install it from the Buster Backports repo, which is very safe to add. It also means any future updates to libseccomp will be applied to the Pi.\n# Get signing keys to verify the new packages, otherwise they will not install\nrpi ~$ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 04EE7237B7D453EC 648ACFD622F3D138\n\n# Add the Buster backport repository to apt sources.list\nrpi ~$ echo 'deb http://httpredir.debian.org/debian buster-backports main contrib non-free' | sudo tee -a /etc/apt/sources.list.d/debian-backports.list\n\nrpi ~$ sudo apt update\nrpi ~$ sudo apt install libseccomp2 -t buster-backports\nNow on to the next build-time error message \ud83d\ude05",
    "How to resolve dockerfile, and docker-compose when main go files are in nested directory?": "To resolve Dockerfile in docker-compose.yml you need to change the build section as below\nversion: '3.9'\nservices:\n    backend:\n        build:\n            context: .\n            dockerfile: Dockerfile\n        ports:\n            - 8000:8000\n        volumes:\n            - .:/app\n        depends_on:\n            - mongodb_container\n\n    mongodb_container:\n        image: mongo:latest\n        environment:\n            MONGO_INITDB_ROOT_USERNAME: queue-delivery\n            MONGO_INITDB_ROOT_PASSWORD: password\n        ports:\n            - 27017:27017\n        volumes:\n            - mongodb_data_container:/data/db\n\nvolumes:\n    mongodb_data_container:\nYour Dockerfile has some issues,\nFROM golang:1.16\n\nWORKDIR /app\n# File changes must be added at the very end, to avoid the installation of dependencies again and again\nRUN curl -sSfL https://raw.githubusercontent.com/cosmtrek/air/master/install.sh | sh -s -- -b $(go env GOPATH)/bin\nCOPY go.mod .\nCOPY go.sum .  # can not find this file in the directory structure\nRUN go mod download\n\nCOPY ../.. .  # doesn't make sense, just use COPY . .\n\nCMD [\"air\"]",
    "Detecting username in Dockerfile": "The only way just as commented by folks: use ARG, next gives you a workable minimal example:\nDockerfile:\nFROM alpine:3.14.0\n\nARG GetMyUsername\n\nRUN echo ${GetMyUsername}\nRUN mkdir -p /home/${GetMyUsername}\nExecution:\ncake@cake:~/3$ docker build --build-arg GetMyUsername=`whoami` -t abc:1 . --no-cache\nSending build context to Docker daemon  2.048kB\nStep 1/4 : FROM alpine:3.14.0\n ---> d4ff818577bc\nStep 2/4 : ARG GetMyUsername\n ---> Running in 4d87a0970dbd\nRemoving intermediate container 4d87a0970dbd\n ---> 8b67912b3788\nStep 3/4 : RUN echo ${GetMyUsername}\n ---> Running in 2d68a7e93715\ncake\nRemoving intermediate container 2d68a7e93715\n ---> 100428a1c526\nStep 4/4 : RUN mkdir -p /home/${GetMyUsername}\n ---> Running in 938d10336daa\nRemoving intermediate container 938d10336daa\n ---> 939729b76f09\nSuccessfully built 939729b76f09\nSuccessfully tagged abc:1\nExplaination:\nWhen docker build, you could use whoami to get the username who run the docker build, then pass to args GetMyUsername. Then, in Dockerfile, you could use ${GetMyUsername} to get the value.",
    "How to run (multiple) dockerized angular apps with nginx inside and nginx as reverse proxy?": "You have to deploy the application as static site. You should not use ng serve on production with 4200 port.\nDo this\nBuild the application as static site\nCopy that to NgInx location /usr/share/nginx/html\nUpdate NgInx to serve from /usr/share/nginx/html\nHere is the updated NgInx file\nuser www-data;\nworker_processes auto;\npid /run/nginx.pid;\ninclude /etc/nginx/modules-enabled/*.conf;\n\nevents \n{\n     worker_connections 768;\n     # multi_accept on;\n}\n\nhttp \n{\n    upstream springboot\n    {\n        server localhost:8080  max_conns=10;\n    }\n\n    server\n    {\n        listen 80;\n        listen [::]:80;\n        server_name     <server ip>;\n\n        location /\n        {\n        root /usr/share/nginx/html;\n        try_files $uri $uri/ /index.html;\n        proxy_http_version 1.1;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header X-Forwarded-Port $server_port;\n        }\n\n    }\n\n        ##\n        # Logging Settings\n        ##\n\n        access_log /var/log/nginx/access.log;\n        error_log /var/log/nginx/error.log;\n\n        ##\n        # Gzip Settings\n        ##\n\n        gzip on;\n}",
    "Azure DevOps Pipeline Dockerfile COPY --from clause": "Came across this question while searching for an answer to the same issue. I have spent the last few hours digging through source code for the Docker task and I think I can answer your questions.\nIt appears that the Docker task tries to parse the Dockerfile to determine the base image, and there is (was) a bug in the task that it was looking for lines with FROM in them, but was incorrectly parsing the --from from the COPY --from line.\nIt then passes that base image to docker pull and docker inspect prior to calling docker build. The first two commands fail because they're being passed garbage, but the third (docker build) reads the dockerfile correctly and does a pull anyway, so it succeeds.\nIt looks like this was fixed on 2021-08-17 to only parse lines that start with FROM, so I assume it will make it to DevOps agents soon.",
    "What is the best way to structure two Docker containers that depend on common code?": "I guess you think option 2 is what you want, you didn't use it just because of next you mentioned:\nThe path must be inside the context of the build\nIf above is the truth, then, in fact you could switch the context, not point the folder where Dockerfile lies. Something like next:\n$ tree\n.\n\u251c\u2500\u2500 common\n\u2502   \u2514\u2500\u2500 common.py\n\u251c\u2500\u2500 docker-compose.yaml\n\u251c\u2500\u2500 image1\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 image2\n    \u2514\u2500\u2500 Dockerfile\n\n3 directories, 4 files\nimage1/Dockerfile:\nFROM python:3\nCOPY common common\nRUN ls common\nimage2/Dockerfile:\nFROM python:alpine\nCOPY common common\nRUN ls common\ndocker-compose.yaml:\nversion: '3'\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: image1/Dockerfile\n    tty: true\n    stdin_open: true\n  app2:\n    build:\n      context: .\n      dockerfile: image2/Dockerfile\n    tty: true\n    stdin_open: true\nThen, execute it:\n$ ls\ncommon  docker-compose.yaml  image1  image2\n$ docker-compose build --no-cache\nBuilding app1\nStep 1/3 : FROM python:3\n ---> 5b3b4504ff1f\nStep 2/3 : COPY common common\n ---> 17274c6dfa45\nStep 3/3 : RUN ls common\n ---> Running in d9f4b326e0b7\ncommon.py\nRemoving intermediate container d9f4b326e0b7\n ---> af605b7b3e1e\nSuccessfully built af605b7b3e1e\nSuccessfully tagged 20210721_app1:latest\nBuilding app2\nStep 1/3 : FROM python:alpine\n ---> 56302acacaa7\nStep 2/3 : COPY common common\n ---> cde0c866beff\nStep 3/3 : RUN ls common\n ---> Running in 7b7264d8ab9e\ncommon.py\nRemoving intermediate container 7b7264d8ab9e\n ---> 2835fe4d9c0f\nSuccessfully built 2835fe4d9c0f\nSuccessfully tagged 20210721_app2:latest\nYou can see now the common.py in both docker images, meanwhile, your Dockerfile still in different sub folders.\nAdditional, if you directly use docker build ..., above equal to next:\n$ ls\ncommon  docker-compose.yaml  image1  image2\n$ docker build -t abc:1 . -f image1/Dockerfile --no-cache\nSending build context to Docker daemon  6.144kB\nStep 1/3 : FROM python:3\n ---> 5b3b4504ff1f\nStep 2/3 : COPY common common\n ---> 4641d7ca2a98\nStep 3/3 : RUN ls common\n ---> Running in 9173c56335c9\ncommon.py\nRemoving intermediate container 9173c56335c9\n ---> 83ff4c9737c2\nSuccessfully built 83ff4c9737c2\nSuccessfully tagged abc:1\nHere, you execute docker build in project_package folder, and specify context as ., then your dockerfile definitely could find the common. The magic is you could use -f to specify the path of Dockerfile. Another word, build context & Dockerfile no need to be the same folder.",
    "Correct syntax for concatenating multiple ENV in Dockerfile?": "Environment variables are declared with the ENV statement in Dockerfiles. You can try the following approach\nDockerfile:\nFROM alpine\n\nENV parent=parent_dir\nENV child=child_dir\n\nWORKDIR /tmp\n\n#create nested dir structure in /tmp\nRUN mkdir -p $parent\\/$child\nAfter you build the image and start the container , you will have the following structure inside the container:\n/ # tree /tmp/\n/tmp/\n\u2514\u2500\u2500 parent_dir\n    \u2514\u2500\u2500 child_dir",
    "Error during template compile of module. Could not resolve component relative to [object Object]": "It seems like the error occurs due to docker engine version issue. I was using latest and switched to older version, 18.03.1-ce-win65 and the error is fixed.",
    "Dockerfile won't install cron": "In a multistage build, only the last FROM will be used to generate final image.\nE.g., for next example, the a.txt only could be seen in the first stage, can't be seen in the final image.\nDockerfile:\nFROM python:3.9-slim-buster\n\nWORKDIR /tmp\nRUN touch a.txt\nRUN ls /tmp\n\nFROM ubuntu:16.04\nRUN ls /tmp\nExecution:\n# docker build -t abc:1 . --no-cache\nSending build context to Docker daemon  2.048kB\nStep 1/6 : FROM python:3.9-slim-buster\n ---> c2f204720fdd\nStep 2/6 : WORKDIR /tmp\n ---> Running in 1e6ed4ef521d\nRemoving intermediate container 1e6ed4ef521d\n ---> 25282e6f7ed6\nStep 3/6 : RUN touch a.txt\n ---> Running in b639fcecff7e\nRemoving intermediate container b639fcecff7e\n ---> 04985d00ed4c\nStep 4/6 : RUN ls /tmp\n ---> Running in bfc2429d6570\na.txt\ntmp6_uo5lcocacert.pem\nRemoving intermediate container bfc2429d6570\n ---> 3356850a7653\nStep 5/6 : FROM ubuntu:16.04\n ---> 065cf14a189c\nStep 6/6 : RUN ls /tmp\n ---> Running in 19755da110b8\nRemoving intermediate container 19755da110b8\n ---> 890f13e709dd\nSuccessfully built 890f13e709dd\nSuccessfully tagged abc:1\nBack to your example, you copy crontab to the stage of swift:5.3-focal, but the final stage is swift:5.3-focal-slim which won't have any crontab.\nEDIT:\nFor you, the compose for cron also needs to update as next:\ncron:\n    image: prizmserver:latest\n    entrypoint: cron\n    command: [\"-f\"]\ncron don't need to use /bash to start, directly use cron to override the entrypoint could make the trick.",
    ".dockerignore - exclude directory except certain file-types": "You should change the ignore file as follow:\n/src/*\n!/src/*.json\n!/src/**/*.json\nThe reason is that !/src/**/*.json searches for json files inside subfolder of /src, and not in /src itself too.\nLook at the official documentation if you need further info https://docs.docker.com/engine/reference/builder/#dockerignore-file .",
    "Where is the .cargo folder in the rust docker image?": "So, I'm trying to get into embedded rust, for which I had to use the nightly version of rust, and modify my .cargo/config.toml to change the target device, and stuff\nYou can put a file in the folder wherever/your/project/is/.cargo/config.toml, and it will only impact the project(s) in that directory.\nsource: Cargo Book\nI don't know much about docker, but I'm assuming, it's quite similar to pipenv\nDocker is actually quite different to Pipenv. Cargo is similar to Pipenv in that it manages your dependencies for you (Cargo.toml vs Pipfile), distinguishes between regular dependencies vs dev dependencies vs build-time dependencies, etc. Docker is a level of isolation beyond this -- a Docker container is a completely different filesystem from your actual computer. The Dockerfile is a recipe that tells Docker how to build an image of your container, which Docker can run.\nBasically, WORKDIR /usr/source/myapp creates a folder /usr/source/app in the Docker container's file system, and cd's into that for the rest of the Dockerfile. This means that the following line, COPY . ., will copy everything in the same folder as the Dockerfile into the folder in the container /usr/source/app.\nI bet if you open a shell into the Docker container like so:\n# Build the docker container\ndocker build . -t my-cool-project:latest\n\n# Run it\ndocker run -it my-cool-project:latest bash\nyou should be able to cd /usr/source/app and see all your stuff.",
    "How to make Gradle cache persistent through Docker containers?": "Gradle caches artifacts in USER_HOME/.gradle folder. For this either you have to use Bind mounts or Volumes and attach your containers $USER_HOME/.gradle path to that volume.\nFor volumes,\ndocker volume create <your_volume_name>\nAnd when running your container attach the volume like this\ndocker run -v <your_volume_name>:USER_HOME/.gradle yourImage:tag\nor When you use a bind mount, a file or directory on the host machine is mounted into a container.\ndocker run --mount type=bind,source=<your_host_path>,target=/USER_HOME/.gradle yourImage:tag",
    "Docker: COPY failed: stat <file>: file does not exist": "Could you please clarify which line in your Dockerfile causes the error message?\nIs the file you are trying to copy from your working directory yolov3-tiny_obj.cfg?\nIf that is the case, it fails because you specify to copy it from the builder stage. The line should probably look like this:\nCOPY yolov3-tiny_obj.cfg /params",
    "How to configure SAP SNC using Docker": "If you have a Java-based Spring app you can utilize SAP Jco library which can be used to connect to SAP system externally. The configuration steps are described here:\nhttps://help.mulesoft.com/s/article/Enabling-SNC-in-SAP-connector\nIt is given in Mulesoft help but the main points will be the same for any Java-based system.\nThe most important JCo parameters that need to be set up for SNC:\nParameter Name Description\njco.client.snc_mode SNC mode 1: SNC is activated 0: SNC is not activated\njco.client.snc_lib SNC library path Specifies the path and file name of the external library. The default is the system-defined library as defined in the environment variable SNC_LIB. Example: C:SAP\\J2EE_Engine\\SAPCrypto\\libs\\apcrypto.dll\njco.client.snc_qop SNC level Specifies the level of protection to use for the connection. 1: Authentication only 2: Integrity protection 3: Privacy protection (default) 8: Use the value from snc/data protection/use on the SAP application server 9: Use the value from snc/data_protection/max on the SAP application server\njco.client.snc_myname SNC name Specifies the SNC name. This parameter should be set to ensure that the correct SNC name is used for the connection. Example: p:CN=SAPJ2EE, O=MyCompany, C=US\njco.client.snc_partnername SNC partner Specifies the SAP application server's SNC name. It can be found in the SAP profile parameter snc\\identity\\as. Example: p:CN=ABC, O=MyCompany, C=US",
    "Add TensorFlow and Numpy libraries to osgeo/gdal docker image": "I found the solution:\nFROM osgeo/gdal\nRUN apt-get update\nRUN apt install -y python3-pip\nWORKDIR /work\nCOPY requirements.txt ./\nRUN pip install -r requirements.txt\nAfter that, you can install TF via requirements.txt file and you are done!",
    "Add health check for Nginx in docker file": "First WebAssmebly Blazor runs at the client side therefore to hackaround K8s health check I created static reply on specific routes in Nginx by modifying Nginx config file. For a valid health check I recomamand to use Server Blazor type since its run on the server side, there is possibility to add an actual healthy check in startup class.\nRegarding the issuer with Nginx config file the template I was was wrong, updated to this\nserver {\n  #listen                80;\n  #root                 /usr/share/nginx;\n\n  location / {\n      root /usr/share/nginx/html; #sepecify Blazor app route\n  }\n\n  location /health/liveness {\n    #access_log off;\n    error_log   off;\n    return 200 'ok';\n  }\n\n  location /health/readiness {\n    #access_log off;\n    error_log   off;\n    return 200 'ok';\n  }\n}\nAlso an update is needed in Dockerfile:\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build\nWORKDIR /source\n\nFROM build AS publish\nWORKDIR /source/app.CustomerApplication/\nRUN dotnet publish -c release\n\nFROM nginx AS runtime\n\nCOPY --from=publish /source/app.CustomerApplication/bin/release/netstandard2.1/publish/wwwroot/. /usr/share/nginx/html/.\nADD ./app.CustomerApplication/default.conf /etc/nginx/conf.d/default.conf\nK8s health check logs after updating Nginx:\n10.244.0.1 - - [22/Apr/2021:07:15:43 +0000] \"GET /health/liveness HTTP/1.1\" 200 2 \"-\" \"kube-probe/1.17\" \"-\"\n10.244.0.1 - - [22/Apr/2021:07:15:58 +0000] \"GET /health/liveness HTTP/1.1\" 200 2 \"-\" \"kube-probe/1.17\" \"-\"",
    "How to Docker task schedule? An alternate option for Windows task scheduler": "As @David Maze rightly pointed out, you might be interested in CronJobs.\nWe can find in the CronJob documentation:\nCronJobs are useful for creating periodic and recurring tasks, like running backups or sending emails. CronJobs can also schedule individual tasks for a specific time, such as scheduling a Job for when your cluster is likely to be idle.\nYou can use a CronJob to run Jobs on a time-based schedule, it's similar to Cron tasks on a Linux or UNIX system.\nI'll create a simple example from scratch to illustrate how it works.\nYou typically create a container image of your application and push it to a registry before referring to it in a CronJob. I'm not sure if you have a docker image already built, so I'll create one too.\nSuppose I have a job.py Python script and want to \"package\" it as a docker image:\n$ cat job.py\nprint(\"Starting job...\")\nfor i in range(1, 6):\n    print(i)\n\nprint(\"Done\")\nDocker can build images automatically by reading the instructions from a Dockerfile. I have a single Python script, so I will use the python:3 image as the base image:\n$ cat Dockerfile\nFROM python:3\n\nWORKDIR /usr/src/app\n\nCOPY job.py .\n\nCMD [ \"python\", \"./job.py\" ]\nAfter creating a Dockerfile we can use the docker build command to build Docker image and docker push to share this image to the Docker Hub registry or to a self-hosted one.\nNOTE: I'm using Docker Hub in this example.\n### docker build -t <hub-user>/<repo-name>[:<tag>]\n$ docker build -t zyrafywchodzadoszafy/cronjob:latest .\n...\nSuccessfully built cc46cde8fcdd\nSuccessfully tagged zyrafywchodzadoszafy/cronjob:latest\n\n### docker push <hub-user>/<repo-name>:<tag>\n$ docker push zyrafywchodzadoszafy/cronjob:latest\nThe push refers to repository [docker.io/zyrafywchodzadoszafy/cronjob]\nadabca8949d9: Pushed\na1e07bb90a13: Pushed\n...\n\n$ docker image ls\nREPOSITORY                     TAG       IMAGE ID       CREATED          SIZE\nzyrafywchodzadoszafy/cronjob   latest    cc46cde8fcdd   14 minutes ago   885MB\nWe can quickly make sure that everything is working by running this docker image:\n$ docker run -it --rm zyrafywchodzadoszafy/cronjob:latest\nStarting job...\n1\n2\n3\n4\n5\nDone\nNow it's time to create a CronJob:\n$ cat cronjob.yml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-test\nspec:\n  jobTemplate:\n    metadata:\n      name: cronjob-test\n    spec:\n      template:\n        metadata:\n        spec:\n          containers:\n          - image: zyrafywchodzadoszafy/cronjob:latest\n            name: cronjob-test\n          restartPolicy: OnFailure\n  schedule: '*/1 * * * *'\n\n$ kubectl apply -f cronjob.yml\ncronjob.batch/cronjob-test created\n\n$ kubectl get cronjob cronjob-test\nNAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\ncronjob-test   */1 * * * *   False     1        10s             36s\nIn the example above, cronjob-test CronJob runs the job.py Python script every minute.\nFinally, to see if it works as expected, let's take a look at the Pods spawned by the cronjob-test CronJob:\n$ kubectl get pod\nNAME                            READY   STATUS      RESTARTS   AGE\ncronjob-test-1618581120-vmqtc   0/1     Completed   0          2m37s\ncronjob-test-1618581180-nqqsd   0/1     Completed   0          97s\ncronjob-test-1618581240-vhrhm   0/1     Completed   0          37s\n\n$ kubectl logs -f cronjob-test-1618581120-vmqtc\nStarting job...\n1\n2\n3\n4\n5\nDone\nMuch more information on specific configuration options can be found in the CronJob documentation.",
    "Building a binary inside docker and mount back to host": "You can create a dockerfile that'll have the right tools in to build your binary, but you'll still have to use docker run to do the build itself because you can't mount drives during the build process nor can you copy things out of the image during the build. However, you can do this:\nA dockerfile\nfrom envoyproxy/envoy-build-ubuntu:e33c93e6d79804bf95ff80426d10bdcc9096c785\nworkdir /examples\nentrypoint [\"bazel\", \"build\"]\nBuild it like this:\ndocker build -t MyBuildkit .\nAnd run it like this:\ndocker run -it --rm \\\n  -v $(pwd)/examples:/examples \\\n  -v $(pwd)/bin:/bazel-bin/examples/wasm-cc \\\n  MyBuildkit /examples/wasm-cc:envoy_filter_http_wasm_updated_example.wasm\nNow, I don't know enough about the directories here to work out if that's exactly right, but the gist is there.\nThe first volume mount (-v) is there to mount your source code (which I'm assuming is examples) into a folder in the container (which I've also called examples). The final bin directory is also mounted, in the second mount, which I've mounted into a host folder called bin and I've assumed that the copy command you had contained the binary so that would ma to /bazel-bin/examples/wasm-cc in the container.\nAnother assumption I've made is around the command to send to the container. I've set the entrypoint to be what is presumably your compiler (basel build) and to that I've passed in what is presumably the name of the thing to build (/examples/wasm-cc:envoy_filter_http_wasm_updated_example.wasm).\nBecause I don't know basel at all it is entirely possible that I've got one or more of these details wrong, but the general pattern stands. Mount your source and your bin, pass the target of the build into the entrypoint, and build into the bin.",
    "error MSB4236: The SDK 'Microsoft.NET.Sdk.web' specified could not be found - .NET 5.0": "In my case, I got this error for older ASP.NET Core 3.1 projects, after installing .NET SDK 6. The issue was resolved after adding the global.json file, specifying the SDK version 3.1.\nTry adding a global.json file to the root of the project folder with the following code:\n{  \"SDK\": { \"version\": \"5.0.201\" } }\nmore details The SDK 'name' specified could not be found",
    "Passing an argument at the docker run command": "The \"exec form\" of entrypoint doesn't support environment variables. If you want to use environment variables, you should use the \"shell form\" entrypoint.\nFrom your example, it'd look something like this:\nENTRYPOINT exec jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=\"$pwd\" --NotebookApp.password=\"$pwd\"\nDocs: https://docs.docker.com/engine/reference/builder/#shell-form-entrypoint-example\nOther relevant SO answer: https://stackoverflow.com/a/37904830/399007",
    "How to install java 8 using dockerfile in python:3.8-slim-buster base image": "Another approach is that you can build your Dockerfile based on FROM ubuntu:20.04 where Python 3.8 is set as default (here). Then, install java and pip later.\nFROM ubuntu:20.04\nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/\nRUN apt-get update -y \\\n&& apt-get install -y software-properties-common \\\n&& add-apt-repository ppa:deadsnakes/ppa \\\n&& apt-get install openjdk-8-jdk -y \\\n&& apt-get install python3-pip -y \\\n&& export JAVA_HOME \\\n&& apt-get clean \\\n&& rm -rf /var/lib/apt/lists/*",
    "How to connect to a remote mysql hosting from docker container?": "It should be: SPRING_DATASOURCE_URL, MYSQL_USER, MYSQL_ROOT_PASSWORD.",
    "Windows Docker build logs": "You need tool like GitBash. After you can work like with Linux :\ndocker build . &> docker_build.log",
    "Converting a program into a docker container": "I have slightly modified your Dockerfile.\nFROM ubuntu:20.04\n\nEXPOSE 33322\n\nRUN apt-get update && \\\n    apt-get install -y apt-utils wget libunwind8 icu-devtools supervisor \n\n#RUN apt-get install -y systemctl\n#RUN apt-get install -y systemd\n\nRUN mkdir -p -- /var/log/supervisor /opt/print/download /opt/print/edgeinstaller\n\nRUN wget https://app.jetadvice.com/install/edge/jetadvice-edge-linux-x64.tar -P /opt/print/download && \\\n    tar -xvf /opt/print/download/jetadvice-edge-linux-x64.tar -C /opt/print/edgeinstaller && \\\n    chmod +x /opt/print/edgeinstaller/Installer && \\\n    /opt/print/edgeinstaller/Installer install\n    \nRUN chmod +x /usr/bin/JetAdvice/Edge/Edge\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\nCMD [\"/usr/bin/supervisord\"]\nI have added this line RUN chmod +x /usr/bin/JetAdvice/Edge/Edge for adding +x to the executable.\nNow when I started the container gave me an error and it stop the supervisord. Under /usr/share/JetAdvice/Edge/LogFiles you can find the JetAdvice Edge logs whitin it this error\ninstall.json - Install key is missing. (Parameter 'InstallKey')\nat EF.JA.Edge.Service.Windows.Program.<>c.b__2_3(HostBuilderContext hostContext, IServiceCollection services) at Microsoft.Extensions.Hosting.HostBuilder.CreateServiceProvider() at Microsoft.Extensions.Hosting.HostBuilder.Build() at EF.JA.Edge.Service.Windows.Program.Main(String[] args)\nI have found the install.json file under /usr/share/JetAdvice/Edge/Config/Shared, inside there are the InstallKey json field\n{\n  \"InstallConfig\": {\n    \"ApiClient\": {\n      \"SignIn\": {\n        \"InstallKey\": \"\",\n        \"DcaInstanceID\": \"e5860224-12a6-429e-b931-86c50a7a5900\"\n      },\n[..]\n}\nI found on google this web site about JetAdvice where says:\nDownload JetAdvice Edge and generate Install Key\nI don't have an account but I suppose you must generate the key and setup the install.json.",
    "Running nginx container as non-root from nginx-alpine image": "You have the correct intuition.\nPorts in the range 1-1024 need privileged permission to be bound. As you are starting nginx as a non-root user, you cannot bind nginx to listen on port 80.\nOnly way to fix this is to make Nginx listen on a non-privilege port >1024. To do this, you will need to feed a custom nginx.conf file. This should solve your immediate problem.\nBut there will be other permission issues down the line as nginx starts trying to access /var/log to write logs, /var/tmp/ for temp files etc.\nThe best option is to use the non-root nginx docker image itself. https://hub.docker.com/r/nginxinc/nginx-unprivileged",
    "Prometheus with Dockerfile": "The issue you are having seems not related to prometheus, it seems it is at the docker network level.\nInside your prometheus container you are saying this:\n    static_configs:\n      - targets: ['localhost:8080']\nBut remember that localhost is NOT now your physical host (As when you ran it locally outside Docker), it's now inside the container, and inside the same container most likely you don't have your service running...\nWith the information provided I suggest you the following:\nTry first instead localhost use your real IP, depending on the network configuration you are using for your container, it will be enough...\nYou can use instead localhost the ip address of your auth-service, this is the one given by docker, you can run a docker inspect... to get it.\nIf #1 and #2 didn't work and if auth-service is running in another container inside the same physical host, then you could use a bridge network to make the communication between the containers possible, more details here: https://docs.docker.com/network/bridge/ \ud83d\udc46 Once both containers are running in the same network you can use the container name to reference it instead localhost, something like:\n    static_configs:\n      - targets: ['auth-service:8080']",
    "Issue with Python statsmodels requirement in Docker image build": "It's not entirely clear why from your Dockerfile (I'm surprised the tag 3 isn't sufficient to just pick the latest stable of 3.9.1-buster), but pip is trying to resolve dependencies for several versions of Python (implying there are several installed in the base container)\n#12 488.6 ERROR: Command errored out with exit status 1: /usr/local/bin/python /usr/local/lib/python3.9/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-_0xxe4fy/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools wheel 'cython>=0.29.14' 'numpy==1.14.5; python_version=='\"'\"'3.5'\"'\"'' 'numpy==1.14.5; python_version=='\"'\"'3.6'\"'\"'' 'numpy==1.14.5; python_version>='\"'\"'3.7'\"'\"'' 'numpy==1.17.5; python_version>='\"'\"'3.8'\"'\"'' 'scipy>=1.0' Check the logs for full command output.\nSpecifying a minor version may clear this up.\nIf you don't have a particular version need, 3.8-slim-buster (Debian 10 with Python 3.8 .. actually 3.8.9 for now though this may change with bug fixes to that version) may be a practical choice and fix this for you!\nfrom python:3.8-slim-buster\n...\nIf this does not fix the error, then it's likely your 3 tag is just fine and instead extremely-likely that your .env is problematic (presumably it's a Python virtual environment you are trying to preserve).\nRegenerate the venv within the context of your container with a new RUN block rather than COPY-ing what your host system has.\nAfter the comments, I think it's worth trying a different Dockerfile which is much more what I would use with at least the following changes\nbase directly off Debian 10 (you may find another container is more efficient/smaller/whatever, but it's extremely likely this works out of the box)\ninstall Python 3 yourself\ndon't use /home/ as a base\nFROM debian:10-slim\n\n# install Python, pip, and misc build needs\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n        build-essential \\\n        python3-dev \\\n        python3-pip \\\n        python3-setuptools \\\n    && apt-get clean\n\n# bring in program logic\nCOPY .env /app/  # this may need to go into /root/.env (or a custom user)\nCOPY src /app/\nCOPY process.py /app/\nCOPY requirements.txt /app/\n\n# install Python dependencies\nRUN pip3 install --no-cache-dir -r /app/requirements.txt\n\n# set env vars\nENV TZ=America/New_York\n\n# running process logic\nWORKDIR /app\nENTRYPOINT [\"python3\", \"process.py\"]\nIf this works\nusing /home may be problematic (rather than /home/someuser/...)\nthere could be something weird with the upstream python base container (I have never personally used it, though that would also be surprising)\nIf this does not work\nyou may not be working in the directory you think you are (ie. copying the current directory in, another similar-looking directory, etc.)\nyour src may have something bizarre in it\nstatsmodels may be broken (it's versioned 0.12, so that's not totally unreasonable)",
    "Is there a way to preserve the USER on a docker base image?": "well... there's no way to do it the way you'd like it but you can always use a workaround with multistage builds:\nFROM solr:alpine as build\n\nRUN whoami\n\nFROM build as prepare_dependencies\nUSER root\nRUN echo 'I am root' > /the_root_important_message\n\nFROM build\n\nCOPY --from=prepare_dependencies /the_root_important_message /the_root_vital_message\n\nRUN echo \"now I'm $( whoami )\"\nRUN cat /the_root_vital_message\nCMD echo 'this is a pain'\nSomehow I'm fairly sure this is not what you're looking for...\nWell, since this topic is pretty fun, I decided to attempt a different approach:\nFROM solr:alpine as build\n\nRUN whoami\n\nFROM build as prepare_dependencies\nUSER root\nRUN apk --no-cache --update add sudo \\\n    && echo 'ALL ALL=(ALL) NOPASSWD: ALL' > /etc/sudoers\n\nRUN echo \"this is secret\" > /secretfile\nRUN chmod 400 /secretfile\n\nFROM build\n\nCOPY --from=prepare_dependencies /usr/bin/sudo /usr/bin/sudo\nCOPY --from=prepare_dependencies /usr/lib/sudo /usr/lib/sudo\nCOPY --from=prepare_dependencies /etc/sudoers /etc/sudoers\nCOPY --from=prepare_dependencies /secretfile /secretfile\n\nRUN sudo -l\nRUN sudo cat /secretfile\n\nRUN echo \"now I'm $( whoami )\"\n\nRUN echo \"cleanup\" \\\n    && sudo rm -rf \\\n        /etc/sudoers /secretfile \\\n        /usr/lib/sudo /usr/bin/sudo\n\nCMD echo 'this is a pain'",
    "Check docker connection for user and password": "The most obvious solution would be calling\ndocker login -u username -p password server\n(or any pendant from library if your wizard uses some sort of library like Docker.DotNet or similar) from your wizard and check the resultcode. If it's 0 the login is ok, otherwise it's not.",
    "Dockerfile: adding and removing files during build: problem with RUN rm -r": "you need to add -f argument to rm command which means: \"ignore nonexistent files and arguments, never prompt\"\nfor example:\nRUN rm -rf /var/www/html/wp-content/themes/twentytwenty",
    "Play Framework: use h2 database for development and postgresql in production mode and how to connect to the postgresql via conf-file": "The -Dconfig.resource flag needs to be passed to the JVM, but you are passing it to your application instead.\nIf you're using sbt-native-packager (which Play uses by default), you should be able to pass the flag to the JVM by prefixing it with -J. So you need to pass -J-Dconfig.resource=prod.conf.\nHere is the relevant documentation: https://www.scala-sbt.org/sbt-native-packager/archetypes/java_app/customize.html#via-build-sbt\nBy the way, there is also a Docker plugin for sbt-native-packager. https://www.scala-sbt.org/sbt-native-packager/formats/docker.html I recommend you use it instead of writing Dockerfiles manually.",
    "how does docker use apt-get in mac?": "Maybe this post or this post can help you. There you can find the following lines:\nIf containers are possible because of the features available in the Linux kernel, then the obvious question is how do non-Linux systems run containers. Both Docker for Mac and Windows use Linux VMs to run the containers. Docker Toolbox used to run containers in Virtual Box VMs. But, the latest Docker uses Hyper-V in Windows and Hypervisor.framework in Mac.",
    "Docker custom user Permission Denied, while accessing container's file system": "You are running chown when you build the image. But you are mapping that folder as a volume when you run the container, which happens afterwards. During build time the chown runs successfully and the folder becomes owned by my_user, but when you run the container, the -v option overrides the container's /opt folder with the host's /var, and so the ownership and permissions of the host's folder apply.\nThis is because Docker builds images as a set of overlay filesystems, which become read-only when the image is built (the result of Dockerfile). When you run a container from an image, Docker adds an additional layer to that overlay filesystem stack, which is read/write. The layers above (Dockerfile) do not change the layers below (your running container), but the other way around.\nMore info in the Docker overview.",
    "I accidentally deleted my images on minio server. Is it possible to restore minio backup (with and without docker)?": "If you have a zip file of the docker images, send the zip to your server, unzip it and load the image using this command:\ndocker load -i backupimage.img",
    "Copy file(s) within the container in Dockerfile": "as @tkausl and @JohnKugelman pointed out files can be copied by the following command syntax:\nRUN cp <src-path-within-the-container> <dest-path-within-the-container>",
    "Jira with plugins in 1 docker image": "We use docker-compose to include our plugin in Jira for testing purposes. You have to map volume from your local plugin file, to Jira's plugins directory, so when it starts, it will automatically pick up your plugin. It looks more or less like this:\nversion: \"3.5\"\nservices:\n  jira:\n    image: atlassian/jira-software:latest\n    restart: \"no\"\n    volumes:\n      - ./plugins/plugin.jar:/opt/atlassian/jira/atlassian-jira/WEB-INF/atlassian-bundled-plugins/plugin.jar\nYou can also easily build your own image, cloning their repo and modifying Dockerfile to copy your plugin in Jira's folder. e.g.\nCOPY my-plugin.jar\nRUN cp /my-plugin.jar ${JIRA_INSTALL_DIR}/atlassian-jira/WEB-INF/atlassian-bundled-plugins/my-plugin.jar",
    "Azure Devops build a docker image for asp.net with react": "See Description of code 127.\nThe error MSB3073: The command \"npm install\" exited with code 127 indicates that npm install command is not recognized by the system since it's not defined in PATH variable or located in current working directory.\nYour image contains dotnet sdk while it doesn't have npm installed so it's expected behavior to get such error.\nWorkaround:\nModify your Dockerfile like this:\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\nRUN curl -sL https://deb.nodesource.com/setup_12.x |  bash -\nRUN apt-get install -y nodejs\n\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build\nRUN curl -sL https://deb.nodesource.com/setup_12.x |  bash -\nRUN apt-get install -y nodejs\nWORKDIR /src\nCOPY [\"./react.csproj\", \"./\"]\nRUN dotnet restore \"./react.csproj\"\nCOPY . .\nWORKDIR \"/src/\"\nRUN dotnet build \"react.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"react.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"react.dll\"]\nYou should choose the correct node.js version for your project. For example, change the setup_12.x(Line5 and Line9) to setup_10.x if node.js version 10.x is more suitable for you.\nMore details about that you can refer to this document.",
    "Docker, mount all user directories to container": "While building a image through dockerfile, COPY or ADD is used to copy a file with necessary content in the process of building the image example, installing npm binaries and all.\nSince you are looking to have the flexibility of having a same local FS as inside the conatiner, you can try out \"Bind Mounts\".\nbash-3.2$ docker run \\\n>       -it \\\n>       --name devtest \\\n>       --mount type=bind,source=/Users/anku/,target=/app \\\n>       nginx:latest \\\n>       bash\nroot@c072896c7bb2:/# \nroot@c072896c7bb2:/# pwd\n/\nroot@c072896c7bb2:/# cd app\nroot@c072896c7bb2:/app# ls\n Applications   Documents   Library   Music  Projects   PycharmProjects   anaconda3  'iCloud Drive (Archive)'  'pCloud Drive'   testrun.bash\n Desktop        Downloads   Movies    Pictures   Public    'VirtualBox VMs'   gitlab      minikube-linux-amd64      starup.sh\nroot@c072896c7bb2:/app# \nThere are two kinds of mechanism to mange persisting data.\nVolumes are completely managed by Docker.\nBind Mount, mounts a file or directory on the host machine into container. Any changes made from the host machine or from inside the container are synced.\nSuggest to go through Differences between --volume and --mount behavior\nChoose what best work for you.",
    "Run vscode in docker": "Have you tried using VSCode's built in functionality for developing in a container?\nCheckout this page which describes how to do this:\nDeveloping inside a Container\nYou can try out some of the sample container configurations provided by VSCode and use any of those devcontainer.json files as an example to configure a custom development container to your liking. According to the page above:\nWorkspace files are mounted from the local file system or copied or cloned into the container. Extensions are installed and run inside the container, where they have full access to the tools, platform, and file system. This means that you can seamlessly switch your entire development environment just by connecting to a different container.\nThis is a very handy way to have different environments for development that are isolated within the container.",
    "Nestjs exiting on Container Start": "So i solved it using Node version 13\nI do not know whether the issue is in node 14 or if the Nest is not compatible with Node 14 yet.\nFor someone reading in future, here is the issue you can track :\nhttps://github.com/nestjs/nest/issues/5045",
    "How to retrieve or view build/test artifacts/results from a docker multi stage build?": "Yes, but it depends on where you have written the build artifacts to disk inside the container. That location needs to be mounted in when running the docker command from the host machine using the -v or --mount flag\nOnce mounted, writing to the mounted folder inside the container, makes the files available in the corresponding folder on the host machine.\nWord of warning if running as root inside your container, the permissions on your generated files may be too strict for your build service to remove! Use the --user flag to ease this.",
    "cannot connect to localdb from docker container": "LocalDB isn't supported to run on Linux. Despite you can run it on your Win PC, but on Docker you run with Linux OS. At this time, Microsoft didn't support to bring LocalDB outside Windows. So you need to change LocalDB to SqlLite if you want to run it on Docker.\nIt is still one solution to help you run LocalDB with Docker by calling database on Host. This is the article to help you:\nhttps://nickjanetakis.com/blog/docker-tip-35-connect-to-a-database-running-on-your-docker-host",
    "How to create Dockerfile for multi-project .NET Core WebAPI?": "Much depends on your specific situation so it is difficult to provide an exact recommendation.\nIn short, including the Dockerfile at the .sln level, or even at the root of your source repository, is acceptable. As a general practice, most developers that work with docker prefer to keep the Dockerfile in the working directory for the docker build context. I recommend you read the following guidelines on docker build:\nhttps://docs.docker.com/develop/develop-images/dockerfile_best-practices/#understand-build-context\nYour build can use a COPY instruction to reference a file in the context. Therefore, if you want to COPY multiple projects from your solution into your image during docker build, then you should save the Dockerfile in a parent directory that contains all projects which need to be included.\nYou might benefit from this dotnet docker sample repository on github to give you some ideas about the structure of your repo:\nhttps://github.com/dotnet/dotnet-docker/tree/master/samples/aspnetapp\nI also recommend that you read about how to exclude files from your image using a .dockerignore. This will help you minimize your final image size.\nHope this helps!",
    "Why isn't this container running properly?": "You are using WORKDIR /app together with ENTRYPOINT [\"usr/local/bin/python\"] and that ends up with executing:\n/app/usr/local/bin/python\ninstead of:\n/usr/local/bin/python\nAdd absolute path to ENTRYPOINT directive, in your case: /usr/local/bin/python.",
    "Error installing cygwin in docker container: The system cannot find the path specified. (0x3) error=hcsshim::ImportLayer - failed failed in Win32": "It seems that it is hardlinks which Cygwin uses a lot are not handled correctly by Docker. And in particular when Docker tries to commit an image it fails with \"hcsshim::ImportLayer - cannot find the path\" error.\nI run in the same problem recently and after I got rid of the hardlinks in Cygwin installation I was able to commit the image without problems.\nTo get rid of the hardlinks I have zipped and unzipped Cygwin folder.",
    "the Database field must be set on Operation": "This error happens when the mongodb database name is not available for the connection.\nMake sure your config.MONGODB_* variables are available to set the connection string for mongodb.",
    "Docker-Compose for react and flask execute the containers with docker-compose up": "I have seen this issue before while trying to run react app container using 'node' image. I believe the conatiner starts and stops when we try to ru docker-compose.\nPlease try adding \"stdin_open: true\" to the 'cloud-react-front' service in docker-compose.yml. This worked for me.",
    "Run a simple shell script before running CMD command in Dockerfile": "You can use a Docker ENTRYPOINT to support this. Consider the following Dockerfile fragment:\nCOPY entrypoint.sh .\nRUN chmod +x entrypoint.sh replacevariables.sh\nENTRYPOINT [\"./entrypoint.sh\"]\n# Same as above\nCMD [\"/opt/startup.sh\"]\nThe ENTRYPOINT becomes the main container process, and it gets passed the CMD as arguments. So your entrypoint can do the first-time setup, and then run the special shell command exec \"$@\" to replace itself with the command it was given.\n#!/bin/sh\n./replacevariables.sh \"${app_dir}\" dev\nexec \"$@\"\nEven if you're launching some alternate command in your container (docker run --rm -it yourimage bash to get a debugging shell, for example) this will only replace the \"command\" part, so bash becomes the \"$@\" in the script, and you still do the first-time setup before launching the shell.\nThe important caveats are that ENTRYPOINT must be the JSON-array form (CMD can be a bare string that gets wrapped in /bin/sh -c, but this setup breaks ENTRYPOINT) and you only get one ENTRYPOINT. If you already have an ENTRYPOINT (many SO questions seem to like naming an interpreter there) move it into the start of CMD (CMD [\"python3\", \"./script.py\"]).",
    "docker arg misssing in multi-stage builds": "ARG BUILD_DIR=/build\n\n# build\n# \u043e\u0434\u043d\u043e\u0440\u0430\u0437\u043e\u0432\u044b\u0439 \u043a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440\nFROM node:14 as build\n\nARG BUILD_DIR\nThere is global and local scope Global can be translated into local of each container",
    "Mounting a volume overwrites files stored in the Docker image": "This behavior is expected. Docker mounts work in the same way as Linux mounts, i.e. overwriting contents of the target directory with the source directory contents.\nMy suggestion is to use another destination directory for your volume, e.g.\nvolumes:\n - ./frontend:/someotherdir \nAnd then adjust your nginx configuration to look for JS files there.",
    "I have a docker alphine container however all curl calls fails with curl: (6) Could not resolve host": "A restart of the docker container fixed it.",
    "How to run a .NET Core 3.1.3 x86 App in Docker": "This is an imperfect answer, but I did manage to get a x86 .NET Core Web API that depends on a VC++ x86 DLL up and running on Azure Kubernetes Service. Basically I have three stages:\nBuild a custom image based on mcr.microsoft.com/windows/servercore:ltsc2019 that add the C++ Redistributable 2015 x86;\nAnother image is created based on 1. that adds the .NET Core x86 SDK, and;\nA final image is created based on 2. that adds my application code.\nThe three Dockerfiles are included below. Next I'll be looking to consolidate this into a single Dockerfile.\nAdd C++ Redist x86: waptx86custom\nNote I pulled a copy of the redist exe from my own Azure account, but you could also just include it in the same folder as your Dockerfile and ADD it from there.\nFROM mcr.microsoft.com/windows/servercore:ltsc2019 AS base\n\n# Installing Microsoft Visual C++ 2015 x86 Redistributable.\nADD https://lqsts.blob.core.windows.net/temp/vc_redist_2015_3.x86.exe C:/vc_redist.x86.exe\nRUN C:\\vc_redist.x86.exe /install /norestart /quiet /log vc_log.txt\nAdd .NET Core SDK x86: waptx86corecustom\n# escape=`\n\n# Installer image\nFROM acswebwapt.azurecr.io/waptx86custom AS installer\n\n# Apply latest patch\nRUN curl -fSLo patch.msu http://download.windowsupdate.com/c/msdownload/update/software/updt/2020/01/windows10.0-kb4534119-x64_a2dce2c83c58ea57145e9069f403d4a5d4f98713.msu `\n    && mkdir patch `\n    && expand patch.msu patch -F:* `\n    && del /F /Q patch.msu `\n    && DISM /Online /Quiet /Add-Package /PackagePath:C:\\patch\\windows10.0-kb4534119-x64.cab `\n    && rmdir /S /Q patch\n\nENV COMPLUS_NGenProtectedProcess_FeatureEnabled 0\n\nRUN \\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\ngen uninstall \"Microsoft.Tpm.Commands, Version=10.0.0.0, Culture=Neutral, PublicKeyToken=31bf3856ad364e35, processorArchitecture=amd64\" `\n    && \\Windows\\Microsoft.NET\\Framework64\\v4.0.30319\\ngen update `\n    && \\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\ngen update\n\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue'; $verbosePreference='Continue';\"]\n\n# Retrieve .NET Core SDK\nRUN $dotnet_sdk_version = '3.1.201'; `\n    Invoke-WebRequest -OutFile dotnet.zip https://dotnetcli.azureedge.net/dotnet/Sdk/$dotnet_sdk_version/dotnet-sdk-$dotnet_sdk_version-win-x86.zip; `\n    $dotnet_sha512 = '48aa1afaf7a52effb367bbb14a66e2c3bf8da468025795daf0fa0d18e3b9650ba3bd23800c9965a4d4ec1d891afecbce51b2487730f1b0d6040ee7cb73a15ec6'; `\n    if ((Get-FileHash dotnet.zip -Algorithm sha512).Hash -ne $dotnet_sha512) { `\n        Write-Host 'CHECKSUM VERIFICATION FAILED!'; `\n        exit 1; `\n    }; `\n    `\n    Expand-Archive dotnet.zip -DestinationPath dotnet; `\n    Remove-Item -Force dotnet.zip\n\n# Install PowerShell global tool\nRUN $powershell_version = '7.0.0'; `\n    Invoke-WebRequest -OutFile PowerShell.Windows.x64.$powershell_version.nupkg https://pwshtool.blob.core.windows.net/tool/$powershell_version/PowerShell.Windows.x64.$powershell_version.nupkg; `\n    $powershell_sha512 = '1980da63a4f6017235e7af810bfda66be8fa53d0475d147a8219a36c76a903af99adb6cd5309e3dadc610389ae3525bca1ca2d30e7a991640e924334fd4e4638'; `\n    if ((Get-FileHash PowerShell.Windows.x64.$powershell_version.nupkg -Algorithm sha512).Hash -ne $powershell_sha512) { `\n        Write-Host 'CHECKSUM VERIFICATION FAILED!'; `\n        exit 1; `\n    }; `\n    `   \n    \\dotnet\\dotnet tool install --add-source . --tool-path \\powershell --version $powershell_version PowerShell.Windows.x64; `\n    \\dotnet\\dotnet nuget locals all --clear; `\n    Remove-Item -Force PowerShell.Windows.x64.$powershell_version.nupkg; `\n    Remove-Item -Path \\powershell\\.store\\powershell.windows.x64\\$powershell_version\\powershell.windows.x64\\$powershell_version\\powershell.windows.x64.$powershell_version.nupkg -Force\n\n\n# SDK image\nFROM acswebwapt.azurecr.io/waptx86custom\n\nENV `\n    # Enable detection of running in a container\n    DOTNET_RUNNING_IN_CONTAINER=true `\n    ASPNETCORE_URLS=http://+:443;http://+:80 `\n    # Enable correct mode for dotnet watch (only mode supported in a container)\n    DOTNET_USE_POLLING_FILE_WATCHER=true `\n    # Skip extraction of XML docs - generally not useful within an image/container - helps performance\n    NUGET_XMLDOC_MODE=skip `\n    # PowerShell telemetry for docker image usage\n    POWERSHELL_DISTRIBUTION_CHANNEL=PSDocker-DotnetCoreSDK-NanoServer-1909\n\n# In order to set system PATH, ContainerAdministrator must be used\nUSER ContainerAdministrator\nRUN setx /M PATH \"%PATH%;C:\\Program Files (x86)\\dotnet;C:\\Program Files\\powershell\"\nUSER ContainerUser\n\nCOPY --from=installer [\"/dotnet\", \"/Program Files (x86)/dotnet\"]\n\nCOPY --from=installer [\"/powershell\", \"/Program Files/powershell\"]\n\n# Trigger first run experience by running arbitrary cmd\nRUN dotnet help\nAdd application\nFROM acswebwapt.azurecr.io/waptx86corecustom AS base\n\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM acswebwapt.azurecr.io/waptx86corecustom AS build\n\nWORKDIR /src\nCOPY [\"WAPTCoreWebService/WAPTCoreWebService.csproj\", \"WAPTCoreWebService/\"]\nRUN dotnet restore \"WAPTCoreWebService/WAPTCoreWebService.csproj\"\nCOPY . .\nWORKDIR \"/src/WAPTCoreWebService\"\nRUN dotnet build \"WAPTCoreWebService.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"WAPTCoreWebService.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"WAPTCoreWebService.dll\"]",
    "How to implement and run a docker container over SSL": "You will need to configure nginx to use SSL.\nA nice reference can be found here.\nHere is the jist of it:\nserver {\n\n    listen 443;\n    server_name jenkins.domain.com;\n\n    ssl_certificate           /etc/nginx/cert.crt;\n    ssl_certificate_key       /etc/nginx/cert.key;\n\n    ssl on;\n    ssl_session_cache  builtin:1000  shared:SSL:10m;\n    ssl_protocols  TLSv1 TLSv1.1 TLSv1.2;\n    ssl_ciphers HIGH:!aNULL:!eNULL:!EXPORT:!CAMELLIA:!DES:!MD5:!PSK:!RC4;\n    ssl_prefer_server_ciphers on;\n}",
    "Uncaught SyntaxError: Unexpected token '<' From ReactJS when behind Gateway (Reverse Proxy)": "I had the same problem when trying to use a custom base path. I researched a lot until I got an answer, the problem is in the nginx settings because we have to add a redirect configuration for the path in your case \"/testservice/ui/\", like this:\nserver {\n\n  listen 8080;\n  root   /usr/share/nginx/html;\n  index  index.html index.htm;\n\n  location /testservice/ui {\n    alias   /usr/share/nginx/html;\n    index  index.html index.htm;\n    error_page 405 =200 $uri;\n    try_files $uri $uri/ /testservice/ui/index.html;\n\n    location = /testservice/ui/index.html {\n        add_header Cache-Control \"no-cache,no-store\";\n    }\n    location ~* \\.(css|js){\n        add_header Cache-Control \"max-age=31536000\";\n    }\n  }\n\n  error_page   500 502 503 504  /50x.html;\n\n  location = /50x.html {\n    root   /usr/share/nginx/html;\n  }\n\n}",
    "Docker not properly installing python packages using pip install -r requirements.txt": "Both the main problem and the problem mentioned in the comments of itamar-turner-trauring's answer were solved by instead of running docker-compose up running\ndocker-compose up --build\nNot 100% sure why this fixed it but I'd guess the compose file was loaing up the container from an old image which didn't include the new python packages. So forcing it to rebuild made it include the new python packages.",
    "Docker: exposing port on container IP": "The port is not published using the EXPOSE instruction, you need to use -p.\nSee https://docs.docker.com/engine/reference/builder/#expose\nThe EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To actually publish the port when running the container, use the -p flag on docker run to publish and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports.",
    "Building the Dockerfile executes with non-zero code 139": "To answer the updated question: You can build for your Raspberry Pi with the experimental docker buildx command.\nThis will create some virtualized builders that will build your container for the desired architectures. As such, the build process can take much longer (I think a factor of 10 is realistic for my projects).\nThe most important command for your case would be something like\ndocker buildx build --platform linux/arm/v7 .",
    "Docker: Run an automatically determined amount of containers": "What you need is a container orchestrator.\nThe simplest solution would be to write a shell script and spawn container with the JSON file name as an argument. You can also select the JSON file to use through the script and only copy this file to the container.\nEventually, consider running these containers through Kubernetes/docker swarm. For Kubernetes, use https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/. We can define templates here. Each JSON file can be named in sequence, and the parameter can be templated.\nEg: config-1.json config-2.json",
    "How to install Solidity compiler in Docker container - solc": "You should use docker image to compile solidity files ,For example\ndocker run -v YourLocalDirectory:/sources ethereum/solc:stable -o /sources/output --abi --bin\nyou can find artifact in output directory of your local contract directory More information visit this site: https://docs.soliditylang.org/en/v0.5.11/installing-solidity.html",
    "Hot reloading in Docker container stops working after a while": "Adding watchOptions to vue.config.js seems to help.\nvue.config.js\nmodule.exports = {\n  devServer: {\n    watchOptions: {\n      aggregateTimeout: 300,\n      poll: 1000,\n    },\n  },\n};\nI am still testing to see if this resolves the issue.",
    "conditional set of a variable based on another ENV variable [duplicate]": "Let\u2019s take a look at what\u2019s going on when a Dockerfile is used to build a Docker image.\nEach line, is executed in a fresh container. The resulting container state, after the line has been interpreted, is saved in a temporary image and used to start a container for the next command.\nThis temp image do not save any state apart from the files on disk, Docker-specific properties like EXPOSE and a few image settings. Temp images has the advantage of fast subsequent builds using cache.\nNow come to your question, if you would want to do using RUN instead of writing in shell script, here is a work around\nRUN if [ \"$ENABLE_REMOTE_DEBUG\" = \"true\" ] ; then echo \"set debug flag\" ;echo 'export DEBUG_FLAG=\"some_flags\"' >>/tmp/myenv; else echo \"remote debug not set\" ; fi\n\nRUN source /tmp/myenv;echo debug flags: ${DEBUG_FLAG}\nSince the image is based on CentOS, the default shell is bash, thus sourcing your own environment file will work fine. In other shells where source might not work, then reading the file will help.",
    "Failed to deploy Spring and MySQL application using Docker": "As I found out, the problem was that the 2 containers were not on the same docker network.\nWhen I started them with docker-compose using a docker-compose.yml, the 2 containers were started on the same default network, so that the spring container could connect to the mysql container ip address.\ndocker-compose.yml:\nversion: '3'\n\nservices: \n  mysql-container:\n    container_name: mysql-container\n    image: mysql:latest\n    environment:\n      - MYSQL_ROOT_PASSWORD=password\n      - MYSQL_DATABASE=db_name\n      - MYSQL_PASSWORD=root\n  spring_app_container:\n    container_name: spring_app_container\n    image: spring_app\n    depends_on:\n      - mysql-container\n    ports:\n      - 8085:8085\n      - 8080:8080\n    environment:\n      - DATABASE_HOST=mysql-container\n      - DATABASE_USER=root\n      - DATABASE_PASSWORD=password\n      - DATABASE_NAME=db_name\n      - DATABASE_PORT=3306",
    "COPY files - next to Dockerfile - don't work and block docker build": "Try docker build -t test .\ninstead of docker build - < Dockerfile",
    "Add seed data to a Docker based Microsoft SQL Server at image build time": "This command ended up doing it:\nRUN ( /opt/mssql/bin/sqlservr --accept-eula & ) | grep -q \"Service Broker manager has started\" && /createScript.sh\ncreateScript.sh is a bash script that calls SqlCmd on the sql script you want to run.\nThe key is to do a RUN, so it is done at image build time.\nSource: https://github.com/microsoft/mssql-docker/issues/229",
    "getting error : Cannot start service db: error while creating mount source path '/var/www/trialriskincident-backend/db-init':": "you are mounting a volume in which you don't have write access with the container's user.\nNotice that your sudo applies to the docker-compose command, but not to the content of the container.\nThis is a wild guess, but probably you are mounting /var/www as a volume to the container, and the container is not run as root but as another user, and therefore, that user doesn't have write access to /var/www.",
    ".dockerignore isn't working on subdirectory... still ends up in the image": "Using the following .dockerignore:\nsrc/.unwantedsub\nworks well. I've made a complete working example. Run it in an empty directory.\n$ mkdir -p src/.unwantedsub                                                                                                                                                                               \n$ echo src/.unwantedsub > .dockerignore\n$ echo FROM debian > Dockerfile\n$ echo ADD . /data >> Dockerfile\n$ docker build -t test  .\nSending build context to Docker daemon  4.096kB\nStep 1/2 : From debian\n ---> de8b49d4b0b3\nStep 2/2 : Add . /data\n ---> a15b1ddf86e4\nSuccessfully built a15b1ddf86e4\nSuccessfully tagged test:latest\n$ docker run -it test:latest                                                                                        \nroot@b3c8ffc73b7f:/# ls -l data/src/\ntotal 0",
    "Did you mean to run dotnet SDK commands? Please install dotnet SDK from": "I was missing a COPY . . in the dockerfile after dotnet restore. I guess I was missing some .cs files in order for it to be compiled correctly.\nThe working dockerfile ended up looking like this:\nFROM mcr.microsoft.com/dotnet/core/sdk:2.2.105 AS build-env\nWORKDIR /app\n\n# Copy csproj and restore as distinct layers\nCOPY src/Fightplan_v1/Fightplan_v1.csproj ./\n# Copy everything else and build\nCOPY . ./\nRUN dotnet restore\nCOPY . .\nRUN dotnet publish -c Release -o /app\n\n# Build runtime image\nFROM mcr.microsoft.com/dotnet/core/aspnet:2.2\nWORKDIR /app\nCOPY --from=build-env /app .\nENTRYPOINT [\"dotnet\", \"Fightplan_v1.dll\"]",
    "Dockerfile ENV var character replace": "No. The only substitutions it's possible to do in Dockerfile ENV statements are the ones shown in the Dockerfile documentation: $variable, ${variable}, ${variable:-default}, or ${variable:+yes it is set}.\nFor URLs like that you don't really need them in an environment variable. If you do need to compute it and then fetch it you could do it within a single RUN statement\nRUN tableauVersionDots=$(echo \"$tableauVersion\" | sed 's/-/./g') \\\n && curl -LO https://downloads.tableau.com/esdalt/${tableauVersionDots}/tableau-tabcmd-${tableauVersion}_all.deb\nThe variable setting won't survive beyond this RUN statement (and in shell space I haven't even bothered to export it) but that's probably okay just for fetching a URL.",
    "Dockerfile can't copy & Docker-compose volume does not sync with container": "It does work when you add the WORKDIR because you have all files in that directory.\nHowever, in your first Dockerfile, you copy everything to /usr/src/app, but you don't change your workdir to that directory, therefore the requirements.txt is not reachable as it's located in /root/requirements.txt",
    "How can I set the best configurations for the project working with django, docker and mysql?": "Your problem is that you use localhost as the host of mysql in django's config. But docker containers have their own IP, they are not localhost.\nSo first in your docker-compose file, name your containers :\ndb:\n  image: mysql:5.7\n  container_name: db\n  ...\nThen in your django settings, set your db HOST to your db container name : \"db\" :\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.mysql',\n        'HOST': 'db',\n        'PORT' : 3306, # (?)\n...\nAlso you are missing the db 'PORT' in django settings, I think that for Mysql it is 3306 (I've added it above).",
    "RUN command throws \"not found\"": "Okaaaay... Problem was in Windows-style line separator. I change CRLF to LF in my configure.sh and it works!",
    "NodeJS is not detecting change in Docker Bind Mount until Swarm is restarted": "If your volume mapping is correct, the source code changes should reach your node.js app container.\nYou can verify it by inspecting the source code inside the container after you make a change on docker host.\nI'm currently in development mode, and I have to test the source code repeatedly so I want to use bind mounts to make development and testing easier.\nHowever, your source code change won't be effective until node process inside the container reloads and picks up the changes.\nIn order to achieve this you have to use nodemon. Nodemon will pick the changes in the source code and reload node process along with the changes.\nAnother, longer alternative would be building new docker image and then updating your app using: docker service update --image=...\nYou can also use tilt to automate all of the above actions.",
    "docker service create interactive mode": "That because of swarm, it runs container by default in detach mode, so no tty will be allocated to interact with container.\nDid you tried to run with\ndocker service create --name m-docker --tty   m-docker:latest\nThis will allocate pseudo-TTY\n--tty , -t      API 1.25+   Allocate a pseudo-TTY\nservice_create",
    "Docker multistage build vs. keeping artifacts in git": "I'd like to propose changing your first attempt to something like this:\nFROM ubuntu:18.04 as third_party\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        build-essential \\\n        ...\nRUN wget http://.../boost.tar.gz -O /boost.tar.gz && \\\n    tar xvf boost.tar.gz && \\\n        ... && \\\n        make --prefix /boost_out ... && \\\n        find -name \\*.o -delete && \\\n        rm /boost.tar.gz  # this is important!\n\nFrom ubuntu:18.04 as final\nCOPY --from=third_party /boost_out/ /usr/\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        build-essential \\\n        ...\nCMD [\"bash\"]\nThis way, you are paying for the download of boost only once (when building the image without a cache), and you do not pay for the storage/pull-time of the original tar-ed sources. Additionally, you should remove unneeded target files (.o?) from the build in the same step in which they are generated. Otherwise, they are stored and pulled as well.\nIf you are at liberty posting the whole Dockerfile, I'll gladly take a deeper look at it and give you some hints.",
    "Docker container - MYSQL_RANDOM_ROOT_PASSWORD not showing output in STDOUT": "I suggest to add this to your \"run-lamp.sh\":\nPASS=`head /dev/urandom | tr -dc A-Za-z0-9 | head -c 13 ; echo ''`\nmysqladmin -u root password $PASS\necho \"Root Password is $PASS\"\nin this way you setup and print the password to docker logs.\nPS: the ENV you use will only works on mysql official image.",
    "Docker, how to copy all .ear folders?": "Maybe you can use .dockerignore to add only Test1.ear, Test2.ear, etc to build context, then use COPY . config/apps to meet your requirements:\n.dockerignore:\n*\n!*.ear\nDockerfile:\nFROM alpine\n\nCOPY . config/apps\nTest:\ncake@cake:~/1$ ls\nabc  Dockerfile  Test1.ear  Test2.ear\n\ncake@cake:~/1$ docker build -t abc:1 .\nSending build context to Docker daemon   5.12kB\nStep 1/2 : FROM alpine\n ---> b7b28af77ffe\nStep 2/2 : COPY . config/apps\n ---> 4726a5828435\nSuccessfully built 4726a5828435\nSuccessfully tagged abc:1\n\ncake@cake:~/1$ docker run --rm -it abc:1 ls /config/apps\nTest1.ear  Test2.ear",
    "P4Python installation via pipenv fails inside docker file": "It looks like something is blocking your FTP access to ftp.perforce.com. If you are behind a firewall, check that you can access port 21 (the FTP port) on ftp.perforce.com, or ask your IT team to open that up for you.",
    "Error while making an external connection to install git on docker image on Windows": "I was facing this issue at the company I work at and I was eventually able to solve it by doing the following.\nDownload the proxy.pac file (can just google this part, not too difficult)\nOpen the with choice of editor (sublime, vs, jetbrains, notepad)\nOpen config.json file in C:\\Users\\<user>\\.docker\\config.json\nadd the following to the file but replace http and https proxy with one of the proxys in the proxy.pac file. noProxy I left as is, it did not affect the outcome.\n{\n\"proxies\": {\n   \"default\": {\n     \"httpProxy\": \"http://my_proxy:8080\",\n     \"httpsProxy\": \"http://my_proxy:8080\",\n     \"noProxy\": \"*.test.example.com,.example2.com\"\n   }\n }\n}\nrestart docker\nand try again. This is the only thing that worked for me. Changing ctnml.ini file and changing proxy via Docker GUI or changing my DNS server did not work.",
    "Using docker entrypoint and files outside docker": "The reason for the error is that the file does not exist inside the container while it exists in your filesystem. There are at least two possible solutions.\nYou can either ADD the files into the container at build stage (build your own Dockerfile) or map a local directory to a directory inside the container:\ndocker run -v ./templates:/templates image:tag --template-body /templates/template.json\nThis way when you run the container it will have the same file located at /templates/template.json as the contents of your local templates folder. Read more about bind mounts.",
    "How do I connect to DB2 from a docker container using ASP.Net Core?": "I was able to get it to work by adding the following to the Dockerfile above the ENTRYPOINT:\nENV LD_LIBRARY_PATH=\"/app/clidriver/lib/\" Env PATH=$PATH:\"/app/clidriver/bin:/app/clidriver/lib\" RUN apt-get update; \\ apt-get install -y libxml2-dev;\nPlease note that I switched to a Linux container: sdk:2.2.300-stretch",
    "Docker - Import local module on executable Python script": "Navigating through the running container I found out that folders were not being properly created and I was only copying its content instead of the folder itself, so the path filter.filter did not exist.\nA possible solution is editing the service Dockerfile and copying everything in one single command instead of file by file, like this:\nCOPY . /\nor if you don't want to copy all the files and just certain folders, you can solve this problem by specificating the folder on the destination path:\nCOPY filter/ filter/",
    "How can I get the output of the echo command?": "Each line in the dockerfile creates a new layer in the resulting filesystem. Unless a modification is made to your dockerfile that hasn't previously been encountered, Docker optimizes the build by reusing existing layers that have previously been built. You can see these intermediate images with the command docker images --all.\nThis means that Docker only needs to build from the 1st changed line in the dockerfile onwards, saving much time with repeated builds on a well-crafted dockerfile. You've already built this layer in a previous build, so it's being skipped and taken from cache.\ndocker build --no-cache .\nshould prevent the build process from using cached layers.\nChange the final path parameter above to suit your environment.",
    "error when installing from pipfile.lock in docker": "In my experience, errors like this are often the result of a transitive, native dependency that's missing. From the error message, it looks like cffi won't install. That library depends on an OS package libffi, so you should just need add libffi-dev to the packages installed by apk.\nI tested this manually, by starting a docker container and installing the failed libs manually. Via docker run --rm -it python:3.7-alpine /bin/sh, then pip install ... or apk add ...",
    "File not found exception while running DockerFile": "The error message is quite clear. When the container tries to run it is not able to find properties file.\nYou need to add config.properties file to your docker image.\nADD path_to_config_file/config.properties /data/config/config.properties\nNOTE: path_to_config_file refers to the file path in your local where you are building the dockerfile",
    "Find the underlying Docker file you are importing from": "In a general sense, the answer is \"no\". The Dockerfile you want may not exist at all. (There are more ways to make an image than using Dockerfile, after all.)\nYou can get quite a lot of information about an image using docker inspect, including \"Created\": \"2018-01-04T04:08:09.885971603Z\" for php:5.6.32-apache. Using that information I was able to dig back in the repository history to find old commits containing 5.6. Considering further, git log --grep 5.6.32 might do the trick, too, and is easier to try:\nThere are only two commits with 5.6.32 in the commit message. This is a good place to begin your search. More generally for 5.6, you can see that 5.6.40 was the last one, and it was removed some time ago.",
    "Problem using Kaniko to build containers from a Kubernetes CloudBees Jenkins shared-library": "I had a similar problem in kaniko container, what i had to do was add PATH to the environment:\n    withEnv(['PATH+EXTRA=/busybox:/kaniko']) {\n      sh '''#!/busybox/sh\n      /kaniko/executor (....)\n    }",
    "Trying to understand VOLUME command in dockerfile and its directories": "You're not supposed to be able to directly access Docker named volumes (or anything else that lives in /var/lib/docker). The official Docker examples on, for example, backup and restore for named volumes launch additional containers mounting the same volumes for basic data-management operation.\nIf you need to directly access the data in a volume, using a host-directory bind-mount is probably the easiest option.\nYou should avoid writing VOLUME directives in your Dockerfile. You don't need it to use docker run -v to mount a directory into a container; its key effect is to automatically add a -v option for you if you didn't already have one. In practice, you'll get no benefit and suffer from some confusing side effects (notably, RUN steps will make no persistent changes inside a VOLUME directory).\nThe anonymous volume you're seeing in docker volume ls is probably the result of your VOLUME directive.",
    "Dockerfile: sudo apt-get install snmp asks if iptable rules should be saved, after the answer the process does not continue": "So i fixed it in my case..\nSomehow the installation process of the mentioned packages in the Dockerfile is crazy.\nI modified my Dockerfile so that I first install apt-utils in one layer, then all packages except snmp in another layer and snmp seperate in one layer.\nlet me know if it worked for u as well",
    "COPY failed: stat /var/lib/docker/tmp/docker-builder076499369/files/nginx.conf: no such file or directory": "Check that you have the following files structure:\n|-- project\n|   |-- Dockerfile\n|   |-- files\n|   |   |-- nginx.conf\nAlso make sure that next to the Dockerfile you do not have a file \".dockerignore\" and if you have one make sure it does not contain an entry for \"files\" or \"nginx.conf\".\nThen it should work.",
    "E: Package 'oracle-java8-installer' has no installation candidate in Docker Ubuntu": "there is an image in Docker Hub that Ubuntu+java8. so can use this repository (enter link description here)",
    "How to use ray in a docker swarm": "I found out how to fix it:\nThe hostname of the ray-head container is NOT 'ray-head', but 'tasks.ray-head'.\nTo make it work i needed to change the hostnames inside the docker-compose files like this:\nFor ray-head:\ncommand: ['start', '--head', '--redis-port', '6379', '--redis-shard-ports','6380,6381', '--object-manager-port','12345', '--node-manager-port','12346', '--node-ip-address', 'tasks.ray-head', '--block']\nFor ray-worker:\ncommand: ['start', '--redis-address', 'tasks.ray-head:6379', '--object-manager-port', '12345', '--node-manager-port', '12346', '--block']\nNow i can run this on any host:\nray.init('tasks.ray-head:6379')\nI hope this helps someone else in the same situation",
    "Docker volume mount windows container": "This seems to be followed with docker/for-win issue 676 which includes:\nI was also having this exact issue:\ndocker: Error response from daemon: container XYZ encountered an error during Start: failure in a Windows system call: The compute system exited unexpectedly. (0xc0370106).\nI found 2 solutions for my case:\nI was able to successfully build and run the image by reducing the number of layers in the history. (For me this number happened to be a max of 37 layers in history.) (If your dockerfile is based on a 2nd dockerfile, you may need to reduce the number of steps in the 2nd dockerfile.)\nHow to debug: I was able to debug this by cutting the number of steps in half until the image ran, then re-adding steps until I discovered how many steps the history could have before breaking the image.\nI was able to successfully build and run the image without reducing the number of layers by making sure that the root image was a certain version of windowsservercore:1709 (specifically, the 10.0.16299.904_en-us version of 1709, which does not appear to be pull-able anymore; however, it might also work with the latest version of windowsservercore:1709, I haven't tried).\nI didn't debug this, I discovered this by blind luck.\nNote: the same issue reports that mounting can be problematic.",
    "Cannot start a Jupyter notebook from inside this docker container": "Apparently, just modifying the startup.sh script to\n/bin/bash -c \"jupyter notebook --ip 0.0.0.0 --allow-root --no-browser --NotebookApp.token='sometoken'\"\nfixed everything.",
    "Dockerfile with Go plugins": "So first of all i see some problems with your Dockerfile, i can only address those since i don't fully understand your 'plugins' issues.\nIt's a bad idea to copy /etc/passwd like that, you could get away with this approach but you would have to copy all relevant files for users and groups, cp /etc/{passwd,shadow,group,gshadow} for example, and then be sure to fix permissions. You are safer just creating the user only in the final image, you're not even using it in the builder, and the context size is negligible if that is what you're aiming for by running the adduser in the builder.\nIf your go program calls C code then you will need to base your final image on something other than scratch because it does not have the required libraries. You need a base image with some variant of libc, the most light weight is musl and you get that with the alpine images. If your go program does not call any C code then you can use the CGO_ENABLED=0 flag during go build and then your built binary program won't depend on anything, thus you can use the scratch or busybox base images. You can find more info on cgo here.",
    "Docker log network traffic during build": "All network traffic = traffic on all interfaces (loopback included) and all protocols (not only http/https, where you can use logging via proxy). Start tcpdump in the background at the beggining of each RUN command for all interfaces (eth0, lo). Example, which will print all packets to stdout:\nFROM alpine\n\nRUN apk add tcpdump\n\n# start tcpdumps in the background for each RUN\nRUN sh -c 'tcpdump -nnXSs 0 -i eth0 &' \\\n    && sh -c 'tcpdump -nnXSs 0 -i lo &' \\\n    && ping -c 5 google.com\n\nRUN sh -c 'tcpdump -nnXSs 0 -i eth0 &' \\\n    && sh -c 'tcpdump -nnXSs 0 -i lo &' \\\n    && apk add curl\nBuild output:\n...\nStep 3/4 : RUN sh -c 'tcpdump -nnXSs 0 -i eth0 &'     && sh -c 'tcpdump -nnXSs 0 -i lo &'     && ping -c 5 google.com\n ---> Running in 63249712af4a\nPING google.com (216.58.204.78): 56 data bytes\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on lo, link-type EN10MB (Ethernet), capture size 262144 bytes\nlistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes\n64 bytes from 216.58.204.78: seq=0 ttl=127 time=17.529 ms\n13:01:09.987047 IP 8.8.4.4.53 > 172.17.0.2.43264: 41194 1/0/0 A 216.58.204.46 (44)\n        0x0000:  4500 0048 7096 0000 7f11 12f0 0808 0404  E..Hp...........\n        0x0010:  ac11 0002 0035 a900 0034 0472 a0ea 8180  .....5...4.r....\n        0x0020:  0001 0001 0000 0000 0667 6f6f 676c 6503  .........google.\n        0x0030:  636f 6d00 0001 0001 c00c 0001 0001 0000  com.............\n        0x0040:  001e 0004 d83a cc2e \n...\nOf course, you can send these logs to Elasticsearch/Splunk/..., but you will need to install more toolings. And probably it will be good idea to exclude this traffic from tcpdump.",
    "docker-compose ERROR: Cannot locate specified Dockerfile": "Assuming you have already pushed the alicoskun/project:latest image into a repository where your production droplet can find it, you have no need to include the docker-compose.yml file as part of your docker-compose command. Instead, just run:\ndocker-compose -f docker-compose.prod.yml up -d --build\nIncluding the docker-compose.yml in your docker-compose command-line will require that the Dockerfile be present, even though it will not be used to build the system.",
    "how to pack jar along with config file as docker image": "You could start with this simple Dockerfile:\nFROM java:8\nWORKDIR /\nADD app /app\nCMD [\"java\", \"-jar\", \"/app/appserver.jar\", \"server\", \"/app/config.yml\"]\nThe Dockerfile should be created in a directory which contains also the app directory with your appserver.jar and config.yml files:\n\u251c\u2500\u2500 workdir/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 app/\n|       \u251c\u2500\u2500 application.jar\n|       \u2514\u2500\u2500 config.yml\nNow, to build the docker image, run, in the workdir, the following command:\ndocker build -t app-name:1.0 .",
    "Docker container for Deep Learning with Python, Chollet": "This is a bug in pip 19.0 See https://github.com/pypa/pip/issues/6158\nDowngrade to pip==18.1 (for now) or wait for a fix to be released.\nEdit: Pip 19.0.1 was just released to fix this",
    "Docker issues on Mac": "I had the same problem after install docker on my mac (brew cask install docker).\ndocker --version works, but docker ps or any other docker command results in the error: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\nTo solve the problem you have to :\nInstall Virtual Box\nrun : docker-machine create default to create a virtual machine (mandatory on mac os)\nrun: docker-machine env default to set the environment\nrun: eval $(docker-machine env default)\ntry docker ps or docker version to check that everything is ready.",
    "How to convert a Spring-Boot web service into a Docker image?": "When you run a Docker container, all the ports, which any application happen to listen on within it, are not published by default.\nIn order to publish a port, you need to specify it while running the container using your image. For all the details on how to do it, you can check the \"EXPOSE\" section in the documentation of docker run command: https://docs.docker.com/engine/reference/run/\nShortly speaking, you want to add another option while running your container:\ndocker run --name mycontainer1 -p 8080:8080 myimage1\nI'm not sure if you wanted to achieve this by adding\nEXPOSE 8080\nin your Dockerfile. Actually, this does not mean that the port will be exposed when the image is used to run a container. As you might find in Dockerfile reference:\nThe EXPOSE instruction does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To actually publish the port when running the container, use the -p flag on docker run to publish and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports.",
    "Multiple Dockerfiles in project with different contexts": "Edit: Previous answer was adding folders to the image that were outside the build context, which won't work. Additionally OP clarified the contents and how the image will be used in the comments, showing a very good use case for multi stage builds.\nI'll take a stab at it, based on the info provided.\nFirstly, you can't exclude folders from a given docker context. If you use docker build -t bluescores/myimage /some/path, your context is /some/path/**/*. There's no excluding the huge folders, or the little folders, or anything in them.\nSecond, in order to use ADD or COPY to bring files into your docker image, they must exist in the build context.\nThat said, it sounds like you'll end up using various combinations of the huge and little folders. I think you'll be better off sticking with your existing strategy you've outlined, with some optimizations - namely using multi stage builds.\nSkip docker-compose for now\nThe solution here that you reference isn't really aiming to solve the problem of closely controlling context. It's a good answer to a totally different question than what you're asking. You are just trying to build the images right now, and while docker-compose can handle that, it doesn't bring anything to the table you don't have with docker build. When you need to orchestrate these containers you're building, then docker-compose will be incredible.\nIf you aren't sure you need docker-compose, try doing this without it. You can always bring it back into the mix later.\nNow my image is gigantic\nSee if multi-stage builds are something you can make use of.\nThis would essentially let you cherry pick the build output from the image you COPY the huge folders into, and put that output in a new, smaller, cleaner image.",
    "Can't create docker file and Project file using Docker": "I've got this up and running in WSL As stated in the readme one should make sure everything is set up correctly.\nStart up WSL\nrun wget -q https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb\nThis will download a file needed to install azure functions correctly.\nrun sudo dpkg -i packages-microsoft-prod.deb to install the previously downloaded package.\nrun sudo apt-get update to update your local aptitude cache.\nrun sudo apt-get install azure-functions-core-tools to install the azure functions core tools.\nMake sure dotnet is installed: run sudo apt-get install dotnet-sdk-2.2\nRun func init . --docker and select dotnet as framework (option 1)\nThis will result into a list of files in your current directory:\n.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 host.json\n\u251c\u2500\u2500 local.settings.json\n\u2514\u2500\u2500 test.csproj\n\n0 directories, 4 files\nYou can now create an Azure function by running func new. Select a template (HttpTrigger, option 2), provide a name SampleFunction and you have your function ready.\nYour directory now looks like this:\n.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 SampleFunction.cs\n\u251c\u2500\u2500 host.json\n\u251c\u2500\u2500 local.settings.json\n\u2514\u2500\u2500 test.csproj\n\n0 directories, 5 files\nThis is all done with func version 2.3.199",
    "Get env variables from script and pass to Dockerfile [duplicate]": "The problem is passing the environment variable, not how you set it (dotenv or not) or how you use it (to set a variable or COPY).\nHere's a complete demo with the technique from the duplicate. This is script.sh:\n#!/bin/bash\nmycustomvar=\"file.$RANDOM\"\ntouch \"$mycustomvar\"\necho \"I am running outside Docker with mycustomvar=$mycustomvar\"\ndocker build --build-arg mycustomvar=\"$mycustomvar\" .\nAnd a Dockerfile:\nFROM alpine\nARG mycustomvar\nRUN echo \"I am running in docker with mycustomvar=$mycustomvar\"\nCOPY $mycustomvar /tmp/\nRUN ls /tmp\nHere's what happens when you run it:\n$ sudo ./script.sh\nI am running outside Docker with mycustomvar=file.10518\nSending build context to Docker daemon  17.41kB\nStep 1/5 : FROM alpine\n ---> 3fd9065eaf02\nStep 2/5 : ARG mycustomvar\n ---> Using cache\n ---> a6dfa6001164\nStep 3/5 : RUN echo \"I am running in docker with mycustomvar=$mycustomvar\"\n ---> Running in e958044bfd11\nI am running in docker with mycustomvar=file.10518\n ---> 95c107e49291\nRemoving intermediate container e958044bfd11\nStep 4/5 : COPY $mycustomvar /tmp/\n ---> d36445b49261\nRemoving intermediate container e3ac014d1ba9\nStep 5/5 : RUN ls /tmp\n ---> Running in 590409a81df5\nfile.10518\n ---> d734f83cc8ec\nRemoving intermediate container 590409a81df5\nSuccessfully built d734f83cc8ec\nAs you can see, script.sh sets a variable, and the RUN and COPY statements both have access to it.",
    "Pass argument to python script running in a docker container": "I would strongly recommend using tcp/ip for such purposes. By the way, in this case you benefit from:\nYou can detect whether your python service is online\nYou can move python container to another machine\nImplementation is really simple. You can choose any framework, but for me suitable is Twisted, and implement your python script as follows:\nfrom twisted.internet.protocol import Factory, Protocol\nfrom twisted.protocols.basic import LineReceiver\n\nclass DataProcessor(LineReceiver):\n  def lineReceived(self, line):\n    # line contains your data\n    pass\n\nFactory factory = Factory()\nfactory.protocol = DataProcessor\nreactor.listenTCP(8080, factory)",
    "connect tomcat and mysql in docker container with jdbc driver": "You might try this (assumes web.xml is the Web Application Context file):\nFROM tomcat:latest\nRUN mkdir -p /usr/local/tomcat/webapps/test/WEB-INF/lib\nCOPY ./mysql-connector-java-5.1.44-bin.jar /usr/local/tomcat/webapps/test/WEB-INF/lib\nCOPY ./db.jsp /usr/local/tomcat/webapps/test \nCOPY ./web.xml /usr/local/tomcat/webapps/test/WEB-INF\nCMD [\"catalina.sh\", \"run\"]",
    "Is it possible to set a umask/chmod value for docker volume?": "This isn't possible. Not only do the container and host have different umasks, but each process has its own umask. For that matter, nothing in umask(1) or umask(2) suggests that a process can't subsequently change its own umask to something more permissive: it simply isn't a good security or policy-enforcement control.",
    "Running docker image with a separate local gemspec": "I've been dealing with a very similar issue, and think this is working for me:\nGemfile\nsource 'https://rubygems.org'\ngit_source(:github) { |repo| \"https://github.com/#{repo}.git\" }\n\ngem_path = ENV.fetch('LOCAL_GEM_PATH', '.')\n\ngem 'my_personal_gem', path: File.join(gem_path, 'my_personal_gem')\nThen you can set LOCAL_GEM_PATH to where the directory containing the Gems are.\nHope this helps.",
    "Docker image creation (weblogic and java)": "Still, we should reference docs of Docker. There is a best practice about Dockerfile.\nConsequently, the best use for ADD is local tar file auto-extraction into the image, as in ADD rootfs.tar.xz /.\nThat means you could just use the only instruction below:\nADD jdk-8u181-linux-x64.tar.gz  /home/docuser/myimages/\nTo answer your question, I put the whole Dockerfile below.\nFROM oraclelinux:7-slim\nADD jdk-8u181-linux-x64.tar.gz /home/docuser/myimages\nCOPY wls1036_generic.jar /home/docuser/myimages\nWORKDIR /home/docuser/myimages\nRUN chmod u+x /home/docuser/myimages/jdk1.8.0_181/bin/*\nRUN /home/docuser/myimages/jdk1.8.0_181/bin/java -jar wls1036_generic.jar",
    "Run a Docker Image as a Container (for windows users)": "You have to map the docker container port to the host port. The reason is docker container is an isolated environment and it's public ip is same as that of the host machine. You have to ensure that host knows when to redirect requests to the container. So when you map the host port to docker container port, all the requests coming to HOST_IP_ADDRESS:HOST_PORT are redirected to the docker container whose port is mapped to the HOST_PORT.\nYou do that by using -p flag while using the docker run command as shown below:\ndocker run -it -p 8080:8080 IMAGE_NAME\nNow all the requests coming to localhost:8080 will be directed to application running in container.",
    "spotify dockerfile-maven Dockerfile": "Answer\nBy default, yes the build context will include your source (and your target) directory. But, you can add a .dockerignore file that tells it not to do this.\nIf you're not sure what this is, take a look at this tutorial.\nOpinion\nIMO: I can imagine scenarios where running docker from maven would be the right thing to do. However, my imagination is very good and there can't be that many shops that are adopting docker but not changing their legacy maven build pipelines.\nIn the majority of cases though, you don't want to do this.\nRather, you should use one docker container to build the java artifact and run the unit-tests. This can then push the artifact to nexus (or whatever repo you're using). If this is a webapp or other http service, then you can use a second container to host it and deploy this to an environment to integration test it.",
    "ENV/ARG command not populating variables in Dockerfile": "According to the ENV documentation:\nEnvironment variables are supported by the following list of instructions in the Dockerfile:\nADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR as well as:\nONBUILD (when combined with one of the supported instructions above)\nTherefore it appears variables defined with ENV are not supported by the RUN directive.\nHowever, you can instead replace the ENV directive with the ARG directive and NODE_VERSION will be availablein subsequent RUN directives.\nExample:\nFROM microsoft/nanoserver\n\nARG NODE_VERSION=8.11.4\n\nADD https://nodejs.org/dist/v${NODE_VERSION}/node-v${NODE_VERSION}-win-x64.zip C:\\\\build\\\\node-v${NODE_VERSION}-win-x64.zip\n\n\nRUN powershell -Command Expand-Archive C:\\build\\node-v${NODE_VERSION}-win-x64.zip C:\\; Rename-Item C:\\node-v${NODE_VERSION}-win-x64 node\nRUN SETX PATH C:\\node\n\nENTRYPOINT C:\\node\\node.exe\nAdditionally you can override the value of NODE_VERSION in your docker build command.\n$ docker build -t base-image:latest --build-arg NODE_VERSION=10.0.0 .\nUsing the ARG directive will not make NODE_VERSION available in the environment of a running container. Depending on your use case you may also need to use an additional ENV definition.",
    "How to create Docker multistage build with Linux dependencies and environment vars from parent image?": "You could forward environment variables etc from the parent image by writing to a file and the copy it into the next image. Then in your entrypoint somehow read it and export variables etc. But i would say it's a bit exotic design.\nBut in your case there seems to be quite a bit of dependencies on variables and packages so maybe it just easier to not user multi-stage at all?",
    "Docker COPY issue while building the docker image": "Which directory are you in when you run this command? Could you do ls /web/target/ from that directory? I ask because I think your Dockerfile is expecting to find a web.war in ./web/target relative to the directory you are running in.\nEdit (to save anyone digging through the comments on this): The target directory did contain the file but it was invisible to docker due to a .dockerignore file with **/target.",
    "Is there a way to delay Docker ONBUILD instructions to a future child image?": "No, you cannot delay the processing of ONBUILD. It will run on the next child image.\nNote that I tend to recommend against ONBUILD for all but very specialized use cases because having steps performed during your build that are not listed in your Dockerfile tends to confuse users. Your use case may be more appropriate to have the users run the commands directly in their Dockerfile instead of forcing it on them from the parent image.",
    "Escape dollar sign in a Windows dockerfile": "Inside of a Dockerfile, you escape a $ with a backslash (see the documentation for more details). For a path separator, you may be able to switch to the forward slash. However, Docker will only expand build arguments and environment variables defined inside the Dockerfile, not values from the external environment or PowerShell. (To understand why, realize that a build runs on the Docker engine, not in the client command line, and the engine can be a remote server.)\nOne last note, if you want to use backslashes while building Windows containers, it is possible to change the escape character. This is done by setting a parser directive at the top of the Dockerfile:\n# escape=`",
    "connect failed between containers with zookeeper and kafka": "First of all, 123.345.567 isn't four numbers, so that's not a valid fake IP\nDocker compose sets up a DNS network for you using the compose service names. You will never need the external address of your Mac inside a container.\nFor a fully working Compose file, look at Confluent's. Start with one Zookeeper and Broker, and get that working (Kafka won't run faster on a single machine with multiple containers because they all share one disk)\nI also suggest not adding a random chroot to only one of your Zookeepers (/kafka_test)",
    "\"ssh: Could not resolve hostname github.com: Try again\" - Inside docker container": "If you are using a lish shell session, try, as suggested here:\nssh -Tvvv git@github.com\nAnd do a traceroute to see where the resolution fails.",
    "Can I do a save in docker after some manual installation?": "You can save the container in a new image using docker container commit\ndocker container commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\nDescription from the docs:\nCreate a new image from a container\u2019s changes",
    "Could not find ast-2.3.0 in any of the sources": "Try:\n$ docker-compose build\nImages get stale and don't rebuild automatically.",
    "Docker Compose and locally cached images": "Does anyone know how to get docker compose to find an image in the local repository?\nYou first need to build the image locally from the dockerfile and tag it. Then you can reference that image in the docker-compose.yml\nThis answer explains this process perfectly.",
    "How to split pip install into steps in Dockerfile?": "Every instruction you add in the Dockerfile on build adds a new layer to the build. For each instruction run on build the image layer caches the contents of that layer on top of the other ones. Layers can be cached between builds if the results are expected to be the same every time.\nIn your case, if the step where it's building the requirements file fails, that step will not be considered complete, meaning the next time you run your build it'll start that step over (which will run the installation for everything in requirements.txt).\nOne thing you can do to shorten rebuilds is to introduce a multi-stage build where the first stage installs requirements and the second stage runs your application; this way the only time the first stage of requirements is run is when you change your requirements.txt.\n# FROM x as y\n# will name this stage of the build as \"dependencies\"\nFROM python:3.6-alpine as dependencies\n\nRUN apk add --no-cache --virtual .fetch-deps \\\n    zlib-dev \\\n    jpeg-dev \\\n    geoip-dev\n\nENV PYTHONUNBUFFERED 1\n\nRUN mkdir /src\nWORKDIR /src\nADD requirements.txt /src/\n\nRUN pip install -r requirements.txt\n\n# This is the next stage of the build building off your dependencies\nFROM dependencies as application\n\nRUN my_application.py",
    "Extract variable from text file during Docker build": "Possible Answer\nI don't think the Dockerfile syntax will support what you want out of the box currently. I would suggest you create a simple bash script to do the downloading/renaming for you and then just ADD the static file pointers to your Dockerfile.\nExample\nPull your version file eg. ADD http://example.com/latest-version.txt\nAdd/run some simple bash script/oneliner to process your version file (see below a suggestion to get started)\nThis script reads the latest-version.txt file and downloads the relevant files, eg.\nwget http://example.com/package1.${packageVersion1}.tar.gz -O /tmp/package1.tar.gz\nwget http://example.com/package2.${packageVersion2}.tar.gz -O /tmp/package2.tar.gz\netc\nSince all relevant packages have been saved to static file names you can hardcode it in your Dockerfile as eg.\nADD /tmp/package1.tar.gz\nADD /tmp/package2.tar.gz\netc\nClosing thoughts\nI would suggest you employ some kind of Configuration Management (CM) to make your life easier as this method of solving issues can get messy quickly. CM has the upside of providing not only more structure to solve problems like this, but also two other important attributes namely idempotence and convergence when building an artifact.\nAppendix\nwhile IFS='' read -r line || [[ -n \"$line\" ]]; do\n  NAME=`echo $line | cut -d '=' -f1`; VERSION=`echo $line | cut -d '=' -f2`; echo $NAME; echo $VERSION;\ndone < latest-version.txt",
    "Docker multi-stage builds and Codeship running container": "I was able to solve this problem using multi-stage builds split into two different files following this guide: https://documentation.codeship.com/pro/common-issues/caching-multi-stage-dockerfile/\nBasically, you'll take your existing Dockerfile and split it into two files like so, with the second referencing the first:\n# Dockerfile.build-stage\nFROM golang:1.10 as build-stage\n\nENV TEMP /go/src/github.com/my-id/my-go-project\nWORKDIR $TEMP\nCOPY . .\nRUN make build\n# Dockerfile\nFROM build-stage as build-stage\n\nFROM alpine:3.4\n\n# ...\nENV HOME /home/$USER\nENV TEMP /go/src/github.com/my-id/my-go-project\n\nCOPY --from=build $TEMP/bin/my-daemon $HOME/bin/\nRUN chown -R $USER:$GROUP $HOME\n\nUSER $USER\nENTRYPOINT [\"my-daemon\"]\nThen, in your codeship-service.yml file:\n# codeship-services.yml\ncachemanager-build:\n  build:\n    dockerfile: Dockerfile.build-stage\ncachemanager-app:\n  build:\n    image: my-daemon\n    dockerfile: Dockerfile\nAnd in your codeship-steps.yml file:\n# codeship-steps.yml\n- name: cachemanager build\n  tag: master\n  service: cachemanager-build\n  command: <here you can run tests or linting>\n- name: publish to registry\n  tag: master\n  service: cachemanager-app\n  ...\nI don't think you want to actually run the Dockerfile because it will start your app. We use the second stage to push a smaller build to an image registry.",
    "Auto input to make multiple chooses of a script in Dockerfile": "It would be easier for your script to ask for those values (to be read from stdin) only if a known environment variable was not set.\nThat way, your Dockerfile can set ENV app=xxx before RUN install-proxy.sh\nIf it detects that app is set, it would skip the question part, and go directly to the printing \"you chose '$app'\".\nThat way, you bypass entirely the tricky part of sending stdin content to a script through pipe in a Dockerfile RUN step.",
    "Dockerfile can COPY files into a declared volume, but RUN cannot create files?": "I don't have the exact explanation of this but when you build an image with a Dockerfile it will make the lightest image possible. When you use RUN you don't make data persistant but you only do an action that will give a result that will not stay in the image.\nNote that apt-get and yum commands make installations persist. It's kinda weird.",
    "Running docker build on a dockerfile located within a container": "There is a Docker image called \"Docker in Docker\" which lets you call Docker inside the container, maybe that could point some hints in your setup.\nhttps://hub.docker.com/_/docker/",
    "Docker image Size increases if I remove few lines of code": "I tried building in both ways and found not much difference.\nMost of the disk space is consumed by /opt/ModSecurity\nInitially it was 74MB after git clone.\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\noldimage            latest              924a8d4f941e        11 minutes ago      867MB\nnewimage            latest              d1ca029927c2        About an hour ago   867MB\nnginx               alpine              ebe2c7c61055        6 days ago          18MB\nHowever after building the complete build - it has grown to ~650MB.\n$ du -sh *\n639.7M  ModSecurity\n408.0K  ModSecurity-nginx\n7.5M    nginx-1.13.12\n996.0K  nginx-1.13.12.tar.gz    ",
    "Docker - no access to other containers in the network during docker image build": "I see your confusion.\nThe RUN statement isn't doing what you think it is; it's running the wait-for-it.sh while the container is building, and isn't under the docker-compose's control. It will not run when your container runs! You should check out the documentation from docker about container start-up order and docker-compose.\nDetached mode for your database will have no effect on build/run process; expect you won't be able to interact with it.\nUsing docker-compose will, by default, have all the container in non-interactive mode; but that's okay because you can still attach/detach to the containers.\nYou should add a depends-on option to your docker-compose.yml and add your wait-for-it.sh to the command option in the docker-compose.yml, not the Dockerfile.\nversion: '3'\n\nnetworks:\n  backend:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.1.0/24\n\nservices:\n  main-db:\n    container_name: main-db\n    image: mysql\n    environment:\n      MYSQL_DATABASE: Main\n      MYSQL_ROOT_PASSWORD: root\n    ports:\n      - \"5000:3306\"\n    networks:\n      backend:\n        ipv4_address: 172.20.1.2\n\n  atlanta-ms:\n    container_name: atlanta\n    build:\n      context: ./Atlanta\n      dockerfile: Dockerfile\n\n    # Add this `depends-on`\n    depends-on: \n      - \"main-db\"\n\n    # Add this `command` option\n    command: [\"Scripts/wait-for-it.sh\", \"-t\", \"30\", \"172.20.1.2:3306\"]\n\n    image: atlanta:ms\n    ports:\n      - \"5001:80\"\n    networks:\n      backend:\n        ipv4_address: 172.20.1.3\nI would recommend moving wait-for-it.sh to your WORKDIR, and make sure it is passed as \"./wait-for-it.sh\" to command, just to make your life easier.\nDon't forget to remove RUN Scripts/wait-for-it.sh -t 30 172.20.1.2:3306 from your Dockerfile! (Because docker-compose is handling it now.)\nAnd remember that the command for using docker-compose is docker-compose up, unless, that is, you'd like to use docker swarm instead.",
    "Kubernetes with Docker unable to change from default port of 80": "use \"targetPort\" to indicate what port your pod is listening on. Your yaml spec should be something like:\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myApp\nspec:\n  selector:\n    app: myApp\n  ports:\n  - name: http\n    port: 8080\n    targetPort: 8080\n    nodePort: 30001",
    "yum install not working from Dockerfile": "Try adding the repo manually first instead of using yum-config-manager :\ncat <<EOF | tee /etc/yum.repos.d/proxysql.repo\n[proxysql_repo]\nname= ProxySQL YUM repository\nbaseurl=http://repo.proxysql.com/ProxySQL/proxysql-1.4.x/centos/\\$releasever\ngpgcheck=1\ngpgkey=http://repo.proxysql.com/ProxySQL/repo_pub_key\nEOF\nThis works properly on a FROM centos:7 image.",
    "JAX-RS Rest service giving \"HTTP Status 404 \u2013 Not Found\" error tomcat while accessing in docker": "I have just overwrite context.xml file in docker using volume path and issue resolved,\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <Context antiResourceLocking=\"true\" privileged=\"true\" >\n <!--<Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n         allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" />-->\n</Context>\nNow I can access the rest service.\nEdit:\nyou can overwrite context.xml in the docker-compose file as bellow,\nvolumes:\n      - <path to docker whaere it is located>/context.xml:/usr/local/tomcat/webapps/manager/META-INF/context.xml\nyou can put context.xml file in your project directory.",
    "installing chrome in docker file": "I found a mirror in my network. Replacing source with my mirror, job was done!",
    "Docker command RUN not working with git init --bare?": "It must be something with the environment setup in the Dockerfile or the base image you use. The following minimal example works:\nFROM alpine:latest\nRUN apk update && apk add git\n\nRUN mkdir -p /root/repository\nWORKDIR /root/repository\nRUN git init --bare\nYou can for example verify that $GIT_DIR is not set in the base image or as build argument to docker build. If it is set the git init --bare will init the repo in $GIT_DIR instead of /root/repository",
    "How can I Dockeries a python script which contains spark dependencies?": "You have Debian os, not ubuntu os. These ppas are for ubuntu os. According to this, article oracle java8 is not available in Debian due to licensing issues.\nYou have following options-\n1. Use an Ubuntu docker image which comes with preinstalled oracle java8 like this one\n2. Follow this tutorial on how to install Oracle java8 on Debian Jessie\n3. Install open_jdk sudo apt-get install openjdk-8-jre",
    "How to set retention policies on influxdb docker container using entrypoint script in dockerfile": "If you are on the host machine use commands below, otherwise replace 'localhost' with your influxdb address and port.\nDatabase creation:\ncurl -i -XPOST http://localhost:8086/query --data-urlencode \"q=CREATE DATABASE mydb\"\nRetention policy:\ncurl -i -XPOST http://localhost:8086/query --data-urlencode \"q=CREATE RETENTION POLICY \"one_week_only\" ON \"mydb\" DURATION 1w REPLICATION 1 DEFAULT\"",
    "Docker build_image cannot reach my local maven repo": "Problem solved as per: https://success.docker.com/article/How_do_I_configure_the_default_bridge_(docker0)_network_for_Docker_Engine_to_a_different_subnet\nThe docker bridge was conflicting with my network. We need to make an entry in daemon.json\n{\n    \"bip\": \"172.26.0.1/16\"\n}\nYou can use any value that is suitable for you.",
    "Running Spring boot application inside docker container": "Apart from publishing the port from docker as mentioned by @Van0SS you need to create a port forwarding rule as well. Open virtualbox and Navigate to VM -> Settings -> Network -> Advanced -> Port forwarding Create a new rule:\nName : <Anything - Purpose of port>\nProtocol: TCP\nHostIP: 127.0.0.1\nHost port: 7000\nGuest Port: 7000",
    "Deploy Docker to offline PC": "The only way to transfer an image offline is to save it into tarball using docker save\nAs for the size of the image, the solution is to use a smaller aspnet. The one you are using is 7GB large. Thus you need to choose a smaller aspnet image that would be sufficient from the available ones\nAnother solution is to transfer the source code and build the image on the target machine. In this case, you save the microsoft/aspnet:latest to a tarball and transfer it once to the target machine. Whenever you have new updates in the source, you copy the source and the Dockerfile to the target machine and you build the image there.",
    "Use Kong to expose Konga": "You should create a network and connect it to kong. Use created network in your konga configuration.\ndocker network create my-net\n\ndocker network connect my-net kong\n\ndocker run --rm --network my-net -p 8080:8080 pantsel/konga start --kong-url http://kong:8001",
    "How to add known_hosts for passwordless ssh within docker container in docker-compse.yml?": "I don't know any solution using docker-compose.yml. The solution I propose implies create a Dockerfile and execute (creating a shellscript as CMD):\nssh-keyscan -t rsa whateverdomain >> ~/.ssh/known_hosts\nMaybe you can scan /ect/hosts or pass a variable as ENV.",
    "How can I fix \u201cstandard_init_linux.go:187: exec user process caused \"exec format error\"\u201d in Docker?": "No further insights, but I fixed my issues by doing a factory reset of Docker.",
    "Docker compose is not starting all containers (possibly Flask related?)": "You have a RUN command that can not finish in your app dockerfile. This is preventing your dockerfile from ever finishing building. It's why it says step 8/8 and never says \"successfully built\" like your web container. I'd move that last run command into an entrypoint or CMD, like you have it in your web docker file, so that way the docker build command can actually finish.\nIE CMD [\"python\", \"src/main.py\"] or ENTRYPOINT [\"python\", \"src/main.py\"]\nThat is most likely what is preventing your docker images from being built. Is because of that one last line in APP where you are starting a webserver in the foreground which never closes. Best bet is to place the command into entrypoint so it does not actually run during the build process, but will run when the image is started.\nthis is the line I'm talking about:\nStep 8/8 : RUN python src/main.py\n ---> Running in e86c8adf46f0\n * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)\nVS this one out of the web build.\nStep 10/10 : ENTRYPOINT npm start\n ---> Using cache\n ---> 9c5ec6770c47\nSuccessfully built 9c5ec6770c47\nSuccessfully tagged testcomposemachine_web:latest\nRecreating testcomposemachine_web_1\nStarting testcomposemachine_db_1\nAttaching to testcomposemachine_db_1, testcomposemachine_web_1\nthe build will never finish if the command never completes. So yes, in a way the Flask API is preventing your docker-compose command from building all the other files.",
    "cqlsh is failing when executed from docker container": "The RUN is executed when you build the container, not when the container executes.\nYou need to write custom script that will start Cassandra (same as in original container - look into Cassandra's Dockerfile), then wait until it starts, and only after that - execute commands via cqlsh.",
    "Is it advisable to use apt-get clean twice in Dockerfile?": "It's useless to run these 2 times. The last one is enough.\nAlso, the rm -rf /var/lib/apt/lists/* command should be moved to the end.",
    "can't install libpulse-dev in Docker": "you can use pulseaudio-dev to replace libpulse-dev\nRUN apk --update add",
    "gpgp key request fails in dockerfile but not bash": "Your systemd unit file's environment variables are not passed into a container when building Docker images. Instead, make use of the --build-arg parameter for passing the proxy values:\ndocker build --build-arg=HTTP_PROXY=http://<corporate proxy>/ --build-arg=HTTPS_PROXY=http://<corporate proxy>/ -t hgk/test-container .\nYou should not bake runtime options like proxy variables into the Dockerfile, always pass configuration through environment variables (or this build argument parameter).\nAdditional hint: most applications rely on http_proxy and https_proxy -- while most environment variables are indeed all-caps, the proxy variables have a more wide-spread lower-case usage.",
    "Why is docker build prefixing my copy path with a temp directory?": "I had the same issue. Turned out I made a silly mistake. I added the following to my .dockerignore file, just out of habit when setting up a new project:\nbin\nobj\n.vs\n.git\nThen I tried running this in my Dockerfile\nCOPY ./bin/publish/ .\nDocker gave the strange tmp path error, because it was falling back to that path since I told it to ignore my /bin folder. Once I copied to a different publish path (not bin), the problem went away.",
    "docker: removing file/image dependencies on other docker images": "You could build it using\ndocker build --no-cache my_new_image:v1.0.0 .",
    "Dockerfile which won't run": "As you already know, this usually happens when your file doesn't have the execute bit set, though your chmod 750 should, in theory, handle this. One way to debug if that's actually the issue would simply be to pass in a different CMD at docker run time (e.g. docker run -it <image> sh) and check the script's permissions with ls -l. More often than not, not having execute bit set is the culprit.\nIf it's not that, I'd recommend clarifying the question with the full Dockerfile just in case something like a VOLUME directive may be discarding the effects of the RUN you're using.",
    "Is it possible to create a docker image whitout OS?": "Yes, look at the scratch keyword (docs):\nYou can use Docker\u2019s reserved, minimal image, scratch, as a starting point for building containers.\nAlso you may find useful using multi-stage builds.\nAn example:\nFROM scratch\nADD hello /\n\nFROM fedora\nRUN yum -y update && yum clean all\nRUN yum -y install nginx",
    "Same Docker image forwards X11 one host but not on another": "Enabling verbose info in ssh, I noticed the following messages:\ndebug2: x11_get_proto: /usr/bin/xauth list :0 2>/dev/null\ndebug1: Requesting X11 forwarding with authentication spoofing.\n...\nX11 forwarding request failed on channel 0\nI then searched the web for \"X11 forwarding request failed on channel 0\" and found the solution: in /etc/ssh/sshd_config, add:\nX11UseLocalhost no\nAnd then ssh -X works correctly everywhere.\nSo, this command must be added to the Dockerfile for my containers:\nRUN echo \"X11UseLocalhost no\" >> /etc/ssh/sshd_config",
    "Arch Linux - dotnet core 2 with docker": "Edit: As I answered here:\nOne reason why you can see this error message is when the argument passed to dotnet is unknown.\nThe runtime doesn't recognize the argument you are passing to it and wonder if you perhaps should be using the sdk instead.\nMake sure the name of the dll and the path to it are correct in:\nENTRYPOINT [\"dotnet\", \"api.bauernsuche.de.dll\"]\nAlso, make sure the COPY instruction works as expected so the dll is at the WORKDIR location.\nOriginal answer:\nYou are using an image that doesn't have the SDK.\nHere's an example shows building the app within an image, then creating a new one with the result:\nFROM microsoft/dotnet:2.0-sdk AS build-env\nWORKDIR /app\n\n# copy csproj and restore as distinct layers\nCOPY *.csproj ./\nRUN dotnet restore\n\n# copy everything else and build\nCOPY . ./\nRUN dotnet publish -c Release -o out -r debian.8-x64\n\n# build runtime image\nFROM microsoft/dotnet:2.0-runtime\n\nWORKDIR /app\nCOPY --from=build-env /app/out ./\nEXPOSE 5000/tcp\nENTRYPOINT [\"dotnet\", \"api.dll\"]\n# To be able to pass parameters (e.g.: override configuration)\nCMD []\nTaken from here.\nNote the first one is a sdk image and the second is a runtime. Just as you can choose when downloading from the website.",
    "Can't build ASP.Net container": "Try with this dockerfile instead. It will use the aspnet image:\nFROM microsoft/aspnet\nSHELL [\"powershell\", \"-Command\", \"$ErrorActionPreference = 'Stop';\"]\n\nEXPOSE 80\n\nRUN Remove-WebSite -Name 'Default Web Site'  \nRUN New-Website -Name 'YourSiteName' -Port 80 -PhysicalPath 'C:\\YourFolderName' -ApplicationPool '.NET v4.5'\n\nADD \"LocalFolderPath\" /YourFolderName",
    "Docker swarm stop spin up containers at 250": "I just figure it out. The problem is not with the service or the swarm, it's with the network.\nWhen I use driver: overlay the default subnet is 10.0.0.0/24 which result in 254 address. So I change the mask in the subnet, to 22, which result in 1022 address, I added:\nipam:\n  config:\n    -subnet: 10.0.0.0/22\nAnd now the network section in the docker-compose file looks like this:\nnetworks:\n  web:\n    driver: overlay\n    ipam:\n      config:\n        - subnet: 10.0.0.0/22",
    "docker-compose down deletes volume declared on dockerfile?": "A volume declared in a Dockerfile is an \"anonymous volume\", so it would be removed.\nSee this example: https://gist.github.com/dnephin/0aa8e8962ebcdcebff1cec7315a224dd",
    "provide two docker images from the same dockerfile": "You can do that using Docker Multi Stage builds. Have two Docker files\nDockerfile\nFROM alpine\nRUN apk update && apk add gcc\nRUN echo \"This is a test\" > /tmp/builtfile\nDockerfile-prod\nFROM myapp:testing as source\n\nFROM alpine\nCOPY --from=source /tmp/builtfile /tmp/builtfile\nRUN cat /tmp/builtfile\nbuild.sh\ndocker build -t myapp:testing .\ndocker build -t myapp:production -f Dockerfile-prod .\nSo to explain, what we do is build the image with dependencies first. Then in our second Dockerfile-prod, we just include a FROM of our previously build image. And copy the built file to the production image.\nTruncated output from my build\nvagrant@vagrant:~/so$ ./build.sh\nStep 1/3 : FROM alpine\nStep 2/3 : RUN apk update && apk add gcc\nStep 3/3 : RUN echo \"This is a test\" > /tmp/builtfile\nSuccessfully tagged myapp:testing\n\nStep 1/4 : FROM myapp:testing as source\nStep 2/4 : FROM alpine\nStep 3/4 : COPY --from=source /tmp/builtfile /tmp/builtfile\nStep 4/4 : RUN cat /tmp/builtfile\nThis is a test\nSuccessfully tagged myapp:production\nFor more information refer to https://docs.docker.com/engine/userguide/eng-image/multistage-build/#name-your-build-stages",
    "missing m2 repository after a docker build": "@khmarbaise Yes, you're right. I just had to specify a VOLUME /root/.m2/repository entry in my Dockerfile. I also removed cached containers and volumes, just to be sure. And sire enough, the repository is there, as I expect.\nSo my Dockerfile now looks something like this. I didn't have a full grokking of docker volumes. But that all makes sense now. Thanks.\nFROM maven:3.5.0-jdk-8-alpine\n\nRUN apk update \\\n    && apk add ca-certificates \\\n    && update-ca-certificates \\\n    && apk add openssl \\\n               bash \\\n               git\n\nWORKDIR /app\nCOPY . /app\nVOLUME /root/.m2/repository\n\nRUN mvn compile ",
    "Server not binding to port within docker swarm": "Turns out that rocket was listening on localhost and not 0.0.0.0. Setting the environment variable ROCKET_ADDRESS=0.0.0.0 fixed things.\nIn the Docker tutorial example python server, changing 0.0.0.0 to localhost also broke things in exactly the same way.\nWhat is the difference between 0.0.0.0, 127.0.0.1 and localhost?",
    "Creating Marklogic docker instance in Ubuntu 16.04 exposing the ports": "Try these three steps:\nConfirm the Docker container is still running with\ndocker ps\nIf there is no container running, you probably need a command in the Dockerfile to keep the container running indefinitely. Try adding the following at the end of your Dockerfile:\nCMD tail -f /dev/null\nwhich will keep the container running indefinitely.\nIn your Dockerfile, confirm the EXPOSE setting contains the three ports you wish to access:\nEXPOSE 8000 8001 8002\nConfirm that the MarkLogic service is started in a command in the Dockerfile. For example in a default install of MarkLogic 8 on CentOS 7, I start the MarkLogic service in the Dockerfile with\nCMD /etc/init.d/MarkLogic start",
    "Docker compose new image and instance": "As is: you can't because you have only one 3306 port on you host machine. But if you let docker compose manage the links you can use docker-compose scale SERVICE=N where SERVICE is your service name, and N the number of copy. For example whit this comnfiguration :\nversion: '2.1'\n\nservices:\n  client:\n    image: explorer-client\n    ports:\n      - 80:80\n      - 443:443\n    depends_on:\n      - server\n    networks:\n      - client-server\n      - client-watcher\n  server:\n    image: explorer-server\n    depends_on:\n      - redis\n    networks:\n      - client-server\n      - server\n\nnetworks:\n  client-server:\n    driver: bridge\n  server:\n    driver: bridge\nyou can scale the server service by doing docker-compose scale server=2 because it's not explicitely linked to the client by a port.",
    "Dockerfile: mkdir and COPY commands run fine but I can't see the directory and file": "Existing volumes can be mounted with\ndocker container run -v MY-VOLUME:/var/jenkins_home ...\nFurthermore, the documentation of COPY states:\nAll new files and directories are created with a UID and GID of 0.\nSo COPY does not reflect your USER directive. This seems to be the second part of your problem.",
    "Docker Java 8 Container with Heavy CPU Usage": "As discussed in our chat, there was a thread in the Java application that was spinning in a tight loop because there was no user input. Stopping that thread caused the CPU to go back to where it belonged.",
    "Docker/Rails - Permission denied @ dir_s_mkdir Errno::EACCESS": "@Filipe can you please confirm if your user has write access to the directory \".\" on windows? Running as Administrator doesn't guarantee full read/write access to the mounted volumes due to the samba integration.",
    "Docker swarm mode create service with --mount": "To run Nginx docker container as a service in swarm mode one should ensure that all the services, used into the nginx as a upstream server, are running otherwise service discovery would get fail and your nginx service would also get failed.",
    "How can I expose the dynamic $PORT from Heroku in a Dockerfile?": "I don't think you can (isn't the port dynamic?).\nI think you are supposed to use the PORT environment variable in your app (ie, your app has to bind to that port inside the container).\nSupposing your app would start the python http server, your CMD should be something like this:\nCMD [\"sh\", \"-c\", \"/usr/bin/python -m SimpleHTTPServer ${PORT}\"]",
    "Launch service at container startup": "I've already searched for ideas for my problem but I didn't found anything that I could use.\nIn almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.\nhttps://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/#/run-only-one-process-per-container",
    "How to edit source code of a python project deployed in docker": "I got image from docker hub and using kitematic.Yeah I am looking to modify its contents\nI need to edit the core\nThen you need to define your own image, starting with\n FROM my_Image_From_DockerHub\nAnd you can COPY your modifications from your disk to that new image, overwriting the python sources need.\nFrom there, docker build then docker run your new image.",
    "Maven's Lifecycle in Dockerfile": "You haven't posted the exact proxy issue you have, but whatever it is, you probably have to put your custom settings.xml file (ADD settings.xml /app/) and then use it with Maven (RUN mvn -s settings.xml package).\nIf it will be more than one invocation, I recommend to put settings.xml to ~/.m2/ so it will be used by default on every Maven call.",
    "Building docker image from Dockerfile: cannot start container": "You don't need to call apt-get update many times. It is also a good practice to update and install all packages in one row because it create only one layer in docker image\nFROM debian:latest\n\nMAINTAINER me@example.com\n\nRUN apt-get update \\\n    && upgrade -y \\\n    && apt-get install -y sudo dialog curl wget vim net-tools apt-utils net-tools cron vim debsecan links apache2 salt-master\nRUN cd /var/www && mv html htmlapache\nRUN mkdir -p /var/www/html/repo\nRUN touch /var/www/html/index.html\nCOPY entrypoint.sh /\nCMD [ \"/entrypoint.sh\" ]\nEXPOSE 8000",
    "Can't see output of Docker container": "Use it in the following manner :\nFROM ubuntu:14.04\nCMD [\"Hello docker!\"]\nENTRYPOINT [\"echo\"] ",
    "Building an web app which can perform npm tasks": "After you call docker-compose up, you can get an interactive shell for your app container with:\ndocker-compose run app\nYou can also run one-off commands with:\ndocker-compose run app [command]\nThe reason your app container is not running after docker-compose up completes is that your Dockerfile does not define a service. For app to run as a service, you would need to keep a thread running in the foreground of the container by adding something like:\nCMD ./run-my-service\nto the end of your Dockerfile.",
    "Cannot access port on host mapped to docker container port": "I found that jboss server running inside the container was not listening on 0.0.0.0. One option to do this is, while starting the standalone server use -b 0.0.0.0.\n/bin/standalone.sh -b 0.0.0.0",
    "COPY of existing file in Dockerfile fails mysteriously": "First try your Dockerfile building in another machine or VM. Your error looks like a corruption in aufs layers. If it succeed on another machine, you need to clean docker. This answer talks about resetting docker: click here\nRegards",
    "Docker - Hostname was NOT found in DNS cache": "The local dns address is the issue you're facing. The container can't connect to your localhost ip from inside the container. The solution is to pass an ip address of the DNS server in your docker run or update your /etc/resolv.conf to point to a non-loopback ip address.\nFrom Docker's DNS documentation:\nFiltering is necessary because all localhost addresses on the host are unreachable from the container\u2019s network. After this filtering, if there are no more nameserver entries left in the container\u2019s /etc/resolv.conf file, the daemon adds public Google DNS nameservers (8.8.8.8 and 8.8.4.4) to the container\u2019s DNS configuration. If IPv6 is enabled on the daemon, the public IPv6 Google DNS nameservers will also be added (2001:4860:4860::8888 and 2001:4860:4860::8844).\nNote: If you need access to a host\u2019s localhost resolver, you must modify your DNS service on the host to listen on a non-localhost address that is reachable from within the container.",
    "Docker: modify files during image creation": "Alternative a) I would say you are going the wrong path here. You do not want to do this during image creation, but rather during the entrypoint.\nIt is very common and best practise in docker to configure the service during the first container start e.g. seed the database, generate passwords and seeds and, as in you case, generate configuration based on templates.\nUsually those configuration files are either controlled by ENV variables you pass on to docker run or rather in your docker-compose.yml, in more complex environments the source of the configuration variables can be consul or etcd.\nFor your example, e.g. you could introduce a ENV variable 'USE_SSL' and then either use sed in your entrypoint to replace something in the server.xml when it is set, but since you need much more, like setting the revers_proxy domain and things, you should go with tiller : https://github.com/markround/tiller\nCreate a server.xml.erb file, place the variables you want to be dynamic, use if conditions if you want to exclude a section if USE_SSL is not set, and let tiller use ENVIRONMENT as a datasources.\nAlternative b) If you really want to stay with the \"on image build\" concept ( not recommended ) you should use the so called build_args https://docs.docker.com/engine/reference/commandline/build/\nAdd this to your docker file\nARG USE_SSL\n\nRUN /some_script_you_created_to_generate_server_xml.sh $USE_SSL\nYou still need to have a bash/whatever script some_script_you_created_to_generate_server_xml.sh which takes the args, and creates by conditions, whatever you want. Tiller though will be much more convenient when stuff gets bigger (compared to running some seds/awks)\nand then, when building the image, you could use `docker build . --build-arg USE_SSL=no -t yourtag",
    "Dockerfile vim installation does not work": "Apparently I had to add tty: true to docker-compose.yml because I was executing with the -t option.\nhttps://stackoverflow.com/a/32110513/1882337 has a nice explanation about the -t option.",
    "How to save config file inside a running container?": "Question\nHow to save a change committed by/into a container?\nAnswer\nThe command docker commit creates a new image from a container's changes (from the man page).\nBest Practice\nYou actually should not do this to save a configuration file. A Docker image is supposed to be immutable. This increases sharing, and image customization through mounted volume. What you should do is create the configuration file on the host and share it at through parameters with docker run. This is done by using the option -v|--volume. Check the man page, you'll then be able to share files (or directories) between host and containers allowing to persists the data through different runs.",
    "Copy log files from Docker container to host after CMD is run": "This may be a solution: Create a named docker volume:\ndocker volume create --name centos-volume\nSo the path to his volume on your host is /var/lib/docker/volumes/centos-volume/_data\nThan I created something which is hopefully comparable with your app. My dockerfile (copies a script which will create a log in my container. I start the script when I start the container with docker run (like you did)):\nFROM centos:7\nRUN mkdir /script\nCOPY create-log.sh /script/\nRUN chmod +x /script/create-log.sh\nContent of the script: (it creates a logfile in /var/log/anaconda)\n#!/bin/bash\ntouch /var/log/anaconda/my-log.log\necho \"hello world\" >> /var/log/anaconda/my-log.log\nI start my container with:\ndocker run --rm -ti -v centos-volume:/var/log/anaconda/ my-centos ./script/create-log.sh\nSo this commands execute the script in my container and mounts the content of my /var/log/anaconda folder of my container to my volume on my host.\nThan I check:\ncat /var/lib/docker/volumes/centos-volume/_data/my-log.log\nhello world\nAfter rerunning the container the second log appears:\ndocker run --rm -ti -v centos-volume:/var/log/anaconda/ my-centos ./script/create-log.sh\n\n$ sudo su\n[root@localhost centos]# cat /var/lib/docker/volumes/centos-volume/_data/my-log.log\nhello world\nhello world",
    "docker-compose build throwing error ERROR: Untar re-exec error: signal: killed: output:": "Apparently there are some things wrong in your Dockerfile, try running it as follows:\nFROM node:latest\n\nMAINTAINER Erkan Demir <erkan.demir@peopleplan.com.au>\n\n#Add everything in the current directory to our image\nADD . /var/www\n\nRUN cd /var/www/ && \\\n   npm install && \\\n   npm install -g quorra-cli\n\nEXPOSE 3000\n\nCMD['quorra', 'ride']",
    "Using Docker Compose to connect to mysql running in another container": "Ok, found the answer to it. In case anyone else comes across the same problem, just do\ndocker network ls\nThis command lists all the docker networks.\n| NETWORK ID | NAME | DRIVER | SCOPE |\n| ------------- | --------------- | -------- | -------- |\n| c24bee6b5b8e | network-mysql | bridge | local |\nThus, a compose file demonstrating this might look like the following:\nanother-image:\n  image: another-image\n  command: /opt/start.sh\n  ports:\n    - \"8080:8080\"\n  environment:\n    DB_URL: container-mysql\n    DB_USER: root\n    DB_PASSWORD: password\n  external_links:\n    - container-mysql\n  network_mode: network-mysql",
    "Referencing the first argument passed to the docker entrypoint?": "Change ENTRYPOINT to next:\nENTRYPOINT [\"bash\", \"run.sh\"]\nIt works for me. Read more about entrypoint args here https://docs.docker.com/engine/reference/builder/#entrypoint",
    "Docker: Error, Container command 'docker-entrypoint.sh' not found or does not exist": "In my case, the docker-entrypoint.sh was using Windows instead of Unix line endings. Converting the file to \\n fixed the issue.",
    "Docker intercontainer communication": "You should link your containers. There are some variants how you can implement this.\n1) Publish ports:\ndocker run -p 50070:50070 hadoop\noption p binds port 50070 of your docker container with port 50070 of host machine\n2) Link containers (using docker-compose)\ndocker-compose.yml\nversion: '2'\nservices:\n hadoop:\n  image: hadoop:2.6\n flume:\n image: flume:last\n links:\n - hadoop\nlink option here binds your flume container with hadoop\nmore info about this https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/",
    "Is it possible to set docker port mappings to host in Dockerfile?": "docker run --net=host ... might solve your problem, but generally it is not possible.",
    "How to build a large docker image with relatively small disk space consumption?": "TL;DR\nAssume the installer is 3G and the installed package is 8G.\nIf I release the Dokcerfile, then the minimum disk requirement would be at least 22 G [ (3+8)*2 = 22 ] to build this image.\nIf I release the image and have pushed to Dockerhub, then user only needs 11 G to pull the image and run a container based on it.\n==================================================================\nBuild\nI got a machine about 50G in size to rerun the build and monitored the disk consumption.\nBefore the build starts\n# df -h\nFilesystem                 Size  Used Avail Use% Mounted on\n/dev/mapper/vg_01-lv_root   59G   6G   53G  9% /\n...\nAfter the installation has completed\n# df -h\nFilesystem                 Size  Used Avail Use% Mounted on\n/dev/mapper/vg_01-lv_root   59G   19G   38G  34% /\n...\nAfter the image is commited\n# df -h\nFilesystem                 Size  Used Avail Use% Mounted on\n/dev/mapper/vg_01-lv_root   59G   27G   30G  48% /\n...\nSo to answer my own question: (1) It used up all the disk because it requires at least that much disk space. Previous calculation had wishfully assumed the running container and image to be built will share the same layer.\n(2) Haven't figure out this part yet. Right now I am just throwing away the VM and let the reclaim it.\n(3) The minimum disk required would be (3G + 8G) * 2 = 22 G. So I guess for future reference, I should reserve twice as much of theoretically calculated image size, since the layer seems to be a copied instead of shared when committing a running container to a image. [Building a Dockerfile is essentially the same as the manually running a container and committing to image.]\n=================================================================\nRun\nAnd to follow up, after I commit the image and delete the container, the disk can be reclaimed\n# df -h\nFilesystem                 Size  Used Avail Use% Mounted on\n/dev/mapper/vg_01-lv_root   59G   15G   42G  26% /\n....\nAnd from then on, bringing up a running container will not increase the disk consumption (significantly).\n# df -h\nFilesystem                 Size  Used Avail Use% Mounted on\n/dev/mapper/vg_01-lv_root   59G   15G   42G  26% /\n...",
    "Docker & PostgreSQL : can not create database having '-' in database name during initialization": "At my local Postgres installation, the following query works without a problem:\ncreate database \"1stName-2ndName\" with owner vagrant;",
    "Creating docker container with Jetty and Cassandra": "As mentioned in the Docker Cassandra Documentation instead of running cassandra with\ndocker run --name some-cassandra -d cassandra:tag\nmention all the ports that cassandra uses so that the same can be communicated or interacted from external resources like below\ndocker run --name some-cassandra -d -p 7000:7000 -p 7001:7001 -p 7199:7199 -p 9042:9042 -p 9160:9160 cassandra:tag\nNow my web application running in Jetty Container can access the Cassandra database using Java Driver.\nNote: if you are using Cassandra version > 3.0 then you should use cassandra-driver-core of version >= 3.0\nMaven dependency for cassandra-driver-core\n<dependency>\n    <groupId>com.datastax.cassandra</groupId>\n    <artifactId>cassandra-driver-core</artifactId>\n    <version>3.0.0</version>\n</dependency>\nand cassandra-driver-mapping\n<dependency>\n    <groupId>com.datastax.cassandra</groupId>\n    <artifactId>cassandra-driver-mapping</artifactId>\n    <version>3.0.0</version>\n</dependency>",
    "How to get contents generated by a docker container on the local fileystem (minimal failing example)": "This is from the guide you've pointed to\nThis won\u2019t happen if you specify a host directory for the volume\nVolumes you share from other containers or host filesystem replace directories from container.\nIf you need to add some files to volume, you should do it after you start container. You can do an entrypoint for example which does touch and then runs your main process.",
    "How to combine a Nginx and a NodeJS Docker container": "You need to link the containers. Also when containera talks to containerb it will use http://container-name not http://localhost so your code should account for this. You will notice when you use link that inside the container docker will add an entry in the hosts file to this effect. Use environment variables so you can make the domain name to which you will make requests flexible. And pass this environment variables when you run the container\nMake sure u expose the right ports between containers",
    "docker push to registry does not work": "A tag boot2docker (even though it has been obsoleted by docker machine) means you are not on Linux directly, but on Windows or Mac, using a Linux VM.\nYou have a similar error message reported in issue 523 of docker/distribution (the new registry server)\nWhen you bind to localhost inside the container, the service won't be available outside the container, even though you are specifying the port mapping.\nWhen the docker daemon goes to connect, it cannot connect to the port since the service is not bound to the \"external\" interface of the container.\nThat means you need to setup port forwarding, or to use the docker-machine ip new_registry ip address.\ndocker push $(docker-machine ip new_registry):5001/test:latest\n5000:5000 works but 5001:5002 does not work when creating registry.\nIt is possible that the VM was already set to port forward port 5000, but not 5001 (similar to this picture).\nIt means the registry image exposes port 5000: you can map that on host port 5000 or 5001 or any port you want: but it has to be port 5000 that you map:\ndocker run -d -p 5001:5000 --restart=always --name new_registry registry:2\n                      ^^^^",
    "How can the current build context directory be referenced inside of a Dockerfile?": "With docker 1.9, you can pass build-time environment variable:\ndocker build --build-arg APP_SRC=$BUILD_CONTEXT -y tag .\n$APP_SRC will then be valued like $BUILD_CONTEXT.\nNote that this is not yet supported by docker compose: both issue 2111 and 2163 are asking for that feature.",
    "Can I run a bash script from a file in a separate docker volume before a container starts?": "The problem is that the shell within which your /src/app/bin/start-app is ran is not an interactive shell => .bashrc is not read!\nYou can fix this by doing this: Add the env file to instead: /root/.profile\nRUN echo \"source /src/puppet/path/to/file/env/bootstrap\" >> /root/.profile\nAnd also run your command as login shell via (sh is bash anyhow for you via the hack in the Dockerfile :) ):\ncommand: \"sh -lc '/src/app/bin/start-app'\"\nas the command. This should work fine :) The problem really is just missing to source that file because you're running in a non-interactive shell when running via the docker command instruction. It works when you shell into the container, because bam you got an interactive shell that sources that file :)",
    "Docker Hub automated build failure. Local build is fine": "So the image that you are pushing has the /var/www/html/ directory in it (and probably has the git repo in it).\nTry this in your docker file to make sure the directory doesn't exist:\nRUN rm -rf /var/www/html\nRUN git clone git://github.com/symphonycms/symphony-2.git /var/www/html && git checkout --track origin/bundle && git submodule update --init --recursive && git clone git://github.com/symphonycms/workspace.git && chown -R www-data:www-data",
    "Error running commands with Dockerfile": "Your source command is in a RUN, so it is forgotten in the next command",
    "Docker - unable to mount start mongodb in container - operation not permitted": "Main answer: Docker Machine\nIf you're running Docker on Windows (in 2015), then you must be running it using Docker Machine. This is because Docker does not yet run natively on Windows; instead it uses a virtual computer in VirtualBox and docker runs on this instead.\nNow the crucial thing is that the host (Windows) folders shared with Docker are done through this VirtualBox machine, which means they use a virtual file system vfs - not ntfs or xfs or any of the file systems as normally understood.\nAnd the problem with this vfs file system is that it doesn't have all the features which MongoDB requires; see the docs on MongoDB Platform Specific Considerations :\nIMPORTANT\nMongoDB requires a filesystem that supports fsync() on directories. For example, HGFS and Virtual Box\u2019s shared folders do not support this operation.\nSo when you try to run MongoDB in a docker container on Windows, using a volume mapping to a shared windows folder, it won't work because the VirtualBox's intermediate file system isn't capable of supporting MongoDB file system commands.\nAppendix: Native Docker on Windows 10\nFrom Windows 10 onwards, there is a new option to run Docker on Windows natively without using Docker Machine or VirtualBox. However, there is again a problem with the file system; it uses ntfs shared over smb, which again does not support all the file operations which MongoDB needs, and leads to similar Operation Not Supported errors.",
    "Customize mongo docker raise an error when I try to create a new user for database": "You have some errors in run.sh and set_mongodb_password.sh.\nIn script run.sh you need to add \"set -m\" right after the line #!bash, because in the last line you use fg. Read the post https://unix.stackexchange.com/questions/196603/can-someone-explain-in-detail-what-set-m-does to understand \"set -m\" and \"fg\".\nIn the same script you need to call set_mongodb_password like /usr/bin/set_mongodb_password.sh instead of set_mongodb_password\nThe problem of the set_mongodb_password.sh is that it run before run.sh, so you need to add the line below after \"#!bash\" to wait the mongo goes up:\nRET=1\nwhile [[ RET -ne 0 ]]; do\n    echo \"=> Waiting for confirmation of MongoDB service startup\"\n    sleep 5\n    mongo admin --eval \"help\" >/dev/null 2>&1\n    RET=$?\ndone\nAfter doing this you will be able to execute mongo commands and create the user you need.",
    "Unknown filesystem type on /dev/mapper/docker-202": "Well, I solved it. Sort of. I don't know why the problem was really caused (if someone does, please let me know, I'm curious), but here's how it was solved: Instead of the device mapper filesystem, which seemed like the source of the problem, I created a btrfs filesystem, which docker appears to be quite happy with.\nThese are the steps I took:\nI started by adding a new volume to my EC2 instance (2GB of General Purpose SSD). This is a really good guide for doing just that.\nNow I started the switch from device mapper to btrfs:\nsudo service docker stop\nsudo rm -rf /var/lib/docker\nsudo mkfs.btrfs -f -O ^extref -L docker /dev/xvdf # The extref option is not fully supported and causes all kinds of trouble, which is why I disabled it\nsudo echo \"/dev/xvdf /var/lib/docker btrfs defaults 0 0\" >> /etc/fstab\nsudo mount /dev/xvdf /mnt\nNow I configured docker to use btrfs by editing the file /etc/sysconfig/docker like so:\nOptions=\"-s btrfs\"\nWhen I started the service (sudo service docker start), I could see, by running docker info, that the switch was successful: Storage Driver: btrfs.\nI ran the build again, and it worked like a charm :)",
    "Working on a Dockerfile in order to build a WordPress image": "The WordPress image Dockerfile does begin with FROM php:5.6-apache.\nBut the php:5.6-apache image Dockerfile starts with FROM debian:jessie.\nHence the apt-get.\nEach image builds up based on another base image.",
    "DockerFile with private github repo in python requirements.txt": "Generate a Personal access token .\nYou should select a scope of this token to limit access to your personal project.\nYou can pass a token to curl instead of user/password\ncurl -u <token>:x-oauth-basic",
    "How can I mount a volume from a data container while preserving the owner and permissions?": "I would modify your image a little bit. Write a shell script that wraps the /usr/bin/start-server command in your fig.yml and place that inside your container.\nThen you can chown rails anything that you need before starting up your server.\nRunning the container with a default user rails is not really needed either, as long as you start up the server as the rails user: sudo -u rails /usr/bin/start-server (or something like that).\nPersonally haven't used the litdistco-base image yet, so do not know all the specifics on how it works.",
    "Why does the path command doesn't work in Docker file": "According to\nhttps://github.com/docker/docker/issues/684\nyou should use the\nENV\ncommand to set $PATH to your value\nhttps://docs.docker.com/reference/builder/#ENV",
    "Is it possible to use a private git(hub) repo in Dockerfile ADD?": "It's not possible yet because docker build doesn't support build-time parameters, although I think this feature may be coming soon. You'll have to do a build using the docker context for now.\nFor example, if you look at the way hub.docker.com auto-build works, you have to add a deploy key into the github repo that allows their build servers to read that repo.",
    "tf-random won't install in Docker container": "Try running locale-gen with your LANG value as it's argument.",
    "How Can I install `passenger-install-nginx-module` via Dockerfile": "try replace RUN passenger-install-nginx-module by\nRUN /bin/bash -l -c 'passenger-install-nginx-module --auto-download --auto --prefix=/opt/nginx' ",
    "Go requirements.txt for Dockerfile": "You can use the project cespare/deplist\nOr check this thread which uses go list.\ngo list -f '{{.ImportPath}}' P/... | xargs -n 1 deplist | grep -v P | sort -u\nwith P being the partial package path.",
    "Docker not reading API key in .env file": "You need to ensure that your requirements.txt file has python-dotenv package. So that you should be able to read .env files. Hence add python-dotenv into your requirments.txt file.\nSecondly make .env file and use COPY Command to copy it in your Dockerfile (Although this will be unsafe way of doing things... Can be done for Testing purpose).\nCOPY .env .env\nFor Production Environment:\nIt is recommended to us Google Cloud Run -> (Your Service Name ) -> Edit & Deploy New Revision -> Environment variables\nYou can go there and define your API_Key over there and deploy it in cloud run it will be able to pick it up.",
    "Build a Docker Image from VisualStudio ASP.net project and use it on different docker host": "Docker images are usually not transported from server A to server B (like drag and drop or like some file be like), either you build them together with a Dockerfile or you pull them from a repo-server e.g. hub.docker.com.\nSince you have a Dockerfile, you can upload this Dockerfile and your source code (just your whole project folder) to your other Docker server (via ftp, ssh or similar) and then build the image with the following command docker build -t yourImageName .\ndocker build: This part of the command tells Docker to build a new image.\n-t yourImageName: The \"-t\" flag is used to tag the image with a name. In this case, \"yourImageName\". You can replace \"yourImageName\" with whatever name you prefer.\n.: The dot at the end of the command specifies the build context. It tells Docker to look for the Dockerfile in the current directory.\nNormally, you should now see bars in the console that work from left to right, pulling, building or extracting your/other images.\nOnce the image has been built, it can be used normally as an example: docker run -d --name myContainerName yourImageName of course you can add other parameters like -p for port or -v for the volume mapping or how you did it in a docker compose file.",
    "Building Docker image via Visual Studio fails: \"error: Docker is not installed or is not in the current PATH.\"": "you need to change Propierties/PublishProfiles/registry.hub.docker.com_UserName\nyou need change the URL and you can find more information documentation file en docker.\nhttps://registry.hub.docker.com/repositories/XXXXX\nFlavio",
    "How to build a Dockerfile with a Testcontainers?": "I found your post because I have the similar problem with \"Could not find a valid Docker environment\". I have exactly the same info as you with docker-machine executable was not found etc. In your dockerfile, there's a typo. Instead of ESTCONTAINERS_HOST_OVERRIDE you should have TESTCONTAINERS_HOST_OVERRIDE. I have correct variable name but it still does not work.",
    "Cannot install requirements.txt in Dockerfile with Docker": "The problem is that the network is not set to host. To make it work use the network option like this: docker build --network=host or, if you're using docker compose like I was, use the following syntax:\n...\nbuild:\n    context: .\n    network: host\n...\nand then build with docker compose up --build",
    "Docker Compose Secret from Environment Variable": "I am working on a similar problem right now. The documentation does not seem to clearly explain how to use this, but containers define secrets separately at build-time and run-time.\nThis will make the secrets available at run-time but not build-time:\n    build:\n      context: $LOCATION/directory\n      target: local_dev\n    secrets:\n      - ARTIFACTORY_USER\n      - ARTIFACTORY_PASS\nThis will make them available at build-time but not run-time:\n    build:\n      context: $LOCATION/directory\n      target: local_dev\n      secrets:\n        - ARTIFACTORY_USER\n        - ARTIFACTORY_PASS",
    "Docker ubuntu 22.04 create user directory with non root ownership": "Thanks for the answers. Solved by making user with no home directory as @DavidMaze suggested.",
    "How can I run dotnet CLI commands inside a running container?": "The dotnet tools must be installed inside the Dockerfile. The following worked for me:\nFROM mcr.microsoft.com/dotnet/sdk:5.0\nWORKDIR /app\n\nRUN dotnet tool install --global dotnet-ef --version 5.0.17\n\nENV PATH=\"$PATH:/root/.dotnet/tools\"\n\nEXPOSE 8010",
    "How to deploy on fly.io when package.json contains (private) packages from Github?": "add this to your docker file\nARG GITHUB_TOKEN\nENV env_name $GITHUB_TOKEN\n...\nThen, on your deploy command: fly deploy --build-arg GITHUB_TOKEN=YOUR_TOKEN",
    "Docker pull access denied for docker-image": "You should try to log out docker logout [options] and then login docker login [options] from the command line to your docker hub account before to pull/push the images.",
    "ImportError: libGL.so.1: cannot open shared object file: No such file or directory": "Add the following lines to your Dockerfile:\nRUN apt-get update && apt-get install ffmpeg libsm6 libxext6  -y\nThese commands install the cv2 dependencies that are normally present on the local machine, but might be missing in your Docker container causing the issue.\n[minor update on 20 Jan 2022: as Docker recommends, never put RUN apt-get update alone, causing cache issue]",
    "Frontend can't connect to backend API in Docker": "It seems like a CORS issue. After deployed to docker, the front-end origin has changed. You can try to modify the back-end asp.netcore-api program.cs to allow all origins to see if it works.\nbuilder.Services.AddCors(o => o.AddPolicy(\"CorsPolicy\", builder =>\n{\n    builder\n        .AllowAnyOrigin()\n        .AllowAnyHeader()\n        .AllowAnyMethod();\n}));\n...\napp.UseHttpsRedirection();\napp.UseCors(\"CorsPolicy\");\n...\nIf it is the problem, then you need try to only allow the correct orgins\nbuilder.Services.AddCors(o => o.AddPolicy(\"CorsPolicy\", builder =>\n{\n    builder.WithOrigins(\"http://127.0.0.1:5500\").AllowAnyHeader().AllowAnyMethod();  //for front-end host at 127.0.0.1:5500\n}));",
    "How do I update my Docker container so that it's unprivileged?": "Solution\nI updated the Dockerfile like so:\n# nginxinc/nginx-unprivileged:alpine-slim has container vulnerabilities that do not pass AquaScan, even after updating. 3/21/2023.\nFROM nginx:alpine-slim\n# implement changes required to run NGINX as an unprivileged user\nRUN sed -i 's,listen       80;,listen       8080;,' /etc/nginx/conf.d/default.conf \\\n  && sed -i '/user  nginx;/d' /etc/nginx/nginx.conf \\\n  && sed -i 's,/var/run/nginx.pid,/tmp/nginx.pid,' /etc/nginx/nginx.conf \\\n  && sed -i \"/^http {/a \\    proxy_temp_path /tmp/proxy_temp;\\n    client_body_temp_path /tmp/client_temp;\\n    fastcgi_temp_path /tmp/fastcgi_temp;\\n    uwsgi_temp_path /tmp/uwsgi_temp;\\n    scgi_temp_path /tmp/scgi_temp;\\n\" /etc/nginx/nginx.conf \\\n  # nginx user must own the cache and etc directory to write cache and tweak the nginx config    && chown -R $UID:0 /var/cache/nginx \\\n  && chmod -R g+w /var/cache/nginx \\\n  && chown -R $UID:0 /etc/nginx \\\n  && chmod -R g+w /etc/nginx\nCOPY --chown=nginx:nginx ./docker/nginx/nginx.conf /etc/nginx/conf.d/default.conf\nWORKDIR /usr/share/nginx/html\nUSER nginx\nEXPOSE 8080\nCOPY build .\nNow my frontend app deploys in a container and passes AquaScan.",
    "Powershell variable assignment inside a dockerfile and modification": "In the Dockerfile, we can use ENV to declare environment variables, and refer them in the powershell commands, like below.\nENV key=value\nRUN echo $env:key\nEg.:\nFROM mcr.microsoft.com/windows/servercore:ltsc2019\n# Using powershell shell \nSHELL [\"powershell\", \"-NoProfile\", \"-Command\", \"$ErrorActionPreference = 'Stop'; $ProgressPreference = 'SilentlyContinue';\"] \n# Powershell commands\nENV USER=\"user1\"\nENV PASS=\"pass1\"\nRUN echo \"1 - $USER and $PASS\"\nRUN echo \"2 - $env:USER and $env:PASS\"\nOutput for command RUN echo \"1 - $USER and $PASS\":\n1\n-\nand\nOutput for command RUN echo \"2 - $env:USER and $env:PASS\":\n2\n-\nuser1\nand\npass1\nCredits to @jeroen-mostert's comment.",
    "Nginx can't load /usr/share/nginx/html/ files are missing": "Thanks to @bassxzero solved this problem.\nnginx.conf:\nevents {\n    worker_connections 1024;\n}\nhttp {\n  server {\n    listen 80;\n    server_name localhost;\n\n    location / {\n        root /usr/share/nginx/html;\n        index index.html index.htm;\n        try_files $uri $uri/ /index.html;\n    }\n  }\n}",
    "Installing specific node.js version in Dockerfile": "There's a solution for building a Docker image with nvm, which includes running the install commands under a login shell.\nRegardless, note that you need to make sure that the installation directory ($NVM_DIR) exists before you install nvm-sh. Since the Base docker run under a non-admin user you cannot set it to be under the root user.\nSee an example for Dockerfile below which works based on provided solution:\nFROM jetbrains/teamcity-agent:2022.10.1-linux-sudo\n\nRUN curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\nRUN sudo sh -c 'echo deb https://apt.kubernetes.io/ kubernetes-xenial main > /etc/apt/sources.list.d/kubernetes.list'\n\nRUN curl -sL https://deb.nodesource.com/setup_16.x | sudo -E bash -\n\n# https://github.com/AdoptOpenJDK/openjdk-docker/blob/master/12/jdk/ubuntu/Dockerfile.hotspot.releases.full\nRUN sudo apt-get update && \\\n    sudo apt-get install -y ffmpeg gnupg2 git sudo kubectl \\\n    binfmt-support qemu-user-static mc jq\nRUN sudo apt install -y cmake build-essential wget\n\nSHELL [\"/bin/bash\", \"--login\", \"-c\"]\n\nENV NODE_VERSION=14.17.3\nENV NVM_DIR /tmp/nvm\nWORKDIR $NVM_DIR\n\nRUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash \\\n  && . $NVM_DIR/nvm.sh \\\n  && nvm install $NODE_VERSION \\\n  && nvm alias default $NODE_VERSION \\\n  && nvm use default\n\nENV NODE_PATH $NVM_DIR/v$NODE_VERSION/lib/node_modules\nENV PATH      $NVM_DIR/v$NODE_VERSION/bin:$PATH\n\nRUN node --version\nRUN npm --version",
    "The Spring boot app doesn't seem to run after putting it in container": "try to use this driverClassName in application.properties\nspring.datasource.driverClassName=com.mysql.jdbc.Driver\nHave you tried to start only Mysql db in the container and connect to it from you IDE?",
    "Why is npm install not working inside docker?": "From the official guide you've sent, you can clearly see in the following link, I highlighted the version they used in the main guide and in your Dockerfile your using FROM node:latest meanwhile they are using node:10 can you please edit your Dockerfile and change the latest to 10 in order to fix node version to the",
    "java command is not found using jlink in docker alpine": "I've got the solution changing the first stage image to alpine-based image as follow\n# using image on alpine-based\nFROM gradle:7.5.1-jdk17-alpine As build\n\nWORKDIR /build\n\nCOPY my-source my-source\n\n# install required packages\nRUN apk update && apk add gcompat binutils\n\nRUN gradle -p my-source clean build\n\nRUN jlink \\\n    --module-path /opt/java/openjdk/jmods \\\n    --add-modules $(jdeps --ignore-missing-deps --print-module-deps --multi-release 17 my-source/build/libs/*.jar ) \\\n    --output jre-custom \\\n    --strip-debug \\\n    --no-header-files \\\n    --no-man-pages \\\n    --compress 2\n    \n# reduce image size a little bit more (-4MB)\nRUN strip -p --strip-unneeded jre-custom/lib/server/libjvm.so && \\\n   find jre-custom -name '*.so' | xargs -i strip -p --strip-unneeded {}\n\nFROM alpine:latest\nWORKDIR /deployment\n\nCOPY --from=build /build/jre-custom jre-custom/\nCOPY --from=build /build/my-source/build/libs/*.jar build/app.jar\n\nCMD [\"jre-custom/bin/java\",\"-jar\",\"build/app.jar\"] \n  \nEXPOSE 3009",
    "Restart NodeJS app running in docker container": "You have to run your container with this command:\n docker run -d --restart unless-stopped [Container_name]\nwith unless-stopped flag your container will restart automatically unless you stop it manually.\nWith this approach you can stop your Node js process using process.exit() and your container will restart.\nFor more info head to this page",
    "Docker buildx failing to find local docker image": "I believe this is a current limitation of the buildx builder. I found https://github.com/moby/buildkit/issues/2343, which has a lot of discussion but no resolution yet.",
    "Error: EACCES: permission denied, mkdir '/app/node_modules/.vite/deps_temp'": "I have tried suggestions of change /app owner to \"node\" but it does't works. It only works putting \"RUN chmod 777 /app/node_modules\" before \"CMD [ \"npm\", \"run\", \"dev\" ]\". Thank you all.",
    "Error failed to fetch an image or build from source: error building: failed to solve with frontend docker": "This quay.io/evl.ms/fullstaq-ruby:2.6.1-jemalloc-slim image (probably used in you Dockerfile's FROM clause) doesn't really exist. Check it yourself here:\nSo, the lowest available 2.6.x tag on quay.io is 2.6.3-jemalloc-stretch-slim. Probably, they just purged old, unsupported versions.\nTry to upgrade to a newer version.",
    "Implementing Poetry inside Docker Container": "Three main things are important while it's poetry,\nRUN pip3 install -U pip poetry\nRUN poetry config virtualenvs.create false\nRUN poetry install --no-interaction --no-ansi\nInstall the poetry\nSet not to create a virtualenv\nInstall the packages.\nHow you can test.\nYou can just build the Dockerfile to create an image.\nSo, docker build -t tag_name_your_project .",
    "Docker compose does not work if node_modules are not installed on local machine": "Can it be that you are copying the whole directory with the node_modules when you execute the COPY . . command, try and either specifiy the directory to copy or add a .dockerignore.\nFound this in the documentation, https://docs.docker.com/language/nodejs/build-images/#create-a-dockerignore-file",
    "Is it possible to mount a directory in a docker image during build?": "On windows:\ndocker buildx create --driver-opt image=moby/buildkit:master ^\n    --use --name insecure-builder ^\n    --buildkitd-flags \"--allow-insecure-entitlement security.insecure\"\ndocker buildx use insecure-builder\ndocker buildx build --allow security.insecure .\ndocker buildx rm insecure-builder",
    "Does the code gets copied from docker image to docker container": "Does everything in the image get copied over to the container?\nYes. A container is like an instance of the image.\nIf the same image is used among all the containers, then how does docker handle when a file is changed in a container?\nThe change will only affect the container in which the change was made. It does not affect the image nor any other containers using the same image (unless the change was made to a file on a volume that multiple containers share).\nWhen I saved the file, it doesn't get reflected in the get call of the express app\nIf you stop and start the container (without recreating it), you should still see your changes persisted. The Express app may need to be restarted for your saved changes to take into effect.\nThough generally, if you need to make a change, you should do it to your local files, rebuild the image, and start a new container. This is because the image is what gets distributed, not the container. If the changes are made in the container, then you won't be able to persist them if you need to deploy your image elsewhere.\nFor development purposes, you can look into using bind mounts, which make it easy for changes made to your local files to propagate to the container.",
    "Problem installing Python Poetry on Docker": "According to official documentation installing via pip is completely fine.\nYou can do it like this (version is pinned for stability):\nENV POETRY_VERSION 1.5.1\n\nRUN pip install \"poetry==$POETRY_VERSION\"\n\nRUN poetry config virtualenvs.create false && poetry install --no-interaction --no-ansi",
    "OpenShift Dockerfile Build that references an ImageStream?": "You can add the \"from\" field to force Openshift replacing the \"FROM\" image in your Dockerfile like this:\nspec:\n  strategy:\n    type: Docker\n    dockerStrategy:\n      from:\n        kind: ImageStreamTag\n        name: 'myImageStream:tag'\nfrom is a reference to an DockerImage, ImageStreamTag, or ImageStreamImage which overrides the FROM image in the Dockerfile for the build. If the Dockerfile uses multi-stage builds, this will replace the image in the last FROM directive of the file.",
    "How to add win32com.client as a dependency in requirements.txt for a docker image?": "I guess the closest you can get is installing wine inside the container and executing your code under wine.\nYou will need to install python with wine prior.\nThere are wine ready docker images.\nIn any case I do not think it is going to work for your purpose, Outlook if exists in the container will not be properly configured.",
    "NextJS Docker error: Couldn't find a `pages` directory. Please create one under the project root": "Here is my working Dockerfile with nextjs:\nFROM node:16.14.0\n\nRUN npm install -g npm@8.5.5\n\nRUN mkdir -p /app\n\nWORKDIR /app\n\nCOPY package*.json /app\n\nRUN npm i\n\nCOPY . /app\n\nEXPOSE 3000\n\nRUN npm run build\n\nCMD [\"npm\", \"run\", \"dev\"]\nAnd docker-compose.yml :\nversion: \"3.7\"\n\nservices:\n  project-name:\n    image: project-name\n    build:\n      context: .\n      dockerfile: Dockerfile\n    container_name: project-name\n    restart: always\n    volumes:\n      - ./:/app\n      - /app/node_modules\n      - /app/.next\n    ports:\n      - \"3000:3000\"",
    "Django and postgresql in docker - How to run custom sql to create views after django migrate?": "Thanks to hints from Erik, solved by the following\npython manage.py makemigrations --empty yourappname\nThen added the following (note cut down for space)\nfrom django.db import migrations\n\ndef get_all_items_view(s=None):\n    s = \"\"\n    s += \"create or replace view v_all_items_report\"\n    s += \" as\"\n    s += \" SELECT project.id, project.slug, project.name as p_name,\"\n    ...\n    return s\n\ndef get_full_summary_view(s=None):\n    s = \"\"\n    s += \"CREATE or replace VIEW v_project_summary\"\n    s += \" AS\"\n    s += \" SELECT project.id, project.slug, project.name as p_name, \n    ...\n    return s\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('payment', '0002_payment_user'),\n    ]\n\n    operations = [\n        migrations.RunSQL(get_all_items_view()),\n        migrations.RunSQL(get_full_summary_view()),\n        migrations.RunSQL(get_invoice_view()),\n        migrations.RunSQL(get_payment_view()),\n    ]\nNote to ensure you list the last table that needs to be created in your dependencies before the views get created. In my case django defaulted all other migration to be dependant on one another in a chain order.\nIn my dockerfile entrypoint.sh script is where I had the commands to makemigrations, migrate and build some sample data",
    "Docker run doesn't start the container": "This error will happen if the casing of the DLL name you're targeting in your Dockerfile doesn't match the casing of the file that exists on disk (this is only true for Linux). Be sure that ENTRYPOINT [ \"dotnet\", \"PlatformService.dll\" ] uses a DLL name that matches the casing of the actual file.",
    "Powershell: Get-Package: A parameter cannot be found that matches parameter name 'Scope'": "I was just struggling with the same, or at least similar (for me it's Install-Package), issue right now.\nWorkaround / Solution\nOur workaround is to simply call the command within the powershell environment:\npowershell Install-Package -Name My.Package -Source MySource -Scope CurrentUser -Force -AllowPrereleaseVersions\nElaborations & Observations\nI run a few tests on our build server with our desired call (stripped to the essentials for the sake of the elaboration below, including screenshot):\nInstall-Package -Name My.Package -Source MySource -Scope CurrentUser -Force -AllowPrereleaseVersions\nThe call failed with pwsh (7.2.5) with the error you describe.\nHowever, when I call with powershell (5.1.19041.1682), it succeeds.\nEven stranger: When I start pwsh, the call fails as mentioned above. However when I switch to powershell, back to pwsh and execute the command again, it succeeds as you can verify in the screenshot:\nIt is still a mystery to me and I'd be glad if someone can elaborate on this behavior.",
    "Dockerfile : Current path and its contents": "the files you put at left side of copy must be in same folder of your dockerfile.\nyou need to do ls -l /tmp always after copy to see that files are copied.",
    "Connect from a docker container to a mysql database in another container": "The following method in db container solved the problem.\ndatabase setting\nUSE mysql;\nGRANT ALL ON *.* TO 'root'@'%' identified by 'pass' WITH GRANT OPTION ;\nGRANT ALL ON *.* TO 'root'@'localhost' identified by 'pass' WITH GRANT OPTION ;\nFLUSH PRIVILEGES ;\nmysql config file setting\necho \"bind-address = app\" >> etc/mysql/mariadb.conf.d/50-server.cnf",
    "Docker copy command does not copy file": "The COPY command accept only relative paths, I suggest you to rewrite the docker file as:\nFROM tiangolo/uwsgi-nginx-flask:python3.6\nLABEL maintainer=\"Simranjeet Singh <simranjeetsingh1497@gmail.com>\"\n\nCOPY app_dash/requirements.txt /tmp/\nCOPY app_dash /app\nCOPY dags/helpfiles/ml_pipeline_config.py /app\n\nRUN pip install -U pip && pip install -r /tmp/requirements.txt\n\nENV NGINX_WORKER_PROCESSES auto\nThe relative path starts from the context path you provide to the docker build command, so you should do:\ncd <docker-file-folder>\ndocker build .\nSo all the file are picked from the . (current) context folder.\nLet me know if it worked.\nRegards.",
    "Building Go project from Dockerfile says package not in GOROOT": "Go has a system variable set location for the build location (which is strict). This is either your home directory +go/src/ or the GOPATH.\nIn your case you have to set your GOPATH:\nENV GOPATH /myApp",
    "Dockerfile skip maven dependencies download again and again": "You should not have to change anything in your Dockerfile. Docker will cache pom.xml and if it has not been modified since the last build, the COPY instruction is skipped.",
    "Unable to update Openssl.cnf file using Dockerfile command": "Some of possible reasons for failure\nWebApplication/openssl.cnf might be different from openssl.cnf used in example\nYour application might destroy openssl.cnf on the container start - be sure, there is nothing toxic in source code, which might cause it\nupdate-ca-certificates might have some side effects, which might with the new openssl.cnf break your app.\nRecommendation for debug\nTry to diff the original openssl.cnf with the openssl.cnf in the container. It will help you to find the origin of the error.\nIf the openssl.cnf is intact, I would recommend to check, whether the new certificate bundle is the same as is in older images.\nIf the openssl.cnf is changed, try to find the source of the change. Changing the order of Dockerfile execution might help you with it.",
    "size of saved docker images for base versus derived": "Best guess is something else was tagged alpine on your host when you did the build, and between the two commands, something pulled the correct alpine image down. But rather than guessing, you have the file to tell you exactly what was in that first alpine image. Extract the tar, review the manifest, and review the config the manifest points to. Here's an example with whatever old version of alpine:latest I happened to have on my machine:\n$ cat manifest.json | jq .\n[\n  {\n    \"Config\": \"45683da4f97c23d92b878d2ad15eb66e94c9cab368e54797bcd4a4b20c915815.json\",\n    \"RepoTags\": [\n      \"test-alpine:latest\"\n    ],\n    \"Layers\": [\n      \"4833c0e9afedb6022373ef7da2cee392c97a5f4438e7f52a1f940903db618437/layer.tar\"\n    ]\n  }\n]\n\n$ cat 45683da4f97c23d92b878d2ad15eb66e94c9cab368e54797bcd4a4b20c915815.json | jq .\n{\n  \"architecture\": \"amd64\",\n  \"config\": {\n    \"Env\": [\n      \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n    ],\n    \"Cmd\": [\n      \"/bin/sh\"\n    ],\n    \"OnBuild\": null\n  },\n  \"created\": \"2021-08-27T17:19:45.758611523Z\",\n  \"history\": [\n    {\n      \"created\": \"2021-08-27T17:19:45.553092363Z\",\n      \"created_by\": \"/bin/sh -c #(nop) ADD file:aad4290d27580cc1a094ffaf98c3ca2fc5d699fe695dfb8e6e9fac20f1129450 in / \"\n    },\n    {\n      \"created\": \"2021-08-27T17:19:45.758611523Z\",\n      \"created_by\": \"/bin/sh -c #(nop)  CMD [\\\"/bin/sh\\\"]\",\n      \"empty_layer\": true\n    }\n  ],\n  \"os\": \"linux\",\n  \"rootfs\": {\n    \"type\": \"layers\",\n    \"diff_ids\": [\n      \"sha256:e2eb06d8af8218cfec8210147357a68b7e13f7c485b991c288c2d01dc228bb68\"\n    ]\n  }\n}\nWhen you compare the two images, if they were built from the same alpine base, they will have the same initial set of layers. In your case, they almost certainly won't.",
    "Compressed file does not appear after running zip command on dockerfile": "As the friend said above, the /var/www/html path is a reserved volume. If you want to manipulate files, just a new directory from root, example: mkdir -p /temp/files",
    "Dockerfile, RUN git clone does not create required files/folders": "The problem is your environment variable, is empty when the container run TOMCAT_HOME\nFROM tomcat:9.0-alpine\nENV TOMCAT_HOME=/usr/local/tomcat\nRUN apk update\nRUN apk add git\nRUN git clone https://github.com/a1qatraineeship/docker_task.git $TOMCAT_HOME/webapps/whateverApp/\nand with that ENV should work, gook luck!",
    "Error when starting from Dockerfile \"Unable to locate package build-esential\"": "You're missing an s in build-essential (you wrote build-esential).",
    "unable to run npm install in docker image (getting auth error)?": "Thanks to Phil I had the same problem and I managed to unblock my problem thanks to your comment\nHere is my Dockerfile\nFROM node:16\n\n# Create app directory\nWORKDIR /usr/src/app\n\n# Install app dependencies\n# A wildcard is used to ensure both package.json AND package-lock.json are copied\n# where available (npm@5+)\n# copying packages first helps take advantage of docker layers\nCOPY package.json .\n\nRUN npm install prettier -g\n\n# If you are building your code for production\nRUN npm install\n\n\n # Bundle app source\nCOPY . .\n\nRUN npm run build\n\nEXPOSE 4000\n\nCMD [ \"node\", \"dist/server.js\" ]",
    "Databricks Connect: Automatically Accept License Prompt": "You need to use something like this (stolen from this demo), because besides accepting the license terms, you also need to provide other parameters:\necho \"y\n$(databricks_host)\n$(databricks_token)\n$(cluster_id)\n$(org_id)\n15001\" | databricks-connect configure\nOr you can just generate ~/.databricks-connect file that is just JSON:\n{\n  \"host\": \"https://host\",\n  \"cluster_id\": \"cluster\",\n  \"org_id\": \"org_id\",\n  \"port\": \"15001\"\n}",
    "Docker cannot run image due to a \u201chcsshim::System::CreateProcess: failure in a Windows system call\u201d": "Can you try to build your application before to get the dll (msbuild step) on the bin folders. And then docker build again.",
    "Deploy Reactapp using Docker & SSL": "By Looking at your comments it looks like your port configuration is not correct, in NginX port listening is set to listen on port 443, but your docker port configuration is using port 80 as host port. Assuming Node server is listening at port 8080, docker run should be like this\n$ docker run -itd -p 443:443 prod\nAnd try https://ipaddress , based on certificate setting you should see either warning in browser (if certificate is not trusted fully, you might need to add it as an exception), or see proper contents.",
    "Building docker image FROM scratch using a buildroot linux": "There is no need for any containers. Buildroot (and other build systems) do cross compiling, which means you can build for a different target than the machine you build on.\nIn other words, you simply select arm64 as the target architecture, make the build, then install the resulting rootfs on your target.\nHowever, this rootfs completely replaces the target rootfs, so it's not relevant that the target is uclibc. So my guess is that you want to install just a single executable. Doing that is made more difficult with shared libraries, because you need to copy not just the executable, but also any libraries it links with. So it may help to configure Buildroot to link statically (BR2_STATIC_LIBS).\n-EDIT-\nIf you want to run an environment similar to the target, it's not possible to run this in docker unless your build machine is also an arm64. That's what the warning \"requested image's platform (linux/arm64) does not match the detected host platform (linux/amd64)\" is saying. Instead of docker, you need to use virtualisation, e.g. qemu.\nYou can bring up a qemu environment for arm64 with make qemu_aarch64_virt_defconfig. Check out board/qemu/aarch64-virt/readme.txt for how to start qemu.",
    "In Dockerfile, does a CMD instruction add a new layer? [duplicate]": "The short answer is: no.\nHere's an example of the python image history:\n$ regctl image config python --format '{{jsonPretty .History}}'\n[\n  {\n    \"created\": \"2023-11-21T05:21:24.536066751Z\",\n    \"created_by\": \"/bin/sh -c #(nop) ADD file:39d17d28c5de0bd629e5b7c8190228e5a445d61d668e189b7523e90e68f78244 in / \"\n  },\n  {\n    \"created\": \"2023-11-21T05:21:25.128983079Z\",\n    \"created_by\": \"/bin/sh -c #(nop)  CMD [\\\"bash\\\"]\",\n    \"empty_layer\": true\n  },\n  {\n    \"created\": \"2023-11-21T09:52:48.60112971Z\",\n    \"created_by\": \"/bin/sh -c set -eux; \\tapt-get update; \\tapt-get install -y --no-install-recommends \\t\\tca-certificates \\t\\tcurl \\t\\tgnupg \\t\\tnetbase \\t\\tsq \\t\\twget \\t; \\trm -rf /var/lib/apt/lists/*\"\n  },\n  {\n    \"created\": \"2023-11-21T09:53:05.826622089Z\",\n    \"created_by\": \"/bin/sh -c apt-get update && apt-get install -y --no-install-recommends \\t\\tgit \\t\\tmercurial \\t\\topenssh-client \\t\\tsubversion \\t\\t\\t\\tprocps \\t&& rm -rf /var/lib/apt/lists/*\"\n  },\n  {\n    \"created\": \"2023-11-21T09:54:02.653610372Z\",\n    \"created_by\": \"/bin/sh -c set -ex; \\tapt-get update; \\tapt-get install -y --no-install-recommends \\t\\tautoconf \\t\\tautomake \\t\\tbzip2 \\t\\tdpkg-dev \\t\\tfile \\t\\tg++ \\t\\tgcc \\t\\timagemagick \\t\\tlibbz2-dev \\t\\tlibc6-dev \\t\\tlibcurl4-openssl-dev \\t\\tlibdb-dev \\t\\tlibevent-dev \\t\\tlibffi-dev \\t\\tlibgdbm-dev \\t\\tlibglib2.0-dev \\t\\tlibgmp-dev \\t\\tlibjpeg-dev \\t\\tlibkrb5-dev \\t\\tliblzma-dev \\t\\tlibmagickcore-dev \\t\\tlibmagickwand-dev \\t\\tlibmaxminddb-dev \\t\\tlibncurses5-dev \\t\\tlibncursesw5-dev \\t\\tlibpng-dev \\t\\tlibpq-dev \\t\\tlibreadline-dev \\t\\tlibsqlite3-dev \\t\\tlibssl-dev \\t\\tlibtool \\t\\tlibwebp-dev \\t\\tlibxml2-dev \\t\\tlibxslt-dev \\t\\tlibyaml-dev \\t\\tmake \\t\\tpatch \\t\\tunzip \\t\\txz-utils \\t\\tzlib1g-dev \\t\\t\\t\\t$( \\t\\t\\tif apt-cache show 'default-libmysqlclient-dev' 2>/dev/null | grep -q '^Version:'; then \\t\\t\\t\\techo 'default-libmysqlclient-dev'; \\t\\t\\telse \\t\\t\\t\\techo 'libmysqlclient-dev'; \\t\\t\\tfi \\t\\t) \\t; \\trm -rf /var/lib/apt/lists/*\"\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"ENV PATH=/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n    \"comment\": \"buildkit.dockerfile.v0\",\n    \"empty_layer\": true\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"ENV LANG=C.UTF-8\",\n    \"comment\": \"buildkit.dockerfile.v0\",\n    \"empty_layer\": true\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"RUN /bin/sh -c set -eux; \\tapt-get update; \\tapt-get install -y --no-install-recommends \\t\\tlibbluetooth-dev \\t\\ttk-dev \\t\\tuuid-dev \\t; \\trm -rf /var/lib/apt/lists/* # buildkit\",\n    \"comment\": \"buildkit.dockerfile.v0\"\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"ENV GPG_KEY=7169605F62C751356D054A26A821E680E5FA6305\",\n    \"comment\": \"buildkit.dockerfile.v0\",\n    \"empty_layer\": true\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"ENV PYTHON_VERSION=3.12.0\",\n    \"comment\": \"buildkit.dockerfile.v0\",\n    \"empty_layer\": true\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"RUN /bin/sh -c set -eux; \\t\\twget -O python.tar.xz \\\"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz\\\"; \\twget -O python.tar.xz.asc \\\"https://www.python.org/ftp/python/${PYTHON_VERSION%%[a-z]*}/Python-$PYTHON_VERSION.tar.xz.asc\\\"; \\tGNUPGHOME=\\\"$(mktemp -d)\\\"; export GNUPGHOME; \\tgpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \\\"$GPG_KEY\\\"; \\tgpg --batch --verify python.tar.xz.asc python.tar.xz; \\tgpgconf --kill all; \\trm -rf \\\"$GNUPGHOME\\\" python.tar.xz.asc; \\tmkdir -p /usr/src/python; \\ttar --extract --directory /usr/src/python --strip-components=1 --file python.tar.xz; \\trm python.tar.xz; \\t\\tcd /usr/src/python; \\tgnuArch=\\\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\\\"; \\t./configure \\t\\t--build=\\\"$gnuArch\\\" \\t\\t--enable-loadable-sqlite-extensions \\t\\t--enable-optimizations \\t\\t--enable-option-checking=fatal \\t\\t--enable-shared \\t\\t--with-lto \\t\\t--with-system-expat \\t\\t--without-ensurepip \\t; \\tnproc=\\\"$(nproc)\\\"; \\tEXTRA_CFLAGS=\\\"$(dpkg-buildflags --get CFLAGS)\\\"; \\tLDFLAGS=\\\"$(dpkg-buildflags --get LDFLAGS)\\\"; \\tmake -j \\\"$nproc\\\" \\t\\t\\\"EXTRA_CFLAGS=${EXTRA_CFLAGS:-}\\\" \\t\\t\\\"LDFLAGS=${LDFLAGS:-}\\\" \\t\\t\\\"PROFILE_TASK=${PROFILE_TASK:-}\\\" \\t; \\trm python; \\tmake -j \\\"$nproc\\\" \\t\\t\\\"EXTRA_CFLAGS=${EXTRA_CFLAGS:-}\\\" \\t\\t\\\"LDFLAGS=${LDFLAGS:--Wl},-rpath='\\\\$\\\\$ORIGIN/../lib'\\\" \\t\\t\\\"PROFILE_TASK=${PROFILE_TASK:-}\\\" \\t\\tpython \\t; \\tmake install; \\t\\tbin=\\\"$(readlink -ve /usr/local/bin/python3)\\\"; \\tdir=\\\"$(dirname \\\"$bin\\\")\\\"; \\tmkdir -p \\\"/usr/share/gdb/auto-load/$dir\\\"; \\tcp -vL Tools/gdb/libpython.py \\\"/usr/share/gdb/auto-load/$bin-gdb.py\\\"; \\t\\tcd /; \\trm -rf /usr/src/python; \\t\\tfind /usr/local -depth \\t\\t\\\\( \\t\\t\\t\\\\( -type d -a \\\\( -name test -o -name tests -o -name idle_test \\\\) \\\\) \\t\\t\\t-o \\\\( -type f -a \\\\( -name '*.pyc' -o -name '*.pyo' -o -name 'libpython*.a' \\\\) \\\\) \\t\\t\\\\) -exec rm -rf '{}' + \\t; \\t\\tldconfig; \\t\\tpython3 --version # buildkit\",\n    \"comment\": \"buildkit.dockerfile.v0\"\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"RUN /bin/sh -c set -eux; \\tfor src in idle3 pydoc3 python3 python3-config; do \\t\\tdst=\\\"$(echo \\\"$src\\\" | tr -d 3)\\\"; \\t\\t[ -s \\\"/usr/local/bin/$src\\\" ]; \\t\\t[ ! -e \\\"/usr/local/bin/$dst\\\" ]; \\t\\tln -svT \\\"$src\\\" \\\"/usr/local/bin/$dst\\\"; \\tdone # buildkit\",\n    \"comment\": \"buildkit.dockerfile.v0\"\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"ENV PYTHON_PIP_VERSION=23.2.1\",\n    \"comment\": \"buildkit.dockerfile.v0\",\n    \"empty_layer\": true\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"ENV PYTHON_GET_PIP_URL=https://github.com/pypa/get-pip/raw/c6add47b0abf67511cdfb4734771cbab403af062/public/get-pip.py\",\n    \"comment\": \"buildkit.dockerfile.v0\",\n    \"empty_layer\": true\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"ENV PYTHON_GET_PIP_SHA256=22b849a10f86f5ddf7ce148ca2a31214504ee6c83ef626840fde6e5dcd809d11\",\n    \"comment\": \"buildkit.dockerfile.v0\",\n    \"empty_layer\": true\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"RUN /bin/sh -c set -eux; \\t\\twget -O get-pip.py \\\"$PYTHON_GET_PIP_URL\\\"; \\techo \\\"$PYTHON_GET_PIP_SHA256 *get-pip.py\\\" | sha256sum -c -; \\t\\texport PYTHONDONTWRITEBYTECODE=1; \\t\\tpython get-pip.py \\t\\t--disable-pip-version-check \\t\\t--no-cache-dir \\t\\t--no-compile \\t\\t\\\"pip==$PYTHON_PIP_VERSION\\\" \\t; \\trm -f get-pip.py; \\t\\tpip --version # buildkit\",\n    \"comment\": \"buildkit.dockerfile.v0\"\n  },\n  {\n    \"created\": \"2023-10-16T00:14:53Z\",\n    \"created_by\": \"CMD [\\\"python3\\\"]\",\n    \"comment\": \"buildkit.dockerfile.v0\",\n    \"empty_layer\": true\n  }\n]\nNote all the lines that say \"empty_layer\": true. Those indicate the step being run did not create a layer, and that's visible when the layers themself are listed:\n$ regctl manifest get python --platform local --format '{{jsonPretty .Layers}}'\n[\n  {\n    \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n    \"digest\": \"sha256:90e5e7d8b87a34877f61c2b86d053db1c4f440b9054cf49573e3be5d6a674a47\",\n    \"size\": 49582225\n  },\n  {\n    \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n    \"digest\": \"sha256:27e1a8ca91d35598fbae8dee7f1c211f0f93cec529f6804a60e9301c53a604d0\",\n    \"size\": 24049172\n  },\n  {\n    \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n    \"digest\": \"sha256:d3a767d1d12e57724b9f254794e359f3b04d4d5ad966006e5b5cda78cc382762\",\n    \"size\": 64130771\n  },\n  {\n    \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n    \"digest\": \"sha256:711be5dc50448ab08ccab0b44d65962f36574d341749ab30651b78ec0d4bfd1c\",\n    \"size\": 211066535\n  },\n  {\n    \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n    \"digest\": \"sha256:7ad48fee40035670dcaf937f0ac03b16dc5ac98f001dc04c2c84cf56af728d04\",\n    \"size\": 6391205\n  },\n  {\n    \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n    \"digest\": \"sha256:a319993f7bddcfa1ff27981884048de4356a15b667935ca420480c40030b48cd\",\n    \"size\": 22506963\n  },\n  {\n    \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n    \"digest\": \"sha256:c5bc2fe650d8f5c0670e0f23abba8fa89bd0e849855a2e60384a4069edc47df9\",\n    \"size\": 244\n  },\n  {\n    \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n    \"digest\": \"sha256:0303a8131ddc42ea2be42f92ce5c5e6deb404f51e7758728fc26deaf811665cb\",\n    \"size\": 2676803\n  }\n]\nThat shows only 8 filesystem layers while there were 17 steps in the history. Steps like ENV, CMD, and LABEL that don't create filesystem changes do not create a new layer.\nAnother way to see the same result is to compare the output of docker inspect $image --format '{{json .RootFS.Layers}}' to docker history $image and see the difference in lines (the values from regctl show the image on the registry and give visibility to fields like empty_layer).",
    "FileNotFound Exception when run on docker": "in linux path separator is / you can change you filePath to ./Data/pertonas.xlsx it will work on both linux and windwos. be aware paths are case sensitive in Linux",
    "Why does Windows Docker container not wait for exe to finish installing?": "The ENTRYPOINT instruction is used to specify a command that will be executed when the container starts. This means that the bin directory will not exist when you use workdir, since ENTRYPOINT [\"test.exe\", \"-f\", \"params.txt\"] was not executed yet.\nYou can see it as if the ENTRYPOINT instruction is the last line that will run in your script (that's probably the reason of why it's usually placed in the end of the Dockerfile.)\nA RUN instruction can be used to do what you want.",
    "Docker image size for ReactJS": "use the alpine version. the size only 38 MB (the \"standard\" is 332 MB) https://hub.docker.com/layers/node/library/node/current-alpine3.14/images/sha256-2829f9b05a85038bd04412c45e5929f0005c64f8ae444e65ba4e02635f904871?context=explore",
    "Not able to connect to dotnet core 3.1 after dockerizing": "STDout logs would be great to assist.\nYour port is not exposed in the Dockerfile.\nAssuming Kestrel is running on the default port of 5000.\nFROM mcr.microsoft.com/dotnet/aspnet:3.1-nanoserver-1809\n\nENV ASPNETCORE_ENVIRONMENT=Production\n\nWORKDIR /app\nEXPOSE 5000\n\n# copy publish folder contents to web root\nCOPY ./bin/Release/netcoreapp3.1/publish .\n\nENTRYPOINT [\"dotnet\", \"app.WebApo.dll\"]\nTo run the container on 8080 for example:\nFROM mcr.microsoft.com/dotnet/aspnet:3.1-nanoserver-1809\n\nENV ASPNETCORE_ENVIRONMENT=Production\n\nWORKDIR /app\nEXPOSE 8080\n\n# copy publish folder contents to web root\nCOPY ./bin/Release/netcoreapp3.1/publish .\n\nENV ASPNETCORE_URLS=\"http://+:8080\"\n\nENTRYPOINT [\"dotnet\", \"app.WebApo.dll\"]",
    "Dockerfile declare environment variables from file [duplicate]": "I've found a way to do this:\nCMD export $(xargs < my_file.txt)\nOfc this may be dangerous workaround, but it works!",
    "Docker - Error: adduser: The UID 0 is already in use": "I try to guess you (missing) docker file:\nFROM some_image\n\n# ...\n\nRUN adduser -u ${UID} --disabled-password --gecos \"\" appuser\n\n# ...\nThe docker build, unless specified by the image your inheriting from, runs under the root account, then the UID value is 0. So you cannot create another user with the same ID.\nAre you trying to create a root user? If the answer is yes, you should create a new user, then give it root grants.\nFROM some_image\n\n# ... \n\nRUN adduser appuser\nRUN usermod -aG sudo appuser\n\n# ...\nPlease tell me if I missed something.",
    "Docker healthcheck file incorrectly not found": "Please add sh to your HEALTHCHECK\nHEALTHCHECK --interval=600s --timeout=5s CMD sh /root/healthcheck.sh",
    "How can I keep all artifacts when building multi-stage Dockerfiles with docker-compose?": "It looks to me like you need to have separate docker-compose services, one for each of your images. You say there are 4 separate Python projects involved, so it sounds like there should be (at least) 4 services defined in your docker-compose.yaml.\nSo rather than defining all your services in one Dockerfile, I would expect each service to have its own Dockerfile, and to be defined separately in your docker-compose.yaml.\nversion: \"3.7\"\n\nservices:\n\n  e2e:\n    - build ./e2e\n    ...\n  production:\n    - build ./production\n    ...\nYou say all your services have to communicate with one another. Docker Compose makes this very easy as, for example, the production image can access the e2e image just using the URI e2e. (e.g. curl e2e).",
    "Automatically remove FROM image used in multi stage Dockerfile": "I couldn't find a supported approach, so here's my workaround.\nMy build script:\n#!/bin/bash\n\n# build image\n# ...\n\n# get environment variables from .env\nset -o allexport\n. .env\nset +o allexport\n\n# get images to remove from Dockerfile\nIMAGES=($( \\\n  cat Dockerfile | \\\n  grep '^FROM [[:alpha:]]' |\n  cut -d' ' -f2 |\n  xargs -i bash -c \"echo {}\"\n))\n\n# remove images\necho \"${IMAGES[@]}\" | \\\n  xargs -r -n1 docker image rm || true",
    "Dockerfile Run Python Pipenv Build Fails": "Solution! After a lot of troubleshooting I solved the problem by purging my docker containers, the real solution is after a change in your docker files, you need to run docker-compose down before running docker-compose up. I assumed when shutting down containers this process was involved and didnt know the docker-compose down command.",
    "how to execute sql script after oracle docker container startup?": "From what I see I suspect that you just need to add @path_to_sql_script into your ./docker/oracle/oracle-scripts.sql.\nBut I'd suggest to use official Oracle images. In this case you could just mount your directory with sql scripts into as /opt/oracle/scripts/startup and configurations scripts as /opt/oracle/scripts/setup.",
    "Docker image creation using nodejs with redis": "One cannot use docker build - < Dockerfile to build an image that uses COPY instructions, because those instructions require those files to be present in the build context.\nOne must use docker build ., where . is the relative path to the build context.\nUsing docker build - < Dockerfile effectively means that the only thing in the build context is the Dockerfile. The files that one wants to copy into the docker image are not known to docker, because they are not included in the context.",
    "Blazor css is not applied in Docker": "I found. Its me.\nI write\n<link href=\"_content/FanLib/FanLib.css\" rel=\"stylesheet\" />\nbut it's :\n<link href=\"_content/FanLib/fanLib.css\" rel=\"stylesheet\" />\nit's fanLib.css not FanLib.css ...",
    "How to create a container with containerd instead of pulling it": "This is possible with BuildKit: https://github.com/moby/buildkit#containerd-image-store",
    ".dockerignore file located in subdirectory": "You can not include by reference the .dockerignore file form the submodules.\nThe .dockerignore is usually related to a specific development tools and methods, so there is a good chance that the .dockerignore will already cover some of the major common files in the submodule as it covers in the main repository.\nIn case where there are discrepancies, you can add the content from the submodule .dockerignore to your main .dockerignore, corrected for the prefixes that are relevant to the submodule.",
    "Docker run - format issue - docker: invalid reference format": "I encountered a similar issue when attempting to start a Kafka container locally on Windows. The error stemmed from the \\ character not being interpreted as a next line character. I found a solution by using ^ instead.\ndocker run -p 9092:9092 ^\n    -e KAFKA_ZOOKEEPER_CONNECT=<PRIVATE_IP>:2181 ^\n    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://<PRIVATE_IP>:9092 ^\n    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 ^\n    confluentinc/cp-kafka",
    "Permission denied executing jar file downloaded to .Ivy folder in Kubernetes": "It would be easier to answer your question if you share your Dockerfile and your kubernetes Pod template yaml manifest, but in short: you can manipulate your Pod's permissions using the securityContext like in the example from the docs below:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n  volumes:\n  - name: sec-ctx-vol\n    emptyDir: {}\n  containers:\n  - name: sec-ctx-demo\n    image: busybox\n    command: [ \"sh\", \"-c\", \"sleep 1h\" ]\n    volumeMounts:\n    - name: sec-ctx-vol\n      mountPath: /data/demo\n    securityContext:\n      allowPrivilegeEscalation: false\nFor debugging purposes you can start from setting allowPrivilegeEscalation to true or runAsUser: 0 (root) but keep in mind this is not the solution that can be used in production. Running containers as root is generally a bad idea and in most cases it can be avoided.\nTherefore any permissions given to the spark folder are not present for this newly dowloaded jar file which is downloaded at runtime into /opt/spark/.ivy2/xxx which has root permissions.\nIs it really necessary for it to have root permissions ? Most likely it can be fixed in your Dockerfile.",
    "Docker status Exited (2)": "Ok I found the answer :\nReplacing this - ./web/:/usr/src/web/ by - .:/usr/src/web/",
    "Build and Run Docker Image of Quarkus": "As described by @RobertvanderSpek, the microservices are not able to talk to the keycloak and the postgres container anymore. To allow the communication in between the services as before, the easiest workaround is to connect the microservice containers to the network host upon startup because containers in this network are not isolated from the host OS.\nThe following command represents an example command for running a container in the provided situation.\ndocker run --network host <user>/<artifact>\nTo see the whole microservice-based application system, see the GitHub repo. Detailed explanations on how to Dockerise a microservice can be found in the subfolders of each microservice.",
    "ASP.NET Core build docker image having shared libraries": "Ok first thing first rename the dockerfile to Dockerfile. Next up is the path correct in the first place? I assume your csproj exists in the same folder as your Dockerfile and in your csproj you use    <ProjectReference Include=\"..\\..\\..\\Shared1.csproj\" /> whereas in the docker file you use ../../../Users/boris/Desktop/shared/Shared/Shared/Shared.csproj so if the csproj AND the Dockerfile exist in the same directory the path is simply incorrect.",
    "How to create custom image in docker using VSCode?": "I see that you are trying to build a redis-server, the image of redis is already available on docker hub - have you tried that out?\nhttps://hub.docker.com/_/redis\nCheck this link - the same issue is discussed and resolved.\nIt seems on windows - if you try\ndocker build -f <name of your dockerfile> .\nit should work fine.",
    "Docker (Linux) Error: no space left on device even if I have enough space [closed]": "Question1: why does it stop even when I still have 9GB avilable on that partition?\nYou didn't when the error occurred. The error resulted in docker deleting the partial filesystem layer it had created. Note with overlay filesystems, a recursive chmod will copy every file in that directory tree, doubling the used disk space.\nNote that you should also be watching for inode exhaustion when you get this write (df -i).\nQuestion2: It seems that docker doesnt take space from the other partitions. On dev/sdc for example I have 718GB available. How to tell docker to take space from there?\nDocker stores data in /var/lib/docker. I'd recommend making that it's own partition, or relocating it using a symlink rather than trying to change the location docker looks. There are lots of tools out there that assume this directory name.\nQuestion3: It seems that docker write files into this folder /data/home/oracle/database/... But I go to that folder with winscp I can find only the directory /data. Is the folder /data/home/oracle/database/ in container somehow or its really in linux file system?\nDocker uses namespaces, and one of those namespaces in the filesystem. The root directory in a docker container is not the root directory on the host, otherwise you'd have no isolation. This is typically implemented as an overlay filesystem under the docker directory, and each step of the Dockerfile may create a new filesystem layer used by overlay.",
    "Containers not restarted after update with restart always in docker-compose.yml": "From the comments it appears the docker service was not configured to automatically start on boot. Docker is a client server app, and the server runs from systemd with a separate service for the docker socket used by the client to talk to the server. Therefore it's possible for any call with the docker command to cause the server to get launched by hitting the docker socket.\nThe service state in systemd can be checked with:\nsystemctl status docker\nor you may want to check:\nsystemctl is-enabled docker\nIt can be manually started with:\nsystemctl start docker\nAnd it can be enabled to start with:\nsystemctl enable docker\nAll of the above commands need to be run as root.",
    "GeoServer is not recognizing the copied gwc-layers files": "The default tile formats are stored in the gwc-gs.xml file (in the top level data dir) so you'll need to copy that across too.",
    "Copy a file from local to docker container via a shell script": "Sounds like what you want is a mounting the file into a location of your docker container.\nYou can mount a local directory into your container and access it from the inside:\nmkdir /some/dirname\ncopy filet.txt /some/dirname/\n\n# run as demon, mount /some/dirname to  /directory/in/container, run sh\ndocker run -d -v /some/dirname:/directory/in/container postgres:1.0 sh\nMinimal working example:\nOn windows host:\nd:\\>mkdir d:\\temp\nd:\\>mkdir d:\\temp\\docker\nd:\\>mkdir d:\\temp\\docker\\dir\nd:\\>echo \"SomeDataInFile\" > d:\\temp\\docker\\dir\\file.txt\n\n# mount one local file to root in docker container, renaming it in the process\nd:\\>docker run -it -v d:\\temp\\docker\\dir\\file.txt:/other_file.txt  alpine\nIn docker container:\n/ # ls\nbin       etc       lib      mnt     other_file.txt  root    sbin      sys       usr\ndev       home      media    opt     proc            run     srv       tmp       var\n\n/ # cat other_file.txt\n\"SomeDataInFile\"\n/ # echo 32 >> other_file.txt\n/ # cat other_file.txt\n\"SomeDataInFile\"\n32\n/ # exit\nthis will mount the (outside) directory/file as folder/file inside your container. If you specify a directory/file inside your docker that already exists it will be shadowed.\nBack on windows host:\nd:\\>more d:\\temp\\docker\\dir\\file.txt\n\"SomeDataInFile\"\n32\nSee f.e Docker volumes vs mount bind - Use cases on Serverfault for more info about ways to bind mount or use volumes.",
    "How to migrate war to layered jar in dockerfile with spring-boot-maven-plugin": "Try adding this dependency to the module:\n   <groupId>org.springframework</groupId>\n   <artifactId>spring-core</artifactId>\nThis jar contains the org.springframework.util.ClassUtils and will resolve the error.",
    "How to using env variables in Dockerfile?": "You need to add as ARGs under a different place from the ENV variables. You can use them in the compose, just a little differently.\nARGs will be used during the build only, if you need them to available for the build & again while running the container for some reason you will have to set both an ARG as well as an ENV variable. {{{During the Build stage they will be called in the exact same manner}}}. ENV are set to replace defaults when running a container, so the build's ENV   TZ=Europe/London will be replaced by the Compose's Environment: TZ=America/New_York\nservices:\n  my_node_version:\n    image: my/node\n    build:\n      context: .\n      dockerfile: ../node.Dockerfile\n      args:\n        - VERSION=12\n        - NEEDED_IN_BUILD_AND_RUN=SomeStrangeUseCase\n    environment:\n      - ONLY_USED_IN_IMAGE=MyImageWorks\n      - NEEDED_IN_BUILD_AND_RUN=SomeStrangeUseCase\n      - OVERRIDE_BUILD_ENV=\"Will replace the ENV from the Build\"\nI'm pretty sure that fulfills all the intent from the question (you asked to not use args I assume because you wanted them set in the docker-compose.yaml rather than the dockerfile). You can do this, you just have to set them in a different way.\nENV cannot be used in the build from the Compose (ENV variables set in the Dockerfile just set the defaults that the container will use if none are declared in the OS, the Compose, or anywhere Docker gets them from), so that simply isn't a possibility, so if you need it to be ENV specifically for some reason then the answer is simply \"You can't\" end of discussion. But I don't see anything that could possibly accomplish, & no reason using ARGs wouldn't accomplish exactly what you want. You just need to place them in a different section, but they should function as needed",
    "How to enable xvfb for an express server running inside a docker container?": "I think you are missing the server number as an argument. I am using xvfb-run --auto-servernum <command> to automatically get a free one. The default is number 99.\nYou can also use Xvfb manually with the DISPLAY environment variable set to :99 (default server number), but xvfb-run makes live a lot easier. I think you already read this, but for future users I will quote the stackgl/headless-gl README:\nInteracting with Xvfb requires you to start it on the background and to execute your node program with the DISPLAY environment variable set to whatever was configured when running Xvfb (the default being :99). If you want to do that reliably you'll have to start Xvfb from an init.d script at boot time, which is extra configuration burden. Fortunately there is a wrapper script shipped with Xvfb known as xvfb-run which can start Xvfb on the fly, execute your Node.js program and finally shut Xvfb down.",
    "How to copy a jar-file from one docker stage to another with docker onbuild copy?": "From the ONBUILD reference i understand that the ONBUILD commands are inserted directly after the FROM command when inheriting from it.\nAs i understand it this means your app.jar needs to exist when the FROM command is issued, i.e. your effective Dockerfile would look like\nFROM <repo>/<first_docker_file>:1.0\nCOPY app.jar /app.jar\nCOPY --from=build /home/gradle/src/build/libs/*.jar /app.jar\n[...]\nwhich obviously wouldn't work.\nIt seems like your first dockerfile is not meant to be used in a staged build, you need to get the app.jar in the working directory when issuing the build on the first dockerfile, i.e. by compiling inside a container and copying out your app.jar to your first dockerfile location and then build it.",
    "nginx: [emerg] host not found in upstream \"api:5000\" in /etc/nginx/conf.d/default.conf": "please check your docker-compose file, you must set api under services",
    "How to connect to other container from Dockerfile while docker-compose build": "Both php and postgres need to be on same network and php can access postgres using container_name which is postgres. depends_on will make sure postgres get starts before php.\nversion: '3.0'\nservices:\n  php:\n    build: \n      context: .\n      dockerfile: php/Dockerfile\n    restart: on-failure\n    depends_on:\n      - postgres\n    networks:\n      - test-network\n\n  postgres:\n    container_name: 'postgres'\n    image: \"postgres:13-alpine\"\n    restart: always\n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB_NAME}\n    networks:\n      - test-network\n\nnetworks:\n  test-network:\n    driver: bridge",
    "Build a new docker image from two base images (centos and python)": "TL;DR You generally can't have on an image all there is in two images(The reason is explained below)\nTL;DR List of solutions:\nUse a base image that already is CentOS and has a Python installed, maybe this image.\nStart from CentOS image, install Python using package manager in the Dockerfile.\nStart from CentOS image, build Python in Dockerfile.\nJust use Python in the provided distributions.\nLong Answer:\nUsing FROM imagename is equivalent of downloading a pre-built image and then adding on it the instructions in the current Dockerfile. An image is basically a snapshot of the filesystem.\nOne of them is the snapshot of a container with probably a Linux distribution in place and Python installed.\nThe other is snapshot of the filesystem of another Linux distribution with all its stuff in place. You just can't mix them, because the image isn't just Python. It is all Python needs to run beside a working Linux kernel and mixing them together almost certainly breaks them. because they have conflicting files and ways of running them in place.\nBut you can change one snapshot to make another snapshot off of it, for example you can take image of CentOS and install Python using the package manager, can then take a snapshot off of it and use it as working Docker container.",
    "gradle jar file with yml config file doesn't execute from dockerfile": "I cannot post a comment as I am a newbie.\nI see that you are in build/libs when you run-\nRUN cd build/libs\nThen your ENTRYPOINT command is using relative path for config location-\n\"-Dconfig.location=app/build/libs/config.yml\"\nYou can try starting the location with \"/app\" instead of \"app\".\nAlso, if that doesn't help, can you paste the exact errors you are getting in the 2 scenarios you mentioned?",
    "Make rails and sidekiq work together from different Docker containers (but can't use docker-compose)": "Method 1\nAccording to CapRover docs, it seems that it is possible to run Docker Compose on CapRover, but I haven't tried it myself (yet).\nMethod 2\nAlthough this CapRover example is for a different web app, the Internal Access principle is the same:\nYou can simply add a srv-captain-- prefix to the name of the container if you want to access it from another container.\nHowever, isn't this method how you told your Rails web app where to find the PostgreSQL DB container? Or are you accessing it through an external subdomain name?",
    "What is chrome path inside docker ubuntu image": "wget is a download tool. It is always useful to know what the command is before you run it :). In this case, you are downloading chrome, unpacking the file, and then installing chrome. Unless you specify the path to downloads, get downloads to the current dir. Using docker or not using docker doesn't mean much too.\nUPDATE: The directory to install chrome to is the /usr/bin/ dir. You will need sudo. So your full command would be sudo wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -P /usr/bin/ && \\ dpkg --unpack google-chrome-stable_current_amd64.deb && \\ apt-get install -f -y, or you could first do cd /usr/bin/, and then sudo wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb && \\ dpkg --unpack google-chrome-stable_current_amd64.deb && \\ apt-get install -f -y. You might need to modify the command as needed. Hope this helps:)",
    "Docker does not sync folders from host to container": "If you want to keep your folder sync between host and container, then you'll need to mount it instead of copying it: check this documentation\nFor your problem, you need to remove COPY . /var/www/html/ from Dockerfile\nYou could also check this post.",
    "How to create Angular Universal Dockerfile and docker-compose.yml file?": "May be a little late but in your Dockerfile:\nRUN npm run build:universal\nYou need to have a script in your package.json like this:\n\"build:universal\": \"ng build --prod && ng run jobnow:server:production\"\nThere is also this line:\nRUN npm run test:ssr\nYou can remove it or add another script like this:\n\"test:ssr\": \"run-p test:ssr:*\"",
    "Postgres Initialize script not working Docker version 3.4": "From the postgres docker image documentation you can see that the POSTGRES_USER and POSTGRES_PASSWORD are the necessary environment variables to setup the postgres container\nYou could add these environment variables to your .env file. So the file will be as follow:\n.env\nDATABASE_NAME=tools_development\nDATABASE_USER=user1\nDATABASE_PASSWORD=password\nDATABASE_HOST=database\n\nPOSTGRES_USER=user1\nPOSTGRES_PASSWORD=password\nPOSTGRES_DB=tools_development\nThese environment variables will be used from the postgres container to init the DB and assign the user, so you can get rid of the init.sql file\nAfter that you need to add the reference of the .env file in the database(postgres:10.12) service.\nSo your docker compose file should be as follow:\ndocker-compose.yml\n...\n  database:\n    image: postgres:10.12\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    env_file: .env \n...",
    "Use a docker image to build a different docker image using a dockerfile": "@DavidMaze is correct in that docker-compose is most likely the cleanest way to run multiple docker containers side-by-side on your host. Once you become accustomed to its declaration, it actually serves as a great way to document local/prototypical setups.\nHave a look here at the reference docker-compose provided for the datajoint/mysql image. Specifically in a setup as you shared where you are looking to dockerize datajoint-python, if your Dockerfile is located in the same directory as your docker-compose.yml, you can achieve it simply like this:\nversion: '2.4'\nservices:\n  db:\n    image: datajoint/mysql:5.7\n    environment:\n    - MYSQL_ROOT_PASSWORD=simple\n    networks:\n    - main\n    #ports:\n    #  - \"3306:3306\"\n    #volumes:\n    #  - ./data:/var/lib/mysql\n  dj:\n    build: .\n    depends_on:\n      db:\n        condition: service_healthy\n    environment:\n    - DJ_HOST=db\n    - DJ_USER=root\n    - DJ_PASS=simple\n    networks:\n    - main\nnetworks:\n  main:\nNote: The only reason I've intentionally utilized a 2.X docker-compose version is that 3.X versions are meant for Docker Swarm deployments where granular checks like these are unneeded.",
    "Docker build fails when project reference and nuget feed added": "Docker build needs to be able to see the whole project when it builds, so it can determine if any of the files have changed (among other things).\nYou probably need to run docker build from the root directory of the project, and tell it where the Dockerfile is, like this:\ndocker build -t the_name_of_the_image -f Account/Account.Host/Dockerfile\nThe above might not be quite right, but the directory structure you posted didn't render right :(",
    "Multiple Dockerfiles re-using shared library in project": "NOTE: I suppose your target is to have on the specific server container both the compiled go file ( from specific main.go file ) and the compiled protocol buffer file ( from shared sharedproto.proto file ).\nAssuming your files are organized as follow on your workstation:\nserverfoo/\n   Dockerfile\n   main.go\n\nserverbar/\n   Dockerfile\n   main.go\n\nproto/\n   Dockerfile\n   sharedproto.proto\nYou can structure the specific server Dockerfile using the multistage build as follow ( e.g. serverbar Dockerfile ):\n#####\n# The serverbar Dockerfile\n#####\n\n#----\n# Compile proto stage\n#----\nFROM moul/protoc-gen-gotemplate AS protostage\nWORKDIR /workspace\n# Copy .proto file\nCOPY proto/sharedproto.proto .\n# Compile .pb.go\nRUN protoc -I=. --go_out=. sharedproto.proto\n\n#----\n# Build stage\n#----\nFROM golang:1.12.4-alpine3.9 as buildstage\nWORKDIR /workspace\nCOPY serverbar/main.go .\nRUN GOOS=linux GOARCH=amd64 go build -o serverbar main.go\n\n#----\n# Final stage\n#----\nFROM alpine:3.7\nWORKDIR /home\nCOPY --from=buildstage workspace/serverbar .\nCOPY --from=protostage workspace/sharedproto.pb.go .\nCMD [\"./serverbar\"]\nUsing this approach you basically have the following 3 stages:\nproto stage: On the container created on this stage you need to compile the shared protocol buffer source file into the sharedproto.pb.go that then will be included on the third final stage. So here you would need to install on the container the protoc compiler and the related Go plugin. However, as usual with Docker, you'll find a docker image that already includes your needed tools. For this purpose we can start from the moul/protoc-gen-gotemplate docker image. Specifically the follow Dockerfile instruction generates the workspace/sharedproto.pb.go:\n  RUN protoc -I=. --go_out=. sharedproto.proto\nbuild stage: Here you need to compile the server source file into the executable one. Also this will be included on the third final stage. To avoid to install Golang we can start from the golang:1.12.4-alpine3.9 docker image that already includes all the needed tools. Specifically the follow Dockerfile instruction generates the workspace/serverbar executable:\n  RUN GOOS=linux GOARCH=amd64 go build -o serverbar main.go\nfinal stage: This is the server container that we'll then upload on our Docker registry for test or production where we'll copy the files compiled on the previous two stage with the following commands:\n  COPY --from=buildstage workspace/serverbar .\n  COPY --from=protostage workspace/sharedproto.pb.go .\nOne of the advantages of this solution is that, for each server build, you can cache the compiled protobufs until the underlying protos are modified.\nExample: Building first time the serverbar container we can note that .proto compilation is performed on a new container with id 92ae211bd27d:\n> docker build -f serverbar/Dockerfile .\nSending build context to Docker daemon  10.24kB\nStep 1/13 : FROM moul/protoc-gen-gotemplate AS protostage\n ---> 635345fde953\nStep 2/13 : WORKDIR /workspace\n ---> Using cache\n ---> de8890a5e775\nStep 3/13 : COPY proto/sharedproto.proto .\n ---> 1253fa0576aa\nStep 4/13 : RUN protoc -I=. --go_out=. sharedproto.proto\n ---> Running in 8426f5810b98 \nRemoving intermediate container 8426f5810b98\n ---> 92ae211bd27d <=========================================\nStep 5/13 : FROM golang:1.12.4-alpine3.9 as buildstage\n ---> b97a72b8e97d\nStep 6/13 : WORKDIR /workspace\n\n....\nBuilding then a second time without modifying the sharedproto.proto we can note that container with id 92ae211bd27d is re-used from cache.\n> docker build -f serverbar/Dockerfile .\nSending build context to Docker daemon  10.24kB\nStep 1/13 : FROM moul/protoc-gen-gotemplate AS protostage\n ---> 635345fde953\nStep 2/13 : WORKDIR /workspace\n ---> Using cache\n ---> de8890a5e775\nStep 3/13 : COPY proto/sharedproto.proto .\n ---> Using cache\n ---> 1253fa0576aa\nStep 4/13 : RUN protoc -I=. --go_out=. sharedproto.proto\n ---> Using cache <=========================================\n ---> 92ae211bd27d\nStep 5/13 : FROM golang:1.12.4-alpine3.9 as buildstage\n ---> b97a72b8e97d\n\n....",
    "Docker mounting rootfs caused not a directory error": "src should be a directory containing your web content, not a file.",
    "Pre-loading data into a SQL Server docker image on start up": "I think the problem is that, the way it is currently written, the restore.sh script is completing successfully, and then the container terminates (which is what is supposed to happen). Docker containers only live as long as that command continues to run. I would rearrange it like this:\nRUN /data/restore.sh\n[CMD \"<DB command>\"]\nThe data will get populated in a separate image layer that will be included in the final image that will be used to run the container. But the command that runs, <db command>, needs to start the DBMS and then not return, because once that command returns the container is going to stop.\nI tried running with a copy/paste of your docker-compose.yml file, and this Dockerfile:\nFROM mcr.microsoft.com/mssql/server:2019-latest\nCMD [\"/opt/mssql/bin/sqlservr\", \"--accept-eula\"]\nThat worked for me.\nAs an alternative, you could keep restore.sh as CMD, but then add /opt/mssql/bin/sqlservr --accept-eula at the end of that script. That will start the database after the restore completes and prevent the container from exiting.",
    "Trying to set up PIA with OVPN client (docker)": "As I can see in your logs you've received Inactivity timeout (--ping-restart), restarting message after successfull connection in short period of time.\nI had the same issue. My client successfully connected and in few seconds (20-40) has been restarted. In my case I've actually run two clients with the same client name (CN) on different hosts.\nTo fix it I've generated different clients for each host.",
    "Prevent collectstatic from trying to access a database": "Unfortunately there is no way to prevent collectstatic from trying to access the database. Using a dummy database in certain situations could solve your problem. If this doesn't help you will probably have to implement it yourself.",
    "Docker build: runnable is in parent folder [duplicate]": "Try building docker images from the parent directory with \"-f\" flag:\n docker build -t <your_tag> -f dirA/dirB/Dockerfile .",
    "How to correctly format a nested bash command in Dockerfile?": "Your container does not have bash installed, use sh -c instead to run your command. Nonetheless, most looks like the container already runs /bin/sh for your commands. Which means running echo SHLVL: \\$SHLVL is enough.",
    "Can I create a docker volume where files and folders cannot be overriden or deleted?": "Along the lines of @masseyb's comment, I'd perform any file checks at image build time and write a small file (maybe even the shell script to execute) inside the image that tells me if the model files are writeable or not. Then the script you call in your ENTRYPOINT can use that file to execute the appropriate copy logic.\nIt's also possible for your compose shell script to perform host-level checks by running the container and looking for the file your build step wrote within it.\nHope this helps!",
    "For existing ASP.NET Core MVC project on Windows build server, how to target a Docker container without disturbing earlier build pipeline stages?": "Fixed it by two changes:\n1) Change the DLL name in the Dockerfile to be case sensitive. The app runs inside a container whose OS is case sensitive Linux.\n2) Change the port number mapping in the \"docker run\" commmand\nTroubleshooting steps:\n1. In a Visual Studio 2019 command prompt window, go to the publish directory\n2. Run the web site \"dotnet AspNetMvcCoreTestApp2.dll\". This command is the same one run inside the Docker container.\nIt produces this output. Notice the port number 5000\ninfo: Microsoft.Hosting.Lifetime[0]\nNow listening on: http://localhost:5000\ninfo: Microsoft.Hosting.Lifetime[0]\nNow listening on: https://localhost:5001\ninfo: Microsoft.Hosting.Lifetime[0]\nApplication started. Press Ctrl+C to shut down.\n3. Open a browser and go to http://localhost:5000/ and the start page of the site is displayed\n4. Change the Dockerfile to list the correct DLL name including case sensitive letters. Also verify thta the .NET core version in the Dockerfile is the same as the one in the Visual Stuido 2019 explorer.\n# Build runtime image\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.0\nWORKDIR /app\nCOPY . .\nENTRYPOINT [\"dotnet\", \"AspNetCoreTestProject2.dll\"]\nEXPOSE 80\nRemove any images for the container in Docker if needed\nRun \"docker image ls\" and make note of the IMAGE_ID of the existing image if it exists\nRun \"docker rmi IMAGE_ID\" to remove the image\nBuild the docker image \"docker build -t aspnetcoretestproject2 .\"\nEnter \"docker image ls\" and make note of the IMAGE_ID value for aspnetcoretestproject2\n    REPOSITORY                             TAG                 IMAGE ID            CREATED             SIZE\n    aspnetcoretestproject2                 latest              58c92979be61        3 minutes ago       212MB\n    mcr.microsoft.com/dotnet/core/sdk      3.0-buster          4422e7fb740c        3 weeks ago         689MB\nRun the docker image mapping local host port 5000 to port 80 inside the Docker container\n\"docker run -d -p 5000:80 58c92979be61\"\nVerify the docker container is running by \"docker container ls\"\n    CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES\n    c52ab21b9c25        58c92979be61        \"dotnet AspNetCoreTe\u2026\"   4 seconds ago       Up 2 seconds        0.0.0.0:5000->80/tcp   magical_leavitt\n    e4152fcb2233        f80194ae2e0c        \"ping 8.8.8.8\"           3 minutes ago       Up 3 minutes                               k8s_testpod_demo_default_dd2702f7-eeac-11e9-a181-00155dde9201_2\nOpen web browser and enter http://localhost:5000 in the URL bar. This should show the start page of the ASP.NET web site",
    "Access Angular application in a Docker container": "I recently worked on an Angular project, which deploys it on Docker container. This is an example of Dockerfile.\nDocker command:\ndocker build -t your-app-name .\ndocker run -itd -p 4200:80 --name=your-app-name your-app-name\nIn Dockerfile:\n### STAGE 1: Build ###\n\nFROM node:10-alpine as builder\n\nCOPY package.json package-lock.json ./\n\nRUN npm set progress=false && npm config set depth 0 && npm cache clean --force\n\n## Storing node modules on a separate layer will prevent unnecessary npm installs at each build\nRUN npm i && mkdir /ng-app && cp -R ./node_modules ./ng-app\n\nWORKDIR /ng-app\n\nCOPY . .\n\n## Build the angular app in production mode and store the artifacts in dist folder\nRUN $(npm bin)/ng build --prod --build-optimizer\n\n### STAGE 2: Setup ###\n\nFROM nginx:1.17.0-alpine\n\n## Copy our default nginx config\nCOPY nginx/default.conf /etc/nginx/conf.d/\n\n## Remove default nginx website\nRUN rm -rf /usr/share/nginx/html/*\n\n## From 'builder' stage copy over the artifacts in dist folder to default nginx public folder\nCOPY --from=builder /ng-app/dist/your-app-name /usr/share/nginx/html\n\nCMD [\"nginx\", \"-g\", \"daemon off;\"]",
    "Docker Unable to find file": "Docker CMD is only designed to run a single process, following the docker philosophy of one process per container. Try using a start script to modify your template and then launch clair.\nFROM quay.io/coreos/clair-git\nCOPY config.template /config/config.template\nCOPY start.sh /start.sh\n\n#Set Defaults\nENV USER=clair PASSWORD=johnnybegood INSTANCE_NAME=postgres PORT=5432\n\nRUN apk add gettext\nENTRYPOINT [\"/start.sh\"]\nand have a startscript (with executable permissions) copied into the container using your Dockerfile\n!#/bin/sh\n\nenvsubst </config/config.template > /config/config.yaml\n/clair -config=/config/config.yaml\nedit: changed the answer after a comment from David maze",
    "docker container: can't access dotnet web api container from another container": "Try 'http://host.docker.internal:5000/api/'\nLocalhost would be the loopback of the container but you want the host PC's port 5000",
    "Trying to bind volume to a docker container": "you are overwriting all you data in /srv/app that you add during the build process . you may change your mount to use other target as /srv/app.\nUpdate\nstart your container using:\ndocker run -v /ful/path/folder:/srv/app/data IMAGE",
    "docker-maven-plugin not creating dockerfile": "You may need to run \"maven package docker:build\" rather than just \"maven package\". In order to bind the docker build into a maven phase, you could also try something like this:\n<execution>\n<id>build-image</id>\n<phase>package</phase>\n<goals>\n<goal>build</goal>\n</goals>\n</execution>\nFrom https://github.com/spotify/docker-maven-plugin#bind-docker-commands-to-maven-phases",
    "How to create Docker arguments during the building process of a Docker image": "You set the value of ARG when you run docker build, e.g. docker build --build-arg SOME_ARG=1.2.3.\nHowever, since your goal is labeling, you don't even need build args. Instead, just set the labels on the image using the appropriate docker build option:\n$ docker build --label IMAGE_A_VERSION=1.2.3 -t yourimage .",
    "how can I mount local folder with dockerfile RUN --mount?": "basically, I want to preserve the advantage that make only compiles out of date targets which rules out using COPY . /home because this appears not too preserve the timestamps on the files copied across.\nI don't believe this is accurate. First, an example Dockerfile:\nFROM busybox\nCOPY . /build-context\nWORKDIR /build-context\nCMD find .\nI build that into an image called context:\n$ docker build -f df.build-context -t context .\n[+] Building 7.4s (8/8) FINISHED\n => [internal] load build definition from df.build-context         1.2s\n => => transferring dockerfile: 118B                               0.0s\n => [internal] load .dockerignore                                  0.7s\n => => transferring context: 34B                                   0.0s\n => [internal] load metadata for docker.io/library/busybox:latest  0.3s\n => CACHED [1/3] FROM docker.io/library/busybox                    0.0s\n => [internal] load build context                                  1.1s\n => => transferring context: 2.56kB                                0.4s\n => [2/3] COPY . /build-context                                    0.9s\n => [3/3] WORKDIR /build-context                                   0.6s\n => exporting to image                                             1.6s\n => => exporting layers                                            1.3s\n => => writing image sha256:c6971f5f817b746afd785c77d3...bcfb58f1  0.2s\n => => naming to docker.io/library/context                         0.1s\nAnd if I look at files on the host and compare them to files that got included in the context, I'm seeing the last modified timestamp is identical (the only difference in display is from the difference in UTC and EDT timezones):\n$ docker run -it --rm context stat hello.sh\n  File: hello.sh\n  Size: 29              Blocks: 8          IO Block: 4096   regular file\nDevice: fe03h/65027d    Inode: 24910346    Links: 1\nAccess: (0755/-rwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2019-06-28 23:58:34.000000000\nModify: 2016-12-06 18:17:43.000000000\nChange: 2019-06-28 23:58:32.000000000\n\n$ stat hello.sh\n  File: hello.sh\n  Size: 29              Blocks: 8          IO Block: 4096   regular file\nDevice: fe03h/65027d    Inode: 16526503    Links: 1\nAccess: (0755/-rwxr-xr-x)  Uid: ( 1000/  bmitch)   Gid: ( 1000/  bmitch)\nAccess: 2019-06-28 19:58:30.984110011 -0400\nModify: 2016-12-06 13:17:43.937302516 -0500\nChange: 2017-06-21 17:17:15.052283710 -0400\n Birth: -\nFrom the man page on make:\nThe make program uses the makefile description and the last-modification times of the files to decide which of the files need to be updated.\nYou should check elsewhere for something changing your timestamps because if they are changing outside of docker, the RUN --mount will still include the broken last-modified timestamp.",
    "docker-composer node js give error when running the docker composer . mysql ER_NOT_SUPPORTED_AUTH_MODE": "When you use:\nFROM mysql\nDocker pull the latest version, in this case (8.0) that does not support type of authentication\nYou can specify version tag on you Dockerfile like :\nFROM mysql:5.7.26\nIf you want to stay in Mysql 8.0.\nYou should execute this command line inside container:\ndocker exec -it container_name mysql -u root -p [root-password] mysql -e \"update user set authentication_string=password(''), plugin='mysql_native_password' where user='root';\"\nOr connect to your container and execute the command:\ndocker exec -it container_name /bin/bash mysql -u root -p [root-password] mysql -e \"update user set authentication_string=password(''), plugin='mysql_native_password' where user='root';\"\nHope it helps",
    "How to mount shared volume during docker build?": "I'm not familiar with Bind9, but the -v /path/to/zonefiles/folder:/var/named argument mounts a volume at /var/named in the container at runtime, in place of whatever may have been on the image. The only way you can have files in /var/named at runtime is if those same files happen to be in /path/to/zonefiles on your host at runtime.\nYou should probably consider mounting the files to a different point than /var/named. So -v /path/to/zonefiles/folder:/var/namedSomethingElse.",
    "Problem building a docker container with Haskell-stack: How can I ensure that Haskell-stack continues building till the end?": "Your best bet is probably to pass --jobs 1 to stack. This will turn off concurrent builds which will reduce the memory requirements. GHC is generally a memory hog, and some code in particular really uses a lot of memory to compile. What's probably happening is that two modules that both take a lot of memory are ending up being built at the same time, and when that happens you'll end up with the OOM. But each time you run the build, a few more packages get built and the build order can change, so eventually you luck out and the memory hogs don't end up building concurrently, so the build is able to finish.",
    "Docker compose cannot find build path": "You could try the same command, but from a CMD session, instead of a git bash session.\nThat way, a path like C:\\... would be recognized by docker compose, from Docker Toolbox for Windows.",
    "How to run tests in Dockerfile using xunit": "Your test code references some files (containing the type MyService) that have not been copied to the image. This happens because your COPY . . instruction is executed after the WORKDIR /tests/Tests instruction, therefore you are copying everything inside the /tests/Tests folder, and not the referenced code which, according to your description, resides in the src folder.\nYour problem should be solved by performing COPY . . in your second line, right after the FROM instruction. That way, all the required files will be correctly copied to the image. If you proceed this way, you can simplify your Dockerfile to something like this (not tested):\nFROM microsoft/dotnet:2.2.103-sdk AS build\nCOPY . .                       # Copy all files\nWORKDIR /tests/Tests           # Go to tests directory\nENTRYPOINT [\"dotnet\", \"test\"]  # Run tests (this will perform a restore + build before launching the tests)",
    "docker influxdb restore db on start up": "You need to copy the backup file in /docker-entrypoint-initdb.d inside the container.\nChange:\nCOPY backedUpInfluxDBFolder /tmp/backedUpInfluxDBFolder\nto\nCOPY backedUpInfluxDBFolder/* /docker-entrypoint-initdb.d/\nI don't know Influx, but I assume the backup folder contains sql files? (my solution is based on this assumption).\nHope this helps.",
    "Programatically remove docker images associated with docker build from Dockerfile": "I think you can use docker history (Show the history of an image) to find all images related to the final one. I want to inform you that this solution doesn't work for multi-stage builds, because the final image has only references to images which were base layers for it.\nLet me show you some example:\n1. We create a simple Dockerfile:\nFROM ubuntu:latest\nRUN touch newfile\n2. We build a docker image from it:\n$ docker build -t new-image .\nSending build context to Docker daemon  2.048kB\nStep 1/2 : FROM ubuntu:latest\nlatest: Pulling from library/ubuntu\n898c46f3b1a1: Pull complete\n63366dfa0a50: Pull complete\n041d4cd74a92: Pull complete\n6e1bee0f8701: Pull complete\nDigest: sha256:017eef0b616011647b269b5c65826e2e2ebddbe5d1f8c1e56b3599fb14fabec8\nStatus: Downloaded newer image for ubuntu:latest\n ---> 94e814e2efa8\nStep 2/2 : RUN touch newfile\n ---> Running in ac93702f363b\nRemoving intermediate container ac93702f363b\n ---> 4d52ac122761\nSuccessfully built 4d52ac122761\nSuccessfully tagged new-image:latest\n3. And now we can clean all images which were used to build the final one:\n$ docker history new-image:latest | awk '!/IMAGE|<missing>/ {print $1}' | xargs -I {} docker rmi {}\nUntagged: new-image:latest\nDeleted: sha256:4d52ac122761ec1ba9f3d77606c49f98cf8c7d728d60834a87f25cede39a2027\nDeleted: sha256:7f62b9533dbd6514fe0cb7f4ddc3086ad5ae45fe1c13f5eeea1cc38f575e92bd\nUntagged: ubuntu:latest\nUntagged: ubuntu@sha256:017eef0b616011647b269b5c65826e2e2ebddbe5d1f8c1e56b3599fb14fabec8\nDeleted: sha256:94e814e2efa8845d95b2112d54497fbad173e45121ce9255b93401392f538499\nDeleted: sha256:e783d8ee44ce099d51cbe699f699a04e43c9af445d85d8576f0172ba92e4e16c\nDeleted: sha256:cc7fae10c2d465c5e4b95167987eaa53ae01a13df6894493efc5b28b95c1bba2\nDeleted: sha256:99fc3504db138523ca958c0c1887dd5e8b59f8104fbd6fd4eed485c3e25d2446\nDeleted: sha256:762d8e1a60542b83df67c13ec0d75517e5104dee84d8aa7fe5401113f89854d9",
    "Running docker-compose in another Dockerized Container": "Docker Compose volumes: directives always reference paths on the host. (Same for docker run -v.) If you're launching Docker in some form from a container, there's no way to inject one container's local filesystem into another.\nFor the application you're describing, I'd suggest two paths:\nIf your goal is to run the application in development mode, with debuggers and live reloading, put away Docker entirely and just use npm/yarn locally. (I bet you have vim and tmux installed on your host anyways for basic administration of that's the tooling you use, and installing Node is strictly easier than installing Docker.)\nIf your goal is to run the application in Docker for production or pre-production testing, remove the volumes: directives from the Dockerfile. After having built and tested your application from step 1, docker build an image and run the image itself, without attempting to replace its code.\nAlso remember that anyone or anything that can run Docker commands has unrestricted root access over the host. I don't see an obvious benefit to running Docker Compose in a container the way you're describing.",
    "Docker In Docker IN Docker Compose (Networking Issues)": "Your Dockerfile exposes port 80, but you are not mapping it in your docker-compose.yml file.\nTry updating the docker-compose-yml file to the following:\ndocker-compose-yml\nversion: '3.4'\n\nservices:\n  apiserver:\n    image: ${DOCKER_REGISTRY-}apiserver\n    build:\n      context: ./\n      dockerfile: ./ApiServer/Dockerfile\n    volumes:\n      - /var/run/docker.sock:/tmp/docker.sock\n    ports:\n      - 8080:80\nThe ports key, will map ports from your host machine to the container. Simplified syntax:\n    ports:\n      - <hostport>:<containerport>\nWhen starting a container with docker-compose, docker will create a new bridge network for your application stack. The containers in this network are not accessible from the host, and the containers inside the network cannot address the host either. This is why you need the port mapping - it will map the port 8080 on the host machine to the port 80 of the container.\nYou can then address the exposed service in the container with http://localhost:8080 from the host machine.\nUpdate\nFrom your comment, I gather that you may have other services running on the host that you want to be able to connect to from inside the ApiServer container, like a database. There is one quick solution to this I guess, and that is connecting the new container to the \"host\" network.\nYou can try updating your docker-compose.yml file to this:\ndocker-compose-yml\nversion: '3.4'\n\nservices:\n  apiserver:\n    image: ${DOCKER_REGISTRY-}apiserver\n    build:\n      context: ./\n      dockerfile: ./ApiServer/Dockerfile\n    volumes:\n      - /var/run/docker.sock:/tmp/docker.sock\n    ports:\n      - 8080:80\n    network_mode: \"host\"\nThe effect of this is, that when you resolve localhost and 127.0.0.1 from inside your container - you will actually get the hostmachine. This does not work if you deploy your container to a swarm though.",
    "Docker ubuntu 18.04 ssh-keyscan": "You should start sshd in same layer as ssh-keyscan, it's not started by default in new layer, so just couple the commands in same RUN:\nRUN \\\n  ./etc/init.d/ssh start && \\\n  ssh-keyscan localhost >> /home/postgres/.ssh/known_hosts",
    "Finding dependencies of containerized windows apps": "I ran into this and ended up downloading depends.exe from the website and then copied the extracted files to a mounted location between my computer and the container.\nThen I used depends.exe from the command line and was able to profile my app to find my missing late-loaded dependency without a GUI.\nThe /c option runs depends without opening the GUI.\nThis is the command line I used: depends.exe /c /f 1 /pg 1 /pl 1 /pf 1 /pb /of c:\\build\\profile.txt c:\\build\\source\\path\\to\\executable-to-profile.exe\nThe command prompt returned right away, making me feel like it did nothing, but I waited about 5 minutes or so and got a text file with the details.\nI realize this is an old question, but I stumbled here during my search, so I thought maybe this would help someone.",
    "Where does the data of postgresql container save ans, and also where the location of container is?": "You have to mount storage for the postgres container. https://github.com/nameko/nameko-examples/blob/master/docker-compose.yml\npostgres:\n    container_name: nameko-example-postgres\n    image: postgres\n    ports:\n        - \"5433:5432\" # Exposing Postgres on different port for convenience\n    environment:\n        POSTGRES_DB: \"orders\"\n        POSTGRES_PASSWORD: \"password\"\n        POSTGRES_USER: \"postgres\"\n    restart: always\nyou can add columes to the above snippet by\n  volumes:\n    - ./database:/var/lib/postgresql",
    "Save and Run Container created from visual Studio Docker Support": "In meantime i was able to solve the problem, but looks like a strange solution. If you end up having the container running just fine in Debug but not in release, i advice you to:\nDon't use .Net Standard projects with .net core dependencies (in our case we end up using a GlobalExceptionFilter in a .NET Standard project).\nTo solve this problem you can make a nuget with this dependencies and then use it across projects. (the nuget in this case is here for exemplification: https://www.nuget.org/packages/Backend.BaseApi)\nIf you are not successful, you can always change everything to .Net Core, or start a new application, run in release, and incrementally add code to troubleshoot the problem.\nGood luck",
    "A python script in Docker needs to accessAPI data present in different Docker image": "With your -p 5000:5000 option you can communicate your docker with your host, but if you want to communicate two docker in the same host, you need to define a docker network for them.\nThe easiest way to do that is launching dockers with --net=host options. This network allows your docker to use host interfaces, including localhost.\nEDIT: Adding more info\nCreate docker image (I recommend without ENTRYPOINT lines, copying your binary to a directory that belongs to the path.\nJust add net=host to your docker run command, just before creating image with docker build: docker run -dit --net=host -p 5000:5000 abc/xyz:v1.0.0 python3 <your_entry_point_path>\nAdd --net=host to other dockers that you launch.",
    "Building Docker Image with Gradle fails with \"COPY failed: no source files were specified\"": "Thanks guys, I got to work.\nI had to add :\ndoFirst {\n    copy {\n        from war\n        into stageDir\n    }\n       copy {\n           from \"${projectDir}/camunda/\"\n           into stageDir\n       }\n       copy {\n           from \"${projectDir}/definitions/\"\n           into stageDir\n       }\n}\nin my build.gradle. This would copy all my artifacts into build/docker/ and then my Dockerfile just need the filenames in their COPY src dest and NOT COPY /path/to/src/ dest.\nThis fixed it.\nThank u everyone!",
    "Running java app in kubernetes container is not working": "there is no need top use BOTH cmd and entrypoint commands.\nin my projects I create a small bash script that the java command is written inside.\nthat small script is running in my entry-point without any cmd configured.",
    "How can I pass to my source code the package-lock.json file generated inside my container?": "You need to add /app to volumes because package-lock.json will be generated in this directory, won't be?\nSince this you could replace your volumes:\nvolumes:\n\n    - ./app/src:/app/src\n\n    - /app/node_modules/\non:\nvolumes:\n    - ./app:/app\nAlso I would choose /var/app instead of /app.",
    "Run redis docker container from Dockerfile with authentication": "Omit target file name in COPY command:\nCOPY redis.conf /usr/local/etc/redis/\nTarget is expected to be a path, not file name:\nThe dest is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container.\nAs far as I undestood from you comments, you are trying to launch redis container instead of your own! This is a mistake, you need to use your one.\nLets say if you build it with command:\ndocker build -t my-redis .\nThen you launch it with something like\ndocker run -d -p 6379:6379 -v /data:/data --name \"redis-elk\" my-redis redis-server --appendonly yes",
    "django static files are not copied to saticfiles folder on docker's container": "if you are bringing up the containers via docker-compose you can remove\nRUN mkdir /www\nWORKDIR /www\nCOPY . /www/\nfrom your web Dockerfile since you're mounting the volume in the composer file\nI would think you'd be trying to serve static files from your nginx container, so you'd docker exec -it <nginx_container> bash and making sure the files were copied to /static and /media there right?",
    "How can I copy results from docker build without running": "You don't have to run a container, but you have to create one in order to be able to cp (copy) the binary from that container afterwards. The 2 commands needed are:\ndocker container create ...\ndocker container cp $container_name:/path/in/container /path/on/host\nExample:\nmain.go:\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n  fmt.Println(\"hello world\")\n}\nDockerfile:\nFROM golang:1.10-alpine3.7\n\nWORKDIR /go/src/app\nCOPY . .\n\nRUN go get -d -v ./...\nRUN go install -v ./...\n\nCMD [\"app\"]\nBuild - create temp container - copy binary - cleanup:\ndocker build -t go-build-test .\ndocker container create --name temp go-build-test\ndocker container cp temp:/go/bin/app ./\ndocker container rm temp\nThe binary has been copied to your current folder:\n~/docker_tests/go-build-test$ ls\napp  Dockerfile  main.go\n~/docker_tests/go-build-test$ ./app\nhello world",
    "Docker add fails with no such file or directory": "It could be expecting a single file because the destination does not end with a trailing slash:\nIf does not end with a trailing slash, it will be considered a regular file and the contents of will be written at . Dockerfile reference\nTry ADD mysql /var/lib/mysql/ (and make sure there is no .dockerignore with mysql in it!)",
    "User that run Docker health check": "My problem was in that I rewrite\n...\nUSER root\n...\nin child image as a result HEALTHCHECK run from root.",
    "How do I debug a Java app running in a Docker container in IntelliJ IDEA 2018.2?": "You can even use dcevm / HotswapAgent in dockerized application and modify code/resorces without restart. Look at this project https://github.com/HotswapProjects/hotswap-docklands",
    "Flask in docker: different relative import in Dockerfile build and docker-compose": "As mentioned here, to make the relative import work, one of the easier options is to specify the fully dotted name of the module to Gunicorn, like so:\napp.__init__.create_app()\nOther options to get this working include:\nChanging your relative import to a form like from mymodule import as_int. This will only work if your module name is unique. Otherwise, you can:\nfrom mypackage.mymodule import as_int",
    "How to include a local language server in a dockerfile and build a docker image from it?": "I might think that the problem lies in the ADD line. This adds the local file xtextls3 to your layer. However, the file cannot be found. I have the idea that you have to swap the first and second argument on the ADD-instruction.",
    "Add python project into a docker and test it in jupyter notebook": "It's easy, you only have to create a Dockerfile, with the base image tensorflow/tensorflow, copy your current folder with the Jupyter notebooks in it to /notebooks and add an entrypoint which executes jupyterhub for you at startup:\nFROM tensorflow/tensorflow\nCOPY . /notebooks/my-notebooks\nENTRYPOINT [\"/run_jupyter.sh\", \"--allow-root\"]\nBuild with\ndocker build -t my-tf .\nAnd run it!\ndocker run -it -p 8888:8888 my-tf\nTo access jupyterhub use http://127.0.0.1:8888",
    "how to install mongo-org-tools in centos docker container using dockerfile?": "You need to also install the mongodb-org package to create the user.",
    "RUN echo in Dockerfile with docker-compose fails": "RUN commands to install nano and echo lines into files seems correct. Most likely the problem is in RUN mysql --defaults-extra-file='/home/secret' nubuilder4 < /home/nubuilder4.sql\nSince you are mounting volume /home, there is no way for us to debug it. Are you sure that this RUN mysql... command passes successfully?\nTo debug: change your docker-compose.yml, instead of build: ./bin/mysql, use image: mariadb:10.1 and then connect to the container and run the first command with mysql.",
    "Setting up docker with php7.1, laravel and nginx server, getting Bad Gateway 502 exception": "First of, are you running standard docker or docker swarm? The version 3.x formats are for docker swarm.\nI would not suspect that is the issue with starting up and connecting to the FPM container though.\nI can't seem to find the virajkaulkar/laravel-app image on docker hub, so I suspect that this is a local or private repository image(?), in that case, you should first of confirm that it is actually using PHP-FPM and not some other version of php.\nAs it seems to be an issue in the connection with the fpm server, make sure that the container is running as it is supposed to and that it did not fail in the start up phase, if it's fine, I would start with trying to remove some of the fastcgi parameters and see if any of them creates issues in the connection.\nFor example, you have two SCRIPT_FILENAME parameters in the ~\\.php$ scope, start with removing one of them (the one that does not match the fpm containers index file), until everything works as you wish it to, remove the optimization parameters and re-add them when it does.\n   fastcgi_pass phpserver;\n   fastcgi_index  index.php;\n   fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n   # fastcgi_param SCRIPT_FILENAME /usr/share/nginx/html$fastcgi_script_name;\n   # fastcgi_buffers 256 128k;\n   # fastcgi_connect_timeout 300s;\n   # fastcgi_send_timeout 300s;\n   # fastcgi_read_timeout 300s;\n   include fastcgi_params;\nIf this does not help, I would recommend testing with another FPM container just to make sure that it actually works, bind the local directory (the one with the project) to the docker container and see if you can reach it, and if you can, it's likely that the container have some issues.",
    "Docker + Rails + MySQL = Ignoring environment variables in database.yml": "I was stumped on this issue for a day or two. Things were working fine out of Docker but for some reason within the container the ERB wasn't being parsed.\nThis post seemed to help: rails database.yml not accepting ERB (which I found via Rails not replace ENV's value in database.yml)\nI just experienced the same thing, and came across your post. I had been following a tutorial that had me create a puma.conf file that contained the code below:\nActiveRecord::Base.establish_connection( YAML.load_file( \"#{app_dir}/config/database.yml\" )[rails_env])\nI modified to the following, and everything worked as expected:\nrequire 'erb'\nActiveRecord::Base.establish_connection( YAML.load( ERB.new( File.read( \"#{app_dir}/config/database.yml\" )).result)[rails_env])",
    "Source files are updated, but CMD does not reflect": "I see several things:\nAccording to your Dockerfile\nMaybe you need a dep init before dep ensure\nProbably you need to check if main.go path is correct.\nAccording to docker philosophy\nIn my humble opinion, you should create an image with docker build -t <your_image_name> ., executing that where your Dockerfile is, but without CMD line.\nI would execute your go run <your main.go> in your docker run -d <your_image_name> go run <cmd/pkg/main.go> or whatever is your command.\nIf something is wrong, you can check exited containers with docker ps -a and furthermore check logs with docker logs <your_CONTAINER_name/id>\nOther way to check logs is access to the container using bash and execute go run manually:\ndocker run -ti <your_image_name> bash\n# go run blablabla",
    "unable to add server.xml on tomcat8 dockerfile. error in xml": "Can you open a shell in your new created container and try to cat the server.xml file? Use cat -A to show all the hidden characters.\nThat error is caused probably by a non-existent file or a character before the beginning of the xml content.\nAlso, if the only thing you're doing is changing the tomcat's port you have other solutions:\nExpose the port you want when you run the container: -p 443:8080\nMount the server.xml with your configuration when running the container as a volume -v ./server.xml:/conf/server.xml\nOr if you want to really build your image and change just the port you can do RUN value=cat conf/server.xml&& echo \"${value//8080/443}\" >| conf/server.xml",
    "UnknownHostException within Docker Container on Alpine openjdk:8-jdk-alpine": "It turns out the solution was fairly simple, and there's a couple options..\nFor you experts, this is probably funny, but at least its one more idea for the next guy..\nSo what I've found is I can specify the DNS on each of the nodes in my swarm via:\n/etc/docker/daemon.json\n\n{\n    \"dns\": [\"10.0.0.2\", \"8.8.8.8\", etc.. ]\n}\nAfter setting this on each node, specifically 8.8.8.8 for google's DNS, then \"google.com\" resolved and no prob. Note that its a google specific DNS, but its provides a public DNS. Yahoo, Amazon, etc all resolved.. The 10.0.0.2 address would be any other DNS you want to specify, and you can specify multiples.\nThis came from the following post: Fix Docker's networking DNS config\nHowever, it even easier if you want to specify the DNS via your compose/stack file.\nRather than go to each node in your swarm and update the daemon.json DNS entries, you can specify the DNS directly in your compose.\nversion: '3.3'\n\nservices:\n  my-sample-service:\n    image: my-repo/my-sample:1.0.0\n    ports: 8081:8080\n    networks:\n      - my-network\n    dns:\n      - 10.0.0.1 #this would be whatever your say internal DNS is priority 1\n      - 10.0.0.2 #this would be whatever other DNS you'd provide  priority 2\n      - 8.8.8.8  #default google address, even though docker specifies this\n                 #as a default, it wasn't working until I specifically set it\n                 #this would be the last one checked if no resolution happened\n                 #on the first two.",
    "Why I can't run a newly created Docker image?": "Your docker run command doesn't work because you don't have the :version1 tag at the end of it. (Your question claims you do, but the actual errors you quote don't have it.)\nHowever: if this was anything more than a simple typo, the community would probably be unable to help you, because you have no documentation about what went into your image. (And if not \"the community\", then \"you, six months later, when you discover your image has a critical security vulnerability\".) Learn how the Dockerfile system works and use docker build to build your custom images. Best-practice workflows tend to not use docker commit at all.",
    "docker - Any way to view all preceding Dockerfiles?": "docker history <IMAGE> will show you the history of an image. Some histories are better than others but you can get a sense of what is being done at each layer on the way. Size 0B usually means metadata added to the image while anything greater than 0B means ADD, COPY, RUN, or other file system related change.\nif there an IMAGE ID then you can also do docker inspect <image id> which will tell you the details about the image. If you just want to know the tag you can do\ndocker inspect --format='{{range .RepoTags}}{{.}}{{end}}' <image id>\nA little example of formatting a history:\ndocker history --format \"{{.ID}}\" <IMAGE> | grep -v \"<missing>\" | xargs docker inspect --format='{{range .RepoTags}}{{.}}{{end}} {{range .ContainerConfig.Cmd}}{{.}}{{end}}'",
    "Connection failed error when building docker image from ubuntu base image using Docker CE on macOS 10.13": "This could potentially because you are behind a proxy, and proxy env is not set in your docker box. You can set proxy while running docker build command, (replace proxy url with your own proxy server)\ndocker build --build-arg HTTP_PROXY=http://10.20.30.2:1234 --build-arg HTTPS_PROXY=http://10.20.30.2:1234\nIf you are using a Dockerfile,\nAdd the following (replace proxy url with your own proxy server) after FROM line,\nENV HTTP_PROXY=http://10.20.30.2:1234\nENV HTTPS_PROXY=http://10.20.30.2:1234",
    "Run executable with args inside container based on windows server 1709": "Try this way:\nStart-Process -FilePath \"$PSScriptRoot\\example.exe\" -ArgumentList \"/install\",\"/q\"\n'Example 7' at https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.management/start-process?view=powershell-5.1",
    "Openshift/original - create image based on Wildfly": "* Q1: S2I is a good way of working when you don't want to think about creating images / containers. The standard Openshift/Wildfly images can be used for S2I.\nWhen you want to work with a docker client with Openshift as a Container As A Service (CAAS) provider you will need another Wildfly image.\nThis Dockerfile will get you going. You can of course improve it with limiting the user permissions, etc.\nFROM jboss/wildfly\nCOPY target/ROOT.war /opt/jboss/wildfly/standalone/deployments/\n# CMD - use the default wildfly default start command\n* Q2: Via the Openshift Console you can add a route to explicitly do the port mapping of your ports. Just select via the Console the Applications > Routes. I used the default settings with the 443 secure port.\nYou should NOT use 8080. In my experience I use Openshift always with a secure 443 port.\nCAAS OVERVIEW: For your convenience you can find an overview of the steps for CAAS. Any improvements or further suggestions are very welcom!",
    "How to set up openshift document in Docker container?": "Use this:\nhttps://github.com/openshift-s2i/s2i-asciibinder\nThey even supply an example of deploying:\nhttps://github.com/openshift/openshift-docs.git\nThe README only shows s2i command line usage to build a docker image and run it, but to deploy in OpenShift you can run:\noc new-app openshift/asciibinder-018-centos7~https://github.com/openshift/openshift-docs.git\noc expose svc openshift-docs",
    "Copying files in tomcat war extract directory using Dockerfile removes existing files": "Yes, I also had a similar scenario. Whenever you copy into the extracted folder using the dockerfile, rest of the content will get cleared off of the extracted folder and only the copied content was left. So, instead of keeping my war file inside the webapps, I moved it to the tmp folder and extracted it right into the webapps directory. This way if you copy anything inside the extracted directory, Tomcat will not try to compare it with the war file content and hence wont reset anything but copy that particular file.",
    "Docker \"docker-entrypoint-initdb.d\" script doesn't get executed": "This image does not support /docker-entrypoint-initdb.d\nYou can use ENTRYPOINT. Just add to Dockerfile\nENTRYPOINT sh crond.sh",
    "command option gets ignored in docker-compose.yaml": "I think you could run node in command directly, without wrapping with bash -c, such as\n  command: node ui-configurator.js -f configure-ui.yaml\nAs long as you make sure WORKDIR is correct so ui-configurator.js and configure-ui.yaml can be found, this should work.",
    "How to install oracle-java8-installer on docker debian:jessie": "Found the solution on https://hub.docker.com/r/anapsix/docker-oracle-java8/~/dockerfile/:\n## JAVA INSTALLATION\nRUN echo \"oracle-java8-installer shared/accepted-oracle-license-v1-1 select true\" | debconf-set-selections\nRUN echo \"deb http://ppa.launchpad.net/webupd8team/java/ubuntu trusty main\" > /etc/apt/sources.list.d/webupd8team-java-trusty.list\nRUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys EEA14886\nRUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes --no-install-recommends oracle-java8-installer && apt-get clean all\nThe \"secret sauce\" you were looking for is the first line:\nRUN echo \"oracle-java8-installer shared/accepted-oracle-license-v1-1 select true\" | debconf-set-selections",
    "Setting up a mongoDB in a Dockerfile": "The problem is related to how Docker processes the Dockerfile. Each statement in the Dockerfile is executed in a container, which was created from the image of the previous line. In case of the RUN statement, Docker runs the code in a container and creates a new image based on the current state of the container. The image does not contain any information about running processes.\nIn your case, this means you have an image, which contains the mongodb installation. Let's call it A (instead of hash values from the docker build log). The first RUN statement\nRUN mongod --fork --logpath /var/log/mongod.log\nis processed in a container, let's call it a, which is based on image A. The process mongod in a probably appends a few lines to the log file. Once the RUN statement has been processed, the container's file system is frozen and labeled as image B. A Docker image does not contain any information about running processes. In fact, at this point, there is not mongod process running anymore. However, image B contains the log entries from this previous RUN.\nThe second RUN statement\nRUN mongo --eval \"use db\"\nis executed in container b, which is based on image B. This commands fails because there is no running process to which mongo could talk to.\nI think there are two solutions to your problem:\nIn general, you can run both scripts in one step\nRUN mongod --fork --logpath /var/log/mongod.log && mongo --eval \"use db\"\nor move them to a bash script, such that they are executed in the same container.\nUse the official mongo image on Docker hub.",
    "How to deploy an application running in docker - best practice?": "When you build image every time with new application you have easy way to deploy it later on to the customer or on your production server. When the docker image is ready you can keep it in the repository. Additionally you have full control on that that your docker is working with current application.\nIn case of keeping the application in mounted volume you have to keep in mind following problems:\nlife cycle of application - what to do with container when you have to update the application - gently stop, overwrite and run again\nhow do you deploy your application - you have to do it manually over SSH, or you want just to run simple command docker run, and it runs your latest version from your repository\nThe mounted volumes are rather for following casses:\nyou want to have externally exposed settings for container - what is also not a good idea\nyou want to have externally access to the data produced by the application like logs, db, etc\nTo automate it totally, you can:\nbuild image for each application and push to the repository\nuse for example watchtower to automatic update of the system on your production servers",
    "File permissions in Docker Container": "I tried to build and run the Dockerfile you provided and ran into multiple errors. You are asking about the file permissions your \"app\" creates. So here is my starting point:\nI assume that the \"app\" is catalina.sh. The process which creates your files in /tmp/. Since we are running the container as the user tomcat it automatically creates the files with the according file permissions. Have a look at the code comments below to get some more information about what is going on here.\nDockerfile:\nFROM httpd \n# I switched the image since you would need to configure tomcat to make it run\n# httpd works out of the box without any application specific config. That's why\n\nCOPY ./catalina.sh /\nRUN chmod +x /catalina.sh  # copy your 'app' / entrypoint into the image\n\nRUN groupadd -r tomcat \\\n    && useradd -r -g tomcat tomcat\n# create a new user and group under which the container runs\nRUN chown -R tomcat:tomcat /tmp/  # change initial file permisisons. \n# So that our new user tomcat is allowed to access and fill the directory with files\n\nUSER tomcat  # switch the user under which the container runs\n\nENTRYPOINT [\"/catalina.sh\"]\ncatalina.sh:\n#!/bin/bash\n\nmkdir /tmp/test  # create a dir\ntouch /tmp/yolo.txt  # and some files\ntouch /tmp/test/yolo2.txt  # to check the file permissions later on\n\nwhile true; do echo \"sleep\"; sleep 2; done\n# endless loop so that the container doesn't exit\nTo check the file permissions exec into the running container.\ndocker exec -ti <container_name> bash",
    "What does \"RUN sh -c 'touch /app.jar'\" do?": "touch command will update the timestamp to file, directory.\nSo that you can keep track of files when it's get created and updated.\nRUN sh -c 'touch /app.jar'\nwhen you invoke docker build, above command will update timestamp to app.jar.\nFor complete details of touch command refer below link https://www.computerhope.com/unix/utouch.htm",
    "How to get Docker IP in windows Container?": "When you're running the command docker inspect --format=\"{{.Id}}\" 7003460ebe84 - you're currently running this against the image ID not the container ID.\nImages are the static asset that you build, and of which containers are run from. So what you need to do is first bring your image online, via:\ndocker-compose up\nNow you'll be able to see the running containers via:\ndocker ps\nFind the container you want; let's say it's abcd1234\nNow you'll be able to run your original command against the container - rather than the image.\ndocker inspect --format=\"{{.Id}}\" abcd1234\nThis will return the full SHA of the container; and since you originally asked about the network settings; you'll be able to run something like:\ndocker inspect -f \"{{ .NetworkSettings.Networks.your_network_here.IPAddress }}\" abcd1234\nIf you're unsure exactly what your network name is (Looks like it should be nat) just do a full docker inspect abcd1234 and look at the output and adjust the filter as needed.",
    "Successfully installed magento2 but the admin page throwing not found error": "give file permission for /var/html/{magento_folder/file giving issue} as\nsudo chown -R www-data:www-data /var/html/{magento_folder/file giving issue}",
    "docker is not running django server": "You have not mounted any database inside sqlite image. The readme of the sqlite repository says to mount the database at /db/db.db inside container.\nYou can do this by including volumes in the docker-compose.yml file. Your new file should look like:\nversion: '3'\n\nservices:\n  db:\n    image: spartakode/sqlite3:latest\n    volumes:\n      - ./database.db:/db/db.db\n  web:\n    build: .\n    command: python3 manage.py runserver 0.0.0.0:8000\n    volumes:\n      - .:/code\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - db",
    "\"dotnet run\" in Docker Container results in inaccessible url": "When using microsoft/aspnetcore-build or microsoft/aspnetcore, the image is pre-configured to expose port 80. Launch your container like this instead:\nFROM microsoft/aspnetcore-build:2.0.0-jessie\nWORKDIR /app\nENTRYPOINT [\"dotnet\", \"run\"]\nOn command-line,\ndocker run -p 5000:80\nOr in docker-compose.yml,\nports: \n  - \"5000:80\"\nThis default comes from the definition of the microsoft/aspnetcore images, which set the ASPNETCORE_URL environment variable to http://+:80.\nENV ASPNETCORE_URLS http://+:80\nSee https://github.com/aspnet/aspnet-docker/blob/v2.0.0/2.0/jessie/runtime/Dockerfile#L4",
    "Angular 4 with Docker and Nginx": "I have solved the issue. The port 80 was used by some other process. I have changed\ndocker run -p 80:80 -it nikhilbaby/testing12\nto\ndocker run --name angular-app -p 8080:80 -d  nikhilbaby/testing12\nI can access the site in http://localhost:8080/",
    "Cannot docker build because of \"Couldn't run auplink before unmount\" error": "Is this running on a non-Intel/ARM 64-bit Ubuntu? For example a Raspberry Pi or ARM64 CPU? This error:\nshim error: fork/exec /usr/bin/docker-runc: exec format error\nWould imply that either (a) the binary install on your machine is corrupted in some way, or (b) you are attempting to run a binary for a different architecture on your system.\nCan you post the output of uname -a and file /usr/bin/docker-runc? That might help narrow down the source of your problem.",
    "Docker ENTRYPOINT/CMD cannot find file": "Oh damn, so in the docker build step, the files were there, and ls -a revealed that.\nBut when I was running with docker run, I had this:\ndocker run -v \"${project_root}/node_modules\":/usr/src/app --name  ${container_name} ${image_tag}\nso the node_modules dir in the container was being overwritten with the node_modules from outside the container which did not have the suman directory.\nMy bad fam.",
    "Cannot keep the scala application up and running on docker": "Are you including the & in the command? This forks the command into the background and then the container just exits. You can probably solve this by just removing the &. Otherwise, some more information is needed.\nCan you provide the Dockerfile including the full command used to start the container? Also, always posting versions of things is a good idea (what versions of Scala, Docker, etc.).",
    "Extend Wordpress Latest Dockerfile": "there are different ways. the easiest way would be to go inside of the builded and running image and change the line of you config file.\nif the container runs you can access it with\ndocker exec -it <imageid> sh (or bash, whatever OS you use)\nthen search your file and change it with a local editor. then you could save or commit your image to save the changes permanently.\ndocker save / docker commit\nsecond way is to overwrite your file with a COPY command.\ndocker cp foo.txt mycontainer:/foo.txt\ni am not sure if its usefull to volume mount the folder and change it directly on your host.",
    "Get the container ID when triggered from a docker file": "you have to build your Dockerfile and after it successfully build, you have to RUN it. Only a running container gets a container ID.\nhave you tried docker ps or docker inspect?\nfor example:\n$ docker inspect --format=\"{{.Id}}\" <container_name>\n756535dc6e9ab9b560f84c85063f55952273a23192641fc2756aa9721d9d1000",
    "Ruby and Rails \"path not found\" when installed using RVM on Docker": "If acceptable to your team it is probably easier to start with the base dockerfile that Eclipse Che provides for RoR: https://github.com/eclipse/che-dockerfiles/blob/master/recipes/ruby_rails/Dockerfile\nThat should get you the basics and it's kept up-to-date by the community. You can then extend this with any other tools, libraries, deps you need.",
    "Unable to open application after docker-compose": "Update Dockerfile in web-app like this:\nFROM aallam/oracle-java\n\nENV DEBIAN_FRONTEND noninteractive\nENV TOMCAT_MAJOR_VERSION=8\nENV TOMCAT_VERSION=8.5.14\nENV TOMCAT_HOME=/opt/tomcat\n\nRUN apt-get update && \\\napt-get -yq install supervisor && \\\n  rm -rf /var/lib/apt/lists/*\n\nWORKDIR /tmp\n\nRUN groupadd tomcat && \\\n    useradd -s /bin/false -g tomcat -d $TOMCAT_HOME tomcat && \\\n    mkdir $TOMCAT_HOME && \\\n    #wget http://mirrors.standaloneinstaller.com/apache/tomcat/tomcat-$TOMCAT_MAJOR_VERSION/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz && \\\n    wget http://archive.apache.org/dist/tomcat/tomcat-8/v8.0.23/bin/apache-tomcat-8.0.23.tar.gz && \\\n    tar xzvf apache-tomcat-8*tar.gz -C $TOMCAT_HOME --strip-components=1 && \\\n    chown -R tomcat:tomcat $TOMCAT_HOME && \\\n    chmod -R g+r $TOMCAT_HOME/conf && \\\n    chmod g+x $TOMCAT_HOME/conf && \\\n    rm -rf apache-tomcat-$TOMCAT_VERSION.tar.gz\n\nWORKDIR /\n\nADD /web-app/tomcat-run.sh /tomcat-run.sh\nADD /web-app/run.sh /run.sh\nADD /web-app/supervisord-tomcat.conf /etc/supervisor/conf.d/supervisord-tomcat.conf\nADD /web-app/settings.xml $TOMCAT_HOME/conf/settings.xml\nADD /web-app/tomcat-users.xml $TOMCAT_HOME/conf/tomcat-users.xml\nADD /web-app/context.xml $TOMCAT_HOME/webapps/manager/META-INF/context.xml\nRUN chmod 755 /*.sh\n\nCOPY /web-app/target/*.war $TOMCAT_HOME/webapps/\n\nexpose 8080\nENTRYPOINT [\"/run.sh\"]\nDuplicate run.sh for both web-app and app-db folder.\nUpdate Dockerfile in app-db like this:\nFROM aallam/oracle-java\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update && \\\n  apt-get -yq install mysql-server supervisor && \\\n  rm -rf /var/lib/apt/lists/*\n\n#WORKDIR /\nADD /app-db/run.sh /run.sh\nADD /app-db/bind_0.cnf /etc/mysql/conf.d/bind_0.cnf\nADD /app-db/mysql-run.sh /mysql-run.sh\nADD /app-db/supervisord-mysql.conf /etc/supervisor/conf.d/supervisord-mysql.conf\n\nVOLUME [\"/var/lib/mysql\"]\nexpose 3306\nENTRYPOINT [\"/run.sh\"]\nThen run commands:\ndocker-compose up\ndocker ps \nOutput will be like this and system is up:\nCONTAINER ID        IMAGE               COMMAND             CREATED                  STATUS              PORTS                    NAMES\na60682b914a4        test_web-app        \"/run.sh\"           Less than a second ago   Up 9 seconds        0.0.0.0:8080->8080/tcp   test_web-app_1\n607522cac623        test_app-db         \"/run.sh\"           Less than a second ago   Up 9 seconds        0.0.0.0:3306->3306/tcp   test_app-db_1\nTo connect MySQL server in app-db container, your JDBC connection url in your project should be something like this:\n\"jdbc:mysql://DOCKER_MACHINE_IP:3306/DB_NAME\".\nAt the app-db container side, access MySQL from command line, create and give permissions for user with DOCKER_MACHINE_IP:\n> CREATE USER 'root'@'DOCKER_MACHINE_IP' IDENTIFIED BY 'root_password'; \n> GRANT ALL ON *.* to root@'DOCKER_MACHINE_IP' IDENTIFIED BY 'root_password';\n> FLUSH PRIVILEGES;",
    "ONBUILD in nestled builded Docker images": "I know it is old question, but I struggled with it also and I found in 'The Docker Book' by James Turnbull following explanation:\nThe ONBUILD triggers are executed in the order specified in the parent image and are only inherited once. If we build another image from this new image, [..], then the triggers would not be executed when that image is built.\nSo for anyone looking for it in the future, it's not possible.",
    "docker secrets for configuration files": "Since version 17.06 you can use docker configs for that (If you use swarm mode). Configs are like docker secrets but not encrypted. https://docs.docker.com/engine/swarm/configs/",
    "how to run two microservices on same network?": "edit:\nThe OP used 2 different .yaml files and 2 different networks were being created maybe because docker-compose uses as project name (by default) the name of the directory with the docker-compose.yaml file. In the comments it was proposed to use:\ndocker-compose -p project_name up\noriginal answer:\nI have changed your yaml a little bit in order to make something that works and can be demonstrated.\nI have used the alpine image\nI have changed one of the two 8900 ports to 8901 because you can't bind it twice (you can run as many containers as you want making use of their 8900 port but your host has only one 8900 port).\nI added entrypoint: ping msX in order to show you that they ping each other.\ndocker-compose.yaml (run it with docker-compose up):\nversion: '2'\n\nservices:\n  ms1:\n    image: alpine\n    mem_limit: 512M\n    environment:\n     SPRING_PROFILES_ACTIVE: docker-development-cloud\n     JAVA_OPTS: -Xms256m -Xmx512m\n    ports:\n    - \"8900:8900\"\n    restart: always\n    networks:\n    - docker_dev_cloud\n    entrypoint: ping ms2\n\n  ms2:\n    image: alpine\n    mem_limit: 512M\n    environment:\n     SPRING_PROFILES_ACTIVE: docker-development-cloud\n     JAVA_OPTS: -Xms256m -Xmx512m\n    ports:\n    - \"8901:8900\"\n    restart: always\n    networks:\n    - docker_dev_cloud\n    entrypoint: ping ms1\n\nnetworks:\n  docker_dev_cloud:\n   driver: bridge\nyou can see that it works, because ms1 - ms2 actually ping each other.\nubuntu@ubuntu:~/docker_compose_tests/test$ docker-compose up\nCreating network \"test_docker_dev_cloud\" with driver \"bridge\"\nCreating test_ms1_1\nCreating test_ms2_1\nAttaching to test_ms2_1, test_ms1_1\nms2_1  | PING ms1 (172.22.0.3): 56 data bytes\nms2_1  | 64 bytes from 172.22.0.3: seq=0 ttl=64 time=0.070 ms\nms1_1  | PING ms2 (172.22.0.2): 56 data bytes\nms1_1  | 64 bytes from 172.22.0.2: seq=0 ttl=64 time=0.054 ms\nms2_1  | 64 bytes from 172.22.0.3: seq=1 ttl=64 time=0.286 ms\nms1_1  | 64 bytes from 172.22.0.2: seq=1 ttl=64 time=0.113 ms\nms2_1  | 64 bytes from 172.22.0.3: seq=2 ttl=64 time=0.129 ms\nms1_1  | 64 bytes from 172.22.0.2: seq=2 ttl=64 time=0.086 ms\nms2_1  | 64 bytes from 172.22.0.3: seq=3 ttl=64 time=0.137 ms\nms1_1  | 64 bytes from 172.22.0.2: seq=3 ttl=64 time=0.113 ms\nms2_1  | 64 bytes from 172.22.0.3: seq=4 ttl=64 time=0.246 ms\nms1_1  | 64 bytes from 172.22.0.2: seq=4 ttl=64 time=0.115 ms\nms2_1  | 64 bytes from 172.22.0.3: seq=5 ttl=64 time=0.078 ms\nand the network is also created and can be listed:\nubuntu@ubuntu:~/docker_compose_tests/test$ docker network ls\nNETWORK ID          NAME                    DRIVER              SCOPE\nc8562a9231c3        bridge                  bridge              local\n412040f6cf69        host                    host                local\n1cbabce12616        none                    null                local\n59de206c1ffa        test_docker_dev_cloud   bridge              local",
    "exposing container ports for django application": "When running application on docker machine you should be using docker-machine ip instead of localhost\nuse : docker-machine ip default to get docker machine ip, where default is your machine",
    "docker-compose where can I get a detail log (info) about what happened": "Instead of looking for how to find an error in your composition, you should change your process. With docker it is easy to fall into using all of the lovely tools to make things work seamlessly together, but this sounds like an individual container issue, not an issue with docker compse. You should try building an empty container with the baseimage you want then using docker exec -it <somecontainername> sh to go into it and run the commands you're trying to execute in your entrypoint manually to find the specific breakpoint.",
    "docker run vs. docker-compose up: same image, different outcome": "Turns out it is the volume mount of the config file into the config folder of php - can only be found in my compose file. It overwrites the auto-generated config of PHP.\nOne of these things that keep you busy the whole day without delivering the tiniest piece of functionality...\nThanks for your help, guys!",
    "How to launch app with webpack for production in Docker container?": "The problem is solved with only adding --host=0.0.0.0 parameters to npm start.\nSo my package.json after editing seems like:\n...\n\"scripts\": {\n    \"start\": \"webpack-dev-server --host=0.0.0.0\"\n},\n...",
    "Unable to start embedded container due to NoClassDefFoundError": "Check the Java Version in Docker Container. hope you are running the app after artifact. if so make sure it is adding all the dependencies.",
    "Is it possible to create named volume via Dockerfile during docker build?": "I would solve this problem by creating an image where I install/update all my gems as a volume. You will need following in Dockerfile for gems container:\nVOLUME /PATH/TO/GEMS\nthen importing these gems in all the other containers using shared volumes:\ndocker run --volumes-from CONTAINER_WITH_GEMS --name CONTAINER_NAME IMAGE\nMultiple containers can also share one or more data volumes. However, multiple containers writing to a single shared volume can cause data corruption. Given this if you can make sure your applications are designed to write to shared data stores we can solve this other way around as well.",
    "Dockerfile dependency installed properly but resulting image doesn't have it": "This question is quite specific to Ubuntu and Debian so it probably belongs on askubuntu.com where it's been asked already.\nIn all versions of Ubuntu based on Debian jessie, you'll find add-apt-repository in the package python-software-common:\nsudo apt-get install python-software-common\nIn all versions of Ubuntu based on Debian wheezy, you'll find add-apt-repository in the package python-software-properties:\nsudo apt-get install python-software-properties\nTip: type cat /etc/debian_version in a terminal to find out which Debian version your current Ubuntu distro is based on.\nAll the jessie/wheezy Ubuntu versions:\n15.10  wily       jessie  / sid\n15.04  vivid      jessie  / sid\n14.10  utopic     jessie  / sid\n14.04  trusty     jessie  / sid  <-- the OP is using trusty/jessie\n13.10  saucy      wheezy  / sid\n13.04  raring     wheezy  / sid\n12.10  quantal    wheezy  / sid\n12.04  precise    wheezy  / sid\n11.10  oneiric    wheezy  / sid\nYou can look up packages and specific files within packages here: http://packages.ubuntu.com/\nSearching for the file add-apt-repository in Ubuntu trusty: http://packages.ubuntu.com/search?searchon=contents&keywords=add-apt-repository&mode=exactfilename&suite=trusty&arch=any\nFile                            Packages \n/usr/bin/add-apt-repository     software-properties-common",
    "Hyperledger Fabric Chaincode Deploment - DockerFile missing": "I think the rest interface was discontinued in HyperLedger 1.0 so the above command will not work.",
    "Redirect application logs to docker logs": "We need more information, like which application, your Dockerfile, etc.\nAs a general answer you must have an entry point script which will send your application log to stdout:\nrunapp &\ntail -f logfile\nSomething like that.\nRegards",
    "Not able to install packages in docker containers": "When you test by running the container manually you don't update the cache whith apt-get update hence the Unable to locate package error\nBut your Dockerfile example should work",
    "Docker and SSH for development with phpStorm": "I too have implemented the ssh server workaround when using jetbrains IDEs.\nUsually what I do is add a public ssh key to the ~/.ssh/authorized_keys file for the SSH user in the target container/system, and enable passwordless sudo.\nOne solution that I've thought of, but not yet had the time to implement, would be to make some sort of SSH service that would be a gateway to a docker exec command. That would potentially allow at least some functionality without having to modify your images in any way for this dev requirement.",
    "Docker shared volume creation": "The volume you created does not match the inspect output. This would indicate that your volume create command failed, or perhaps you're checking the wrong host for the volume. With the current version of docker, the expected output is:\n$ docker volume create --driver local --opt type=nfs \\\n         --opt o=addr=10.10.10.10,rw --opt device=:/path/to/dir nfs_test\n\n$ docker volume inspect nfs_test                                       \n[\n    {\n        \"CreatedAt\": \"2018-02-18T12:10:03-05:00\",\n        \"Driver\": \"local\",\n        \"Labels\": {},\n        \"Mountpoint\": \"/home/var-docker/volumes/nfs_test/_data\",\n        \"Name\": \"nfs_test\",\n        \"Options\": {\n            \"device\": \":/path/to/dir\",\n            \"o\": \"addr=10.10.10.10,rw\",\n            \"type\": \"nfs\"\n        },\n        \"Scope\": \"local\"\n    }\n]\nThe output you produced matches a local volume created without any options:\n$ docker volume create foo\n\n$ docker volume inspect foo\n[\n    {\n        \"CreatedAt\": \"2018-02-18T12:10:51-05:00\",\n        \"Driver\": \"local\",\n        \"Labels\": {},\n        \"Mountpoint\": \"/home/var-docker/volumes/foo/_data\",\n        \"Name\": \"foo\",\n        \"Options\": {},\n        \"Scope\": \"local\"\n    }\n]",
    "Cannot exit from Docker container": "su manpage says:\n-, -l, --login\nProvide an environment similar to what the user would expect had the user logged in directly\nIt spawns another shell from where you exit to comeback to parent container shell.\nSo changing\nsu - -c \"python <path_to_script>/myscript.py\" $USER\nto\nsu \"$USER\" -c \"python <path_to_script>/myscript.py\" \nmight be the solution\nSIDENOTE : The - together with -c doesn't make sense when your objective is just to execute some commands and finish the job",
    "Execute chmod in Dockerfile on Raspberry pi /dev/spi*": "In your Dockerfile, just add this line:\nRUN chmod 666 /dev/spi*",
    "ERROR: for app no such device": "Why you need driver_opts here. It is for network option.\nI think you should try:\nversion: '2'\n\nservices:\n  app:\n    build: ./app\n    container_name: myapp\n    volumes:\n      - \"myapp:/root/www/myapp\"\n\nvolumes:\n  myapp:",
    "'undo' or 'cancel' dockerfile VOLUME to share mysql DB in registry": "I'm giving up on this -- seems simpler to distribute a zipped copy of the DB data directory. If you have a better option, please post.",
    "PyCharm Docker Deployment \"[Errno 2] No such file or directory\"": "My current solution to this problem is to move the Dockerfile to the root directory of my PyCharm project so that when ADD . /var/app is run, it copies over the correct files. As I mentioned in the \"UPDATE\", the PyCharm plugin uses the directory the Dockerfile is run from as the build context/path.\nWould still like to be able to specify the build path, but this may be the best solution given the limitations of the plugin.",
    "Do Dockerfile WORKDIR, ENTRYPOINT, VOLUME ... apply to child images?": "Those properties do propagate from one base image to another using said base image (with a FROM directive).\nBut regarding WORKDIR, it is better to repeat it in order to document what '.' means in a COPY . xxx",
    "Check for updated package via yum in Dockerfile": "From \"Build cache\", you could insert an ADD or COPY directive (of a dummy file) just before those RUN commands.\nWhenever you want to invalidate the cache for the next RUN, modify the content of the dummy file, and the ADD/COPY (with the rest of the Dockerfile commands) won't rely on the cache.",
    "Docker postgres persistance and container lifetime": "You're correct that you can start the container using 'docker run' and start it again in the future using 'docker start' assuming you haven't removed the container. You're also correct that docker containers are supposed to be ephemeral and you shouldn't be in bad shape if the container disappears. What you can do is mount a volume into the docker container to the storage location of the database.\ndocker run -v /postgres/storage:/container/postgres --name some-postgres -e POSTGRES_PASSWORD=mysecretpassword -d postgres\nIf you know the location of where the database writes to inside the container you can mount it correctly and then even if you remove the postgres container, when you start back up all your data will persist. You may need to mount some other areas that control configurations as well unless you modify and save the container.",
    "How to get contents generated by a docker container on the local fileystem": "I think you have a small misunderstanding of what VOLUME in your Dockerfile does and what -v does when you run a docker container.\nThe fundamental difference between VOLUME and -v is this: -v will mount existing files from your operating system inside your docker container and VOLUME will create a new, empty volume on your host (stored in /var/lib/docker/xxxxx) and mount it inside your container.\nExample:\nWe have a file test-file in /home/test/\ncmckinnel at arch in /home/test\n$ ll\ntotal 8\ndrwxr-xr-x 2 cmckinnel adm  4096 Jan 21 14:40 .\ndrwxr-xr-x 4 root      root 4096 Jan 21 14:40 ..\n-rw-r--r-- 1 cmckinnel adm     0 Jan 21 14:40 test-file\nWe want to mount this directory inside our docker container so when we change a file on our host, it changes inside the container.\nThis is what you're currently doing (creating an empty VOLUME inside your container at /home/test):\nDockerfile\nFROM ubuntu:14.04\n\nVOLUME /home/test\nBuild this image and run a container from it:\n$ docker build -t test-creating-volume .\n$ docker run -ti test-creating-volume bash\n$ cd /home/test\n$ ll\ntotal 8\ndrwxr-xr-x 2 root root 4096 Jan 21 14:41 ./\ndrwxr-xr-x 3 root root 4096 Jan 21 14:41 ../\nWhat you actually want to do instead is use -v when you docker run:\nDockerfile\nFROM ubuntu:14.04\nBuild this image and run a container from it:\n$ docker build -t test-mounting-volume .\n$ docker run -ti -v /home/test:/home/test test-mounting-volume bash\n$ cd /home/test\n$ ll\ntotal 8\ndrwxr-xr-x 2 1000 adm  4096 Jan 21 14:40 ./\ndrwxr-xr-x 3 root root 4096 Jan 21 14:51 ../\n-rw-r--r-- 1 1000 adm     0 Jan 21 14:40 test-file\nEdit\nOops, I think your actual problem above is you don't have the full path in your volume: declaration in your docker-compose.yml file.\nExample:\ndocker-compose.yml (note .home doesn't exist)\ntest-mounting-volume:\n    image: test-mounting-volume:\n    volumes:\n        - .home:/home/test\nThen:\n$ docker-compose run test-mounting-volume\n$ cd /home/test\n$ ll\ntotal 8\ndrwxr-xr-x 2 root root 4096 Jan 21 15:01 ./\ndrwxr-xr-x 3 root root 4096 Jan 21 15:01 ../\nBut if you put the full path:\ndocker-compose.yml:\ntest-mounting-volume:\n    image: test-mounting-volume:\n    volumes:\n        - /home/test:/home/test\nThen:\n$ docker-compose run test-mounting-volume\n$ cd /home/test\n$ ll\ntotal 8\ndrwxr-xr-x 2 root root 4096 Jan 21 15:01 ./\ndrwxr-xr-x 3 root root 4096 Jan 21 15:01 ../\n-rw-r--r-- 1 1000 adm     0 Jan 21 14:40 test-file",
    "How to write a long command within dockerfile?": "If you want to run this directly from the Dockerfile (without an additional script), you can also echo it and escape the line breaks:\nprintf \"line 1, ${kernel}\\n line 2,\\n line 3, ${distro}\\n line 4\\n line 5\" >> /etc/myconfig.conf\nBut be careful: on windows systems you would need a \\r\\n",
    "Docker port mapping not working": "docker run --it -p --rm --privileged 6443:443 image1\nshould be:\ndocker run --it -p 6443:443 --rm --privileged image1",
    "Initialization of schema during oracle initialization using docker file": "Use this Dockerfile. Build the docker image using:\nbash ./buildDockerImage.sh -v 18.4.0 -i -x\nStart the container; then, you can execute your script with this:\ndocker exec -t --user oracle <container_id> bash -c \"echo exit | sqlplus system/oracle @<path_to_your_script>DbScripts.sql\"\nNote, for this to work, you will need to mount the volume to the location of your scripts.",
    "php app files permission after docker build": "The permission does not seem to be on a file, but on 'open stream' operation. This could be one of the causes:\nWhen you install your application in the docker file, the hostname of the final container will no longer be the same as the hostname of the temporary container used while building the image. The app installer might fetch the hostname and store it in some configuration file.\nIf that is the case, then, when you run the container, you should execute a script which replaces the hostname in the config.",
    "Unable to pull private repo in docker": "Use the full image name, including the tag, when pushing and pulling:\ndocker push werain/digitdem:latest\ndocker pull werain/digitdem:latest\nDocker generally assumes you mean latest when you don't specify, but if you want to use your own tag or if you didn't push the same tag as you're trying to pull, then omitting the tag won't work.",
    "Cron job break other foreground server in docker": "Your entrypoint script needs to be called from either the ENTRYPOINT or CMD instruction, or it won't get run when the container is started.\nYou'll need to remove the CMD and start cron from the entrypoint script before launching the application. You might want to use a process manager such as runit or supervisord to handle this (see https://docs.docker.com/articles/using_supervisord/).",
    "Vagrant and Docker provider: a way to force proxy VM for Linux hosts?": "Just use d.force_host_vm = true option\nFrom Vagrant docs:\nforce_host_vm (boolean) - If true, then a host VM will be spun up even if the computer running Vagrant supports Linux containers. This is useful to enforce a consistent environment to run Docker. This value defaults to \"true\" on Mac and Windows hosts and defaults to \"false\" on Linux hosts. Mac/Windows users who choose to use a different Docker provider or opt-in to the native Docker builds can explicitly set this value to false to disable the behavior.",
    "docker mysql persistent storage": "I have not used fig but will be looking into it as the simplistic syntax in your post looks pretty great. Every day I spend a couple more hours expanding my knowledge about Docker and rewire my brain as to what is possible. For about 3 weeks now I have been running a stateless data container for my MySQL instances. This has been working great with no issues.\nIf the content inside /var/lib/mysql does not exist when the container starts, then a script installs the needed database files. The script checks to see if the initial database files exist not just the /var/lib/mysql path.\nif [[ ! -f $VOLUME_HOME/ibdata1 ]]; then\n    echo \"=> An empty or uninitialized MySQL volume is detected in $VOLUME_HOME\"\n    echo \"=> Installing MySQL ...\"\n\nelse\n    echo \"=> Using an existing volume of MySQL\"\nfi\nHere is a direct link to a MySQL repo I am continuing to enhance",
    "Docker - How do I pull/push multiple images at once?": "When you pull Docker images from Docker Hub, it's important to note that the images themselves only contain the base installation of the respective software (MongoDB, Redis, etc.). If you need specific configurations, initial data, volume mappings, or port mappings, you'll have to set those up in your docker-compose.yml file or Docker run commands.add service as mango:latest....it will work.!!!",
    "Docker is too slow to run the application on the MacBook Pro (M1) local machine": "Docker is not a good solution to dev. use RVM to avoid ruby version mismatching .\n\\curl -sSL https://get.rvm.io  | bash\nand after you source the right things in the post install logs you can specify the version you want to have :\nrvm install x.X.X\nWhere x.X.X is the ruby version you want clone or copy pour project , and run bundle install . google your error of installs (if you have some) IT should be now BLAZIN'FAST",
    "Does Docker alpine image support openjdk19": "You've got this a little backwards.\nYou're looking for someone that produces a supported image of OpenJDK 19 with an Alpine Linux image.\nThe Eclipse Temurin project supports official builds of OpenJDK in Docker, and it also comes with an Alpine image.\nBe very careful, given that it is up to this maintainer to decide what base images they choose to use, and they might drop support for Alpine.",
    "cannot copy to non-directory: Docker compose build": "ADD copies files, folders, or remote URLs from source to the dest path in the image's filesystem.\nADD source dest\nEg:\nADD hello.txt /absolute/path\nADD hello.txt relative/to/workdir\nIn your case,\nADD . /. does not mention any valid destination. /. is a root path. If you want to copy to the current work directory use ADD . ./ or to any other folder use ADD . <path to folder>.\nNote: When adding multiple source files or directories, there must be a / at the end of the destination directory.",
    "Traefik v2 listen on port": "For anyone else having similar problems, I found a solution. I needed to change ports section in docker-compose-traefik.yml from\nports:\n  - \"80:80\"\n  - \"9000:8080\"\nto\nports:\n  - \"80:80\"\n  - \"9000:8080\"\n  - \"5000:5000\" <-- add this\nHope this helps someone. :)",
    "How to specify c drive in a dockerfile (windows image)": "copy files from local disk to docker image https://www.youtube.com/watch?v=ht4gqt4zSyw",
    "Install SQL Server Driver to use Pyodbc lib in Airflow": "I found a much simpler lib to work with Airflow:\nimport pymssql\n\n    driver = pymssql.connect(server='192.168.0.1',user='user',password='pass',database='dbd',port='1433')\n    cursor = db.cursor()",
    "Webpacker manifest.json reverts to old asset references after docker-compose up --build": "Try this in config/YOUR_ENVIRONMENT.rb\n# Disable serving static files from the /public folder by default since # Apache or NGINX already handles this.\nconfig.public_file_server.enabled = true",
    "Can't install nokogiri with bundle when build docker container in archlinux": "Here is a Dockerfile based on your example which have installed nokogiri from Gemfile.\nDockerfile:\nFROM archlinux/base\nENV HOME=\"/srv\"\nENV LANG=ru_RU.UTF-8\nENV APP_ENV=production\nENV LANG=ru_RU.UTF-8\nENV PATH=$HOME/sbin:$HOME/bin:$HOME/.gem/ruby/2.5.0/bin:/usr/lib/ruby/gems/2.5.0/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl\n\nRUN pacman -Syu --noconfirm libxml2 libxslt pkg-config pkgconf procps-ng iproute2 iputils supervisor ruby make gcc grep postgresql-libs awk && \\\n  gem install bundler --no-ri --no-rdoc --no-document --no-user-install && \\\n  pacman -Scc --noconfirm\n\nCOPY Gemfile /srv/\nRUN cd /srv && rm -rf vendor && bundle config build.nokogiri --use-system-libraries --with-xml2-include=/usr/include/libxml2 --with-xslt-include=/usr/include/libxslt && bundle install --path=vendor --jobs=4 --binstubs\nGemfile\nsource 'https://rubygems.org'\n\ngem 'nokogiri'\nFeel free to use it.",
    "Docker Compose Could not locate Gemfile or .bundle/ directory": "try this:\nFROM ruby:2.5\nRUN apt-get update -qq && apt-get install -y build-essential libpq-dev nodejs\n\nWORKDIR app\n\nCOPY Gemfile Gemfile\n\n# Install Gems\nRUN bundle install\nADD . app",
    "Docker login within docker:stable image container": "try running that command like this\nRUN sudo docker login dtrurl -u username -password",
    "Windows Server 2016 OpenJDK docker container": "Why you are providing . at the end of the command\n\"docker build -f Dockerfile -t eurekaserverone .\"\nWhen you already provided -f flag with dockerfile. Try removing the . at the end and run the build command again.",
    "transform command line docker to Dockerfile": "Not sure if this help. If you are using Windows then just create a .bat file and put all your commands in it. You just have execute this .bat file and all your command will get executed.\nIf you are using unix/linux OS then create a .sh file and put all your commands in it.\nAs these are plain commands these should help.",
    "exec not found using Dockerfile ENTRYPOINT": "Try with ENTRYPOINT [\"exec\", \"/etc/init.d/hook\", \"/run/apache2/apache2.pid\", \"/etc/init.d/apache2\",  \"start\"]\ncheck the doc\nhttps://docs.docker.com/engine/reference/builder/#/entrypoint\nshould also work\nENTRYPOINT /etc/init.d/hook /run/apache2/apache2.pid /etc/init.d/apache2 start",
    "Running playwright in docker. Error:BrowserType.launch: Executable doesn't exist at /.cache/ms-playwright/chromium-1134/chrome-linux/chrome": "Check out this medium post. It might be helpful.\nIf the medium post doesn't work. The crude hack is to specify the execuatble_path in your browser launch in your code. Make it match the location installed when RUN playwright install --with-deps was executed.\nFor example\nbrowsers_path = \"/root/.cache/ms-playwright\"\nchromium_version = \"chromium-1134\"  # Replace this dynamically if needed\nexecutable_path = f\"{browsers_path}/{chromium_version}/chrome-linux/chrome\"\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(\n        executable_path=executable_path,  # Path to installed Chromium\n        headless=True\n    )\n    context = browser.new_context()\n    page = context.new_page()\n\n   \n\n    browser.close()",
    "Error when running \"npm install\" in ARM64 Dockerfile with Node.js 18": "I had same issue and tried to use below dockerfile FROM statement.\nFROM arm64v8/node:20.9.0\nMaybe node.js 18.x has some issue.\nuse node 20.x",
    "Entrypoint not found in Dockerfile": "There might be several reasons which are causing the issue, try one of the solution:\nParticularly in Windows 11, running the Alpine Linux image (alpine:3.14) might be the issue itself. Try with another image.\nTry adding the chmod +x /myapp before the ENTRYPOINT [\"/myapp\"] it helps you to check whether /myapp is executable inside the container.",
    "Docker with Java Unable to access jarfile application.jar": "This error raised because docker cannot find MANIFEST.MF file in the jar file. You can check it by typing:\njava -Djarmode=layertools -jar target/api-gateway-0.0.1-SNAPSHOT.jar list\nIf you haven't fixed the issue yet, follow these steps:\nAdd this plugin:\n    <plugin>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-maven-plugin</artifactId>\n            <configuration>\n                <layers>\n                    <enabled>true</enabled>\n                </layers>\n            </configuration>\n    </plugin>\nIn terminal, type:\nmvn clean package\nNote: You should have maven installed locally\nTry viewing the contents of the newly generated jar file:\njava -Djarmode=layertools -jar target/api-gateway-0.0.1-SNAPSHOT.jar list\nYous should see the following output:\ndependencies\nspring-boot-loader\nsnapshot-dependencies\napplication\nNow, you can build the image by typing:\ndocker build -t api-gateway-layered -f Dockerfile.layered .\nYou can explore more from baeldung (for v3.0.0+ too), spring-boot-2.3.0-M, and other sites.",
    "Why don't we build jar file in a dockerfile [closed]": "One advantage of a Spring Boot executable jar is that it contains all of your application's code as well as its dependencies. This gives you a single unit that you can deploy. When you're packaging your application in a container, it becomes this single unit of deployment with the added benefit that it also contains the JVM and anything else your application needs. Given that the container is a single unit of deployment, the jar packaging arguably isn't needed any more.\nAs described in the Spring Boot reference documentation, unpacking the executable jar when building the container can result in a slight improvement in startup time. You can explode the jar file and then use Spring Boot's JarLauncher to launch the exploded \"jar\":\n$ jar -xf myapp.jar\n$ java org.springframework.boot.loader.JarLauncher\nThis approach ensures that the classpath has the same order as it would have when running the application using java -jar.\nAlternatively, you can specify the classpath manually and run the application's main method directly:\n$ jar -xf myapp.jar\n$ java -cp BOOT-INF/classes:BOOT-INF/lib/* com.example.MyApplication\nThis will result in a further small boost in startup time at the cost of potentially changing the order of the classpath.",
    "cannot load certificate \"/etc/ssl/ServerCertificate.crt\": BIO_new_file() failed (SSL: error:02001002:system library:fopen:No such file or directory": "Added port 80 in Expose and it worked!!",
    "W: Failed to fetch http://httpredir.debian.org/debian/dists/jessie-updates/InRelease Unable to find": "I had the same issue recently and I find this solution.\nSome quick snippet from the solution:\nRUN echo \"deb [check-valid-until=no] http://cdn-fastly.deb.debian.org/debian jessie main\" > /etc/apt/sources.list.d/jessie.list\nRUN echo \"deb [check-valid-until=no] http://archive.debian.org/debian jessie-backports main\" > /etc/apt/sources.list.d/jessie-backports.list\nRUN sed -i '/deb http:\\/\\/deb.debian.org\\/debian jessie-updates main/d' /etc/apt/sources.list\nRUN apt-get -o Acquire::Check-Valid-Until=false update\nHowever in my case I have .dockerfile working with install.sh, the solution above give me a new error \"Release file expired and need to add this extra line:\nRUN echo 'Acquire::Check-Valid-Until no;' > /etc/apt/apt.conf.d/99no-check-valid-until",
    "IntelliJ IDEA not showing quick documentation in Dockerfile": "use VSCode idea to show document see image",
    "How can I allocate more memory to the Windows VM that Docker for Windows is using when I run a Windows container?": "I am using docker desktop for windows(use hyper-v). Steps to change memory:\nright click docker desktop icon.\nSettings\nResources\nADVANCED you can change cpus/memory/swap in this subpage.\nApply and restart.\nCheck effects:",
    "In Docker not able to register a third party DLL using regsvr32 [On Windows]": "regsvr32 /S\nThe /S makes it graphically silent, and allows the registration to complete.\nHave fun!",
    "How to copy multiple files in one layer using a Docker file to different locations?": "Create a file .dockerignore in your docker build context directory. Create a soft link (ln) of the root directory you want to copy and exclude the directories you don't want in your .dockerignore.\nIn your case, you don't need the soft link as the directory is already in the docker build context. Now, add the directories in the .dockerignore file that you don't want eg. if you don't want bin directory you can do that as,\n# this is .dockerignore file\noutputs/output/build/bin*\nFinally in your Dockerfile,\nCOPY outputs/output/build/ /root/\nDetails are here.",
    "Increasing docker volume size using docker-compose": "Yes, this is a common practice, you can use a host volume. Change the volume line to - ./uploads:/usr/src/flask-app/uploads.",
    "Using variable interpolation in Gunicorn inside DockerFile": "Using Docker's ENV directive works for me:\nENV PORT 8080\n\nCMD [\"gunicorn\", \"--timeout\", \"120\", \"--bind\", \"0.0.0.0:${PORT}\", \"wsgi:app\"]\nWhen I change the PORT container environment variable to something other than the specified default of 8080, gunicorn binds to that new port as you'd expect.",
    "How to enable port bindings on rootless containers?": "I suspect the user on your machine using which you're running docker command is a non-root user.\nSuppose your current user on ec2-machine is user1 (You can verify that by running users command). Then run this command to add the user1 to docker group.\nusermod -aG docker user1 and then try to run your container.\nTry this and let me know.\nFor more info about what is rootless container check this out.",
    "Container Docker is exited after run": "You can set the user already when you ADD the file:\nFROM sonarqube\n\nADD --chown=sonarqube:sonarqube https://github.com/gabrie-allaigre/sonar-gitlab-plugin/releases/download/4.0.0/sonar-gitlab-plugin-4.0.0.jar /opt/sonarqube/extensions/plugins/\nI tested this and it works.",
    "Dockerfile placeholders not replaced during build": "ARG and ENV are not replaced by Docker in an ENTRYPOINT or a CMD when you use the EXEC form inside []. If the ENTRYPOINT or CMD use the shell form (string, not an array) the shell will be able to do the variable substitution for you.\nARG and ENV variables will be available in RUN commands in the container as environment variables.\nDocker will also replace $VARIABLES in the Dockerfile in the following instructions:\nADD\nCOPY\nENV\nEXPOSE\nFROM\nLABEL\nSTOPSIGNAL\nUSER\nVOLUME\nWORKDIR\nOnly ENV variables will become available in a CMD or ENTRYPOINT, only in the environment of the running container:\nUsing ARG\nFROM frolvlad/alpine-oraclejdk8:slim\nARG APP_NAME=client-default\nENV APP_NAME=$APP_NAME\nRUN mkdir -p /client/\nADD build/libs/$APP_NAME.jar /client/$APP_NAME.jar\nENTRYPOINT [\"sh\", \"-c\", \"java -jar /client/$APP_NAME.jar\"]\nor with only ENV\nFROM frolvlad/alpine-oraclejdk8:slim\nENV APP_NAME=client-default\nRUN mkdir -p /client/\nADD build/libs/$APP_NAME.jar /client/$APP_NAME.jar\nENTRYPOINT [\"sh\", \"-c\", \"java -jar /client/$APP_NAME.jar\"]",
    "How to install kubectl and helm using dockerfile?": "Working Dockerfile. This will install the latest and stable versions of kubectl and helm-3\nFROM python:3.7-alpine\nCOPY ./ /usr/src/app/\nWORKDIR /usr/src/app\nRUN apk add curl openssl bash --no-cache\nRUN curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" \\\n    && chmod +x ./kubectl \\\n    && mv ./kubectl /usr/local/bin/kubectl \\\n    && curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \\\n    && chmod +x get_helm.sh && ./get_helm.sh",
    "Using ENV var in CMD in the Dockerfile [duplicate]": "When you use an execution list, Docker will interpret the given command literally (without variable expansion).\nIf you want to expand variables, you need to use a shell in your CMD. You can replace your ENTRYPOINT and CMD to use only this:\nCMD [\"sh\", \"-c\", \"ng serve --host ${HOST} --port ${PORT} --disable-host-check\"]\nFurther reading: Dockerfile CMD doesn't understand ENV variables [#5509]",
    "Docker: exec /usr/local/openjdk-11/bin/java: exec format error": "solved;\ndocker buildx build --platform linux/amd64,linux/arm64 -t madenmustafa/postmage --push .",
    "Getting executable file not found in $PATH: un known error message": "You dont need to use go run ... since you previously run go build, built file will be named after directory and looks like its app, try CMD [\"./app\"]\nBTW the right usage of CMD in your case would be CMD [\"go\", \"run\", \"main.go\"], the error you have is related to CMD command its assume go run main.go is one file but it's not.",
    "Docker build from local Dockerfile hangs (Windows 10)": "What worked for me was adding a .dockerignore file and add there the folders that are not part of the built image (in my case /node_modules).",
    "Docker Volumes - Create options (Driver)": "There are two main categories of data \u2014 persistent and non-persistent.\nPersistent is the data you need to keep. Things like; customer records, financial data, research results, audit logs, and even some types of application log data. Non-persistent is the data you don\u2019t need to keep.\nBoth are important, and Docker has solutions for both. To deal with non-persistent data, every Docker container gets its own non-persistent storage. This is automatically created for every container and is tightly coupled to the lifecycle of the container. As a result, deleting the container will delete the storage and any data on it. To deal with persistent data, a container needs to store it in a volume. Volumes are separate objects that have their lifecycles decoupled from containers. This means you can create and manage volumes independently, and they\u2019re not tied to the lifecycle of any container. Net result, you can delete a container that\u2019s using a volume, and the volume won\u2019t be deleted.\nThis writable layer of local storage is managed on every Docker host by a storage driver (not to be confused with a volume driver). If you\u2019re running Docker in production on Linux, you\u2019ll need to make sure you match the right storage driver with the Linux distribution on your Docker host. Use the following list as a guide:\nRed Hat Enterprise Linux: Use the overlay2 driver with modern versions of RHEL running Docker 17.06 or higher. Use the devicemapper driver with older versions. This applies to Oracle Linux and other Red Hat related upstream and downstream distros.\nUbuntu: Use the overlay2 or aufs drivers. If you\u2019re using a Linux 4.x kernel or higher you should go with overlay2.\nSUSE Linux Enterprise Server: Use the btrfs storage driver.\nWindows Windows only has one driver and it is configured by default.\nBy default, Docker creates new volumes with the built-in local driver. As the name suggests, volumes created with the local driver are only available to containers on the same node as the volume. You can use the -d flag to specify a different driver. Third-party volume drivers are available as plugins. These provide Docker with seamless access external storage systems such as cloud storage services and on-premises storage systems including SAN or NAS.\n$ docker volume inspect myvol\n[\n{\n\"CreatedAt\": \"2020-05-02T17:44:34Z\",\n\"Driver\": \"local\",\n\"Labels\": {},\n\"Mountpoint\": \"/var/lib/docker/volumes/myvol/_data\",\n\"Name\": \"myvol\",\n\"Options\": {},\n\"Scope\": \"local\"\n}\n]\nNotice that the Driver and Scope are both local. This means the volume was created with the local driver and is only available to containers on this Docker host. The Mountpoint property tells us where in the Docker host\u2019s filesystem the volume exists.\nWith bind mounts\nversion: '3.7'\nservices:\n   maria_db:\n    image: mariadb:10.4.13\n    environment:\n      MYSQL_ROOT_PASSWORD: Test123@123\n      MYSQL_DATABASE: database\n    ports:\n      - 3306:3306\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - ./data_mariadb/:/var/lib/mysql/  \nWith volume mount\nversion: \"3.8\"\nservices:\n  web:\n    image: mariadb:10.4.13\n    volumes:\n      - type: volume\n        source: dbdata\n        target: /var/lib/mysql/ \n\nvolumes:\n  dbdata:  \nBind mounts explanation\nBind mounts have been around since the early days of Docker. Bind mounts have limited functionality compared to volumes. When you use a bind mount, a file or directory on the host machine is mounted into a container. The file or directory is referenced by its full or relative path on the host machine. By contrast, when you use a volume, a new directory is created within Docker\u2019s storage directory on the host machine, and Docker manages that directory\u2019s contents.\ntmpfs mounts explanation\nVolumes and bind mounts let you share files between the host machine and container so that you can persist data even after the container is stopped. If you\u2019re running Docker on Linux, you have a third option: tmpfs mounts. When you create a container with a tmpfs mount, the container can create files outside the container\u2019s writable layer. As opposed to volumes and bind mounts, a tmpfs mount is temporary and only persisted in the host memory. When the container stops, the tmpfs mount is removed, and files are written there won\u2019t be persisted.\nVolume explanation\nVolumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker.",
    "Flask and React App in single Docker Container": "A React application doesn't usually have a server per se (development-only Docker setups aside). Instead, you run a tool like Webpack to compile it down to static files, which you can then serve to the browser, which then runs them.\nOn your host system you'd run something like\nyarn build\nwhich produces a dist directory; then you'd copy this into your Flask static directory.\nIf you do this entirely ahead-of-time, then you can run your application out of a Python virtual environment, which will be a much easier development and test setup, and the Dockerfile you show won't change.\nIf you want to build this entirely in Docker (for example to take advantage of a more Docker-native automated build system) a multi-stage build matches well here. You can use a first stage to build the front-end application, and then COPY that into the final application in the second stage. That looks roughly like:\nFROM node:12.18.0-alpine as build\nWORKDIR /app/react_frontend\nCOPY ./react_frontend/package.json ./\nCOPY ./react_frontend/package-lock.json ./\nRUN npm ci\nCOPY ./react_frontend ./\nRUN npm run build\n\nFROM python:3.6.9-slim-buster\nWORKDIR /app/flask_backend\nENV PYTHONPATH \"${PYTHONPATH}:/app\"\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY ./flask_backend ./\nCOPY --from=build /app/react_frontend/dist/ ./static/\n\nCMD python3 app/webapp/app.py\nThis approach is not compatible with setups that overwrite Docker image contents using bind mounts. A non-Docker host Node and Python setup will be a much easier development environment, and for this particular setup isn't likely to be substantially different from the Docker setup.",
    "Alpine with AMQP php extension": "You have to install those librairies rabbitmq-c & rabbitmq-c-dev.\nSo you can do:\nFROM php:7.3-fpm\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        git \\\n        zlib1g-dev \\\n        libxml2-dev \\\n        rabbitmq-c rabbitmq-c-dev \\\n        libzip-dev \\\n        libonig-dev \\\n    && docker-php-ext-install \\\n        pdo_mysql \\\n        zip \\\n    && pecl install amqp \\\n    && docker-php-ext-enable amqp # Enable the Extension\n\nRUN curl -sS https://getcomposer.org/installer | php && mv composer.phar /usr/local/bin/composer\nCOPY project/ /var/www/project\nWORKDIR /var/www/project/",
    "docker change image name?": "Compose generates its own names for most things, and it usually does this by combining the project name (by default, the name of the current directory) with the name in the docker-compose.yml file. If you look at docker ps output, you'll probably see a container named poc_tool_poc_tool_1; if you look at docker network ls, you'll see poc_tool_default; and if you had named volumes, they'd also have this prefix.\nIn the docker-compose.yml file, it's valid to specify both build: and image:. If you do, then docker-compose build uses the image: name instead of generating its own.\nversion: \"3\"\nservices:\n  poc_tool:\n    build: .\n    image: registry.example.com/name/poc_tool:${TAG:-latest}\n(Style-wise, this means you don't generally need to include the project name as part of your Compose service names. It also means it's important to not name image: as the base image of your Dockerfile, since Compose will happily overwrite it with your application code.)",
    "How to use cp command in dockerfile": "You can't do that.\nCOPY copies from the host to the image.\nRUN cp copies from a location in the image to another location in the image.\nTo get it all into a single COPY statement, you can create the file structure you want on the host and then use tar to make it a single file. Then when you COPY or ADD that tar file, Docker will unpack it and put the files in the correct place. But with the current structure your files have on the host, it's not possible to do in a single COPY command.",
    "How to pass .env file to docker image in a Go project?": "COPY . /go/src/gothamcity only copies .env to build container. You have to explicitly copy it to your main container like this:\nFROM golang:alpine AS build-env\nLABEL MAINTAINER \"Amit Pal <amit.pal@fynchmobility.com>\"\nENV GOPATH /go\nWORKDIR /go/src\nCOPY . /go/src/gothamcity\nRUN cd /go/src/gothamcity && go build .\n\nFROM alpine\nRUN apk update && apk add ca-certificates && rm -rf /var/cache/apk*\nWORKDIR /app\nCOPY --from=build-env /go/src/gothamcity/gothamcity /app\nCOPY .env /app\n\nEXPOSE 8080\n\nENTRYPOINT [ \"./gothamcity\" ]\nReason is that when you are saying \"FROM alpine\" this becomes a brand new blank container. That's how multi-stage builds are working.",
    "When I run my dotnet server in docker I get socket hang up error": "Just in case you are here looking for and answer for .net8 and getting the error with the command:\ndocker run -p 1414:80 dotnetapi\napparently there is a new default port (8080) for .net 8, so now you will have to run it like this:\ndocker run -p 1414:8080 dotnetapi\nWhere the first -p flag is {external_port}:{internal_port} and 'dotnetapi' is your docker container",
    "What does this command do?: `docker build . -f Dockerfile2 -t `": ". means the current directory where you are. And the Dockerfile is in it. If you do not in the directory of the Dockerfile, you will get the error.\nThe full command : docker build path -f Dockfile -t containerName. Also the document docker build [OPTIONS] PATH | URL | -.",
    "Simpliest Dockerfile ever to untar: file not found": "Always read the output carefully for hints:\ntar (child): xz: Cannot exec: No such file or directory\nYou are missing the xz-utils package in the image which is required to handle xz archives. Try:\nRUN apt-get install xz-utils && tar -C /tmp/ -xf /tmp/glib-2.52.1.tar.xz",
    "Append argument to ENTRYPOINT in Docker run where some args are already defined": "The main container command is made up of two parts. The string you pass after the docker run image-name replaces the Dockerfile CMD, and it's appended to the Dockerfile ENTRYPOINT.\nFor your docker run command to work, you need to provide the command you want to run as ENTRYPOINT and its arguments as CMD. You do not need an environment variable here. However, it is important that both parts use JSON-array syntax and that neither invokes a shell. If ENTRYPOINT includes a shell then things get syntactically complex (see @KamilCuk's answer); if CMD includes a shell then it won't get invoked but the command will be invoked with /bin/sh and -c as parameters instead.\nFROM debian:latest\nCOPY myscript.sh /usr/local/bin/myscript # preserves execute permissions\nENTRYPOINT [\"myscript\"]                  # in a $PATH directory\nCMD [\"my\", \"arg\", \"in\", \"Dockerfile\"]\ndocker run --rm the-image\ndocker run --rm the-image my arg from command line\nIf you want the initial set of command-line arguments to be included and the docker run arguments to be appended, you can move them into the ENTRYPOINT line; note that the docker run --entrypoint is syntactically awkward if you ever do decide you need to remove some of the options.\nENTRYPOINT [\"myscript\", \"--first-default\", \"--second-default\"]\n# CMD []\ndocker run --rm the-image\ndocker run --rm the-image --user-option\ndocker run --entrypoint myscript the-image --first-default --no-second-default\nIf you can update your application to accept options as environment variables in addition to command-line settings, this makes all of this syntactically easier.\nENV FIRST_DEFAULT=yes\nENV SECOND_DEFAULT=yes\nCMD [\"myscript\"]\ndocker run --rm the-image\ndocker run --rm -e USER_OPTION=yes the-image\ndocker run --rm -e SECOND_DEFAULT=no the-image",
    "How can I make packages installed using Poetry accessible in Docker?": "Poetry documents itself as trying very very hard to always run inside a virtual environment. However, a Docker container is itself isolation from other Pythons, and it's normal (and easiest) to install packages in the \"system\" Python.\nThere is a poetry export command that can convert Poetry's files to a normal pip requirements.txt file, and from there you can RUN pip install in your Dockerfile. You could use a multi-stage Dockerfile to generate that file without actually including Poetry in your main image.\nFROM python:3.9-slim-buster AS poetry\nRUN pip install poetry\nWORKDIR /app\nCOPY pyproject.toml poetry.lock .\nRUN poetry export -f requirements.txt --output requirements.txt\n\nFROM python:3.9-slim-buster\nWORKDIR /app\nCOPY --from=poetry /app/requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"./manage.py\", \"runserver\", \"0.0.0.0:8000\"]\ndjango should show up in the generated requirements.txt file, and since pip install installs it as a \"normal\" \"system\" Python package, your application should be able to see it normally, without tweaking environment variables or other settings."
}